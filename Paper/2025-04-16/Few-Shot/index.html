<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Siamese Network with Dual Attention for EEG-Driven Social Learning   Bridging the Human-Robot Gap in Long-Tail Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b795a39a5921ceccac85887f872f59fc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-16-æ›´æ–°"><a href="#2025-04-16-æ›´æ–°" class="headerlink" title="2025-04-16 æ›´æ–°"></a>2025-04-16 æ›´æ–°</h1><h2 id="Siamese-Network-with-Dual-Attention-for-EEG-Driven-Social-Learning-Bridging-the-Human-Robot-Gap-in-Long-Tail-Autonomous-Driving"><a href="#Siamese-Network-with-Dual-Attention-for-EEG-Driven-Social-Learning-Bridging-the-Human-Robot-Gap-in-Long-Tail-Autonomous-Driving" class="headerlink" title="Siamese Network with Dual Attention for EEG-Driven Social Learning:   Bridging the Human-Robot Gap in Long-Tail Autonomous Driving"></a>Siamese Network with Dual Attention for EEG-Driven Social Learning:   Bridging the Human-Robot Gap in Long-Tail Autonomous Driving</h2><p><strong>Authors:Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat</strong></p>
<p>Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments. </p>
<blockquote>
<p>å¸¦æœ‰è½®å­ã€å››è¶³æˆ–äººå½¢å½¢å¼çš„æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°è¢«é›†æˆåˆ°å»ºç­‘ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œä¸äººç±»çš„ç¤¾ä¼šå­¦ä¹ ä¸åŒï¼Œå®ƒä»¬åœ¨å†…åœ¨è®¤çŸ¥å‘å±•çš„å…³é”®é€”å¾„ä¸Šæœ‰æ‰€ç¼ºå¤±ï¼Œå³åœ¨ä¸äººç±»äº’åŠ¨è¿‡ç¨‹ä¸­ä»äººç±»åé¦ˆä¸­å­¦ä¹ ã€‚ä¸ºäº†ç†è§£äººç±»åœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­çš„æ™®éè§‚å¯Ÿã€ç›‘ç£ä»¥åŠå…±äº«æ§åˆ¶ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè„‘æœºæ¥å£ï¼ˆBCIï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¯¹è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·è¿›è¡Œåˆ†ç±»ï¼Œä»¥æ£€æµ‹è®¤çŸ¥éœ€æ±‚å’Œå®‰å…¨å…³é”®äº‹ä»¶ã€‚ä½œä¸ºä¸€ä¸ªåŠæ—¶ä¸”é¼“èˆäººå¿ƒçš„ååŒæœºå™¨äººå·¥ç¨‹åº”ç”¨ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäº†ä¸€ä¸ªæœ‰äººç±»å‚ä¸çš„é—­ç¯åœºæ™¯æ¥æ ‡è®°åŠè‡ªä¸»æœºå™¨äººé©¾é©¶ä¸­çš„å±é™©äº‹ä»¶â€”â€”è¿™ä»£è¡¨äº†æ™ºèƒ½ç§»åŠ¨ç³»ç»Ÿå’Œæœºå™¨äººè½¦è¾†å®‰å…¨æ€§èƒ½ä¸­æŒä¹…çš„ç“¶é¢ˆçš„é•¿æœŸå°¾éƒ¨æ¡ˆä¾‹ã€‚åŸºäºæœ€è¿‘çš„å°æ ·æœ¬å­¦ä¹ è¿›å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæ³¨æ„åŠ›å­ªç”Ÿå·ç§¯ç½‘ç»œï¼Œç»“åˆåŠ¨æ€æ—¶é—´è§„æ•´è´¨å¿ƒå¹³å‡æ³•ï¼Œç”Ÿæˆç¨³å¥çš„EEGç¼–ç ä¿¡å·è¡¨ç¤ºã€‚é€†å‘æºå®šä½æ˜¾ç¤ºBroadman 4åŒºå’Œ9åŒºçš„æ¿€æ´»ï¼Œè¿™è¡¨æ˜åœ¨ä»»åŠ¡ç›¸å…³çš„å¿ƒç†æ„è±¡è¿‡ç¨‹ä¸­æ„ŸçŸ¥-è¡ŒåŠ¨è€¦åˆã€‚è¯¥æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„æ¡ä»¶ä¸‹å®ç°äº†80%çš„åˆ†ç±»ç²¾åº¦ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—ç‰¹å¾çš„å®ç”¨æ€§å¢åŠ äº†è¿‘100%ï¼Œè¿™æ˜¯é€šè¿‡é›†æˆæ¢¯åº¦å½’å±åº¦æ¥è¡¡é‡çš„ã€‚é™¤äº†æ€§èƒ½ä¹‹å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæˆ‘ä»¬ç†è§£è„‘æœºæ¥å£ä»£ç†æ‰€éœ€çš„è®¤çŸ¥æ¶æ„â€”â€”ç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å’Œè®°å¿†æœºåˆ¶åœ¨åˆ†ç±»ä¸åŒå¿ƒç†çŠ¶æ€å’Œæ”¯æŒä¸»ä½“é—´å’Œä¸»ä½“å†…é€‚åº”ä¸­çš„ä½œç”¨ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹ç ”ç©¶æ¨åŠ¨äº†è®¤çŸ¥æœºå™¨äººä»¥åŠåœ¨å¤æ‚å»ºç­‘ç¯å¢ƒä¸­æœåŠ¡æœºå™¨äººçš„ç¤¾ä¼šæŒ‡å¯¼å­¦ä¹ çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10296v1">PDF</a> 50 pages, 18 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè„‘æœºæ¥å£ï¼ˆBCIï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡é‡‡é›†å’Œåˆ†æè„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·ï¼Œä»¥å®ç°å¯¹äººç±»åœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­æ™®éçš„è§‚å¯Ÿã€ç›‘ç£å’Œå…±äº«æ§åˆ¶çš„ç†è§£ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å…ˆè¿›çš„å°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡åŒæ³¨æ„åŠ›Siameseå·ç§¯ç½‘ç»œå’ŒåŠ¨æ€æ—¶é—´è§„æ•´é‡å¿ƒå¹³å‡æ³•ç”Ÿæˆç¨³å¥çš„EEGä¿¡å·è¡¨ç¤ºã€‚é€†æºå®šä½æ­ç¤ºäº†Broadman 4åŒºå’Œ9åŒºçš„æ¿€æ´»ï¼Œè¡¨æ˜åœ¨ä»»åŠ¡ç›¸å…³å¿ƒç†å›¾åƒä¸­çš„æ„ŸçŸ¥åŠ¨ä½œè€¦åˆã€‚åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹ï¼Œè¯¥æ¨¡å‹å®ç°äº†80%çš„åˆ†ç±»ç²¾åº¦ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—ç‰¹å¾æ•ˆç”¨æé«˜äº†è¿‘100%ã€‚æœ¬ç ”ç©¶ä¸ä»…æé«˜äº†æ€§èƒ½ï¼Œè€Œä¸”åŠ æ·±äº†å¯¹BCIä»£ç†æ‰€éœ€è®¤çŸ¥æ¶æ„çš„ç†è§£ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å’Œè®°å¿†æœºåˆ¶åœ¨åˆ†ç±»å„ç§å¿ƒç†çŠ¶æ€å’Œæ”¯æŒè·¨ä¸»ä½“å’Œä¸»ä½“å†…é€‚åº”ä¸­çš„ä½œç”¨ã€‚è¿™é¡¹ç ”ç©¶æ¨åŠ¨äº†è®¤çŸ¥æœºå™¨äººå’Œå¤æ‚å»ºç­‘ç¯å¢ƒä¸­æœåŠ¡æœºå™¨äººçš„ç¤¾ä¼šæŒ‡å¯¼å­¦ä¹ çš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†åŸºäºè„‘æœºæ¥å£ï¼ˆBCIï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç†è§£äººç±»åœ¨ä¸æœºå™¨äººäº¤äº’ä¸­çš„è§‚å¯Ÿã€ç›‘ç£å’Œå…±äº«æ§åˆ¶ã€‚</li>
<li>ç»“åˆå°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡åŒæ³¨æ„åŠ›Siameseå·ç§¯ç½‘ç»œå’ŒDynamic Time Warping Barycenter Averagingæ–¹æ³•å¤„ç†EEGä¿¡å·ã€‚</li>
<li>ç ”ç©¶é€šè¿‡é€†æºå®šä½å‘ç°äº†ä¸ä»»åŠ¡ç›¸å…³å¿ƒç†å›¾åƒæ„ŸçŸ¥åŠ¨ä½œè€¦åˆçš„è„‘éƒ¨æ¿€æ´»åŒºåŸŸã€‚</li>
<li>æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹è¾¾åˆ°äº†80%çš„åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹çš„æ˜¾è‘—ç‰¹å¾æ•ˆç”¨æå‡äº†è¿‘100%ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ä»…å…³æ³¨æ€§èƒ½æå‡ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†BCIæ‰€éœ€çš„è®¤çŸ¥æ¶æ„ã€‚</li>
<li>ç ”ç©¶æ¨åŠ¨äº†è®¤çŸ¥æœºå™¨äººåœ¨å¤æ‚å»ºç­‘ç¯å¢ƒä¸­çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœåŠ¡æœºå™¨äººé¢†åŸŸçš„ç¤¾ä¼šæŒ‡å¯¼å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-21431de1f7e72376c874e43864193ca1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AGO-Adaptive-Grounding-for-Open-World-3D-Occupancy-Prediction"><a href="#AGO-Adaptive-Grounding-for-Open-World-3D-Occupancy-Prediction" class="headerlink" title="AGO: Adaptive Grounding for Open World 3D Occupancy Prediction"></a>AGO: Adaptive Grounding for Open World 3D Occupancy Prediction</h2><p><strong>Authors:Peizheng Li, Shuxiao Ding, You Zhou, Qingwen Zhang, Onat Inak, Larissa Triess, Niklas Hanselmann, Marius Cordts, Andreas Zell</strong></p>
<p>Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D representation from sensor inputs while recognizing both known and unknown objects. Transferring open-vocabulary knowledge from vision-language models (VLMs) offers a promising direction but remains challenging. However, methods based on VLM-derived 2D pseudo-labels with traditional supervision are limited by a predefined label space and lack general prediction capabilities. Direct alignment with pretrained image embeddings, on the other hand, fails to achieve reliable performance due to often inconsistent image and text representations in VLMs. To address these challenges, we propose AGO, a novel 3D occupancy prediction framework with adaptive grounding to handle diverse open-world scenarios. AGO first encodes surrounding images and class prompts into 3D and text embeddings, respectively, leveraging similarity-based grounding training with 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into a space aligned with VLM-derived image embeddings, reducing modality gaps. Experiments on Occ3D-nuScenes show that AGO improves unknown object prediction in zero-shot and few-shot transfer while achieving state-of-the-art closed-world self-supervised performance, surpassing prior methods by 4.09 mIoU. </p>
<blockquote>
<p>å¼€æ”¾ä¸–ç•Œ3Dè¯­ä¹‰å ç”¨é¢„æµ‹æ—¨åœ¨ä»ä¼ æ„Ÿå™¨è¾“å…¥ç”Ÿæˆä½“ç´ åŒ–3Dè¡¨ç¤ºï¼ŒåŒæ—¶è¯†åˆ«å·²çŸ¥å’ŒæœªçŸ¥å¯¹è±¡ã€‚ä»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è½¬ç§»å¼€æ”¾è¯æ±‡çŸ¥è¯†æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œä½†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç„¶è€Œï¼ŒåŸºäºVLMè¡ç”Ÿçš„å…·æœ‰ä¼ ç»Ÿç›‘ç£çš„2Dä¼ªæ ‡ç­¾çš„æ–¹æ³•å—é™äºé¢„å®šä¹‰çš„æ ‡ç­¾ç©ºé—´ï¼Œå¹¶ä¸”ç¼ºä¹é€šç”¨çš„é¢„æµ‹èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸é¢„è®­ç»ƒå›¾åƒåµŒå…¥çš„ç›´æ¥å¯¹é½ç”±äºVLMä¸­å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºç»å¸¸ä¸ä¸€è‡´ï¼Œæ— æ³•å®ç°å¯é æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AGOï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰è‡ªé€‚åº”å®šä½åŠŸèƒ½çš„æ–°å‹3Då ç”¨é¢„æµ‹æ¡†æ¶ï¼Œç”¨äºå¤„ç†å„ç§å¼€æ”¾ä¸–ç•Œåœºæ™¯ã€‚AGOé¦–å…ˆç¼–ç å‘¨å›´å›¾åƒå’Œç±»åˆ«æç¤ºä¸º3Då’Œæ–‡æœ¬åµŒå…¥ï¼Œåˆ†åˆ«åˆ©ç”¨åŸºäºç›¸ä¼¼æ€§çš„å®šä½è®­ç»ƒå’Œ3Dä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæ¨¡æ€é€‚é…å™¨å°†3DåµŒå…¥æ˜ å°„åˆ°ä¸VLMè¡ç”Ÿçš„å›¾åƒåµŒå…¥å¯¹é½çš„ç©ºé—´ï¼Œå‡å°‘æ¨¡æ€é—´éš™ã€‚åœ¨Occ3D-nuScenesä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAGOåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è½¬ç§»ä¸­æ”¹è¿›äº†æœªçŸ¥å¯¹è±¡çš„é¢„æµ‹ï¼ŒåŒæ—¶åœ¨å°é—­ä¸–ç•Œè‡ªç›‘ç£æƒ…å†µä¸‹è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”å…ˆå‰çš„æ–¹æ³•é«˜å‡º4.09 mIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10117v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼€æ”¾ä¸–ç•Œä¸‹çš„ä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨ä»ä¼ æ„Ÿå™¨è¾“å…¥ç”Ÿæˆä½“ç´ åŒ–çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶è¯†åˆ«å·²çŸ¥å’ŒæœªçŸ¥ç‰©ä½“ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è½¬ç§»å¼€æ”¾è¯æ±‡çŸ¥è¯†æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œä½†é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¦‚åŸºäºVLMè¡ç”Ÿçš„äºŒç»´ä¼ªæ ‡ç­¾å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•å—é™äºé¢„å®šä¹‰çš„æ ‡ç­¾ç©ºé—´ï¼Œç¼ºä¹é€šç”¨é¢„æµ‹èƒ½åŠ›ã€‚ç›´æ¥å¯¹é½é¢„è®­ç»ƒå›¾åƒåµŒå…¥åˆ™ç”±äºVLMsä¸­å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºçš„ä¸ä¸€è‡´æ€§è€Œæ— æ³•å®ç°å¯é æ€§èƒ½ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAGOçš„æ–°å‹ä¸‰ç»´å ç”¨é¢„æµ‹æ¡†æ¶ï¼Œå…·æœ‰è‡ªé€‚åº”å®šä½åŠŸèƒ½ï¼Œä»¥å¤„ç†å¤šæ ·åŒ–çš„å¼€æ”¾ä¸–ç•Œåœºæ™¯ã€‚AGOé€šè¿‡ç¼–ç å‘¨å›´å›¾åƒå’Œç±»åˆ«æç¤ºåˆ°ä¸‰ç»´å’Œæ–‡æœ¬åµŒå…¥ï¼Œåˆ©ç”¨åŸºäºç›¸ä¼¼æ€§çš„å®šä½è®­ç»ƒå’Œä¸‰ç»´ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæ¨¡æ€é€‚é…å™¨å°†ä¸‰ç»´åµŒå…¥æ˜ å°„åˆ°ä¸VLMè¡ç”Ÿçš„å›¾åƒåµŒå…¥å¯¹é½çš„ç©ºé—´ï¼Œå‡å°‘æ¨¡æ€å·®è·ã€‚åœ¨Occ3D-nuScenesçš„å®éªŒæ˜¾ç¤ºï¼ŒAGOåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è½¬ç§»ä¸­æ”¹è¿›äº†æœªçŸ¥ç‰©ä½“çš„é¢„æµ‹ï¼ŒåŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å°é—­ä¸–ç•Œè‡ªç›‘ç£æ€§èƒ½ï¼Œè¶…å‡ºä¹‹å‰çš„æ–¹æ³•4.09 mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾ä¸–ç•Œä¸‹çš„ä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹æ—¨åœ¨ä»ä¼ æ„Ÿå™¨è¾“å…¥ç”Ÿæˆä½“ç´ åŒ–çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶è¯†åˆ«å·²çŸ¥å’ŒæœªçŸ¥ç‰©ä½“ã€‚</li>
<li>ä»è§†è§‰è¯­è¨€æ¨¡å‹è½¬ç§»å¼€æ”¾è¯æ±‡çŸ¥è¯†åœ¨è¿™ä¸€é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºé¢„å®šä¹‰çš„æ ‡ç­¾ç©ºé—´ï¼Œç¼ºä¹é€šç”¨é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>ç›´æ¥å¯¹é½é¢„è®­ç»ƒå›¾åƒåµŒå…¥ç”±äºè¡¨ç¤ºä¸ä¸€è‡´æ€§è€Œæ— æ³•å®ç°å¯é æ€§èƒ½ã€‚</li>
<li>AGOæ¡†æ¶é€šè¿‡ç¼–ç å‘¨å›´å›¾åƒå’Œç±»åˆ«æç¤ºåˆ°ä¸‰ç»´å’Œæ–‡æœ¬åµŒå…¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>AGOåˆ©ç”¨åŸºäºç›¸ä¼¼æ€§çš„å®šä½è®­ç»ƒå’Œä¸‰ç»´ä¼ªæ ‡ç­¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc4f1c69d9ed9d9eefa64a32654ec3dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-694a1e667b83021d67281ec43a40babb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0904340c6f18ce30f9b401fa88008e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-048da004411e345bd2625ce4d95a7471.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Relation-augmented-Representation-Generalization-for-Few-shot-Action-Recognition"><a href="#Hierarchical-Relation-augmented-Representation-Generalization-for-Few-shot-Action-Recognition" class="headerlink" title="Hierarchical Relation-augmented Representation Generalization for   Few-shot Action Recognition"></a>Hierarchical Relation-augmented Representation Generalization for   Few-shot Action Recognition</h2><p><strong>Authors:Hongyu Qu, Ling Xing, Rui Yan, Yazhou Yao, Guo-Sen Xie, Xiangbo Shu</strong></p>
<p>Few-shot action recognition (FSAR) aims to recognize novel action categories with few exemplars. Existing methods typically learn frame-level representations independently for each video by designing various inter-frame temporal modeling strategies. However, they neglect explicit relation modeling between videos and tasks, thus failing to capture shared temporal patterns across videos and reuse temporal knowledge from historical tasks. In light of this, we propose HR2G-shot, a Hierarchical Relation-augmented Representation Generalization framework for FSAR, which unifies three types of relation modeling (inter-frame, inter-video, and inter-task) to learn task-specific temporal patterns from a holistic view. In addition to conducting inter-frame temporal interactions, we further devise two components to respectively explore inter-video and inter-task relationships: i) Inter-video Semantic Correlation (ISC) performs cross-video frame-level interactions in a fine-grained manner, thereby capturing task-specific query features and learning intra- and inter-class temporal correlations among support features; ii) Inter-task Knowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge from the bank, which stores diverse temporal patterns from historical tasks. Extensive experiments on five benchmarks show that HR2G-shot outperforms current top-leading FSAR methods. </p>
<blockquote>
<p>å°æ ·åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰æ—¨åœ¨é€šè¿‡å°‘é‡çš„æ ·æœ¬è¯†åˆ«æ–°çš„åŠ¨ä½œç±»åˆ«ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡è®¾è®¡å„ç§å¸§é—´æ—¶åºå»ºæ¨¡ç­–ç•¥ï¼Œä¸ºæ¯ä¸ªè§†é¢‘ç‹¬ç«‹åœ°å­¦ä¹ å¸§çº§è¡¨ç¤ºã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†è§†é¢‘ä¸ä»»åŠ¡ä¹‹é—´çš„æ˜¾å¼å…³ç³»å»ºæ¨¡ï¼Œå› æ­¤æ— æ³•æ•è·è·¨è§†é¢‘çš„å…±äº«æ—¶åºæ¨¡å¼ï¼Œä¹Ÿæ— æ³•é‡ç”¨æ¥è‡ªå†å²ä»»åŠ¡çš„æ—¶é—´çŸ¥è¯†ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†HR2G-shotï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºFSARçš„åˆ†å±‚å…³ç³»å¢å¼ºè¡¨ç¤ºæ³›åŒ–æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€äº†ä¸‰ç§å…³ç³»å»ºæ¨¡ï¼ˆå¸§é—´ã€è§†é¢‘é—´å’Œä»»åŠ¡é—´ï¼‰ï¼Œä»¥ä»æ•´ä½“è§†è§’å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„æ—¶åºæ¨¡å¼ã€‚é™¤äº†è¿›è¡Œå¸§é—´æ—¶åºäº¤äº’å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è®¾è®¡äº†ä¸¤ä¸ªç»„ä»¶æ¥åˆ†åˆ«æ¢ç´¢è§†é¢‘é—´å’Œä»»åŠ¡é—´çš„å…³ç³»ï¼šä¸€ã€è§†é¢‘é—´è¯­ä¹‰ç›¸å…³æ€§ï¼ˆISCï¼‰ä»¥ç²¾ç»†çš„æ–¹å¼æ‰§è¡Œè·¨è§†é¢‘å¸§çº§äº¤äº’ï¼Œä»è€Œæ•è·ç‰¹å®šä»»åŠ¡çš„æŸ¥è¯¢ç‰¹å¾ï¼Œå¹¶å­¦ä¹ æ”¯æŒç‰¹å¾å†…éƒ¨å’Œè·¨ç±»çš„æ—¶åºç›¸å…³æ€§ï¼›äºŒã€ä»»åŠ¡é—´çŸ¥è¯†è½¬ç§»ï¼ˆIKTï¼‰ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å¹¶èšåˆç›¸å…³çš„æ—¶åºçŸ¥è¯†ï¼Œè¯¥çŸ¥è¯†åº“å­˜å‚¨äº†æ¥è‡ªå†å²ä»»åŠ¡çš„å„ç§æ—¶åºæ¨¡å¼ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHR2G-shotçš„æ€§èƒ½ä¼˜äºå½“å‰é¢†å…ˆçš„FSARæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10079v1">PDF</a> </p>
<p><strong>Summary</strong><br>å°‘é‡æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰æ—¨åœ¨é€šè¿‡å°‘é‡æ ·æœ¬è¯†åˆ«æ–°å‹åŠ¨ä½œç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡è®¾è®¡å„ç§å¸§é—´æ—¶åºå»ºæ¨¡ç­–ç•¥ç‹¬ç«‹å­¦ä¹ æ¯ä¸ªè§†é¢‘å¸§çº§åˆ«çš„è¡¨ç¤ºï¼Œä½†å¿½ç•¥äº†è§†é¢‘ä¸ä»»åŠ¡ä¹‹é—´çš„å…³ç³»å»ºæ¨¡ï¼Œå› æ­¤æ— æ³•æ•è·è·¨è§†é¢‘çš„å…±äº«æ—¶åºæ¨¡å¼å¹¶æ— æ³•åˆ©ç”¨å†å²ä»»åŠ¡ä¸­çš„æ—¶åºçŸ¥è¯†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºHR2G-shotï¼Œä¸€ä¸ªç”¨äºFSARçš„å±‚æ¬¡å…³ç³»å¢å¼ºè¡¨ç¤ºæ³›åŒ–æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€äº†ä¸‰ç§å…³ç³»å»ºæ¨¡ï¼ˆå¸§é—´ã€è§†é¢‘é—´å’Œä»»åŠ¡é—´ï¼‰ï¼Œä»æ•´ä½“ä¸Šå­¦ä¹ ç‰¹å®šä»»åŠ¡çš„æ—¶åºæ¨¡å¼ã€‚é™¤äº†è¿›è¡Œå¸§é—´æ—¶åºäº¤äº’å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†åˆ«è®¾è®¡äº†ä¸¤ä¸ªç»„ä»¶æ¥æ¢ç´¢è§†é¢‘é—´å’Œä»»åŠ¡é—´çš„å…³ç³»ï¼šä¸€æ˜¯è·¨è§†é¢‘è¯­ä¹‰ç›¸å…³æ€§ï¼ˆISCï¼‰ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼æ‰§è¡Œè·¨è§†é¢‘å¸§çº§åˆ«çš„äº¤äº’ï¼Œä»è€Œæ•è·ç‰¹å®šä»»åŠ¡çš„æŸ¥è¯¢ç‰¹å¾ï¼Œå¹¶å­¦ä¹ æ”¯æŒç‰¹å¾ä¸­çš„ç±»å†…å’Œç±»é—´æ—¶åºç›¸å…³æ€§ï¼›äºŒæ˜¯ä»»åŠ¡é—´çŸ¥è¯†è½¬ç§»ï¼ˆIKTï¼‰ï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å’Œèšåˆç›¸å…³çš„æ—¶åºçŸ¥è¯†ï¼Œè¯¥çŸ¥è¯†åº“å­˜å‚¨äº†æ¥è‡ªå†å²ä»»åŠ¡çš„å„ç§æ—¶åºæ¨¡å¼ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHR2G-shotä¼˜äºå½“å‰é¢†å…ˆçš„FSARæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot action recognition (FSAR) æ—¨åœ¨é€šè¿‡å°‘é‡æ ·æœ¬è¯†åˆ«æ–°å‹åŠ¨ä½œç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¸§çº§åˆ«çš„ç‹¬ç«‹è¡¨ç¤ºå­¦ä¹ ï¼Œå¿½ç•¥äº†è§†é¢‘ä¸ä»»åŠ¡ä¹‹é—´çš„å…³ç³»å»ºæ¨¡ã€‚</li>
<li>HR2G-shotæ¡†æ¶é›†æˆäº†ä¸‰ç§å…³ç³»å»ºæ¨¡ï¼ˆå¸§é—´ã€è§†é¢‘é—´å’Œä»»åŠ¡é—´ï¼‰ï¼Œä»¥å…¨é¢å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„æ—¶åºæ¨¡å¼ã€‚</li>
<li>HR2G-shoté€šè¿‡è·¨è§†é¢‘è¯­ä¹‰ç›¸å…³æ€§ï¼ˆISCï¼‰å’Œä»»åŠ¡é—´çŸ¥è¯†è½¬ç§»ï¼ˆIKTï¼‰ä¸¤ä¸ªç»„ä»¶æ¥æ¢ç´¢è§†é¢‘é—´å’Œä»»åŠ¡é—´çš„å…³ç³»ã€‚</li>
<li>ISCæ‰§è¡Œç²¾ç»†çš„è·¨è§†é¢‘å¸§çº§åˆ«äº¤äº’ï¼Œæ•è·ç‰¹å®šä»»åŠ¡çš„æŸ¥è¯¢ç‰¹å¾å¹¶å­¦ä¹ ç±»å†…å’Œç±»é—´çš„æ—¶åºç›¸å…³æ€§ã€‚</li>
<li>IKTä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å’Œèšåˆç›¸å…³æ—¶åºçŸ¥è¯†ï¼Œè¯¥çŸ¥è¯†åº“åŒ…å«å†å²ä»»åŠ¡ä¸­çš„å¤šæ ·æ—¶åºæ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3e4ea7f2f63eac80b6adbf9c78002d5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80e799dd41f2de9b76f8caf3fecb8936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716acf6998ae84c18644fb207f51056d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-225a832f5b74bd506064afe35e377cba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reasoning-Court-Combining-Reasoning-Action-and-Judgment-for-Multi-Hop-Reasoning"><a href="#Reasoning-Court-Combining-Reasoning-Action-and-Judgment-for-Multi-Hop-Reasoning" class="headerlink" title="Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop   Reasoning"></a>Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop   Reasoning</h2><p><strong>Authors:Jingtian Wu, Claire Cardie</strong></p>
<p>While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”å’Œäº‹å®æ ¸æŸ¥ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨ç€å¹»æƒ³å’Œæ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ•´åˆå¤šä¸ªä¿¡æ¯æºçš„å¤šè·³ä»»åŠ¡ä¸­ã€‚å½“å‰çš„æ–¹æ³•é€šè¿‡åŸºäºæ£€ç´¢çš„æŠ€æœ¯ï¼ˆä»¥å¤–éƒ¨è¯æ®ä¸ºæ¨ç†åŸºç¡€ï¼‰ã€åŸºäºæ¨ç†çš„æ–¹æ³•ï¼ˆé€šè¿‡æ”¹è¿›æç¤ºæ¥æé«˜è¿è´¯æ€§ï¼‰æˆ–ç»“åˆä¸¤è€…çš„æ··åˆç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸€ç§çªå‡ºçš„æ··åˆæ–¹æ³•ReActå·²ç»è¶…è¶Šäº†çº¯ç²¹çš„æ£€ç´¢æˆ–åŸºäºæ¨ç†çš„æ–¹æ³•ï¼›ç„¶è€Œï¼Œå®ƒç¼ºä¹å¯¹ä¸­é—´æ¨ç†æ­¥éª¤çš„å†…éƒ¨éªŒè¯ï¼Œä½¿å¾—æ½œåœ¨é”™è¯¯å¯èƒ½é€šè¿‡å¤æ‚çš„æ¨ç†ä»»åŠ¡è€Œä¼ æ’­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Reasoning Courtï¼ˆRCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒå°†è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ–¹æ³•ï¼ˆå¦‚ReActï¼‰ä¸ä¸“ç”¨çš„LLMæ³•å®˜ç›¸ç»“åˆã€‚ä¸åŒäºReActï¼ŒRCä½¿ç”¨æ³•å®˜ç‹¬ç«‹è¯„ä¼°å¤šä¸ªå€™é€‰ç­”æ¡ˆåŠå…¶ç›¸å…³çš„æ¨ç†ï¼Œè¿™äº›ç­”æ¡ˆå’Œæ¨ç†ç”±å•ç‹¬çš„LLMä»£ç†ç”Ÿæˆã€‚æ³•å®˜è¢«è¦æ±‚é€‰æ‹©å®ƒè®¤ä¸ºæœ€åŸºäºäº‹å®ã€é€»è¾‘ä¸Šè¿è´¯çš„ç­”æ¡ˆï¼Œè¯¥ç­”æ¡ˆåŸºäºæä¾›çš„æ¨ç†å’Œè¯æ®ï¼Œæˆ–è€…å½“æ‰€æœ‰å€™é€‰ç­”æ¡ˆéƒ½ä¸è¶³ã€æœ‰ç¼ºé™·æˆ–æ— æ•ˆæ—¶ï¼Œåˆ©ç”¨å¯ç”¨è¯æ®å’Œå…¶é¢„è®­ç»ƒçŸ¥è¯†åˆæˆæ–°çš„ç­”æ¡ˆã€‚åœ¨Multi-hopåŸºå‡†æµ‹è¯•ï¼ˆHotpotQAã€MuSiQueï¼‰å’Œäº‹å®æ ¸æŸ¥ï¼ˆFEVERï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRCå§‹ç»ˆä¼˜äºæœ€æ–°çš„å°‘æç¤ºæç¤ºæ–¹æ³•ï¼Œä¸”æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09781v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºReasoning Courtï¼ˆRCï¼‰çš„æ–°æ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ–¹æ³•ï¼ˆå¦‚ReActï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹çš„LLMæ³•å®˜æ¥è¯„ä¼°å¤šä¸ªå€™é€‰ç­”æ¡ˆåŠå…¶ç›¸å…³æ¨ç†ã€‚RCçš„æ³•å®˜èƒ½å¤ŸåŸºäºæä¾›çš„æ¨ç†å’Œè¯æ®ï¼Œé€‰æ‹©å®ƒè®¤ä¸ºæœ€ç¬¦åˆäº‹å®å’Œé€»è¾‘è¿è´¯æ€§çš„ç­”æ¡ˆï¼Œæˆ–è€…åœ¨æ‰€æœ‰å€™é€‰ç­”æ¡ˆéƒ½ä¸è¶³ã€æœ‰ç¼ºé™·æˆ–æ— æ•ˆæ—¶ï¼Œåˆ©ç”¨å¯ç”¨è¯æ®å’Œé¢„è®­ç»ƒçŸ¥è¯†åˆæˆæ–°çš„ç­”æ¡ˆã€‚åœ¨å¤šè·³åŸºå‡†æµ‹è¯•ï¼ˆHotpotQAã€MuSiQueï¼‰å’Œäº‹å®éªŒè¯ï¼ˆFEVERï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRCåœ¨æ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºæœ€æ–°çš„å°‘æ ·æœ¬æç¤ºæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®ç­”å’Œäº‹å®éªŒè¯ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æ•´åˆå¤šä¸ªä¿¡æ¯æºçš„å¤šè·³ä»»åŠ¡ä¸­ä»å­˜åœ¨å¹»è§‰å’Œæ¨ç†é”™è¯¯ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šè¿‡æ£€ç´¢ã€æ¨ç†æˆ–æ··åˆç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†å­˜åœ¨è¯¯å·®ä¼ æ’­é£é™©ã€‚</li>
<li>ReActç­‰è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ–¹æ³•å¾—åˆ°äº†æ‰©å±•ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶Reasoning Courtï¼ˆRCï¼‰ã€‚</li>
<li>RCçš„å…³é”®ç‰¹æ€§æ˜¯å¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹çš„LLMæ³•å®˜ï¼Œç”¨äºè¯„ä¼°å¤šä¸ªå€™é€‰ç­”æ¡ˆåŠå…¶ç›¸å…³æ¨ç†ï¼Œé€‰æ‹©æœ€ç¬¦åˆäº‹å®å’Œé€»è¾‘è¿è´¯æ€§çš„ç­”æ¡ˆã€‚</li>
<li>å¦‚æœæ‰€æœ‰å€™é€‰ç­”æ¡ˆéƒ½ä¸è¶³ã€æœ‰ç¼ºé™·æˆ–æ— æ•ˆï¼ŒRCçš„æ³•å®˜èƒ½å¤Ÿåˆ©ç”¨å¯ç”¨è¯æ®å’Œé¢„è®­ç»ƒçŸ¥è¯†åˆæˆæ–°çš„ç­”æ¡ˆã€‚</li>
<li>åœ¨å¤šè·³åŸºå‡†æµ‹è¯•å’Œäº‹å®éªŒè¯ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRCåœ¨æ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>RCæ¡†æ¶ä¸ºæ”¹è¿›LLMsåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„æ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b795a39a5921ceccac85887f872f59fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db5218c13782dfaeed6dfb41a21b204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cea842cfd0f19308d4daa86e641bb599.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Model-for-Object-Detection-and-Segmentation-A-Review-and-Evaluation"><a href="#Vision-Language-Model-for-Object-Detection-and-Segmentation-A-Review-and-Evaluation" class="headerlink" title="Vision-Language Model for Object Detection and Segmentation: A Review   and Evaluation"></a>Vision-Language Model for Object Detection and Segmentation: A Review   and Evaluation</h2><p><strong>Authors:Yongchao Feng, Yajie Liu, Shuai Yang, Wenrui Cai, Jinqing Zhang, Qiqi Zhan, Ziyue Huang, Hongxi Yan, Qiao Wan, Chenguang Liu, Junzhe Wang, Jiahui Lv, Ziqi Liu, Tengyuan Shi, Qingjie Liu, Yunhong Wang</strong></p>
<p>Vision-Language Model (VLM) have gained widespread adoption in Open-Vocabulary (OV) object detection and segmentation tasks. Despite they have shown promise on OV-related tasks, their effectiveness in conventional vision tasks has thus far been unevaluated. In this work, we present the systematic review of VLM-based detection and segmentation, view VLM as the foundational model and conduct comprehensive evaluations across multiple downstream tasks for the first time: 1) The evaluation spans eight detection scenarios (closed-set detection, domain adaptation, crowded objects, etc.) and eight segmentation scenarios (few-shot, open-world, small object, etc.), revealing distinct performance advantages and limitations of various VLM architectures across tasks. 2) As for detection tasks, we evaluate VLMs under three finetuning granularities: \textit{zero prediction}, \textit{visual fine-tuning}, and \textit{text prompt}, and further analyze how different finetuning strategies impact performance under varied task. 3) Based on empirical findings, we provide in-depth analysis of the correlations between task characteristics, model architectures, and training methodologies, offering insights for future VLM design. 4) We believe that this work shall be valuable to the pattern recognition experts working in the fields of computer vision, multimodal learning, and vision foundation models by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research. A project associated with this review and evaluation has been created at <a target="_blank" rel="noopener" href="https://github.com/better-chao/perceptual_abilities_evaluation">https://github.com/better-chao/perceptual_abilities_evaluation</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚å°½ç®¡å®ƒä»¬åœ¨OVç›¸å…³ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨ä¼ ç»Ÿè§†è§‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§è¿„ä»Šä¸ºæ­¢å°šæœªå¾—åˆ°è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºVLMçš„æ£€æµ‹å’Œåˆ†å‰²è¿›è¡Œäº†ç³»ç»Ÿç»¼è¿°ï¼Œå°†VLMè§†ä¸ºåŸºç¡€æ¨¡å‹ï¼Œé¦–æ¬¡å¯¹å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼š1ï¼‰è¯„ä¼°æ¶µç›–å…«ç§æ£€æµ‹åœºæ™¯ï¼ˆå°é—­é›†æ£€æµ‹ã€åŸŸé€‚åº”ã€æ‹¥æŒ¤ç›®æ ‡ç­‰ï¼‰å’Œå…«ç§åˆ†å‰²åœºæ™¯ï¼ˆå°æ ·æœ¬ã€å¼€æ”¾ä¸–ç•Œã€å°ç›®æ ‡ç­‰ï¼‰ï¼Œæ­ç¤ºäº†å„ç§VLMæ¶æ„åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„ä¸åŒæ€§èƒ½ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚2ï¼‰å¯¹äºæ£€æµ‹ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§å¾®è°ƒç²’åº¦ä¸‹è¯„ä¼°VLMï¼š\emph{é›¶é¢„æµ‹}ã€\emph{è§†è§‰å¾®è°ƒ}å’Œ\emph{æ–‡æœ¬æç¤º}ï¼Œå¹¶è¿›ä¸€æ­¥åˆ†æä¸åŒå¾®è°ƒç­–ç•¥åœ¨ä¸åŒä»»åŠ¡ä¸‹å¯¹æ€§èƒ½çš„å½±å“ã€‚3ï¼‰åŸºäºå®è¯å‘ç°ï¼Œæˆ‘ä»¬å¯¹ä»»åŠ¡ç‰¹æ€§ã€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ä¹‹é—´çš„å…³è”æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¸ºæœªæ¥çš„VLMè®¾è®¡æä¾›äº†è§è§£ã€‚4ï¼‰æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™é¡¹å·¥ä½œå¯¹äºè®¡ç®—æœºè§†è§‰ã€å¤šæ¨¡æ€å­¦ä¹ å’Œè§†è§‰åŸºç¡€æ¨¡å‹é¢†åŸŸçš„æ¨¡å¼è¯†åˆ«ä¸“å®¶æ¥è¯´å°†å…·æœ‰å¾ˆé«˜çš„ä»·å€¼ï¼Œé€šè¿‡å¼•å…¥é—®é¢˜ï¼Œè®©ä»–ä»¬ç†Ÿæ‚‰å½“å‰çš„ç ”ç©¶è¿›å±•ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä¸æ­¤æ¬¡å®¡æŸ¥å’Œè¯„ä¼°ç›¸å…³çš„é¡¹ç›®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/better-chao/perceptual_abilities_evaluation%E5%88%9B%E5%BB%BA%E3%80%82">https://github.com/better-chao/perceptual_abilities_evaluationåˆ›å»ºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09480v1">PDF</a> A Review and Evaluation about Vision-Language Model for Object   Detection and Segmentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç³»ç»Ÿè¯„è¿°äº†åŸºäºVision-Language Modelï¼ˆVLMï¼‰çš„æ£€æµ‹å’Œåˆ†å‰²æŠ€æœ¯ã€‚ç ”ç©¶å¯¹VLMåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬å°é—­é›†æ£€æµ‹ã€åŸŸé€‚åº”ã€æ‹¥æŒ¤å¯¹è±¡ç­‰å…«ä¸ªæ£€æµ‹åœºæ™¯ï¼Œä»¥åŠå°‘æ ·æœ¬ã€å¼€æ”¾ä¸–ç•Œã€å°ç‰©ä½“ç­‰å…«ä¸ªåˆ†å‰²åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†ä¸åŒå¾®è°ƒç­–ç•¥å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶ä¸ºæœªæ¥VLMè®¾è®¡æä¾›äº†æ·±å…¥çš„åˆ†æå’Œè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMåœ¨Open-Vocabularyï¼ˆOVï¼‰ç‰©ä½“æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨ä¼ ç»Ÿè§†è§‰ä»»åŠ¡ä¸­çš„æ•ˆæœå°šæœªè¯„ä¼°ã€‚</li>
<li>é¦–æ¬¡å¯¹VLMåŸºç¡€çš„æ£€æµ‹å’Œåˆ†å‰²è¿›è¡Œå…¨é¢è¯„ä»·ï¼Œæ¶µç›–å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>åœ¨æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œè¯„ä¼°äº†VLMåœ¨é›¶é¢„æµ‹ã€è§†è§‰å¾®è°ƒã€æ–‡æœ¬æç¤ºä¸‰ç§å¾®è°ƒç²’åº¦ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>å‘ç°ä¸åŒå¾®è°ƒç­–ç•¥å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æä¾›ä»»åŠ¡ç‰¹æ€§ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ–¹æ³•çš„æ·±åº¦åˆ†æã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4ec0e9bd0dbc84e7142409fd8702c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0109435619729faa8321591015dcaac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02e7695515944037da0150bee979d55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f8d005d7606375a5cc4cd2ac519d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddf6dd92bd6c2ff15b453c28a5d54cac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6bbd25db966e64e5743b1fb0c2b1ea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2196185bdf1208a46dfd7e06aa15500.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning"><a href="#Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning" class="headerlink" title="Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning"></a>Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning</h2><p><strong>Authors:Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</strong></p>
<p>Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods. </p>
<blockquote>
<p>å°†æ–°ç±»åˆ«ä¿¡æ¯é›†æˆåˆ°æ¨¡å‹ä¸­è€Œä¸ä¸¢å¤±å…ˆå‰è·å–çš„çŸ¥è¯†ä»ç„¶æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œé€šå¸¸è¢«ç§°ä¸ºç¾éš¾æ€§é—å¿˜ã€‚å°‘æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é€šè¿‡é¦–å…ˆä½¿ç”¨åŸºç¡€ç±»åˆ«çš„ç¨³å¥æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œç„¶åé€æ­¥ä»…ä½¿ç”¨æ¯ä¸ªæ–°ç±»åˆ«çš„å°‘é‡æ ‡è®°æ ·æœ¬å¯¹å…¶è¿›è¡Œé€‚åº”æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯¹æ–°æ•°æ®çš„è¿‡åº¦æ‹Ÿåˆå€¾å‘è¾ƒé«˜ï¼Œå¯èƒ½ä¼šæŸå®³æ•´ä½“æ€§èƒ½å¹¶åŠ å‰§é—å¿˜é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„FSCILæ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å†»ç»“çš„Vision Transformerï¼ˆViTï¼‰ä¸»å¹²å¹¶è¾…ä»¥é«˜æ•ˆçš„å‚æ•°æ·»åŠ æ›´æ–°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å†»ç»“äº†é¢„è®­ç»ƒçš„ViTå‚æ•°ï¼Œå¹¶é€šè¿‡æ·»åŠ æ›´æ–°æœºåˆ¶æœ‰é€‰æ‹©åœ°å°†å¯è®­ç»ƒæƒé‡æ³¨å…¥è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚è¿™ç§è®¾è®¡åªæ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ä»¥é€‚åº”æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸ç‰ºç‰²åŸºç¡€ä¼šè¯æœŸé—´å­¦ä¹ çš„è¡¨ç¤ºã€‚é€šè¿‡å¾®è°ƒæœ‰é™æ•°é‡çš„å‚æ•°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿ç•™å†»ç»“ViTçš„é€šç”¨ç‰¹å¾çš„åŒæ—¶é™ä½äº†è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼Œç”±äºå¤§å¤šæ•°å‚æ•°ä¿æŒä¸å˜ï¼Œå½“å¼•å…¥å°‘é‡æ–°æ•°æ®æ‰¹æ¬¡æ—¶ï¼Œæ¨¡å‹é¿å…äº†è¦†ç›–å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸åŸºçº¿FSCILæ–¹æ³•ç›¸æ¯”è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08982v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å†»ç»“çš„Vision Transformerï¼ˆViTï¼‰ä¸»å¹²ç½‘ï¼Œå¹¶é€šè¿‡å‚æ•°æœ‰æ•ˆçš„æ·»åŠ æ›´æ–°è¿›è¡Œå¢å¼ºã€‚è¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒçš„ViTå‚æ•°å†»ç»“çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡æ·»åŠ æ›´æ–°æœºåˆ¶æœ‰é€‰æ‹©åœ°å°†å¯è®­ç»ƒæƒé‡æ³¨å…¥è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚è¯¥è®¾è®¡ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ä»¥é€‚åº”æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€ä¼šè¯æœŸé—´å­¦ä¹ çš„è¡¨ç¤ºã€‚é€šè¿‡å¾®è°ƒæœ‰é™æ•°é‡çš„å‚æ•°ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†å†»ç»“ViTçš„é€šç”¨ç‰¹å¾ï¼Œå¹¶é™ä½äº†è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼Œç”±äºå¤§éƒ¨åˆ†å‚æ•°ä¿æŒä¸å˜ï¼Œå½“å¼•å…¥å°‘é‡æ–°æ•°æ®æ‰¹æ¬¡æ—¶ï¼Œæ¨¡å‹é¿å…äº†è¦†ç›–å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸åŸºçº¿FSCILæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ•´åˆæ–°ç±»åˆ«ä¿¡æ¯è€Œä¸ä¼šä¸¢å¤±å…ˆå‰è·å¾—çš„çŸ¥è¯†ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„FSCILæ¡†æ¶ï¼Œåˆ©ç”¨å†»ç»“çš„Vision Transformerï¼ˆViTï¼‰ä¸»å¹²ç½‘ï¼Œé€šè¿‡å‚æ•°æœ‰æ•ˆçš„æ·»åŠ æ›´æ–°è¿›è¡Œå¢å¼ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„ViTå‚æ•°å¹¶ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ä»¥é€‚åº”æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€ä¼šè¯æœŸé—´å­¦ä¹ çš„è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å¾®è°ƒæœ‰é™æ•°é‡çš„å‚æ•°ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™é€šç”¨ç‰¹å¾çš„åŒæ—¶é™ä½äº†è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚</li>
<li>å¤§éƒ¨åˆ†å‚æ•°ä¿æŒä¸å˜ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¼•å…¥å°‘é‡æ–°æ•°æ®æ‰¹æ¬¡æ—¶é¿å…è¦†ç›–å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚</li>
<li>åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–FSCILæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b28bfe5d16a2e4d78b04a9bde0aae140.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5d833abb92541ba15d49cfb0300e89d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3a54371e37affb4221eca75caed2ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851985bb320feb6264afab7c7cdd12e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b87b356799882e2673404bdb4fcf26cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0a31ee5fe4c921c67cfb818185fb9e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-Few-Shot-Fusion-Granular-Ball-Priors-Enable-General-Purpose-Deep-Image-Fusion"><a href="#Rethinking-Few-Shot-Fusion-Granular-Ball-Priors-Enable-General-Purpose-Deep-Image-Fusion" class="headerlink" title="Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose   Deep Image Fusion"></a>Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose   Deep Image Fusion</h2><p><strong>Authors:Minjie Deng, Yan Wei, Hao Zhai, An Wu, Yuncan Ouyang, Qianyao Peng</strong></p>
<p>In image fusion tasks, due to the lack of real fused images as priors, most deep learning-based fusion methods obtain global weight features from original images in large-scale data pairs to generate images that approximate real fused images. However, unlike previous studies, this paper utilizes Granular Ball adaptation to extract features in the brightness space as priors for deep networks, enabling the fusion network to converge quickly and complete the fusion task. This leads to few-shot training for a general image fusion network, and based on this, we propose the GBFF fusion method. According to the information expression division of pixel pairs in the original fused image, we classify pixel pairs with significant performance as the positive domain and non-significant pixel pairs as the boundary domain. We perform split inference in the brightness space using Granular Ball adaptation to compute weights for pixels that express information to varying degrees, generating approximate supervision images that provide priors for the neural network in the structural brightness space. Additionally, the extracted global saliency features also adaptively provide priors for setting the loss function weights of each image in the network, guiding the network to converge quickly at both global and pixel levels alongside the supervised images, thereby enhancing the expressiveness of the fused images. Each modality only used 10 pairs of images as the training set, completing the fusion task with a limited number of iterations. Experiments validate the effectiveness of the algorithm and theory, and qualitative and quantitative comparisons with SOTA methods show that this approach is highly competitive in terms of fusion time and image expressiveness. </p>
<blockquote>
<p>åœ¨å›¾åƒèåˆä»»åŠ¡ä¸­ï¼Œç”±äºç¼ºä¹çœŸå®çš„èåˆå›¾åƒä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œå¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•éƒ½æ˜¯ä»å¤§è§„æ¨¡æ•°æ®å¯¹çš„åŸå§‹å›¾åƒä¸­æå–å…¨å±€æƒé‡ç‰¹å¾ï¼Œä»¥ç”Ÿæˆè¿‘ä¼¼çœŸå®çš„èåˆå›¾åƒã€‚ç„¶è€Œï¼Œä¸åŒäºä»¥å¾€çš„ç ”ç©¶ï¼Œæœ¬æ–‡åˆ©ç”¨ç²’çƒé€‚åº”æ³•æå–äº®åº¦ç©ºé—´ä¸­çš„ç‰¹å¾ä½œä¸ºæ·±åº¦ç½‘ç»œçš„å…ˆéªŒçŸ¥è¯†ï¼Œä½¿èåˆç½‘ç»œèƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›å¹¶å®Œæˆèåˆä»»åŠ¡ã€‚è¿™å¯¼è‡´äº†å¯¹é€šç”¨å›¾åƒèåˆç½‘ç»œè¿›è¡Œå°æ ·æœ¬è®­ç»ƒï¼Œå¹¶åŸºäºæ­¤æˆ‘ä»¬æå‡ºäº†GBFFèåˆæ–¹æ³•ã€‚æ ¹æ®åŸå§‹èåˆå›¾åƒä¸­åƒç´ å¯¹çš„ä¿¡æ¯è¡¨è¾¾åˆ’åˆ†ï¼Œæˆ‘ä»¬å°†è¡¨ç°æ˜¾è‘—çš„åƒç´ å¯¹åˆ†ç±»ä¸ºæ­£å‘åŸŸï¼Œå°†éæ˜¾è‘—çš„åƒç´ å¯¹åˆ†ç±»ä¸ºè¾¹ç•ŒåŸŸã€‚æˆ‘ä»¬åœ¨äº®åº¦ç©ºé—´ä¸­ä½¿ç”¨ç²’çƒé€‚åº”æ³•è¿›è¡Œåˆ†å‰²æ¨ç†ï¼Œè®¡ç®—ä¸åŒç¨‹åº¦è¡¨è¾¾ä¿¡æ¯çš„åƒç´ æƒé‡ï¼Œç”Ÿæˆè¿‘ä¼¼ç›‘ç£å›¾åƒï¼Œä¸ºç¥ç»ç½‘ç»œåœ¨ç»“æ„äº®åº¦ç©ºé—´ä¸­æä¾›å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæå–çš„å…¨å±€æ˜¾è‘—æ€§ç‰¹å¾ä¹Ÿè‡ªé€‚åº”åœ°ä¸ºè®¾ç½®ç½‘ç»œä¸­æ¯å¼ å›¾åƒçš„æŸå¤±å‡½æ•°æƒé‡æä¾›å…ˆéªŒçŸ¥è¯†ï¼Œå¼•å¯¼ç½‘ç»œåœ¨å…¨å±€å’Œåƒç´ çº§åˆ«ä¸Šå¿«é€Ÿæ”¶æ•›ï¼ŒåŒæ—¶ç»“åˆç›‘ç£å›¾åƒï¼Œä»è€Œæé«˜èåˆå›¾åƒçš„è¡¨è¾¾æ€§ã€‚æ¯ç§æ¨¡æ€ä»…ä½¿ç”¨10å¯¹å›¾åƒä½œä¸ºè®­ç»ƒé›†ï¼Œåœ¨æœ‰é™çš„è¿­ä»£æ¬¡æ•°ä¸­å®Œæˆèåˆä»»åŠ¡ã€‚å®éªŒéªŒè¯äº†ç®—æ³•å’Œç†è®ºçš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€å…ˆè¿›æ–¹æ³•çš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èåˆæ—¶é—´å’Œå›¾åƒè¡¨è¾¾æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºGranular Ballé€‚åº”æ€§çš„å›¾åƒèåˆæ–¹æ³•ï¼Œé€šè¿‡æå–äº®åº¦ç©ºé—´ç‰¹å¾ä½œä¸ºæ·±åº¦ç½‘ç»œçš„å…ˆéªŒä¿¡æ¯ï¼Œå®ç°å¿«é€Ÿèåˆä»»åŠ¡ã€‚åˆ©ç”¨åƒç´ å¯¹çš„ä¿¡æ¯è¡¨è¾¾åˆ†ç±»ï¼Œå°†å…·æœ‰æ˜¾è‘—æ€§èƒ½çš„åƒç´ å¯¹å½’ä¸ºæ­£åŸŸï¼Œéæ˜¾è‘—åƒç´ å¯¹å½’ä¸ºè¾¹ç•ŒåŸŸã€‚é€šè¿‡åˆ†å‰²æ¨ç†è®¡ç®—ä¸åŒè¡¨è¾¾ç¨‹åº¦çš„åƒç´ æƒé‡ï¼Œç”Ÿæˆè¿‘ä¼¼ç›‘ç£å›¾åƒï¼Œä¸ºç½‘ç»œæä¾›ç»“æ„äº®åº¦ç©ºé—´çš„å…ˆéªŒä¿¡æ¯ã€‚åŒæ—¶ï¼Œæå–çš„å…¨å±€æ˜¾è‘—æ€§ç‰¹å¾è‡ªé€‚åº”åœ°ä¸ºè®¾ç½®ç½‘ç»œä¸­çš„æ¯ä¸ªå›¾åƒæŸå¤±å‡½æ•°æƒé‡æä¾›å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œåœ¨å…¨å±€å’Œåƒç´ çº§åˆ«å¿«é€Ÿå¼•å¯¼ç½‘ç»œæ”¶æ•›ã€‚ä»…ä½¿ç”¨å°‘é‡å›¾åƒå¯¹å³å¯å®Œæˆèåˆä»»åŠ¡ã€‚å®éªŒéªŒè¯äº†ç®—æ³•å’Œç†è®ºçš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€æ–°æ–¹æ³•çš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èåˆæ—¶é—´å’Œå›¾åƒè¡¨ç°åŠ›æ–¹é¢è¡¨ç°å‡ºé«˜åº¦ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨Granular Ballé€‚åº”æ€§æå–äº®åº¦ç©ºé—´ç‰¹å¾ä½œä¸ºæ·±åº¦ç½‘ç»œçš„å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>é€šè¿‡åˆ†ç±»åƒç´ å¯¹å®ç°å¿«é€Ÿèåˆä»»åŠ¡ï¼Œæ˜¾è‘—æ€§èƒ½åƒç´ å¯¹å½’ä¸ºæ­£åŸŸï¼Œéæ˜¾è‘—åƒç´ å¯¹å½’ä¸ºè¾¹ç•ŒåŸŸã€‚</li>
<li>é€šè¿‡åˆ†å‰²æ¨ç†è®¡ç®—åƒç´ æƒé‡ï¼Œç”Ÿæˆè¿‘ä¼¼ç›‘ç£å›¾åƒï¼Œä¸ºç½‘ç»œæä¾›ç»“æ„äº®åº¦ç©ºé—´çš„å…ˆéªŒã€‚</li>
<li>æå–çš„å…¨å±€æ˜¾è‘—æ€§ç‰¹å¾è‡ªé€‚åº”åœ°è®¾ç½®ç½‘ç»œä¸­çš„æŸå¤±å‡½æ•°æƒé‡ã€‚</li>
<li>ä»…éœ€å°‘é‡å›¾åƒå¯¹å³å¯å®Œæˆèåˆä»»åŠ¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>å®éªŒéªŒè¯äº†ç®—æ³•å’Œç†è®ºçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13cbe48cd6bd2aea54cfd465754d57ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1976c864cc8fe5248968b04742b66314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38d01c17d9ddcc45f4ef7f755006c653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73746229a5ac5ef7acc78e197e55a3b7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Derm1M-A-Million-scale-Vision-Language-Dataset-Aligned-with-Clinical-Ontology-Knowledge-for-Dermatology"><a href="#Derm1M-A-Million-scale-Vision-Language-Dataset-Aligned-with-Clinical-Ontology-Knowledge-for-Dermatology" class="headerlink" title="Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical   Ontology Knowledge for Dermatology"></a>Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical   Ontology Knowledge for Dermatology</h2><p><strong>Authors:Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge</strong></p>
<p>The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot&#x2F;full-shot learning, and cross-modal retrieval. Our dataset and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/Derm1M">https://github.com/SiyuanYan1/Derm1M</a> upon acceptance. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¶Œç°å·²ç»æ¨åŠ¨äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œä½¿å¾—è¯Šæ–­èƒ½åŠ›å’Œä¸´åºŠåº”ç”¨æ–¹é¢å–å¾—äº†å‰æ‰€æœªæœ‰çš„è¿›å±•ã€‚ç„¶è€Œï¼Œçš®è‚¤ç—…å­¦æ–¹é¢çš„è¿›å±•ç”±äºç¼ºå°‘æ ‡å‡†å›¾åƒæ–‡æœ¬é…å¯¹è€Œè½åäºå…¶ä»–åŒ»å­¦é¢†åŸŸã€‚ç°æœ‰çš„çš®è‚¤ç—…æ•°æ®é›†åœ¨è§„æ¨¡å’Œæ·±åº¦ä¸Šå‡æœ‰é™ï¼Œåªæä¾›ç‹­çª„ç–¾ç—…èŒƒå›´å†…çš„å•ä¸€æ ‡ç­¾æ³¨é‡Šï¼Œç¼ºä¹ä¸°å¯Œçš„æ–‡æœ¬æè¿°å’Œç°å®ä¸–ç•Œåº”ç”¨æ‰€éœ€çš„å…³é”®ä¸´åºŠèƒŒæ™¯ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Derm1Mï¼Œè¿™æ˜¯çš®è‚¤ç—…å­¦é¢†åŸŸç¬¬ä¸€ä¸ªå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«1,029,761ä¸ªå›¾åƒæ–‡æœ¬å¯¹ã€‚Derm1Mç”±å¤šæ ·åŒ–çš„æ•™è‚²èµ„æºæ„å»ºè€Œæˆï¼Œå›´ç»•ä¸“å®¶å…±åŒå¼€å‘çš„æ ‡å‡†æœ¬ä½“ç»“æ„æ„å»ºï¼Œå¯¹è¶…è¿‡390ç§çš®è‚¤çŠ¶å†µè¿›è¡Œäº†å…¨é¢è¦†ç›–ï¼Œæ¶‰åŠå››ä¸ªå±‚æ¬¡å’Œ130ä¸ªä¸´åºŠæ¦‚å¿µï¼Œå¹¶æä¾›äº†ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚ç—…å²ã€ç—‡çŠ¶å’Œè‚¤è‰²ç­‰ã€‚ä¸ºäº†è¯æ˜Derm1Måœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½ç ”ç©¶å’Œä¸´åºŠåº”ç”¨æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†ä¸€ç³»åˆ—ç±»ä¼¼äºCLIPçš„æ¨¡å‹ï¼Œç»Ÿç§°ä¸ºDermLIPç³»åˆ—ã€‚DermLIPå®¶æ—åœ¨å¤šä¸ªä»»åŠ¡çš„å…«ä¸ªä¸åŒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬çš®è‚¤ç–¾ç—…åˆ†ç±»ã€ä¸´åºŠå’Œæ–‡ç‰©æ¦‚å¿µè¯†åˆ«ã€å°æ ·æœ¬&#x2F;å…¨æ ·æœ¬å­¦ä¹ å’Œè·¨æ¨¡æ€æ£€ç´¢ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/Derm1M%E3%80%82">https://github.com/SiyuanYan1/Derm1Mã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14911v2">PDF</a> Our dataset and code will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/Derm1M">https://github.com/SiyuanYan1/Derm1M</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°å…´çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—AIé¢†åŸŸçš„å˜é©æ€§ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯Šæ–­èƒ½åŠ›å’Œä¸´åºŠåº”ç”¨æ–¹é¢çš„çªç ´ã€‚ç„¶è€Œï¼Œçš®è‚¤ç—…å­¦é¢†åŸŸçš„è¿›å±•å› ç¼ºä¹æ ‡å‡†å›¾åƒæ–‡æœ¬é…å¯¹è€Œæ»åã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡çš®è‚¤ç—…å­¦è§†è§‰è¯­è¨€æ•°æ®é›†Derm1Mï¼ŒåŒ…å«1,029,761ä¸ªå›¾åƒæ–‡æœ¬å¯¹ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è¶…è¿‡390ç§çš®è‚¤ç–¾ç—…ï¼Œæä¾›äº†ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚ç—…å²ã€ç—‡çŠ¶å’Œè‚¤è‰²ç­‰ã€‚ä¸ºå±•ç¤ºDerm1Måœ¨æ¨è¿›AIç ”ç©¶å’Œä¸´åºŠåº”ç”¨æ–¹é¢çš„æ½œåŠ›ï¼Œæœ¬æ–‡é¢„è®­ç»ƒäº†ä¸€ç³»åˆ—åä¸ºDermLIPçš„CLIPç±»æ¨¡å‹ã€‚DermLIPå®¶æ—åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬çš®è‚¤ç–¾ç—…åˆ†ç±»ã€ä¸´åºŠå’Œä¼ªæ¦‚å¿µè¯†åˆ«ã€å°‘æ ·æœ¬&#x2F;å…¨æ ·æœ¬å­¦ä¹ ä»¥åŠè·¨æ¨¡æ€æ£€ç´¢ã€‚æ•°æ®é›†å’Œç›¸å…³ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡ºç°åœ¨åŒ»ç–—AIé¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯Šæ–­å’Œä¸´åºŠåº”ç”¨æ–¹é¢ã€‚</li>
<li>çš®è‚¤ç—…å­¦é¢†åŸŸåœ¨AIåº”ç”¨æ–¹é¢çš„è¿›å±•ç›¸å¯¹æ»åï¼Œä¸»è¦ç”±äºç¼ºä¹æ ‡å‡†çš„å›¾åƒæ–‡æœ¬é…å¯¹æ•°æ®é›†ã€‚</li>
<li>Derm1Mæ˜¯é¦–ä¸ªé’ˆå¯¹çš®è‚¤ç—…å­¦çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œè¦†ç›–äº†å¤šç§çš®è‚¤ç–¾ç—…å’Œä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>DermLIPç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€ä¸´åºŠå’Œä¼ªæ¦‚å¿µè¯†åˆ«ç­‰ã€‚</li>
<li>Derm1Mæ•°æ®é›†å¯¹AIç ”ç©¶å’Œä¸´åºŠåº”ç”¨å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>Derm1Mæ•°æ®é›†å’Œç›¸å…³ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€æä¾›ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ec35247ba9d4768824ed15cf6b15628.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae9640105bedf3752cb9a63e9b78577d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebc2724bba1f810efdeddb31028ccad1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85d1674fbc61b98d12db21b0f7252e7f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TurtleBench-A-Visual-Programming-Benchmark-in-Turtle-Geometry"><a href="#TurtleBench-A-Visual-Programming-Benchmark-in-Turtle-Geometry" class="headerlink" title="TurtleBench: A Visual Programming Benchmark in Turtle Geometry"></a>TurtleBench: A Visual Programming Benchmark in Turtle Geometry</h2><p><strong>Authors:Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi</strong></p>
<p>Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce \Turtle, a benchmark designed to evaluate LMMsâ€™ capacity to interpret geometric patterns â€“ given visual examples, textual instructions, or both â€“ and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4o achieving only 19% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance ($&lt;2%$). \Turtle highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area. \Turtle stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research. Code and Dataset for this paper is provided here: \href{<a target="_blank" rel="noopener" href="https://github.com/sinaris76/TurtleBench%7D%7Bhttps://github.com/sinaris76/TurtleBench%7D">https://github.com/sinaris76/TurtleBench}{https://github.com/sinaris76/TurtleBench}</a> </p>
<blockquote>
<p>äººç±»ä»å°å°±å…·å¤‡ç†è§£å’Œåˆ†æå›¾åƒå’Œåœºæ™¯ä¸­çš„å‡ ä½•æ¨¡å¼çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€å‘å…·æœ‰ç±»ä¼¼æ¨ç†èƒ½åŠ›çš„å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™çªæ˜¾äº†éœ€è¦å¯é çš„è¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†TurtleBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LMMè§£é‡Šå‡ ä½•æ¨¡å¼çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç»™å®šè§†è§‰ç¤ºä¾‹ã€æ–‡æœ¬æŒ‡ä»¤æˆ–ä¸¤è€…å…¼æœ‰ï¼Œå¹¶ç”Ÿæˆç²¾ç¡®çš„ä»£ç è¾“å‡ºã€‚TurtleBenchä»¥æµ·é¾Ÿå‡ ä½•å­¦ä¸ºçµæ„Ÿï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ•™æˆå„¿ç«¥åŸºç¡€ç¼–ç å’Œå‡ ä½•æ¦‚å¿µçš„æ¦‚å¿µï¼Œå…¶ç‰¹å¾æ˜¯å…·æœ‰æ½œåœ¨ç®—æ³•é€»è¾‘çš„æ¨¡å¼å½¢çŠ¶ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°ï¼Œé¢†å…ˆçš„LMMåœ¨è¿™äº›ä»»åŠ¡ä¸Šé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼ŒGPT-4åœ¨æœ€ç®€å•ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º19%ï¼Œè€Œä¸”few-shotæç¤ºåªç•¥å¾®æé«˜äº†å…¶æ€§èƒ½ï¼ˆå°äº2%ï¼‰ã€‚Turtleçªå‡ºäº†äººç±»åœ¨ç›´è§‚å’Œè§†è§‰å‡ ä½•ç†è§£æ–¹é¢ä¸äººå·¥æ™ºèƒ½çš„æ€§èƒ½å·®è·ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚Turtleæ˜¯å°‘æ•°å‡ ä¸ªèƒ½å¤Ÿè¯„ä¼°LMMä¸­è§†è§‰ç†è§£ä¸ä»£ç ç”Ÿæˆèƒ½åŠ›èåˆçš„åŸºå‡†æµ‹è¯•ä¹‹ä¸€ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚æœ¬æ–‡çš„ä»£ç å’Œæ•°æ®é›†è¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/sinaris76/TurtleBench%E3%80%82">https://github.com/sinaris76/TurtleBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00264v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTurtleçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹å¯¹å›¾åƒå’Œåœºæ™¯ä¸­çš„å‡ ä½•æ¨¡å¼è¿›è¡Œè§£è¯»çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡è§†è§‰ç¤ºä¾‹ã€æ–‡æœ¬æŒ‡ä»¤æˆ–ä¸¤è€…çš„ç»„åˆæ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆç²¾ç¡®ä»£ç è¾“å‡ºçš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢†å…ˆçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ï¼ŒGPT-4çš„å‡†ç¡®ç‡ä»…ä¸º19%ï¼Œä¸”å°‘æ ·æœ¬æç¤ºä»…ç•¥å¾®æ”¹å–„å…¶è¡¨ç°ã€‚Turtleå‡¸æ˜¾äº†äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨ç›´è§‚å’Œè§†è§‰å‡ ä½•ç†è§£æ–¹é¢çš„å·®è·ï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Turtleæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è§£è¯»å‡ ä½•æ¨¡å¼èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å¹³å°æ¶µç›–çš„ä»»åŠ¡åŒ…æ‹¬è§£è¯»å›¾æ¡ˆå½¢çŠ¶èƒŒåçš„ç®—æ³•é€»è¾‘ã€‚</li>
<li>ç°æœ‰é¢†å…ˆçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨Turtleçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼ŒGPT-4çš„å‡†ç¡®ç‡ä»…ä¸º19%ã€‚</li>
<li>å°‘æ ·æœ¬æç¤ºå¯¹æ¨¡å‹çš„æ€§èƒ½æå‡æœ‰é™ã€‚</li>
<li>Turtleæ­ç¤ºäº†äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨ç›´è§‚å’Œè§†è§‰å‡ ä½•ç†è§£æ–¹é¢çš„å·®è·ã€‚</li>
<li>Turtleæ˜¯å°‘æ•°èƒ½å¤Ÿè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ä¸­è§†è§‰ç†è§£ä¸ä»£ç ç”Ÿæˆèƒ½åŠ›æ•´åˆçš„åŸºå‡†æµ‹è¯•ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ecc281439abde9f9e3e80666968d1744.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41bf8a5d09214a821764b6dd107ddc65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73e16883639b3056b4f9aed0862498cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecdcf952d1cd566a0c1e2fe951c733c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d6d567f09c41ded5f0d15836fca71e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Exploration-and-Discriminative-World-Model-Learning-with-an-Object-Centric-Abstraction"><a href="#Efficient-Exploration-and-Discriminative-World-Model-Learning-with-an-Object-Centric-Abstraction" class="headerlink" title="Efficient Exploration and Discriminative World Model Learning with an   Object-Centric Abstraction"></a>Efficient Exploration and Discriminative World Model Learning with an   Object-Centric Abstraction</h2><p><strong>Authors:Anthony GX-Chen, Kenneth Marino, Rob Fergus</strong></p>
<p>In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.   We demonstrate the modelâ€™s ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to learn low level object-perturbing policies via reinforcement learning, and the object mapping itself by supervised learning. </p>
<blockquote>
<p>é¢å¯¹å¼ºåŒ–å­¦ä¹ ä¸­çš„å›°éš¾æ¢ç´¢é—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç»™æ™ºèƒ½ä½“ä¸€ä¸ªä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ˜ å°„ï¼ˆæè¿°ä¸€ç»„ç‰©å“åŠå…¶å±æ€§ï¼‰æ˜¯å¦èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œå­¦ä¹ ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸ªé—®é¢˜æœ€å¥½é€šè¿‡åˆ†å±‚å»ºæ¨¡æ¥è§£å†³ï¼Œå°†ç‰©å“å»ºæ¨¡ä¸ºæ›´é«˜å±‚æ¬¡çš„çŠ¶æ€æŠ½è±¡ï¼Œåƒç´ ä¸ºæ›´ä½å±‚æ¬¡çš„ç»†èŠ‚ï¼›å°†å±æ€§å˜åŒ–å»ºæ¨¡ä¸ºæ›´é«˜å±‚æ¬¡çš„ä¸´æ—¶æŠ½è±¡ï¼ŒåŸºæœ¬è¡ŒåŠ¨ä¸ºæ›´ä½å±‚æ¬¡çš„ç»†èŠ‚ã€‚è¿™ç§æŠ½è±¡é€šè¿‡ä½¿ç‰¹å®šæœªæ¥çŠ¶æ€æ›´å®¹æ˜“é¢„æµ‹æ¥ç®€åŒ–è½¬ç§»åŠ¨æ€ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç‚¹æå‡ºäº†ä¸€ç§å®Œå…¨åŸºäºæ¨¡å‹çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å­¦ä¹ å…·æœ‰åŒºåˆ†æ€§çš„ä¸–ç•Œæ¨¡å‹ï¼Œä»…ä½¿ç”¨åŸºäºè®¡æ•°çš„å†…åœ¨å¥–åŠ±æ¥æœ‰æ•ˆåœ°è¿›è¡Œæ¢ç´¢ï¼Œå¹¶ä¸”å¯ä»¥éšåè¾¾åˆ°ä»»ä½•å‘ç°çš„ï¼ˆæŠ½è±¡ï¼‰çŠ¶æ€ã€‚æˆ‘ä»¬è¯æ˜äº†è¯¥æ¨¡å‹èƒ½å¤Ÿï¼ˆiï¼‰æœ‰æ•ˆåœ°è§£å†³å•ä¸€ä»»åŠ¡ï¼Œï¼ˆiiï¼‰åœ¨ä¸åŒç‰©å“ç±»å‹å’Œç¯å¢ƒä¸­å®ç°é›¶å¯åŠ¨å’Œå°‘é‡å¯åŠ¨è¿ç§»ï¼Œï¼ˆiiiï¼‰è¿›è¡Œé•¿æœŸè§„åˆ’ã€‚åœ¨ä¸€ç³»åˆ—2Då·¥è‰ºå’ŒMiniHackç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶è¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä½å±‚æ¬¡æ–¹æ³•ï¼ˆæ²¡æœ‰æŠ½è±¡ï¼‰ï¼Œä»¥åŠä½¿ç”¨ç›¸åŒæŠ½è±¡çš„é«˜æ€§èƒ½æ— æ¨¡å‹æ¨¡å‹å’ŒåŸºäºæ¨¡å‹çš„æ¨¡å‹æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ ä½å±‚æ¬¡çš„å¯¹è±¡æ‰°åŠ¨ç­–ç•¥ï¼Œä»¥åŠé€šè¿‡ç›‘ç£å­¦ä¹ æ¥å­¦ä¹ å¯¹è±¡æ˜ å°„æœ¬èº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11816v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨è§£å†³å›°éš¾æ¢ç´¢é—®é¢˜æ—¶ï¼Œé€šè¿‡ä½¿ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ˜ å°„ï¼ˆæè¿°ä¸€ç³»åˆ—ç‰©å“åŠå…¶å±æ€§ï¼‰å¯ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å±‚çº§å»ºæ¨¡ï¼Œå°†ç‰©å“ç½®äºçŠ¶æ€æŠ½è±¡çš„é«˜å±‚ï¼Œå°†å±æ€§å˜åŒ–ç½®äºæ—¶é—´æŠ½è±¡çš„é«˜å±‚ï¼ˆåŸå§‹åŠ¨ä½œï¼‰ï¼Œå¯ä»¥æ›´å¥½åœ°è§£å†³æ­¤é—®é¢˜ã€‚è¿™ç§æŠ½è±¡ç®€åŒ–äº†çŠ¶æ€è½¬ç§»çš„åŠ¨æ€è¿‡ç¨‹ï¼Œä½¿ç‰¹å®šæœªæ¥çŠ¶æ€æ›´å®¹æ˜“é¢„æµ‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§å…¨æ¨¡å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥å­¦ä¹ åˆ¤åˆ«ä¸–ç•Œæ¨¡å‹ï¼Œä»…é€šè¿‡åŸºäºè®¡æ•°çš„å†…åœ¨å¥–åŠ±è¿›è¡Œé«˜æ•ˆæ¢ç´¢è§„åˆ’ï¼Œå¹¶å¯ä»¥è§„åˆ’åˆ°è¾¾ä»»ä½•å‘ç°çš„ï¼ˆæŠ½è±¡ï¼‰çŠ¶æ€ã€‚æ¨¡å‹èƒ½å¤Ÿåœ¨ï¼ˆiï¼‰è§£å†³å•ä¸€ä»»åŠ¡ï¼Œï¼ˆiiï¼‰åœ¨ä¸åŒç‰©å“ç±»å‹å’Œç¯å¢ƒä¸­å®ç°é›¶èµ·ç‚¹å’Œå°‘é‡èµ·ç‚¹è½¬ç§»ï¼Œï¼ˆiiiï¼‰è¿›è¡Œé•¿æœŸè§„åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨å¤šä¸ªäºŒç»´åˆ¶ä½œå’ŒMiniHackç¯å¢ƒä¸­è¿›è¡Œçš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ä½å±‚æ¬¡æ–¹æ³•ï¼ˆæ— æŠ½è±¡ï¼‰ï¼Œä»¥åŠä½¿ç”¨ç›¸åŒæŠ½è±¡çš„æ¨¡å‹è‡ªç”±å’ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•ã€‚æœ€åï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ å­¦ä¹ ä½å±‚æ¬¡çš„å¯¹è±¡æ‰°åŠ¨ç­–ç•¥ä»¥åŠé€šè¿‡ç›‘ç£å­¦ä¹ å­¦ä¹ å¯¹è±¡æ˜ å°„æœ¬èº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸­é¢å¯¹å›°éš¾æ¢ç´¢é—®é¢˜æ—¶ï¼Œä½¿ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ˜ å°„èƒ½æé«˜å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>å±‚çº§å»ºæ¨¡æ˜¯è§£å†³æ­¤é—®é¢˜çš„æœ€ä½³é€”å¾„ï¼Œå°†ç‰©å“å’Œå±æ€§å˜åŒ–ç½®äºä¸åŒçš„æŠ½è±¡å±‚çº§ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç®€åŒ–çŠ¶æ€è½¬ç§»åŠ¨æ€ï¼Œä½¿é¢„æµ‹æœªæ¥çŠ¶æ€æ›´ä¸ºå®¹æ˜“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨æ¨¡å‹ç®—æ³•ï¼Œèƒ½å­¦ä¹ åˆ¤åˆ«ä¸–ç•Œæ¨¡å‹å¹¶é«˜æ•ˆæ¢ç´¢è§„åˆ’ã€‚</li>
<li>æ¨¡å‹åœ¨è§£å†³å•ä¸€ä»»åŠ¡ã€è·¨ç‰©å“å’Œç¯å¢ƒè½¬ç§»ä»¥åŠé•¿æœŸè§„åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb7d6decba96352656ccde6db12f55b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b850ff89f8fcbba9e5dd77201b9dab4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccb6b4ca67443f73b7a6fb458cf6c260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea10df4a042e5f4cc09da91d391c4ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b7525703114d9e89a7d217c862b4504.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01ce23a5ca6d3d3889891fca53d0d3f5.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Anchor Token Matching Implicit Structure Locking for Training-free AR   Image Editing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-69ef5a21e4c999ebe06305e103031cd5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  GUI-R1  A Generalist R1-Style Vision-Language Action Model For GUI   Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17259.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
