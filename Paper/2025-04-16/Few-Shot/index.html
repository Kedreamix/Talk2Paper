<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-16  Siamese Network with Dual Attention for EEG-Driven Social Learning   Bridging the Human-Robot Gap in Long-Tail Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b795a39a5921ceccac85887f872f59fc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-16-更新"><a href="#2025-04-16-更新" class="headerlink" title="2025-04-16 更新"></a>2025-04-16 更新</h1><h2 id="Siamese-Network-with-Dual-Attention-for-EEG-Driven-Social-Learning-Bridging-the-Human-Robot-Gap-in-Long-Tail-Autonomous-Driving"><a href="#Siamese-Network-with-Dual-Attention-for-EEG-Driven-Social-Learning-Bridging-the-Human-Robot-Gap-in-Long-Tail-Autonomous-Driving" class="headerlink" title="Siamese Network with Dual Attention for EEG-Driven Social Learning:   Bridging the Human-Robot Gap in Long-Tail Autonomous Driving"></a>Siamese Network with Dual Attention for EEG-Driven Social Learning:   Bridging the Human-Robot Gap in Long-Tail Autonomous Driving</h2><p><strong>Authors:Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat</strong></p>
<p>Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments. </p>
<blockquote>
<p>带有轮子、四足或人形形式的机器人越来越多地被集成到建筑环境中。然而，与人类的社会学习不同，它们在内在认知发展的关键途径上有所缺失，即在与人类互动过程中从人类反馈中学习。为了理解人类在动态和不确定环境中的普遍观察、监督以及共享控制，本研究提出了一个脑机接口（BCI）框架，该框架能够对脑电图（EEG）信号进行分类，以检测认知需求和安全关键事件。作为一个及时且鼓舞人心的协同机器人工程应用，我们模拟了一个有人类参与的闭环场景来标记半自主机器人驾驶中的危险事件——这代表了智能移动系统和机器人车辆安全性能中持久的瓶颈的长期尾部案例。基于最近的小样本学习进展，我们提出了一种双注意力孪生卷积网络，结合动态时间规整质心平均法，生成稳健的EEG编码信号表示。逆向源定位显示Broadman 4区和9区的激活，这表明在任务相关的心理意象过程中感知-行动耦合。该模型在数据稀缺的条件下实现了80%的分类精度，与最先进的方法相比，显著特征的实用性增加了近100%，这是通过集成梯度归属度来衡量的。除了性能之外，该研究还有助于我们理解脑机接口代理所需的认知架构——特别是注意力和记忆机制在分类不同心理状态和支持主体间和主体内适应中的作用。总体而言，这项研究推动了认知机器人以及在复杂建筑环境中服务机器人的社会指导学习的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10296v1">PDF</a> 50 pages, 18 figures</p>
<p><strong>摘要</strong></p>
<p>本研究提出了一种基于脑机接口（BCI）的框架，通过采集和分析脑电图（EEG）信号，以实现对人类在动态和不确定环境中普遍的观察、监督和共享控制的理解。该研究利用先进的少样本学习技术，通过双注意力Siamese卷积网络和动态时间规整重心平均法生成稳健的EEG信号表示。逆源定位揭示了Broadman 4区和9区的激活，表明在任务相关心理图像中的感知动作耦合。在数据稀缺条件下，该模型实现了80%的分类精度，与现有方法相比，显著特征效用提高了近100%。本研究不仅提高了性能，而且加深了对BCI代理所需认知架构的理解，特别是注意力和记忆机制在分类各种心理状态和支持跨主体和主体内适应中的作用。这项研究推动了认知机器人和复杂建筑环境中服务机器人的社会指导学习的发展。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究提出了基于脑机接口（BCI）的框架，旨在理解人类在与机器人交互中的观察、监督和共享控制。</li>
<li>结合少样本学习技术，通过双注意力Siamese卷积网络和Dynamic Time Warping Barycenter Averaging方法处理EEG信号。</li>
<li>研究通过逆源定位发现了与任务相关心理图像感知动作耦合的脑部激活区域。</li>
<li>模型在数据稀缺条件下达到了80%的分类精度。</li>
<li>与现有方法相比，该模型的显著特征效用提升了近100%。</li>
<li>研究结果不仅关注性能提升，还深入探讨了BCI所需的认知架构。</li>
<li>研究推动了认知机器人在复杂建筑环境中的发展，特别是在服务机器人领域的社会指导学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-21431de1f7e72376c874e43864193ca1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AGO-Adaptive-Grounding-for-Open-World-3D-Occupancy-Prediction"><a href="#AGO-Adaptive-Grounding-for-Open-World-3D-Occupancy-Prediction" class="headerlink" title="AGO: Adaptive Grounding for Open World 3D Occupancy Prediction"></a>AGO: Adaptive Grounding for Open World 3D Occupancy Prediction</h2><p><strong>Authors:Peizheng Li, Shuxiao Ding, You Zhou, Qingwen Zhang, Onat Inak, Larissa Triess, Niklas Hanselmann, Marius Cordts, Andreas Zell</strong></p>
<p>Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D representation from sensor inputs while recognizing both known and unknown objects. Transferring open-vocabulary knowledge from vision-language models (VLMs) offers a promising direction but remains challenging. However, methods based on VLM-derived 2D pseudo-labels with traditional supervision are limited by a predefined label space and lack general prediction capabilities. Direct alignment with pretrained image embeddings, on the other hand, fails to achieve reliable performance due to often inconsistent image and text representations in VLMs. To address these challenges, we propose AGO, a novel 3D occupancy prediction framework with adaptive grounding to handle diverse open-world scenarios. AGO first encodes surrounding images and class prompts into 3D and text embeddings, respectively, leveraging similarity-based grounding training with 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into a space aligned with VLM-derived image embeddings, reducing modality gaps. Experiments on Occ3D-nuScenes show that AGO improves unknown object prediction in zero-shot and few-shot transfer while achieving state-of-the-art closed-world self-supervised performance, surpassing prior methods by 4.09 mIoU. </p>
<blockquote>
<p>开放世界3D语义占用预测旨在从传感器输入生成体素化3D表示，同时识别已知和未知对象。从视觉语言模型（VLM）转移开放词汇知识提供了一个有前途的方向，但仍然具有挑战性。然而，基于VLM衍生的具有传统监督的2D伪标签的方法受限于预定义的标签空间，并且缺乏通用的预测能力。另一方面，与预训练图像嵌入的直接对齐由于VLM中图像和文本表示经常不一致，无法实现可靠性能。为了解决这些挑战，我们提出了AGO，这是一种具有自适应定位功能的新型3D占用预测框架，用于处理各种开放世界场景。AGO首先编码周围图像和类别提示为3D和文本嵌入，分别利用基于相似性的定位训练和3D伪标签。此外，模态适配器将3D嵌入映射到与VLM衍生的图像嵌入对齐的空间，减少模态间隙。在Occ3D-nuScenes上的实验表明，AGO在零样本和少样本转移中改进了未知对象的预测，同时在封闭世界自监督情况下达到最先进的性能，比先前的方法高出4.09 mIoU。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10117v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了开放世界下的三维语义占用预测技术，该技术旨在从传感器输入生成体素化的三维表示，并识别已知和未知物体。文章指出，从视觉语言模型（VLMs）转移开放词汇知识是一个有前途的方向，但面临挑战。现有方法如基于VLM衍生的二维伪标签和传统监督方法受限于预定义的标签空间，缺乏通用预测能力。直接对齐预训练图像嵌入则由于VLMs中图像和文本表示的不一致性而无法实现可靠性能。为应对这些挑战，本文提出了一种名为AGO的新型三维占用预测框架，具有自适应定位功能，以处理多样化的开放世界场景。AGO通过编码周围图像和类别提示到三维和文本嵌入，利用基于相似性的定位训练和三维伪标签。此外，模态适配器将三维嵌入映射到与VLM衍生的图像嵌入对齐的空间，减少模态差距。在Occ3D-nuScenes的实验显示，AGO在零样本和少样本转移中改进了未知物体的预测，同时实现了最先进的封闭世界自监督性能，超出之前的方法4.09 mIoU。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开放世界下的三维语义占用预测旨在从传感器输入生成体素化的三维表示，并识别已知和未知物体。</li>
<li>从视觉语言模型转移开放词汇知识在这一领域具有潜力，但面临诸多挑战。</li>
<li>现有方法受限于预定义的标签空间，缺乏通用预测能力。</li>
<li>直接对齐预训练图像嵌入由于表示不一致性而无法实现可靠性能。</li>
<li>AGO框架通过编码周围图像和类别提示到三维和文本嵌入来解决这些问题。</li>
<li>AGO利用基于相似性的定位训练和三维伪标签。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10117">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cc4f1c69d9ed9d9eefa64a32654ec3dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-694a1e667b83021d67281ec43a40babb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0904340c6f18ce30f9b401fa88008e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-048da004411e345bd2625ce4d95a7471.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Relation-augmented-Representation-Generalization-for-Few-shot-Action-Recognition"><a href="#Hierarchical-Relation-augmented-Representation-Generalization-for-Few-shot-Action-Recognition" class="headerlink" title="Hierarchical Relation-augmented Representation Generalization for   Few-shot Action Recognition"></a>Hierarchical Relation-augmented Representation Generalization for   Few-shot Action Recognition</h2><p><strong>Authors:Hongyu Qu, Ling Xing, Rui Yan, Yazhou Yao, Guo-Sen Xie, Xiangbo Shu</strong></p>
<p>Few-shot action recognition (FSAR) aims to recognize novel action categories with few exemplars. Existing methods typically learn frame-level representations independently for each video by designing various inter-frame temporal modeling strategies. However, they neglect explicit relation modeling between videos and tasks, thus failing to capture shared temporal patterns across videos and reuse temporal knowledge from historical tasks. In light of this, we propose HR2G-shot, a Hierarchical Relation-augmented Representation Generalization framework for FSAR, which unifies three types of relation modeling (inter-frame, inter-video, and inter-task) to learn task-specific temporal patterns from a holistic view. In addition to conducting inter-frame temporal interactions, we further devise two components to respectively explore inter-video and inter-task relationships: i) Inter-video Semantic Correlation (ISC) performs cross-video frame-level interactions in a fine-grained manner, thereby capturing task-specific query features and learning intra- and inter-class temporal correlations among support features; ii) Inter-task Knowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge from the bank, which stores diverse temporal patterns from historical tasks. Extensive experiments on five benchmarks show that HR2G-shot outperforms current top-leading FSAR methods. </p>
<blockquote>
<p>小样动作识别（FSAR）旨在通过少量的样本识别新的动作类别。现有的方法通常通过设计各种帧间时序建模策略，为每个视频独立地学习帧级表示。然而，他们忽略了视频与任务之间的显式关系建模，因此无法捕获跨视频的共享时序模式，也无法重用来自历史任务的时间知识。鉴于此，我们提出了HR2G-shot，这是一个用于FSAR的分层关系增强表示泛化框架，它统一了三种关系建模（帧间、视频间和任务间），以从整体视角学习特定任务的时序模式。除了进行帧间时序交互外，我们还进一步设计了两个组件来分别探索视频间和任务间的关系：一、视频间语义相关性（ISC）以精细的方式执行跨视频帧级交互，从而捕获特定任务的查询特征，并学习支持特征内部和跨类的时序相关性；二、任务间知识转移（IKT）从知识库中检索并聚合相关的时序知识，该知识库存储了来自历史任务的各种时序模式。在五个基准测试上的广泛实验表明，HR2G-shot的性能优于当前领先的FSAR方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10079v1">PDF</a> </p>
<p><strong>Summary</strong><br>少量样本动作识别（FSAR）旨在通过少量样本识别新型动作类别。现有方法主要通过设计各种帧间时序建模策略独立学习每个视频帧级别的表示，但忽略了视频与任务之间的关系建模，因此无法捕获跨视频的共享时序模式并无法利用历史任务中的时序知识。因此，我们提出HR2G-shot，一个用于FSAR的层次关系增强表示泛化框架，它统一了三种关系建模（帧间、视频间和任务间），从整体上学习特定任务的时序模式。除了进行帧间时序交互外，我们还分别设计了两个组件来探索视频间和任务间的关系：一是跨视频语义相关性（ISC），以精细的方式执行跨视频帧级别的交互，从而捕获特定任务的查询特征，并学习支持特征中的类内和类间时序相关性；二是任务间知识转移（IKT），从知识库中检索和聚合相关的时序知识，该知识库存储了来自历史任务的各种时序模式。在五个基准测试上的广泛实验表明，HR2G-shot优于当前领先的FSAR方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot action recognition (FSAR) 旨在通过少量样本识别新型动作类别。</li>
<li>现有方法主要关注帧级别的独立表示学习，忽略了视频与任务之间的关系建模。</li>
<li>HR2G-shot框架集成了三种关系建模（帧间、视频间和任务间），以全面学习特定任务的时序模式。</li>
<li>HR2G-shot通过跨视频语义相关性（ISC）和任务间知识转移（IKT）两个组件来探索视频间和任务间的关系。</li>
<li>ISC执行精细的跨视频帧级别交互，捕获特定任务的查询特征并学习类内和类间的时序相关性。</li>
<li>IKT从知识库中检索和聚合相关时序知识，该知识库包含历史任务中的多样时序模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3e4ea7f2f63eac80b6adbf9c78002d5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80e799dd41f2de9b76f8caf3fecb8936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716acf6998ae84c18644fb207f51056d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-225a832f5b74bd506064afe35e377cba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reasoning-Court-Combining-Reasoning-Action-and-Judgment-for-Multi-Hop-Reasoning"><a href="#Reasoning-Court-Combining-Reasoning-Action-and-Judgment-for-Multi-Hop-Reasoning" class="headerlink" title="Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop   Reasoning"></a>Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop   Reasoning</h2><p><strong>Authors:Jingtian Wu, Claire Cardie</strong></p>
<p>While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning. </p>
<blockquote>
<p>大型语言模型（LLM）在问答和事实核查等任务中表现出了强大的能力，但它们仍然存在着幻想和推理错误的问题，特别是在需要整合多个信息源的多跳任务中。当前的方法通过基于检索的技术（以外部证据为推理基础）、基于推理的方法（通过改进提示来提高连贯性）或结合两者的混合策略来解决这些问题。一种突出的混合方法ReAct已经超越了纯粹的检索或基于推理的方法；然而，它缺乏对中间推理步骤的内部验证，使得潜在错误可能通过复杂的推理任务而传播。在本文中，我们引入了Reasoning Court（RC），这是一个新的框架，它将迭代推理和检索方法（如ReAct）与专用的LLM法官相结合。不同于ReAct，RC使用法官独立评估多个候选答案及其相关的推理，这些答案和推理由单独的LLM代理生成。法官被要求选择它认为最基于事实、逻辑上连贯的答案，该答案基于提供的推理和证据，或者当所有候选答案都不足、有缺陷或无效时，利用可用证据和其预训练知识合成新的答案。在Multi-hop基准测试（HotpotQA、MuSiQue）和事实核查（FEVER）上的评估表明，RC始终优于最新的少提示提示方法，且无需针对特定任务进行微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09781v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Reasoning Court（RC）的新框架，它扩展了迭代推理和检索方法（如ReAct），并引入了一个独立的LLM法官来评估多个候选答案及其相关推理。RC的法官能够基于提供的推理和证据，选择它认为最符合事实和逻辑连贯性的答案，或者在所有候选答案都不足、有缺陷或无效时，利用可用证据和预训练知识合成新的答案。在多跳基准测试（HotpotQA、MuSiQue）和事实验证（FEVER）上的评估表明，RC在无需特定任务微调的情况下，始终优于最新的少样本提示方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLMs）在问答和事实验证等任务中表现出强大的能力，但在需要整合多个信息源的多跳任务中仍存在幻觉和推理错误。</li>
<li>当前方法通过检索、推理或混合策略来解决这些问题，但存在误差传播风险。</li>
<li>ReAct等迭代推理和检索方法得到了扩展，引入了一个新的框架Reasoning Court（RC）。</li>
<li>RC的关键特性是引入了一个独立的LLM法官，用于评估多个候选答案及其相关推理，选择最符合事实和逻辑连贯性的答案。</li>
<li>如果所有候选答案都不足、有缺陷或无效，RC的法官能够利用可用证据和预训练知识合成新的答案。</li>
<li>在多跳基准测试和事实验证上的评估表明，RC在无需任务特定微调的情况下表现优异。</li>
<li>RC框架为改进LLMs在处理复杂推理任务时的性能提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09781">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b795a39a5921ceccac85887f872f59fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db5218c13782dfaeed6dfb41a21b204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cea842cfd0f19308d4daa86e641bb599.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Model-for-Object-Detection-and-Segmentation-A-Review-and-Evaluation"><a href="#Vision-Language-Model-for-Object-Detection-and-Segmentation-A-Review-and-Evaluation" class="headerlink" title="Vision-Language Model for Object Detection and Segmentation: A Review   and Evaluation"></a>Vision-Language Model for Object Detection and Segmentation: A Review   and Evaluation</h2><p><strong>Authors:Yongchao Feng, Yajie Liu, Shuai Yang, Wenrui Cai, Jinqing Zhang, Qiqi Zhan, Ziyue Huang, Hongxi Yan, Qiao Wan, Chenguang Liu, Junzhe Wang, Jiahui Lv, Ziqi Liu, Tengyuan Shi, Qingjie Liu, Yunhong Wang</strong></p>
<p>Vision-Language Model (VLM) have gained widespread adoption in Open-Vocabulary (OV) object detection and segmentation tasks. Despite they have shown promise on OV-related tasks, their effectiveness in conventional vision tasks has thus far been unevaluated. In this work, we present the systematic review of VLM-based detection and segmentation, view VLM as the foundational model and conduct comprehensive evaluations across multiple downstream tasks for the first time: 1) The evaluation spans eight detection scenarios (closed-set detection, domain adaptation, crowded objects, etc.) and eight segmentation scenarios (few-shot, open-world, small object, etc.), revealing distinct performance advantages and limitations of various VLM architectures across tasks. 2) As for detection tasks, we evaluate VLMs under three finetuning granularities: \textit{zero prediction}, \textit{visual fine-tuning}, and \textit{text prompt}, and further analyze how different finetuning strategies impact performance under varied task. 3) Based on empirical findings, we provide in-depth analysis of the correlations between task characteristics, model architectures, and training methodologies, offering insights for future VLM design. 4) We believe that this work shall be valuable to the pattern recognition experts working in the fields of computer vision, multimodal learning, and vision foundation models by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research. A project associated with this review and evaluation has been created at <a target="_blank" rel="noopener" href="https://github.com/better-chao/perceptual_abilities_evaluation">https://github.com/better-chao/perceptual_abilities_evaluation</a>. </p>
<blockquote>
<p>视觉语言模型（VLM）在开放词汇（OV）目标检测和分割任务中得到了广泛应用。尽管它们在OV相关任务上显示出潜力，但它们在传统视觉任务中的有效性迄今为止尚未得到评估。在这项工作中，我们对基于VLM的检测和分割进行了系统综述，将VLM视为基础模型，首次对多个下游任务进行了全面评估：1）评估涵盖八种检测场景（封闭集检测、域适应、拥挤目标等）和八种分割场景（小样本、开放世界、小目标等），揭示了各种VLM架构在不同任务之间的不同性能优势和局限性。2）对于检测任务，我们在三种微调粒度下评估VLM：\emph{零预测}、\emph{视觉微调}和\emph{文本提示}，并进一步分析不同微调策略在不同任务下对性能的影响。3）基于实证发现，我们对任务特性、模型架构和训练方法之间的关联性进行了深入分析，为未来的VLM设计提供了见解。4）我们相信，这项工作对于计算机视觉、多模态学习和视觉基础模型领域的模式识别专家来说将具有很高的价值，通过引入问题，让他们熟悉当前的研究进展，并为未来的研究提供有前景的方向。与此次审查和评估相关的项目已在<a target="_blank" rel="noopener" href="https://github.com/better-chao/perceptual_abilities_evaluation%E5%88%9B%E5%BB%BA%E3%80%82">https://github.com/better-chao/perceptual_abilities_evaluation创建。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09480v1">PDF</a> A Review and Evaluation about Vision-Language Model for Object   Detection and Segmentation</p>
<p><strong>Summary</strong></p>
<p>本文系统评述了基于Vision-Language Model（VLM）的检测和分割技术。研究对VLM在多个下游任务上的表现进行了全面评估，包括封闭集检测、域适应、拥挤对象等八个检测场景，以及少样本、开放世界、小物体等八个分割场景。此外，还探讨了不同微调策略对性能的影响，并为未来VLM设计提供了深入的分析和见解。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLM在Open-Vocabulary（OV）物体检测和分割任务中得到广泛应用，但在传统视觉任务中的效果尚未评估。</li>
<li>首次对VLM基础的检测和分割进行全面评价，涵盖多个下游任务。</li>
<li>在检测任务中，评估了VLM在零预测、视觉微调、文本提示三种微调粒度下的性能。</li>
<li>发现不同微调策略对性能的影响，并提供任务特性、模型架构、训练方法的深度分析。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09480">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f4ec0e9bd0dbc84e7142409fd8702c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0109435619729faa8321591015dcaac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02e7695515944037da0150bee979d55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f8d005d7606375a5cc4cd2ac519d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddf6dd92bd6c2ff15b453c28a5d54cac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6bbd25db966e64e5743b1fb0c2b1ea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2196185bdf1208a46dfd7e06aa15500.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning"><a href="#Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning" class="headerlink" title="Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning"></a>Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning</h2><p><strong>Authors:Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</strong></p>
<p>Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods. </p>
<blockquote>
<p>将新类别信息集成到模型中而不丢失先前获取的知识仍然是人工智能领域的一个核心挑战，通常被称为灾难性遗忘。少样本类别增量学习（FSCIL）通过首先使用基础类别的稳健数据集训练模型，然后逐步仅使用每个新类别的少量标记样本对其进行适应来解决这个问题。然而，这种方法对新数据的过度拟合倾向较高，可能会损害整体性能并加剧遗忘问题。在这项工作中，我们提出了一种简单有效的FSCIL新框架，该框架利用冻结的Vision Transformer（ViT）主干并辅以高效的参数添加更新。我们的方法冻结了预训练的ViT参数，并通过添加更新机制有选择地将可训练权重注入自注意力模块。这种设计只更新一小部分参数以适应新类别，同时不牺牲基础会话期间学习的表示。通过微调有限数量的参数，我们的方法在保留冻结ViT的通用特征的同时降低了过度拟合的风险。此外，由于大多数参数保持不变，当引入少量新数据批次时，模型避免了覆盖先前学习的知识。在基准数据集上的大量实验表明，我们的方法与基线FSCIL方法相比达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08982v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的少样本类增量学习（FSCIL）框架，该框架利用冻结的Vision Transformer（ViT）主干网，并通过参数有效的添加更新进行增强。该方法在预训练的ViT参数冻结的基础上，通过添加更新机制有选择地将可训练权重注入自注意力模块。该设计仅更新一小部分参数以适应新类别，同时保留基础会话期间学习的表示。通过微调有限数量的参数，该方法保留了冻结ViT的通用特征，并降低了过度拟合的风险。此外，由于大部分参数保持不变，当引入少量新数据批次时，模型避免了覆盖先前学习的知识。在基准数据集上的实验表明，该方法与基线FSCIL方法相比具有最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本类增量学习（FSCIL）是人工智能领域的一个重要挑战，旨在整合新类别信息而不会丢失先前获得的知识。</li>
<li>本文提出了一种新的FSCIL框架，利用冻结的Vision Transformer（ViT）主干网，通过参数有效的添加更新进行增强。</li>
<li>该方法通过冻结预训练的ViT参数并仅更新一小部分参数以适应新类别，同时保留基础会话期间学习的表示。</li>
<li>通过微调有限数量的参数，该方法在保留通用特征的同时降低了过度拟合的风险。</li>
<li>大部分参数保持不变，使得模型在引入少量新数据批次时避免覆盖先前学习的知识。</li>
<li>基准数据集上的实验表明，该方法具有最佳性能，优于其他FSCIL方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b28bfe5d16a2e4d78b04a9bde0aae140.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5d833abb92541ba15d49cfb0300e89d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3a54371e37affb4221eca75caed2ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851985bb320feb6264afab7c7cdd12e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b87b356799882e2673404bdb4fcf26cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0a31ee5fe4c921c67cfb818185fb9e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-Few-Shot-Fusion-Granular-Ball-Priors-Enable-General-Purpose-Deep-Image-Fusion"><a href="#Rethinking-Few-Shot-Fusion-Granular-Ball-Priors-Enable-General-Purpose-Deep-Image-Fusion" class="headerlink" title="Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose   Deep Image Fusion"></a>Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose   Deep Image Fusion</h2><p><strong>Authors:Minjie Deng, Yan Wei, Hao Zhai, An Wu, Yuncan Ouyang, Qianyao Peng</strong></p>
<p>In image fusion tasks, due to the lack of real fused images as priors, most deep learning-based fusion methods obtain global weight features from original images in large-scale data pairs to generate images that approximate real fused images. However, unlike previous studies, this paper utilizes Granular Ball adaptation to extract features in the brightness space as priors for deep networks, enabling the fusion network to converge quickly and complete the fusion task. This leads to few-shot training for a general image fusion network, and based on this, we propose the GBFF fusion method. According to the information expression division of pixel pairs in the original fused image, we classify pixel pairs with significant performance as the positive domain and non-significant pixel pairs as the boundary domain. We perform split inference in the brightness space using Granular Ball adaptation to compute weights for pixels that express information to varying degrees, generating approximate supervision images that provide priors for the neural network in the structural brightness space. Additionally, the extracted global saliency features also adaptively provide priors for setting the loss function weights of each image in the network, guiding the network to converge quickly at both global and pixel levels alongside the supervised images, thereby enhancing the expressiveness of the fused images. Each modality only used 10 pairs of images as the training set, completing the fusion task with a limited number of iterations. Experiments validate the effectiveness of the algorithm and theory, and qualitative and quantitative comparisons with SOTA methods show that this approach is highly competitive in terms of fusion time and image expressiveness. </p>
<blockquote>
<p>在图像融合任务中，由于缺乏真实的融合图像作为先验知识，大多数基于深度学习的方法都是从大规模数据对的原始图像中提取全局权重特征，以生成近似真实的融合图像。然而，不同于以往的研究，本文利用粒球适应法提取亮度空间中的特征作为深度网络的先验知识，使融合网络能够快速收敛并完成融合任务。这导致了对通用图像融合网络进行小样本训练，并基于此我们提出了GBFF融合方法。根据原始融合图像中像素对的信息表达划分，我们将表现显著的像素对分类为正向域，将非显著的像素对分类为边界域。我们在亮度空间中使用粒球适应法进行分割推理，计算不同程度表达信息的像素权重，生成近似监督图像，为神经网络在结构亮度空间中提供先验知识。此外，提取的全局显著性特征也自适应地为设置网络中每张图像的损失函数权重提供先验知识，引导网络在全局和像素级别上快速收敛，同时结合监督图像，从而提高融合图像的表达性。每种模态仅使用10对图像作为训练集，在有限的迭代次数中完成融合任务。实验验证了算法和理论的有效性，与最先进方法的定性和定量比较表明，该方法在融合时间和图像表达方面具有很强的竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Granular Ball适应性的图像融合方法，通过提取亮度空间特征作为深度网络的先验信息，实现快速融合任务。利用像素对的信息表达分类，将具有显著性能的像素对归为正域，非显著像素对归为边界域。通过分割推理计算不同表达程度的像素权重，生成近似监督图像，为网络提供结构亮度空间的先验信息。同时，提取的全局显著性特征自适应地为设置网络中的每个图像损失函数权重提供先验知识，从而在全局和像素级别快速引导网络收敛。仅使用少量图像对即可完成融合任务。实验验证了算法和理论的有效性，与最新方法的定性和定量比较表明，该方法在融合时间和图像表现力方面表现出高度竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用Granular Ball适应性提取亮度空间特征作为深度网络的先验信息。</li>
<li>通过分类像素对实现快速融合任务，显著性能像素对归为正域，非显著像素对归为边界域。</li>
<li>通过分割推理计算像素权重，生成近似监督图像，为网络提供结构亮度空间的先验。</li>
<li>提取的全局显著性特征自适应地设置网络中的损失函数权重。</li>
<li>仅需少量图像对即可完成融合任务，提高了效率。</li>
<li>实验验证了算法和理论的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-13cbe48cd6bd2aea54cfd465754d57ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1976c864cc8fe5248968b04742b66314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38d01c17d9ddcc45f4ef7f755006c653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73746229a5ac5ef7acc78e197e55a3b7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Derm1M-A-Million-scale-Vision-Language-Dataset-Aligned-with-Clinical-Ontology-Knowledge-for-Dermatology"><a href="#Derm1M-A-Million-scale-Vision-Language-Dataset-Aligned-with-Clinical-Ontology-Knowledge-for-Dermatology" class="headerlink" title="Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical   Ontology Knowledge for Dermatology"></a>Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical   Ontology Knowledge for Dermatology</h2><p><strong>Authors:Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge</strong></p>
<p>The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot&#x2F;full-shot learning, and cross-modal retrieval. Our dataset and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/Derm1M">https://github.com/SiyuanYan1/Derm1M</a> upon acceptance. </p>
<blockquote>
<p>视觉语言模型的涌现已经推动了医疗人工智能的进步，使得诊断能力和临床应用方面取得了前所未有的进展。然而，皮肤病学方面的进展由于缺少标准图像文本配对而落后于其他医学领域。现有的皮肤病数据集在规模和深度上均有限，只提供狭窄疾病范围内的单一标签注释，缺乏丰富的文本描述和现实世界应用所需的关键临床背景。为了解决这些局限性，我们推出了Derm1M，这是皮肤病学领域第一个大规模视觉语言数据集，包含1,029,761个图像文本对。Derm1M由多样化的教育资源构建而成，围绕专家共同开发的标准本体结构构建，对超过390种皮肤状况进行了全面覆盖，涉及四个层次和130个临床概念，并提供了丰富的上下文信息，如病史、症状和肤色等。为了证明Derm1M在推动人工智能研究和临床应用方面的潜力，我们在该数据集上预训练了一系列类似于CLIP的模型，统称为DermLIP系列。DermLIP家族在多个任务的八个不同数据集上显著优于最新的基础模型，包括零样本皮肤疾病分类、临床和文物概念识别、小样本&#x2F;全样本学习和跨模态检索。我们的数据集和代码将在论文被接受后公开于<a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/Derm1M%E3%80%82">https://github.com/SiyuanYan1/Derm1M。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14911v2">PDF</a> Our dataset and code will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/Derm1M">https://github.com/SiyuanYan1/Derm1M</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了新兴的视觉语言模型在医疗AI领域的变革性作用，特别是在诊断能力和临床应用方面的突破。然而，皮肤病学领域的进展因缺乏标准图像文本配对而滞后。为解决此问题，本文提出了首个大规模皮肤病学视觉语言数据集Derm1M，包含1,029,761个图像文本对。该数据集涵盖了超过390种皮肤疾病，提供了丰富的上下文信息，如病史、症状和肤色等。为展示Derm1M在推进AI研究和临床应用方面的潜力，本文预训练了一系列名为DermLIP的CLIP类模型。DermLIP家族在多个任务上的表现显著优于现有最先进的模型，包括零样本皮肤疾病分类、临床和伪概念识别、少样本&#x2F;全样本学习以及跨模态检索。数据集和相关代码将在论文被接受后公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型的出现在医疗AI领域带来了革命性的进步，特别是在诊断和临床应用方面。</li>
<li>皮肤病学领域在AI应用方面的进展相对滞后，主要由于缺乏标准的图像文本配对数据集。</li>
<li>Derm1M是首个针对皮肤病学的大规模视觉语言数据集，包含大量的图像文本对，覆盖了多种皮肤疾病和丰富的上下文信息。</li>
<li>DermLIP系列模型在多个任务上表现出卓越性能，包括零样本分类、临床和伪概念识别等。</li>
<li>Derm1M数据集对AI研究和临床应用具有巨大潜力。</li>
<li>Derm1M数据集和相关代码将在论文被接受后公开提供，便于其他研究者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6ec35247ba9d4768824ed15cf6b15628.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae9640105bedf3752cb9a63e9b78577d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebc2724bba1f810efdeddb31028ccad1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85d1674fbc61b98d12db21b0f7252e7f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TurtleBench-A-Visual-Programming-Benchmark-in-Turtle-Geometry"><a href="#TurtleBench-A-Visual-Programming-Benchmark-in-Turtle-Geometry" class="headerlink" title="TurtleBench: A Visual Programming Benchmark in Turtle Geometry"></a>TurtleBench: A Visual Programming Benchmark in Turtle Geometry</h2><p><strong>Authors:Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi</strong></p>
<p>Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce \Turtle, a benchmark designed to evaluate LMMs’ capacity to interpret geometric patterns – given visual examples, textual instructions, or both – and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4o achieving only 19% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance ($&lt;2%$). \Turtle highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area. \Turtle stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research. Code and Dataset for this paper is provided here: \href{<a target="_blank" rel="noopener" href="https://github.com/sinaris76/TurtleBench%7D%7Bhttps://github.com/sinaris76/TurtleBench%7D">https://github.com/sinaris76/TurtleBench}{https://github.com/sinaris76/TurtleBench}</a> </p>
<blockquote>
<p>人类从小就具备理解和分析图像和场景中的几何模式的能力。然而，开发具有类似推理能力的大型多模式模型（LMM）仍然是一个挑战，这突显了需要可靠的评估方法来评估这些能力。我们推出了TurtleBench，一个旨在评估LMM解释几何模式的能力的基准测试。给定视觉示例、文本指令或两者兼有，并生成精确的代码输出。TurtleBench以海龟几何学为灵感，这是一种用于教授儿童基础编码和几何概念的概念，其特征是具有潜在算法逻辑的模式形状任务。我们的评估发现，领先的LMM在这些任务上面临巨大挑战，GPT-4在最简单任务上的准确率仅为19%，而且few-shot提示只略微提高了其性能（小于2%）。Turtle突出了人类在直观和视觉几何理解方面与人工智能的性能差距，为这一领域的未来研究奠定了基础。Turtle是少数几个能够评估LMM中视觉理解与代码生成能力融合的基准测试之一，为未来的研究奠定了基础。本文的代码和数据集请参见：<a target="_blank" rel="noopener" href="https://github.com/sinaris76/TurtleBench%E3%80%82">https://github.com/sinaris76/TurtleBench。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00264v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为Turtle的基准测试平台，旨在评估多模态模型对图像和场景中的几何模式进行解读的能力。该基准测试通过视觉示例、文本指令或两者的组合来评估模型生成精确代码输出的能力。研究表明，领先的大型多模态模型在这些任务上表现挣扎，GPT-4的准确率仅为19%，且少样本提示仅略微改善其表现。Turtle凸显了人类与人工智能在直观和视觉几何理解方面的差距，为未来研究奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Turtle是一个用于评估大型多模态模型（LMMs）解读几何模式能力的基准测试平台。</li>
<li>平台涵盖的任务包括解读图案形状背后的算法逻辑。</li>
<li>现有领先的大型多模态模型在Turtle的任务上表现不佳，GPT-4的准确率仅为19%。</li>
<li>少样本提示对模型的性能提升有限。</li>
<li>Turtle揭示了人类与人工智能在直观和视觉几何理解方面的差距。</li>
<li>Turtle是少数能够评估多模态模型中视觉理解与代码生成能力整合的基准测试之一。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ecc281439abde9f9e3e80666968d1744.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41bf8a5d09214a821764b6dd107ddc65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73e16883639b3056b4f9aed0862498cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecdcf952d1cd566a0c1e2fe951c733c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d6d567f09c41ded5f0d15836fca71e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Exploration-and-Discriminative-World-Model-Learning-with-an-Object-Centric-Abstraction"><a href="#Efficient-Exploration-and-Discriminative-World-Model-Learning-with-an-Object-Centric-Abstraction" class="headerlink" title="Efficient Exploration and Discriminative World Model Learning with an   Object-Centric Abstraction"></a>Efficient Exploration and Discriminative World Model Learning with an   Object-Centric Abstraction</h2><p><strong>Authors:Anthony GX-Chen, Kenneth Marino, Rob Fergus</strong></p>
<p>In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.   We demonstrate the model’s ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to learn low level object-perturbing policies via reinforcement learning, and the object mapping itself by supervised learning. </p>
<blockquote>
<p>面对强化学习中的困难探索问题，我们研究了给智能体一个以物体为中心的映射（描述一组物品及其属性）是否能够更有效地进行学习。我们发现这个问题最好通过分层建模来解决，将物品建模为更高层次的状态抽象，像素为更低层次的细节；将属性变化建模为更高层次的临时抽象，基本行动为更低层次的细节。这种抽象通过使特定未来状态更容易预测来简化转移动态。我们利用这一点提出了一种完全基于模型的算法，该算法学习具有区分性的世界模型，仅使用基于计数的内在奖励来有效地进行探索，并且可以随后达到任何发现的（抽象）状态。我们证明了该模型能够（i）有效地解决单一任务，（ii）在不同物品类型和环境中实现零启动和少量启动迁移，（iii）进行长期规划。在一系列2D工艺和MiniHack环境中，我们通过实证研究证明，我们的模型显著优于最新的低层次方法（没有抽象），以及使用相同抽象的高性能无模型模型和基于模型的模型方法。最后，我们展示了如何通过强化学习来学习低层次的对象扰动策略，以及通过监督学习来学习对象映射本身。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11816v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>强化学习在解决困难探索问题时，通过使用对象为中心的映射（描述一系列物品及其属性）可以提高学习效率。研究发现，通过层级建模，将物品置于状态抽象的高层，将属性变化置于时间抽象的高层（原始动作），可以更好地解决此问题。这种抽象简化了状态转移的动态过程，使特定未来状态更容易预测。在此基础上，提出了一种全模型算法，该算法可以学习判别世界模型，仅通过基于计数的内在奖励进行高效探索规划，并可以规划到达任何发现的（抽象）状态。模型能够在（i）解决单一任务，（ii）在不同物品类型和环境中实现零起点和少量起点转移，（iii）进行长期规划方面表现出色。在多个二维制作和MiniHack环境中进行的实证研究表明，该模型显著优于最先进的低层次方法（无抽象），以及使用相同抽象的模型自由和基于模型的方法。最后，展示了如何通过强化学习学习低层次的对象扰动策略以及通过监督学习学习对象映射本身。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习中面对困难探索问题时，使用对象为中心的映射能提高学习效率。</li>
<li>层级建模是解决此问题的最佳途径，将物品和属性变化置于不同的抽象层级。</li>
<li>该模型通过简化状态转移动态，使预测未来状态更为容易。</li>
<li>提出了一种全模型算法，能学习判别世界模型并高效探索规划。</li>
<li>模型在解决单一任务、跨物品和环境转移以及长期规划方面表现出色。</li>
<li>该模型在多个环境中显著优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb7d6decba96352656ccde6db12f55b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b850ff89f8fcbba9e5dd77201b9dab4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccb6b4ca67443f73b7a6fb458cf6c260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea10df4a042e5f4cc09da91d391c4ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b7525703114d9e89a7d217c862b4504.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01ce23a5ca6d3d3889891fca53d0d3f5.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-16  Anchor Token Matching Implicit Structure Locking for Training-free AR   Image Editing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-69ef5a21e4c999ebe06305e103031cd5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-16  GUI-R1  A Generalist R1-Style Vision-Language Action Model For GUI   Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17259.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
