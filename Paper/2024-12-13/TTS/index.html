<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2024-12-13  CSSinger End-to-End Chunkwise Streaming Singing Voice Synthesis System   Based on Conditional Variational Autoencoder">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9d2a965cd2de60999b72ce640f89da05.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-13-更新"><a href="#2024-12-13-更新" class="headerlink" title="2024-12-13 更新"></a>2024-12-13 更新</h1><h2 id="CSSinger-End-to-End-Chunkwise-Streaming-Singing-Voice-Synthesis-System-Based-on-Conditional-Variational-Autoencoder"><a href="#CSSinger-End-to-End-Chunkwise-Streaming-Singing-Voice-Synthesis-System-Based-on-Conditional-Variational-Autoencoder" class="headerlink" title="CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System   Based on Conditional Variational Autoencoder"></a>CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System   Based on Conditional Variational Autoencoder</h2><p><strong>Authors:Jianwei Cui, Yu Gu, Shihao Chen, Jie Zhang, Liping Chen, Lirong Dai</strong></p>
<p>Singing Voice Synthesis (SVS) {aims} to generate singing voices {of high} fidelity and expressiveness. {Conventional SVS systems usually utilize} an acoustic model to transform a music score into acoustic features, {followed by a vocoder to reconstruct the} singing voice. It was recently shown that end-to-end modeling is effective in the fields of SVS and Text to Speech (TTS). In this work, we thus present a fully end-to-end SVS method together with a chunkwise streaming inference to address the latency issue for practical usages. Note that this is the first attempt to fully implement end-to-end streaming audio synthesis using latent representations in VAE. We have made specific improvements to enhance the performance of streaming SVS using latent representations. Experimental results demonstrate that the proposed method achieves synthesized audio with high expressiveness and pitch accuracy in both streaming SVS and TTS tasks. </p>
<blockquote>
<p>歌唱声音合成（SVS）旨在生成高保真和富有表现力的歌唱声音。传统的SVS系统通常使用声学模型将乐谱转换为声学特征，随后由声码器重建歌唱声音。最近的研究表明，端到端建模在SVS和文本到语音（TTS）领域是有效的。因此，我们在这项工作中提出了一种完全端到端的SVS方法，并采用了分段流式推理来解决实际使用中的延迟问题。需要注意的是，这是首次尝试使用VAE中的潜在表示来完全实现端到端流式音频合成。我们对使用潜在表示提高流式SVS的性能进行了特定的改进。实验结果表明，该方法在流式SVS和TTS任务中合成的音频具有高度的表现力和音高准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08918v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>总结</strong></p>
<p>针对SVS（Singing Voice Synthesis，歌声合成技术），本文提出了一种全新的端到端歌声合成方法，并引入了分块流式推理技术以解决实际应用中的延迟问题。该方法是首次尝试使用VAE中的潜在表示进行完全端到端的流式音频合成。研究团队针对使用潜在表示的流式SVS性能进行了特定改进。实验结果表明，无论是在流式SVS还是语音合成任务中，此方法合成的音频都能表现出较高的表现力和音准准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本研究提出了一种全新的端到端歌声合成方法。</li>
<li>引入分块流式推理技术以解决实际应用中的延迟问题。</li>
<li>这是首次尝试在VAE中使用潜在表示进行完全端到端的流式音频合成。</li>
<li>研究团队对使用潜在表示的流式SVS性能进行了特定改进。</li>
<li>实验结果证明了该方法在合成音频的表达力和音准准确性上表现优异。</li>
<li>此方法适用于流式SVS和语音合成（TTS）任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个基于条件变分自编码器（Conditional Variational Autoencoder, VAE）的端到端分块流式（chunkwise streaming）歌唱语音合成系统（ChunkStreamSinger, CSSinger），旨在解决以下问题：

<ol>
<li><p><strong>实时性和计算资源限制</strong>：传统的歌唱语音合成（Singing Voice Synthesis, SVS）系统和文本到语音（Text to Speech, TTS）系统通常包含一个声学模型和一个声码器，这些系统在处理长序列时计算量大，实时性能不佳，尤其是在边缘设备和在线网络服务中部署时。</p>
</li>
<li><p><strong>端到端流式音频合成</strong>：这是首次尝试完全实现使用VAE中的潜在表示进行端到端流式音频合成。流式合成对于减少延迟和提高实时性至关重要。</p>
</li>
<li><p><strong>潜在表示用于声码器</strong>：使用VAE的潜在表示作为声码器的输入并不直观，尤其是对于歌唱语音，直接使用潜在表示进行音频生成会导致合成音频质量下降。</p>
</li>
<li><p><strong>提高流式SVS性能</strong>：论文中提出了特定的改进措施，以增强使用潜在表示的流式SVS性能。</p>
</li>
</ol>
<p>综上所述，论文的主要贡献在于提出了一种新的端到端流式歌唱语音合成方法，该方法不仅能够生成高表达性和音高准确性的合成音频，而且相较于传统的平行计算模型，能够显著降低延迟，提高实时性，更好地适应边缘设备和在线服务等实际应用场景。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 相关研究主要涉及以下几个领域：

<ol>
<li><p><strong>Singing Voice Synthesis (SVS)</strong>: 论文提到了多个基于深度学习的SVS系统，例如XiaoiceSing、ByteSing、VISinger和VISinger2。这些系统利用深度神经网络生成基于乐谱的高质量歌唱语音。</p>
</li>
<li><p><strong>Text-to-Speech (TTS)</strong>: 论文中提到了TTS系统中使用的端到端建模方法，如VITS (Kim, Kong, and Son 2021) 和SiFiSinger (Cui et al. 2024)，它们基于条件VAE框架进行声学建模和音频合成。</p>
</li>
<li><p><strong>Conditional Variational Autoencoder (VAE)</strong>: VAE作为一种深度生成模型，在语音合成领域有广泛应用，如VITS、VISinger和SiFiSinger等系统，它们利用VAE学习潜在空间表示，以提高合成音频的自然度和真实感。</p>
</li>
<li><p><strong>Streaming and Autoregressive Modeling</strong>: 论文中提到了一些研究工作，它们关注于流式自回归建模，例如Tacotron2和WaveNet。这些模型分别用于声学建模和波形生成，但存在推理效率低下和生成速度慢的问题。</p>
</li>
<li><p><strong>Neural Audio Synthesis</strong>: 论文中提到了HiFi-GAN，这是一个用于高保真语音合成的生成对抗网络，它在CSSinger系统中被用于流式音频合成。</p>
</li>
<li><p><strong>Efficient Neural Audio Synthesis</strong>: 论文中提到了WaveRNN，这是一个用于实时应用的神经音频合成模型，它通过减小模型大小和权重稀疏化来加速生成过程。</p>
</li>
</ol>
<p>这些相关研究为CSSinger系统的提出提供了理论基础和技术背景，CSSinger系统在这些研究的基础上，通过引入流式处理和潜在表示的改进，旨在实现更高效、更自然的歌唱语音合成。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为ChunkStreamSinger（CSSinger）的系统来解决上述问题，具体解决方案包括以下几个关键点：

<ol>
<li><p><strong>端到端条件VAE框架</strong>：</p>
<ul>
<li>使用条件变分自编码器（VAE）来学习音乐评分（如歌词、音高、时长）的潜在表示，并直接生成合成音频。</li>
<li>该框架包括一个先验编码器（Prior Encoder）和一个后验编码器（Posterior Encoder），用于学习潜在空间的表示，并从这些表示中直接生成合成音频。</li>
</ul>
</li>
<li><p><strong>分块流式推理（Chunkwise Streaming Inference）</strong>：</p>
<ul>
<li>采用分块方式处理音频合成任务，降低实时处理的延迟，并减少计算资源消耗。</li>
<li>在声码器（vocoder）中使用因果转置卷积（causal transposed convolutions）来生成音频，避免了传统填充方法导致的音质下降问题。</li>
</ul>
</li>
<li><p><strong>自然填充（Natural Padding）策略</strong>：</p>
<ul>
<li>为了避免在因果卷积中使用传统常数填充（如零填充或复制填充）导致的音质问题，论文提出了自然填充策略。</li>
<li>该策略通过在训练过程中动态地使用实际的潜在表示作为填充，而不是固定的常数值，以提高音频合成质量。</li>
</ul>
</li>
<li><p><strong>ChunkStream Decoder</strong>：</p>
<ul>
<li>为了实现完全的流式推理，提出了ChunkStream Decoder，它将潜在表示的生成转换为基于块的流式处理。</li>
<li>该解码器借鉴了Emformer模型的机制，将完整的输入特征向量分解为多个固定长度的块，并在各个块之间提供上下文信息，确保注意力机制可以在每个块上并行计算。</li>
</ul>
</li>
<li><p><strong>因果平滑层（Causal Smooth Layer）</strong>：</p>
<ul>
<li>为了减少块之间的边界效应，引入了因果平滑层，该层由一维因果卷积和LayerNorm层组成。</li>
<li>这层保留了前一个块的相同层级的特征，并为下一个块提供特征，以增强合成音频的自然度和流畅性。</li>
</ul>
</li>
<li><p><strong>实验验证</strong>：</p>
<ul>
<li>在多个中文歌唱语音数据集和一个TTS数据集上进行了实验，验证了所提出方法在主观和客观评价指标上相较于现有系统的优越性。</li>
<li>展示了CSSinger-FS在MOS评分和客观评价指标上的优势，并在不同硬件配置下评估了模型的延迟和实时性。</li>
</ul>
</li>
</ol>
<p>通过这些技术方案，论文成功地实现了一个低延迟、高效率的端到端流式歌唱语音合成系统，该系统不仅提高了合成音频的质量，还降低了实时应用中的延迟，使其更适合于边缘设备和在线服务等资源受限环境。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来评估所提出的ChunkStreamSinger（CSSinger）系统的性能，包括主观和客观的评估。以下是实验的详细内容：

<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li><strong>Opencpop2</strong>：一个公开的高质量中文歌唱语音数据集，包含100首普通话歌曲，由专业女歌手演唱，包含3,756个话语，总时长5.2小时。</li>
<li><strong>PopCS</strong>：另一个中文普通话歌唱语音数据集，包含117首歌曲，总时长约5.89小时，由专业女歌手演唱。</li>
<li><strong>Baker3</strong>：一个中文女性语音数据集，包含约12小时的标准普通话女性语音，包含10,000个句子，平均每个句子16个字符。</li>
</ul>
<h3 id="比较方法"><a href="#比较方法" class="headerlink" title="比较方法"></a>比较方法</h3><ul>
<li><strong>Recording</strong>：使用真实的歌唱语音音频作为基准。</li>
<li><strong>SiFiSinger</strong>：一个完全并行推理的基线系统。</li>
<li><strong>CSSinger-SS</strong>：CSSinger的半流式方法，只有HiFiGAN声码器以分块流式方式运行。</li>
<li><strong>CSSinger-SS-NP</strong>：CSSinger-SS加上自然填充（Natural Padding）的半流式结构。</li>
<li><strong>CSSingerFS</strong>（提出的模型）：CSSinger的全流式框架，使用ChunkStream Decoder实现潜在表示z的流式推理。</li>
</ul>
<h3 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h3><ul>
<li>使用44.1KHz采样的音频处理Opencpop和PopCS数据集，而Baker数据集的音频采样率为48KHz，被重新采样到16KHz。</li>
<li>模型隐藏层大小设置为192，FFN隐藏层的通道数为768。</li>
<li>Acoustic Model Decoder和ChunkStream Decoder都有4个注意力层。</li>
<li>使用diffsptk4提取80维melcepstrum（mcep）作为声学特征。</li>
<li>所有实验中HiFi-GAN生成器的训练随机切片大小设置为20，CSSinger模型的块大小也为20。</li>
</ul>
<h3 id="主要结果与分析"><a href="#主要结果与分析" class="headerlink" title="主要结果与分析"></a>主要结果与分析</h3><ul>
<li><strong>主观评估（MOS测试）</strong>：在Opencpop和PopCS数据集上进行，随机选择20个样本，邀请20名母语人士进行主观评估。</li>
<li><strong>客观评估</strong>：计算合成音频与真实音频之间的基频均方根误差（F0 RMSE）、基频相关系数（F0 Corr）、无声&#x2F;有声错误率（U&#x2F;UV Err）、Mel-cepstrum均方误差（MSE）和Mel-cepstral失真（MCD）。</li>
</ul>
<h3 id="延迟评估"><a href="#延迟评估" class="headerlink" title="延迟评估"></a>延迟评估</h3><ul>
<li>测量从输入特征到合成音频生成的延迟（秒）、从输入特征到音频处理完成的总处理时间（秒）和实时因子（RTF）作为系统效率的指标。</li>
<li>在GPU、CPU和单核单线程CPU限制（CPU-Limited）三种情况下测试延迟指标。</li>
</ul>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>CSSinger-FS在多个客观和主观指标上表现出色，优于现有的并行和半流式系统。</li>
<li>在GPU和CPU硬件配置下，CSSinger-FS显示出较低的延迟和优越的实时推理效率，这对于高实时需求的应用和计算资源受限的场景非常重要。</li>
</ul>
<p>这些实验全面评估了CSSinger系统的性能，证明了其在歌唱语音合成任务中的有效性和优越性。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 尽管论文提出的ChunkStreamSinger（CSSinger）系统在歌唱语音合成方面取得了显著的性能提升，但仍有一些领域可以进一步探索和改进：

<ol>
<li><p><strong>模型泛化能力</strong>：</p>
<ul>
<li>在更多种类和风格的歌唱数据集上测试模型的泛化能力，包括不同语言、不同性别和不同演唱技巧的数据集。</li>
</ul>
</li>
<li><p><strong>更深入的客观评估</strong>：</p>
<ul>
<li>开发更精细的客观评价指标，以更全面地评估合成音频的自然度、清晰度和音乐性等。</li>
</ul>
</li>
<li><p><strong>实时性能优化</strong>：</p>
<ul>
<li>进一步优化模型以降低延迟和计算资源消耗，特别是在资源受限的设备上，如移动设备或嵌入式系统。</li>
</ul>
</li>
<li><p><strong>多风格和多歌手建模</strong>：</p>
<ul>
<li>探索如何使模型能够处理多个歌手和不同演唱风格，提供更丰富的合成选项。</li>
</ul>
</li>
<li><p><strong>端到端训练的改进</strong>：</p>
<ul>
<li>研究如何改进端到端训练过程，可能包括更好的损失函数、优化算法或正则化技术，以进一步提高合成质量。</li>
</ul>
</li>
<li><p><strong>声码器的改进</strong>：</p>
<ul>
<li>研究新型声码器或改进现有声码器，以生成更高保真度和更自然的音频。</li>
</ul>
</li>
<li><p><strong>上下文感知建模</strong>：</p>
<ul>
<li>提高模型对歌曲上下文的感知能力，例如，理解和模拟歌曲的情绪和节奏变化。</li>
</ul>
</li>
<li><p><strong>交互式应用</strong>：</p>
<ul>
<li>探索CSSinger在交互式应用中的潜力，如卡拉OK系统或音乐教育软件，其中实时反馈和调整是必要的。</li>
</ul>
</li>
<li><p><strong>模型解释性</strong>：</p>
<ul>
<li>提高模型的可解释性，帮助研究人员和开发者更好地理解模型的决策过程和潜在空间的结构。</li>
</ul>
</li>
<li><p><strong>多任务学习</strong>：</p>
<ul>
<li>考虑将歌唱语音合成与其他任务（如音乐生成、节奏识别）结合起来，以创建更全面的音乐生成系统。</li>
</ul>
</li>
<li><p><strong>模型鲁棒性测试</strong>：</p>
<ul>
<li>对模型进行鲁棒性测试，尤其是在面对不完整或噪声数据时的性能。</li>
</ul>
</li>
<li><p><strong>跨领域应用</strong>：</p>
<ul>
<li>探索CSSinger在其他领域的应用，如语音编码、语音增强或语音转换。</li>
</ul>
</li>
</ol>
<p>这些探索方向不仅可以推动歌唱语音合成技术的发展，还可能为整个语音合成和音频处理领域带来新的突破。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个基于条件变分自编码器（Conditional Variational Autoencoder, VAE）的端到端分块流式（chunkwise streaming）歌唱语音合成系统ChunkStreamSinger（CSSinger）。以下是论文的主要内容总结：

<ol>
<li><p><strong>问题陈述</strong>：</p>
<ul>
<li>论文旨在解决传统歌唱语音合成（SVS）系统中存在的实时性和计算资源限制问题，特别是在边缘设备和在线网络服务中部署时的高延迟问题。</li>
</ul>
</li>
<li><p><strong>系统架构</strong>：</p>
<ul>
<li>CSSinger基于SiFiSinger构建，采用条件VAE框架，包含先验编码器和后验编码器，直接从音乐评分生成合成音频。</li>
<li>系统支持分块流式推理，以减少延迟并提高实时性。</li>
</ul>
</li>
<li><p><strong>关键技术</strong>：</p>
<ul>
<li><strong>自然填充（Natural Padding）</strong>：解决VAE潜在表示用于声码器输入时的音质下降问题。</li>
<li><strong>ChunkStream Decoder</strong>：实现完全流式推理，将潜在表示的生成转换为基于块的流式处理。</li>
<li><strong>因果平滑层（Causal Smooth Layer）</strong>：减少块之间的边界效应，增强合成音频的自然度和流畅性。</li>
</ul>
</li>
<li><p><strong>实验验证</strong>：</p>
<ul>
<li>在两个中文歌唱语音数据集和一个文本到语音（TTS）数据集上进行了实验，验证了CSSinger系统在主观和客观评价指标上相较于现有系统的优越性。</li>
<li>展示了CSSinger在不同硬件配置下（GPU和CPU）的低延迟和优越的实时推理效率。</li>
</ul>
</li>
<li><p><strong>结论</strong>：</p>
<ul>
<li>CSSinger-FS（全流式框架）在MOS评分和客观评价指标上优于现有的并行和半流式系统。</li>
<li>该系统不仅提高了合成音频的质量，还降低了实时应用中的延迟，使其更适合于边缘设备和在线服务等资源受限环境。</li>
</ul>
</li>
<li><p><strong>未来工作</strong>：</p>
<ul>
<li>论文提出了一些可以进一步探索的方向，包括模型泛化能力的提升、实时性能的进一步优化、多风格和多歌手建模等。</li>
</ul>
</li>
</ol>
<p>总体而言，这篇论文提出了一个创新的端到端流式歌唱语音合成系统，通过引入先进的技术方案，有效地提高了合成音频的质量和实时性，为SVS领域的发展提供了新的方向。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System Based on Conditional Variational Autoencoder，链接是 https://arxiv.org/pdf/2412.08918 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.08918 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8eab5d04234da8c9132ad70e77ac227e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd6ec655dffedb8217a0036c312b35fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6c931b350f7c2a12489cb7dee75028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b2ab67bc4a7feea7de1b1bcbefc1d25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761e8346bb886976fd93d7c51cc904bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a7dbd5b98e050ba9eca4d27d4cbd65b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23a41de53e1b9ee13dddda8ce665c227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b7199fa13af85804dc4f522e51bad7a.jpg" align="middle">
</details>



<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>基于大模型的文本转语音（TTS）系统对数据有着极高的需求。近期基于大模型的TTS工作通常采用复杂的数据处理流程来获得高质量的训练数据。这些复杂流程需要在每个阶段都使用出色的模型（例如语音降噪、语音增强、说话人识别和标点模型），而这些模型本身也需要高质量的训练数据且很少开源。即使采用最先进的技术模型，仍然存在一些问题，例如背景噪声去除不完全、标点与实际语音停顿之间的不匹配等。此外，严格的过滤策略往往只会保留原始数据的10-30%，极大地阻碍了数据扩展工作。</p>
</blockquote>
<p>在此工作中，我们利用噪声鲁棒的音频分词器（S3Tokenizer）设计了一个简化而有效的TTS数据处理流程，该流程在保持数据质量的同时，大大降低了数据获取成本，实现了超过50%的数据保留率。除了数据扩展挑战之外，基于大模型的TTS系统的部署成本也高于传统方法。当前系统通常仅将大模型用于文本到代币的生成，而需要额外的模型（如流程匹配模型）来进行代币到波形生成，这些无法由大模型推理引擎直接执行，进一步增加了部署的复杂性。为了解决这些挑战，我们消除了大模型和流程组件中的冗余模块，并用大模型架构替代流程模型的主干。基于这种简化的流程主干，我们提出了用于流式和非流式推理的统一架构，大大降低了部署成本。最后，我们探索了使用同一数据进行TTS和语音识别（ASR）任务训练的可行性，这得益于简化的流程和S3Tokenizer降低了对TTS训练数据的质量要求。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v2">PDF</a> Technical Report</p>
<p><strong>摘要</strong></p>
<p>LLM模型依赖大规模数据。近期的LLM-based TTS工作中多采用复杂的数据处理流程以获取高质量的训练数据。这些流程需要各阶段都有优秀的模型（如语音降噪、增强、说话人识别和标点模型），而这些模型也需要高质量的训练数据且很少开源。尽管有最先进的模型，仍存在背景噪声去除不完全和标点与实际语音停顿不匹配的问题。本文利用噪声鲁棒的音频分词器（S3Tokenizer）设计简化有效的TTS数据处理流程，在保持数据质量的同时降低数据获取成本，实现超过50%的数据保留率。此外，LLM-based TTS系统在部署成本上相较于传统方法更高。当前系统通常仅使用LLM进行文本到标记生成，需要单独的模型（如流匹配模型）进行标记到波形生成，不能由LLM推理引擎直接执行，部署过程更为复杂。为应对这些挑战，我们简化了LLM和流组件中的冗余模块，用LLM架构替代流模型主干。在此简化流主干的基础上，我们提出用于流式和非流式推理的统一架构，显著降低部署成本。最后，我们探索了使用简化管道和S3Tokenizer统一TTS和ASR任务的可行性，降低了对TTS训练数据的质量要求。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM模型对数据需求量大，需要复杂的数据处理流程获取高质量训练数据。</li>
<li>当前数据处理流程中的模型需要大量高质量训练数据且很少开源。</li>
<li>尽管有先进的模型，仍存在背景噪声去除不完全和标点与语音停顿不匹配的问题。</li>
<li>利用噪声鲁棒的音频分词器（S3Tokenizer）设计简化有效的TTS数据处理流程，提高数据保留率。</li>
<li>LLM-based TTS系统在部署成本上较高，需要简化架构以降低部署复杂性。</li>
<li>提出统一架构用于流式和非流式推理，降低部署成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的主要问题包括：

<ol>
<li><p><strong>数据扩展和部署效率的挑战</strong>：大型语言模型（LLM）基础的文本到语音（TTS）系统在数据扩展和部署效率方面面临新的挑战。现有的TTS系统或大规模TTS数据集通常需要构建复杂的数据处理流程，将大量原始音频转换为高质量的TTS训练数据，但这些流程后只有10%到30%的数据可用于训练，导致数据扩展成为一个“烧钱游戏”。此外，即使经过精细的处理，也无法保证每个阶段不出现错误案例。</p>
</li>
<li><p><strong>部署成本高</strong>：当前的LLM基础TTS系统通常需要将文本到令牌模型（例如流匹配模型）串联起来生成波形。这些串联的模型不仅不能直接由LLM推理引擎执行，增加了部署成本，同时也使得流式推理变得更加困难。</p>
</li>
<li><p><strong>数据质量要求高</strong>：传统的TTS系统需要高质量的训练数据，而本文提出的解决方案旨在降低对TTS训练数据质量的要求，使得可以使用“脏数据”来克服数据扩展的挑战。</p>
</li>
<li><p><strong>模型部署复杂性</strong>：由于LLM推理引擎无法重用特殊的U-Net结构，这增加了部署成本，并且使得流式推理更加困难。</p>
</li>
</ol>
<p>为了解决这些问题，论文提出了一个简化的TTS数据处理流程，并基于此设计了一个面向部署的TTS架构，旨在降低数据获取成本，提高数据保留率，并简化部署过程。此外，论文还探索了使用相同的数据训练TTS和自动语音识别（ASR）任务的可行性。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是一些与本研究相关的工作：

<ol>
<li><p><strong>CosyVoice</strong> [1]：这是一个由LLM用于文本到令牌生成和条件流匹配模型用于令牌到语音合成的系统。本文扩展了CosyVoice，通过简化数据处理流程和模型架构，减少了数据获取和部署成本，并探索了统一流式和非流式推理的能力，以及使用相同数据训练TTS和ASR任务的可行性。</p>
</li>
<li><p><strong>FishSpeech</strong> [7]：利用大型语言模型进行高级多语言文本到语音合成的研究。</p>
</li>
<li><p><strong>FireRedTTS</strong> [8]：一个工业级生成性语音应用的基础文本到语音框架。</p>
</li>
<li><p><strong>XimalayaTTS</strong> [9]：一个高质量的零拍摄语音生成模型。</p>
</li>
<li><p><strong>WenetSpeech4TTS</strong> [10]：一个普通话TTS语料库，用于大型语音生成模型的基准测试。</p>
</li>
<li><p><strong>Emilia</strong> [11]：一个广泛、多语言、多样化的语音数据集，用于大规模语音生成。</p>
</li>
<li><p><strong>AutoPrep</strong> [12]：一个用于野外语音数据的自动预处理框架。</p>
</li>
<li><p><strong>Matcha-TTS</strong> [2]：一个快速的TTS架构，具有条件流匹配。</p>
</li>
<li><p><strong>Qwen2</strong> [3]：一个用于因果语言模型的LLM架构，本文中用作LLM和流模型的骨干网络。</p>
</li>
<li><p><strong>TensorRT-LLM</strong> [4] 和 <strong>vLLM</strong> [5]：用于LLM推理的库，本文旨在使模型能够直接在这些引擎上执行。</p>
</li>
</ol>
<p>这些相关工作涵盖了TTS领域的多个方面，包括数据处理、模型架构、多语言支持、零拍摄生成以及推理优化。论文通过结合和扩展这些相关工作，提出了一个简化的TTS框架，旨在提高数据扩展性和部署效率。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过以下几个关键贡献来解决提出的问题：

<h3 id="简化数据处理流程"><a href="#简化数据处理流程" class="headerlink" title="简化数据处理流程"></a>简化数据处理流程</h3><ol>
<li><p><strong>去除复杂模块</strong>：论文提出了一个高度简化的数据处理流程，去除了传统的语音处理中的噪声降低、语音增强、说话人分离和标点模块。</p>
</li>
<li><p><strong>使用S3Tokenizer</strong>：利用S3Tokenizer的鲁棒性，该流程减少了对TTS训练数据质量的要求，提高了数据保留率，首次实现了超过50%的数据保留率。</p>
</li>
<li><p><strong>Copilot-ASR交叉验证策略</strong>：通过使用两个不同的ASR模型进行二次转录，并基于两次转录结果的一致性来过滤数据，从而确保数据量和质量。</p>
</li>
</ol>
<h3 id="简化TTS架构"><a href="#简化TTS架构" class="headerlink" title="简化TTS架构"></a>简化TTS架构</h3><ol>
<li><p><strong>简化前端</strong>：对于中文使用字符单元，对于英文保持BPE分词，减少插入和删除错误。</p>
</li>
<li><p><strong>Qwen骨干网络</strong>：用Qwen模型替换LLM和流模型中的文本编码器和令牌编码器，简化模型结构，并使其能够直接在TensorRT和vLLM等推理引擎上执行。</p>
</li>
<li><p><strong>统一流式和非流式策略</strong>：通过控制输入的注意力掩码，实现了一个支持流式和非流式场景的统一流模型。</p>
</li>
</ol>
<h3 id="统一TTS和ASR任务"><a href="#统一TTS和ASR任务" class="headerlink" title="统一TTS和ASR任务"></a>统一TTS和ASR任务</h3><ol>
<li><strong>使用相同的数据训练</strong>：基于简化的数据处理流程和S3Tokenizer减少了TTS训练数据的质量要求，论文探索了使用相同的数据训练TTS和ASR任务的可行性。</li>
</ol>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><ol>
<li><p><strong>大规模数据集</strong>：论文在百万小时级别的数据上验证了简化数据处理流程的有效性。</p>
</li>
<li><p><strong>模型比较</strong>：与现有TTS系统进行比较，展示了简化模型架构的性能。</p>
</li>
<li><p><strong>流式推理分析</strong>：分析了不同流式配置下的性能，验证了动态块策略的有效性。</p>
</li>
<li><p><strong>统一TTS和ASR模型评估</strong>：评估了使用连续特征和离散标记进行ASR任务的性能差异。</p>
</li>
</ol>
<p>通过上述方法，论文旨在降低LLM基础TTS系统的数据获取成本，简化部署过程，并探索TTS和ASR任务的统一训练，以提高整体效率和效果。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来验证所提出方法的有效性，具体实验包括：

<ol>
<li><p><strong>前端比较实验</strong>：</p>
<ul>
<li>对比了中文文本处理中使用字（character-based）单元和BPE（byte pair encoding）单元的效果。</li>
<li>通过控制变量，仅替换LLM的backbone为Qwen2ForCausalLM，并在1M小时数据上训练一个epoch来比较不同建模方法的效果。</li>
</ul>
</li>
<li><p><strong>模型比较实验</strong>：</p>
<ul>
<li>使用TouchLLM-0.5B模型与其他TTS系统进行比较。</li>
<li>比较了仅使用说话人嵌入（NoPrompt）和使用提示文本及提示波形（ZeroShot）两种解码方法的效果。</li>
</ul>
</li>
<li><p><strong>流式推理结果分析</strong>：</p>
<ul>
<li>对比了流式和非流式性能，特别是对于不同模型大小（TouchFlow-170M和TouchFlow-50M）的影响。</li>
<li>使用动态块策略训练的Qwen流模型，并在不同流式配置下进行比较。</li>
</ul>
</li>
<li><p><strong>推理基准测试</strong>：</p>
<ul>
<li>对LLM（TouchLLM-0.5B）和流（TouchFlow-170M）模块进行了性能测试，测试了FP16和FP32精度下的处理速度。</li>
<li>分析了首块（First Chunk）生成的延迟。</li>
</ul>
</li>
<li><p><strong>统一TTS &amp; ASR评估</strong>：</p>
<ul>
<li>比较了独立的TTS模型和统一的TTS&#x2F;ASR模型在TTS任务上的性能。</li>
<li>比较了在统一模型中，使用连续特征和离散标记进行ASR任务的性能差异。</li>
</ul>
</li>
</ol>
<p>这些实验覆盖了前端处理、模型性能、流式推理、推理效率和统一架构评估等多个方面，全面验证了论文提出方法的有效性和实用性。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 论文在最后部分提出了一些未来可能的研究方向，以下是一些可以进一步探索的点：

<ol>
<li><p><strong>细粒度的风格控制</strong>：</p>
<ul>
<li>情绪、呼吸、停顿等的精细控制尚未在模型中实现，未来可以通过对预训练模型进行微调来实现更细致的风格控制。</li>
</ul>
</li>
<li><p><strong>文本指令生成</strong>：</p>
<ul>
<li>根据文本指令生成语音的能力，例如，根据指令“用悲伤的语气说这句话”，模型能够产生具有相应情感色彩的语音输出。</li>
</ul>
</li>
<li><p><strong>端到端语音交互</strong>：</p>
<ul>
<li>类似于gpt-4o的研究，实现端到端的语音交互系统，这可能包括语音识别、理解和生成等能力。</li>
</ul>
</li>
<li><p><strong>数据预处理和增强</strong>：</p>
<ul>
<li>探索更高效的数据预处理和增强技术，以进一步提高模型的鲁棒性和泛化能力。</li>
</ul>
</li>
<li><p><strong>模型压缩和优化</strong>：</p>
<ul>
<li>研究模型压缩和优化技术，以减少模型的计算需求，使其更适合在资源受限的设备上部署。</li>
</ul>
</li>
<li><p><strong>多语言和跨语言能力</strong>：</p>
<ul>
<li>扩展模型的多语言能力，特别是对于低资源语言，提高跨语言的语音合成和识别能力。</li>
</ul>
</li>
<li><p><strong>提高数据利用效率</strong>：</p>
<ul>
<li>研究如何更有效地利用未标注数据，通过自监督学习等方法提高数据利用效率。</li>
</ul>
</li>
<li><p><strong>提高流式推理的效率和效果</strong>：</p>
<ul>
<li>进一步优化流式推理策略，减少延迟，提高语音合成的自然度和流畅性。</li>
</ul>
</li>
<li><p><strong>统一TTS和ASR任务的深入研究</strong>：</p>
<ul>
<li>更深入地研究如何有效地使用相同的数据训练TTS和ASR任务，并探索这种统一训练方法的优势和局限性。</li>
</ul>
</li>
<li><p><strong>可解释性和安全性</strong>：</p>
<ul>
<li>提高模型的可解释性，确保生成语音的安全性和合规性，避免生成有害或不当的内容。</li>
</ul>
</li>
</ol>
<p>这些方向能够帮助研究者进一步提升TTS系统的性能，扩展其应用范围，并解决实际部署中遇到的问题。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个名为TouchTTS的简化文本到语音（TTS）框架，旨在解决LLM（大型语言模型）基础TTS系统在数据扩展和部署效率方面面临的挑战。以下是论文的主要内容总结：

<h3 id="1-问题识别"><a href="#1-问题识别" class="headerlink" title="1. 问题识别"></a>1. 问题识别</h3><ul>
<li>LLM基础系统需要大量训练数据，且现有系统采用复杂的数据处理流程，导致数据保留率低，增加数据获取成本。</li>
<li>部署成本高，现有系统使用LLM仅进行文本到令牌生成，需要额外模型进行令牌到波形生成，增加部署复杂性。</li>
</ul>
<h3 id="2-简化数据处理流程"><a href="#2-简化数据处理流程" class="headerlink" title="2. 简化数据处理流程"></a>2. 简化数据处理流程</h3><ul>
<li>提出简化的数据处理流程，去除噪声降低、语音增强、说话人分离和标点模块。</li>
<li>使用S3Tokenizer提高对“脏数据”的鲁棒性，减少对高质量训练数据的需求。</li>
<li>采用Copilot-ASR交叉验证策略确保数据质量，实现超过50%的数据保留率。</li>
</ul>
<h3 id="3-简化TTS架构"><a href="#3-简化TTS架构" class="headerlink" title="3. 简化TTS架构"></a>3. 简化TTS架构</h3><ul>
<li>提出面向部署的架构，简化前端处理，使用字符单元处理中文，BPE处理英文。</li>
<li>使用Qwen作为LLM和流模型的骨干网络，去除冗余模块，简化模型结构。</li>
<li>实现统一的流式和非流式推理策略，通过控制注意力掩码实现。</li>
</ul>
<h3 id="4-统一TTS和ASR任务"><a href="#4-统一TTS和ASR任务" class="headerlink" title="4. 统一TTS和ASR任务"></a>4. 统一TTS和ASR任务</h3><ul>
<li>探索使用相同数据训练TTS和ASR任务的可行性，减少数据准备的复杂性。</li>
</ul>
<h3 id="5-实验验证"><a href="#5-实验验证" class="headerlink" title="5. 实验验证"></a>5. 实验验证</h3><ul>
<li>在百万小时级别的数据上验证了简化数据处理流程的有效性。</li>
<li>与现有TTS系统比较，展示了简化模型架构的性能。</li>
<li>分析了不同流式配置下的性能，验证了动态块策略的有效性。</li>
<li>评估了使用连续特征和离散标记进行ASR任务的性能差异。</li>
</ul>
<h3 id="6-未来工作"><a href="#6-未来工作" class="headerlink" title="6. 未来工作"></a>6. 未来工作</h3><ul>
<li>提出了未来可能的研究方向，包括细粒度的风格控制、文本指令生成、端到端语音交互等。</li>
</ul>
<p>总体而言，论文提出了一个简化的TTS框架，通过简化数据处理流程和模型架构，降低了数据获取和部署成本，提高了数据扩展性和部署效率，并探索了TTS和ASR任务的统一训练，为未来TTS技术的发展提供了新的方向。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch，链接是 https://arxiv.org/pdf/2412.08237 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.08237 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-41c149866ee8238c17609f810d804b9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>



<h2 id="CAS-GAN-for-Contrast-free-Angiography-Synthesis"><a href="#CAS-GAN-for-Contrast-free-Angiography-Synthesis" class="headerlink" title="CAS-GAN for Contrast-free Angiography Synthesis"></a>CAS-GAN for Contrast-free Angiography Synthesis</h2><p><strong>Authors:De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Hao Li, Tian-Yu Xiang, Zeng-Guang Hou</strong></p>
<p>Iodinated contrast agents are widely utilized in numerous interventional procedures, yet posing substantial health risks to patients. This paper presents CAS-GAN, a novel GAN framework that serves as a “virtual contrast agent” to synthesize X-ray angiographies via disentanglement representation learning and vessel semantic guidance, thereby reducing the reliance on iodinated contrast agents during interventional procedures. Specifically, our approach disentangles X-ray angiographies into background and vessel components, leveraging medical prior knowledge. A specialized predictor then learns to map the interrelationships between these components. Additionally, a vessel semantic-guided generator and a corresponding loss function are introduced to enhance the visual fidelity of generated images. Experimental results on the XCAD dataset demonstrate the state-of-the-art performance of our CAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising results highlight {\tt CAS-GAN}’s potential for clinical applications. </p>
<blockquote>
<p>碘化造影剂在众多介入手术中广泛应用，但给患者带来较大的健康风险。本文提出了CAS-GAN，这是一种新型的GAN框架，可作为“虚拟造影剂”通过解耦表示学习和血管语义指导来合成X射线血管造影图像，从而减少介入手术中对碘化造影剂的依赖。具体来说，我们的方法将X射线血管造影图像分解为背景和血管成分，并利用医学先验知识。然后，一个专门的预测器学习这些成分之间的相互作用。此外，还引入了血管语义引导生成器和相应的损失函数，以提高生成图像的可视保真度。在XCAD数据集上的实验结果表明，我们的CAS-GAN达到了最先进的性能，FID为5.87，MMD为0.016。这些令人鼓舞的结果突出了CAS-GAN在临床应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08490v2">PDF</a> IEEE Symposium Series on Computational Intelligence (SSCI 2025)</p>
<p><strong>Summary</strong><br>     论文提出了一种名为CAS-GAN的新型GAN框架，作为“虚拟造影剂”用于合成X射线血管造影图像。它通过解耦表示学习和血管语义引导，减少了对碘造影剂的需求。实验结果表明，CAS-GAN在XCAD数据集上达到了先进性能，具有潜在的临床应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAS-GAN框架被提出作为一种“虚拟造影剂”，用于合成X射线血管造影图像。</li>
<li>该框架通过解耦表示学习将X射线血管造影图像分解为背景与血管成分。</li>
<li>医学先验知识被用于指导这一过程，同时有一个专门的预测器来学习这些成分之间的相互关系。</li>
<li>引入了一种血管语义引导的生成器和相应的损失函数，以提高生成图像的可视真实感。</li>
<li>实验结果表明，CAS-GAN在XCAD数据集上的性能达到了先进水平，FID为5.87，MMD为0.016。</li>
<li>CAS-GAN的应用潜力在于减少碘造影剂的使用，降低患者的健康风险。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个名为CAS-GAN的新型生成对抗网络（GAN）框架，旨在解决介入性医疗程序中使用碘造影剂带来的健康风险问题。通过使用CAS-GAN，研究者们希望能够合成出无需使用碘造影剂的X射线血管造影图像，从而减少对这些可能对患者造成潜在健康风险的造影剂的依赖。

<p>具体来说，论文中提到了以下几个关键问题：</p>
<ol>
<li><p><strong>心血管疾病（CVDs）的治疗需求</strong>：心血管疾病是全球死亡的主要原因，而图像引导的血管介入手术是治疗心血管疾病的主流方法之一。</p>
</li>
<li><p><strong>碘造影剂的健康风险</strong>：在X射线血管造影中，为了使血管显影，需要使用碘基的造影剂。然而，这些造影剂存在副作用，包括可能危及生命的过敏反应，以及对肾脏的潜在损害，尤其是在已有肾脏疾病或糖尿病患者中。</p>
</li>
<li><p><strong>减少造影剂使用的需求</strong>：为了满足临床需求，同时显著减少造影剂剂量甚至不使用造影剂，同时保持成像质量，这是X射线血管造影系统必须解决的关键挑战。</p>
</li>
<li><p><strong>生成对抗网络（GANs）的应用</strong>：论文提出了利用GANs作为“虚拟造影剂”来生成逼真的X射线血管造影图像的可能性，这可能会减少对碘造影剂的依赖，并提高介入手术的安全性和效率。</p>
</li>
<li><p><strong>图像到图像的翻译挑战</strong>：将非对比X射线图像转换为X射线血管造影图像是一个具有挑战性的问题，因为它不仅需要在两个图像域之间进行风格转换，还需要精确的一对一图像映射。</p>
</li>
</ol>
<p>论文提出的CAS-GAN框架通过解耦X射线血管造影图像的背景和血管成分，并利用医学先验知识，使用神经网络学习这些成分之间的相互关系，从而解决了上述挑战。此外，还引入了血管语义引导生成器和相应的损失函数，以增强生成图像的视觉保真度。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 论文中提到了多个与CAS-GAN相关的研究工作，可以归纳为以下几个方面：

<h3 id="1-生成对抗网络（GANs）"><a href="#1-生成对抗网络（GANs）" class="headerlink" title="1. 生成对抗网络（GANs）"></a>1. 生成对抗网络（GANs）</h3><ul>
<li>GANs由Goodfellow等人在2014年引入，通过建立一个最小最大优化游戏，其中生成器G旨在模仿真实数据样本的分布，而判别器D评估它们的真伪。</li>
<li>论文中提到了一些GAN的变种，它们通过更稳定的训练目标来解决训练GANs时的挑战，比如梯度消失和模式崩溃问题。</li>
</ul>
<h3 id="2-图像到图像的翻译（Image-to-image-translation）"><a href="#2-图像到图像的翻译（Image-to-image-translation）" class="headerlink" title="2. 图像到图像的翻译（Image-to-image translation）"></a>2. 图像到图像的翻译（Image-to-image translation）</h3><ul>
<li>这项工作涉及将图像从一个域转换到另一个域，同时保持其底层结构或内容。</li>
<li>Isola等人提出了一个条件GAN框架，使用对抗损失和L1重建损失在成对数据上进行训练。</li>
<li>近期的研究集中在无成对数据的翻译上，引入了循环一致性等额外约束以保持域转换期间的语义一致性。</li>
</ul>
<h3 id="3-解耦表示学习（Disentanglement-representation-learning）"><a href="#3-解耦表示学习（Disentanglement-representation-learning）" class="headerlink" title="3. 解耦表示学习（Disentanglement representation learning）"></a>3. 解耦表示学习（Disentanglement representation learning）</h3><ul>
<li>解耦表示学习通过将图像分解为具有物理意义的成分来提高生成图像的可控性和可解释性。</li>
<li>一些工作尝试将图像分解为内容和风格表示，以实现通过交换风格表示进行图像翻译。</li>
</ul>
<h3 id="4-医学图像生成和翻译"><a href="#4-医学图像生成和翻译" class="headerlink" title="4. 医学图像生成和翻译"></a>4. 医学图像生成和翻译</h3><ul>
<li>论文中提到了几项与医学图像生成和翻译相关的研究，包括使用GANs从非对比X射线图像生成X射线血管造影图像的研究。</li>
</ul>
<h3 id="5-相关技术方法"><a href="#5-相关技术方法" class="headerlink" title="5. 相关技术方法"></a>5. 相关技术方法</h3><ul>
<li>论文还提到了一些技术方法，如注意力机制（attention mechanisms）、自监督对比损失（self-supervised contrast loss）等，这些方法被用于增强不同域之间语义关系的翻译。</li>
</ul>
<h3 id="具体论文和方法"><a href="#具体论文和方法" class="headerlink" title="具体论文和方法"></a>具体论文和方法</h3><ul>
<li>CycleGAN：用于无成对图像到图像的翻译。</li>
<li>UNIT、MUNIT：用于无监督的图像到图像的翻译网络。</li>
<li>AttentionGAN：使用注意力引导的生成对抗网络进行无成对图像到图像的翻译。</li>
<li>QS-Attn：使用查询选择注意力进行对比学习。</li>
<li>StegoGAN：用于非双射图像到图像的翻译，利用隐写术。</li>
</ul>
<p>这些相关研究为CAS-GAN提供了理论基础和技术背景，同时也展示了在医学图像翻译领域中，如何利用深度学习技术来解决实际的临床问题。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为CAS-GAN的新型生成对抗网络（GAN）框架来解决这个问题。CAS-GAN框架的核心思想是将X射线血管造影图像解耦为背景和血管成分，然后通过神经网络学习这两部分之间的相互关系，以此来生成逼真的血管造影图像。以下是该方法解决该问题的关键步骤：

<ol>
<li><p><strong>解耦表示学习（Disentanglement Representation Learning）</strong>：</p>
<ul>
<li>使用两个专门的编码器（一个背景编码器和一个血管编码器）将X射线血管造影图像分解为背景和血管成分。</li>
<li>通过一个称为“预测器”的神经网络，从背景表示中推断出血管表示。</li>
</ul>
</li>
<li><p><strong>血管语义引导生成器（Vessel Semantic-Guided Generator）</strong>：</p>
<ul>
<li>为了提高生成图像中血管的真实性，引入了两个专门的生成器：一个用于注意力掩模，一个用于上下文掩模。</li>
<li>这些生成器利用预测器提供的血管表示来生成逼真的血管结构。</li>
</ul>
</li>
<li><p><strong>血管语义引导的对抗损失（Vessel Semantic-Guided Adversarial Loss）</strong>：</p>
<ul>
<li>为了提高GAN对真实和生成血管图像之间差异的识别能力，引入了一种新的对抗损失，专注于血管的语义差异。</li>
<li>使用预训练的U-Net网络提取原始和生成血管造影图像中的血管语义图像，然后定义基于这些语义图像的对抗损失。</li>
</ul>
</li>
<li><p><strong>训练目标（Training Objective）</strong>：</p>
<ul>
<li>由于两个图像域之间的映射高度不确定，因此结合了多种有效的损失函数来训练CAS-GAN，包括预测损失、对抗损失、循环一致性损失和重建损失。</li>
<li>这些损失函数的加权和构成了CAS-GAN的训练目标，以优化图像翻译过程的各个方面。</li>
</ul>
</li>
</ol>
<p>通过这些方法，CAS-GAN能够有效地从非对比X射线图像中生成逼真的X射线血管造影图像，从而减少对碘造影剂的依赖，并提高介入手术的安全性和效率。论文中的实验结果表明，CAS-GAN在XCAD数据集上实现了最先进的性能，证明了其在临床应用中的潜力。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了多项实验来验证CAS-GAN模型的性能，包括与其他最先进方法的比较、消融研究以及外部验证。以下是实验的具体内容：

<h3 id="A-数据集（Dataset）"><a href="#A-数据集（Dataset）" class="headerlink" title="A. 数据集（Dataset）"></a>A. 数据集（Dataset）</h3><ul>
<li>使用了X射线血管造影冠状血管分割数据集（XCAD），该数据集包含非对比X射线图像和X射线血管造影图像。</li>
<li>数据集分为两个子集：一个用于图像翻译任务，另一个用于训练U-Net提取血管语义信息。</li>
<li>随机选择了621张非对比X射线图像和621张X射线血管造影图像作为测试集，其余作为训练集。</li>
</ul>
<h3 id="B-实现细节（Implementation-details）"><a href="#B-实现细节（Implementation-details）" class="headerlink" title="B. 实现细节（Implementation details）"></a>B. 实现细节（Implementation details）</h3><ul>
<li>采用了与CycleGAN相同的生成器和判别器架构，以进行公平比较。</li>
<li>预测器使用多层感知机（MLP）实现。</li>
<li>使用PyTorch、Python和Ubuntu环境，搭载NVIDIA GeForce RTX 4090 GPU进行实验。</li>
<li>使用Adam优化器进行1000个周期的训练，初始学习率设置为2e-4，并在700个周期后线性降低至零。</li>
</ul>
<h3 id="C-评估指标（Evaluation-metrics）"><a href="#C-评估指标（Evaluation-metrics）" class="headerlink" title="C. 评估指标（Evaluation metrics）"></a>C. 评估指标（Evaluation metrics）</h3><ul>
<li>使用了Fréchet Inception Distance（FID）和Maximum Mean Discrepancy（MMD）两个指标来评估生成图像的性能。</li>
</ul>
<h3 id="D-实验结果（Results）"><a href="#D-实验结果（Results）" class="headerlink" title="D. 实验结果（Results）"></a>D. 实验结果（Results）</h3><h4 id="1-与最先进方法的比较（Comparisons-with-state-of-the-arts）"><a href="#1-与最先进方法的比较（Comparisons-with-state-of-the-arts）" class="headerlink" title="1. 与最先进方法的比较（Comparisons with state-of-the-arts）"></a>1. 与最先进方法的比较（Comparisons with state-of-the-arts）</h4><ul>
<li>将CAS-GAN与其他几种领先的无成对图像到图像翻译方法进行了比较，包括CycleGAN、UNIT、MUNIT、CUT、AttentionGAN、QS-Attn和StegoGAN。</li>
<li>在XCAD数据集上的定量结果表明，CAS-GAN在FID和MMD两个指标上均优于其他基线方法。</li>
</ul>
<h4 id="2-消融研究（Ablation-studies）"><a href="#2-消融研究（Ablation-studies）" class="headerlink" title="2. 消融研究（Ablation studies）"></a>2. 消融研究（Ablation studies）</h4><ul>
<li>进行了广泛的消融实验来验证CAS-GAN中几个设计的有效性，包括解耦表示学习（DRL）、血管语义引导生成器（VSGG）和血管语义引导损失（VSGL）。</li>
<li>实验结果显示，这些设计显著提高了模型的性能。</li>
</ul>
<h4 id="3-外部验证（External-validation）"><a href="#3-外部验证（External-validation）" class="headerlink" title="3. 外部验证（External validation）"></a>3. 外部验证（External validation）</h4><ul>
<li>为了验证模型的泛化能力，还在来自合作医院的外部数据集上进行了实验。</li>
<li>尽管所有方法在处理XCAD和外部数据集之间的显著差异时都遇到了困难，但CAS-GAN展示了相对较好的性能。</li>
</ul>
<p>这些实验结果证明了CAS-GAN在无需对比剂的X射线血管造影合成任务中的有效性和优越性能。论文还指出，未来的工作将通过扩大训练数据集和纳入更先进的模型来进一步提高CAS-GAN的能力。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 论文在结论部分提出了一些未来工作的方向，以下是一些可以进一步探索的关键点：

<ol>
<li><p><strong>扩展训练数据集</strong>：通过增加数据集的大小和多样性来提高模型在外部数据集上的性能。这可能包括收集更多的X射线血管造影图像以及对应的非对比X射线图像，以提高模型的泛化能力。</p>
</li>
<li><p><strong>纳入更先进的模型</strong>：探索使用更先进的生成模型，如StyleGAN等，以进一步提高生成图像的质量和真实感。</p>
</li>
<li><p><strong>改进损失函数</strong>：研究和设计新的损失函数，以更好地捕捉血管造影图像的特定特征，从而提高生成图像的质量。</p>
</li>
<li><p><strong>多模态数据融合</strong>：考虑将CAS-GAN与其他成像模态（如CT、MRI）结合，以实现多模态图像融合和翻译。</p>
</li>
<li><p><strong>实时性能优化</strong>：优化模型的计算效率，使其能够在临床环境中实时生成高质量的血管造影图像。</p>
</li>
<li><p><strong>临床验证</strong>：与医疗专家合作，进行临床试验以验证CAS-GAN生成的图像在实际诊断和治疗规划中的有效性和安全性。</p>
</li>
<li><p><strong>交互式系统开发</strong>：开发交互式系统，使医生能够根据需要调整生成的血管造影图像，以更好地满足特定的临床需求。</p>
</li>
<li><p><strong>模型解释性</strong>：提高模型的可解释性，使医生能够理解模型的决策过程，从而增加对生成图像的信任。</p>
</li>
<li><p><strong>跨领域应用</strong>：探索CAS-GAN在其他医疗图像翻译任务中的应用，如不同模态之间的图像转换或病理图像的合成。</p>
</li>
<li><p><strong>鲁棒性和安全性测试</strong>：对模型进行广泛的鲁棒性和安全性测试，以确保其在各种临床条件下都能稳定地工作。</p>
</li>
</ol>
<p>这些方向不仅可以推动CAS-GAN技术的发展，还可能对医疗图像处理和计算机视觉领域产生更广泛的影响。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文介绍了一种名为CAS-GAN的新型生成对抗网络（GAN）框架，旨在解决介入性医疗程序中使用碘造影剂带来的健康风险问题。CAS-GAN通过解耦表示学习和血管语义引导生成血管造影图像，以减少对碘造影剂的依赖。

<p>以下是论文的主要内容概述：</p>
<ol>
<li><p><strong>问题背景</strong>：心血管疾病是全球主要死因之一，而X射线血管造影是诊断和治疗这些疾病的关键技术。然而，传统的X射线血管造影需要使用碘基造影剂，这些造影剂可能引起严重的副作用，包括过敏反应和肾脏损害。</p>
</li>
<li><p><strong>CAS-GAN框架</strong>：提出了一种新颖的GAN框架，通过解耦背景和血管成分，利用医学先验知识，使用神经网络学习这两部分之间的相互关系，从而合成逼真的血管造影图像。</p>
</li>
<li><p><strong>方法论</strong>：</p>
<ul>
<li><strong>解耦表示学习</strong>：使用专门的编码器将血管造影图像分解为背景和血管成分，并用一个神经网络（预测器）来推断血管表示。</li>
<li><strong>血管语义引导生成器</strong>：引入两个专门的生成器来生成注意力掩模和上下文掩模，以增强生成图像中血管的真实性。</li>
<li><strong>血管语义引导的对抗损失</strong>：提出了一种新的对抗损失，专注于血管的语义差异，以提高GAN对真实和生成血管图像之间差异的识别能力。</li>
</ul>
</li>
<li><p><strong>实验</strong>：</p>
<ul>
<li>使用XCAD数据集进行实验，该数据集包含非对比X射线图像和X射线血管造影图像。</li>
<li>在定量和定性上与其他最先进方法进行比较，CAS-GAN在FID和MMD指标上均优于其他方法。</li>
<li>进行消融研究验证了解耦表示学习、血管语义引导生成器和血管语义引导损失的有效性。</li>
<li>在外部数据集上进行验证，展示了CAS-GAN的泛化能力。</li>
</ul>
</li>
<li><p><strong>结论与未来工作</strong>：CAS-GAN为减少介入手术中碘造影剂的使用提供了一种有前景的方法。未来的工作将通过扩大训练数据集和纳入更先进的模型来进一步提高CAS-GAN的性能。</p>
</li>
</ol>
<p>总的来说，这篇论文提出了一个创新的解决方案来应对医疗成像中的一个关键挑战，并展示了有希望的初步结果。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CAS-GAN for Contrast-free Angiography Synthesis，链接是 https://arxiv.org/pdf/2410.08490 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2410.08490 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d2a965cd2de60999b72ce640f89da05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b34f313828085f5cd7ade9b47c8691a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85dd2a6e0b0bd41d382b3bf5801d30e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a3df5cea34408d0a91ae0f96c1b836.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21c107c51b27691b4d6c20d36f43b7d3.jpg" align="middle">
</details>




                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-13/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-13/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-13/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-790f2773fd5c408063be0051056b75af.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2024-12-13  Dial-In LLM Human-Aligned Dialogue Intent Clustering with   LLM-in-the-loop
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-77d928868bf5623fca9706a53a4cefd6.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-13  Can Modern LLMs Act as Agent Cores in Radiology~Environments?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
