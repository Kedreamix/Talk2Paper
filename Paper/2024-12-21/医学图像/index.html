<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-21  Preventing Local Pitfalls in Vector Quantization via Optimal Transport">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    71 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-21-更新"><a href="#2024-12-21-更新" class="headerlink" title="2024-12-21 更新"></a>2024-12-21 更新</h1><h2 id="Preventing-Local-Pitfalls-in-Vector-Quantization-via-Optimal-Transport"><a href="#Preventing-Local-Pitfalls-in-Vector-Quantization-via-Optimal-Transport" class="headerlink" title="Preventing Local Pitfalls in Vector Quantization via Optimal Transport"></a>Preventing Local Pitfalls in Vector Quantization via Optimal Transport</h2><p><strong>Authors:Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</strong></p>
<p>Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality. </p>
<blockquote>
<p>向量量化网络（VQNs）在各种任务中表现出卓越的性能，但它们容易遇到训练不稳定的问题，这由于需要微妙初始化和模型蒸馏等技术而使得训练过程复杂化。在这项研究中，我们确定局部最小值问题是这种不稳定性的主要原因。为了解决这个问题，我们整合了一种最优传输方法，以替代最近邻搜索，实现更全局的信息分配。我们引入了OptVQ，这是一种新型向量量化方法，采用Sinkhorn算法来解决最优传输问题，从而提高训练过程的稳定性和效率。为了减轻不同数据分布对Sinkhorn算法的影响，我们实施了一种简单有效的归一化策略。我们在图像重建任务上的综合实验表明，OptVQ实现了100%的代码本利用率，并在重建质量上超越了当前最先进的VQNs。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15195v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/zbr17/OptVQ">https://github.com/zbr17/OptVQ</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了Vector-quantized网络（VQNs）的训练不稳定问题，并识别出局部最小值问题为主要原因。为解决此问题，文章提出了一种新型向量量化方法OptVQ，采用Sinkhorn算法优化最优传输问题，从而提高训练过程的稳定性和效率。通过实施有效的归一化策略，减轻了不同数据分布对Sinkhorn算法的影响。在图像重建任务上的实验表明，OptVQ实现了100%的代码本利用率，并在重建质量上超越了现有的最先进的VQNs。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vector-quantized网络（VQNs）在多种任务上表现出卓越性能，但存在训练不稳定的问题。</li>
<li>局部最小值问题是导致训练不稳定的主要原因。</li>
<li>引入了一种新的向量量化方法OptVQ，使用Sinkhorn算法优化最优传输问题以提高训练稳定性和效率。</li>
<li>OptVQ通过实施有效的归一化策略，减轻了不同数据分布对训练过程的影响。</li>
<li>OptVQ实现了100%的代码本利用率。</li>
<li>在图像重建任务上，OptVQ超越了现有的最先进的VQNs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15195">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cbbe20b8e8337839f20c255bb4b2810a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669b555259120893e8aab2a94a5d261b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26676f827d8631e1cbf830601c361e39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8bfd0ed709ff705b9b58ee710112ad4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MultiverSeg-Scalable-Interactive-Segmentation-of-Biomedical-Imaging-Datasets-with-In-Context-Guidance"><a href="#MultiverSeg-Scalable-Interactive-Segmentation-of-Biomedical-Imaging-Datasets-with-In-Context-Guidance" class="headerlink" title="MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging   Datasets with In-Context Guidance"></a>MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging   Datasets with In-Context Guidance</h2><p><strong>Authors:Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca</strong></p>
<p>Medical researchers and clinicians often need to perform novel segmentation tasks on a set of related images. Existing methods for segmenting a new dataset are either interactive, requiring substantial human effort for each image, or require an existing set of manually labeled images. We introduce a system, MultiverSeg, that enables practitioners to rapidly segment an entire new dataset without requiring access to any existing labeled data from that task or domain. Along with the image to segment, the model takes user interactions such as clicks, bounding boxes or scribbles as input, and predicts a segmentation. As the user segments more images, those images and segmentations become additional inputs to the model, providing context. As the context set of labeled images grows, the number of interactions required to segment each new image decreases. We demonstrate that MultiverSeg enables users to interactively segment new datasets efficiently, by amortizing the number of interactions per image to achieve an accurate segmentation. Compared to using a state-of-the-art interactive segmentation method, using MultiverSeg reduced the total number of scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images from unseen tasks. We release code and model weights at <a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.edu/">https://multiverseg.csail.mit.edu</a> </p>
<blockquote>
<p>医学研究人员和临床医生经常需要对一组相关图像执行新的分割任务。现有分割新数据集的方法要么是交互式的，需要针对每张图像投入大量的人力，要么需要现有的手动标记图像集。我们引入了一个系统MultiverSeg，它能让实践者无需访问该任务或领域中的任何现有标记数据即可快速分割整个新数据集。除待分割的图像外，该模型还接受用户交互作为输入，例如点击、边界框或涂鸦，并预测分割结果。随着用户分割的图像越来越多，这些图像和分割结果成为模型的额外输入，提供了上下文信息。随着标记图像上下文集的增长，分割每张新图像所需的交互次数减少。我们通过将每张图像所需的交互次数摊销来高效地对新数据集进行交互式分割，从而证明了MultiverSeg使用户能够实现精确分割。与使用最先进的交互式分割方法相比，使用MultiverSeg减少了涂鸦步骤总数达53%，点击次数减少36%，在来自未见任务的图像集上达到了90%的Dice系数。我们在<a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.edu发布了代码和模型权重./">https://multiverseg.csail.mit.edu发布了代码和模型权重。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15058v1">PDF</a> Project Website: <a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.edu/">https://multiverseg.csail.mit.edu</a> Keywords:   interactive segmentation, in-context learning, medical image analysis,   biomedical imaging, image annotation, visual prompting</p>
<p><strong>Summary</strong></p>
<p>本研究开发了一种名为MultiverSeg的系统，用于在无需任何现有标记数据的情况下快速分割新数据集。该系统能够根据用户提供的互动操作（如点击、绘制边界框或涂鸦）进行预测分割。随着用户分割的图像数量的增加，这些图像和分割结果会作为额外的输入，为模型提供上下文信息。随着标记图像集的增多，分割每张新图像所需的互动操作数量逐渐减少。MultiverSeg通过减少涂鸦步骤和点击次数，实现了对未见任务图像集的准确分割。其代码和模型权重已发布在<a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.edu上./">https://multiverseg.csail.mit.edu上。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MultiverSeg系统能够在无需任何现有标记数据的情况下快速分割新数据集。</li>
<li>用户可通过提供互动操作（如点击、绘制边界框或涂鸦）来指导系统完成预测分割。</li>
<li>随着用户分割的图像数量增加，系统的上下文信息更丰富，分割效率逐渐提高。</li>
<li>MultverSeg通过减少涂鸦步骤和点击次数，提高了分割新图像的效率和准确性。</li>
<li>该系统适用于医学图像分割等需要高效、准确处理大量图像的应用场景。</li>
<li>MultverSeg的性能优于现有的交互式分割方法，能够在未见任务图像集上实现高准确率的分割。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15058">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e98a6565a4866bdbad84393ac1b9679e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5319051f5cf3239f7ccae88ff2c049ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015b5ab92f3167834b3a366e11937115.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d02ffb1e1020e16cccb48db73ba0b71.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Joint-estimation-of-activity-attenuation-and-motion-in-respiratory-self-gated-time-of-flight-PET"><a href="#Joint-estimation-of-activity-attenuation-and-motion-in-respiratory-self-gated-time-of-flight-PET" class="headerlink" title="Joint estimation of activity, attenuation and motion in   respiratory-self-gated time-of-flight PET"></a>Joint estimation of activity, attenuation and motion in   respiratory-self-gated time-of-flight PET</h2><p><strong>Authors:Masoud Elhamiasl, Frederic Jolivet, Ahmadreza Rezaei, Michael Fieseler, Klaus Schäfers, Johan Nuyts, Georg Schramm, Fernando Boada</strong></p>
<p>Whole-body PET imaging is often hindered by respiratory motion during acquisition, causing significant degradation in the quality of reconstructed activity images. An additional challenge in PET&#x2F;CT imaging arises from the respiratory phase mismatch between CT-based attenuation correction and PET acquisition, leading to attenuation artifacts. To address these issues, we propose two new, purely data-driven methods for the joint estimation of activity, attenuation, and motion in respiratory self-gated TOF PET. These methods enable the reconstruction of a single activity image free from motion and attenuation artifacts.   The proposed methods were evaluated using data from the anthropomorphic Wilhelm phantom acquired on a Siemens mCT PET&#x2F;CT system, as well as 3 clinical FDG PET&#x2F;CT datasets acquired on a GE DMI PET&#x2F;CT system. Image quality was assessed visually to identify motion and attenuation artifacts. Lesion uptake values were quantitatively compared across reconstructions without motion modeling, with motion modeling but static attenuation correction, and with our proposed methods.   For the Wilhelm phantom, the proposed methods delivered image quality closely matching the reference reconstruction from a static acquisition. The lesion-to-background contrast for a liver dome lesion improved from 2.0 (no motion correction) to 5.2 (proposed methods), matching the contrast from the static acquisition (5.2). In contrast, motion modeling with static attenuation correction yielded a lower contrast of 3.5. In patient datasets, the proposed methods successfully reduced motion artifacts in lung and liver lesions and mitigated attenuation artifacts, demonstrating superior lesion to background separation.   Our proposed methods enable the reconstruction of a single, high-quality activity image that is motion-corrected and free from attenuation artifacts, without the need for external hardware. </p>
<blockquote>
<p>全身PET成像在采集过程中经常受到呼吸运动的影响，导致重建后的活动图像质量显著下降。PET&#x2F;CT成像中另一个挑战来自于基于CT的衰减校正与PET采集之间呼吸阶段的不匹配，从而导致衰减伪影。为了解决这些问题，我们提出两种全新的、纯粹的数据驱动方法，用于联合估计呼吸自门控TOF PET中的活动、衰减和运动。这些方法能够重建一个不受运动和衰减伪影影响的单一活动图像。所提出的方法使用在西门子mCT PET&#x2F;CT系统上获取的威尔姆人形 Phantom 数据以及通用电气DMI PET&#x2F;CT系统上获取的3个临床FDG PET&#x2F;CT数据集进行了评估。通过视觉评估图像质量，以识别运动和衰减伪影。通过定量比较重建图像中的病灶摄取值，包括无运动建模的重建、有运动建模但静态衰减校正的重建以及我们提出的方法。对于威尔姆 Phantom，所提出的方法提供的图像质量与来自静态采集的参考重建图像非常接近。对于肝穹顶病灶，病灶与背景的对比度从2.0（无运动校正）提高到5.2（所提出的方法），与静态采集的对比度相匹配（5.2）。相比之下，带有静态衰减校正的运动建模产生的对比度较低，为3.5。在患者数据集中，所提出的方法成功减少了肺部和肝脏病灶的运动伪影，并减轻了衰减伪影，显示出优越的病灶与背景分离能力。我们的方法能够重建一个单一的高质量活动图像，该图像经过运动校正，无衰减伪影，且无需外部硬件。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15018v1">PDF</a> 18 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong><br>     全身PET成像过程中由于呼吸运动导致的图像质量下降是一个普遍存在的问题。为解决PET&#x2F;CT成像中基于CT的衰减校正与PET采集之间呼吸相位不匹配导致的衰减伪影问题，本文提出了两种全新的、纯粹的数据驱动方法，用于联合估计活动、衰减和运动。这些方法能够重建出无运动和衰减伪影的高质量活动图像。通过采用Siemens mCT PET&#x2F;CT系统的人体仿真Wilhelm Phantom数据和GE DMI PET&#x2F;CT系统的实际临床FDG PET&#x2F;CT数据集进行验证，本文提出的方法重建出的图像质量接近静态采集的参考重建图像。特别是在肝顶病灶方面，相对于没有运动校正的图像，本文方法的病灶与背景对比度有了显著提升。总体来说，本文方法无需额外的硬件设备即可实现无运动校正、无衰减伪影的高质量活动图像重建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全身PET成像受到呼吸运动的影响，导致重建的活动图像质量下降。</li>
<li>呼吸运动在PET&#x2F;CT成像中引起的挑战包括活动估计和衰减校正的不准确。</li>
<li>本文提出了两种数据驱动的方法来解决这些问题，实现了对活动、衰减和运动的联合估计。</li>
<li>使用Wilhelm Phantom数据和实际临床数据集验证了方法的有效性。</li>
<li>本文方法能够重建出高质量的活动图像，消除运动和衰减伪影。</li>
<li>在肝顶病灶方面，相对于没有运动校正的图像，本文方法的病灶与背景对比度显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15018">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b43853ef115cdcc2734dfdaa796b059.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24c7be63bf28b2baff4450b8a6650eb3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Accessing-the-topological-properties-of-human-brain-functional-sub-circuits-in-Echo-State-Networks"><a href="#Accessing-the-topological-properties-of-human-brain-functional-sub-circuits-in-Echo-State-Networks" class="headerlink" title="Accessing the topological properties of human brain functional   sub-circuits in Echo State Networks"></a>Accessing the topological properties of human brain functional   sub-circuits in Echo State Networks</h2><p><strong>Authors:Bach Nguyen, Tianlong Chen, Shu Yang, Bojian Hou, Li Shen, Duy Duong-Tran</strong></p>
<p>Recent years have witnessed an emerging trend in neuromorphic computing that centers around the use of brain connectomics as a blueprint for artificial neural networks. Connectomics-based neuromorphic computing has primarily focused on embedding human brain large-scale structural connectomes (SCs), as estimated from diffusion Magnetic Resonance Imaging (dMRI) modality, to echo-state networks (ESNs). A critical step in ESN embedding requires pre-determined read-in and read-out layers constructed by the induced subgraphs of the embedded reservoir. As \textit{a priori} set of functional sub-circuits are derived from functional MRI (fMRI) modality, it is unknown, till this point, whether the embedding of fMRI-induced sub-circuits&#x2F;networks onto SCs is well justified from the neuro-physiological perspective and ESN performance across a variety of tasks. This paper proposes a pipeline to implement and evaluate ESNs with various embedded topologies and processing&#x2F;memorization tasks. To this end, we showed that different performance optimums highly depend on the neuro-physiological characteristics of these pre-determined fMRI-induced sub-circuits. In general, fMRI-induced sub-circuit-embedded ESN outperforms simple bipartite and various null models with feed-forward properties commonly seen in MLP for different tasks and reservoir criticality conditions. We provided a thorough analysis of the topological properties of pre-determined fMRI-induced sub-circuits and highlighted their graph-theoretical properties that play significant roles in determining ESN performance. </p>
<blockquote>
<p>近年来，神经形态计算领域出现了一个以脑连接组作为人工神经网络蓝图的新兴趋势。基于连接组的神经形态计算主要关注将大规模结构性脑连接组（SC）嵌入回声状态网络（ESN）。这些连接组是通过扩散磁共振成像（dMRI）技术估计得到的。在ESN嵌入过程中，一个关键步骤是构建预定义的输入和输出层，这些层由嵌入存储库的诱导子图构成。由于先验的功能性子电路集是从功能磁共振成像（fMRI）技术中得出的，到目前为止，尚不清楚从神经生理学角度将fMRI诱导的子电路&#x2F;网络嵌入到SC是否合理，以及在不同任务中ESN的性能表现如何。本文提出了一个管道流程来实现和评估具有不同嵌入拓扑结构和处理&#x2F;记忆任务的ESN。为此，我们展示了不同的性能最优值在很大程度上取决于这些预定义的fMRI诱导子电路的神经生理学特征。一般来说，基于fMRI诱导的子电路嵌入的ESN在多种任务和存储库临界条件下，其性能优于简单的前馈性质的MLP模型中的二分模型和各种空模型。我们对预定义的fMRI诱导子电路的拓扑特性进行了深入分析，并重点介绍了其在确定ESN性能中发挥重要作用的图论特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14999v1">PDF</a> 10 pages, 12 figures</p>
<p><strong>摘要</strong></p>
<p>近期，神经形态计算领域涌现出一种新兴趋势，即以脑连接组作为人工神经网络蓝图。基于连接组的神经形态计算主要关注将人类大脑的大规模结构连接组（SCs）嵌入回声状态网络（ESNs）。ESN嵌入的关键步骤需要预先确定的读写层，这些层由嵌入存储库的诱导子图构建而成。尽管从功能磁共振成像（fMRI）模态中衍生出先验的功能性子电路集，但尚不清楚从神经生理学的角度将fMRI诱导的子电路&#x2F;网络嵌入SCs是否合理，以及在不同任务中ESN的性能表现如何。本文提出了一条管道，用于实现和评估具有各种嵌入拓扑结构和处理&#x2F;记忆任务的ESNs。结果表明，不同的性能最优值高度依赖于这些预先确定的fMRI诱导子电路的神经生理学特征。一般来说，fMRI诱导的子电路嵌入的ESN优于在多层感知器中常见的具有前馈特性的简单二分图和各种空模型，并且在不同的任务和存储库临界条件下表现出卓越性能。本文彻底分析了预先确定的fMRI诱导子电路拓扑属性，并重点介绍了其在确定ESN性能中发挥重要作用的图论属性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>神经形态计算领域正关注将脑连接组作为人工神经网络蓝图的方法。</li>
<li>基于连接组的神经形态计算重点在于将结构连接组（SCs）嵌入回声状态网络（ESNs）。</li>
<li>ESN嵌入涉及预定义的读写层，这些层由嵌入存储库的诱导子图构成。</li>
<li>尚不清楚从神经生理学角度将功能磁共振成像（fMRI）诱导的子电路&#x2F;网络嵌入SCs的合理性。</li>
<li>fMRI诱导的子电路嵌入的ESN在不同任务和存储库临界条件下的性能优于简单模型和常见的前馈特性。</li>
<li>fMRI诱导子电路的拓扑属性对确定ESN性能起着重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14999">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d29e78debcbd670fc5998cbea2f564e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4d5fe7d7772e2dbd9cd06c1f10fd162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57b2031a63241735afef8a86d82238c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819f1159b2980d646f3cf55b54f002f1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Artifact2Artifact-Self-incentive-artifact-removal-for-photoacoustic-imaging-without-any-data"><a href="#Zero-Shot-Artifact2Artifact-Self-incentive-artifact-removal-for-photoacoustic-imaging-without-any-data" class="headerlink" title="Zero-Shot Artifact2Artifact: Self-incentive artifact removal for   photoacoustic imaging without any data"></a>Zero-Shot Artifact2Artifact: Self-incentive artifact removal for   photoacoustic imaging without any data</h2><p><strong>Authors:Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li</strong></p>
<p>Photoacoustic imaging (PAI) uniquely combines optical contrast with the penetration depth of ultrasound, making it critical for clinical applications. However, the quality of 3D PAI is often degraded due to reconstruction artifacts caused by the sparse and angle-limited configuration of detector arrays. Existing iterative or deep learning-based methods are either time-consuming or require large training datasets, significantly limiting their practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a zero-shot self-supervised artifact removal method based on a super-lightweight network, which leverages the fact that reconstruction artifacts are sensitive to irregularities caused by data loss. By introducing random perturbations to the acquired PA data, it spontaneously generates subset data, which in turn stimulates the network to learn the artifact patterns in the reconstruction results, thus enabling zero-shot artifact removal. This approach requires neither training data nor prior knowledge of the artifacts, and is capable of artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or slice images in 3D PAI acquired with arbitrarily sparse or angle-limited detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in both simulation study and $ in\ vivo $ animal experiments. Results demonstrate that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing zero-shot methods, and for the $ in\ vivo $ rat liver, ZS-A2A improves CNR from 17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in the following GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/JaegerCQ/ZS-A2A">https://github.com/JaegerCQ/ZS-A2A</a>. </p>
<blockquote>
<p>光声成像（PAI）独特地结合了光学对比度和超声的穿透深度，使其成为临床应用的关键技术。然而，由于探测器阵列配置稀疏且角度受限导致的重建伪影，常常会导致三维光声成像（3D PAI）的质量下降。现有的迭代或基于深度学习的方法要么耗时过长，要么需要大量训练数据集，从而极大地限制了它们的实际应用。在这里，我们提出了Zero-Shot Artifact2Artifact（ZS-A2A），这是一种基于超轻量级网络的零样本自监督伪影去除方法，它利用重建伪影对由数据丢失引起的不规则性的敏感性。通过对获取的PA数据引入随机扰动，它自发地生成子集数据，从而刺激网络学习重建结果中的伪影模式，从而实现零样本伪影去除。这种方法既不需要训练数据，也不需要关于伪影的先验知识，并且能够对3D PAI进行伪影去除。对于使用任意稀疏或角度受限的探测器阵列获得的三维光声成像中的最大振幅投影（MAP）图像或切片图像，ZS-A2A采用自我激励策略来完成伪影去除，并提高了信噪比（CNR）。我们通过在模拟研究和体内动物实验验证了ZS-A2A。结果表明，与现有的零样本方法相比，ZS-A2A达到了最先进的性能，对于体内大鼠肝脏，ZS-A2A在8秒内将CNR从17.48提高到43.46。ZS-A2A项目将在以下GitHub仓库中提供：<a target="_blank" rel="noopener" href="https://github.com/JaegerCQ/ZS-A2A%E3%80%82">https://github.com/JaegerCQ/ZS-A2A。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于零样本自监督学习的光声成像（PAI）重建伪影去除方法——Zero-Shot Artifact2Artifact（ZS-A2A）。该方法利用超轻量级网络，通过引入随机扰动刺激网络学习重建结果中的伪影模式，实现零样本伪影去除，无需训练数据和先验知识。实验验证显示，ZS-A2A在模拟和体内动物实验中均表现出卓越性能，与现有零样本方法相比达到领先水平，可在短时间内显著提高对比噪声比（CNR）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAI结合了光学对比度和超声穿透深度，对于临床应用至关重要。</li>
<li>3D PAI的质量因重建伪影而降低，这些伪影由检测器阵列的稀疏和角度限制配置引起。</li>
<li>现有方法如迭代或深度学习方法耗时或需要大量训练数据，限制了实际应用。</li>
<li>ZS-A2A是一种零样本自监督伪影去除方法，基于超轻量级网络，利用伪影对数据丢失引起的不规则性的敏感性。</li>
<li>通过引入随机扰动到获取的PA数据，ZS-A2A自发地生成子集数据，刺激网络学习重建结果中的伪影模式。</li>
<li>ZS-A2A无需训练数据和先验知识，能够去除3D PAI的伪影。</li>
<li>ZS-A2A在模拟和体内动物实验中表现出卓越性能，与现有方法相比具有更高的对比噪声比（CNR），并且处理速度快。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14873">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e9a372bd15df0cd13c2a7449acd7a3be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5eee0138563284fbbd307d6848075a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec9d96c4e56a2ee66b5d47f0353d918e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-085003f0421491061ea1f0b7816e82fa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-Powered-Intracranial-Hemorrhage-Detection-A-Co-Scale-Convolutional-Attention-Model-with-Uncertainty-Based-Fuzzy-Integral-Operator-and-Feature-Screening"><a href="#AI-Powered-Intracranial-Hemorrhage-Detection-A-Co-Scale-Convolutional-Attention-Model-with-Uncertainty-Based-Fuzzy-Integral-Operator-and-Feature-Screening" class="headerlink" title="AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional   Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature   Screening"></a>AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional   Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature   Screening</h2><p><strong>Authors:Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan</strong></p>
<p>Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such as decreased consciousness, permanent neurological disabilities, or even death.The primary aim of this study is to detect the occurrence or non-occurrence of ICH, followed by determining the type of subdural hemorrhage (SDH). These tasks are framed as two separate binary classification problems. By adding two layers to the co-scale convolutional attention (CCA) classifier architecture, we introduce a novel approach for ICH detection. In the first layer, after extracting features from different slices of computed tomography (CT) scan images, we combine these features and select the 50 components that capture the highest variance in the data, considering them as informative features. We then assess the discriminative power of these features using the bootstrap forest algorithm, discarding those that lack sufficient discriminative ability between different classes. This algorithm explicitly determines the contribution of each feature to the final prediction, assisting us in developing an explainable AI model. The features feed into a boosting neural network as a latent feature space. In the second layer, we introduce a novel uncertainty-based fuzzy integral operator to fuse information from different CT scan slices. This operator, by accounting for the dependencies between consecutive slices, significantly improves detection accuracy. </p>
<blockquote>
<p>颅内出血（ICH）是指血液在颅骨内泄漏或积聚，这是由于大脑内或周围的血管破裂所导致的。如果这种情况未能及时诊断并适当治疗，可能会导致意识减退、永久性神经功能障碍甚至死亡等严重并发症。本研究的主要目的是检测颅内出血是否发生，并确定蛛网膜下腔出血（SDH）的类型。这两项任务被划分为两个单独的二分类问题。通过对共尺度卷积注意力（CCA）分类器架构增加两层，我们提出了一种新的ICH检测方法。在第一层，从不同层面的计算机断层扫描（CT）图像中提取特征后，我们结合这些特征并选择50个组件，这些组件捕获数据中的最高方差，被视为具有信息量的特征。然后，我们使用自助森林算法评估这些特征的判别力，并丢弃那些在区分不同类别时缺乏足够判别力的特征。该算法能明确确定每个特征对最终预测的贡献，有助于我们开发可解释的AI模型。这些特征被输入到一个增强神经网络中作为潜在特征空间。在第二层，我们引入了一种基于不确定性的模糊积分算子来融合不同CT扫描层面的信息。该算子通过考虑相邻层面之间的依赖性，显著提高了检测准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14869v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该摘要简洁介绍了一项关于颅内出血检测与诊断研究。研究中提出使用两层卷积神经网络架构进行颅内出血（ICH）检测与硬膜下出血（SDH）类型判断。第一层通过提取CT扫描图像的不同切片特征，筛选出最具信息量的特征成分；第二层采用基于不确定性的模糊积分算子融合不同CT扫描切片的信息，以提高检测准确性。整个模型注重解释性，能明确各特征对预测结果的贡献。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文中七个关键要点，以简化形式呈现：</p>
<ol>
<li>颅内出血（ICH）是血液在颅内的泄漏或积聚，若不及时诊断和适当治疗，可能导致严重并发症，甚至死亡。</li>
<li>此研究的主要目标是检测颅内出血的发生与否，并确定硬膜下出血的类型。</li>
<li>采用两层卷积神经网络架构，引入新型方法用于ICH检测。</li>
<li>第一层中，从CT扫描图像的不同切片提取特征，选择最具信息量的50个特征组件，并利用bootstrap森林算法评估其判别力。</li>
<li>引入解释性AI模型，明确各特征对预测结果的贡献。</li>
<li>特征输入到增强神经网络中作为潜在特征空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-41b809b7909eaa6bf52d1caba95003da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc9fb89d5b40177587f047b3ed5c1ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-014a13c02c2a280a3dd0b9f1685ede77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-714966a7b6f69c810badd27439c2bcd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f85f4d323b6eba640f59f69f90dac5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Head-and-Neck-Tumor-Segmentation-of-MRI-from-Pre-and-Mid-radiotherapy-with-Pre-training-Data-Augmentation-and-Dual-Flow-UNet"><a href="#Head-and-Neck-Tumor-Segmentation-of-MRI-from-Pre-and-Mid-radiotherapy-with-Pre-training-Data-Augmentation-and-Dual-Flow-UNet" class="headerlink" title="Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy   with Pre-training, Data Augmentation and Dual Flow UNet"></a>Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy   with Pre-training, Data Augmentation and Dual Flow UNet</h2><p><strong>Authors:Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang</strong></p>
<p>Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/WltyBY/HNTS-MRG2024_train_code">https://github.com/WltyBY/HNTS-MRG2024_train_code</a>. </p>
<blockquote>
<p>头颈部肿瘤和转移性淋巴结对于治疗方案的制定和预后分析至关重要。这些结构的精确分割和定量分析需要像素级别的标注，因此自动分割技术对于头颈部癌症的诊断和治疗至关重要。本研究中，我们研究了多种策略对放疗前（pre-RT）和放疗中（mid-RT）图像分割的影响。对于放疗前图像的分割，我们采用了1）全监督学习方法；2）使用预训练权重和MixUp数据增强技术增强同一方法。对于放疗中图像，我们引入了一种计算友好的新型网络架构，该架构具有针对放疗中图像和注册放疗前图像的单独编码器，并带有其标签。放疗中编码器分支在正向传播过程中逐步整合放疗前图像和标签的信息。我们从每份数据中选出表现最佳的模型，使用其预测结果创建集成平均值来进行推断。在最终测试中，我们的模型在HiLab的聚合Dice相似系数（DSC）上达到了放疗前分割性能为82.38%，放疗中分割性能为72.53%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/WltyBY/HNTS-MRG2024_train_code">https://github.com/WltyBY/HNTS-MRG2024_train_code</a>处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14846v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究探讨了多种策略在放射治疗前后图像分割中的效果，采用深度学习方法，对头部和颈部肿瘤以及转移性淋巴结进行准确分割和定量分析。通过融合预训练权重和MixUp数据增强技术，提高模型性能，并在测试中获得较好的分割效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>头颈部肿瘤和转移性淋巴结的治疗规划和预后分析至关重要，需要准确的分割和定量分析。</li>
<li>自动化分割技术对于头颈部癌症的诊断和治疗至关重要。</li>
<li>研究中使用了多种策略进行放射治疗前后图像的分割。</li>
<li>对于放射治疗前的图像分割，采用了全监督学习方法和融合预训练权重及MixUp数据增强技术的增强方法。</li>
<li>对于放射治疗中的图像分割，引入了一种计算友好的网络架构，该架构具有针对中期放射治疗图像和已注册的前期放射治疗图像的单独编码器。</li>
<li>中期放射治疗编码器分支在正向传播过程中逐步整合前期图像和标签的信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14846">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24b3b5514558de3d032519719698af60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d20352c3aa04f048cb44632cf46b006.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b192492412e5674598b5b0a9eaa49db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33f744d54a26efc5c92774c85ce8f6f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations"><a href="#Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations" class="headerlink" title="Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations"></a>Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations</h2><p><strong>Authors:Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen</strong></p>
<p>Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8% increase in success rates for complex real-world dexterous manipulation tasks. </p>
<blockquote>
<p>近期机器人技术的进步主要集中在开发能够执行多种任务的一般性策略上。通常，这些策略利用预训练的视觉编码器来捕获当前观察中的关键信息。然而，以前训练的视觉编码器主要依赖于对比学习或单图像重建，无法完全捕获对于实体任务至关重要的序列信息。最近，视频扩散模型（VDMs）已显示出准确预测未来图像序列的能力，展现出对物理动态的良好理解。受VDM强大视觉预测能力的启发，我们假设它们天生就具有反映物理世界演变的视觉表征，我们称之为预测性视觉表征。基于这一假设，我们提出视频预测策略（VPP），这是一种以VDMs的预测性视觉表征为条件的一般性机器人策略。为了进一步增强这些表征，我们融入了多样化的人类或机器人操作数据集，采用统一的视频生成训练目标。VPP在两个模拟和两个真实世界的基准测试中均表现超越现有方法。尤其值得一提的是，与之前的最新技术相比，它在Calvin ABC-D基准测试中实现了28.1%的相对改进，并且在复杂的真实世界灵巧操作任务中成功率提高了28.8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14803v1">PDF</a> The first two authors contribute equally. Project Page at   <a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io/">https://video-prediction-policy.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>近期机器人技术的进步促使了通用策略的发展，这些策略可以执行多种任务。利用预训练的视觉编码器捕捉当前观察信息是关键。然而，过去以两图像对比学习或单图像重建方式训练的视觉编码器无法完全捕获躯体任务所需的序列信息。视频扩散模型（VDMs）能准确预测未来图像序列，展现出对物理动态的良好理解。基于VDMs的强大视觉预测能力，我们假设其具备反映物理世界演变的内在视觉表征，称为预测性视觉表征。我们提出基于预测性视觉表征的视频预测策略（VPP），这是一种通用机器人策略。为进一步优化这些表征，我们纳入多样的人类或机器人操作数据集，采用统一的视频生成训练目标。VPP在模拟和真实世界的基准测试中均表现优异，相对于前序最佳方案，在Calvin ABC-D基准测试中实现了28.1%的相对改进，并在复杂的真实世界灵巧操作任务中成功率提高了28.8%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期机器人技术关注开发能执行多种任务的通用策略。</li>
<li>传统的视觉编码器难以捕获重要序列信息，对于躯体任务至关重要。</li>
<li>视频扩散模型（VDMs）能预测未来图像序列，反映物理动态的理解。</li>
<li>VDMs的预测性视觉表征对于机器人任务至关重要。</li>
<li>提出基于预测性视觉表征的视频预测策略（VPP）。</li>
<li>VPP在模拟和真实世界测试中表现优异，相对改进显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0633cab27a1dab8bc8deae652cb614b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd64e25d03d7b9d8221e2740f77f2d95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-746a0ed8f218c569b507d8cde6f6f142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ec1419d9d6845568c183d648993c172.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MUSTER-Longitudinal-Deformable-Registration-by-Composition-of-Consecutive-Deformations"><a href="#MUSTER-Longitudinal-Deformable-Registration-by-Composition-of-Consecutive-Deformations" class="headerlink" title="MUSTER: Longitudinal Deformable Registration by Composition of   Consecutive Deformations"></a>MUSTER: Longitudinal Deformable Registration by Composition of   Consecutive Deformations</h2><p><strong>Authors:Edvard O. S. Grødem, Donatas Sederevičius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle Bjørnerud, Till Schellhorn, Øystein Sørensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell</strong></p>
<p>Longitudinal imaging allows for the study of structural changes over time. One approach to detecting such changes is by non-linear image registration. This study introduces Multi-Session Temporal Registration (MUSTER), a novel method that facilitates longitudinal analysis of changes in extended series of medical images. MUSTER improves upon conventional pairwise registration by incorporating more than two imaging sessions to recover longitudinal deformations. Longitudinal analysis at a voxel-level is challenging due to effects of a changing image contrast as well as instrumental and environmental sources of bias between sessions. We show that local normalized cross-correlation as an image similarity metric leads to biased results and propose a robust alternative. We test the performance of MUSTER on a synthetic multi-site, multi-session neuroimaging dataset and show that, in various scenarios, using MUSTER significantly enhances the estimated deformations relative to pairwise registration. Additionally, we apply MUSTER on a sample of older adults from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. The results show that MUSTER can effectively identify patterns of neuro-degeneration from T1-weighted images and that these changes correlate with changes in cognition, matching the performance of state of the art segmentation methods. By leveraging GPU acceleration, MUSTER efficiently handles large datasets, making it feasible also in situations with limited computational resources. </p>
<blockquote>
<p>纵向成像允许研究随时间发生的结构变化。检测这种变化的一种方法是图像非线性配准。本研究介绍了多会话时序配准（MUSTER），这是一种新方法，可以促进对一系列医学图像变化的纵向分析。与传统的配对配准相比，MUSTER通过结合多个成像会话来恢复纵向变形，从而改进了传统配对配准的局限性。由于图像对比度变化的影响以及不同会话之间仪器和环境来源的偏见，在体素水平上进行纵向分析是一个挑战。我们证明了局部归一化互相关作为图像相似度度量会导致有偏结果，并提出了稳健的替代方案。我们在合成多站点、多会话的神经成像数据集上测试了MUSTER的性能，并显示在各种情况下，与使用配对配准相比，使用MUSTER可以大大增强估计的变形。此外，我们还将在阿尔茨海默病神经影像学倡议（ADNI）研究中的老年受试者样本应用于MUSTER。结果表明，MUSTER可以有效地从T1加权图像中识别神经退变的模式，这些变化与认知变化相关，符合最先进的分割方法的性能。通过利用GPU加速，MUSTER能够高效处理大数据集，即使在计算资源有限的情况下也完全可行。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14671v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究提出一种名为MUSTER的多会话时序注册方法，可实现对医学图像序列的纵向分析。相较于传统的配对注册方法，MUSTER通过纳入超过两个成像会话的数据来恢复纵向变形，提高了性能。研究指出局部归一化交叉相关作为图像相似度量可能导致偏差，并提出一种稳健的替代方案。在合成多站点、多会话的神经成像数据集上进行的测试表明，在各种场景下，相较于配对注册，使用MUSTER可显著提高估计变形的效果。此外，对ADNI研究中老年人群样本的应用表明，MUSTER可从T1加权图像有效地识别神经退化模式，且与认知变化相关联，达到最新分割方法的性能水平。借助GPU加速，MUSTER可高效处理大数据集，甚至在计算资源有限的情况下也能实现应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MUSTER是一种多会话时序注册方法，用于医学图像序列的纵向分析。</li>
<li>它通过纳入超过两个成像会话的数据来提高恢复纵向变形的性能。</li>
<li>局部归一化交叉相关作为图像相似度量可能导致偏差，研究提出了一种稳健的替代方案。</li>
<li>在合成数据集上的测试表明，MUSTER在估计变形方面优于传统的配对注册方法。</li>
<li>在ADNI研究的应用中，MUSTER可有效地从T1加权图像识别神经退化模式，并与认知变化相关联。</li>
<li>MUSTER借助GPU加速可高效处理大数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14671">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c26a916b5b769ded1e8a2af74f0b52d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78e8d5d396bc726393f5b22c75a63965.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Spike2Former-Efficient-Spiking-Transformer-for-High-performance-Image-Segmentation"><a href="#Spike2Former-Efficient-Spiking-Transformer-for-High-performance-Image-Segmentation" class="headerlink" title="Spike2Former: Efficient Spiking Transformer for High-performance Image   Segmentation"></a>Spike2Former: Efficient Spiking Transformer for High-performance Image   Segmentation</h2><p><strong>Authors:Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li</strong></p>
<p>Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNs in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0 efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU and 6.6 efficiency on CityScapes. </p>
<blockquote>
<p>脉冲神经网络（SNNs）具有低功耗优势，但在图像分割任务中的表现不佳。原因是直接将为分割任务设计的复杂架构神经网络直接转换为脉冲版本会导致性能下降和非收敛。为了应对这一挑战，我们首先确定导致脉冲点火严重减少的架构设计模块，进行有针对性的改进，并提出Spike2Former架构。其次，为了解决复杂架构下SNNs的训练稳定性问题，我们提出了归一化整数脉冲神经元。我们在各种语义分割数据集上为SNNs设置了一个新的最先进的标准，在ADE20K上实现了+12.7%的mIoU和5.0的效率提升，在VOC2012上实现了+14.3%的mIoU和5.2的效率提升，以及在CityScapes上实现了+9.1%的mIoU和6.6的效率提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14587v1">PDF</a> This work has been accepted on Association for the Advancement of   Artificial Intelligence 2025</p>
<p><strong>Summary</strong><br>     脉冲神经网络（SNNs）具有低功耗优势，但在图像分割任务中表现不佳。原因是直接将复杂架构设计的分割任务神经网络转换为脉冲版本会导致性能下降和非收敛。为解决此挑战，我们提出Spike2Former架构，并引入归一化整数脉冲神经元以解决复杂架构下SNNs的训练稳定性问题。在多个语义分割数据集上，我们为SNNs设立了新的业界标准，其中ADE20K上的mIoU提升了+12.7%且效率提高5.0%，VOC2012上的mIoU提升了+14.3%且效率提高5.2%，CityScapes上的mIoU提升了+9.1%且效率提高6.6。</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. 脉冲神经网络（SNNs）在图像分割任务中表现不佳，直接转换复杂设计的神经网络会导致性能下降和非收敛。
 2. 提出Spike2Former架构，针对导致性能下降的关键模块进行改进。
 3. 引入归一化整数脉冲神经元，解决复杂架构下SNNs的训练稳定性问题。
 4. 在多个语义分割数据集上，SNNs的性能达到新的业界标准。
 5. 在ADE20K数据集上，mIoU提升+12.7%且效率提高5.0%。
 6. 在VOC2012数据集上，mIoU提升+14.3%且效率提高5.2%。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14587">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bf19ddf865cc876fe921a29b2b334171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e8fb96621bde5f2e9a2aae8bff3d188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422e6495063084a4b5aa86c66a512695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9bc0ac16973466f0dc779b4477277c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-860e1eec41434fea1033dd5fdb120a0a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="S-3-Mamba-Small-Size-Sensitive-Mamba-for-Lesion-Segmentation"><a href="#S-3-Mamba-Small-Size-Sensitive-Mamba-for-Lesion-Segmentation" class="headerlink" title="{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation"></a>{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation</h2><p><strong>Authors:Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen</strong></p>
<p>Small lesions play a critical role in early disease diagnosis and intervention of severe infections. Popular models often face challenges in segmenting small lesions, as it occupies only a minor portion of an image, while down_sampling operations may inevitably lose focus on local features of small lesions. To tackle the challenges, we propose a {\bf S}mall-{\bf S}ize-{\bf S}ensitive {\bf Mamba} ({\bf S$^3$-Mamba}), which promotes the sensitivity to small lesions across three dimensions: channel, spatial, and training strategy. Specifically, an Enhanced Visual State Space block is designed to focus on small lesions through multiple residual connections to preserve local features, and selectively amplify important details while suppressing irrelevant ones through channel-wise attention. A Tensor-based Cross-feature Multi-scale Attention is designed to integrate input image features and intermediate-layer features with edge features and exploit the attentive support of features across multiple scales, thereby retaining spatial details of small lesions at various granularities. Finally, we introduce a novel regularized curriculum learning to automatically assess lesion size and sample difficulty, and gradually focus from easy samples to hard ones like small lesions. Extensive experiments on three medical image segmentation datasets show the superiority of our S$^3$-Mamba, especially in segmenting small lesions. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ErinWang2023/S3-Mamba">https://github.com/ErinWang2023/S3-Mamba</a>. </p>
<blockquote>
<p>微小病变在早期疾病诊断和严重感染干预中扮演着关键角色。常见的模型在分割微小病变时常常面临挑战，因为微小病变只占图像的一小部分，而下采样操作可能会不可避免地忽略其局部特征。为了应对这些挑战，我们提出了一种名为小型病变敏感Mamba（S$^3$-Mamba）的方法，通过通道、空间和训练策略三个维度提高对微小病变的敏感性。具体来说，设计了一个增强视觉状态空间块，通过多个残差连接专注于微小病变，保留局部特征，并通过通道注意力有选择地放大重要细节并抑制无关细节。基于张量的跨特征多尺度注意力旨在将输入图像特征与边缘特征结合中间层特征，并利用多尺度特征的注意力支持，从而在不同粒度上保留微小病变的空间细节。最后，我们引入了一种新型正则化课程学习法，自动评估病变大小和样本难度，并从简单样本逐步聚焦到困难样本（如微小病变）。在三个医学图像分割数据集上的大量实验表明，我们的S$^3$-Mamba具有优越性，尤其在分割微小病变方面。我们的代码位于 <a target="_blank" rel="noopener" href="https://github.com/ErinWang2023/S3-Mamba%E3%80%82">https://github.com/ErinWang2023/S3-Mamba。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14546v1">PDF</a> Accept by AAAI 2025</p>
<p><strong>Summary</strong><br>     针对医学图像中小病灶分割的挑战，提出一种名为S$^3$-Mamba的方法，通过增强视觉状态空间、基于张量的跨特征多尺度注意力和正则化课程学习等技术，提高对小病灶的敏感性。在三个医学图像分割数据集上的实验表明，S$^3$-Mamba在分割小病灶方面具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>小病灶在疾病早期诊断和治疗中起关键作用，但现有模型在分割小病灶时面临挑战。</li>
<li>S$^3$-Mamba方法通过三个维度提高小病灶的敏感性：通道、空间和训练策略。</li>
<li>采用增强视觉状态空间块，通过多重残差连接保留局部特征，并选择性放大重要细节，抑制无关信息。</li>
<li>基于张量的跨特征多尺度注意力能够整合输入图像特征与中间层特征，同时利用多尺度特征的注意力支持，保留小病灶的空间细节。</li>
<li>引入正则化课程学习，自动评估病灶大小与样本难度，从小样本逐渐聚焦到困难样本（如小病灶）。</li>
<li>在三个医学图像分割数据集上的实验表明S$^3$-Mamba的优越性，特别是在小病灶分割方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c2b8eafef48812b7cdc7909f8ee5480a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5000f5ddb6f87dffa73e538224b5110.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ea18973265af11a941bb1c5a3fe03b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dcdbecc4b8fa2781cd00eac3206971c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Summary-of-Point-Transformer-with-Federated-Learning-for-Predicting-Breast-Cancer-HER2-Status-from-Hematoxylin-and-Eosin-Stained-Whole-Slide-Images"><a href="#Summary-of-Point-Transformer-with-Federated-Learning-for-Predicting-Breast-Cancer-HER2-Status-from-Hematoxylin-and-Eosin-Stained-Whole-Slide-Images" class="headerlink" title="Summary of Point Transformer with Federated Learning for Predicting   Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide   Images"></a>Summary of Point Transformer with Federated Learning for Predicting   Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide   Images</h2><p><strong>Authors:Kamorudeen A. Amuda, Almustapha A. Wakili</strong></p>
<p>This study introduces a federated learning-based approach to predict HER2 status from hematoxylin and eosin (HE)-stained whole slide images (WSIs), reducing costs and speeding up treatment decisions. To address label imbalance and feature representation challenges in multisite datasets, a point transformer is proposed, incorporating dynamic label distribution, an auxiliary classifier, and farthest cosine sampling. Extensive experiments demonstrate state-of-the-art performance across four sites (2687 WSIs) and strong generalization to two unseen sites (229 WSIs). </p>
<blockquote>
<p>本研究介绍了一种基于联邦学习的方法，用于从苏木精和伊红（HE）染色的全切片图像（WSI）预测HER2状态，降低成本并加快治疗决策。为了解决多站点数据集中的标签不平衡和特征表示挑战，提出了一种点转换器，它结合了动态标签分布、辅助分类器和最远余弦采样。大量实验表明，该方法在四个站点（2687张WSI）上表现出卓越的性能，并且对两个未见站点（229张WSI）具有很强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14545v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像研究中采用基于联邦学习的预测HER2状态的方法，利用HE染色全幻灯片图像（WSIs），降低成本并加快治疗决策。提出一种点转换器来解决多站点数据集中的标签不平衡和特征表示挑战，包括动态标签分布、辅助分类器和最远余弦采样。在四个站点（2687张幻灯片）进行了广泛实验，证明了其卓越性能，并对两个未见过的新站点（229张幻灯片）具有较强的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>采用联邦学习预测HER2状态。</li>
<li>研究涉及使用HE染色全幻灯片图像（WSIs）。</li>
<li>通过联邦学习降低成本并加速治疗决策。</li>
<li>提出点转换器来解决标签不平衡和特征表示的挑战。</li>
<li>点转换器包括动态标签分布、辅助分类器和最远余弦采样技术。</li>
<li>实验在四个站点进行，表现出卓越性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14545">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9b163bfa6cde1cbe725833d58e63e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f088365703823d01aee53e9837edd28b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0687e98cabcd19fdbbbc119d34245b15.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DAMPER-A-Dual-Stage-Medical-Report-Generation-Framework-with-Coarse-Grained-MeSH-Alignment-and-Fine-Grained-Hypergraph-Matching"><a href="#DAMPER-A-Dual-Stage-Medical-Report-Generation-Framework-with-Coarse-Grained-MeSH-Alignment-and-Fine-Grained-Hypergraph-Matching" class="headerlink" title="DAMPER: A Dual-Stage Medical Report Generation Framework with   Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching"></a>DAMPER: A Dual-Stage Medical Report Generation Framework with   Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching</h2><p><strong>Authors:Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen</strong></p>
<p>Medical report generation is crucial for clinical diagnosis and patient management, summarizing diagnoses and recommendations based on medical imaging. However, existing work often overlook the clinical pipeline involved in report writing, where physicians typically conduct an initial quick review followed by a detailed examination. Moreover, current alignment methods may lead to misaligned relationships. To address these issues, we propose DAMPER, a dual-stage framework for medical report generation that mimics the clinical pipeline of report writing in two stages. In the first stage, a MeSH-Guided Coarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image features with medical subject headings (MeSH) features to generate a rough keyphrase representation of the overall impression. In the second stage, a Hypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs hypergraphs for image patches and report annotations, modeling high-order relationships within each modality and performing hypergraph matching to capture semantic correlations between image regions and textual phrases. Finally,the coarse-grained visual features, generated MeSH representations, and visual hypergraph features are fed into a report decoder to produce the final medical report. Extensive experiments on public datasets demonstrate the effectiveness of DAMPER in generating comprehensive and accurate medical reports, outperforming state-of-the-art methods across various evaluation metrics. </p>
<blockquote>
<p>医学报告生成对临床诊断和治疗管理至关重要，它是基于医学成像对诊断和建议进行汇总的关键环节。然而，现有工作往往忽略了报告编写所涉及的临床流程，医生通常先进行初步快速审查，然后进行详细检查。此外，当前的对齐方法可能导致关系错位。为了解决这些问题，我们提出了DAMPER，这是一个用于医学报告生成的双阶段框架，它模仿了两阶段报告写作的临床流程。在第一阶段，我们采用MeSH指导的粗粒度对齐（MCG）阶段，将胸部X射线（CXR）图像特征与医学主题词表（MeSH）特征对齐，以生成整体印象的粗略关键词表示。在第二阶段，我们采用超图增强细粒度对齐（HFG）阶段，为图像补丁和报告注释构建超图，对每种模态内的高阶关系进行建模，并通过超图匹配来捕获图像区域和文本短语之间的语义相关性。最后，将粗粒度视觉特征、生成的MeSH表示和视觉超图特征输入报告解码器，以生成最终的医学报告。在公共数据集上的大量实验表明，DAMPER在生成全面准确的医学报告方面非常有效，并在各种评估指标上优于最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种双阶段框架DAMPER用于医学报告生成，模仿医生书写报告的临床流程。第一阶段为MeSH引导下的粗粒度对齐（MCG），将胸X光片图像特征与医学主题标题（MeSH）特征对齐，生成整体印象的粗略关键词表示。第二阶段为超图增强的细粒度对齐（HFG），构建图像补丁和报告注释的超图，对每种模态内的高阶关系进行建模，并通过超图匹配捕获图像区域和文本短语之间的语义关联。最终，将粗粒度视觉特征、生成的MeSH表示和超图特征输入报告解码器，生成最终医学报告。在公共数据集上的广泛实验表明，DAMPER在生成全面、准确的医学报告方面效果显著，优于各种评估指标下的最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗报告生成对临床诊断和治疗方案至关重要，其生成过程通常包含两个阶段：初步快速审查和详细检查。</li>
<li>当前医疗报告生成方法忽略了临床流程，可能导致关系错位。</li>
<li>DAMPER框架模仿医生书写报告的临床流程，分为两个阶段：粗粒度对齐和细粒度对齐。</li>
<li>粗粒度对齐阶段通过MeSH引导，将图像特征与医学主题标题特征对齐，形成整体印象的粗略关键词表示。</li>
<li>细粒度对齐阶段通过构建超图，对图像补丁和报告注释进行高阶关系建模，并捕获图像和文本之间的语义关联。</li>
<li>DAMPER使用粗粒度视觉特征、MeSH表示和超图特征来生成最终医疗报告。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14535">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-587573d7382ce7647125550460002b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72b8629732b4374a0cfdecbc99b5eeda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d3f448a827787a605f8b2bd2b79dcb0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-794b3cd3e8212de13cc04d29c046d076.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments"><a href="#Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments" class="headerlink" title="Can Modern LLMs Act as Agent Cores in Radiology Environments?"></a>Can Modern LLMs Act as Agent Cores in Radiology Environments?</h2><p><strong>Authors:Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Advancements in large language models (LLMs) have paved the way for LLM-based agent systems that offer enhanced accuracy and interpretability across various domains. Radiology, with its complex analytical requirements, is an ideal field for the application of these agents. This paper aims to investigate the pre-requisite question for building concrete radiology agents which is, &#96;Can modern LLMs act as agent cores in radiology environments?’ To investigate it, we introduce RadABench with three-fold contributions: First, we present RadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based agents, generated from an extensive taxonomy encompassing 6 anatomies, 5 imaging modalities, 10 tool categories, and 11 radiology tasks. Second, we propose RadABench-EvalPlat, a novel evaluation platform for agents featuring a prompt-driven workflow and the capability to simulate a wide range of radiology toolsets. Third, we assess the performance of 7 leading LLMs on our benchmark from 5 perspectives with multiple metrics. Our findings indicate that while current LLMs demonstrate strong capabilities in many areas, they are still not sufficiently advanced to serve as the central agent core in a fully operational radiology agent system. Additionally, we identify key factors influencing the performance of LLM-based agent cores, offering insights for clinicians on how to apply agent systems in real-world radiology practices effectively. All of our code and data are open-sourced in <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench">https://github.com/MAGIC-AI4Med/RadABench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的进步为基于LLM的代理系统铺平了道路，这些系统在各个领域提供了更高的准确性和可解释性。放射学由于其复杂的分析要求，是这些代理应用的理想领域。本文旨在探讨构建具体放射学代理的先决问题，即“现代LLM能否在放射学环境中作为代理核心？”为了调查这个问题，我们推出了RadABench，它有三方面的贡献：首先，我们展示了RadABench-Data，这是一套全面的合成评估数据集，用于基于LLM的代理，数据来自广泛的分类，包括6个解剖学、5种成像模式、10个工具类别和11个放射学任务。其次，我们提出了RadABench-EvalPlat，这是一个新的代理评估平台，具有提示驱动的工作流程，能够模拟广泛的放射学工具集。第三，我们从5个角度对7款领先的大型语言模型进行了评估，使用了多个指标。我们的研究结果表明，虽然当前的大型语言模型在许多领域表现出强大的能力，但它们仍然不足以作为完全运行的放射学代理系统的核心代理。此外，我们还确定了影响基于LLM的代理核心性能的关键因素，为临床医生提供了如何在现实世界的放射学实践中有效应用代理系统的见解。我们的所有代码和数据都在<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/MAGIC-AI4Med/RadABench上开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09529v2">PDF</a> 22 pages,7 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了现代大型语言模型（LLMs）在放射学环境中的表现，提出了RadABench数据集与评估平台。通过评估发现，虽然LLMs在许多领域表现出强大的能力，但仍不足以作为完全操作化的放射学代理系统的核心。本文还公开了代码和数据集。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在放射学领域具有应用潜力。</li>
<li>RadABench是一个综合性的合成评估数据集，为基于LLM的代理提供了评估标准。</li>
<li>RadABench-EvalPlat是一个新颖的评估平台，为代理提供了以提示为中心的工作流程和模拟各种放射学工具集的能力。</li>
<li>评估发现当前LLMs在许多领域表现出强大的能力，但仍不足以作为完全操作化的放射学代理系统的核心。</li>
<li>公开的代码和数据集有助于进一步研究和应用。</li>
<li>LLMs在放射学领域的性能受到多种因素的影响，需要更多研究来提升其表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09529">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e3c80dd1ef26eb84ad020d169c4df0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190becf8a05e2e891b2dd0eb3f6052c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-465e5a878f0cbf8788985f515e140601.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM"><a href="#Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM" class="headerlink" title="Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM"></a>Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM</h2><p><strong>Authors:Hiromasa Suzuki, Naomi Tsuji, Yoshiaki Kanemaru, Megumi Shidatsu, Laura Olivera-Nieto, Samar Safi-Harb, Shigeo S. Kimura, Eduardo de la Fuente, Sabrina Casanova, Kaya Mori, Xiaojie Wang, Sei Kato, Dai Tateishi, Hideki Uchiyama, Takaaki Tanaka, Hiroyuki Uchida, Shun Inoue, Dezhi Huang, Marianne Lemoine-Goumard, Daiki Miura, Shoji Ogawa, Shogo B. Kobayashi, Chris Done, Maxime Parra, María Díaz Trigo, Teo Muñoz-Darias, Montserrat Armas Padilla, Ryota Tomaru, Yoshihiro Ueda</strong></p>
<p>A recent report on the detection of very-high-energy gamma rays from V4641 Sagittarii (V4641 Sgr) up to <del>0.8 peta-electronvolt has made it the second confirmed “PeVatron” microquasar. Here we report on the observation of V4641 Sgr with X-Ray Imaging and Spectroscopy Mission (XRISM) in September 2024. Thanks to the large field of view and low background, the CCD imager Xtend successfully detected for the first time X-ray extended emission around V4641 Sgr with a significance of &gt; 4.5 sigma and &gt; 10 sigma based on our imaging and spectral analysis, respectively. The spatial extent is estimated to have a radius of $7 \pm 3$ arcmin ($13 \pm 5$ pc at a distance of 6.2 kpc) assuming a Gaussian-like radial distribution, which suggests that the particle acceleration site is within ~10 pc of the microquasar. If the X-ray morphology traces the diffusion of accelerated electrons, this spatial extent can be explained by either an enhanced magnetic field (</del>80 uG) or a suppressed diffusion coefficient (~$10^{27}$ cm$^2$ s$^{-1}$ at 100 TeV). The integrated X-ray flux, (4-6)$\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ (2-10 keV), would require a magnetic field strength higher than the galactic mean (&gt; 8 uG) if the diffuse X-ray emission originates from synchrotron radiation and the gamma-ray emission is predominantly hadronic. If the X-rays are of thermal origin, the measured extension, temperature, and plasma density can be explained by a jet with a luminosity of ~$2\times 10^{39}$ erg s$^{-1}$, which is comparable to the Eddington luminosity of this system. </p>
<blockquote>
<p>最近的一份关于从V4641天箭星（V4641 Sgr）检测到超高能伽马射线的报告，能量高达~0.8拍电子伏特，使其成为第二个确认的“拍伏仑”微类星。这里我们报告了2024年9月使用X射线成像和光谱任务（XRISM）观察到的V4641 Sgr的情况。由于视场范围大且背景低，CCD成像仪Xtend首次成功检测到V4641 Sgr周围的X射线扩展发射，其显著性基于我们的成像和光谱分析分别大于4.5σ和大于10σ。空间范围估计半径为$7±3$角分（在距离6.2千秒差距的情况下为$13±5$秒差距），假设径向分布类似于高斯分布，这表明粒子加速位点位于微类星周围约10秒差距内。如果X射线的形态追踪了加速电子的扩散，那么这种空间范围可以用增强的磁场（约80微高斯）或抑制的扩散系数（在100TeV时约为$10^{27}$厘米$^2$每秒）来解释。X射线积分流量为（介于4-6）× $10^{-12}$erg s$^{-1}$ cm$^{-2}$ （在（在波动范围内的光度较复杂 ）之间），如果扩散X射线发射来自同步辐射并且伽马射线发射主要是强子过程，那么磁场强度需要高于银河系平均值（大于8微高斯）。如果X射线是热起源的，那么所测量的延伸范围、温度和等离子体密度可以用光度约为$2× 10^{39}$erg s$^{-1}$的喷射流来解释，这与该系统的爱丁顿光度相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08089v2">PDF</a> 9 pages, 5 figures, accepted for publication in ApJL</p>
<p><strong>Summary</strong><br>     近期对V4641 Sagittarii的高能伽马射线检测显示其为第二个确认的PeVatron微类星。使用XRISM对V4641 Sgr进行观测，其X射线成像光谱仪首次检测到明显的X射线扩展发射。空间范围估计为半径约7±3弧分（距离地球约6.2千秒差距时的距离），表明粒子加速部位可能位于微类星周围约10秒内区域。研究提出了增强磁场或抑制扩散系数的假设来解释这种空间范围的延伸性。对磁场强度有更高要求，如果X射线是同步辐射，伽马射线主要是强子产生，那么磁场强度必须高于银河系平均值。如果X射线是热起源，则可以解释射流现象，射流亮度与系统的爱丁顿亮度相当。 </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>V4641 Sagittarii（V4641 Sgr）已成为第二个确认的PeVatron微类星。它的伽马射线能量极高，达到约0.8 peta-electronvolt。</li>
<li>使用XRISM观测显示其周围的X射线扩展发射首次被检测到。</li>
<li>通过成像和光谱分析发现X射线辐射区域呈高斯分布形态，估计其半径为约$7 \pm 3$弧分，推测粒子加速部位距离微类星不超过约10秒差距。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33a5d6ce77b16f42a3a984a3e385104a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72c15493fab70e3833a02e374cd4b4b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee798d1003758e369c83e7667a798c19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e739af33d407bc0df04a94b658e461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4354e72bf6504d124144b3307fd5725b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b880e3fd2f1a9acd5075154c622764f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>最近，基于深度学习的联合源信道编码（DJSCC）的最新进展为端到端语义图像传输展现出了潜力。然而，大多数现有方案主要侧重于优化像素级指标，这些指标通常与人类感知不符，导致感知质量较低。在这封信中，我们提出了一种新的基于条件扩散模型生成式DJSCC方法，以提高传输图像的感知质量。具体来说，我们利用熵模型，根据传输符号的估计熵有效地管理传输带宽。这些符号然后在接收器端作为条件信息，用于指导图像重建中的条件扩散解码器。我们的模型建立在新兴的马姆巴状线性注意力（MLLA）骨架之上，该骨架在图像处理任务中表现出色，同时提供快速推理速度。此外，我们引入了一种多阶段训练策略，以确保模型的稳定性并提高其整体性能。仿真结果表明，我们提出的方法在感知质量方面显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>最新深度学习方法联合源信道编码技术提高了端到端的语义图像传输效果。但现有的方法主要集中在优化像素级指标，这往往与人类感知不一致，导致感知质量下降。本文提出一种基于条件扩散模型的生成式联合源信道编码方法，以提高传输图像的感知质量。我们利用��d模型有效管理传输带宽，基于传输符号的估计熵进行估算。这些符号在接收端作为条件信息引导图像重建的条件扩散解码器。我们的模型建立在新兴的马姆巴式线性注意力骨架上，既擅长图像处理任务，又提供快速推理速度。此外，我们引入了一种多阶段训练策略，以确保模型的稳定性并提高其整体性能。模拟结果表明，我们提出的方法在感知质量方面显著优于现有方法。</p>
<p><strong>要点</strong></p>
<ol>
<li>深度学习方法联合源信道编码技术提升语义图像传输效果。</li>
<li>现有方法主要关注像素级优化，与人类感知不一致。</li>
<li>提出基于条件扩散模型的生成式DJSCC方法提高图像感知质量。</li>
<li>利用熵模型管理传输带宽，基于传输符号的估计熵。</li>
<li>接收端利用这些符号作为条件信息引导图像重建。</li>
<li>模型基于马姆巴式线性注意力骨架，适合图像处理且快速推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-90b3a0658014bd140d80a5609c0caf99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d6eb91831daa0dcbcae7edc72d4ae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20e02759ca70cd725aed915d7feac78f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VHM-Versatile-and-Honest-Vision-Language-Model-for-Remote-Sensing-Image-Analysis"><a href="#VHM-Versatile-and-Honest-Vision-Language-Model-for-Remote-Sensing-Image-Analysis" class="headerlink" title="VHM: Versatile and Honest Vision Language Model for Remote Sensing Image   Analysis"></a>VHM: Versatile and Honest Vision Language Model for Remote Sensing Image   Analysis</h2><p><strong>Authors:Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He</strong></p>
<p>This paper develops a Versatile and Honest vision language Model (VHM) for remote sensing image analysis. VHM is built on a large-scale remote sensing image-text dataset with rich-content captions (VersaD), and an honest instruction dataset comprising both factual and deceptive questions (HnstD). Unlike prevailing remote sensing image-text datasets, in which image captions focus on a few prominent objects and their relationships, VersaD captions provide detailed information about image properties, object attributes, and the overall scene. This comprehensive captioning enables VHM to thoroughly understand remote sensing images and perform diverse remote sensing tasks. Moreover, different from existing remote sensing instruction datasets that only include factual questions, HnstD contains additional deceptive questions stemming from the non-existence of objects. This feature prevents VHM from producing affirmative answers to nonsense queries, thereby ensuring its honesty. In our experiments, VHM significantly outperforms various vision language models on common tasks of scene classification, visual question answering, and visual grounding. Additionally, VHM achieves competent performance on several unexplored tasks, such as building vectorizing, multi-label classification and honest question answering. We will release the code, data and model weights at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/VHM">https://github.com/opendatalab/VHM</a> . </p>
<blockquote>
<p>本文开发了一种通用诚信视觉语言模型（VHM），用于遥感图像分析。VHM建立在一个大规模的遥感图像文本数据集（VersaD）和包含事实和欺骗性问题的诚信指令数据集（HnstD）之上。与现有的遥感图像文本数据集不同，VersaD的标题不仅关注一些突出对象及其关系，而且提供关于图像属性、对象属性和整体场景的综合信息。这种全面的标题功能使VHM能够充分了解遥感图像并执行多种遥感任务。此外，与仅包含事实问题的现有遥感指令数据集不同，HnstD还包含由对象不存在引起的欺骗性问题。这一特点使VHM不会为非常识查询给出肯定答案，从而确保其诚信。在我们的实验中，VHM在场景分类、视觉问答和视觉定位等常见任务上的表现优于各种视觉语言模型。此外，VHM在未探索的任务上也表现出了不俗的性能，如建筑矢量化、多标签分类和诚信问答等。我们将在<a target="_blank" rel="noopener" href="https://github.com/opendatalab/VHM%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BB%A3%E7%A0%81%E3%80%81%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/opendatalab/VHM上发布代码、数据和模型权重。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.20213v4">PDF</a> Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding   author: Gui-Song Xia, Conghui He</p>
<p><strong>Summary</strong><br>医学图像研究领域的一篇论文介绍了一种通用且诚实的视觉语言模型（VHM），用于遥感图像分析。该模型基于大规模遥感图像文本数据集（VersaD）和诚实指令数据集（HnstD）。VersaD数据集提供了详细的图像属性、对象特征和整体场景信息，使VHM能够全面理解遥感图像并执行各种遥感任务。HnstD数据集包含欺诈性问题，使模型不会对所有查询给出肯定答复，确保模型的诚实性。在场景分类、视觉问答和视觉定位等常见任务上，VHM显著优于其他视觉语言模型，并在建筑矢量化、多标签分类和诚实问答等未探索的任务上表现出竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VHM模型是一种用于遥感图像分析的通用和诚实的视觉语言模型。</li>
<li>VHM建立在两个数据集上：提供详细图像信息的VersaD数据集和包含欺骗性问题的HnstD数据集。</li>
<li>VersaD数据集的丰富内容描述有助于模型全面理解遥感图像。</li>
<li>HnstD数据集增强了模型的诚实度，使其不会对所有查询给出肯定答复。</li>
<li>VHM在场景分类、视觉问答和视觉定位等任务上表现出卓越性能。</li>
<li>VHM在未经探索的任务（如建筑矢量化、多标签分类和诚实问答）上也表现出竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.20213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e2b50aa4e94067749437ae796545ca12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cccabc1387ce3837549461306c6df77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2974e1e710d786597f0ac0a6d93a1372.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09adb95bcf771f6c55b0cfc991c21b46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-861ce231df7de364514a7c3bbc29265a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92b24cb9b1047cdf7319e3e02e6cefd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-536e51d537e1a51b989f9e460adb5bfb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d042495ce62f0174fb10b9a1c00aa7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b88cd905be18d6f72a346373bb015fb3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-68a361198e5e1f8fe7228d19d7a3b863.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2024-12-21  ProsodyFM Unsupervised Phrasing and Intonation Control for Intelligible   Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-750202e1a98a34f8ad6157aefde1aebd.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-21  LeviTor 3D Trajectory Oriented Image-to-Video Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
