<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  Preventing Local Pitfalls in Vector Quantization via Optimal Transport">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    71 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-21-æ›´æ–°"><a href="#2024-12-21-æ›´æ–°" class="headerlink" title="2024-12-21 æ›´æ–°"></a>2024-12-21 æ›´æ–°</h1><h2 id="Preventing-Local-Pitfalls-in-Vector-Quantization-via-Optimal-Transport"><a href="#Preventing-Local-Pitfalls-in-Vector-Quantization-via-Optimal-Transport" class="headerlink" title="Preventing Local Pitfalls in Vector Quantization via Optimal Transport"></a>Preventing Local Pitfalls in Vector Quantization via Optimal Transport</h2><p><strong>Authors:Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</strong></p>
<p>Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality. </p>
<blockquote>
<p>å‘é‡é‡åŒ–ç½‘ç»œï¼ˆVQNsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å®¹æ˜“é‡åˆ°è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¿™ç”±äºéœ€è¦å¾®å¦™åˆå§‹åŒ–å’Œæ¨¡å‹è’¸é¦ç­‰æŠ€æœ¯è€Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹å¤æ‚åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šå±€éƒ¨æœ€å°å€¼é—®é¢˜æ˜¯è¿™ç§ä¸ç¨³å®šæ€§çš„ä¸»è¦åŸå› ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ•´åˆäº†ä¸€ç§æœ€ä¼˜ä¼ è¾“æ–¹æ³•ï¼Œä»¥æ›¿ä»£æœ€è¿‘é‚»æœç´¢ï¼Œå®ç°æ›´å…¨å±€çš„ä¿¡æ¯åˆ†é…ã€‚æˆ‘ä»¬å¼•å…¥äº†OptVQï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‘é‡é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨Sinkhornç®—æ³•æ¥è§£å†³æœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼Œä»è€Œæé«˜è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†å‡è½»ä¸åŒæ•°æ®åˆ†å¸ƒå¯¹Sinkhornç®—æ³•çš„å½±å“ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å½’ä¸€åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒOptVQå®ç°äº†100%çš„ä»£ç æœ¬åˆ©ç”¨ç‡ï¼Œå¹¶åœ¨é‡å»ºè´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„VQNsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15195v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/zbr17/OptVQ">https://github.com/zbr17/OptVQ</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Vector-quantizedç½‘ç»œï¼ˆVQNsï¼‰çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œå¹¶è¯†åˆ«å‡ºå±€éƒ¨æœ€å°å€¼é—®é¢˜ä¸ºä¸»è¦åŸå› ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹å‘é‡é‡åŒ–æ–¹æ³•OptVQï¼Œé‡‡ç”¨Sinkhornç®—æ³•ä¼˜åŒ–æœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼Œä»è€Œæé«˜è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚é€šè¿‡å®æ–½æœ‰æ•ˆçš„å½’ä¸€åŒ–ç­–ç•¥ï¼Œå‡è½»äº†ä¸åŒæ•°æ®åˆ†å¸ƒå¯¹Sinkhornç®—æ³•çš„å½±å“ã€‚åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOptVQå®ç°äº†100%çš„ä»£ç æœ¬åˆ©ç”¨ç‡ï¼Œå¹¶åœ¨é‡å»ºè´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„VQNsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vector-quantizedç½‘ç»œï¼ˆVQNsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>å±€éƒ¨æœ€å°å€¼é—®é¢˜æ˜¯å¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„ä¸»è¦åŸå› ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å‘é‡é‡åŒ–æ–¹æ³•OptVQï¼Œä½¿ç”¨Sinkhornç®—æ³•ä¼˜åŒ–æœ€ä¼˜ä¼ è¾“é—®é¢˜ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚</li>
<li>OptVQé€šè¿‡å®æ–½æœ‰æ•ˆçš„å½’ä¸€åŒ–ç­–ç•¥ï¼Œå‡è½»äº†ä¸åŒæ•°æ®åˆ†å¸ƒå¯¹è®­ç»ƒè¿‡ç¨‹çš„å½±å“ã€‚</li>
<li>OptVQå®ç°äº†100%çš„ä»£ç æœ¬åˆ©ç”¨ç‡ã€‚</li>
<li>åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šï¼ŒOptVQè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„VQNsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbbe20b8e8337839f20c255bb4b2810a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669b555259120893e8aab2a94a5d261b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26676f827d8631e1cbf830601c361e39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8bfd0ed709ff705b9b58ee710112ad4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MultiverSeg-Scalable-Interactive-Segmentation-of-Biomedical-Imaging-Datasets-with-In-Context-Guidance"><a href="#MultiverSeg-Scalable-Interactive-Segmentation-of-Biomedical-Imaging-Datasets-with-In-Context-Guidance" class="headerlink" title="MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging   Datasets with In-Context Guidance"></a>MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging   Datasets with In-Context Guidance</h2><p><strong>Authors:Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca</strong></p>
<p>Medical researchers and clinicians often need to perform novel segmentation tasks on a set of related images. Existing methods for segmenting a new dataset are either interactive, requiring substantial human effort for each image, or require an existing set of manually labeled images. We introduce a system, MultiverSeg, that enables practitioners to rapidly segment an entire new dataset without requiring access to any existing labeled data from that task or domain. Along with the image to segment, the model takes user interactions such as clicks, bounding boxes or scribbles as input, and predicts a segmentation. As the user segments more images, those images and segmentations become additional inputs to the model, providing context. As the context set of labeled images grows, the number of interactions required to segment each new image decreases. We demonstrate that MultiverSeg enables users to interactively segment new datasets efficiently, by amortizing the number of interactions per image to achieve an accurate segmentation. Compared to using a state-of-the-art interactive segmentation method, using MultiverSeg reduced the total number of scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images from unseen tasks. We release code and model weights at <a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.edu/">https://multiverseg.csail.mit.edu</a> </p>
<blockquote>
<p>åŒ»å­¦ç ”ç©¶äººå‘˜å’Œä¸´åºŠåŒ»ç”Ÿç»å¸¸éœ€è¦å¯¹ä¸€ç»„ç›¸å…³å›¾åƒæ‰§è¡Œæ–°çš„åˆ†å‰²ä»»åŠ¡ã€‚ç°æœ‰åˆ†å‰²æ–°æ•°æ®é›†çš„æ–¹æ³•è¦ä¹ˆæ˜¯äº¤äº’å¼çš„ï¼Œéœ€è¦é’ˆå¯¹æ¯å¼ å›¾åƒæŠ•å…¥å¤§é‡çš„äººåŠ›ï¼Œè¦ä¹ˆéœ€è¦ç°æœ‰çš„æ‰‹åŠ¨æ ‡è®°å›¾åƒé›†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç³»ç»ŸMultiverSegï¼Œå®ƒèƒ½è®©å®è·µè€…æ— éœ€è®¿é—®è¯¥ä»»åŠ¡æˆ–é¢†åŸŸä¸­çš„ä»»ä½•ç°æœ‰æ ‡è®°æ•°æ®å³å¯å¿«é€Ÿåˆ†å‰²æ•´ä¸ªæ–°æ•°æ®é›†ã€‚é™¤å¾…åˆ†å‰²çš„å›¾åƒå¤–ï¼Œè¯¥æ¨¡å‹è¿˜æ¥å—ç”¨æˆ·äº¤äº’ä½œä¸ºè¾“å…¥ï¼Œä¾‹å¦‚ç‚¹å‡»ã€è¾¹ç•Œæ¡†æˆ–æ¶‚é¸¦ï¼Œå¹¶é¢„æµ‹åˆ†å‰²ç»“æœã€‚éšç€ç”¨æˆ·åˆ†å‰²çš„å›¾åƒè¶Šæ¥è¶Šå¤šï¼Œè¿™äº›å›¾åƒå’Œåˆ†å‰²ç»“æœæˆä¸ºæ¨¡å‹çš„é¢å¤–è¾“å…¥ï¼Œæä¾›äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚éšç€æ ‡è®°å›¾åƒä¸Šä¸‹æ–‡é›†çš„å¢é•¿ï¼Œåˆ†å‰²æ¯å¼ æ–°å›¾åƒæ‰€éœ€çš„äº¤äº’æ¬¡æ•°å‡å°‘ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯å¼ å›¾åƒæ‰€éœ€çš„äº¤äº’æ¬¡æ•°æ‘Šé”€æ¥é«˜æ•ˆåœ°å¯¹æ–°æ•°æ®é›†è¿›è¡Œäº¤äº’å¼åˆ†å‰²ï¼Œä»è€Œè¯æ˜äº†MultiverSegä½¿ç”¨æˆ·èƒ½å¤Ÿå®ç°ç²¾ç¡®åˆ†å‰²ã€‚ä¸ä½¿ç”¨æœ€å…ˆè¿›çš„äº¤äº’å¼åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨MultiverSegå‡å°‘äº†æ¶‚é¸¦æ­¥éª¤æ€»æ•°è¾¾53%ï¼Œç‚¹å‡»æ¬¡æ•°å‡å°‘36%ï¼Œåœ¨æ¥è‡ªæœªè§ä»»åŠ¡çš„å›¾åƒé›†ä¸Šè¾¾åˆ°äº†90%çš„Diceç³»æ•°ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.eduå‘å¸ƒäº†ä»£ç å’Œæ¨¡å‹æƒé‡./">https://multiverseg.csail.mit.eduå‘å¸ƒäº†ä»£ç å’Œæ¨¡å‹æƒé‡ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15058v1">PDF</a> Project Website: <a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.edu/">https://multiverseg.csail.mit.edu</a> Keywords:   interactive segmentation, in-context learning, medical image analysis,   biomedical imaging, image annotation, visual prompting</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åä¸ºMultiverSegçš„ç³»ç»Ÿï¼Œç”¨äºåœ¨æ— éœ€ä»»ä½•ç°æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å¿«é€Ÿåˆ†å‰²æ–°æ•°æ®é›†ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„äº’åŠ¨æ“ä½œï¼ˆå¦‚ç‚¹å‡»ã€ç»˜åˆ¶è¾¹ç•Œæ¡†æˆ–æ¶‚é¸¦ï¼‰è¿›è¡Œé¢„æµ‹åˆ†å‰²ã€‚éšç€ç”¨æˆ·åˆ†å‰²çš„å›¾åƒæ•°é‡çš„å¢åŠ ï¼Œè¿™äº›å›¾åƒå’Œåˆ†å‰²ç»“æœä¼šä½œä¸ºé¢å¤–çš„è¾“å…¥ï¼Œä¸ºæ¨¡å‹æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚éšç€æ ‡è®°å›¾åƒé›†çš„å¢å¤šï¼Œåˆ†å‰²æ¯å¼ æ–°å›¾åƒæ‰€éœ€çš„äº’åŠ¨æ“ä½œæ•°é‡é€æ¸å‡å°‘ã€‚MultiverSegé€šè¿‡å‡å°‘æ¶‚é¸¦æ­¥éª¤å’Œç‚¹å‡»æ¬¡æ•°ï¼Œå®ç°äº†å¯¹æœªè§ä»»åŠ¡å›¾åƒé›†çš„å‡†ç¡®åˆ†å‰²ã€‚å…¶ä»£ç å’Œæ¨¡å‹æƒé‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://multiverseg.csail.mit.eduä¸Š./">https://multiverseg.csail.mit.eduä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MultiverSegç³»ç»Ÿèƒ½å¤Ÿåœ¨æ— éœ€ä»»ä½•ç°æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å¿«é€Ÿåˆ†å‰²æ–°æ•°æ®é›†ã€‚</li>
<li>ç”¨æˆ·å¯é€šè¿‡æä¾›äº’åŠ¨æ“ä½œï¼ˆå¦‚ç‚¹å‡»ã€ç»˜åˆ¶è¾¹ç•Œæ¡†æˆ–æ¶‚é¸¦ï¼‰æ¥æŒ‡å¯¼ç³»ç»Ÿå®Œæˆé¢„æµ‹åˆ†å‰²ã€‚</li>
<li>éšç€ç”¨æˆ·åˆ†å‰²çš„å›¾åƒæ•°é‡å¢åŠ ï¼Œç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ›´ä¸°å¯Œï¼Œåˆ†å‰²æ•ˆç‡é€æ¸æé«˜ã€‚</li>
<li>MultverSegé€šè¿‡å‡å°‘æ¶‚é¸¦æ­¥éª¤å’Œç‚¹å‡»æ¬¡æ•°ï¼Œæé«˜äº†åˆ†å‰²æ–°å›¾åƒçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ç­‰éœ€è¦é«˜æ•ˆã€å‡†ç¡®å¤„ç†å¤§é‡å›¾åƒçš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>MultverSegçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„äº¤äº’å¼åˆ†å‰²æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æœªè§ä»»åŠ¡å›¾åƒé›†ä¸Šå®ç°é«˜å‡†ç¡®ç‡çš„åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e98a6565a4866bdbad84393ac1b9679e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5319051f5cf3239f7ccae88ff2c049ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015b5ab92f3167834b3a366e11937115.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d02ffb1e1020e16cccb48db73ba0b71.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Joint-estimation-of-activity-attenuation-and-motion-in-respiratory-self-gated-time-of-flight-PET"><a href="#Joint-estimation-of-activity-attenuation-and-motion-in-respiratory-self-gated-time-of-flight-PET" class="headerlink" title="Joint estimation of activity, attenuation and motion in   respiratory-self-gated time-of-flight PET"></a>Joint estimation of activity, attenuation and motion in   respiratory-self-gated time-of-flight PET</h2><p><strong>Authors:Masoud Elhamiasl, Frederic Jolivet, Ahmadreza Rezaei, Michael Fieseler, Klaus SchÃ¤fers, Johan Nuyts, Georg Schramm, Fernando Boada</strong></p>
<p>Whole-body PET imaging is often hindered by respiratory motion during acquisition, causing significant degradation in the quality of reconstructed activity images. An additional challenge in PET&#x2F;CT imaging arises from the respiratory phase mismatch between CT-based attenuation correction and PET acquisition, leading to attenuation artifacts. To address these issues, we propose two new, purely data-driven methods for the joint estimation of activity, attenuation, and motion in respiratory self-gated TOF PET. These methods enable the reconstruction of a single activity image free from motion and attenuation artifacts.   The proposed methods were evaluated using data from the anthropomorphic Wilhelm phantom acquired on a Siemens mCT PET&#x2F;CT system, as well as 3 clinical FDG PET&#x2F;CT datasets acquired on a GE DMI PET&#x2F;CT system. Image quality was assessed visually to identify motion and attenuation artifacts. Lesion uptake values were quantitatively compared across reconstructions without motion modeling, with motion modeling but static attenuation correction, and with our proposed methods.   For the Wilhelm phantom, the proposed methods delivered image quality closely matching the reference reconstruction from a static acquisition. The lesion-to-background contrast for a liver dome lesion improved from 2.0 (no motion correction) to 5.2 (proposed methods), matching the contrast from the static acquisition (5.2). In contrast, motion modeling with static attenuation correction yielded a lower contrast of 3.5. In patient datasets, the proposed methods successfully reduced motion artifacts in lung and liver lesions and mitigated attenuation artifacts, demonstrating superior lesion to background separation.   Our proposed methods enable the reconstruction of a single, high-quality activity image that is motion-corrected and free from attenuation artifacts, without the need for external hardware. </p>
<blockquote>
<p>å…¨èº«PETæˆåƒåœ¨é‡‡é›†è¿‡ç¨‹ä¸­ç»å¸¸å—åˆ°å‘¼å¸è¿åŠ¨çš„å½±å“ï¼Œå¯¼è‡´é‡å»ºåçš„æ´»åŠ¨å›¾åƒè´¨é‡æ˜¾è‘—ä¸‹é™ã€‚PET&#x2F;CTæˆåƒä¸­å¦ä¸€ä¸ªæŒ‘æˆ˜æ¥è‡ªäºåŸºäºCTçš„è¡°å‡æ ¡æ­£ä¸PETé‡‡é›†ä¹‹é—´å‘¼å¸é˜¶æ®µçš„ä¸åŒ¹é…ï¼Œä»è€Œå¯¼è‡´è¡°å‡ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸¤ç§å…¨æ–°çš„ã€çº¯ç²¹çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œç”¨äºè”åˆä¼°è®¡å‘¼å¸è‡ªé—¨æ§TOF PETä¸­çš„æ´»åŠ¨ã€è¡°å‡å’Œè¿åŠ¨ã€‚è¿™äº›æ–¹æ³•èƒ½å¤Ÿé‡å»ºä¸€ä¸ªä¸å—è¿åŠ¨å’Œè¡°å‡ä¼ªå½±å½±å“çš„å•ä¸€æ´»åŠ¨å›¾åƒã€‚æ‰€æå‡ºçš„æ–¹æ³•ä½¿ç”¨åœ¨è¥¿é—¨å­mCT PET&#x2F;CTç³»ç»Ÿä¸Šè·å–çš„å¨å°”å§†äººå½¢ Phantom æ•°æ®ä»¥åŠé€šç”¨ç”µæ°”DMI PET&#x2F;CTç³»ç»Ÿä¸Šè·å–çš„3ä¸ªä¸´åºŠFDG PET&#x2F;CTæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ã€‚é€šè¿‡è§†è§‰è¯„ä¼°å›¾åƒè´¨é‡ï¼Œä»¥è¯†åˆ«è¿åŠ¨å’Œè¡°å‡ä¼ªå½±ã€‚é€šè¿‡å®šé‡æ¯”è¾ƒé‡å»ºå›¾åƒä¸­çš„ç—…ç¶æ‘„å–å€¼ï¼ŒåŒ…æ‹¬æ— è¿åŠ¨å»ºæ¨¡çš„é‡å»ºã€æœ‰è¿åŠ¨å»ºæ¨¡ä½†é™æ€è¡°å‡æ ¡æ­£çš„é‡å»ºä»¥åŠæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚å¯¹äºå¨å°”å§† Phantomï¼Œæ‰€æå‡ºçš„æ–¹æ³•æä¾›çš„å›¾åƒè´¨é‡ä¸æ¥è‡ªé™æ€é‡‡é›†çš„å‚è€ƒé‡å»ºå›¾åƒéå¸¸æ¥è¿‘ã€‚å¯¹äºè‚ç©¹é¡¶ç—…ç¶ï¼Œç—…ç¶ä¸èƒŒæ™¯çš„å¯¹æ¯”åº¦ä»2.0ï¼ˆæ— è¿åŠ¨æ ¡æ­£ï¼‰æé«˜åˆ°5.2ï¼ˆæ‰€æå‡ºçš„æ–¹æ³•ï¼‰ï¼Œä¸é™æ€é‡‡é›†çš„å¯¹æ¯”åº¦ç›¸åŒ¹é…ï¼ˆ5.2ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¸¦æœ‰é™æ€è¡°å‡æ ¡æ­£çš„è¿åŠ¨å»ºæ¨¡äº§ç”Ÿçš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œä¸º3.5ã€‚åœ¨æ‚£è€…æ•°æ®é›†ä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æˆåŠŸå‡å°‘äº†è‚ºéƒ¨å’Œè‚è„ç—…ç¶çš„è¿åŠ¨ä¼ªå½±ï¼Œå¹¶å‡è½»äº†è¡°å‡ä¼ªå½±ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„ç—…ç¶ä¸èƒŒæ™¯åˆ†ç¦»èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé‡å»ºä¸€ä¸ªå•ä¸€çš„é«˜è´¨é‡æ´»åŠ¨å›¾åƒï¼Œè¯¥å›¾åƒç»è¿‡è¿åŠ¨æ ¡æ­£ï¼Œæ— è¡°å‡ä¼ªå½±ï¼Œä¸”æ— éœ€å¤–éƒ¨ç¡¬ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15018v1">PDF</a> 18 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong><br>     å…¨èº«PETæˆåƒè¿‡ç¨‹ä¸­ç”±äºå‘¼å¸è¿åŠ¨å¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜ã€‚ä¸ºè§£å†³PET&#x2F;CTæˆåƒä¸­åŸºäºCTçš„è¡°å‡æ ¡æ­£ä¸PETé‡‡é›†ä¹‹é—´å‘¼å¸ç›¸ä½ä¸åŒ¹é…å¯¼è‡´çš„è¡°å‡ä¼ªå½±é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§å…¨æ–°çš„ã€çº¯ç²¹çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œç”¨äºè”åˆä¼°è®¡æ´»åŠ¨ã€è¡°å‡å’Œè¿åŠ¨ã€‚è¿™äº›æ–¹æ³•èƒ½å¤Ÿé‡å»ºå‡ºæ— è¿åŠ¨å’Œè¡°å‡ä¼ªå½±çš„é«˜è´¨é‡æ´»åŠ¨å›¾åƒã€‚é€šè¿‡é‡‡ç”¨Siemens mCT PET&#x2F;CTç³»ç»Ÿçš„äººä½“ä»¿çœŸWilhelm Phantomæ•°æ®å’ŒGE DMI PET&#x2F;CTç³»ç»Ÿçš„å®é™…ä¸´åºŠFDG PET&#x2F;CTæ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•é‡å»ºå‡ºçš„å›¾åƒè´¨é‡æ¥è¿‘é™æ€é‡‡é›†çš„å‚è€ƒé‡å»ºå›¾åƒã€‚ç‰¹åˆ«æ˜¯åœ¨è‚é¡¶ç—…ç¶æ–¹é¢ï¼Œç›¸å¯¹äºæ²¡æœ‰è¿åŠ¨æ ¡æ­£çš„å›¾åƒï¼Œæœ¬æ–‡æ–¹æ³•çš„ç—…ç¶ä¸èƒŒæ™¯å¯¹æ¯”åº¦æœ‰äº†æ˜¾è‘—æå‡ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ¬æ–‡æ–¹æ³•æ— éœ€é¢å¤–çš„ç¡¬ä»¶è®¾å¤‡å³å¯å®ç°æ— è¿åŠ¨æ ¡æ­£ã€æ— è¡°å‡ä¼ªå½±çš„é«˜è´¨é‡æ´»åŠ¨å›¾åƒé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨èº«PETæˆåƒå—åˆ°å‘¼å¸è¿åŠ¨çš„å½±å“ï¼Œå¯¼è‡´é‡å»ºçš„æ´»åŠ¨å›¾åƒè´¨é‡ä¸‹é™ã€‚</li>
<li>å‘¼å¸è¿åŠ¨åœ¨PET&#x2F;CTæˆåƒä¸­å¼•èµ·çš„æŒ‘æˆ˜åŒ…æ‹¬æ´»åŠ¨ä¼°è®¡å’Œè¡°å‡æ ¡æ­£çš„ä¸å‡†ç¡®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°äº†å¯¹æ´»åŠ¨ã€è¡°å‡å’Œè¿åŠ¨çš„è”åˆä¼°è®¡ã€‚</li>
<li>ä½¿ç”¨Wilhelm Phantomæ•°æ®å’Œå®é™…ä¸´åºŠæ•°æ®é›†éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•èƒ½å¤Ÿé‡å»ºå‡ºé«˜è´¨é‡çš„æ´»åŠ¨å›¾åƒï¼Œæ¶ˆé™¤è¿åŠ¨å’Œè¡°å‡ä¼ªå½±ã€‚</li>
<li>åœ¨è‚é¡¶ç—…ç¶æ–¹é¢ï¼Œç›¸å¯¹äºæ²¡æœ‰è¿åŠ¨æ ¡æ­£çš„å›¾åƒï¼Œæœ¬æ–‡æ–¹æ³•çš„ç—…ç¶ä¸èƒŒæ™¯å¯¹æ¯”åº¦æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b43853ef115cdcc2734dfdaa796b059.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24c7be63bf28b2baff4450b8a6650eb3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Accessing-the-topological-properties-of-human-brain-functional-sub-circuits-in-Echo-State-Networks"><a href="#Accessing-the-topological-properties-of-human-brain-functional-sub-circuits-in-Echo-State-Networks" class="headerlink" title="Accessing the topological properties of human brain functional   sub-circuits in Echo State Networks"></a>Accessing the topological properties of human brain functional   sub-circuits in Echo State Networks</h2><p><strong>Authors:Bach Nguyen, Tianlong Chen, Shu Yang, Bojian Hou, Li Shen, Duy Duong-Tran</strong></p>
<p>Recent years have witnessed an emerging trend in neuromorphic computing that centers around the use of brain connectomics as a blueprint for artificial neural networks. Connectomics-based neuromorphic computing has primarily focused on embedding human brain large-scale structural connectomes (SCs), as estimated from diffusion Magnetic Resonance Imaging (dMRI) modality, to echo-state networks (ESNs). A critical step in ESN embedding requires pre-determined read-in and read-out layers constructed by the induced subgraphs of the embedded reservoir. As \textit{a priori} set of functional sub-circuits are derived from functional MRI (fMRI) modality, it is unknown, till this point, whether the embedding of fMRI-induced sub-circuits&#x2F;networks onto SCs is well justified from the neuro-physiological perspective and ESN performance across a variety of tasks. This paper proposes a pipeline to implement and evaluate ESNs with various embedded topologies and processing&#x2F;memorization tasks. To this end, we showed that different performance optimums highly depend on the neuro-physiological characteristics of these pre-determined fMRI-induced sub-circuits. In general, fMRI-induced sub-circuit-embedded ESN outperforms simple bipartite and various null models with feed-forward properties commonly seen in MLP for different tasks and reservoir criticality conditions. We provided a thorough analysis of the topological properties of pre-determined fMRI-induced sub-circuits and highlighted their graph-theoretical properties that play significant roles in determining ESN performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç¥ç»å½¢æ€è®¡ç®—é¢†åŸŸå‡ºç°äº†ä¸€ä¸ªä»¥è„‘è¿æ¥ç»„ä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œè“å›¾çš„æ–°å…´è¶‹åŠ¿ã€‚åŸºäºè¿æ¥ç»„çš„ç¥ç»å½¢æ€è®¡ç®—ä¸»è¦å…³æ³¨å°†å¤§è§„æ¨¡ç»“æ„æ€§è„‘è¿æ¥ç»„ï¼ˆSCï¼‰åµŒå…¥å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNï¼‰ã€‚è¿™äº›è¿æ¥ç»„æ˜¯é€šè¿‡æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æŠ€æœ¯ä¼°è®¡å¾—åˆ°çš„ã€‚åœ¨ESNåµŒå…¥è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯æ„å»ºé¢„å®šä¹‰çš„è¾“å…¥å’Œè¾“å‡ºå±‚ï¼Œè¿™äº›å±‚ç”±åµŒå…¥å­˜å‚¨åº“çš„è¯±å¯¼å­å›¾æ„æˆã€‚ç”±äºå…ˆéªŒçš„åŠŸèƒ½æ€§å­ç”µè·¯é›†æ˜¯ä»åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æŠ€æœ¯ä¸­å¾—å‡ºçš„ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œå°šä¸æ¸…æ¥šä»ç¥ç»ç”Ÿç†å­¦è§’åº¦å°†fMRIè¯±å¯¼çš„å­ç”µè·¯&#x2F;ç½‘ç»œåµŒå…¥åˆ°SCæ˜¯å¦åˆç†ï¼Œä»¥åŠåœ¨ä¸åŒä»»åŠ¡ä¸­ESNçš„æ€§èƒ½è¡¨ç°å¦‚ä½•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç®¡é“æµç¨‹æ¥å®ç°å’Œè¯„ä¼°å…·æœ‰ä¸åŒåµŒå…¥æ‹“æ‰‘ç»“æ„å’Œå¤„ç†&#x2F;è®°å¿†ä»»åŠ¡çš„ESNã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸åŒçš„æ€§èƒ½æœ€ä¼˜å€¼åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¿™äº›é¢„å®šä¹‰çš„fMRIè¯±å¯¼å­ç”µè·¯çš„ç¥ç»ç”Ÿç†å­¦ç‰¹å¾ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒåŸºäºfMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥çš„ESNåœ¨å¤šç§ä»»åŠ¡å’Œå­˜å‚¨åº“ä¸´ç•Œæ¡ä»¶ä¸‹ï¼Œå…¶æ€§èƒ½ä¼˜äºç®€å•çš„å‰é¦ˆæ€§è´¨çš„MLPæ¨¡å‹ä¸­çš„äºŒåˆ†æ¨¡å‹å’Œå„ç§ç©ºæ¨¡å‹ã€‚æˆ‘ä»¬å¯¹é¢„å®šä¹‰çš„fMRIè¯±å¯¼å­ç”µè·¯çš„æ‹“æ‰‘ç‰¹æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å…¶åœ¨ç¡®å®šESNæ€§èƒ½ä¸­å‘æŒ¥é‡è¦ä½œç”¨çš„å›¾è®ºç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14999v1">PDF</a> 10 pages, 12 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸï¼Œç¥ç»å½¢æ€è®¡ç®—é¢†åŸŸæ¶Œç°å‡ºä¸€ç§æ–°å…´è¶‹åŠ¿ï¼Œå³ä»¥è„‘è¿æ¥ç»„ä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œè“å›¾ã€‚åŸºäºè¿æ¥ç»„çš„ç¥ç»å½¢æ€è®¡ç®—ä¸»è¦å…³æ³¨å°†äººç±»å¤§è„‘çš„å¤§è§„æ¨¡ç»“æ„è¿æ¥ç»„ï¼ˆSCsï¼‰åµŒå…¥å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNsï¼‰ã€‚ESNåµŒå…¥çš„å…³é”®æ­¥éª¤éœ€è¦é¢„å…ˆç¡®å®šçš„è¯»å†™å±‚ï¼Œè¿™äº›å±‚ç”±åµŒå…¥å­˜å‚¨åº“çš„è¯±å¯¼å­å›¾æ„å»ºè€Œæˆã€‚å°½ç®¡ä»åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ¨¡æ€ä¸­è¡ç”Ÿå‡ºå…ˆéªŒçš„åŠŸèƒ½æ€§å­ç”µè·¯é›†ï¼Œä½†å°šä¸æ¸…æ¥šä»ç¥ç»ç”Ÿç†å­¦çš„è§’åº¦å°†fMRIè¯±å¯¼çš„å­ç”µè·¯&#x2F;ç½‘ç»œåµŒå…¥SCsæ˜¯å¦åˆç†ï¼Œä»¥åŠåœ¨ä¸åŒä»»åŠ¡ä¸­ESNçš„æ€§èƒ½è¡¨ç°å¦‚ä½•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€æ¡ç®¡é“ï¼Œç”¨äºå®ç°å’Œè¯„ä¼°å…·æœ‰å„ç§åµŒå…¥æ‹“æ‰‘ç»“æ„å’Œå¤„ç†&#x2F;è®°å¿†ä»»åŠ¡çš„ESNsã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŒçš„æ€§èƒ½æœ€ä¼˜å€¼é«˜åº¦ä¾èµ–äºè¿™äº›é¢„å…ˆç¡®å®šçš„fMRIè¯±å¯¼å­ç”µè·¯çš„ç¥ç»ç”Ÿç†å­¦ç‰¹å¾ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒfMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥çš„ESNä¼˜äºåœ¨å¤šå±‚æ„ŸçŸ¥å™¨ä¸­å¸¸è§çš„å…·æœ‰å‰é¦ˆç‰¹æ€§çš„ç®€å•äºŒåˆ†å›¾å’Œå„ç§ç©ºæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ä»»åŠ¡å’Œå­˜å‚¨åº“ä¸´ç•Œæ¡ä»¶ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ¬æ–‡å½»åº•åˆ†æäº†é¢„å…ˆç¡®å®šçš„fMRIè¯±å¯¼å­ç”µè·¯æ‹“æ‰‘å±æ€§ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å…¶åœ¨ç¡®å®šESNæ€§èƒ½ä¸­å‘æŒ¥é‡è¦ä½œç”¨çš„å›¾è®ºå±æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»å½¢æ€è®¡ç®—é¢†åŸŸæ­£å…³æ³¨å°†è„‘è¿æ¥ç»„ä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œè“å›¾çš„æ–¹æ³•ã€‚</li>
<li>åŸºäºè¿æ¥ç»„çš„ç¥ç»å½¢æ€è®¡ç®—é‡ç‚¹åœ¨äºå°†ç»“æ„è¿æ¥ç»„ï¼ˆSCsï¼‰åµŒå…¥å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNsï¼‰ã€‚</li>
<li>ESNåµŒå…¥æ¶‰åŠé¢„å®šä¹‰çš„è¯»å†™å±‚ï¼Œè¿™äº›å±‚ç”±åµŒå…¥å­˜å‚¨åº“çš„è¯±å¯¼å­å›¾æ„æˆã€‚</li>
<li>å°šä¸æ¸…æ¥šä»ç¥ç»ç”Ÿç†å­¦è§’åº¦å°†åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰è¯±å¯¼çš„å­ç”µè·¯&#x2F;ç½‘ç»œåµŒå…¥SCsçš„åˆç†æ€§ã€‚</li>
<li>fMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥çš„ESNåœ¨ä¸åŒä»»åŠ¡å’Œå­˜å‚¨åº“ä¸´ç•Œæ¡ä»¶ä¸‹çš„æ€§èƒ½ä¼˜äºç®€å•æ¨¡å‹å’Œå¸¸è§çš„å‰é¦ˆç‰¹æ€§ã€‚</li>
<li>fMRIè¯±å¯¼å­ç”µè·¯çš„æ‹“æ‰‘å±æ€§å¯¹ç¡®å®šESNæ€§èƒ½èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d29e78debcbd670fc5998cbea2f564e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4d5fe7d7772e2dbd9cd06c1f10fd162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57b2031a63241735afef8a86d82238c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819f1159b2980d646f3cf55b54f002f1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Artifact2Artifact-Self-incentive-artifact-removal-for-photoacoustic-imaging-without-any-data"><a href="#Zero-Shot-Artifact2Artifact-Self-incentive-artifact-removal-for-photoacoustic-imaging-without-any-data" class="headerlink" title="Zero-Shot Artifact2Artifact: Self-incentive artifact removal for   photoacoustic imaging without any data"></a>Zero-Shot Artifact2Artifact: Self-incentive artifact removal for   photoacoustic imaging without any data</h2><p><strong>Authors:Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li</strong></p>
<p>Photoacoustic imaging (PAI) uniquely combines optical contrast with the penetration depth of ultrasound, making it critical for clinical applications. However, the quality of 3D PAI is often degraded due to reconstruction artifacts caused by the sparse and angle-limited configuration of detector arrays. Existing iterative or deep learning-based methods are either time-consuming or require large training datasets, significantly limiting their practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a zero-shot self-supervised artifact removal method based on a super-lightweight network, which leverages the fact that reconstruction artifacts are sensitive to irregularities caused by data loss. By introducing random perturbations to the acquired PA data, it spontaneously generates subset data, which in turn stimulates the network to learn the artifact patterns in the reconstruction results, thus enabling zero-shot artifact removal. This approach requires neither training data nor prior knowledge of the artifacts, and is capable of artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or slice images in 3D PAI acquired with arbitrarily sparse or angle-limited detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in both simulation study and $ in\ vivo $ animal experiments. Results demonstrate that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing zero-shot methods, and for the $ in\ vivo $ rat liver, ZS-A2A improves CNR from 17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in the following GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/JaegerCQ/ZS-A2A">https://github.com/JaegerCQ/ZS-A2A</a>. </p>
<blockquote>
<p>å…‰å£°æˆåƒï¼ˆPAIï¼‰ç‹¬ç‰¹åœ°ç»“åˆäº†å…‰å­¦å¯¹æ¯”åº¦å’Œè¶…å£°çš„ç©¿é€æ·±åº¦ï¼Œä½¿å…¶æˆä¸ºä¸´åºŠåº”ç”¨çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºæ¢æµ‹å™¨é˜µåˆ—é…ç½®ç¨€ç–ä¸”è§’åº¦å—é™å¯¼è‡´çš„é‡å»ºä¼ªå½±ï¼Œå¸¸å¸¸ä¼šå¯¼è‡´ä¸‰ç»´å…‰å£°æˆåƒï¼ˆ3D PAIï¼‰çš„è´¨é‡ä¸‹é™ã€‚ç°æœ‰çš„è¿­ä»£æˆ–åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è¦ä¹ˆè€—æ—¶è¿‡é•¿ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæå¤§åœ°é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†Zero-Shot Artifact2Artifactï¼ˆZS-A2Aï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¶…è½»é‡çº§ç½‘ç»œçš„é›¶æ ·æœ¬è‡ªç›‘ç£ä¼ªå½±å»é™¤æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é‡å»ºä¼ªå½±å¯¹ç”±æ•°æ®ä¸¢å¤±å¼•èµ·çš„ä¸è§„åˆ™æ€§çš„æ•æ„Ÿæ€§ã€‚é€šè¿‡å¯¹è·å–çš„PAæ•°æ®å¼•å…¥éšæœºæ‰°åŠ¨ï¼Œå®ƒè‡ªå‘åœ°ç”Ÿæˆå­é›†æ•°æ®ï¼Œä»è€Œåˆºæ¿€ç½‘ç»œå­¦ä¹ é‡å»ºç»“æœä¸­çš„ä¼ªå½±æ¨¡å¼ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬ä¼ªå½±å»é™¤ã€‚è¿™ç§æ–¹æ³•æ—¢ä¸éœ€è¦è®­ç»ƒæ•°æ®ï¼Œä¹Ÿä¸éœ€è¦å…³äºä¼ªå½±çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¯¹3D PAIè¿›è¡Œä¼ªå½±å»é™¤ã€‚å¯¹äºä½¿ç”¨ä»»æ„ç¨€ç–æˆ–è§’åº¦å—é™çš„æ¢æµ‹å™¨é˜µåˆ—è·å¾—çš„ä¸‰ç»´å…‰å£°æˆåƒä¸­çš„æœ€å¤§æŒ¯å¹…æŠ•å½±ï¼ˆMAPï¼‰å›¾åƒæˆ–åˆ‡ç‰‡å›¾åƒï¼ŒZS-A2Aé‡‡ç”¨è‡ªæˆ‘æ¿€åŠ±ç­–ç•¥æ¥å®Œæˆä¼ªå½±å»é™¤ï¼Œå¹¶æé«˜äº†ä¿¡å™ªæ¯”ï¼ˆCNRï¼‰ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ¨¡æ‹Ÿç ”ç©¶å’Œä½“å†…åŠ¨ç‰©å®éªŒéªŒè¯äº†ZS-A2Aã€‚ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„é›¶æ ·æœ¬æ–¹æ³•ç›¸æ¯”ï¼ŒZS-A2Aè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¯¹äºä½“å†…å¤§é¼ è‚è„ï¼ŒZS-A2Aåœ¨8ç§’å†…å°†CNRä»17.48æé«˜åˆ°43.46ã€‚ZS-A2Aé¡¹ç›®å°†åœ¨ä»¥ä¸‹GitHubä»“åº“ä¸­æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/JaegerCQ/ZS-A2A%E3%80%82">https://github.com/JaegerCQ/ZS-A2Aã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé›¶æ ·æœ¬è‡ªç›‘ç£å­¦ä¹ çš„å…‰å£°æˆåƒï¼ˆPAIï¼‰é‡å»ºä¼ªå½±å»é™¤æ–¹æ³•â€”â€”Zero-Shot Artifact2Artifactï¼ˆZS-A2Aï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¶…è½»é‡çº§ç½‘ç»œï¼Œé€šè¿‡å¼•å…¥éšæœºæ‰°åŠ¨åˆºæ¿€ç½‘ç»œå­¦ä¹ é‡å»ºç»“æœä¸­çš„ä¼ªå½±æ¨¡å¼ï¼Œå®ç°é›¶æ ·æœ¬ä¼ªå½±å»é™¤ï¼Œæ— éœ€è®­ç»ƒæ•°æ®å’Œå…ˆéªŒçŸ¥è¯†ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒZS-A2Aåœ¨æ¨¡æ‹Ÿå’Œä½“å†…åŠ¨ç‰©å®éªŒä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ç›¸æ¯”è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå¯åœ¨çŸ­æ—¶é—´å†…æ˜¾è‘—æé«˜å¯¹æ¯”å™ªå£°æ¯”ï¼ˆCNRï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAIç»“åˆäº†å…‰å­¦å¯¹æ¯”åº¦å’Œè¶…å£°ç©¿é€æ·±åº¦ï¼Œå¯¹äºä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>3D PAIçš„è´¨é‡å› é‡å»ºä¼ªå½±è€Œé™ä½ï¼Œè¿™äº›ä¼ªå½±ç”±æ£€æµ‹å™¨é˜µåˆ—çš„ç¨€ç–å’Œè§’åº¦é™åˆ¶é…ç½®å¼•èµ·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚è¿­ä»£æˆ–æ·±åº¦å­¦ä¹ æ–¹æ³•è€—æ—¶æˆ–éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>ZS-A2Aæ˜¯ä¸€ç§é›¶æ ·æœ¬è‡ªç›‘ç£ä¼ªå½±å»é™¤æ–¹æ³•ï¼ŒåŸºäºè¶…è½»é‡çº§ç½‘ç»œï¼Œåˆ©ç”¨ä¼ªå½±å¯¹æ•°æ®ä¸¢å¤±å¼•èµ·çš„ä¸è§„åˆ™æ€§çš„æ•æ„Ÿæ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥éšæœºæ‰°åŠ¨åˆ°è·å–çš„PAæ•°æ®ï¼ŒZS-A2Aè‡ªå‘åœ°ç”Ÿæˆå­é›†æ•°æ®ï¼Œåˆºæ¿€ç½‘ç»œå­¦ä¹ é‡å»ºç»“æœä¸­çš„ä¼ªå½±æ¨¡å¼ã€‚</li>
<li>ZS-A2Aæ— éœ€è®­ç»ƒæ•°æ®å’Œå…ˆéªŒçŸ¥è¯†ï¼Œèƒ½å¤Ÿå»é™¤3D PAIçš„ä¼ªå½±ã€‚</li>
<li>ZS-A2Aåœ¨æ¨¡æ‹Ÿå’Œä½“å†…åŠ¨ç‰©å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„å¯¹æ¯”å™ªå£°æ¯”ï¼ˆCNRï¼‰ï¼Œå¹¶ä¸”å¤„ç†é€Ÿåº¦å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9a372bd15df0cd13c2a7449acd7a3be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5eee0138563284fbbd307d6848075a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec9d96c4e56a2ee66b5d47f0353d918e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-085003f0421491061ea1f0b7816e82fa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-Powered-Intracranial-Hemorrhage-Detection-A-Co-Scale-Convolutional-Attention-Model-with-Uncertainty-Based-Fuzzy-Integral-Operator-and-Feature-Screening"><a href="#AI-Powered-Intracranial-Hemorrhage-Detection-A-Co-Scale-Convolutional-Attention-Model-with-Uncertainty-Based-Fuzzy-Integral-Operator-and-Feature-Screening" class="headerlink" title="AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional   Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature   Screening"></a>AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional   Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature   Screening</h2><p><strong>Authors:Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan</strong></p>
<p>Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such as decreased consciousness, permanent neurological disabilities, or even death.The primary aim of this study is to detect the occurrence or non-occurrence of ICH, followed by determining the type of subdural hemorrhage (SDH). These tasks are framed as two separate binary classification problems. By adding two layers to the co-scale convolutional attention (CCA) classifier architecture, we introduce a novel approach for ICH detection. In the first layer, after extracting features from different slices of computed tomography (CT) scan images, we combine these features and select the 50 components that capture the highest variance in the data, considering them as informative features. We then assess the discriminative power of these features using the bootstrap forest algorithm, discarding those that lack sufficient discriminative ability between different classes. This algorithm explicitly determines the contribution of each feature to the final prediction, assisting us in developing an explainable AI model. The features feed into a boosting neural network as a latent feature space. In the second layer, we introduce a novel uncertainty-based fuzzy integral operator to fuse information from different CT scan slices. This operator, by accounting for the dependencies between consecutive slices, significantly improves detection accuracy. </p>
<blockquote>
<p>é¢…å†…å‡ºè¡€ï¼ˆICHï¼‰æ˜¯æŒ‡è¡€æ¶²åœ¨é¢…éª¨å†…æ³„æ¼æˆ–ç§¯èšï¼Œè¿™æ˜¯ç”±äºå¤§è„‘å†…æˆ–å‘¨å›´çš„è¡€ç®¡ç ´è£‚æ‰€å¯¼è‡´çš„ã€‚å¦‚æœè¿™ç§æƒ…å†µæœªèƒ½åŠæ—¶è¯Šæ–­å¹¶é€‚å½“æ²»ç–—ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ„è¯†å‡é€€ã€æ°¸ä¹…æ€§ç¥ç»åŠŸèƒ½éšœç¢ç”šè‡³æ­»äº¡ç­‰ä¸¥é‡å¹¶å‘ç—‡ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦ç›®çš„æ˜¯æ£€æµ‹é¢…å†…å‡ºè¡€æ˜¯å¦å‘ç”Ÿï¼Œå¹¶ç¡®å®šè››ç½‘è†œä¸‹è…”å‡ºè¡€ï¼ˆSDHï¼‰çš„ç±»å‹ã€‚è¿™ä¸¤é¡¹ä»»åŠ¡è¢«åˆ’åˆ†ä¸ºä¸¤ä¸ªå•ç‹¬çš„äºŒåˆ†ç±»é—®é¢˜ã€‚é€šè¿‡å¯¹å…±å°ºåº¦å·ç§¯æ³¨æ„åŠ›ï¼ˆCCAï¼‰åˆ†ç±»å™¨æ¶æ„å¢åŠ ä¸¤å±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ICHæ£€æµ‹æ–¹æ³•ã€‚åœ¨ç¬¬ä¸€å±‚ï¼Œä»ä¸åŒå±‚é¢çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­æå–ç‰¹å¾åï¼Œæˆ‘ä»¬ç»“åˆè¿™äº›ç‰¹å¾å¹¶é€‰æ‹©50ä¸ªç»„ä»¶ï¼Œè¿™äº›ç»„ä»¶æ•è·æ•°æ®ä¸­çš„æœ€é«˜æ–¹å·®ï¼Œè¢«è§†ä¸ºå…·æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªåŠ©æ£®æ—ç®—æ³•è¯„ä¼°è¿™äº›ç‰¹å¾çš„åˆ¤åˆ«åŠ›ï¼Œå¹¶ä¸¢å¼ƒé‚£äº›åœ¨åŒºåˆ†ä¸åŒç±»åˆ«æ—¶ç¼ºä¹è¶³å¤Ÿåˆ¤åˆ«åŠ›çš„ç‰¹å¾ã€‚è¯¥ç®—æ³•èƒ½æ˜ç¡®ç¡®å®šæ¯ä¸ªç‰¹å¾å¯¹æœ€ç»ˆé¢„æµ‹çš„è´¡çŒ®ï¼Œæœ‰åŠ©äºæˆ‘ä»¬å¼€å‘å¯è§£é‡Šçš„AIæ¨¡å‹ã€‚è¿™äº›ç‰¹å¾è¢«è¾“å…¥åˆ°ä¸€ä¸ªå¢å¼ºç¥ç»ç½‘ç»œä¸­ä½œä¸ºæ½œåœ¨ç‰¹å¾ç©ºé—´ã€‚åœ¨ç¬¬äºŒå±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ¨¡ç³Šç§¯åˆ†ç®—å­æ¥èåˆä¸åŒCTæ‰«æå±‚é¢çš„ä¿¡æ¯ã€‚è¯¥ç®—å­é€šè¿‡è€ƒè™‘ç›¸é‚»å±‚é¢ä¹‹é—´çš„ä¾èµ–æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14869v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ‘˜è¦ç®€æ´ä»‹ç»äº†ä¸€é¡¹å…³äºé¢…å†…å‡ºè¡€æ£€æµ‹ä¸è¯Šæ–­ç ”ç©¶ã€‚ç ”ç©¶ä¸­æå‡ºä½¿ç”¨ä¸¤å±‚å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œé¢…å†…å‡ºè¡€ï¼ˆICHï¼‰æ£€æµ‹ä¸ç¡¬è†œä¸‹å‡ºè¡€ï¼ˆSDHï¼‰ç±»å‹åˆ¤æ–­ã€‚ç¬¬ä¸€å±‚é€šè¿‡æå–CTæ‰«æå›¾åƒçš„ä¸åŒåˆ‡ç‰‡ç‰¹å¾ï¼Œç­›é€‰å‡ºæœ€å…·ä¿¡æ¯é‡çš„ç‰¹å¾æˆåˆ†ï¼›ç¬¬äºŒå±‚é‡‡ç”¨åŸºäºä¸ç¡®å®šæ€§çš„æ¨¡ç³Šç§¯åˆ†ç®—å­èåˆä¸åŒCTæ‰«æåˆ‡ç‰‡çš„ä¿¡æ¯ï¼Œä»¥æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚æ•´ä¸ªæ¨¡å‹æ³¨é‡è§£é‡Šæ€§ï¼Œèƒ½æ˜ç¡®å„ç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡ä¸­ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼Œä»¥ç®€åŒ–å½¢å¼å‘ˆç°ï¼š</p>
<ol>
<li>é¢…å†…å‡ºè¡€ï¼ˆICHï¼‰æ˜¯è¡€æ¶²åœ¨é¢…å†…çš„æ³„æ¼æˆ–ç§¯èšï¼Œè‹¥ä¸åŠæ—¶è¯Šæ–­å’Œé€‚å½“æ²»ç–—ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡å¹¶å‘ç—‡ï¼Œç”šè‡³æ­»äº¡ã€‚</li>
<li>æ­¤ç ”ç©¶çš„ä¸»è¦ç›®æ ‡æ˜¯æ£€æµ‹é¢…å†…å‡ºè¡€çš„å‘ç”Ÿä¸å¦ï¼Œå¹¶ç¡®å®šç¡¬è†œä¸‹å‡ºè¡€çš„ç±»å‹ã€‚</li>
<li>é‡‡ç”¨ä¸¤å±‚å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¼•å…¥æ–°å‹æ–¹æ³•ç”¨äºICHæ£€æµ‹ã€‚</li>
<li>ç¬¬ä¸€å±‚ä¸­ï¼Œä»CTæ‰«æå›¾åƒçš„ä¸åŒåˆ‡ç‰‡æå–ç‰¹å¾ï¼Œé€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„50ä¸ªç‰¹å¾ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨bootstrapæ£®æ—ç®—æ³•è¯„ä¼°å…¶åˆ¤åˆ«åŠ›ã€‚</li>
<li>å¼•å…¥è§£é‡Šæ€§AIæ¨¡å‹ï¼Œæ˜ç¡®å„ç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®ã€‚</li>
<li>ç‰¹å¾è¾“å…¥åˆ°å¢å¼ºç¥ç»ç½‘ç»œä¸­ä½œä¸ºæ½œåœ¨ç‰¹å¾ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-41b809b7909eaa6bf52d1caba95003da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc9fb89d5b40177587f047b3ed5c1ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-014a13c02c2a280a3dd0b9f1685ede77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-714966a7b6f69c810badd27439c2bcd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f85f4d323b6eba640f59f69f90dac5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Head-and-Neck-Tumor-Segmentation-of-MRI-from-Pre-and-Mid-radiotherapy-with-Pre-training-Data-Augmentation-and-Dual-Flow-UNet"><a href="#Head-and-Neck-Tumor-Segmentation-of-MRI-from-Pre-and-Mid-radiotherapy-with-Pre-training-Data-Augmentation-and-Dual-Flow-UNet" class="headerlink" title="Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy   with Pre-training, Data Augmentation and Dual Flow UNet"></a>Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy   with Pre-training, Data Augmentation and Dual Flow UNet</h2><p><strong>Authors:Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang</strong></p>
<p>Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/WltyBY/HNTS-MRG2024_train_code">https://github.com/WltyBY/HNTS-MRG2024_train_code</a>. </p>
<blockquote>
<p>å¤´é¢ˆéƒ¨è‚¿ç˜¤å’Œè½¬ç§»æ€§æ·‹å·´ç»“å¯¹äºæ²»ç–—æ–¹æ¡ˆçš„åˆ¶å®šå’Œé¢„ååˆ†æè‡³å…³é‡è¦ã€‚è¿™äº›ç»“æ„çš„ç²¾ç¡®åˆ†å‰²å’Œå®šé‡åˆ†æéœ€è¦åƒç´ çº§åˆ«çš„æ ‡æ³¨ï¼Œå› æ­¤è‡ªåŠ¨åˆ†å‰²æŠ€æœ¯å¯¹äºå¤´é¢ˆéƒ¨ç™Œç—‡çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šç§ç­–ç•¥å¯¹æ”¾ç–—å‰ï¼ˆpre-RTï¼‰å’Œæ”¾ç–—ä¸­ï¼ˆmid-RTï¼‰å›¾åƒåˆ†å‰²çš„å½±å“ã€‚å¯¹äºæ”¾ç–—å‰å›¾åƒçš„åˆ†å‰²ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†1ï¼‰å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼›2ï¼‰ä½¿ç”¨é¢„è®­ç»ƒæƒé‡å’ŒMixUpæ•°æ®å¢å¼ºæŠ€æœ¯å¢å¼ºåŒä¸€æ–¹æ³•ã€‚å¯¹äºæ”¾ç–—ä¸­å›¾åƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è®¡ç®—å‹å¥½çš„æ–°å‹ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„å…·æœ‰é’ˆå¯¹æ”¾ç–—ä¸­å›¾åƒå’Œæ³¨å†Œæ”¾ç–—å‰å›¾åƒçš„å•ç‹¬ç¼–ç å™¨ï¼Œå¹¶å¸¦æœ‰å…¶æ ‡ç­¾ã€‚æ”¾ç–—ä¸­ç¼–ç å™¨åˆ†æ”¯åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ­¥æ•´åˆæ”¾ç–—å‰å›¾åƒå’Œæ ‡ç­¾çš„ä¿¡æ¯ã€‚æˆ‘ä»¬ä»æ¯ä»½æ•°æ®ä¸­é€‰å‡ºè¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œä½¿ç”¨å…¶é¢„æµ‹ç»“æœåˆ›å»ºé›†æˆå¹³å‡å€¼æ¥è¿›è¡Œæ¨æ–­ã€‚åœ¨æœ€ç»ˆæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨HiLabçš„èšåˆDiceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸Šè¾¾åˆ°äº†æ”¾ç–—å‰åˆ†å‰²æ€§èƒ½ä¸º82.38%ï¼Œæ”¾ç–—ä¸­åˆ†å‰²æ€§èƒ½ä¸º72.53%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WltyBY/HNTS-MRG2024_train_code">https://github.com/WltyBY/HNTS-MRG2024_train_code</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14846v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†å¤šç§ç­–ç•¥åœ¨æ”¾å°„æ²»ç–—å‰åå›¾åƒåˆ†å‰²ä¸­çš„æ•ˆæœï¼Œé‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¯¹å¤´éƒ¨å’Œé¢ˆéƒ¨è‚¿ç˜¤ä»¥åŠè½¬ç§»æ€§æ·‹å·´ç»“è¿›è¡Œå‡†ç¡®åˆ†å‰²å’Œå®šé‡åˆ†æã€‚é€šè¿‡èåˆé¢„è®­ç»ƒæƒé‡å’ŒMixUpæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨æµ‹è¯•ä¸­è·å¾—è¾ƒå¥½çš„åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤´é¢ˆéƒ¨è‚¿ç˜¤å’Œè½¬ç§»æ€§æ·‹å·´ç»“çš„æ²»ç–—è§„åˆ’å’Œé¢„ååˆ†æè‡³å…³é‡è¦ï¼Œéœ€è¦å‡†ç¡®çš„åˆ†å‰²å’Œå®šé‡åˆ†æã€‚</li>
<li>è‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯å¯¹äºå¤´é¢ˆéƒ¨ç™Œç—‡çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶ä¸­ä½¿ç”¨äº†å¤šç§ç­–ç•¥è¿›è¡Œæ”¾å°„æ²»ç–—å‰åå›¾åƒçš„åˆ†å‰²ã€‚</li>
<li>å¯¹äºæ”¾å°„æ²»ç–—å‰çš„å›¾åƒåˆ†å‰²ï¼Œé‡‡ç”¨äº†å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•å’Œèåˆé¢„è®­ç»ƒæƒé‡åŠMixUpæ•°æ®å¢å¼ºæŠ€æœ¯çš„å¢å¼ºæ–¹æ³•ã€‚</li>
<li>å¯¹äºæ”¾å°„æ²»ç–—ä¸­çš„å›¾åƒåˆ†å‰²ï¼Œå¼•å…¥äº†ä¸€ç§è®¡ç®—å‹å¥½çš„ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„å…·æœ‰é’ˆå¯¹ä¸­æœŸæ”¾å°„æ²»ç–—å›¾åƒå’Œå·²æ³¨å†Œçš„å‰æœŸæ”¾å°„æ²»ç–—å›¾åƒçš„å•ç‹¬ç¼–ç å™¨ã€‚</li>
<li>ä¸­æœŸæ”¾å°„æ²»ç–—ç¼–ç å™¨åˆ†æ”¯åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ­¥æ•´åˆå‰æœŸå›¾åƒå’Œæ ‡ç­¾çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24b3b5514558de3d032519719698af60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d20352c3aa04f048cb44632cf46b006.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b192492412e5674598b5b0a9eaa49db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33f744d54a26efc5c92774c85ce8f6f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations"><a href="#Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations" class="headerlink" title="Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations"></a>Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations</h2><p><strong>Authors:Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen</strong></p>
<p>Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8% increase in success rates for complex real-world dexterous manipulation tasks. </p>
<blockquote>
<p>è¿‘æœŸæœºå™¨äººæŠ€æœ¯çš„è¿›æ­¥ä¸»è¦é›†ä¸­åœ¨å¼€å‘èƒ½å¤Ÿæ‰§è¡Œå¤šç§ä»»åŠ¡çš„ä¸€èˆ¬æ€§ç­–ç•¥ä¸Šã€‚é€šå¸¸ï¼Œè¿™äº›ç­–ç•¥åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ¥æ•è·å½“å‰è§‚å¯Ÿä¸­çš„å…³é”®ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä»¥å‰è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸»è¦ä¾èµ–äºå¯¹æ¯”å­¦ä¹ æˆ–å•å›¾åƒé‡å»ºï¼Œæ— æ³•å®Œå…¨æ•è·å¯¹äºå®ä½“ä»»åŠ¡è‡³å…³é‡è¦çš„åºåˆ—ä¿¡æ¯ã€‚æœ€è¿‘ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰å·²æ˜¾ç¤ºå‡ºå‡†ç¡®é¢„æµ‹æœªæ¥å›¾åƒåºåˆ—çš„èƒ½åŠ›ï¼Œå±•ç°å‡ºå¯¹ç‰©ç†åŠ¨æ€çš„è‰¯å¥½ç†è§£ã€‚å—VDMå¼ºå¤§è§†è§‰é¢„æµ‹èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾å®ƒä»¬å¤©ç”Ÿå°±å…·æœ‰åæ˜ ç‰©ç†ä¸–ç•Œæ¼”å˜çš„è§†è§‰è¡¨å¾ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºé¢„æµ‹æ€§è§†è§‰è¡¨å¾ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºè§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä»¥VDMsçš„é¢„æµ‹æ€§è§†è§‰è¡¨å¾ä¸ºæ¡ä»¶çš„ä¸€èˆ¬æ€§æœºå™¨äººç­–ç•¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿™äº›è¡¨å¾ï¼Œæˆ‘ä»¬èå…¥äº†å¤šæ ·åŒ–çš„äººç±»æˆ–æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆè®­ç»ƒç›®æ ‡ã€‚VPPåœ¨ä¸¤ä¸ªæ¨¡æ‹Ÿå’Œä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä¸ä¹‹å‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†28.1%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œçµå·§æ“ä½œä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜äº†28.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14803v1">PDF</a> The first two authors contribute equally. Project Page at   <a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io/">https://video-prediction-policy.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæœºå™¨äººæŠ€æœ¯çš„è¿›æ­¥ä¿ƒä½¿äº†é€šç”¨ç­–ç•¥çš„å‘å±•ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥æ‰§è¡Œå¤šç§ä»»åŠ¡ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ•æ‰å½“å‰è§‚å¯Ÿä¿¡æ¯æ˜¯å…³é”®ã€‚ç„¶è€Œï¼Œè¿‡å»ä»¥ä¸¤å›¾åƒå¯¹æ¯”å­¦ä¹ æˆ–å•å›¾åƒé‡å»ºæ–¹å¼è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ— æ³•å®Œå…¨æ•è·èº¯ä½“ä»»åŠ¡æ‰€éœ€çš„åºåˆ—ä¿¡æ¯ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½å‡†ç¡®é¢„æµ‹æœªæ¥å›¾åƒåºåˆ—ï¼Œå±•ç°å‡ºå¯¹ç‰©ç†åŠ¨æ€çš„è‰¯å¥½ç†è§£ã€‚åŸºäºVDMsçš„å¼ºå¤§è§†è§‰é¢„æµ‹èƒ½åŠ›ï¼Œæˆ‘ä»¬å‡è®¾å…¶å…·å¤‡åæ˜ ç‰©ç†ä¸–ç•Œæ¼”å˜çš„å†…åœ¨è§†è§‰è¡¨å¾ï¼Œç§°ä¸ºé¢„æµ‹æ€§è§†è§‰è¡¨å¾ã€‚æˆ‘ä»¬æå‡ºåŸºäºé¢„æµ‹æ€§è§†è§‰è¡¨å¾çš„è§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨æœºå™¨äººç­–ç•¥ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›è¡¨å¾ï¼Œæˆ‘ä»¬çº³å…¥å¤šæ ·çš„äººç±»æˆ–æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆè®­ç»ƒç›®æ ‡ã€‚VPPåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹äºå‰åºæœ€ä½³æ–¹æ¡ˆï¼Œåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†28.1%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œçµå·§æ“ä½œä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜äº†28.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸæœºå™¨äººæŠ€æœ¯å…³æ³¨å¼€å‘èƒ½æ‰§è¡Œå¤šç§ä»»åŠ¡çš„é€šç”¨ç­–ç•¥ã€‚</li>
<li>ä¼ ç»Ÿçš„è§†è§‰ç¼–ç å™¨éš¾ä»¥æ•è·é‡è¦åºåˆ—ä¿¡æ¯ï¼Œå¯¹äºèº¯ä½“ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½é¢„æµ‹æœªæ¥å›¾åƒåºåˆ—ï¼Œåæ˜ ç‰©ç†åŠ¨æ€çš„ç†è§£ã€‚</li>
<li>VDMsçš„é¢„æµ‹æ€§è§†è§‰è¡¨å¾å¯¹äºæœºå™¨äººä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºåŸºäºé¢„æµ‹æ€§è§†è§‰è¡¨å¾çš„è§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ã€‚</li>
<li>VPPåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹æ”¹è¿›æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0633cab27a1dab8bc8deae652cb614b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd64e25d03d7b9d8221e2740f77f2d95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-746a0ed8f218c569b507d8cde6f6f142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ec1419d9d6845568c183d648993c172.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MUSTER-Longitudinal-Deformable-Registration-by-Composition-of-Consecutive-Deformations"><a href="#MUSTER-Longitudinal-Deformable-Registration-by-Composition-of-Consecutive-Deformations" class="headerlink" title="MUSTER: Longitudinal Deformable Registration by Composition of   Consecutive Deformations"></a>MUSTER: Longitudinal Deformable Registration by Composition of   Consecutive Deformations</h2><p><strong>Authors:Edvard O. S. GrÃ¸dem, Donatas SedereviÄius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle BjÃ¸rnerud, Till Schellhorn, Ã˜ystein SÃ¸rensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell</strong></p>
<p>Longitudinal imaging allows for the study of structural changes over time. One approach to detecting such changes is by non-linear image registration. This study introduces Multi-Session Temporal Registration (MUSTER), a novel method that facilitates longitudinal analysis of changes in extended series of medical images. MUSTER improves upon conventional pairwise registration by incorporating more than two imaging sessions to recover longitudinal deformations. Longitudinal analysis at a voxel-level is challenging due to effects of a changing image contrast as well as instrumental and environmental sources of bias between sessions. We show that local normalized cross-correlation as an image similarity metric leads to biased results and propose a robust alternative. We test the performance of MUSTER on a synthetic multi-site, multi-session neuroimaging dataset and show that, in various scenarios, using MUSTER significantly enhances the estimated deformations relative to pairwise registration. Additionally, we apply MUSTER on a sample of older adults from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) study. The results show that MUSTER can effectively identify patterns of neuro-degeneration from T1-weighted images and that these changes correlate with changes in cognition, matching the performance of state of the art segmentation methods. By leveraging GPU acceleration, MUSTER efficiently handles large datasets, making it feasible also in situations with limited computational resources. </p>
<blockquote>
<p>çºµå‘æˆåƒå…è®¸ç ”ç©¶éšæ—¶é—´å‘ç”Ÿçš„ç»“æ„å˜åŒ–ã€‚æ£€æµ‹è¿™ç§å˜åŒ–çš„ä¸€ç§æ–¹æ³•æ˜¯å›¾åƒéçº¿æ€§é…å‡†ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†å¤šä¼šè¯æ—¶åºé…å‡†ï¼ˆMUSTERï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥ä¿ƒè¿›å¯¹ä¸€ç³»åˆ—åŒ»å­¦å›¾åƒå˜åŒ–çš„çºµå‘åˆ†æã€‚ä¸ä¼ ç»Ÿçš„é…å¯¹é…å‡†ç›¸æ¯”ï¼ŒMUSTERé€šè¿‡ç»“åˆå¤šä¸ªæˆåƒä¼šè¯æ¥æ¢å¤çºµå‘å˜å½¢ï¼Œä»è€Œæ”¹è¿›äº†ä¼ ç»Ÿé…å¯¹é…å‡†çš„å±€é™æ€§ã€‚ç”±äºå›¾åƒå¯¹æ¯”åº¦å˜åŒ–çš„å½±å“ä»¥åŠä¸åŒä¼šè¯ä¹‹é—´ä»ªå™¨å’Œç¯å¢ƒæ¥æºçš„åè§ï¼Œåœ¨ä½“ç´ æ°´å¹³ä¸Šè¿›è¡Œçºµå‘åˆ†ææ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯æ˜äº†å±€éƒ¨å½’ä¸€åŒ–äº’ç›¸å…³ä½œä¸ºå›¾åƒç›¸ä¼¼åº¦åº¦é‡ä¼šå¯¼è‡´æœ‰åç»“æœï¼Œå¹¶æå‡ºäº†ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨åˆæˆå¤šç«™ç‚¹ã€å¤šä¼šè¯çš„ç¥ç»æˆåƒæ•°æ®é›†ä¸Šæµ‹è¯•äº†MUSTERçš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºåœ¨å„ç§æƒ…å†µä¸‹ï¼Œä¸ä½¿ç”¨é…å¯¹é…å‡†ç›¸æ¯”ï¼Œä½¿ç”¨MUSTERå¯ä»¥å¤§å¤§å¢å¼ºä¼°è®¡çš„å˜å½¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ç¥ç»å½±åƒå­¦å€¡è®®ï¼ˆADNIï¼‰ç ”ç©¶ä¸­çš„è€å¹´å—è¯•è€…æ ·æœ¬åº”ç”¨äºMUSTERã€‚ç»“æœè¡¨æ˜ï¼ŒMUSTERå¯ä»¥æœ‰æ•ˆåœ°ä»T1åŠ æƒå›¾åƒä¸­è¯†åˆ«ç¥ç»é€€å˜çš„æ¨¡å¼ï¼Œè¿™äº›å˜åŒ–ä¸è®¤çŸ¥å˜åŒ–ç›¸å…³ï¼Œç¬¦åˆæœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•çš„æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨GPUåŠ é€Ÿï¼ŒMUSTERèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤§æ•°æ®é›†ï¼Œå³ä½¿åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿå®Œå…¨å¯è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14671v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºMUSTERçš„å¤šä¼šè¯æ—¶åºæ³¨å†Œæ–¹æ³•ï¼Œå¯å®ç°å¯¹åŒ»å­¦å›¾åƒåºåˆ—çš„çºµå‘åˆ†æã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„é…å¯¹æ³¨å†Œæ–¹æ³•ï¼ŒMUSTERé€šè¿‡çº³å…¥è¶…è¿‡ä¸¤ä¸ªæˆåƒä¼šè¯çš„æ•°æ®æ¥æ¢å¤çºµå‘å˜å½¢ï¼Œæé«˜äº†æ€§èƒ½ã€‚ç ”ç©¶æŒ‡å‡ºå±€éƒ¨å½’ä¸€åŒ–äº¤å‰ç›¸å…³ä½œä¸ºå›¾åƒç›¸ä¼¼åº¦é‡å¯èƒ½å¯¼è‡´åå·®ï¼Œå¹¶æå‡ºä¸€ç§ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨åˆæˆå¤šç«™ç‚¹ã€å¤šä¼šè¯çš„ç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„æµ‹è¯•è¡¨æ˜ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹ï¼Œç›¸è¾ƒäºé…å¯¹æ³¨å†Œï¼Œä½¿ç”¨MUSTERå¯æ˜¾è‘—æé«˜ä¼°è®¡å˜å½¢çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œå¯¹ADNIç ”ç©¶ä¸­è€å¹´äººç¾¤æ ·æœ¬çš„åº”ç”¨è¡¨æ˜ï¼ŒMUSTERå¯ä»T1åŠ æƒå›¾åƒæœ‰æ•ˆåœ°è¯†åˆ«ç¥ç»é€€åŒ–æ¨¡å¼ï¼Œä¸”ä¸è®¤çŸ¥å˜åŒ–ç›¸å…³è”ï¼Œè¾¾åˆ°æœ€æ–°åˆ†å‰²æ–¹æ³•çš„æ€§èƒ½æ°´å¹³ã€‚å€ŸåŠ©GPUåŠ é€Ÿï¼ŒMUSTERå¯é«˜æ•ˆå¤„ç†å¤§æ•°æ®é›†ï¼Œç”šè‡³åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MUSTERæ˜¯ä¸€ç§å¤šä¼šè¯æ—¶åºæ³¨å†Œæ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒåºåˆ—çš„çºµå‘åˆ†æã€‚</li>
<li>å®ƒé€šè¿‡çº³å…¥è¶…è¿‡ä¸¤ä¸ªæˆåƒä¼šè¯çš„æ•°æ®æ¥æé«˜æ¢å¤çºµå‘å˜å½¢çš„æ€§èƒ½ã€‚</li>
<li>å±€éƒ¨å½’ä¸€åŒ–äº¤å‰ç›¸å…³ä½œä¸ºå›¾åƒç›¸ä¼¼åº¦é‡å¯èƒ½å¯¼è‡´åå·®ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒMUSTERåœ¨ä¼°è®¡å˜å½¢æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„é…å¯¹æ³¨å†Œæ–¹æ³•ã€‚</li>
<li>åœ¨ADNIç ”ç©¶çš„åº”ç”¨ä¸­ï¼ŒMUSTERå¯æœ‰æ•ˆåœ°ä»T1åŠ æƒå›¾åƒè¯†åˆ«ç¥ç»é€€åŒ–æ¨¡å¼ï¼Œå¹¶ä¸è®¤çŸ¥å˜åŒ–ç›¸å…³è”ã€‚</li>
<li>MUSTERå€ŸåŠ©GPUåŠ é€Ÿå¯é«˜æ•ˆå¤„ç†å¤§æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c26a916b5b769ded1e8a2af74f0b52d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78e8d5d396bc726393f5b22c75a63965.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Spike2Former-Efficient-Spiking-Transformer-for-High-performance-Image-Segmentation"><a href="#Spike2Former-Efficient-Spiking-Transformer-for-High-performance-Image-Segmentation" class="headerlink" title="Spike2Former: Efficient Spiking Transformer for High-performance Image   Segmentation"></a>Spike2Former: Efficient Spiking Transformer for High-performance Image   Segmentation</h2><p><strong>Authors:Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li</strong></p>
<p>Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNs in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0 efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU and 6.6 efficiency on CityScapes. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å…·æœ‰ä½åŠŸè€—ä¼˜åŠ¿ï¼Œä½†åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚åŸå› æ˜¯ç›´æ¥å°†ä¸ºåˆ†å‰²ä»»åŠ¡è®¾è®¡çš„å¤æ‚æ¶æ„ç¥ç»ç½‘ç»œç›´æ¥è½¬æ¢ä¸ºè„‰å†²ç‰ˆæœ¬ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œéæ”¶æ•›ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šå¯¼è‡´è„‰å†²ç‚¹ç«ä¸¥é‡å‡å°‘çš„æ¶æ„è®¾è®¡æ¨¡å—ï¼Œè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ï¼Œå¹¶æå‡ºSpike2Formeræ¶æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³å¤æ‚æ¶æ„ä¸‹SNNsçš„è®­ç»ƒç¨³å®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å½’ä¸€åŒ–æ•´æ•°è„‰å†²ç¥ç»å…ƒã€‚æˆ‘ä»¬åœ¨å„ç§è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šä¸ºSNNsè®¾ç½®äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„æ ‡å‡†ï¼Œåœ¨ADE20Kä¸Šå®ç°äº†+12.7%çš„mIoUå’Œ5.0çš„æ•ˆç‡æå‡ï¼Œåœ¨VOC2012ä¸Šå®ç°äº†+14.3%çš„mIoUå’Œ5.2çš„æ•ˆç‡æå‡ï¼Œä»¥åŠåœ¨CityScapesä¸Šå®ç°äº†+9.1%çš„mIoUå’Œ6.6çš„æ•ˆç‡æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14587v1">PDF</a> This work has been accepted on Association for the Advancement of   Artificial Intelligence 2025</p>
<p><strong>Summary</strong><br>     è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å…·æœ‰ä½åŠŸè€—ä¼˜åŠ¿ï¼Œä½†åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚åŸå› æ˜¯ç›´æ¥å°†å¤æ‚æ¶æ„è®¾è®¡çš„åˆ†å‰²ä»»åŠ¡ç¥ç»ç½‘ç»œè½¬æ¢ä¸ºè„‰å†²ç‰ˆæœ¬ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œéæ”¶æ•›ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºSpike2Formeræ¶æ„ï¼Œå¹¶å¼•å…¥å½’ä¸€åŒ–æ•´æ•°è„‰å†²ç¥ç»å…ƒä»¥è§£å†³å¤æ‚æ¶æ„ä¸‹SNNsçš„è®­ç»ƒç¨³å®šæ€§é—®é¢˜ã€‚åœ¨å¤šä¸ªè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬ä¸ºSNNsè®¾ç«‹äº†æ–°çš„ä¸šç•Œæ ‡å‡†ï¼Œå…¶ä¸­ADE20Kä¸Šçš„mIoUæå‡äº†+12.7%ä¸”æ•ˆç‡æé«˜5.0%ï¼ŒVOC2012ä¸Šçš„mIoUæå‡äº†+14.3%ä¸”æ•ˆç‡æé«˜5.2%ï¼ŒCityScapesä¸Šçš„mIoUæå‡äº†+9.1%ä¸”æ•ˆç‡æé«˜6.6ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œç›´æ¥è½¬æ¢å¤æ‚è®¾è®¡çš„ç¥ç»ç½‘ç»œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œéæ”¶æ•›ã€‚
 2. æå‡ºSpike2Formeræ¶æ„ï¼Œé’ˆå¯¹å¯¼è‡´æ€§èƒ½ä¸‹é™çš„å…³é”®æ¨¡å—è¿›è¡Œæ”¹è¿›ã€‚
 3. å¼•å…¥å½’ä¸€åŒ–æ•´æ•°è„‰å†²ç¥ç»å…ƒï¼Œè§£å†³å¤æ‚æ¶æ„ä¸‹SNNsçš„è®­ç»ƒç¨³å®šæ€§é—®é¢˜ã€‚
 4. åœ¨å¤šä¸ªè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šï¼ŒSNNsçš„æ€§èƒ½è¾¾åˆ°æ–°çš„ä¸šç•Œæ ‡å‡†ã€‚
 5. åœ¨ADE20Kæ•°æ®é›†ä¸Šï¼ŒmIoUæå‡+12.7%ä¸”æ•ˆç‡æé«˜5.0%ã€‚
 6. åœ¨VOC2012æ•°æ®é›†ä¸Šï¼ŒmIoUæå‡+14.3%ä¸”æ•ˆç‡æé«˜5.2%ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf19ddf865cc876fe921a29b2b334171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e8fb96621bde5f2e9a2aae8bff3d188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422e6495063084a4b5aa86c66a512695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9bc0ac16973466f0dc779b4477277c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-860e1eec41434fea1033dd5fdb120a0a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="S-3-Mamba-Small-Size-Sensitive-Mamba-for-Lesion-Segmentation"><a href="#S-3-Mamba-Small-Size-Sensitive-Mamba-for-Lesion-Segmentation" class="headerlink" title="{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation"></a>{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation</h2><p><strong>Authors:Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen</strong></p>
<p>Small lesions play a critical role in early disease diagnosis and intervention of severe infections. Popular models often face challenges in segmenting small lesions, as it occupies only a minor portion of an image, while down_sampling operations may inevitably lose focus on local features of small lesions. To tackle the challenges, we propose a {\bf S}mall-{\bf S}ize-{\bf S}ensitive {\bf Mamba} ({\bf S$^3$-Mamba}), which promotes the sensitivity to small lesions across three dimensions: channel, spatial, and training strategy. Specifically, an Enhanced Visual State Space block is designed to focus on small lesions through multiple residual connections to preserve local features, and selectively amplify important details while suppressing irrelevant ones through channel-wise attention. A Tensor-based Cross-feature Multi-scale Attention is designed to integrate input image features and intermediate-layer features with edge features and exploit the attentive support of features across multiple scales, thereby retaining spatial details of small lesions at various granularities. Finally, we introduce a novel regularized curriculum learning to automatically assess lesion size and sample difficulty, and gradually focus from easy samples to hard ones like small lesions. Extensive experiments on three medical image segmentation datasets show the superiority of our S$^3$-Mamba, especially in segmenting small lesions. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ErinWang2023/S3-Mamba">https://github.com/ErinWang2023/S3-Mamba</a>. </p>
<blockquote>
<p>å¾®å°ç—…å˜åœ¨æ—©æœŸç–¾ç—…è¯Šæ–­å’Œä¸¥é‡æ„ŸæŸ“å¹²é¢„ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚å¸¸è§çš„æ¨¡å‹åœ¨åˆ†å‰²å¾®å°ç—…å˜æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå¾®å°ç—…å˜åªå å›¾åƒçš„ä¸€å°éƒ¨åˆ†ï¼Œè€Œä¸‹é‡‡æ ·æ“ä½œå¯èƒ½ä¼šä¸å¯é¿å…åœ°å¿½ç•¥å…¶å±€éƒ¨ç‰¹å¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå°å‹ç—…å˜æ•æ„ŸMambaï¼ˆS$^3$-Mambaï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡é€šé“ã€ç©ºé—´å’Œè®­ç»ƒç­–ç•¥ä¸‰ä¸ªç»´åº¦æé«˜å¯¹å¾®å°ç—…å˜çš„æ•æ„Ÿæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¢å¼ºè§†è§‰çŠ¶æ€ç©ºé—´å—ï¼Œé€šè¿‡å¤šä¸ªæ®‹å·®è¿æ¥ä¸“æ³¨äºå¾®å°ç—…å˜ï¼Œä¿ç•™å±€éƒ¨ç‰¹å¾ï¼Œå¹¶é€šè¿‡é€šé“æ³¨æ„åŠ›æœ‰é€‰æ‹©åœ°æ”¾å¤§é‡è¦ç»†èŠ‚å¹¶æŠ‘åˆ¶æ— å…³ç»†èŠ‚ã€‚åŸºäºå¼ é‡çš„è·¨ç‰¹å¾å¤šå°ºåº¦æ³¨æ„åŠ›æ—¨åœ¨å°†è¾“å…¥å›¾åƒç‰¹å¾ä¸è¾¹ç¼˜ç‰¹å¾ç»“åˆä¸­é—´å±‚ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾çš„æ³¨æ„åŠ›æ”¯æŒï¼Œä»è€Œåœ¨ä¸åŒç²’åº¦ä¸Šä¿ç•™å¾®å°ç—…å˜çš„ç©ºé—´ç»†èŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ­£åˆ™åŒ–è¯¾ç¨‹å­¦ä¹ æ³•ï¼Œè‡ªåŠ¨è¯„ä¼°ç—…å˜å¤§å°å’Œæ ·æœ¬éš¾åº¦ï¼Œå¹¶ä»ç®€å•æ ·æœ¬é€æ­¥èšç„¦åˆ°å›°éš¾æ ·æœ¬ï¼ˆå¦‚å¾®å°ç—…å˜ï¼‰ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„S$^3$-Mambaå…·æœ‰ä¼˜è¶Šæ€§ï¼Œå°¤å…¶åœ¨åˆ†å‰²å¾®å°ç—…å˜æ–¹é¢ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº <a target="_blank" rel="noopener" href="https://github.com/ErinWang2023/S3-Mamba%E3%80%82">https://github.com/ErinWang2023/S3-Mambaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14546v1">PDF</a> Accept by AAAI 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­å°ç—…ç¶åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åä¸ºS$^3$-Mambaçš„æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºè§†è§‰çŠ¶æ€ç©ºé—´ã€åŸºäºå¼ é‡çš„è·¨ç‰¹å¾å¤šå°ºåº¦æ³¨æ„åŠ›å’Œæ­£åˆ™åŒ–è¯¾ç¨‹å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œæé«˜å¯¹å°ç—…ç¶çš„æ•æ„Ÿæ€§ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒS$^3$-Mambaåœ¨åˆ†å‰²å°ç—…ç¶æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°ç—…ç¶åœ¨ç–¾ç—…æ—©æœŸè¯Šæ–­å’Œæ²»ç–—ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨åˆ†å‰²å°ç—…ç¶æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>S$^3$-Mambaæ–¹æ³•é€šè¿‡ä¸‰ä¸ªç»´åº¦æé«˜å°ç—…ç¶çš„æ•æ„Ÿæ€§ï¼šé€šé“ã€ç©ºé—´å’Œè®­ç»ƒç­–ç•¥ã€‚</li>
<li>é‡‡ç”¨å¢å¼ºè§†è§‰çŠ¶æ€ç©ºé—´å—ï¼Œé€šè¿‡å¤šé‡æ®‹å·®è¿æ¥ä¿ç•™å±€éƒ¨ç‰¹å¾ï¼Œå¹¶é€‰æ‹©æ€§æ”¾å¤§é‡è¦ç»†èŠ‚ï¼ŒæŠ‘åˆ¶æ— å…³ä¿¡æ¯ã€‚</li>
<li>åŸºäºå¼ é‡çš„è·¨ç‰¹å¾å¤šå°ºåº¦æ³¨æ„åŠ›èƒ½å¤Ÿæ•´åˆè¾“å…¥å›¾åƒç‰¹å¾ä¸ä¸­é—´å±‚ç‰¹å¾ï¼ŒåŒæ—¶åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾çš„æ³¨æ„åŠ›æ”¯æŒï¼Œä¿ç•™å°ç—…ç¶çš„ç©ºé—´ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥æ­£åˆ™åŒ–è¯¾ç¨‹å­¦ä¹ ï¼Œè‡ªåŠ¨è¯„ä¼°ç—…ç¶å¤§å°ä¸æ ·æœ¬éš¾åº¦ï¼Œä»å°æ ·æœ¬é€æ¸èšç„¦åˆ°å›°éš¾æ ·æœ¬ï¼ˆå¦‚å°ç—…ç¶ï¼‰ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜S$^3$-Mambaçš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°ç—…ç¶åˆ†å‰²æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2b8eafef48812b7cdc7909f8ee5480a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5000f5ddb6f87dffa73e538224b5110.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ea18973265af11a941bb1c5a3fe03b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dcdbecc4b8fa2781cd00eac3206971c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Summary-of-Point-Transformer-with-Federated-Learning-for-Predicting-Breast-Cancer-HER2-Status-from-Hematoxylin-and-Eosin-Stained-Whole-Slide-Images"><a href="#Summary-of-Point-Transformer-with-Federated-Learning-for-Predicting-Breast-Cancer-HER2-Status-from-Hematoxylin-and-Eosin-Stained-Whole-Slide-Images" class="headerlink" title="Summary of Point Transformer with Federated Learning for Predicting   Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide   Images"></a>Summary of Point Transformer with Federated Learning for Predicting   Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide   Images</h2><p><strong>Authors:Kamorudeen A. Amuda, Almustapha A. Wakili</strong></p>
<p>This study introduces a federated learning-based approach to predict HER2 status from hematoxylin and eosin (HE)-stained whole slide images (WSIs), reducing costs and speeding up treatment decisions. To address label imbalance and feature representation challenges in multisite datasets, a point transformer is proposed, incorporating dynamic label distribution, an auxiliary classifier, and farthest cosine sampling. Extensive experiments demonstrate state-of-the-art performance across four sites (2687 WSIs) and strong generalization to two unseen sites (229 WSIs). </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åŸºäºè”é‚¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºä»è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆHEï¼‰æŸ“è‰²çš„å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰é¢„æµ‹HER2çŠ¶æ€ï¼Œé™ä½æˆæœ¬å¹¶åŠ å¿«æ²»ç–—å†³ç­–ã€‚ä¸ºäº†è§£å†³å¤šç«™ç‚¹æ•°æ®é›†ä¸­çš„æ ‡ç­¾ä¸å¹³è¡¡å’Œç‰¹å¾è¡¨ç¤ºæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç‚¹è½¬æ¢å™¨ï¼Œå®ƒç»“åˆäº†åŠ¨æ€æ ‡ç­¾åˆ†å¸ƒã€è¾…åŠ©åˆ†ç±»å™¨å’Œæœ€è¿œä½™å¼¦é‡‡æ ·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªç«™ç‚¹ï¼ˆ2687å¼ WSIï¼‰ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹ä¸¤ä¸ªæœªè§ç«™ç‚¹ï¼ˆ229å¼ WSIï¼‰å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14545v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç ”ç©¶ä¸­é‡‡ç”¨åŸºäºè”é‚¦å­¦ä¹ çš„é¢„æµ‹HER2çŠ¶æ€çš„æ–¹æ³•ï¼Œåˆ©ç”¨HEæŸ“è‰²å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰ï¼Œé™ä½æˆæœ¬å¹¶åŠ å¿«æ²»ç–—å†³ç­–ã€‚æå‡ºä¸€ç§ç‚¹è½¬æ¢å™¨æ¥è§£å†³å¤šç«™ç‚¹æ•°æ®é›†ä¸­çš„æ ‡ç­¾ä¸å¹³è¡¡å’Œç‰¹å¾è¡¨ç¤ºæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŠ¨æ€æ ‡ç­¾åˆ†å¸ƒã€è¾…åŠ©åˆ†ç±»å™¨å’Œæœ€è¿œä½™å¼¦é‡‡æ ·ã€‚åœ¨å››ä¸ªç«™ç‚¹ï¼ˆ2687å¼ å¹»ç¯ç‰‡ï¼‰è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ï¼Œå¹¶å¯¹ä¸¤ä¸ªæœªè§è¿‡çš„æ–°ç«™ç‚¹ï¼ˆ229å¼ å¹»ç¯ç‰‡ï¼‰å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡‡ç”¨è”é‚¦å­¦ä¹ é¢„æµ‹HER2çŠ¶æ€ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠä½¿ç”¨HEæŸ“è‰²å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰ã€‚</li>
<li>é€šè¿‡è”é‚¦å­¦ä¹ é™ä½æˆæœ¬å¹¶åŠ é€Ÿæ²»ç–—å†³ç­–ã€‚</li>
<li>æå‡ºç‚¹è½¬æ¢å™¨æ¥è§£å†³æ ‡ç­¾ä¸å¹³è¡¡å’Œç‰¹å¾è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚</li>
<li>ç‚¹è½¬æ¢å™¨åŒ…æ‹¬åŠ¨æ€æ ‡ç­¾åˆ†å¸ƒã€è¾…åŠ©åˆ†ç±»å™¨å’Œæœ€è¿œä½™å¼¦é‡‡æ ·æŠ€æœ¯ã€‚</li>
<li>å®éªŒåœ¨å››ä¸ªç«™ç‚¹è¿›è¡Œï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9b163bfa6cde1cbe725833d58e63e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f088365703823d01aee53e9837edd28b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0687e98cabcd19fdbbbc119d34245b15.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DAMPER-A-Dual-Stage-Medical-Report-Generation-Framework-with-Coarse-Grained-MeSH-Alignment-and-Fine-Grained-Hypergraph-Matching"><a href="#DAMPER-A-Dual-Stage-Medical-Report-Generation-Framework-with-Coarse-Grained-MeSH-Alignment-and-Fine-Grained-Hypergraph-Matching" class="headerlink" title="DAMPER: A Dual-Stage Medical Report Generation Framework with   Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching"></a>DAMPER: A Dual-Stage Medical Report Generation Framework with   Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching</h2><p><strong>Authors:Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen</strong></p>
<p>Medical report generation is crucial for clinical diagnosis and patient management, summarizing diagnoses and recommendations based on medical imaging. However, existing work often overlook the clinical pipeline involved in report writing, where physicians typically conduct an initial quick review followed by a detailed examination. Moreover, current alignment methods may lead to misaligned relationships. To address these issues, we propose DAMPER, a dual-stage framework for medical report generation that mimics the clinical pipeline of report writing in two stages. In the first stage, a MeSH-Guided Coarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image features with medical subject headings (MeSH) features to generate a rough keyphrase representation of the overall impression. In the second stage, a Hypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs hypergraphs for image patches and report annotations, modeling high-order relationships within each modality and performing hypergraph matching to capture semantic correlations between image regions and textual phrases. Finally,the coarse-grained visual features, generated MeSH representations, and visual hypergraph features are fed into a report decoder to produce the final medical report. Extensive experiments on public datasets demonstrate the effectiveness of DAMPER in generating comprehensive and accurate medical reports, outperforming state-of-the-art methods across various evaluation metrics. </p>
<blockquote>
<p>åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ç®¡ç†è‡³å…³é‡è¦ï¼Œå®ƒæ˜¯åŸºäºåŒ»å­¦æˆåƒå¯¹è¯Šæ–­å’Œå»ºè®®è¿›è¡Œæ±‡æ€»çš„å…³é”®ç¯èŠ‚ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œå¾€å¾€å¿½ç•¥äº†æŠ¥å‘Šç¼–å†™æ‰€æ¶‰åŠçš„ä¸´åºŠæµç¨‹ï¼ŒåŒ»ç”Ÿé€šå¸¸å…ˆè¿›è¡Œåˆæ­¥å¿«é€Ÿå®¡æŸ¥ï¼Œç„¶åè¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å¯¹é½æ–¹æ³•å¯èƒ½å¯¼è‡´å…³ç³»é”™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DAMPERï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„åŒé˜¶æ®µæ¡†æ¶ï¼Œå®ƒæ¨¡ä»¿äº†ä¸¤é˜¶æ®µæŠ¥å‘Šå†™ä½œçš„ä¸´åºŠæµç¨‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨MeSHæŒ‡å¯¼çš„ç²—ç²’åº¦å¯¹é½ï¼ˆMCGï¼‰é˜¶æ®µï¼Œå°†èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å›¾åƒç‰¹å¾ä¸åŒ»å­¦ä¸»é¢˜è¯è¡¨ï¼ˆMeSHï¼‰ç‰¹å¾å¯¹é½ï¼Œä»¥ç”Ÿæˆæ•´ä½“å°è±¡çš„ç²—ç•¥å…³é”®è¯è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨è¶…å›¾å¢å¼ºç»†ç²’åº¦å¯¹é½ï¼ˆHFGï¼‰é˜¶æ®µï¼Œä¸ºå›¾åƒè¡¥ä¸å’ŒæŠ¥å‘Šæ³¨é‡Šæ„å»ºè¶…å›¾ï¼Œå¯¹æ¯ç§æ¨¡æ€å†…çš„é«˜é˜¶å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡è¶…å›¾åŒ¹é…æ¥æ•è·å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬çŸ­è¯­ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚æœ€åï¼Œå°†ç²—ç²’åº¦è§†è§‰ç‰¹å¾ã€ç”Ÿæˆçš„MeSHè¡¨ç¤ºå’Œè§†è§‰è¶…å›¾ç‰¹å¾è¾“å…¥æŠ¥å‘Šè§£ç å™¨ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„åŒ»å­¦æŠ¥å‘Šã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDAMPERåœ¨ç”Ÿæˆå…¨é¢å‡†ç¡®çš„åŒ»å­¦æŠ¥å‘Šæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå¹¶åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŒé˜¶æ®µæ¡†æ¶DAMPERç”¨äºåŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼Œæ¨¡ä»¿åŒ»ç”Ÿä¹¦å†™æŠ¥å‘Šçš„ä¸´åºŠæµç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µä¸ºMeSHå¼•å¯¼ä¸‹çš„ç²—ç²’åº¦å¯¹é½ï¼ˆMCGï¼‰ï¼Œå°†èƒ¸Xå…‰ç‰‡å›¾åƒç‰¹å¾ä¸åŒ»å­¦ä¸»é¢˜æ ‡é¢˜ï¼ˆMeSHï¼‰ç‰¹å¾å¯¹é½ï¼Œç”Ÿæˆæ•´ä½“å°è±¡çš„ç²—ç•¥å…³é”®è¯è¡¨ç¤ºã€‚ç¬¬äºŒé˜¶æ®µä¸ºè¶…å›¾å¢å¼ºçš„ç»†ç²’åº¦å¯¹é½ï¼ˆHFGï¼‰ï¼Œæ„å»ºå›¾åƒè¡¥ä¸å’ŒæŠ¥å‘Šæ³¨é‡Šçš„è¶…å›¾ï¼Œå¯¹æ¯ç§æ¨¡æ€å†…çš„é«˜é˜¶å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡è¶…å›¾åŒ¹é…æ•è·å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬çŸ­è¯­ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚æœ€ç»ˆï¼Œå°†ç²—ç²’åº¦è§†è§‰ç‰¹å¾ã€ç”Ÿæˆçš„MeSHè¡¨ç¤ºå’Œè¶…å›¾ç‰¹å¾è¾“å…¥æŠ¥å‘Šè§£ç å™¨ï¼Œç”Ÿæˆæœ€ç»ˆåŒ»å­¦æŠ¥å‘Šã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDAMPERåœ¨ç”Ÿæˆå…¨é¢ã€å‡†ç¡®çš„åŒ»å­¦æŠ¥å‘Šæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä¼˜äºå„ç§è¯„ä¼°æŒ‡æ ‡ä¸‹çš„æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ï¼Œå…¶ç”Ÿæˆè¿‡ç¨‹é€šå¸¸åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šåˆæ­¥å¿«é€Ÿå®¡æŸ¥å’Œè¯¦ç»†æ£€æŸ¥ã€‚</li>
<li>å½“å‰åŒ»ç–—æŠ¥å‘Šç”Ÿæˆæ–¹æ³•å¿½ç•¥äº†ä¸´åºŠæµç¨‹ï¼Œå¯èƒ½å¯¼è‡´å…³ç³»é”™ä½ã€‚</li>
<li>DAMPERæ¡†æ¶æ¨¡ä»¿åŒ»ç”Ÿä¹¦å†™æŠ¥å‘Šçš„ä¸´åºŠæµç¨‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç²—ç²’åº¦å¯¹é½å’Œç»†ç²’åº¦å¯¹é½ã€‚</li>
<li>ç²—ç²’åº¦å¯¹é½é˜¶æ®µé€šè¿‡MeSHå¼•å¯¼ï¼Œå°†å›¾åƒç‰¹å¾ä¸åŒ»å­¦ä¸»é¢˜æ ‡é¢˜ç‰¹å¾å¯¹é½ï¼Œå½¢æˆæ•´ä½“å°è±¡çš„ç²—ç•¥å…³é”®è¯è¡¨ç¤ºã€‚</li>
<li>ç»†ç²’åº¦å¯¹é½é˜¶æ®µé€šè¿‡æ„å»ºè¶…å›¾ï¼Œå¯¹å›¾åƒè¡¥ä¸å’ŒæŠ¥å‘Šæ³¨é‡Šè¿›è¡Œé«˜é˜¶å…³ç³»å»ºæ¨¡ï¼Œå¹¶æ•è·å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚</li>
<li>DAMPERä½¿ç”¨ç²—ç²’åº¦è§†è§‰ç‰¹å¾ã€MeSHè¡¨ç¤ºå’Œè¶…å›¾ç‰¹å¾æ¥ç”Ÿæˆæœ€ç»ˆåŒ»ç–—æŠ¥å‘Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-587573d7382ce7647125550460002b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72b8629732b4374a0cfdecbc99b5eeda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d3f448a827787a605f8b2bd2b79dcb0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-794b3cd3e8212de13cc04d29c046d076.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments"><a href="#Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments" class="headerlink" title="Can Modern LLMs Act as Agent Cores in Radiology Environments?"></a>Can Modern LLMs Act as Agent Cores in Radiology Environments?</h2><p><strong>Authors:Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Advancements in large language models (LLMs) have paved the way for LLM-based agent systems that offer enhanced accuracy and interpretability across various domains. Radiology, with its complex analytical requirements, is an ideal field for the application of these agents. This paper aims to investigate the pre-requisite question for building concrete radiology agents which is, &#96;Can modern LLMs act as agent cores in radiology environments?â€™ To investigate it, we introduce RadABench with three-fold contributions: First, we present RadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based agents, generated from an extensive taxonomy encompassing 6 anatomies, 5 imaging modalities, 10 tool categories, and 11 radiology tasks. Second, we propose RadABench-EvalPlat, a novel evaluation platform for agents featuring a prompt-driven workflow and the capability to simulate a wide range of radiology toolsets. Third, we assess the performance of 7 leading LLMs on our benchmark from 5 perspectives with multiple metrics. Our findings indicate that while current LLMs demonstrate strong capabilities in many areas, they are still not sufficiently advanced to serve as the central agent core in a fully operational radiology agent system. Additionally, we identify key factors influencing the performance of LLM-based agent cores, offering insights for clinicians on how to apply agent systems in real-world radiology practices effectively. All of our code and data are open-sourced in <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench">https://github.com/MAGIC-AI4Med/RadABench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºåŸºäºLLMçš„ä»£ç†ç³»ç»Ÿé“ºå¹³äº†é“è·¯ï¼Œè¿™äº›ç³»ç»Ÿåœ¨å„ä¸ªé¢†åŸŸæä¾›äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ”¾å°„å­¦ç”±äºå…¶å¤æ‚çš„åˆ†æè¦æ±‚ï¼Œæ˜¯è¿™äº›ä»£ç†åº”ç”¨çš„ç†æƒ³é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨æ„å»ºå…·ä½“æ”¾å°„å­¦ä»£ç†çš„å…ˆå†³é—®é¢˜ï¼Œå³â€œç°ä»£LLMèƒ½å¦åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­ä½œä¸ºä»£ç†æ ¸å¿ƒï¼Ÿâ€ä¸ºäº†è°ƒæŸ¥è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RadABenchï¼Œå®ƒæœ‰ä¸‰æ–¹é¢çš„è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†RadABench-Dataï¼Œè¿™æ˜¯ä¸€å¥—å…¨é¢çš„åˆæˆè¯„ä¼°æ•°æ®é›†ï¼Œç”¨äºåŸºäºLLMçš„ä»£ç†ï¼Œæ•°æ®æ¥è‡ªå¹¿æ³›çš„åˆ†ç±»ï¼ŒåŒ…æ‹¬6ä¸ªè§£å‰–å­¦ã€5ç§æˆåƒæ¨¡å¼ã€10ä¸ªå·¥å…·ç±»åˆ«å’Œ11ä¸ªæ”¾å°„å­¦ä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†RadABench-EvalPlatï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä»£ç†è¯„ä¼°å¹³å°ï¼Œå…·æœ‰æç¤ºé©±åŠ¨çš„å·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿå¹¿æ³›çš„æ”¾å°„å­¦å·¥å…·é›†ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ä»5ä¸ªè§’åº¦å¯¹7æ¬¾é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨äº†å¤šä¸ªæŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸è¶³ä»¥ä½œä¸ºå®Œå…¨è¿è¡Œçš„æ”¾å°„å­¦ä»£ç†ç³»ç»Ÿçš„æ ¸å¿ƒä»£ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†å½±å“åŸºäºLLMçš„ä»£ç†æ ¸å¿ƒæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›äº†å¦‚ä½•åœ¨ç°å®ä¸–ç•Œçš„æ”¾å°„å­¦å®è·µä¸­æœ‰æ•ˆåº”ç”¨ä»£ç†ç³»ç»Ÿçš„è§è§£ã€‚æˆ‘ä»¬çš„æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½åœ¨<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/MAGIC-AI4Med/RadABenchä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09529v2">PDF</a> 22 pages,7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œæå‡ºäº†RadABenchæ•°æ®é›†ä¸è¯„ä¼°å¹³å°ã€‚é€šè¿‡è¯„ä¼°å‘ç°ï¼Œè™½ç„¶LLMsåœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»ä¸è¶³ä»¥ä½œä¸ºå®Œå…¨æ“ä½œåŒ–çš„æ”¾å°„å­¦ä»£ç†ç³»ç»Ÿçš„æ ¸å¿ƒã€‚æœ¬æ–‡è¿˜å…¬å¼€äº†ä»£ç å’Œæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>RadABenchæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„åˆæˆè¯„ä¼°æ•°æ®é›†ï¼Œä¸ºåŸºäºLLMçš„ä»£ç†æä¾›äº†è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>RadABench-EvalPlatæ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°å¹³å°ï¼Œä¸ºä»£ç†æä¾›äº†ä»¥æç¤ºä¸ºä¸­å¿ƒçš„å·¥ä½œæµç¨‹å’Œæ¨¡æ‹Ÿå„ç§æ”¾å°„å­¦å·¥å…·é›†çš„èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°å‘ç°å½“å‰LLMsåœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»ä¸è¶³ä»¥ä½œä¸ºå®Œå…¨æ“ä½œåŒ–çš„æ”¾å°„å­¦ä»£ç†ç³»ç»Ÿçš„æ ¸å¿ƒã€‚</li>
<li>å…¬å¼€çš„ä»£ç å’Œæ•°æ®é›†æœ‰åŠ©äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
<li>LLMsåœ¨æ”¾å°„å­¦é¢†åŸŸçš„æ€§èƒ½å—åˆ°å¤šç§å› ç´ çš„å½±å“ï¼Œéœ€è¦æ›´å¤šç ”ç©¶æ¥æå‡å…¶è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e3c80dd1ef26eb84ad020d169c4df0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190becf8a05e2e891b2dd0eb3f6052c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-465e5a878f0cbf8788985f515e140601.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM"><a href="#Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM" class="headerlink" title="Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM"></a>Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM</h2><p><strong>Authors:Hiromasa Suzuki, Naomi Tsuji, Yoshiaki Kanemaru, Megumi Shidatsu, Laura Olivera-Nieto, Samar Safi-Harb, Shigeo S. Kimura, Eduardo de la Fuente, Sabrina Casanova, Kaya Mori, Xiaojie Wang, Sei Kato, Dai Tateishi, Hideki Uchiyama, Takaaki Tanaka, Hiroyuki Uchida, Shun Inoue, Dezhi Huang, Marianne Lemoine-Goumard, Daiki Miura, Shoji Ogawa, Shogo B. Kobayashi, Chris Done, Maxime Parra, MarÃ­a DÃ­az Trigo, Teo MuÃ±oz-Darias, Montserrat Armas Padilla, Ryota Tomaru, Yoshihiro Ueda</strong></p>
<p>A recent report on the detection of very-high-energy gamma rays from V4641 Sagittarii (V4641 Sgr) up to <del>0.8 peta-electronvolt has made it the second confirmed â€œPeVatronâ€ microquasar. Here we report on the observation of V4641 Sgr with X-Ray Imaging and Spectroscopy Mission (XRISM) in September 2024. Thanks to the large field of view and low background, the CCD imager Xtend successfully detected for the first time X-ray extended emission around V4641 Sgr with a significance of &gt; 4.5 sigma and &gt; 10 sigma based on our imaging and spectral analysis, respectively. The spatial extent is estimated to have a radius of $7 \pm 3$ arcmin ($13 \pm 5$ pc at a distance of 6.2 kpc) assuming a Gaussian-like radial distribution, which suggests that the particle acceleration site is within ~10 pc of the microquasar. If the X-ray morphology traces the diffusion of accelerated electrons, this spatial extent can be explained by either an enhanced magnetic field (</del>80 uG) or a suppressed diffusion coefficient (~$10^{27}$ cm$^2$ s$^{-1}$ at 100 TeV). The integrated X-ray flux, (4-6)$\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ (2-10 keV), would require a magnetic field strength higher than the galactic mean (&gt; 8 uG) if the diffuse X-ray emission originates from synchrotron radiation and the gamma-ray emission is predominantly hadronic. If the X-rays are of thermal origin, the measured extension, temperature, and plasma density can be explained by a jet with a luminosity of ~$2\times 10^{39}$ erg s$^{-1}$, which is comparable to the Eddington luminosity of this system. </p>
<blockquote>
<p>æœ€è¿‘çš„ä¸€ä»½å…³äºä»V4641å¤©ç®­æ˜Ÿï¼ˆV4641 Sgrï¼‰æ£€æµ‹åˆ°è¶…é«˜èƒ½ä¼½é©¬å°„çº¿çš„æŠ¥å‘Šï¼Œèƒ½é‡é«˜è¾¾~0.8æ‹ç”µå­ä¼ç‰¹ï¼Œä½¿å…¶æˆä¸ºç¬¬äºŒä¸ªç¡®è®¤çš„â€œæ‹ä¼ä»‘â€å¾®ç±»æ˜Ÿã€‚è¿™é‡Œæˆ‘ä»¬æŠ¥å‘Šäº†2024å¹´9æœˆä½¿ç”¨Xå°„çº¿æˆåƒå’Œå…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰è§‚å¯Ÿåˆ°çš„V4641 Sgrçš„æƒ…å†µã€‚ç”±äºè§†åœºèŒƒå›´å¤§ä¸”èƒŒæ™¯ä½ï¼ŒCCDæˆåƒä»ªXtendé¦–æ¬¡æˆåŠŸæ£€æµ‹åˆ°V4641 Sgrå‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„ï¼Œå…¶æ˜¾è‘—æ€§åŸºäºæˆ‘ä»¬çš„æˆåƒå’Œå…‰è°±åˆ†æåˆ†åˆ«å¤§äº4.5Ïƒå’Œå¤§äº10Ïƒã€‚ç©ºé—´èŒƒå›´ä¼°è®¡åŠå¾„ä¸º$7Â±3$è§’åˆ†ï¼ˆåœ¨è·ç¦»6.2åƒç§’å·®è·çš„æƒ…å†µä¸‹ä¸º$13Â±5$ç§’å·®è·ï¼‰ï¼Œå‡è®¾å¾„å‘åˆ†å¸ƒç±»ä¼¼äºé«˜æ–¯åˆ†å¸ƒï¼Œè¿™è¡¨æ˜ç²’å­åŠ é€Ÿä½ç‚¹ä½äºå¾®ç±»æ˜Ÿå‘¨å›´çº¦10ç§’å·®è·å†…ã€‚å¦‚æœXå°„çº¿çš„å½¢æ€è¿½è¸ªäº†åŠ é€Ÿç”µå­çš„æ‰©æ•£ï¼Œé‚£ä¹ˆè¿™ç§ç©ºé—´èŒƒå›´å¯ä»¥ç”¨å¢å¼ºçš„ç£åœºï¼ˆçº¦80å¾®é«˜æ–¯ï¼‰æˆ–æŠ‘åˆ¶çš„æ‰©æ•£ç³»æ•°ï¼ˆåœ¨100TeVæ—¶çº¦ä¸º$10^{27}$å˜ç±³$^2$æ¯ç§’ï¼‰æ¥è§£é‡Šã€‚Xå°„çº¿ç§¯åˆ†æµé‡ä¸ºï¼ˆä»‹äº4-6ï¼‰Ã— $10^{-12}$erg s$^{-1}$ cm$^{-2}$ ï¼ˆåœ¨ï¼ˆåœ¨æ³¢åŠ¨èŒƒå›´å†…çš„å…‰åº¦è¾ƒå¤æ‚ ï¼‰ä¹‹é—´ï¼‰ï¼Œå¦‚æœæ‰©æ•£Xå°„çº¿å‘å°„æ¥è‡ªåŒæ­¥è¾å°„å¹¶ä¸”ä¼½é©¬å°„çº¿å‘å°„ä¸»è¦æ˜¯å¼ºå­è¿‡ç¨‹ï¼Œé‚£ä¹ˆç£åœºå¼ºåº¦éœ€è¦é«˜äºé“¶æ²³ç³»å¹³å‡å€¼ï¼ˆå¤§äº8å¾®é«˜æ–¯ï¼‰ã€‚å¦‚æœXå°„çº¿æ˜¯çƒ­èµ·æºçš„ï¼Œé‚£ä¹ˆæ‰€æµ‹é‡çš„å»¶ä¼¸èŒƒå›´ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“å¯†åº¦å¯ä»¥ç”¨å…‰åº¦çº¦ä¸º$2Ã— 10^{39}$erg s$^{-1}$çš„å–·å°„æµæ¥è§£é‡Šï¼Œè¿™ä¸è¯¥ç³»ç»Ÿçš„çˆ±ä¸é¡¿å…‰åº¦ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08089v2">PDF</a> 9 pages, 5 figures, accepted for publication in ApJL</p>
<p><strong>Summary</strong><br>     è¿‘æœŸå¯¹V4641 Sagittariiçš„é«˜èƒ½ä¼½é©¬å°„çº¿æ£€æµ‹æ˜¾ç¤ºå…¶ä¸ºç¬¬äºŒä¸ªç¡®è®¤çš„PeVatronå¾®ç±»æ˜Ÿã€‚ä½¿ç”¨XRISMå¯¹V4641 Sgrè¿›è¡Œè§‚æµ‹ï¼Œå…¶Xå°„çº¿æˆåƒå…‰è°±ä»ªé¦–æ¬¡æ£€æµ‹åˆ°æ˜æ˜¾çš„Xå°„çº¿æ‰©å±•å‘å°„ã€‚ç©ºé—´èŒƒå›´ä¼°è®¡ä¸ºåŠå¾„çº¦7Â±3å¼§åˆ†ï¼ˆè·ç¦»åœ°çƒçº¦6.2åƒç§’å·®è·æ—¶çš„è·ç¦»ï¼‰ï¼Œè¡¨æ˜ç²’å­åŠ é€Ÿéƒ¨ä½å¯èƒ½ä½äºå¾®ç±»æ˜Ÿå‘¨å›´çº¦10ç§’å†…åŒºåŸŸã€‚ç ”ç©¶æå‡ºäº†å¢å¼ºç£åœºæˆ–æŠ‘åˆ¶æ‰©æ•£ç³»æ•°çš„å‡è®¾æ¥è§£é‡Šè¿™ç§ç©ºé—´èŒƒå›´çš„å»¶ä¼¸æ€§ã€‚å¯¹ç£åœºå¼ºåº¦æœ‰æ›´é«˜è¦æ±‚ï¼Œå¦‚æœXå°„çº¿æ˜¯åŒæ­¥è¾å°„ï¼Œä¼½é©¬å°„çº¿ä¸»è¦æ˜¯å¼ºå­äº§ç”Ÿï¼Œé‚£ä¹ˆç£åœºå¼ºåº¦å¿…é¡»é«˜äºé“¶æ²³ç³»å¹³å‡å€¼ã€‚å¦‚æœXå°„çº¿æ˜¯çƒ­èµ·æºï¼Œåˆ™å¯ä»¥è§£é‡Šå°„æµç°è±¡ï¼Œå°„æµäº®åº¦ä¸ç³»ç»Ÿçš„çˆ±ä¸é¡¿äº®åº¦ç›¸å½“ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>V4641 Sagittariiï¼ˆV4641 Sgrï¼‰å·²æˆä¸ºç¬¬äºŒä¸ªç¡®è®¤çš„PeVatronå¾®ç±»æ˜Ÿã€‚å®ƒçš„ä¼½é©¬å°„çº¿èƒ½é‡æé«˜ï¼Œè¾¾åˆ°çº¦0.8 peta-electronvoltã€‚</li>
<li>ä½¿ç”¨XRISMè§‚æµ‹æ˜¾ç¤ºå…¶å‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„é¦–æ¬¡è¢«æ£€æµ‹åˆ°ã€‚</li>
<li>é€šè¿‡æˆåƒå’Œå…‰è°±åˆ†æå‘ç°Xå°„çº¿è¾å°„åŒºåŸŸå‘ˆé«˜æ–¯åˆ†å¸ƒå½¢æ€ï¼Œä¼°è®¡å…¶åŠå¾„ä¸ºçº¦$7 \pm 3$å¼§åˆ†ï¼Œæ¨æµ‹ç²’å­åŠ é€Ÿéƒ¨ä½è·ç¦»å¾®ç±»æ˜Ÿä¸è¶…è¿‡çº¦10ç§’å·®è·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33a5d6ce77b16f42a3a984a3e385104a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72c15493fab70e3833a02e374cd4b4b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee798d1003758e369c83e7667a798c19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e739af33d407bc0df04a94b658e461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4354e72bf6504d124144b3307fd5725b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b880e3fd2f1a9acd5075154c622764f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰çš„æœ€æ–°è¿›å±•ä¸ºç«¯åˆ°ç«¯è¯­ä¹‰å›¾åƒä¼ è¾“å±•ç°å‡ºäº†æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ¡ˆä¸»è¦ä¾§é‡äºä¼˜åŒ–åƒç´ çº§æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡é€šå¸¸ä¸äººç±»æ„ŸçŸ¥ä¸ç¬¦ï¼Œå¯¼è‡´æ„ŸçŸ¥è´¨é‡è¾ƒä½ã€‚åœ¨è¿™å°ä¿¡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼Œä»¥æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ç†µæ¨¡å‹ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µæœ‰æ•ˆåœ°ç®¡ç†ä¼ è¾“å¸¦å®½ã€‚è¿™äº›ç¬¦å·ç„¶ååœ¨æ¥æ”¶å™¨ç«¯ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼å›¾åƒé‡å»ºä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æ–°å…´çš„é©¬å§†å·´çŠ¶çº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ä¹‹ä¸Šï¼Œè¯¥éª¨æ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›å¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å¹¶æé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°æ·±åº¦å­¦ä¹ æ–¹æ³•è”åˆæºä¿¡é“ç¼–ç æŠ€æœ¯æé«˜äº†ç«¯åˆ°ç«¯çš„è¯­ä¹‰å›¾åƒä¼ è¾“æ•ˆæœã€‚ä½†ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–åƒç´ çº§æŒ‡æ ‡ï¼Œè¿™å¾€å¾€ä¸äººç±»æ„ŸçŸ¥ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ„ŸçŸ¥è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼è”åˆæºä¿¡é“ç¼–ç æ–¹æ³•ï¼Œä»¥æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚æˆ‘ä»¬åˆ©ç”¨ï¿½ï¿½dæ¨¡å‹æœ‰æ•ˆç®¡ç†ä¼ è¾“å¸¦å®½ï¼ŒåŸºäºä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µè¿›è¡Œä¼°ç®—ã€‚è¿™äº›ç¬¦å·åœ¨æ¥æ”¶ç«¯ä½œä¸ºæ¡ä»¶ä¿¡æ¯å¼•å¯¼å›¾åƒé‡å»ºçš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æ–°å…´çš„é©¬å§†å·´å¼çº¿æ€§æ³¨æ„åŠ›éª¨æ¶ä¸Šï¼Œæ—¢æ“…é•¿å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œåˆæä¾›å¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å¹¶æé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•è”åˆæºä¿¡é“ç¼–ç æŠ€æœ¯æå‡è¯­ä¹‰å›¾åƒä¼ è¾“æ•ˆæœã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åƒç´ çº§ä¼˜åŒ–ï¼Œä¸äººç±»æ„ŸçŸ¥ä¸ä¸€è‡´ã€‚</li>
<li>æå‡ºåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼DJSCCæ–¹æ³•æé«˜å›¾åƒæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>åˆ©ç”¨ç†µæ¨¡å‹ç®¡ç†ä¼ è¾“å¸¦å®½ï¼ŒåŸºäºä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µã€‚</li>
<li>æ¥æ”¶ç«¯åˆ©ç”¨è¿™äº›ç¬¦å·ä½œä¸ºæ¡ä»¶ä¿¡æ¯å¼•å¯¼å›¾åƒé‡å»ºã€‚</li>
<li>æ¨¡å‹åŸºäºé©¬å§†å·´å¼çº¿æ€§æ³¨æ„åŠ›éª¨æ¶ï¼Œé€‚åˆå›¾åƒå¤„ç†ä¸”å¿«é€Ÿæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90b3a0658014bd140d80a5609c0caf99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d6eb91831daa0dcbcae7edc72d4ae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20e02759ca70cd725aed915d7feac78f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VHM-Versatile-and-Honest-Vision-Language-Model-for-Remote-Sensing-Image-Analysis"><a href="#VHM-Versatile-and-Honest-Vision-Language-Model-for-Remote-Sensing-Image-Analysis" class="headerlink" title="VHM: Versatile and Honest Vision Language Model for Remote Sensing Image   Analysis"></a>VHM: Versatile and Honest Vision Language Model for Remote Sensing Image   Analysis</h2><p><strong>Authors:Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He</strong></p>
<p>This paper develops a Versatile and Honest vision language Model (VHM) for remote sensing image analysis. VHM is built on a large-scale remote sensing image-text dataset with rich-content captions (VersaD), and an honest instruction dataset comprising both factual and deceptive questions (HnstD). Unlike prevailing remote sensing image-text datasets, in which image captions focus on a few prominent objects and their relationships, VersaD captions provide detailed information about image properties, object attributes, and the overall scene. This comprehensive captioning enables VHM to thoroughly understand remote sensing images and perform diverse remote sensing tasks. Moreover, different from existing remote sensing instruction datasets that only include factual questions, HnstD contains additional deceptive questions stemming from the non-existence of objects. This feature prevents VHM from producing affirmative answers to nonsense queries, thereby ensuring its honesty. In our experiments, VHM significantly outperforms various vision language models on common tasks of scene classification, visual question answering, and visual grounding. Additionally, VHM achieves competent performance on several unexplored tasks, such as building vectorizing, multi-label classification and honest question answering. We will release the code, data and model weights at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/VHM">https://github.com/opendatalab/VHM</a> . </p>
<blockquote>
<p>æœ¬æ–‡å¼€å‘äº†ä¸€ç§é€šç”¨è¯šä¿¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVHMï¼‰ï¼Œç”¨äºé¥æ„Ÿå›¾åƒåˆ†æã€‚VHMå»ºç«‹åœ¨ä¸€ä¸ªå¤§è§„æ¨¡çš„é¥æ„Ÿå›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼ˆVersaDï¼‰å’ŒåŒ…å«äº‹å®å’Œæ¬ºéª—æ€§é—®é¢˜çš„è¯šä¿¡æŒ‡ä»¤æ•°æ®é›†ï¼ˆHnstDï¼‰ä¹‹ä¸Šã€‚ä¸ç°æœ‰çš„é¥æ„Ÿå›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸åŒï¼ŒVersaDçš„æ ‡é¢˜ä¸ä»…å…³æ³¨ä¸€äº›çªå‡ºå¯¹è±¡åŠå…¶å…³ç³»ï¼Œè€Œä¸”æä¾›å…³äºå›¾åƒå±æ€§ã€å¯¹è±¡å±æ€§å’Œæ•´ä½“åœºæ™¯çš„ç»¼åˆä¿¡æ¯ã€‚è¿™ç§å…¨é¢çš„æ ‡é¢˜åŠŸèƒ½ä½¿VHMèƒ½å¤Ÿå……åˆ†äº†è§£é¥æ„Ÿå›¾åƒå¹¶æ‰§è¡Œå¤šç§é¥æ„Ÿä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä¸ä»…åŒ…å«äº‹å®é—®é¢˜çš„ç°æœ‰é¥æ„ŸæŒ‡ä»¤æ•°æ®é›†ä¸åŒï¼ŒHnstDè¿˜åŒ…å«ç”±å¯¹è±¡ä¸å­˜åœ¨å¼•èµ·çš„æ¬ºéª—æ€§é—®é¢˜ã€‚è¿™ä¸€ç‰¹ç‚¹ä½¿VHMä¸ä¼šä¸ºéå¸¸è¯†æŸ¥è¯¢ç»™å‡ºè‚¯å®šç­”æ¡ˆï¼Œä»è€Œç¡®ä¿å…¶è¯šä¿¡ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒVHMåœ¨åœºæ™¯åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ç­‰å¸¸è§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå„ç§è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVHMåœ¨æœªæ¢ç´¢çš„ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºäº†ä¸ä¿—çš„æ€§èƒ½ï¼Œå¦‚å»ºç­‘çŸ¢é‡åŒ–ã€å¤šæ ‡ç­¾åˆ†ç±»å’Œè¯šä¿¡é—®ç­”ç­‰ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/VHM%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BB%A3%E7%A0%81%E3%80%81%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/opendatalab/VHMä¸Šå‘å¸ƒä»£ç ã€æ•°æ®å’Œæ¨¡å‹æƒé‡ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.20213v4">PDF</a> Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding   author: Gui-Song Xia, Conghui He</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç ”ç©¶é¢†åŸŸçš„ä¸€ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§é€šç”¨ä¸”è¯šå®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVHMï¼‰ï¼Œç”¨äºé¥æ„Ÿå›¾åƒåˆ†æã€‚è¯¥æ¨¡å‹åŸºäºå¤§è§„æ¨¡é¥æ„Ÿå›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼ˆVersaDï¼‰å’Œè¯šå®æŒ‡ä»¤æ•°æ®é›†ï¼ˆHnstDï¼‰ã€‚VersaDæ•°æ®é›†æä¾›äº†è¯¦ç»†çš„å›¾åƒå±æ€§ã€å¯¹è±¡ç‰¹å¾å’Œæ•´ä½“åœºæ™¯ä¿¡æ¯ï¼Œä½¿VHMèƒ½å¤Ÿå…¨é¢ç†è§£é¥æ„Ÿå›¾åƒå¹¶æ‰§è¡Œå„ç§é¥æ„Ÿä»»åŠ¡ã€‚HnstDæ•°æ®é›†åŒ…å«æ¬ºè¯ˆæ€§é—®é¢˜ï¼Œä½¿æ¨¡å‹ä¸ä¼šå¯¹æ‰€æœ‰æŸ¥è¯¢ç»™å‡ºè‚¯å®šç­”å¤ï¼Œç¡®ä¿æ¨¡å‹çš„è¯šå®æ€§ã€‚åœ¨åœºæ™¯åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ç­‰å¸¸è§ä»»åŠ¡ä¸Šï¼ŒVHMæ˜¾è‘—ä¼˜äºå…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å»ºç­‘çŸ¢é‡åŒ–ã€å¤šæ ‡ç­¾åˆ†ç±»å’Œè¯šå®é—®ç­”ç­‰æœªæ¢ç´¢çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VHMæ¨¡å‹æ˜¯ä¸€ç§ç”¨äºé¥æ„Ÿå›¾åƒåˆ†æçš„é€šç”¨å’Œè¯šå®çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>VHMå»ºç«‹åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼šæä¾›è¯¦ç»†å›¾åƒä¿¡æ¯çš„VersaDæ•°æ®é›†å’ŒåŒ…å«æ¬ºéª—æ€§é—®é¢˜çš„HnstDæ•°æ®é›†ã€‚</li>
<li>VersaDæ•°æ®é›†çš„ä¸°å¯Œå†…å®¹æè¿°æœ‰åŠ©äºæ¨¡å‹å…¨é¢ç†è§£é¥æ„Ÿå›¾åƒã€‚</li>
<li>HnstDæ•°æ®é›†å¢å¼ºäº†æ¨¡å‹çš„è¯šå®åº¦ï¼Œä½¿å…¶ä¸ä¼šå¯¹æ‰€æœ‰æŸ¥è¯¢ç»™å‡ºè‚¯å®šç­”å¤ã€‚</li>
<li>VHMåœ¨åœºæ™¯åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>VHMåœ¨æœªç»æ¢ç´¢çš„ä»»åŠ¡ï¼ˆå¦‚å»ºç­‘çŸ¢é‡åŒ–ã€å¤šæ ‡ç­¾åˆ†ç±»å’Œè¯šå®é—®ç­”ï¼‰ä¸Šä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.20213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2b50aa4e94067749437ae796545ca12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cccabc1387ce3837549461306c6df77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2974e1e710d786597f0ac0a6d93a1372.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09adb95bcf771f6c55b0cfc991c21b46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-861ce231df7de364514a7c3bbc29265a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92b24cb9b1047cdf7319e3e02e6cefd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-536e51d537e1a51b989f9e460adb5bfb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d042495ce62f0174fb10b9a1c00aa7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b88cd905be18d6f72a346373bb015fb3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-68a361198e5e1f8fe7228d19d7a3b863.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  ProsodyFM Unsupervised Phrasing and Intonation Control for Intelligible   Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-750202e1a98a34f8ad6157aefde1aebd.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  LeviTor 3D Trajectory Oriented Image-to-Video Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
