<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  DS$^2$-ABSA Dual-Stream Data Synthesis with Label Refinement for   Few-Shot Aspect-Based Sentiment Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-54f4a070763060ed8ee365ac9fa4bd01.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    30 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-21-æ›´æ–°"><a href="#2024-12-21-æ›´æ–°" class="headerlink" title="2024-12-21 æ›´æ–°"></a>2024-12-21 æ›´æ–°</h1><h2 id="DS-2-ABSA-Dual-Stream-Data-Synthesis-with-Label-Refinement-for-Few-Shot-Aspect-Based-Sentiment-Analysis"><a href="#DS-2-ABSA-Dual-Stream-Data-Synthesis-with-Label-Refinement-for-Few-Shot-Aspect-Based-Sentiment-Analysis" class="headerlink" title="DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for   Few-Shot Aspect-Based Sentiment Analysis"></a>DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for   Few-Shot Aspect-Based Sentiment Analysis</h2><p><strong>Authors:Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu</strong></p>
<p>Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \textit{key-point-driven} and \textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods. </p>
<blockquote>
<p>æœ€è¿‘å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºä½èµ„æºåœºæ™¯ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„æ–°è§£å†³æ–¹æ¡ˆã€‚åœ¨åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰çš„å°‘é‡æ ·æœ¬æ–¹é¢ï¼Œä¹‹å‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡ä¿®æ”¹ç°æœ‰æ ·æœ¬ä¿ƒä½¿LLMç”Ÿæˆæ–°æ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ— æ³•äº§ç”Ÿè¶³å¤Ÿå¤šæ ·åŒ–çš„æ•°æ®ï¼Œå½±å“äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸€äº›ç ”ç©¶é€šè¿‡ä½¿ç”¨ç‰¹å®šæŒ‡ä»¤å’Œå‡ ä¸ªç²¾é€‰çš„ç¤ºä¾‹ä½œä¸ºæç¤ºæ¥åº”ç”¨åŸºäºä¸Šä¸‹æ–‡çš„ABSAå­¦ä¹ ã€‚å°½ç®¡æœ‰å‰æ™¯ï¼Œä½†LLMé€šå¸¸äº§ç”Ÿçš„æ ‡ç­¾ä¼šåç¦»ä»»åŠ¡è¦æ±‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DS$^2$-ABSAï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æå°‘é‡æ ·æœ¬çš„åŒæµæ•°æ®åˆæˆæ¡†æ¶ã€‚å®ƒåˆ©ç”¨LLMä»ä¸¤ä¸ªäº’è¡¥çš„è§’åº¦åˆæˆæ•°æ®ï¼šå…³é”®ç‚¹é©±åŠ¨å’Œå®ä¾‹é©±åŠ¨ï¼Œæœ‰æ•ˆåœ°åœ¨èµ„æºç¨€ç¼ºçš„ç¯å¢ƒä¸­ç”Ÿæˆå¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„ABSAæ ·æœ¬ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†æ ‡ç­¾ä¼˜åŒ–æ¨¡å—ï¼Œä»¥æé«˜åˆæˆæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDS$^2$-ABSAæ˜¾è‘—ä¼˜äºä¹‹å‰çš„åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æå°‘é‡æ ·æœ¬è§£å†³æ–¹æ¡ˆä»¥åŠå…¶ä»–é¢å‘LLMçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„DS$^2$-ABSAæ¡†æ¶ï¼Œé€šè¿‡ä¸¤ç§äº’è¡¥çš„è§†è§’ï¼šå…³é”®ç‚¹é©±åŠ¨å’Œå®ä¾‹é©±åŠ¨ï¼Œæœ‰æ•ˆåˆæˆæ•°æ®ï¼Œæé«˜äº†å°‘æ ·æœ¬æ–¹é¢æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬æ ‡ç­¾ä¿®æ­£æ¨¡å—ï¼Œä»¥æé«˜åˆæˆæ•°æ®çš„æ ‡ç­¾è´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒDS$^2$-ABSAæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å°‘æ ·æœ¬ABSAè§£å†³æ–¹æ¡ˆå’Œå…¶ä»–LLMå¯¼å‘çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä¸ºè§£å†³ä½èµ„æºæƒ…å†µä¸‹çš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæŠ€æœ¯åœ¨ABSAä¸­æ— æ³•äº§ç”Ÿè¶³å¤Ÿå¤šæ ·çš„æ•°æ®ã€‚</li>
<li>åœ¨ABSAä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•æœ‰æ—¶ä¼šåç¦»ä»»åŠ¡è¦æ±‚ã€‚</li>
<li>DS$^2$-ABSAæ¡†æ¶é€šè¿‡ä¸¤ç§äº’è¡¥çš„è§†è§’ï¼šå…³é”®ç‚¹é©±åŠ¨å’Œå®ä¾‹é©±åŠ¨ï¼Œè¿›è¡Œæ•°æ®å®‰å…¨åˆæˆã€‚</li>
<li>DS$^2$-ABSAåŒ…æ‹¬æ ‡ç­¾ä¿®æ­£æ¨¡å—ï¼Œä»¥æé«˜åˆæˆæ•°æ®çš„æ ‡ç­¾è´¨é‡ã€‚</li>
<li>DS$^2$-ABSAæ˜¾è‘—ä¼˜äºå…¶ä»–å°‘æ ·æœ¬ABSAè§£å†³æ–¹æ¡ˆå’ŒLLMå¯¼å‘çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºä½èµ„æºç¯å¢ƒä¸‹çš„æ–¹é¢æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c946d29aaa0fcb88fc23c2adb8871435.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90070409bcb7e9bc23fe06cbf8f11b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f4a070763060ed8ee365ac9fa4bd01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5383d29c3745fa906cccc7f7a70b9869.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Efficient-Few-Shot-Neural-Architecture-Search-by-Counting-the-Number-of-Nonlinear-Functions"><a href="#Efficient-Few-Shot-Neural-Architecture-Search-by-Counting-the-Number-of-Nonlinear-Functions" class="headerlink" title="Efficient Few-Shot Neural Architecture Search by Counting the Number of   Nonlinear Functions"></a>Efficient Few-Shot Neural Architecture Search by Counting the Number of   Nonlinear Functions</h2><p><strong>Authors:Youngmin Oh, Hyunju Lee, Bumsub Ham</strong></p>
<p>Neural architecture search (NAS) enables finding the best-performing architecture from a search space automatically. Most NAS methods exploit an over-parameterized network (i.e., a supernet) containing all possible architectures (i.e., subnets) in the search space. However, the subnets that share the same set of parameters are likely to have different characteristics, interfering with each other during training. To address this, few-shot NAS methods have been proposed that divide the space into a few subspaces and employ a separate supernet for each subspace to limit the extent of weight sharing. They achieve state-of-the-art performance, but the computational cost increases accordingly. We introduce in this paper a novel few-shot NAS method that exploits the number of nonlinear functions to split the search space. To be specific, our method divides the space such that each subspace consists of subnets with the same number of nonlinear functions. Our splitting criterion is efficient, since it does not require comparing gradients of a supernet to split the space. In addition, we have found that dividing the space allows us to reduce the channel dimensions required for each supernet, which enables training multiple supernets in an efficient manner. We also introduce a supernet-balanced sampling (SBS) technique, sampling several subnets at each training step, to train different supernets evenly within a limited number of training steps. Extensive experiments on standard NAS benchmarks demonstrate the effectiveness of our approach. Our code is available at <a target="_blank" rel="noopener" href="https://cvlab.yonsei.ac.kr/projects/EFS-NAS">https://cvlab.yonsei.ac.kr/projects/EFS-NAS</a>. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰èƒ½å¤Ÿè‡ªåŠ¨ä»æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æ€§èƒ½æœ€ä½³çš„æ¶æ„ã€‚å¤§å¤šæ•°NASæ–¹æ³•åˆ©ç”¨ä¸€ä¸ªè¿‡åº¦å‚æ•°åŒ–çš„ç½‘ç»œï¼ˆå³è¶…ç½‘ï¼‰ï¼Œè¯¥è¶…ç½‘åŒ…å«æœç´¢ç©ºé—´ä¸­æ‰€æœ‰å¯èƒ½çš„æ¶æ„ï¼ˆå³å­ç½‘ï¼‰ã€‚ç„¶è€Œï¼Œå…±äº«ç›¸åŒå‚æ•°é›†çš„å­ç½‘å¯èƒ½å…·æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç›¸äº’å¹²æ‰°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†å°‘é•œå¤´NASæ–¹æ³•ï¼Œå®ƒå°†ç©ºé—´åˆ’åˆ†ä¸ºå‡ ä¸ªå­ç©ºé—´ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ç©ºé—´ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„è¶…ç½‘æ¥é™åˆ¶æƒé‡å…±äº«ã€‚å®ƒä»¬è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿç›¸åº”å¢åŠ ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å°‘é•œå¤´NASæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨éçº¿æ€§å‡½æ•°çš„æ•°é‡æ¥åˆ’åˆ†æœç´¢ç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†ç©ºé—´åˆ’åˆ†æˆæ¯ä¸ªå­ç©ºé—´åŒ…å«ç›¸åŒæ•°é‡éçº¿æ€§å‡½æ•°çš„å­ç½‘ã€‚æˆ‘ä»¬çš„åˆ’åˆ†æ ‡å‡†æ˜¯é«˜æ•ˆçš„ï¼Œå› ä¸ºå®ƒä¸éœ€è¦æ¯”è¾ƒè¶…ç½‘çš„æ¢¯åº¦æ¥åˆ’åˆ†ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åˆ’åˆ†ç©ºé—´å…è®¸æˆ‘ä»¬å‡å°‘æ¯ä¸ªè¶…ç½‘æ‰€éœ€çš„é€šé“ç»´åº¦ï¼Œä»è€Œä»¥é«˜æ•ˆçš„æ–¹å¼è®­ç»ƒå¤šä¸ªè¶…ç½‘ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è¶…ç½‘å¹³è¡¡é‡‡æ ·ï¼ˆSBSï¼‰æŠ€æœ¯ï¼Œæ¯æ¬¡è®­ç»ƒæ­¥éª¤ä¸­é‡‡æ ·å¤šä¸ªå­ç½‘ï¼Œåœ¨æœ‰é™çš„è®­ç»ƒæ­¥éª¤å†…å‡åŒ€è®­ç»ƒä¸åŒçš„è¶…ç½‘ã€‚åœ¨æ ‡å‡†NASåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://cvlab.yonsei.ac.kr/projects/EFS-NAS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://cvlab.yonsei.ac.kr/projects/EFS-NASä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14678v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéçº¿æ€§å‡½æ•°æ•°é‡çš„æ–°å‹å°‘æ ·æœ¬NASæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å‰²æœç´¢ç©ºé—´ï¼Œä½¿å¾—æ¯ä¸ªå­ç©ºé—´åŒ…å«ç›¸åŒæ•°é‡çš„éçº¿æ€§å‡½æ•°ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‡å°‘æ¯ä¸ªè¶…ç½‘ç»œçš„é€šé“ç»´åº¦ï¼Œå®ç°äº†å¤šä¸ªè¶…ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡ã€‚åŒæ—¶å¼•å…¥äº†ä¸€ç§è¶…ç½‘ç»œå¹³è¡¡é‡‡æ ·æŠ€æœ¯ï¼Œåœ¨æœ‰é™çš„è®­ç»ƒæ­¥éª¤å†…å‡åŒ€è®­ç»ƒä¸åŒçš„è¶…ç½‘ç»œã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰å¯ä»æœç´¢ç©ºé—´ä¸­è‡ªåŠ¨æ‰¾åˆ°æ€§èƒ½æœ€ä½³çš„æ¶æ„ã€‚</li>
<li>å¤§å¤šæ•°NASæ–¹æ³•åˆ©ç”¨è¿‡åº¦å‚æ•°åŒ–çš„ç½‘ç»œï¼ˆå³è¶…ç½‘ï¼‰æ¥åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¶æ„ï¼ˆå³å­ç½‘ï¼‰ã€‚</li>
<li>å°‘æ ·æœ¬NASæ–¹æ³•é€šè¿‡å°†æœç´¢ç©ºé—´åˆ†å‰²æˆå¤šä¸ªå­ç©ºé—´æ¥å‡å°‘æƒé‡å…±äº«çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéçº¿æ€§å‡½æ•°æ•°é‡çš„æ–°å‹å°‘æ ·æœ¬NASæ–¹æ³•ï¼Œé€šè¿‡è¯¥åˆ†å‰²æ ‡å‡†æé«˜äº†æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å¯¹æœç´¢ç©ºé—´çš„åˆ†å‰²ï¼Œå‡å°‘äº†æ¯ä¸ªè¶…ç½‘ç»œçš„é€šé“ç»´åº¦éœ€æ±‚ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è¶…ç½‘ç»œå¹³è¡¡é‡‡æ ·æŠ€æœ¯ï¼Œå¯åœ¨æœ‰é™çš„è®­ç»ƒæ­¥éª¤å†…å‡åŒ€è®­ç»ƒä¸åŒçš„è¶…ç½‘ç»œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a740798c57be43148f5a2d1b21218471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3897f4bb86a6c5db271c69a8816191d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-520a398a31af634a27d2ab4acd82daf1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9894233d64ddde463479b4541b64276.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Adaptive-Prompt-Tuning-Vision-Guided-Prompt-Tuning-with-Cross-Attention-for-Fine-Grained-Few-Shot-Learning"><a href="#Adaptive-Prompt-Tuning-Vision-Guided-Prompt-Tuning-with-Cross-Attention-for-Fine-Grained-Few-Shot-Learning" class="headerlink" title="Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention   for Fine-Grained Few-Shot Learning"></a>Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention   for Fine-Grained Few-Shot Learning</h2><p><strong>Authors:Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich</strong></p>
<p>Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to differentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the Contrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time visual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a cross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-specific alignment of textual features with image patches extracted from the Vision Transformer, making the model more effective for datasets with high intra-class variance and low inter-class differences. The method is evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing significant performance gains over static prompt tuning approaches. To ensure these performance gains translate into trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of the model predictions and uncertainty estimates. This integration provides valuable insights into the modelâ€™s predictive confidence, helping to identify when predictions can be trusted and when additional verification is necessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-grained classification. </p>
<blockquote>
<p>é’ˆå¯¹è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å°æ ·æœ¬ç²¾ç»†åˆ†ç±»ä»»åŠ¡é¢ä¸´é‡è¦æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºéœ€è¦åœ¨æœ‰é™çš„æ•°æ®ä¸‹åŒºåˆ†å¾®å¦™çš„ç±»åˆ«å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”æç¤ºè°ƒæ•´å¢å¼ºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ï¼Œå¹¶ç”±å®æ—¶è§†è§‰è¾“å…¥è¿›è¡Œå¼•å¯¼ã€‚ä¸ç°æœ‰çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰å’Œè§†è§‰æç¤ºè°ƒæ•´ï¼ˆVPTï¼‰ç­‰æŠ€æœ¯ä¸åŒï¼Œè¿™äº›æŠ€æœ¯å—é™äºé™æ€æç¤ºæˆ–è§†è§‰ç¬¦å·ä¾èµ–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€å®Œå–„å½“å‰å›¾åƒæ–‡æœ¬æç¤ºã€‚è¿™æœ‰åŠ©äºå°†ç‰¹å®šå›¾åƒä¸­çš„æ–‡æœ¬ç‰¹å¾ä¸ä»è§†è§‰è½¬æ¢å™¨ä¸­æå–çš„å›¾åƒå—è¿›è¡Œå¯¹é½ï¼Œä½¿å¾—æ¨¡å‹å¯¹äºå…·æœ‰é«˜å¼ºåº¦å†…éƒ¨ç±»åˆ«å·®å¼‚å’Œå¾®å¼±ç±»åˆ«é—´å·®å¼‚çš„æ•°æ®é›†æ›´æœ‰æ•ˆã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬CUBirdsã€ç‰›æ´¥èŠ±å‰å’ŒFGVCé£æœºæ•°æ®é›†ï¼Œç›¸è¾ƒäºé™æ€æç¤ºè°ƒæ•´æ–¹æ³•æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ºç¡®ä¿è¿™äº›æ€§èƒ½æå‡è½¬åŒ–ä¸ºå¯é çš„é¢„æµ‹ï¼Œæˆ‘ä»¬æ•´åˆäº†è’™ç‰¹å¡æ´›Dropoutï¼Œä»¥æé«˜æ¨¡å‹é¢„æµ‹å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å¯é æ€§ã€‚è¿™ä¸€æ•´åˆæä¾›äº†å…³äºæ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦çš„å®è´µè§è§£ï¼Œæœ‰åŠ©äºç¡®å®šä½•æ—¶å¯ä»¥ä¿¡ä»»é¢„æµ‹ä»¥åŠä½•æ—¶éœ€è¦é¢å¤–çš„éªŒè¯ã€‚è¿™ä¸€åŠ¨æ€æ–¹æ³•ä¸ºç¨³å¥çš„è§£å†³æ–¹æ¡ˆæä¾›äº†å¼ºå¤§çš„æ”¯æ’‘ï¼Œæ¨è¿›äº†å°æ ·æœ¬ç²¾ç»†åˆ†ç±»çš„å‰æ²¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14640v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”æç¤ºè°ƒæ•´ï¼ˆAdaptive Prompt Tuningï¼‰çš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³è®¡ç®—æœºè§†è§‰ä¸­çš„å°æ ·æœ¬ç²¾ç»†åˆ†ç±»é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å®æ—¶è§†è§‰è¾“å…¥å¼•å¯¼ï¼ŒåŠ¨æ€è°ƒæ•´æ–‡æœ¬æç¤ºï¼Œä¸å›¾åƒä¸­çš„ç‰¹å®šå†…å®¹å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å…·æœ‰å¤§ç±»å†…å˜åŒ–å’Œè¾ƒå°ç±»é—´å·®å¼‚çš„æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚é›†æˆMonte-Carlo Dropoutæé«˜äº†æ¨¡å‹é¢„æµ‹çš„å¯ä¿¡åº¦å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºè‡ªé€‚åº”æç¤ºè°ƒæ•´çš„CLIPæ¨¡å‹æ–¹æ³•ï¼Œç”¨äºè§£å†³å°æ ·æœ¬ç²¾ç»†åˆ†ç±»é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å®æ—¶è§†è§‰è¾“å…¥è¿›è¡ŒåŠ¨æ€è°ƒæ•´æ–‡æœ¬æç¤ºï¼Œä¸å›¾åƒä¸­çš„ç‰¹å®šå†…å®¹è¿›è¡Œå¯¹é½ã€‚</li>
<li>ä¸ç°æœ‰çš„é™æ€æç¤ºè°ƒæ•´æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>é›†æˆMonte-Carlo Dropoutæé«˜äº†æ¨¡å‹é¢„æµ‹çš„å¯ä¿¡åº¦å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å¯é æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å…·æœ‰é«˜ç±»å†…å˜åŒ–å’Œä½ç±»é—´å·®å¼‚çš„æ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´æ–‡æœ¬æç¤ºï¼Œæé«˜äº†æ¨¡å‹å¯¹ä¸åŒå›¾åƒæ•°æ®çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7a3953169a6dd5014def7ff328b196e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293ed04f2fabd347c8998b78e4a147ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c84160fe3c4e2fc1f7f252e7e20b16a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8b676d9d08045935a64d76fe74cbce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPICA-Retrieving-Scenarios-for-Pluralistic-In-Context-Alignment"><a href="#SPICA-Retrieving-Scenarios-for-Pluralistic-In-Context-Alignment" class="headerlink" title="SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment"></a>SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment</h2><p><strong>Authors:Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang</strong></p>
<p>When different groupsâ€™ values differ, one approach to model alignment is to steer models at inference time towards each groupâ€™s preferences. However, techniques like in-context learning only consider similarity when drawing few-shot examples and not cross-group differences in values. We propose SPICA, a framework that accounts for group-level differences during in-context example retrieval. SPICA introduces three designs: scenario banks, group-informed retrieval metrics, and in-context alignment prompts. From an evaluation of SPICA on an alignment task collecting inputs from four demographic groups ($n &#x3D; 544$), our metrics retrieve in-context examples that more closely match observed preferences, with the best prompt configuration using multiple contrastive responses to demonstrate examples. In an end-to-end evaluation ($n &#x3D; 120$), we observe that SPICA is higher rated than similarity-based retrieval, with groups seeing up to a +0.16 point improvement on a 5 point scale. Additionally, gains from SPICA were more uniform, with all groups benefiting from alignment rather than only some. Finally, we find that while a group-agnostic approach can align to aggregated values, it is not most suited for divergent groups. </p>
<blockquote>
<p>å½“ä¸åŒç¾¤ä½“çš„ä»·å€¼è§‚å­˜åœ¨å·®å¼‚æ—¶ï¼Œä¸€ç§æ¨¡å‹å¯¹é½çš„æ–¹æ³•æ˜¯åœ¨æ¨ç†æ—¶å°†æ¨¡å‹å¯¼å‘æ¯ä¸ªç¾¤ä½“çš„åå¥½ã€‚ç„¶è€Œï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰æŠ€æœ¯ä»…åœ¨ç»˜åˆ¶å°‘é‡ç¤ºä¾‹æ—¶è€ƒè™‘ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†è·¨ç¾¤ä½“çš„ä»·å€¼è§‚å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†SPICAæ¡†æ¶ï¼Œå®ƒåœ¨ä¸Šä¸‹æ–‡ç¤ºä¾‹æ£€ç´¢è¿‡ç¨‹ä¸­è€ƒè™‘äº†ç¾¤ä½“å±‚é¢çš„å·®å¼‚ã€‚SPICAå¼•å…¥äº†ä¸‰é¡¹è®¾è®¡ï¼šæƒ…æ™¯åº“ã€åŸºäºç¾¤ä½“çš„æ£€ç´¢æŒ‡æ ‡å’Œä¸Šä¸‹æ–‡å¯¹é½æç¤ºã€‚é€šè¿‡å¯¹SPICAåœ¨æ”¶é›†æ¥è‡ªå››ä¸ªä¸åŒäººå£ç¾¤ä½“ï¼ˆn&#x3D;544ï¼‰è¾“å…¥çš„å¯¹é½ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æŒ‡æ ‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸è§‚å¯Ÿåˆ°çš„åå¥½æ›´åŒ¹é…ï¼Œæœ€ä½³æç¤ºé…ç½®ä½¿ç”¨å¤šä¸ªå¯¹æ¯”å“åº”æ¥å±•ç¤ºç¤ºä¾‹ã€‚åœ¨ç«¯åˆ°ç«¯è¯„ä¼°ï¼ˆn&#x3D;120ï¼‰ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°SPICAçš„è¯„åˆ†é«˜äºåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æ–¹æ³•ï¼Œå„ç¾¤ä½“åœ¨äº”ç‚¹é‡è¡¨ä¸Šçš„å¾—åˆ†æé«˜äº†+0.16ç‚¹ã€‚æ­¤å¤–ï¼ŒSPICAå¸¦æ¥çš„æ”¶ç›Šæ›´åŠ å‡è¡¡ï¼Œæ‰€æœ‰ç¾¤ä½“éƒ½èƒ½ä»å¯¹é½ä¸­å—ç›Šï¼Œè€Œä¸ä»…ä»…æ˜¯éƒ¨åˆ†ç¾¤ä½“ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶ä¸€ç§æ— ç¾¤ä½“å·®å¼‚çš„æ–¹æ³•å¯ä»¥å®ç°å¯¹æ•´ä½“ä»·å€¼çš„å¯¹é½ï¼Œä½†å®ƒå¹¶ä¸æœ€é€‚åˆäºä¸åŒçš„ç¾¤ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10912v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§åä¸ºSPICAçš„æ¡†æ¶ï¼Œç”¨äºåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹è€ƒè™‘ç¾¤ä½“çº§åˆ«çš„å·®å¼‚è¿›è¡Œæ¨¡å‹å¯¹é½ã€‚SPICAé€šè¿‡æƒ…æ™¯åº“ã€ç¾¤ä½“æ„ŸçŸ¥æ£€ç´¢æŒ‡æ ‡å’Œä¸Šä¸‹æ–‡å¯¹é½æç¤ºçš„è®¾è®¡ï¼Œå¯ä»¥æ›´å¥½åœ°åŒ¹é…ä¸åŒç¾¤ä½“çš„åå¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPICAåœ¨åŒ¹é…ç¾¤ä½“åå¥½æ–¹é¢ä¼˜äºåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æ–¹æ³•ï¼Œä¸”å¯¹æ‰€æœ‰ç¾¤ä½“éƒ½æœ‰æ”¹è¿›æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPICAæ¡†æ¶æ—¨åœ¨è§£å†³ä¸åŒç¾¤ä½“ä»·å€¼è§‚å·®å¼‚çš„é—®é¢˜ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µé€‚åº”æ¯ä¸ªç¾¤ä½“çš„åå¥½æ¥å®ç°æ¨¡å‹å¯¹é½ã€‚</li>
<li>ä¼ ç»Ÿçš„æ–¹æ³•å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ åªè€ƒè™‘å°‘é‡æ ·æœ¬çš„ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†è·¨ç¾¤ä½“çš„ä»·å€¼å·®å¼‚ã€‚</li>
<li>SPICAå¼•å…¥äº†æƒ…æ™¯åº“ã€ç¾¤ä½“æ„ŸçŸ¥æ£€ç´¢æŒ‡æ ‡å’Œä¸Šä¸‹æ–‡å¯¹é½æç¤ºä¸‰ä¸ªè®¾è®¡å…ƒç´ ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSPICAèƒ½å¤Ÿæ›´å¥½åœ°åŒ¹é…è§‚å¯Ÿåˆ°çš„åå¥½ï¼Œå¹¶ä½¿ç”¨å¤šä¸ªå¯¹æ¯”å“åº”æ¥å±•ç¤ºä¾‹å­ã€‚</li>
<li>åœ¨ç«¯åˆ°ç«¯çš„è¯„ä¼°ä¸­ï¼ŒSPICAæ¯”åŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æ–¹æ³•è·å¾—äº†æ›´é«˜çš„è¯„ä»·ï¼Œå„ç¾¤ä½“åœ¨5åˆ†åˆ¶é‡è¡¨ä¸Šæœ€å¤šæå‡äº†0.16åˆ†ã€‚</li>
<li>SPICAå¸¦æ¥çš„æ”¹è¿›æ›´åŠ å‡åŒ€ï¼Œå¯¹æ‰€æœ‰ç¾¤ä½“éƒ½æœ‰ç›Šï¼Œè€Œä¸ä»…ä»…æ˜¯éƒ¨åˆ†ç¾¤ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f650f483952416660ac37ee3e28cc98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee2d78b3a97bdbf81b56d6db11d0254b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Chameleon-A-Data-Efficient-Generalist-for-Dense-Visual-Prediction-in-the-Wild"><a href="#Chameleon-A-Data-Efficient-Generalist-for-Dense-Visual-Prediction-in-the-Wild" class="headerlink" title="Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in   the Wild"></a>Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in   the Wild</h2><p><strong>Authors:Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong</strong></p>
<p>Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/GitGyun/chameleon">https://github.com/GitGyun/chameleon</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»å‘å±•å‡ºäº†æ•°æ®é«˜æ•ˆçš„å…¨èƒ½å‹æ¨¡å‹ï¼Œå—ç›Šäºé€šç”¨è¯­è¨€æ¥å£å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚ç„¶è€Œï¼Œå¯¹äºå¯†é›†çš„è§†è§‰é¢„æµ‹è€Œè¨€ï¼Œæ„å»ºä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„å…¨èƒ½æ¨¡å‹æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒä»»åŠ¡çš„æ ‡ç­¾ç»“æ„å­˜åœ¨å˜åŒ–ã€‚å› æ­¤ï¼Œåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æœªè§è¿‡çš„å¯†é›†é¢„æµ‹ä»»åŠ¡å¹¶ä¸ç®€å•ï¼Œä¸”ä¹‹å‰çš„è§†è§‰å…¨èƒ½æ¨¡å‹å¯¹æ­¤å…³æ³¨è¾ƒå°‘ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿçµæ´»é€‚åº”æœªè§è¿‡çš„å¯†é›†æ ‡ç­¾ç»“æ„ï¼Œå¹¶ä¸”åªéœ€å°‘é‡æ ·æœ¬å³å¯æˆä¸ºæ•°æ®é«˜æ•ˆè§†è§‰å…¨èƒ½æ¨¡å‹ï¼Œé€‚ç”¨äºå¤šç§ç°å®åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŸºäºå¼ºå¤§çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œå¹¶æ¢ç´¢äº†å¤šä¸ªè½´æ¥æé«˜å…¶åœ¨ç°å®ä¸–ç•Œé—®é¢˜ä¸­çš„æ€§èƒ½å’Œé€šç”¨æ€§ï¼Œä¾‹å¦‚çµæ´»çš„é€‚åº”æœºåˆ¶å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æœªè§è¿‡çš„ç°å®åœºæ™¯ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œåœ¨ä½æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼ŒåŒ…æ‹¬è§†é¢‘ã€3Dã€åŒ»ç–—ã€ç”Ÿç‰©å’Œç”¨æˆ·äº¤äº’ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¨¡å‹é…å¤‡äº†é€šç”¨æ¶æ„å’Œæœ‰æ•ˆçš„é€‚åº”æœºåˆ¶ï¼Œèƒ½å¤Ÿçµæ´»åœ°é€‚åº”æ‰€æœ‰è¿™äº›ä»»åŠ¡ï¼Œæœ€å¤šåªéœ€è¦50å¼ æ ‡è®°å›¾åƒï¼Œè¿™æ˜¾ç¤ºäº†ä¸ç°æœ‰æ•°æ®é«˜æ•ˆå…¨èƒ½æ–¹æ³•ç›¸æ¯”çš„é‡å¤§è¿›æ­¥ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GitGyun/chameleon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GitGyun/chameleonæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18459v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ä¿ƒè¿›äº†æ•°æ®é«˜æ•ˆé€šç”¨ä¸»ä¹‰çš„æ¼”è¿›ï¼Œä½†ä¸ºå¯†é›†è§†è§‰é¢„æµ‹æ„å»ºæ•°æ®é«˜æ•ˆé€šç”¨ä¸»ä¹‰è€…ä»å­˜åœ¨ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ç§åŸºäºå…ƒå­¦ä¹ æ¡†æ¶çš„é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”æœªè§è¿‡çš„å¯†é›†æ ‡ç­¾ç»“æ„ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯ä¸­å‘æŒ¥æ•°æ®é«˜æ•ˆè§†è§‰é€šç”¨æ€§çš„ä½œç”¨ã€‚é€šè¿‡çµæ´»çš„é€‚åº”æœºåˆ¶å’Œå¯æ‰©å±•æ€§ç­‰å¤šæ–¹é¢çš„æ”¹è¿›ï¼Œè¯¥æ¨¡å‹åœ¨éœ€è¦ä½æ ·æœ¬å­¦ä¹ çš„å„ç§æœªè§è¿‡çš„çœŸå®åœºæ™¯ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å—ç›Šäºé€šç”¨è¯­è¨€æ¥å£å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œä¿ƒè¿›æ•°æ®é«˜æ•ˆé€šç”¨ä¸»ä¹‰çš„æ¼”è¿›ã€‚</li>
<li>å¯†é›†è§†è§‰é¢„æµ‹é¢†åŸŸçš„é€šç”¨æ€§é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒä»»åŠ¡çš„æ ‡ç­¾ç»“æ„å­˜åœ¨å·®å¼‚ã€‚</li>
<li>åœ¨ä½æ•°æ®æƒ…å†µä¸‹ï¼Œå°†çŸ¥è¯†æ³›åŒ–åˆ°æœªè§è¿‡çš„å¯†é›†é¢„æµ‹ä»»åŠ¡å¹¶ä¸ç®€å•ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…ƒå­¦ä¹ æ¡†æ¶çš„é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”æœªè§è¿‡çš„å¯†é›†æ ‡ç­¾ç»“æ„ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡çµæ´»çš„é€‚åº”æœºåˆ¶å’Œå¯æ‰©å±•æ€§ç­‰å¤šæ–¹é¢çš„æ”¹è¿›ï¼Œæé«˜äº†æ€§èƒ½å’Œé€šç”¨æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨éœ€è¦ä½æ ·æœ¬å­¦ä¹ çš„å„ç§æœªè§è¿‡çš„çœŸå®åœºæ™¯ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬è§†é¢‘ã€3Dã€åŒ»ç–—ã€ç”Ÿç‰©å’Œç”¨æˆ·äº’åŠ¨ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2aeff012717f9a84b7512a889e45603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5616808a65549560af643d350e6a896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f3ea2572ef5a8d6b30ab3a50099b64b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475755f831a6656f579c79941051f6db.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hypothesis-Generation-with-Large-Language-Models"><a href="#Hypothesis-Generation-with-Large-Language-Models" class="headerlink" title="Hypothesis Generation with Large Language Models"></a>Hypothesis Generation with Large Language Models</h2><p><strong>Authors:Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan</strong></p>
<p>Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks. </p>
<blockquote>
<p>æœ‰æ•ˆçš„ç”Ÿæˆæ–°å‡è®¾å¯¹ç§‘å­¦è¿›æ­¥è‡³å…³é‡è¦ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œç ”ç©¶äººå‘˜ä¸€ç›´æ˜¯å‡è®¾ç”Ÿæˆçš„ä¸»è¦åŠ¨åŠ›æ¥æºï¼Œé€šè¿‡è‰°è¾›çš„æ•°æ®åˆ†æå’Œæ€è€ƒï¼ˆä¹Ÿç§°ä¸ºé¡¿æ‚Ÿæ—¶åˆ»ï¼‰æ¥ç”Ÿæˆå‡è®¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå‡è®¾çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä¸“æ³¨äºåŸºäºæ•°æ®ï¼ˆå³å¸¦æœ‰æ ‡ç­¾çš„ç¤ºä¾‹ï¼‰çš„å‡è®¾ç”Ÿæˆã€‚ä¸ºäº†ä½¿LLMèƒ½å¤Ÿå¤„ç†ä»»æ„é•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬ä»å°‘é‡ç¤ºä¾‹ç”Ÿæˆåˆå§‹å‡è®¾ï¼Œç„¶åè¿­ä»£æ›´æ–°å®ƒä»¬ä»¥æé«˜å‡è®¾çš„è´¨é‡ã€‚å—å¤šè‡‚è€è™æœºçš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œä»¥åœ¨æ›´æ–°è¿‡ç¨‹ä¸­å®ç°æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ç®—æ³•èƒ½å¤Ÿç”Ÿæˆå‡è®¾ï¼Œä½¿åˆ†ç±»ä»»åŠ¡çš„é¢„æµ‹æ€§èƒ½è¿œè¿œè¶…è¿‡å°‘æ ·æœ¬æç¤ºï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šæé«˜äº†31.7%çš„å‡†ç¡®ç‡ï¼Œåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜äº†13.9%ã€3.3%å’Œ24.9%ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºç›‘ç£å­¦ä¹ ï¼Œåˆ†åˆ«æé«˜äº†12.8%å’Œ11.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆçš„å‡è®¾ä¸ä»…è¯å®äº†äººç±»éªŒè¯çš„ç†è®ºï¼Œè¿˜ä¸ºä»»åŠ¡æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.04326v3">PDF</a> 28 pages, 6 figures, code link:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/hypothesis_generation">https://github.com/ChicagoHAI/hypothesis_generation</a>. Accepted by the 1st   Workshop on NLP for Science (NLP4Science) at EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–°å‡è®¾æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºæ•°æ®ï¼ˆå³å¸¦æ ‡ç­¾ç¤ºä¾‹ï¼‰çš„å‡è®¾ç”Ÿæˆæ–¹é¢ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œé€šè¿‡å°‘é‡ç¤ºä¾‹ç”Ÿæˆåˆå§‹å‡è®¾ï¼Œå¹¶è¿­ä»£æ›´æ–°ä»¥æé«˜å‡è®¾è´¨é‡ã€‚è¯¥ç®—æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„é¢„æµ‹æ€§èƒ½ä¼˜äºå°‘æ ·æœ¬æç¤ºï¼Œå¯åœ¨åˆæˆæ•°æ®é›†ä¸Šæé«˜å‡†ç¡®ç‡31.7%ï¼Œå¹¶åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜å‡†ç¡®ç‡13.9%ã€3.3%å’Œ24.9%ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„å‡è®¾ä¸ä»…è¯å®äº†äººç±»éªŒè¯çš„ç†è®ºï¼Œè¿˜ä¸ºä»»åŠ¡æä¾›äº†æ–°çš„è§è§£ï¼Œä¸”ä¼˜äºç›‘ç£å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ–°å‡è®¾æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®ï¼ˆå¸¦æ ‡ç­¾ç¤ºä¾‹ï¼‰çš„å‡è®¾ç”Ÿæˆç®—æ³•ã€‚</li>
<li>è¯¥ç®—æ³•é€šè¿‡å°‘é‡ç¤ºä¾‹ç”Ÿæˆåˆå§‹å‡è®¾ï¼Œå¹¶è¿­ä»£æ›´æ–°ä»¥æé«˜å‡è®¾è´¨é‡ã€‚</li>
<li>ç®—æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„é¢„æµ‹æ€§èƒ½ä¼˜äºå°‘æ ·æœ¬æç¤ºå’Œç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œè¯¥ç®—æ³•æ˜¾è‘—æé«˜å‡†ç¡®ç‡ã€‚</li>
<li>ç”Ÿæˆçš„å‡è®¾ä¸ä»…è¯å®äº†äººç±»éªŒè¯çš„ç†è®ºï¼Œè¿˜æä¾›äº†æ–°è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.04326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-236f199e0c9dab15c8096d5cdf08f369.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7fccd51bf3ba59244571cef853ebc45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95252e4903948191a950745562dba9e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ceb791c5b546a39658d8bebb6dbacc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FSL-Rectifier-Rectify-Outliers-in-Few-Shot-Learning-via-Test-Time-Augmentation"><a href="#FSL-Rectifier-Rectify-Outliers-in-Few-Shot-Learning-via-Test-Time-Augmentation" class="headerlink" title="FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time   Augmentation"></a>FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time   Augmentation</h2><p><strong>Authors:Yunwei Bai, Ying Kiat Tan, Shiming Chen, Yao Shu, Tsuhan Chen</strong></p>
<p>Few-shot learning (FSL) commonly requires a model to identify images (queries) that belong to classes unseen during training, based on a few labelled samples of the new classes (support set) as reference. So far, plenty of algorithms involve training data augmentation to improve the generalization capability of FSL models, but outlier queries or support images during inference can still pose great generalization challenges. In this work, to reduce the bias caused by the outlier samples, we generate additional test-class samples by combining original samples with suitable train-class samples via a generative image combiner. Then, we obtain averaged features via an augmentor, which leads to more typical representations through the averaging. We experimentally and theoretically demonstrate the effectiveness of our method, obtaining a test accuracy improvement proportion of around 10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given a pretrained image combiner, our method is training-free for off-the-shelf FSL models, whose performance can be improved without extra datasets nor further training of the models themselves. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub">https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰é€šå¸¸éœ€è¦æ¨¡å‹æ ¹æ®æ–°ç±»åˆ«çš„å°‘é‡æ ‡è®°æ ·æœ¬ï¼ˆæ”¯æŒé›†ï¼‰ä½œä¸ºå‚è€ƒï¼Œè¯†åˆ«å‡ºå±äºè®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«çš„å›¾åƒï¼ˆæŸ¥è¯¢ï¼‰ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¾ˆå¤šç®—æ³•éƒ½æ¶‰åŠè®­ç»ƒæ•°æ®å¢å¼ºï¼Œä»¥æé«˜FSLæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¼‚å¸¸æŸ¥è¯¢æˆ–æ”¯æŒå›¾åƒä»ç„¶å¯èƒ½å¸¦æ¥å¾ˆå¤§çš„æ³›åŒ–æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸ºäº†å‡å°‘å¼‚å¸¸æ ·æœ¬å¼•èµ·çš„åè§ï¼Œæˆ‘ä»¬é€šè¿‡ç”Ÿæˆå¼å›¾åƒç»„åˆå™¨å°†åŸå§‹æ ·æœ¬ä¸åˆé€‚çš„è®­ç»ƒé›†æ ·æœ¬ç›¸ç»“åˆï¼Œç”Ÿæˆé¢å¤–çš„æµ‹è¯•é›†ç±»åˆ«æ ·æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¢å¼ºå™¨è·å¾—å¹³å‡ç‰¹å¾ï¼Œé€šè¿‡å¹³å‡åŒ–å¾—åˆ°æ›´å…¸å‹çš„è¡¨ç¤ºã€‚æˆ‘ä»¬å®éªŒå’Œç†è®ºä¸Šè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¯¹äºç»è¿‡è®­ç»ƒçš„FSLæ¨¡å‹ï¼Œæµ‹è¯•ç²¾åº¦æé«˜æ¯”ä¾‹çº¦ä¸º10%ï¼ˆä¾‹å¦‚ï¼Œä»46.86%æé«˜åˆ°53.28%ï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œç»™å®šä¸€ä¸ªé¢„è®­ç»ƒçš„å›¾åƒç»„åˆå™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹äºç°æˆçš„FSLæ¨¡å‹æ˜¯æ— éœ€è®­ç»ƒçš„ï¼Œå¯ä»¥åœ¨ä¸é¢å¤–ä½¿ç”¨æ•°æ®é›†æˆ–å¯¹æ¨¡å‹æœ¬èº«è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub">https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.18292v6">PDF</a> To be published in AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­æ¨¡å‹é¢å¯¹æœªè§ç±»åˆ«å›¾åƒè¯†åˆ«çš„é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆé¢å¤–çš„æµ‹è¯•ç±»åˆ«æ ·æœ¬å’Œå¹³å‡ç‰¹å¾çš„æ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘äº†å¼‚å¸¸æ ·æœ¬å¯¹æ¨¡å‹çš„å½±å“ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œå³å¯æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰åœ¨è¯†åˆ«æœªè§ç±»åˆ«å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼‚å¸¸æŸ¥è¯¢æˆ–æ”¯æŒå›¾åƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»ç„¶ä¼šå¸¦æ¥æ¨¡å‹æ³›åŒ–æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆé¢å¤–çš„æµ‹è¯•ç±»åˆ«æ ·æœ¬ï¼Œç»“åˆåŸå§‹æ ·æœ¬å’Œé€‚å½“çš„è®­ç»ƒç±»åˆ«æ ·æœ¬ï¼Œå¯ä»¥å‡å°‘å¼‚å¸¸æ ·æœ¬çš„å½±å“ã€‚</li>
<li>é‡‡ç”¨å¹³å‡ç‰¹å¾çš„æ–¹æ³•ï¼Œå¾—åˆ°æ›´å…¸å‹çš„è¡¨ç¤ºã€‚</li>
<li>å®éªŒå’Œç†è®ºéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæµ‹è¯•å‡†ç¡®ç‡æå‡æ¯”ä¾‹çº¦ä¸º10%ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºç°æˆçš„FSLæ¨¡å‹ï¼Œæ— éœ€é¢å¤–æ•°æ®é›†æˆ–è¿›ä¸€æ­¥è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.18292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dd2b7593a4d3363421151ebaa938a1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4644ebcd5ed4094c527ddcae67912f4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c194dbc4b9b46895a97367673b46b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93131fc137fb2a918d81da8fed91064b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of simple OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸åŒçš„æœ¬ä½“ä¹‹é—´å®ç°è¯­ä¹‰äº’æ“ä½œæ€§ï¼Œå¹¶é€šè¿‡å¯¹é½ç›¸å…³å®ä½“è§£å†³å…¶æ¦‚å¿µä¸Šçš„å¼‚è´¨æ€§ã€‚ç›®å‰ï¼ŒOMç³»ç»Ÿä¸»è¦æœ‰ä¸¤ç§æµè¡Œçš„è®¾è®¡èŒƒå¼ï¼šä¼ ç»Ÿçš„åŸºäºçŸ¥è¯†çš„ä¸“å®¶ç³»ç»Ÿå’Œè¾ƒæ–°çš„åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ç³»ç»Ÿã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†å·²ç»å½»åº•æ”¹å˜äº†æ•°æ®å·¥ç¨‹ï¼Œå¹¶åœ¨è®¸å¤šé¢†åŸŸå¾—åˆ°äº†åˆ›é€ æ€§çš„åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨OMä¸­çš„æ½œåŠ›ä»ç„¶è¢«ä½ä¼°ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŸºäºLLMçš„ä»£ç†é©±åŠ¨è®¾è®¡èŒƒå¼ï¼Œç”¨äºOMç³»ç»Ÿã€‚è€ƒè™‘åˆ°åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMæ‰€é¢ä¸´çš„è‹¥å¹²æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå³Agent-OMï¼ˆç”¨äºæœ¬ä½“åŒ¹é…çš„ä»£ç†ï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†å’Œä¸€å¥—ç®€å•çš„OMå·¥å…·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸€ä¸ªæ¦‚å¿µéªŒè¯ç³»ç»Ÿä¸­å®ç°ã€‚å¯¹ä¸‰ä¸ªæœ¬ä½“å¯¹é½è¯„ä¼°å€¡è®®ï¼ˆOAEIï¼‰èµ›é“ä¸Šçš„æœ€æ–°OMç³»ç»Ÿçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šçš„ç»“æœéå¸¸æ¥è¿‘é•¿æœŸä»¥æ¥çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¤æ‚å’Œå°‘é‡æœ¬ä½“åŒ¹é…ä»»åŠ¡ä¸Šå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v5">PDF</a> 19 pages, 13 figures, 4 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ–°å‹äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™ºèƒ½æœ¬ä½“åŒ¹é…æ¡†æ¶Agent-OMï¼Œå…¶è®¾è®¡åˆ›æ–°åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹å¼•å…¥æœ¬ä½“åŒ¹é…ç³»ç»Ÿä¸­ï¼Œæå‡ç³»ç»Ÿåœ¨å¤æ‚å’Œå°‘é‡æ•°æ®ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡è¿ç”¨ä¸¤ç§æ™ºèƒ½ä½“ï¼ˆSiamese agentsï¼‰è¿›è¡Œæ£€ç´¢å’ŒåŒ¹é…ä»»åŠ¡ï¼Œè¯¥æ¡†æ¶å¯å®ç°æ›´ç²¾å‡†çš„æœ¬ä½“åŒ¹é…æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰é€šè¿‡åœ¨ä¸åŒæœ¬ä½“é—´å®ç°è¯­ä¹‰äº’æ“ä½œæ€§ï¼Œè§£å†³æ¦‚å¿µå¼‚è´¨æ€§é—®é¢˜ï¼Œä¿ƒè¿›ç›¸å…³å®ä½“çš„å¯¹é½ã€‚</li>
<li>å½“å‰OMç³»ç»Ÿä¸»è¦æœ‰ä¸¤ç§è®¾è®¡èŒƒå¼ï¼šåŸºäºçŸ¥è¯†çš„ä¸“å®¶ç³»ç»Ÿå’ŒåŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ç³»ç»Ÿã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†åœ¨æ•°æ®å·¥ç¨‹é¢†åŸŸå…·æœ‰é©å‘½æ€§åº”ç”¨æ½œåŠ›ï¼Œä½†åœ¨OMæ–¹é¢çš„åº”ç”¨å°šå¾…æ¢ç´¢ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ–°å‹ä»£ç†é©±åŠ¨OMç³»ç»Ÿè®¾è®¡èŒƒå¼ï¼Œå³Agent-OMæ¡†æ¶ã€‚</li>
<li>Agent-OMæ¡†æ¶åŒ…å«ä¸¤ä¸ªSiameseä»£ç†ï¼Œç”¨äºæ£€ç´¢å’ŒåŒ¹é…ä»»åŠ¡ï¼Œå¹¶é…å¤‡ä¸€å¥—ç®€å•çš„OMå·¥å…·ã€‚</li>
<li>å®æ–½äº†ä¸€ä¸ªæ¦‚å¿µéªŒè¯ç³»ç»Ÿæ¥è¯æ˜Agent-OMçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0dbc27aec0e0a35ba6385cfcae4c3111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dad1989b38d614c47b96192c82520f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1049414e1280085db2a3d9c3ec55934f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7144d14f0bb04f3a3c2414b2ea0a0c73.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bd8b676d9d08045935a64d76fe74cbce.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  Adaptive Prompt Tuning Vision Guided Prompt Tuning with Cross-Attention   for Fine-Grained Few-Shot Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e52f299d172a0c3ea8966cf353feefcd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11189.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
