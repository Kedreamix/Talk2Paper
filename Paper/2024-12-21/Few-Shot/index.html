<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-21  DS$^2$-ABSA Dual-Stream Data Synthesis with Label Refinement for   Few-Shot Aspect-Based Sentiment Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-54f4a070763060ed8ee365ac9fa4bd01.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-21-更新"><a href="#2024-12-21-更新" class="headerlink" title="2024-12-21 更新"></a>2024-12-21 更新</h1><h2 id="DS-2-ABSA-Dual-Stream-Data-Synthesis-with-Label-Refinement-for-Few-Shot-Aspect-Based-Sentiment-Analysis"><a href="#DS-2-ABSA-Dual-Stream-Data-Synthesis-with-Label-Refinement-for-Few-Shot-Aspect-Based-Sentiment-Analysis" class="headerlink" title="DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for   Few-Shot Aspect-Based Sentiment Analysis"></a>DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for   Few-Shot Aspect-Based Sentiment Analysis</h2><p><strong>Authors:Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu</strong></p>
<p>Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \textit{key-point-driven} and \textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods. </p>
<blockquote>
<p>最近开发的大型语言模型（LLM）为低资源场景中的数据稀缺问题提供了有前景的新解决方案。在基于方面的情感分析（ABSA）的少量样本方面，之前的研究已经探索了数据增强技术，通过修改现有样本促使LLM生成新样本。然而，这些方法无法产生足够多样化的数据，影响了其有效性。此外，一些研究通过使用特定指令和几个精选的示例作为提示来应用基于上下文的ABSA学习。尽管有前景，但LLM通常产生的标签会偏离任务要求。为了克服这些局限性，我们提出了DS$^2$-ABSA，这是一个面向基于方面的情感分析少量样本的双流数据合成框架。它利用LLM从两个互补的角度合成数据：关键点驱动和实例驱动，有效地在资源稀缺的环境中生成多样化和高质量的ABSA样本。此外，还集成了标签优化模块，以提高合成标签的准确性。大量实验表明，DS$^2$-ABSA显著优于之前的基于方面的情感分析少量样本解决方案以及其他面向LLM的数据生成方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的DS$^2$-ABSA框架，通过两种互补的视角：关键点驱动和实例驱动，有效合成数据，提高了少样本方面情感分析（ABSA）的性能。该框架还包括标签修正模块，以提高合成数据的标签质量。实验证明，DS$^2$-ABSA显著优于先前的少样本ABSA解决方案和其他LLM导向的数据生成方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs为解决低资源情况下的数据稀缺问题提供了新的途径。</li>
<li>传统的数据增强技术在ABSA中无法产生足够多样的数据。</li>
<li>在ABSA中使用上下文学习的方法有时会偏离任务要求。</li>
<li>DS$^2$-ABSA框架通过两种互补的视角：关键点驱动和实例驱动，进行数据安全合成。</li>
<li>DS$^2$-ABSA包括标签修正模块，以提高合成数据的标签质量。</li>
<li>DS$^2$-ABSA显著优于其他少样本ABSA解决方案和LLM导向的数据生成方法。</li>
<li>该框架适用于低资源环境下的方面情感分析任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14849">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c946d29aaa0fcb88fc23c2adb8871435.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90070409bcb7e9bc23fe06cbf8f11b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f4a070763060ed8ee365ac9fa4bd01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5383d29c3745fa906cccc7f7a70b9869.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Efficient-Few-Shot-Neural-Architecture-Search-by-Counting-the-Number-of-Nonlinear-Functions"><a href="#Efficient-Few-Shot-Neural-Architecture-Search-by-Counting-the-Number-of-Nonlinear-Functions" class="headerlink" title="Efficient Few-Shot Neural Architecture Search by Counting the Number of   Nonlinear Functions"></a>Efficient Few-Shot Neural Architecture Search by Counting the Number of   Nonlinear Functions</h2><p><strong>Authors:Youngmin Oh, Hyunju Lee, Bumsub Ham</strong></p>
<p>Neural architecture search (NAS) enables finding the best-performing architecture from a search space automatically. Most NAS methods exploit an over-parameterized network (i.e., a supernet) containing all possible architectures (i.e., subnets) in the search space. However, the subnets that share the same set of parameters are likely to have different characteristics, interfering with each other during training. To address this, few-shot NAS methods have been proposed that divide the space into a few subspaces and employ a separate supernet for each subspace to limit the extent of weight sharing. They achieve state-of-the-art performance, but the computational cost increases accordingly. We introduce in this paper a novel few-shot NAS method that exploits the number of nonlinear functions to split the search space. To be specific, our method divides the space such that each subspace consists of subnets with the same number of nonlinear functions. Our splitting criterion is efficient, since it does not require comparing gradients of a supernet to split the space. In addition, we have found that dividing the space allows us to reduce the channel dimensions required for each supernet, which enables training multiple supernets in an efficient manner. We also introduce a supernet-balanced sampling (SBS) technique, sampling several subnets at each training step, to train different supernets evenly within a limited number of training steps. Extensive experiments on standard NAS benchmarks demonstrate the effectiveness of our approach. Our code is available at <a target="_blank" rel="noopener" href="https://cvlab.yonsei.ac.kr/projects/EFS-NAS">https://cvlab.yonsei.ac.kr/projects/EFS-NAS</a>. </p>
<blockquote>
<p>神经网络架构搜索（NAS）能够自动从搜索空间中找到性能最佳的架构。大多数NAS方法利用一个过度参数化的网络（即超网），该超网包含搜索空间中所有可能的架构（即子网）。然而，共享相同参数集的子网可能具有不同的特性，在训练过程中会相互干扰。为了解决这一问题，已经提出了少镜头NAS方法，它将空间划分为几个子空间，并为每个子空间使用一个单独的超网来限制权重共享。它们达到了最先进的性能，但计算成本也相应增加。本文介绍了一种新型的少镜头NAS方法，该方法利用非线性函数的数量来划分搜索空间。具体来说，我们的方法将空间划分成每个子空间包含相同数量非线性函数的子网。我们的划分标准是高效的，因为它不需要比较超网的梯度来划分空间。此外，我们发现划分空间允许我们减少每个超网所需的通道维度，从而以高效的方式训练多个超网。我们还引入了一种超网平衡采样（SBS）技术，每次训练步骤中采样多个子网，在有限的训练步骤内均匀训练不同的超网。在标准NAS基准测试上的大量实验证明了我们的方法的有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://cvlab.yonsei.ac.kr/projects/EFS-NAS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://cvlab.yonsei.ac.kr/projects/EFS-NAS上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14678v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于非线性函数数量的新型少样本NAS方法。该方法通过分割搜索空间，使得每个子空间包含相同数量的非线性函数，从而提高效率。此外，通过减少每个超网络的通道维度，实现了多个超网络的训练效率。同时引入了一种超网络平衡采样技术，在有限的训练步骤内均匀训练不同的超网络。实验证明该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络架构搜索（NAS）可从搜索空间中自动找到性能最佳的架构。</li>
<li>大多数NAS方法利用过度参数化的网络（即超网）来包含所有可能的架构（即子网）。</li>
<li>少样本NAS方法通过将搜索空间分割成多个子空间来减少权重共享的问题。</li>
<li>本文提出了一种基于非线性函数数量的新型少样本NAS方法，通过该分割标准提高了效率。</li>
<li>通过对搜索空间的分割，减少了每个超网络的通道维度需求，提高了训练效率。</li>
<li>引入了一种超网络平衡采样技术，可在有限的训练步骤内均匀训练不同的超网络。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14678">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a740798c57be43148f5a2d1b21218471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3897f4bb86a6c5db271c69a8816191d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-520a398a31af634a27d2ab4acd82daf1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9894233d64ddde463479b4541b64276.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Adaptive-Prompt-Tuning-Vision-Guided-Prompt-Tuning-with-Cross-Attention-for-Fine-Grained-Few-Shot-Learning"><a href="#Adaptive-Prompt-Tuning-Vision-Guided-Prompt-Tuning-with-Cross-Attention-for-Fine-Grained-Few-Shot-Learning" class="headerlink" title="Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention   for Fine-Grained Few-Shot Learning"></a>Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention   for Fine-Grained Few-Shot Learning</h2><p><strong>Authors:Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich</strong></p>
<p>Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to differentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the Contrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time visual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a cross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-specific alignment of textual features with image patches extracted from the Vision Transformer, making the model more effective for datasets with high intra-class variance and low inter-class differences. The method is evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing significant performance gains over static prompt tuning approaches. To ensure these performance gains translate into trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of the model predictions and uncertainty estimates. This integration provides valuable insights into the model’s predictive confidence, helping to identify when predictions can be trusted and when additional verification is necessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-grained classification. </p>
<blockquote>
<p>针对计算机视觉领域的小样本精细分类任务面临重要挑战，原因在于需要在有限的数据下区分微妙的类别差异。本文提出了一种新型方法，该方法通过自适应提示调整增强对比语言图像预训练（CLIP）模型，并由实时视觉输入进行引导。与现有的上下文优化（CoOp）和视觉提示调整（VPT）等技术不同，这些技术受限于静态提示或视觉符号依赖，所提出的方法利用交叉注意力机制动态完善当前图像文本提示。这有助于将特定图像中的文本特征与从视觉转换器中提取的图像块进行对齐，使得模型对于具有高强度内部类别差异和微弱类别间差异的数据集更有效。该方法在多个数据集上进行了评估，包括CUBirds、牛津花卉和FGVC飞机数据集，相较于静态提示调整方法显示出显著的性能提升。为确保这些性能提升转化为可靠的预测，我们整合了蒙特卡洛Dropout，以提高模型预测和不确定性估计的可靠性。这一整合提供了关于模型预测置信度的宝贵见解，有助于确定何时可以信任预测以及何时需要额外的验证。这一动态方法为稳健的解决方案提供了强大的支撑，推进了小样本精细分类的前沿水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14640v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于自适应提示调整（Adaptive Prompt Tuning）的对比语言图像预训练（CLIP）模型的新方法，用于解决计算机视觉中的小样本精细分类问题。该方法通过实时视觉输入引导，动态调整文本提示，与图像中的特定内容对齐，从而提高模型在具有大类内变化和较小类间差异的数据集上的性能。集成Monte-Carlo Dropout提高了模型预测的可信度和不确定性估计的可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种新颖的基于自适应提示调整的CLIP模型方法，用于解决小样本精细分类问题。</li>
<li>该方法利用实时视觉输入进行动态调整文本提示，与图像中的特定内容进行对齐。</li>
<li>与现有的静态提示调整技术相比，该方法在多个数据集上显示出显著的性能提升。</li>
<li>集成Monte-Carlo Dropout提高了模型预测的可信度和不确定性估计的可靠性。</li>
<li>该方法能够处理具有高类内变化和低类间差异的数据集。</li>
<li>该方法通过动态调整文本提示，提高了模型对不同图像数据的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14640">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d7a3953169a6dd5014def7ff328b196e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293ed04f2fabd347c8998b78e4a147ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c84160fe3c4e2fc1f7f252e7e20b16a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8b676d9d08045935a64d76fe74cbce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPICA-Retrieving-Scenarios-for-Pluralistic-In-Context-Alignment"><a href="#SPICA-Retrieving-Scenarios-for-Pluralistic-In-Context-Alignment" class="headerlink" title="SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment"></a>SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment</h2><p><strong>Authors:Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang</strong></p>
<p>When different groups’ values differ, one approach to model alignment is to steer models at inference time towards each group’s preferences. However, techniques like in-context learning only consider similarity when drawing few-shot examples and not cross-group differences in values. We propose SPICA, a framework that accounts for group-level differences during in-context example retrieval. SPICA introduces three designs: scenario banks, group-informed retrieval metrics, and in-context alignment prompts. From an evaluation of SPICA on an alignment task collecting inputs from four demographic groups ($n &#x3D; 544$), our metrics retrieve in-context examples that more closely match observed preferences, with the best prompt configuration using multiple contrastive responses to demonstrate examples. In an end-to-end evaluation ($n &#x3D; 120$), we observe that SPICA is higher rated than similarity-based retrieval, with groups seeing up to a +0.16 point improvement on a 5 point scale. Additionally, gains from SPICA were more uniform, with all groups benefiting from alignment rather than only some. Finally, we find that while a group-agnostic approach can align to aggregated values, it is not most suited for divergent groups. </p>
<blockquote>
<p>当不同群体的价值观存在差异时，一种模型对齐的方法是在推理时将模型导向每个群体的偏好。然而，上下文学习等技术仅在绘制少量示例时考虑相似性，而忽略了跨群体的价值观差异。我们提出了SPICA框架，它在上下文示例检索过程中考虑了群体层面的差异。SPICA引入了三项设计：情景库、基于群体的检索指标和上下文对齐提示。通过对SPICA在收集来自四个不同人口群体（n&#x3D;544）输入的对齐任务上的评估，我们的指标检索到的上下文示例与观察到的偏好更匹配，最佳提示配置使用多个对比响应来展示示例。在端到端评估（n&#x3D;120）中，我们观察到SPICA的评分高于基于相似性的检索方法，各群体在五点量表上的得分提高了+0.16点。此外，SPICA带来的收益更加均衡，所有群体都能从对齐中受益，而不仅仅是部分群体。最后，我们发现，虽然一种无群体差异的方法可以实现对整体价值的对齐，但它并不最适合于不同的群体。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10912v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文提出一种名为SPICA的框架，用于在少量样本的情况下考虑群体级别的差异进行模型对齐。SPICA通过情景库、群体感知检索指标和上下文对齐提示的设计，可以更好地匹配不同群体的偏好。实验结果表明，SPICA在匹配群体偏好方面优于基于相似性的检索方法，且对所有群体都有改进效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPICA框架旨在解决不同群体价值观差异的问题，通过引导模型在推理阶段适应每个群体的偏好来实现模型对齐。</li>
<li>传统的方法如上下文学习只考虑少量样本的相似性，忽略了跨群体的价值差异。</li>
<li>SPICA引入了情景库、群体感知检索指标和上下文对齐提示三个设计元素。</li>
<li>实验结果表明，SPICA能够更好地匹配观察到的偏好，并使用多个对比响应来展示例子。</li>
<li>在端到端的评估中，SPICA比基于相似性的检索方法获得了更高的评价，各群体在5分制量表上最多提升了0.16分。</li>
<li>SPICA带来的改进更加均匀，对所有群体都有益，而不仅仅是部分群体。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10912">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f650f483952416660ac37ee3e28cc98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee2d78b3a97bdbf81b56d6db11d0254b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Chameleon-A-Data-Efficient-Generalist-for-Dense-Visual-Prediction-in-the-Wild"><a href="#Chameleon-A-Data-Efficient-Generalist-for-Dense-Visual-Prediction-in-the-Wild" class="headerlink" title="Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in   the Wild"></a>Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in   the Wild</h2><p><strong>Authors:Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong</strong></p>
<p>Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/GitGyun/chameleon">https://github.com/GitGyun/chameleon</a>. </p>
<blockquote>
<p>大型语言模型已经发展出了数据高效的全能型模型，受益于通用语言接口和大规模预训练。然而，对于密集的视觉预测而言，构建一个数据高效的全能模型是一个独特的挑战，因为不同任务的标签结构存在变化。因此，在数据稀缺的情况下推广到未见过的密集预测任务并不简单，且之前的视觉全能模型对此关注较少。在本研究中，我们探索了一种通用模型，该模型能够灵活适应未见过的密集标签结构，并且只需少量样本即可成为数据高效视觉全能模型，适用于多种现实场景。为此，我们的方法基于强大的元学习框架，并探索了多个轴来提高其在现实世界问题中的性能和通用性，例如灵活的适应机制和可扩展性。我们在一系列未见过的现实场景中评估了我们的模型，在低样本学习场景中特别有用，包括视频、3D、医疗、生物和用户交互任务。我们的模型配备了通用架构和有效的适应机制，能够灵活地适应所有这些任务，最多只需要50张标记图像，这显示了与现有数据高效全能方法相比的重大进步。代码可在<a target="_blank" rel="noopener" href="https://github.com/GitGyun/chameleon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GitGyun/chameleon找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18459v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的发展促进了数据高效通用主义的演进，但为密集视觉预测构建数据高效通用主义者仍存在独特挑战。本研究探索了一种基于元学习框架的通用模型，能够灵活适应未见过的密集标签结构，并在多样化的真实场景中发挥数据高效视觉通用性的作用。通过灵活的适应机制和可扩展性等多方面的改进，该模型在需要低样本学习的各种未见过的真实场景任务中表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型受益于通用语言接口和大规模预训练，促进数据高效通用主义的演进。</li>
<li>密集视觉预测领域的通用性面临独特挑战，因为不同任务的标签结构存在差异。</li>
<li>在低数据情况下，将知识泛化到未见过的密集预测任务并不简单。</li>
<li>研究提出了一种基于元学习框架的通用模型，能够灵活适应未见过的密集标签结构。</li>
<li>该模型通过灵活的适应机制和可扩展性等多方面的改进，提高了性能和通用性。</li>
<li>模型在需要低样本学习的各种未见过的真实场景任务中表现出色，包括视频、3D、医疗、生物和用户互动任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e2aeff012717f9a84b7512a889e45603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5616808a65549560af643d350e6a896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f3ea2572ef5a8d6b30ab3a50099b64b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475755f831a6656f579c79941051f6db.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hypothesis-Generation-with-Large-Language-Models"><a href="#Hypothesis-Generation-with-Large-Language-Models" class="headerlink" title="Hypothesis Generation with Large Language Models"></a>Hypothesis Generation with Large Language Models</h2><p><strong>Authors:Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan</strong></p>
<p>Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks. </p>
<blockquote>
<p>有效的生成新假设对科学进步至关重要。迄今为止，研究人员一直是假设生成的主要动力来源，通过艰辛的数据分析和思考（也称为顿悟时刻）来生成假设。在本文中，我们研究了大型语言模型（LLM）生成假设的潜力。我们专注于基于数据（即带有标签的示例）的假设生成。为了使LLM能够处理任意长度的上下文，我们从少量示例生成初始假设，然后迭代更新它们以提高假设的质量。受多臂老虎机的启发，我们设计了一个奖励函数，以在更新过程中实现探索与利用的权衡。我们的算法能够生成假设，使分类任务的预测性能远远超过少样本提示，在合成数据集上提高了31.7%的准确率，在三个真实世界数据集上分别提高了13.9%、3.3%和24.9%。我们在两个具有挑战性的真实世界数据集上的表现也优于监督学习，分别提高了12.8%和11.2%。此外，我们发现生成的假设不仅证实了人类验证的理论，还为任务提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.04326v3">PDF</a> 28 pages, 6 figures, code link:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/hypothesis_generation">https://github.com/ChicagoHAI/hypothesis_generation</a>. Accepted by the 1st   Workshop on NLP for Science (NLP4Science) at EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型在生成新假设方面的潜力，特别是在基于数据（即带标签示例）的假设生成方面。研究提出了一种算法，通过少量示例生成初始假设，并迭代更新以提高假设质量。该算法在分类任务中的预测性能优于少样本提示，可在合成数据集上提高准确率31.7%，并在三个真实世界数据集上分别提高准确率13.9%、3.3%和24.9%。此外，生成的假设不仅证实了人类验证的理论，还为任务提供了新的见解，且优于监督学习。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在生成新假设方面具有潜力。</li>
<li>研究提出了一种基于数据（带标签示例）的假设生成算法。</li>
<li>该算法通过少量示例生成初始假设，并迭代更新以提高假设质量。</li>
<li>算法在分类任务中的预测性能优于少样本提示和现有方法。</li>
<li>在合成数据集和真实世界数据集上，该算法显著提高准确率。</li>
<li>生成的假设不仅证实了人类验证的理论，还提供了新见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.04326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-236f199e0c9dab15c8096d5cdf08f369.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7fccd51bf3ba59244571cef853ebc45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95252e4903948191a950745562dba9e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ceb791c5b546a39658d8bebb6dbacc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FSL-Rectifier-Rectify-Outliers-in-Few-Shot-Learning-via-Test-Time-Augmentation"><a href="#FSL-Rectifier-Rectify-Outliers-in-Few-Shot-Learning-via-Test-Time-Augmentation" class="headerlink" title="FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time   Augmentation"></a>FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time   Augmentation</h2><p><strong>Authors:Yunwei Bai, Ying Kiat Tan, Shiming Chen, Yao Shu, Tsuhan Chen</strong></p>
<p>Few-shot learning (FSL) commonly requires a model to identify images (queries) that belong to classes unseen during training, based on a few labelled samples of the new classes (support set) as reference. So far, plenty of algorithms involve training data augmentation to improve the generalization capability of FSL models, but outlier queries or support images during inference can still pose great generalization challenges. In this work, to reduce the bias caused by the outlier samples, we generate additional test-class samples by combining original samples with suitable train-class samples via a generative image combiner. Then, we obtain averaged features via an augmentor, which leads to more typical representations through the averaging. We experimentally and theoretically demonstrate the effectiveness of our method, obtaining a test accuracy improvement proportion of around 10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given a pretrained image combiner, our method is training-free for off-the-shelf FSL models, whose performance can be improved without extra datasets nor further training of the models themselves. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub">https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub</a>. </p>
<blockquote>
<p>少量学习（FSL）通常需要模型根据新类别的少量标记样本（支持集）作为参考，识别出属于训练期间未见过的类别的图像（查询）。到目前为止，很多算法都涉及训练数据增强，以提高FSL模型的泛化能力，但在推理过程中，异常查询或支持图像仍然可能带来很大的泛化挑战。在这项工作中，为了减少异常样本引起的偏见，我们通过生成式图像组合器将原始样本与合适的训练集样本相结合，生成额外的测试集类别样本。然后，我们通过增强器获得平均特征，通过平均化得到更典型的表示。我们实验和理论上证明了我们的方法的有效性，对于经过训练的FSL模型，测试精度提高比例约为10%（例如，从46.86%提高到53.28%）。重要的是，给定一个预训练的图像组合器，我们的方法对于现成的FSL模型是无需训练的，可以在不额外使用数据集或对模型本身进行进一步训练的情况下提高性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub">https://github.com/WendyBaiYunwei/FSL-Rectifier-Pub</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.18292v6">PDF</a> To be published in AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对少样本学习（FSL）中模型面对未见类别图像识别的问题，通过生成额外的测试类别样本和平均特征的方法，提高了模型的泛化能力，减少了异常样本对模型的影响。该方法无需额外的数据集或对现有模型进行进一步训练，即可提升模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本学习（FSL）在识别未见类别图像时面临挑战。</li>
<li>异常查询或支持图像在推理过程中仍然会带来模型泛化挑战。</li>
<li>通过生成额外的测试类别样本，结合原始样本和适当的训练类别样本，可以减少异常样本的影响。</li>
<li>采用平均特征的方法，得到更典型的表示。</li>
<li>实验和理论验证了该方法的有效性，测试准确率提升比例约为10%。</li>
<li>该方法适用于现成的FSL模型，无需额外数据集或进一步训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.18292">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3dd2b7593a4d3363421151ebaa938a1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4644ebcd5ed4094c527ddcae67912f4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c194dbc4b9b46895a97367673b46b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93131fc137fb2a918d81da8fed91064b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of simple OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>本体匹配（OM）技术能够在不同的本体之间实现语义互操作性，并通过对齐相关实体解决其概念上的异质性。目前，OM系统主要有两种流行的设计范式：传统的基于知识的专家系统和较新的基于机器学习的预测系统。虽然大型语言模型（LLM）和LLM代理已经彻底改变了数据工程，并在许多领域得到了创造性的应用，但它们在OM中的潜力仍然被低估。本研究引入了一种新型基于LLM的代理驱动设计范式，用于OM系统。考虑到利用LLM代理进行OM所面临的若干挑战，我们提出了一个通用框架，即Agent-OM（用于本体匹配的代理），它包含两个用于检索和匹配的Siamese代理和一套简单的OM工具。我们的框架在一个概念验证系统中实现。对三个本体对齐评估倡议（OAEI）赛道上的最新OM系统的评估表明，我们的系统在简单OM任务上的结果非常接近长期以来的最佳性能，并且在复杂和少量本体匹配任务上可以显著提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v5">PDF</a> 19 pages, 13 figures, 4 tables</p>
<p><strong>Summary</strong><br>     本研究提出一种基于新型人工智能技术的智能本体匹配框架Agent-OM，其设计创新地将大型语言模型引入本体匹配系统中，提升系统在复杂和少量数据下的性能表现。通过运用两种智能体（Siamese agents）进行检索和匹配任务，该框架可实现更精准的本体匹配效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本体匹配（OM）通过在不同本体间实现语义互操作性，解决概念异质性问题，促进相关实体的对齐。</li>
<li>当前OM系统主要有两种设计范式：基于知识的专家系统和基于机器学习的预测系统。</li>
<li>大型语言模型（LLM）和LLM代理在数据工程领域具有革命性应用潜力，但在OM方面的应用尚待探索。</li>
<li>本研究提出了一种基于LLM的新型代理驱动OM系统设计范式，即Agent-OM框架。</li>
<li>Agent-OM框架包含两个Siamese代理，用于检索和匹配任务，并配备一套简单的OM工具。</li>
<li>实施了一个概念验证系统来证明Agent-OM的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0dbc27aec0e0a35ba6385cfcae4c3111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dad1989b38d614c47b96192c82520f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1049414e1280085db2a3d9c3ec55934f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7144d14f0bb04f3a3c2414b2ea0a0c73.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bd8b676d9d08045935a64d76fe74cbce.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2024-12-21  Adaptive Prompt Tuning Vision Guided Prompt Tuning with Cross-Attention   for Fine-Grained Few-Shot Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e52f299d172a0c3ea8966cf353feefcd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2024-12-21  Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11189.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
