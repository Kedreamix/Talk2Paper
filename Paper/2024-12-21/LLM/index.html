<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2024-12-21  OpenEMMA Open-Source Multimodal Model for End-to-End Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c533e46ae19cc805c73a90ab515107a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-21-更新"><a href="#2024-12-21-更新" class="headerlink" title="2024-12-21 更新"></a>2024-12-21 更新</h1><h2 id="OpenEMMA-Open-Source-Multimodal-Model-for-End-to-End-Autonomous-Driving"><a href="#OpenEMMA-Open-Source-Multimodal-Model-for-End-to-End-Autonomous-Driving" class="headerlink" title="OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"></a>OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving</h2><p><strong>Authors:Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu</strong></p>
<p>Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in <a target="_blank" rel="noopener" href="https://github.com/taco-group/OpenEMMA">https://github.com/taco-group/OpenEMMA</a>. </p>
<blockquote>
<p>自从多模态大型语言模型（MLLMs）的出现以来，它们在各种现实世界应用中产生了重大影响，特别是在自动驾驶（AD）领域。它们处理复杂视觉数据和推理复杂驾驶场景的能力为端到端AD系统的新范式铺平了道路。然而，开发端到端AD模型的进展缓慢，因为现有的微调方法需要大量资源，包括强大的计算能力、大规模数据集和巨额资金。从最近的推理计算进步中汲取灵感，我们提出了基于MLLMs的开源端到端框架OpenEMMA。通过融入“思维链”推理过程，OpenEMMA在利用多种MLLMs时，相较于基线实现了显著改进。此外，OpenEMMA在多种具有挑战性的驾驶场景中展现了其有效性、通用性和稳健性，为自动驾驶提供了更高效、有效的方法。我们将所有代码发布在<a target="_blank" rel="noopener" href="https://github.com/taco-group/OpenEMMA%E3%80%82">https://github.com/taco-group/OpenEMMA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15208v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型（MLLMs）在自动驾驶（AD）等实际应用领域产生了重大影响。然而，开发端到端的自动驾驶模型进展缓慢，因为现有的微调方法需要大量资源。受推理计算最新进展的启发，我们提出了基于MLLMs的开源端到端框架OpenEMMA。通过融入Chain-of-Thought推理过程，OpenEMMA在利用多种MLLMs时实现了显著改进，并展示了在多种挑战驾驶场景中的有效性、通用性和稳健性，为自动驾驶提供了更高效、更有效的方法。我们已将所有代码发布在<a target="_blank" rel="noopener" href="https://github.com/taco-group/OpenEMMA%E3%80%82">https://github.com/taco-group/OpenEMMA。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在自动驾驶领域具有显著影响。</li>
<li>现有开发端到端自动驾驶模型的进展因资源需求而受到限制。</li>
<li>OpenEMMA是一个基于MLLMs的开源端到端框架，旨在解决自动驾驶问题。</li>
<li>OpenEMMA通过融入Chain-of-Thought推理过程，实现了对多种MLLMs的利用并获得了显著改进。</li>
<li>OpenEMMA在多种挑战驾驶场景中展示了有效性、通用性和稳健性。</li>
<li>OpenEMMA为自动驾驶提供了更高效、更有效的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15208">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-89101137c895e8cd0e60deec7bb92f63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dcd9f5d0abcd66d26e917cffd68ba14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-763c31550ca1a724a9aefe485bd156c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a906c4266210343b24bf43aa51fc47c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMLU-CF-A-Contamination-free-Multi-task-Language-Understanding-Benchmark"><a href="#MMLU-CF-A-Contamination-free-Multi-task-Language-Understanding-Benchmark" class="headerlink" title="MMLU-CF: A Contamination-free Multi-task Language Understanding   Benchmark"></a>MMLU-CF: A Contamination-free Multi-task Language Understanding   Benchmark</h2><p><strong>Authors:Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei</strong></p>
<p>Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs’ understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/MMLU-CF">https://github.com/microsoft/MMLU-CF</a> and the dataset refers to <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/microsoft/MMLU-CF">https://huggingface.co/datasets/microsoft/MMLU-CF</a>. </p>
<blockquote>
<p>类似大规模多任务语言理解（MMLU）这样的多项选择题（MCQ）数据集广泛用于评估大型语言模型（LLM）的常识、理解和问题解决能力。然而，这些基准测试的开源性以及LLM训练数据来源的广泛性不可避免地导致了基准测试污染，从而导致评估结果不可靠。为了缓解这个问题，我们提出了一个无污染且更具挑战性的MCQ基准测试，名为MMLU-CF。这个基准测试通过避免无意和恶意的数据泄露来重新评估LLM对世界知识的理解。为避免无意中的数据泄露，我们从更广泛的领域获取数据，并制定了三条净化规则。为了防止恶意数据泄露，我们将基准测试划分为难度和主题分布相似的验证集和测试集。测试集保持封闭源代码，以确保结果可靠，而验证集面向公众，以促进透明度和独立验证。我们对主流LLM的评估表明，强大的GPT-4在测试集上仅达到5次射击的73.4%得分和0次射击的71.9%得分，这证明了我们方法在创建更严格和无污染评估标准方面的有效性。GitHub仓库位于<a target="_blank" rel="noopener" href="https://github.com/microsoft/MMLU-CF%EF%BC%8C%E6%95%B0%E%E6%8D%AE%E9%9B%86%E8%AF%B7%E5%8F%82%E8%A7%81">https://github.com/microsoft/MMLU-CF，数据集请参见</a>。<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/microsoft/MMLU-CF%E3%80%82">https://huggingface.co/datasets/microsoft/MMLU-CF。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了大规模多任务语言理解（MMLU）等多选题（MCQ）数据集在评估大型语言模型（LLM）的常识、理解和问题解决能力方面的广泛应用。然而，这些基准测试的开源性以及LLM训练数据的广泛来源导致了不可避免的基准测试污染，从而导致评估结果不可靠。为解决这一问题，提出了无污染且更具挑战性的MCQ基准测试MMLU-CF。该基准测试通过避免无意和恶意的数据泄露，重新评估LLM对世界知识的理解。通过从更广泛的领域来源数据并设计三个去污规则，以避免无意的数据泄露。同时，通过将基准测试分为验证集和测试集来防止恶意数据泄露，两者在难度和主题分布上相似。测试集保持封闭以保证结果的可靠性，而验证集则公开以促进透明度和独立验证。对主流LLM的评估显示，强大的GPT-4o在测试集上的5次射击得分仅为73.4%，0次射击得分为71.9%，这证明了我们在创建更严格和无污染的评价标准方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMLU等MCQ数据集广泛用于评估LLM的常识、理解和问题解决能力。</li>
<li>现有基准测试存在污染问题，导致评估结果不可靠。</li>
<li>提出无污染且更具挑战性的MCQ基准测试MMLU-CF。</li>
<li>MMLU-CF通过避免无意和恶意的数据泄露，重新评估LLM的世界知识理解。</li>
<li>通过从更广泛的领域来源数据和设计去污规则来避免数据泄露。</li>
<li>测试集保持封闭以保证结果可靠，验证集公开以促进透明度和独立验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15194">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f0ad7e3bbc84a58d88800a19c5bf43db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d901a83216abe6e124a989444e595749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e14e90e7c07c6feb2e234c50bb6023.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a36e0d6d82d55773f44302be849abd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f2f95adc5f21dd16bee941921d62ef.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues"><a href="#EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues" class="headerlink" title="EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues"></a>EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues</h2><p><strong>Authors:Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and resource management. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 37 downstream applications demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks. </p>
<blockquote>
<p>通过交互式视觉语言模型（VLMs）对大量的地球观测数据进行自动化分析，可以为环境监测、灾害响应和资源管理解锁新的机会。现有的通用VLMs在遥感数据上的表现并不理想，而最近的地理空间VLMs仍然局限于固定的分辨率和少量的传感器模式。在本文中，我们介绍了EarthDial，这是一个专门为地球观测（EO）数据设计的对话助手，将复杂的多感官地球观测转化为交互式的自然语言对话。EarthDial支持多光谱、多时相和多分辨率的影像，能够完成广泛的遥感任务，包括分类、检测、描述、问答、视觉推理和视觉定位。为了实现这一点，我们引入了一个包含超过1111万个指令对的广泛指令调整数据集，涵盖RGB、合成孔径雷达（SAR）和多光谱模式，如近红外（NIR）和红外。此外，EarthDial还处理双时相和多时相序列分析，用于变化检测等应用。我们在37个下游应用上的广泛实验结果证明，EarthDial优于现有的通用和特定领域的模型，在各种EO任务中实现了更好的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15190v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于地球观测的大规模数据，通过交互式的视觉语言模型（VLMs）进行自动化分析，可为环境监测、灾害响应和资源管理带来新的机遇。现有通用VLMs在遥感数据上的表现不佳，而最新的地理空间VLMs仍局限于固定分辨率和少数传感器模态。本文介绍了一款专为地球观测（EO）数据设计的对话助手——EarthDial，它将复杂的多感官地球观测数据转化为交互式自然语言对话。EarthDial支持多光谱、多时相和多分辨率影像，可应对多种遥感任务，包括分类、检测、描述、问答、视觉推理和视觉定位等。为达成这一目标，我们引入了包含超过1亿一千万指令对的庞大指令调整数据集，涵盖RGB、合成孔径雷达（SAR）和多光谱模式，如近红外和红外等。此外，EarthDial还处理双时相和多时相序列分析，适用于变化检测等应用。在37个下游应用上的广泛实验结果表明，EarthDial在多种EO任务上优于现有通用和特定领域的模型，实现了良好的泛化性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>地球观测数据通过交互式视觉语言模型（VLMs）进行自动化分析具有巨大潜力，有助于环境监测、灾害响应和资源管理。</li>
<li>当前VLMs在遥感数据应用上存在局限性，需要专门的模型来处理地球观测数据。</li>
<li>EarthDial是一款专为地球观测数据设计的对话助手，支持多光谱、多时相和多分辨率影像分析。</li>
<li>EarthDial通过引入大规模指令调整数据集，覆盖多种传感器模态，实现了优异的性能。</li>
<li>EarthDial能处理复杂的遥感任务，包括分类、检测、描述、问答、视觉推理和视觉定位等。</li>
<li>EarthDial还具备处理双时相和多时相序列分析的能力，适用于变化检测等应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-89a1b482c8060c0525e5c4207ae63529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ca0e5a40001e6d6bda93656a4ece5dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a6f1eb946288c4b0e889de136f1d2b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24886458342ef2005f55155879b5d0b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b1d45ce232d179cb36fda8ff10ea4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd697d813a0a074d4803710bb9f3d4a2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HPC-Coder-V2-Studying-Code-LLMs-Across-Low-Resource-Parallel-Languages"><a href="#HPC-Coder-V2-Studying-Code-LLMs-Across-Low-Resource-Parallel-Languages" class="headerlink" title="HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages"></a>HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages</h2><p><strong>Authors:Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele</strong></p>
<p>Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date. </p>
<blockquote>
<p>基于大型语言模型（LLM）的编码工具作为软件开发助手取得了巨大的成功，但它们通常是为通用编程任务而设计的，对于高性能计算等特殊领域表现不佳。在这些领域创建专业模型和相关工具对于在诸如高性能计算等领域获得大型语言模型的益处至关重要。尽管先前的工作已经探索了针对高性能计算的专业模型，但大型语言模型在生成并行代码方面仍然面临困难，尚不清楚是什么障碍仍然阻碍着这些大型语言模型的发展，以及需要采取什么措施来克服这些障碍。在这项工作中，我们沿着许多轴对专业高性能计算大型语言模型进行了深入研究，以更好地了解所面临的挑战。基于我们的发现，我们对专业的高性能计算大型语言模型进行了微调并进行了评估，被证明迄今为止是并行代码生成方面表现最好的开源代码大型语言模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在软件开发助手领域取得了巨大成功，但对于专业领域如高性能计算（HPC）等特定任务的编程，其表现并不理想。为了在这些领域获取LLM的益处，创建专门化的模型和工具至关重要。当前对于特定于HPC的模型探索尚存在挑战，尤其是生成并行代码方面。本研究对精细调整HPC特殊LLM的多个方面进行了深入研究，以更好地理解挑战所在，并基于研究对LLM进行了精细调整与评估，显示出迄今为止在并行代码生成方面的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在作为软件开发助手方面取得了显著成功，但在专业领域如高性能计算（HPC）中的表现有待提高。</li>
<li>创建针对特定领域的LLM模型和工具，是提升LLM在HPC等领域表现的关键。</li>
<li>目前LLM在生成并行代码方面存在困难。</li>
<li>本研究通过深入研究并精细调整一个针对HPC的LLM模型，取得显著成果。</li>
<li>此LLM模型在并行代码生成方面被证实是目前最佳的开源代码LLM。</li>
<li>对LLM模型的进一步研究和改进仍有必要，以解决其在特定领域面临的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f921286d8ce697078a2295b2459b222.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edbe578400e7ce985a146a02c68e2cd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df82e98812142b1bf44d6f3c6cdc93d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3762c672d32971de228fe23267c67333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed9477542137f861c90d31815be78578.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Critical-Questions-of-Thought-Steering-LLM-reasoning-with-Argumentative-Querying"><a href="#Critical-Questions-of-Thought-Steering-LLM-reasoning-with-Argumentative-Querying" class="headerlink" title="Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative   Querying"></a>Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative   Querying</h2><p><strong>Authors:Federico Castagna, Isabel Sassoon, Simon Parsons</strong></p>
<p>Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin’s model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models’ reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models’ output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided. </p>
<blockquote>
<p>尽管人工智能研究近期取得了突破性的快速进展，但先进的大型语言模型（LLM）在逻辑和数学推理方面仍然面临挑战。</p>
</blockquote>
<p>研究结果似乎表明，LLM仍然更像是（高度先进的）数据模式识别器，在尝试推广和解决模型之前未见或与其训练数据样本不相似的问题时，表现不佳。</p>
<p>为了解决这一令人关注的问题，本文借鉴了论证理论文献中的批判性问题概念，特别是聚焦于图尔敏的论证模型。</p>
<p>我们表明，利用这些批判性问题可以改善LLM的推理能力。通过探究模型推理过程背后的理性，LLM可以评估是否发生了逻辑错误并在提供最终回答之前进行纠正。</p>
<p>任何有效的论证程序的黄金标准背后的基本思想是：如果结论是由接受的前提所蕴含的就是有效的。或者，用亚里士多德原理在现实世界的近似值来转述，在充满不完全信息和预设逻辑的情况下，如果未被证明是错误的，那么结论就是有效的。</p>
<p>这种方法成功地通过推理管道引导了模型的输出，与基线及其思维链（CoT）实现相比，在逻辑推理数学任务上表现出更好的性能。为此，在多个LLM上对提出的方法进行了广泛的评估。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15177v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>近期AI研究的突破和快速发展并未完全解决大型语言模型（LLM）的逻辑和数学推理能力问题。LLM主要还是作为高级数据模式识别器，对于未曾见过或训练数据样本之外的推理问题，其泛化能力较差。本文借鉴论证理论文献中的批判性问题概念，特别是托尔敏的论证模型，展示运用这些问题可以改善LLM的推理能力。通过探究模型推理过程的理性，LLM能够在出现逻辑错误时识别并纠正，再对用户提示给出最终答复。该方法的成功引导了模型通过推理管道输出，提高了在基准测试和链式思维（CoT）实施中的表现。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM尽管有突破性进展，但在逻辑和数学推理方面仍存在挑战。</li>
<li>LLM主要作为高级数据模式识别器，对未见过的推理问题泛化能力有限。</li>
<li>批判性问题可从论证理论借鉴，用以改善LLM的推理能力。</li>
<li>通过探究模型推理过程的理性，LLM能识别并纠正逻辑错误。</li>
<li>使用批判性问题能成功引导模型通过推理管道输出。</li>
<li>该方法提高了LLM在基准测试和链式思维实施中的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f6796247686731d1ebbce4f77220009f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b8c358101f6b2f45058151979ebc960.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-Uncertainty-Estimation-in-Natural-Language-Generation"><a href="#Rethinking-Uncertainty-Estimation-in-Natural-Language-Generation" class="headerlink" title="Rethinking Uncertainty Estimation in Natural Language Generation"></a>Rethinking Uncertainty Estimation in Natural Language Generation</h2><p><strong>Authors:Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter</strong></p>
<p>Large Language Models (LLMs) are increasingly employed in real-world applications, driving the need to evaluate the trustworthiness of their generated text. To this end, reliable uncertainty estimation is essential. Since current LLMs generate text autoregressively through a stochastic process, the same prompt can lead to varying outputs. Consequently, leading uncertainty estimation methods generate and analyze multiple output sequences to determine the LLM’s uncertainty. However, generating output sequences is computationally expensive, making these methods impractical at scale. In this work, we inspect the theoretical foundations of the leading methods and explore new directions to enhance their computational efficiency. Building on the framework of proper scoring rules, we find that the negative log-likelihood of the most likely output sequence constitutes a theoretically grounded uncertainty measure. To approximate this alternative measure, we propose G-NLL, which has the advantage of being obtained using only a single output sequence generated by greedy decoding. This makes uncertainty estimation more efficient and straightforward, while preserving theoretical rigor. Empirical results demonstrate that G-NLL achieves state-of-the-art performance across various LLMs and tasks. Our work lays the foundation for efficient and reliable uncertainty estimation in natural language generation, challenging the necessity of more computationally involved methods currently leading the field. </p>
<blockquote>
<p>大型语言模型（LLM）在现实世界应用中的使用越来越广泛，这引发了对其生成文本可信度的评估需求。为此，可靠的不确定性估计至关重要。由于当前的LLM通过随机过程自回归地生成文本，相同的提示可能会导致不同的输出。因此，主流的不确定性估计方法会生成并分析多个输出序列来确定LLM的不确定性。然而，生成输出序列的计算成本很高，使得这些方法在大规模应用时不太实用。在这项工作中，我们检查了主流方法的理论基石，并探索了提高计算效率的新方向。基于适当的评分规则框架，我们发现最可能的输出序列的负对数可能性构成了一个有理论依据的不确定性度量。为了近似这个替代度量，我们提出了G-NLL，它的优点是只使用贪婪解码生成的一个输出序列即可获得。这使得不确定性估计更加高效和直观，同时保留了理论严谨性。实证结果表明，G-NLL在各种LLM和任务上达到了最新技术水平。我们的工作奠定了高效可靠的自然语言生成不确定性估计的基础，挑战了当前领域主导的更复杂计算方法的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在现实世界应用中的可信度评估至关重要，不确定性估计是关键。当前LLM通过随机过程自回归生成文本，同一提示可能产生不同输出，导致不确定性。主流的不确定性估计方法通过生成和分析多个输出序列来评估LLM的不确定性，但计算成本高昂，难以大规模应用。本文检查主流方法的理论基础，探索提高计算效率的新方向。基于适当的评分规则框架，我们发现最可能输出序列的负对数可能性是一个理论上可靠的不确定性度量。我们提出G-NLL来近似这一度量，只需使用贪婪解码生成的单个输出序列即可获得，提高了不确定性估计的效率和直观性，同时保持了理论严谨性。实证结果表明，G-NLL在不同LLM和任务上均达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在现实世界应用中的可信度评估很重要，不确定性估计是关键。</li>
<li>当前LLM通过随机过程生成文本，同一提示可能产生不同输出。</li>
<li>主流的不确定性估计方法计算成本高，难以大规模应用。</li>
<li>基于适当的评分规则框架，最可能输出序列的负对数可能性是理论上可靠的不确定性度量。</li>
<li>提出G-NLL方法近似该度量，只需单个输出序列，提高计算效率和直观性。</li>
<li>G-NLL在不同LLM和任务上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5e8427064fb7ee8915283d18b73028b8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adaptive-Pruning-for-Large-Language-Models-with-Structural-Importance-Awareness"><a href="#Adaptive-Pruning-for-Large-Language-Models-with-Structural-Importance-Awareness" class="headerlink" title="Adaptive Pruning for Large Language Models with Structural Importance   Awareness"></a>Adaptive Pruning for Large Language Models with Structural Importance   Awareness</h2><p><strong>Authors:Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han</strong></p>
<p>The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展显著提高了语言理解和生成能力。然而，由于边缘设备资源受限，LLM的部署面临计算资源和存储资源需求高的挑战。为了解决这一问题，我们提出了一种新型LLM模型裁剪方法，称为结构感知自适应裁剪（SAAP），在保持模型性能的同时，显著降低计算和内存成本。我们首先定义了一个自适应重要性融合度量标准，通过考虑同方差不确定性来评估LLM中所有耦合结构的重要性。然后，我们对所有模块进行重要性排名，以确定为满足特定性能要求而应裁剪的特定层。此外，我们开发了一种新的组微调策略，以提高LLM的推理效率。最后，我们在两个常见任务上评估了所提出的SAAP方法，即零样本分类和文本生成。实验结果表明，我们的SAAP方法在LLaMA-7B、Vicuna-7B和LLaMA-13B上优于几种最新基线方法，分别实现了2.17%、2.37%和2.39%的准确率提升。此外，SAAP提高了令牌生成速度5%，在资源受限场景中展现出其实用优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15127v1">PDF</a> 12 pages, 6 figures, 12 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）的最新进展显著提高了语言理解和生成能力。然而，由于LLMs对计算和存储资源的高需求，将其部署在资源受限的边缘设备上具有挑战性。为此，我们提出了一种名为结构感知自适应剪枝（SAAP）的新型LLM模型剪枝方法，以显著降低计算和内存成本同时保持模型性能。我们通过考虑同构不确定性来定义自适应重要性融合指标，评估LLMs中所有耦合结构的重要性。然后，我们根据模块重要性排名，确定应剪枝的特定层，以满足特定的性能要求。此外，我们开发了一种新的组微调策略，以提高LLMs的推理效率。实验结果表明，我们的SAAP方法在多个LLMs上优于几种最新基线方法，在零样本分类和文本生成两个常见任务中分别实现了2.17%、2.37%和2.39%的精度提升。此外，SAAP还提高了令牌生成速度，在资源受限场景中表现出实际优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在语言理解和生成方面取得显著进展，但部署在资源受限的边缘设备上具有挑战性。</li>
<li>提出了一种新型LLM模型剪枝方法——结构感知自适应剪枝（SAAP），以降低计算和内存成本。</li>
<li>通过自适应重要性融合指标评估LLMs中所有耦合结构的重要性。</li>
<li>根据模块重要性排名，确定应剪枝的特定层，以满足性能要求。</li>
<li>开发了一种新的组微调策略，以提高LLMs的推理效率。</li>
<li>SAAP方法在多个LLMs上优于最新基线方法，在零样本分类和文本生成任务中实现了精度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1f572959e6d54fba0f388d5d84de2f84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-108e7259b00e853fba8c43a089ae3e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b55d6b852b1ad68416229694493e04a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-561eb86205e53df60819b45ba86e81e0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Outcome-Refining-Process-Supervision-for-Code-Generation"><a href="#Outcome-Refining-Process-Supervision-for-Code-Generation" class="headerlink" title="Outcome-Refining Process Supervision for Code Generation"></a>Outcome-Refining Process Supervision for Code Generation</h2><p><strong>Authors:Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang</strong></p>
<p>Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: <a target="_blank" rel="noopener" href="https://github.com/zhuohaoyu/ORPS">https://github.com/zhuohaoyu/ORPS</a> </p>
<blockquote>
<p>大型语言模型在代码生成方面展现出显著的能力，但在需要深度算法推理的复杂编程任务上常常遇到困难。虽然通过学习奖励模型进行的进程监督在引导推理步骤方面显示出希望，但它需要昂贵的训练数据，并且存在评价不可靠的问题。我们提出了结果优化进程监督（Outcome-Refining Process Supervision），这是一种将结果优化本身作为要监督的过程的新型范式。我们的框架利用具体的执行信号来夯实推理步骤的监督，同时使用树形探索来同时维持多个解决方案轨迹。实验表明，我们的方法使甚至更小的模型能够在竞争性编程任务上实现高成功率和性能指标，相比于传统奖励模型创建更可靠的验证，无需训练PRMs。我们的方法在5个模型和3个数据集上实现了显著改进：正确率平均提高26.9%，效率提高42.2%。结果表明，提供具有具体验证信号的结构化推理空间对于解决复杂编程任务至关重要。我们在以下网址公开了所有代码和数据：<a target="_blank" rel="noopener" href="https://github.com/zhuohaoyu/ORPS">https://github.com/zhuohaoyu/ORPS</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15118v1">PDF</a> 18 pages, 5 figures, Code: <a target="_blank" rel="noopener" href="https://github.com/zhuohaoyu/ORPS">https://github.com/zhuohaoyu/ORPS</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型在代码生成方面展现出显著能力，但在需要深度算法推理的复杂编程任务上常遇挑战。现有流程监督方法虽有望引导推理步骤，但需依赖昂贵训练数据且评估不可靠。本研究提出一种新型监督范式——成果优化流程监督，将成果优化本身作为监督流程。此框架结合具体执行信号来监督推理步骤，同时采用树形探索来同时维护多个解决方案轨迹。实验证明，该方法使较小模型在竞赛编程任务上实现了高成功率和性能指标，较传统奖励模型创造了更可靠的验证，且无需训练PRMs。该方法在5个模型和3个数据集上的平均正确性提高了26.9%，效率提高了42.2%。结果表明，提供具有具体验证信号的结构化推理空间对于解决复杂编程任务至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在代码生成上表现出优异能力，但在复杂编程任务中的深度算法推理能力受限。</li>
<li>传统流程监督方法需大量昂贵的训练数据且评估结果不稳定。</li>
<li>提出一种新型监督范式——成果优化流程监督，将成果优化过程作为监督对象。</li>
<li>结合具体执行信号来监督推理步骤，并采用树形探索维护多个解决方案轨迹。</li>
<li>实验证明该方法在多个模型和数据集上显著提高模型在复杂编程任务上的正确性和效率。</li>
<li>与传统奖励模型相比，该方法创造更可靠的验证且无需训练PRMs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c533e46ae19cc805c73a90ab515107a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fab10e850b38e57c9f0bb2f200f2b60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67d00aaba27f36f1f0aaa2172cb816f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f52817428f74faad3843703261fa98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8abfddb9a8a18dc8e71338480d0a396.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03501d1b9019d22bddf64da0c7c3ea5c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Qwen2-5-Technical-Report"><a href="#Qwen2-5-Technical-Report" class="headerlink" title="Qwen2.5 Technical Report"></a>Qwen2.5 Technical Report</h2><p><strong>Authors: Qwen,  :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu</strong></p>
<p>In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models. </p>
<blockquote>
<p>在这份报告中，我们介绍了Qwen2.5，这是一系列全面的大型语言模型（LLM），旨在满足不同需求。相较于之前的版本，Qwen 2.5在预训练和后期训练阶段都得到了显著的提升。在预训练方面，我们将高质量的预训练数据集从之前的7万亿标记扩展到18万亿标记。这为常识、专业知识推理能力提供了坚实的基础。在后期训练方面，我们实施了复杂的监督微调，样本数量超过1百万，以及多阶段强化学习。后期训练技术提高了人类偏好，并显著改善了长文本生成、结构化数据分析和指令遵循。为了有效地处理多样化和多变的使用场景，我们推出了丰富的Qwen2.5 LLM系列。开放权重的产品包括基础版和指令调优模型，还有量化版本可供选择。此外，对于托管解决方案，专有模型目前包括两个混合专家（MoE）版本：Qwen2.5-Turbo和Qwen2.5-Plus，都可在阿里云模型工作室使用。Qwen2.5已在广泛的语言理解、推理、数学、编码、人类偏好对齐等基准测试中展现出卓越性能。特别是开放权重的旗舰产品Qwen2.5-72B-Instruct在许多开放和专有模型中表现出色，与最先进的开放权重模型Llama-3-405B-Instruct表现相当，尽管后者规模大约是前者的5倍。Qwen2.5-Turbo和Qwen2.5-Plus具有出色的性价比，与GPT-4o-mini和GPT-4o相比表现良好。此外，作为基石，Qwen2.5模型在训练专业化模型方面发挥了重要作用，如Qwen2.5-Math、Qwen2.5-Coder、QwQ和多模态模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15115v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>介绍全面的大型语言模型系列Qwen2.5，该系列模型在预训练和微调阶段都有显著改进。预训练阶段使用了更大规模的高质数据集，达到18万亿标记，增强了常识、专业知识和推理能力。微调阶段采用复杂的监督微调技术和多阶段强化学习，提高了长文本生成、结构化数据分析和指令遵循能力。提供多种不同大小的模型，包括基础模型和指令微调模型以及量化版本。专有模型包括MoE变体Qwen2.5-Turbo和Qwen2.5-Plus，可在阿里云模型工作室中使用。Qwen2.5在多种评估基准测试中表现出卓越性能，如语言理解、推理、数学、编码、人类偏好对齐等。其中，开源旗舰模型Qwen2.5-72B-Instruct性能优异，与大型开源模型Llama-3-405B-Instruct竞争。Qwen2.5-Turbo和Qwen2.5-Plus具有高性价比，与GPT-4o-mini和GPT-4o相当。Qwen2.5还为特殊模型和跨模态模型（如Qwen2.5-Math、Qwen2.5-Coder和QwQ）的培训奠定了基础。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Qwen2.5是一系列大型语言模型，旨在满足不同的需求。</li>
<li>与之前的版本相比，Qwen 2.5在预训练和微调阶段都有显著的改进。</li>
<li>预训练数据集规模从7万亿标记扩大到18万亿标记，增强了模型的常识、专业知识和推理能力。</li>
<li>复杂的监督微调和多阶段强化学习提高了长文本生成、结构化数据分析和指令遵循能力。</li>
<li>Qwen2.5提供多种不同大小的模型，包括开源和专有模型。</li>
<li>Qwen2.5系列在多种基准测试中表现出卓越性能，包括语言理解、推理、数学和编码等任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0cf4ec0597473eadf289da917a948f25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48058794ecf9bdf047a296b899c8ac64.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Nano-ESG-Extracting-Corporate-Sustainability-Information-from-News-Articles"><a href="#Nano-ESG-Extracting-Corporate-Sustainability-Information-from-News-Articles" class="headerlink" title="Nano-ESG: Extracting Corporate Sustainability Information from News   Articles"></a>Nano-ESG: Extracting Corporate Sustainability Information from News   Articles</h2><p><strong>Authors:Fabian Billert, Stefan Conrad</strong></p>
<p>Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.   An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at <a target="_blank" rel="noopener" href="https://github.com/Bailefan/Nano-ESG">https://github.com/Bailefan/Nano-ESG</a>. </p>
<blockquote>
<p>确定公司的可持续性影响是一个高度复杂的主题，过去几年里越来越受到关注。如今，投资者主要依赖几家已建立评级机构提供的可持续性评级来评估公司的负责任程度。然而，这些评级最近受到批评，因为它们难以理解且几乎无法复制。了解公司可持续性实践的独立方式在于丰富的新闻文章数据景观。在本文中，我们探索一种不同的方法来识别公司在可持续性领域的主要机遇和挑战。我们展示了一个新颖的数据集，其中包含2023年1月至2024年9月为德国主要公司收集的超过84万篇新闻文章。通过应用一系列自然语言处理技术，我们首先识别出相关文章，然后对其进行总结，并利用大型语言模型（LLM）提取与可持续性相关的情感和方面。此外，我们对获得的数据进行了评估，并确定LLM生成的答案是准确的。我们在<a target="_blank" rel="noopener" href="https://github.com/Bailefan/Nano-ESG%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/Bailefan/Nano-ESG发布数据集。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15093v1">PDF</a> To be published at ECIR 2025. Preprint</p>
<p><strong>摘要</strong></p>
<p>公司可持续性影响评估是一个复杂的主题，近年来越来越受到关注。目前，投资者主要依赖成熟的评级机构来评估公司的可持续性。然而，这些评级存在难以理解且难以复制的问题而受到批评。新闻文章数据提供了一个了解公司可持续性实践的独立途径。本文探索了一种不同的方法，以识别公司在可持续性领域的主要机遇和挑战。我们展示了一个包含超过84万篇新闻文章的新数据集，这些文章是关于德国主要公司在2023年1月至2024年9月之间的信息。通过应用自然语言处理技术，我们首先识别出相关文章，然后对其进行总结，并利用大型语言模型提取与可持续性相关的情感和方面。此外，我们对所得数据进行了评估，并确定了大型语言模型答案的准确性。我们在<a target="_blank" rel="noopener" href="https://github.com/Bailefan/Nano-ESG%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E8%BF%99%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/Bailefan/Nano-ESG上发布了这两个数据集。</a></p>
<p><strong>关键要点</strong></p>
<p>一、确定公司可持续性影响的重要性及投资者对此的需求增加。虽然传统评级得到广泛信任，但存在的难以理解和无法复制的问题促使寻求其他独立方法了解公司的可持续性实践。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e92ba8eca6174383b4ce861c6957fd70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb333061295202f1e05dcdce4802c37.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AceMath-Advancing-Frontier-Math-Reasoning-with-Post-Training-and-Reward-Modeling"><a href="#AceMath-Advancing-Frontier-Math-Reasoning-with-Post-Training-and-Reward-Modeling" class="headerlink" title="AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward   Modeling"></a>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward   Modeling</h2><p><strong>Authors:Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</strong></p>
<p>In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/acemath">https://research.nvidia.com/labs/adlr/acemath</a> </p>
<blockquote>
<p>本文介绍了AceMath，这是一系列前沿的数学模型，擅长解决复杂的数学问题，以及高效的奖励模型，能够评估生成的解决方案并可靠地识别正确的解决方案。为了开发指令调优的数学模型，我们提出了一种监督微调（SFT）流程，该流程首先在一般领域实现具有竞争力的性能，然后使用精心挑选的提示和合成生成的响应进行针对数学领域的微调。AceMath-72B-Instruct模型在性能上大大超越了Qwen2.5-Math-72B-Instruct、GPT-4o和Claude-3.5 Sonnet。为了开发专门的数学奖励模型，我们首先构建了AceMath-RewardBench，这是一个全面且稳健的基准测试，用于评估不同问题和难度级别的数学奖励模型。之后，我们提出了一种构建数学奖励模型的系统方法。AceMath-72B-RM模型始终优于最新的奖励模型。此外，当将AceMath-72B-Instruct与AceMath-72B-RM相结合时，我们在数学推理基准测试中获得了最高的平均rm@8分数。我们将在<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/acemath">https://research.nvidia.com/labs/adlr/acemath</a> 发布模型权重、训练数据和评估基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15084v1">PDF</a> </p>
<p><strong>Summary</strong>：本文介绍了AceMath系列的前沿数学模型，该模型擅长解决复杂的数学问题，并配备了高效的奖励模型来评估生成的解决方案和准确识别正确答案。通过监督微调（SFT）过程，AceMath模型在通用领域取得有竞争力的表现后，使用精心挑选的提示和合成生成响应进行有针对性的微调。AceMath模型在多个基准测试中表现优异，且构建了一个专门的奖励模型来评估数学问题的解答质量。将AceMath模型与奖励模型结合使用，可获得最佳的平均rm@8分数。模型的权重、训练数据和评估基准将在NVIDIA的官方网站上发布。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AceMath是一个用于解决复杂数学问题的前沿数学模型套件。</li>
<li>AceMath配备了高效的奖励模型，能够评估生成的解决方案并准确识别正确答案。</li>
<li>通过监督微调（SFT）过程，AceMath模型首先在通用领域取得竞争力，然后针对数学领域进行微调。</li>
<li>AceMath模型在多个基准测试中表现优于其他模型。</li>
<li>构建了AceMath-RewardBench基准，用于评估数学奖励模型在不同问题和难度级别上的表现。</li>
<li>提出了构建数学奖励模型的系统性方法，并发布了AceMath-72B-RM奖励模型，其表现优于现有奖励模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7133db55a42afc04213fe6c92dc96c0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc895e27d85bfd7915d19a8abc9dd501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a733a8c56f4a9e563f396fd866eaa8e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe373e3fc411bdf79ad86d6dca639f99.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLMs-Lost-in-Translation-M-ALERT-uncovers-Cross-Linguistic-Safety-Gaps"><a href="#LLMs-Lost-in-Translation-M-ALERT-uncovers-Cross-Linguistic-Safety-Gaps" class="headerlink" title="LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps"></a>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</h2><p><strong>Authors:Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting</strong></p>
<p>Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities. </p>
<blockquote>
<p>构建多语言安全的大型语言模型（LLM）对于确保安全访问和语言多样性至关重要。为此，我们引入了M-ALERT，这是一个跨五种语言评估LLM安全性的多语言基准测试，包括英语、法语、德语、意大利语和西班牙语。M-ALERT遵循详细的ALERT分类法，每种语言包含1.5万个高质量提示，总计7.5万个。我们对10种最新的大型语言模型进行了广泛的实验，凸显了特定语言的安全分析的重要性，结果表明，模型在语言和类别上的安全性能存在显著的不一致性。例如，Llama 3.2在意大利语犯罪税类别中存在较高的不安全风险，但在其他语言中表现安全。所有模型之间都可以观察到类似的差异。相比之下，某些类别（如大麻和犯罪宣传）在所有模型和语言中都能触发不安全的响应。这些发现强调了在大型语言模型中建立稳健的多语言安全实践的需要，以确保在不同用户群体中的安全和负责任的使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15035v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多语言大型语言模型（LLM）的安全构建对于保障安全访问和语言多样性至关重要。为此，我们推出了M-ALERT，这是一个跨五种语言的多语言基准测试，旨在评估LLM的安全性。M-ALERT包含按详细警报分类的75k高质量提示。对十种最新LLM的广泛实验表明，语言特定的安全分析至关重要，因为模型在不同的语言和类别中经常出现显著的安全不一致性。例如，在某些情况下，某些模型在某些语言和类别中表现出较高的不安全性，而在其他语言和类别中则表现良好。这些发现强调了确保LLM在安全稳健的多语种实践中的必要性，以确保在不同用户群体中的安全和负责任使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M-ALERT是一个用于评估大型语言模型（LLM）安全性的多语言基准测试，涵盖五种语言。</li>
<li>M-ALERT包含基于详细警报分类的75k高质量提示。</li>
<li>实验显示，不同LLM在不同语言和类别中的安全性表现存在显著差异。</li>
<li>某些模型在某些语言和类别中表现出较高的不安全性。</li>
<li>存在某些类别，如物质滥用和犯罪宣传等，始终触发不安全响应。</li>
<li>需要进行稳健的多语言安全实践，以确保LLM在不同用户群体中的安全和负责任使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15035">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-47af63d086ecd6c042e900fb41e51b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-618be958eace0ccb0c9cbe06cf07c69a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c02313a87794259ed3747c051b06baaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6af95263bd2220fb12bd66979b96cde3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7773ef560f44a8d14afbecca0d0e7f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2eeff8cbc94dca6290d8811f1746323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16ee5eaca9853114d527cb236d88755.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Chain-of-MetaWriting-Linguistic-and-Textual-Analysis-of-How-Small-Language-Models-Write-Young-Students-Texts"><a href="#Chain-of-MetaWriting-Linguistic-and-Textual-Analysis-of-How-Small-Language-Models-Write-Young-Students-Texts" class="headerlink" title="Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small   Language Models Write Young Students Texts"></a>Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small   Language Models Write Young Students Texts</h2><p><strong>Authors:Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu</strong></p>
<p>Large Language Models (LLMs) have been used to generate texts in response to different writing tasks: reports, essays, story telling. However, language models do not have a meta-representation of the text writing process, nor inherent communication learning needs, comparable to those of young human students. This paper introduces a fine-grained linguistic and textual analysis of multilingual Small Language Models’ (SLMs) writing. With our method, Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process, such as planning and evaluation. We mainly focused on short story and essay writing tasks in French for schoolchildren and undergraduate students respectively. Our results show that SLMs encounter difficulties in assisting young students on sensitive topics such as violence in the schoolyard, and they sometimes use words too complex for the target audience. In particular, the output is quite different from the human produced texts in term of text cohesion and coherence regarding temporal connectors, topic progression, reference. </p>
<blockquote>
<p>大型语言模型（LLM）已被用于生成回应不同写作任务的文本，如报告、文章、讲故事。然而，语言模型并不具备文本写作过程的元表示，也没有与年轻人类学生相当的内在沟通学习需求。本文介绍了对多语种小型语言模型（SLM）写作的精细语言与文本分析。通过我们的方法——MetaWriting链，SLM可以模仿人类写作过程中的一些步骤，如规划和评估。我们主要关注针对儿童和本科生分别进行的短故事和文章写作任务。结果表明，SLM在辅助年轻学生处理敏感话题（如校园暴力）时遇到困难，它们有时会使用目标受众难以理解的太复杂的词汇。尤其体现在输出文本在文本连贯性和一致性方面与人类产生的文本有很大差异，涉及时间连接词、主题进展和参考等方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14986v1">PDF</a> Accepted at WRAICOGS 2025 (Writing Aids at the Crossroads of AI,   Cognitive Science, and NLP) co-located with COLING 2025</p>
<p><strong>Summary</strong>：本论文介绍了基于Chain-of-MetaWriting方法的对小规模语言模型（SLMs）的写作进行精细的语言和文本分析。尽管大型语言模型（LLMs）广泛应用于文本生成任务，但小型语言模型更关注特定年龄段学生的写作需求。SLMs能模仿人类写作过程的部分步骤，如规划和评估。然而，在某些针对儿童和本科生的短文和写作任务中，小型语言模型面临处理敏感话题的挑战，例如校园暴力，并有时使用对目标受众过于复杂的词汇。其输出文本在连贯性和一致性方面与人类文本存在差异。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>小型语言模型（SLMs）可模仿人类写作过程的规划与评价步骤。</li>
<li>SLMs应用于特定年龄段学生的写作任务时存在挑战，如处理敏感话题。</li>
<li>在处理校园暴力等敏感话题时，SLMs有时使用对目标受众过于复杂的词汇。</li>
<li>与人类文本相比，SLMs输出的文本在连贯性和一致性方面存在差异。特别是在时态连词、话题进展和参考等方面有明显差距。</li>
<li>通过Chain-of-MetaWriting方法进行的文本分析更加细致深入地了解SLMs在写作方面的性能。</li>
<li>大型语言模型（LLMs）在生成文本时缺乏对实际写作过程的元表示和沟通学习需求。相比之下，SLMs更注重特定任务和目标受众的需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14986">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33e0913ad9017149c891c92c9456f6a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d316df2da0d830a800a6f8d1593bc120.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-21\./crop_LLM/2412.14986v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation"><a href="#TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation" class="headerlink" title="TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"></a>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</h2><p><strong>Authors:Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li</strong></p>
<p>In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5% on TOMG-Bench. Our codes and datasets are available through <a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench">https://github.com/phenixace/TOMG-Bench</a>. </p>
<blockquote>
<p>本文提出了基于文本的开放分子生成基准测试（TOMG-Bench），这是第一个评估大型语言模型在开放域分子生成能力方面的基准测试。TOMG-Bench包含三个主要任务的数据集：分子编辑（MolEdit）、分子优化（MolOpt）和定制分子生成（MolCustom）。每个任务进一步包含三个子任务，每个子任务包含5000个测试样本。鉴于开放分子生成的固有复杂性，我们还开发了一个自动化评估系统，帮助衡量生成分子的质量和准确性。我们对25个大型语言模型的全面基准测试揭示了文本引导分子发现当前的局限性和潜在的改进领域。此外，借助专为解决TOMG-Bench提出的挑战而设计的专用指令调整数据集OpenMolIns，Llama3.1-8B可以超越所有开源通用大型语言模型，甚至在TOMG-Bench上的表现超过GPT-3.5-turbo的46.5%。我们的代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/phenixace/TOMG-Bench获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14642v1">PDF</a> A benchmark for text-based open molecule generation</p>
<p><strong>Summary</strong><br>文本提出了一个基于文本的开放分子生成基准测试（TOMG-Bench），这是第一个评估大型语言模型（LLM）在开放域分子生成能力方面的基准测试。TOMG-Bench包括三个主要任务：分子编辑（MolEdit）、分子优化（MolOpt）和定制分子生成（MolCustom）。每个任务包含三个子任务，每个子任务包含5000个测试样本。文本还介绍了一个自动化评估系统，用于评估生成分子的质量和准确性。对25个LLM的全面基准测试揭示了文本引导分子发现方面的当前局限性和潜在的改进领域。使用OpenMolIns这一专用指令调整数据集解决了TOMG-Bench提出的挑战，Llama3.1-8B的表现超过了所有开源通用LLM，甚至超过了GPT-3.5 Turbo的46.5%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本提出了TOMG-Bench，这是一个用于评估大型语言模型在开放域分子生成能力方面的基准测试。</li>
<li>TOMG-Bench包括三个主要任务：分子编辑、分子优化和定制分子生成。</li>
<li>自动化评估系统用于评估生成分子的质量和准确性。</li>
<li>对多个LLM的全面基准测试揭示了当前局限性和潜在的改进领域。</li>
<li>OpenMolIns数据集被用于解决TOMG-Bench的挑战。</li>
<li>Llama3.1-8B在TOMG-Bench上的表现超过了其他LLM，包括GPT-3.5 Turbo。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14642">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d82cb8b8fa1fe22ad856dcbdd8bb87c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3ea1b90efb97df8d551454ed5ab085b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23ed033b9fd9e96bfc0666d948365f59.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models"><a href="#Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models" class="headerlink" title="Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models"></a>Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models</h2><p><strong>Authors:Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai</strong></p>
<p>This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models’ original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs. </p>
<blockquote>
<p>本文介绍了Typhoon 2，这是一系列针对泰语优化的文本和多模态大型语言模型。该系列包括文本、视觉和音频模型。Typhoon2-Text建立在最前沿的开放模型上，如Llama 3和Qwen2，我们对英语和泰语数据的混合进行持续预训练。我们采用后训练技术，在提高泰语性能的同时，保留基础模型的原始功能。我们发布了一系列不同大小的文本模型，参数范围从1亿到70亿，既有基础模型也有指令调整型变种。为了引导文本生成，我们发布了Typhoon2-Safety，这是一个专为泰国文化和语言增强的分类器。Typhoon2-Vision在保留通用视觉功能（如图像描述）的同时，提高了对泰语文档的理解能力。Typhoon2-Audio引入了一种端到端的语音到语音模型架构，能够处理音频、语音和文本输入，并生成文本和语音输出。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13702v2">PDF</a> technical report, 55 pages</p>
<p><strong>Summary</strong>：<br>本文介绍了Typhoon 2系列文本和多模态大型语言模型，该系列模型针对泰语进行优化，包括文本、视觉和音频模型。Typhoon2-Text基于最前沿的开放模型，如Llama 3和Qwen2，进行持续预训练，使用混合的英语和泰语数据。采用后训练技术提高泰语性能，同时保留基础模型的原始能力。发布了一系列不同规模的文本模型，包括基础型和指令调整型。为引导文本生成，推出了Typhoon2-Safety，一个增强泰语文化和语言的分类器。Typhoon2-Vision提高了对泰语文档的理解能力，同时保留了图像的一般功能，如图像标题生成。Typhoon2-Audio引入了一种端到端的语音到语音模型架构，能够处理音频、语音和文本输入，并生成文本和语音输出。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Typhoon 2系列模型是针对泰语优化的文本和多模态大型语言模型。</li>
<li>Typhoon 2包括文本、视觉和音频模型，旨在满足多种需求。</li>
<li>Typhoon2-Text基于前沿开放模型进行持续预训练，并使用混合的英语和泰语数据。</li>
<li>后训练技术用于提高泰语性能，同时保留基础模型的原始能力。</li>
<li>发布了一系列不同规模的文本模型，包括不同尺寸和类型的基础型和指令调整型模型。</li>
<li>Typhoon2-Safety分类器用于引导文本生成，适应泰语文化和语言特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-827c73ec6d26ad52cff0c2a413bda65b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="G-VEval-A-Versatile-Metric-for-Evaluating-Image-and-Video-Captions-Using-GPT-4o"><a href="#G-VEval-A-Versatile-Metric-for-Evaluating-Image-and-Video-Captions-Using-GPT-4o" class="headerlink" title="G-VEval: A Versatile Metric for Evaluating Image and Video Captions   Using GPT-4o"></a>G-VEval: A Versatile Metric for Evaluating Image and Video Captions   Using GPT-4o</h2><p><strong>Authors:Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung</strong></p>
<p>Evaluation metric of visual captioning is important yet not thoroughly explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are limited in zero-shot scenarios. Advanced Language Model-based metrics also struggle with aligning to nuanced human preferences. To address these issues, we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and supports three modes: reference-free, reference-only, and combined, accommodating both video and image inputs. We also propose MSVD-Eval, a new dataset for video captioning evaluation, to establish a more transparent and consistent framework for both human experts and evaluation metrics. It is designed to address the lack of clear criteria in existing datasets by introducing distinct dimensions of Accuracy, Completeness, Conciseness, and Relevance (ACCR). Extensive results show that G-VEval outperforms existing methods in correlation with human annotations, as measured by Kendall tau-b and Kendall tau-c. This provides a flexible solution for diverse captioning tasks and suggests a straightforward yet effective approach for large language models to understand video content, paving the way for advancements in automated captioning. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/ztangaj/gveval">https://github.com/ztangaj/gveval</a> </p>
<blockquote>
<p>视觉标题的评价指标非常重要，但尚未被完全探索。传统的评价指标，如BLEU、METEOR、CIDEr和ROUGE，往往忽略了语义深度，而训练指标如CLIP-Score、PAC-S和Polos在零样本场景中受到限制。先进的语言模型基础指标也很难与微妙的人类偏好对齐。为了解决这些问题，我们引入了G-VEval，这是一个受G-Eval启发的新指标，由新的GPT-4o提供支持。G-VEval使用大型多模态模型中的思维链推理，支持三种模式：无参考、仅有参考和组合，适应视频和图像输入。我们还提出了MSVD-Eval，一个新的视频标题评价数据集，为人工专家和评价指标建立一个更透明和一致的评价框架。它通过引入准确性、完整性、简洁性和相关性（ACCR）的不同维度，解决了现有数据集中缺乏明确标准的问题。大量结果表明，G-VEval在与人类注释的相关性方面优于现有方法，如Kendall tau-b和Kendall tau-c所示。这为各种标题任务提供了灵活的解决方案，并为大型语言模型理解视频内容提供了一种简单有效的方法，为自动标题制作的进步铺平了道路。相关代码可访问：<a target="_blank" rel="noopener" href="https://github.com/ztangaj/gveval">https://github.com/ztangaj/gveval</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13647v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了视觉描述评估的重要性及现有评估方法的不足。为此，引入了新型评估方法G-VEval，并介绍了其三种模式，该评估方法以GPT-4o为驱动支持链式推理方式评估图像和视频描述，效果显著优于传统和现有的自动评估指标。同时介绍了MSVD-Eval数据集的设计和特性。这一改进方案为未来自动描述评估领域提供了新的方向。相关代码已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前视觉描述评估方法的局限：传统的BLEU等方法忽视了语义深度；训练的度量标准在零样本场景下存在局限性；高级语言模型度量难以与微妙的人类偏好对齐。</li>
<li>新提出的评估方法：介绍G-VEval指标及其主要特性，利用大型多模态模型的链式推理技术。提供三种模式以满足不同的需求：无参考模式、仅参考模式和组合模式。支持视频和图像输入。</li>
<li>新数据集MSVD-Eval的介绍：专为视频描述评估设计，旨在解决现有数据集缺乏明确标准的问题。引入四个维度：准确性、完整性、简洁性和相关性（ACCR）。</li>
<li>G-VEval性能表现：与人类注释相比，G-VEval在相关性评价方面表现出优异性能，如Kendall tau-b和Kendall tau-c的评价结果所示。它为多样化的描述任务提供了灵活解决方案。</li>
<li>LLM对视频内容的理解能力提升：使用新型评估方式可以进一步推动大型语言模型对视频内容的理解，为未来自动化描述领域的发展铺平道路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40b8583c074dad2ce2d61f48f2b27ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb206cb892842077b56340e9b1d86e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39e064d904cb63025dbc1ca392b3a98d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b49aff845cc408d02d4d73e4980d4b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-facbbae06cae95267265a876be4c932f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Does-VLM-Classification-Benefit-from-LLM-Description-Semantics"><a href="#Does-VLM-Classification-Benefit-from-LLM-Description-Semantics" class="headerlink" title="Does VLM Classification Benefit from LLM Description Semantics?"></a>Does VLM Classification Benefit from LLM Description Semantics?</h2><p><strong>Authors:Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Björn Ommer</strong></p>
<p>Accurately describing images with text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect, where multiple modified text prompts act as a noisy test-time augmentation for the original one. We propose an alternative evaluation scenario to decide if a performance boost of LLM-generated descriptions is caused by such a noise augmentation effect or rather by genuine description semantics. The proposed scenario avoids noisy test-time augmentation and ensures that genuine, distinctive descriptions cause the performance boost. Furthermore, we propose a training-free method for selecting discriminative descriptions that work independently of classname-ensembling effects. Our approach identifies descriptions that effectively differentiate classes within a local CLIP label neighborhood, improving classification accuracy across seven datasets. Additionally, we provide insights into the explainability of description-based image classification with VLMs. </p>
<blockquote>
<p>用文本准确描述图像是解释性人工智能的基础。像CLIP这样的跨视觉语言模型（VLM）通过在共享嵌入空间中对齐图像和文本，表达视觉和语言嵌入之间的语义相似性，解决了这一问题。利用大型语言模型（LLM）生成的描述可以改善VLM分类。然而，由于性能提升也可能源于一种语义无关的集成效应，即多个修改后的文本提示作为原始提示的噪声测试时间增强，因此很难确定实际描述语义的贡献。我们提出了一种替代的评价场景，以确定LLM生成的描述的性能提升是由于噪声增强效应还是真正的描述语义所造成的。所提出的场景避免了噪声测试时间增强，并确保真正的、有特色的描述会引起性能提升。此外，我们提出了一种无需训练的选择有辨别力的描述方法，该方法独立于类别集成效应。我们的方法能够识别在CLIP标签邻域内有效区分类别的描述，从而提高七个数据集的分类精度。此外，我们还深入探讨了基于描述的图像分类与VLM的可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11917v3">PDF</a> AAAI-25 (extended version), Code: <a target="_blank" rel="noopener" href="https://github.com/CompVis/DisCLIP">https://github.com/CompVis/DisCLIP</a></p>
<p><strong>Summary</strong></p>
<p>基于文本的图像准确描述是解释性人工智能的基础。本文提出一种评估场景，旨在确定大型语言模型（LLM）生成的描述性能提升是由于噪声测试时间增强效应还是真正的描述语义引起的。同时，提出了一种独立于训练的选择性鉴别描述方法，该方法能识别本地CLIP标签邻域内有效区分不同类别的描述，并在七个数据集上提高了分类精度。此外，本文还深入探讨了基于描述的图像分类的可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision-Language Models (VLMs)如CLIP通过对图像和文本在共享嵌入空间中的对齐，表达了视觉和语言嵌入之间的语义相似性，实现了图像的文本描述。</li>
<li>LLM生成的描述可以改善VLM分类，但确定性能提升的真正来源是一个挑战。</li>
<li>提出了一种评估场景来确定LLM生成的描述性能提升是否源于真正的描述语义，避免了噪声测试时间增强效应。</li>
<li>提出了一种独立于训练的选择性鉴别描述方法，能识别在本地CLIP标签邻域内有效区分不同类别的描述，提高了分类精度。</li>
<li>在七个数据集上验证了该方法的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11917">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-29d64018c8376442da19635bbbac8d53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efdcfdda3be02c28fec68a831d4f111f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37c0827cceac6c14ae9606ecddf3511c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b1318707ade6f3c01af6c13f774a599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee3f9109dfac30ac6e5e3f246440a8f2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by “selecting sparsely and computing densely”. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>当前针对LLM的PEFT方法只能同时实现高质量、高效训练或可扩展的服务，而无法三者兼顾。为了解决这个问题，我们研究了稀疏微调技术，并观察到其显著提高了泛化能力。利用这一关键见解，我们针对LLM提出了一系列结构化稀疏微调（S$^{2}$FT）方法，这些方法同时实现了最先进的微调性能、训练效率和推理可扩展性。S$^{2}$FT通过“稀疏选择、密集计算”来实现这一目标。它分别选择MHA和FFN模块中每个Transformer块的几个头和通道。接下来，它对LLM中耦合结构两侧的权重矩阵进行共置换，以将每层的所选组件连接成密集的子矩阵。最后，S$^{2}$FT对所有子矩阵执行原地梯度更新。通过理论分析和实证结果，我们的方法既防止遗忘又简化了优化过程，在常识和算术推理方面都达到了最新性能水平，与LoRA相比平均提高了4.6%和1.3%，在指令微调后推广到不同领域时，较全量微调提高了11.5%。通过使用我们的部分反向传播算法，S$^{2}$FT在训练内存方面节省了高达3倍，与全量微调相比，延迟提高了1.5-2.7倍，同时在两个指标上都较LoRA平均提高了10%。我们进一步证明，S$^{2}$FT中的权重更新可以解耦为适配器，为实现多个微调模型的有效融合、快速切换和高效并行服务提供支持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文探讨了对LLM（大型语言模型）的PEFT（参数高效微调）方法的局限性，提出了一种名为结构化稀疏微调（S²FT）的新方法。该方法能够在实现微调性能、训练效率和推理可扩展性的同时达到最佳状态。S²FT通过选择稀疏计算密集的方式实现这一目标，它通过选择Transformer块中MHA和FFN模块的少数头部和通道，并对LLM中耦合结构的权重矩阵进行共排列，将所选组件在每层中连接成密集子矩阵。最后，S²FT对所有子矩阵进行原地梯度更新。理论分析和实验结果表明，该方法防止遗忘，简化优化，在常识推理和算术推理方面均达到最佳性能。此外，S²FT还通过部分反向传播算法节省训练内存，提高延迟，同时提高性能。最后，文章展示了S²FT中的权重更新可以被解耦为适配器，为融合多个微调模型提供了有效、快速和高效并行的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前PEFT方法对LLMs存在局限性，无法同时实现高质量、高效训练和可扩展服务。</li>
<li>引入稀疏微调并观察到其提高泛化能力。</li>
<li>提出结构化稀疏微调（S²FT）方法，同时实现最佳微调性能、训练效率和推理可扩展性。</li>
<li>S²FT通过选择稀疏部分并进行密集计算来实现这一目标。</li>
<li>S²FT在理论分析和实验结果上表现出优秀的性能，特别是在常识和算术推理方面。</li>
<li>S²FT采用部分反向传播算法，节省训练内存并提高延迟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06289">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0c5cc0d9ede3c4970f3bcc409080770a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BayLing-2-A-Multilingual-Large-Language-Model-with-Efficient-Language-Alignment"><a href="#BayLing-2-A-Multilingual-Large-Language-Model-with-Efficient-Language-Alignment" class="headerlink" title="BayLing 2: A Multilingual Large Language Model with Efficient Language   Alignment"></a>BayLing 2: A Multilingual Large Language Model with Efficient Language   Alignment</h2><p><strong>Authors:Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng</strong></p>
<p>Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life. However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide. An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly. In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment. To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages. Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale. For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages. Furthermore, results on English benchmarks indicate that BayLing maintains high performance in highresource languages while enhancing the performance in low-resource languages. Demo, homepage, code and models of BayLing are available. </p>
<blockquote>
<p>大型语言模型（LLM）具有强大的生成能力和丰富的知识，能够支持日常生活中的各种任务。然而，这些能力主要集中在高资源语言上，导致低资源语言的生成能力较弱，知识相对有限。因此，增强LLM的多语言能力对于服务于全球100多种语言社区至关重要。增强多语言能力的一种直观方法是为各种语言构建指令数据，但是为超过100种语言构建指令数据的成本高昂。在本文中，我们介绍了BayLing 2，它通过语言对齐，有效地将从高资源语言转移到低资源语言的生成能力和知识。为此，我们构建了一个包含320万条指令的数据集，其中包括高资源语言指令（中文和英文）和100多种语言的跨语言指令，并基于该数据集进行指令调整，以促进语言之间的能力转移。我们以Llama为基础模型，开发了BayLing-2-7B、BayLing-2-13B和BayLing-2-8B，并对BayLing进行了全面评估。在100多种语言的跨语言翻译方面，BayLing与类似规模的开源模型相比表现出卓越的性能。在多语言知识和理解基准测试中，BayLing在超过20种低资源语言上取得了显著的改进，这证明了其从高资源语言到低资源语言的有效知识转移能力。此外，在英语基准测试上的结果表表明，BayLing在高资源语言上保持高性能的同时，提高了在低资源语言上的性能。BayLing的演示、主页、代码和模型都已提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16300v3">PDF</a> BayLing 2’s online demo: <a target="_blank" rel="noopener" href="http://nlp.ict.ac.cn/bayling/demo">http://nlp.ict.ac.cn/bayling/demo</a>. BayLing   2’s code and models: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/BayLing">https://github.com/ictnlp/BayLing</a></p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）在日常生活中的各种任务中展现了强大的生成能力和广泛的知识。然而，这些能力主要集中在高资源语言上，导致低资源语言的生成能力较弱，知识相对有限。增强LLM的多语言能力对于服务全球100多种语言社区至关重要。本文介绍了BayLing 2，它通过语言对齐，高效地从高资源语言向低资源语言转移生成能力和知识。我们构建了包含320万条指令的数据集，该数据集包含高资源语言指令（中文和英文）以及适用于100多种语言的跨语言指令，并基于该数据集进行指令调整，以促进语言间的能力转移。使用Llama作为基础模型，我们开发了BayLing-2-7B、BayLing-2-13B和BayLing-2-8B，并对BayLing进行了全面评估。在多语种翻译和多种语言知识理解方面，BayLing表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在日常生活中的任务中展现了强大的生成能力和知识，但低资源语言的生成能力较弱。</li>
<li>增强LLM的多语言能力对服务全球各种语言社区至关重要。</li>
<li>BayLing 2通过语言对齐，从高资源语言向低资源语言转移生成能力和知识。</li>
<li>BayLing构建了包含高资源语言和跨语言指令的数据集，用于指令调整。</li>
<li>BayLing-2模型系列是基于Llama基础模型开发的。</li>
<li>BayLing在多语种翻译和多种语言知识理解方面表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16300">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-14b21ae1803c025f67dafec51a75efaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a41976009602db1b1a7c55c4aacf103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94c3f0552a2c59041ddaf6111b1f3128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f84ffff19295258ea4fe9f1d467e8cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b88bd7042014a6d52ca28cb26744a84c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65cb59c2aaaf5042c9ea47db332b61b7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CodeLutra-Boosting-LLM-Code-Generation-via-Preference-Guided-Refinement"><a href="#CodeLutra-Boosting-LLM-Code-Generation-via-Preference-Guided-Refinement" class="headerlink" title="CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement"></a>CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement</h2><p><strong>Authors:Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra</strong></p>
<p>Large Language Models (LLMs) have revolutionized code generation but require significant resources and often over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective alternative. However, standard supervised approaches rely only on correct examples, missing valuable insights from failures. We introduce CodeLutra, a framework that leverages both correct and incorrect code attempts. Instead of using only correct solutions, CodeLutra applies iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This approach narrows the performance gap with state-of-the-art larger models without requiring massive datasets or auxiliary models. For instance, on a challenging data science coding task, using only 500 samples improved Llama-3-8B’s accuracy from 28.2% to 48.6%, approaching GPT-4’s level. By learning from both successes and mistakes, CodeLutra provides a scalable and efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives. </p>
<blockquote>
<p>大规模语言模型（LLMs）已经彻底改变了代码生成的面貌，但它们需要巨大的资源，并且经常过度泛化，限制了其在特定任务上的效率。微调较小、开源的LLMs提供了一种成本效益高的替代方案。然而，标准的监督方法只依赖于正确的例子，忽略了失败中宝贵的见解。我们引入了CodeLutra框架，它利用正确的和错误的代码尝试。CodeLutra并不只使用正确的解决方案，而是采用基于偏好的迭代优化，比较成功和失败的输出来更好地近似期望的结果。这种方法在不需要大规模数据集或辅助模型的情况下，缩小了与最新先进大型模型之间的性能差距。例如，在一个具有挑战性的数据科学编码任务中，仅使用500个样本就提高了Llama-3-8B的准确率，从28.2%提高到48.6%，接近GPT-4的水平。通过从成功和错误中学习，CodeLutra为高质量代码生成提供了一条可扩展和高效的路径，使得较小的开源模型与领先的闭源替代品更具竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05199v2">PDF</a> 16 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>CodeLutra框架改变了传统的大型语言模型（LLM）的代码生成方式。它通过结合正确和错误的代码尝试，利用偏好基础的迭代优化方法，缩小了与顶尖大型模型的性能差距。CodeLutra使得小型开源模型也能生成高质量代码，提高了任务特定效率，同时降低了成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CodeLutra利用正确和错误的代码尝试，提高了代码生成的效率和质量。</li>
<li>CodeLutra通过比较成功和失败的输出，更好地近似期望结果。</li>
<li>CodeLutra能够缩小小型开源模型与顶尖大型模型之间的性能差距。</li>
<li>CodeLutra提高了小型开源模型的任务特定效率。</li>
<li>CodeLutra降低了代码生成的成本，因为它可以更有效地利用资源。</li>
<li>CodeLutra在仅使用500个样本的情况下，就能显著提高LLama-3-8B模型的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05199">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-51622908dfd1088bcc83b49721f6b855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e4f1b0ba9a0ff8e8b6e3f6fc891e9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cc729737df900fb38e8b36a15260a68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d759c4738fd132fbc28570c43f0b4d9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e52f299d172a0c3ea8966cf353feefcd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2024-12-21  Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e80b19e2c7dc8e34c906e815957f9ebe.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-20  CAD-Recode Reverse Engineering CAD Code from Point Clouds
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
