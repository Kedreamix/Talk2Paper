<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  OpenEMMA Open-Source Multimodal Model for End-to-End Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c533e46ae19cc805c73a90ab515107a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-21-æ›´æ–°"><a href="#2024-12-21-æ›´æ–°" class="headerlink" title="2024-12-21 æ›´æ–°"></a>2024-12-21 æ›´æ–°</h1><h2 id="OpenEMMA-Open-Source-Multimodal-Model-for-End-to-End-Autonomous-Driving"><a href="#OpenEMMA-Open-Source-Multimodal-Model-for-End-to-End-Autonomous-Driving" class="headerlink" title="OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"></a>OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving</h2><p><strong>Authors:Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu</strong></p>
<p>Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in <a target="_blank" rel="noopener" href="https://github.com/taco-group/OpenEMMA">https://github.com/taco-group/OpenEMMA</a>. </p>
<blockquote>
<p>è‡ªä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ä»¥æ¥ï¼Œå®ƒä»¬åœ¨å„ç§ç°å®ä¸–ç•Œåº”ç”¨ä¸­äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸã€‚å®ƒä»¬å¤„ç†å¤æ‚è§†è§‰æ•°æ®å’Œæ¨ç†å¤æ‚é©¾é©¶åœºæ™¯çš„èƒ½åŠ›ä¸ºç«¯åˆ°ç«¯ADç³»ç»Ÿçš„æ–°èŒƒå¼é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œå¼€å‘ç«¯åˆ°ç«¯ADæ¨¡å‹çš„è¿›å±•ç¼“æ…¢ï¼Œå› ä¸ºç°æœ‰çš„å¾®è°ƒæ–¹æ³•éœ€è¦å¤§é‡èµ„æºï¼ŒåŒ…æ‹¬å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›ã€å¤§è§„æ¨¡æ•°æ®é›†å’Œå·¨é¢èµ„é‡‘ã€‚ä»æœ€è¿‘çš„æ¨ç†è®¡ç®—è¿›æ­¥ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMLLMsçš„å¼€æºç«¯åˆ°ç«¯æ¡†æ¶OpenEMMAã€‚é€šè¿‡èå…¥â€œæ€ç»´é“¾â€æ¨ç†è¿‡ç¨‹ï¼ŒOpenEMMAåœ¨åˆ©ç”¨å¤šç§MLLMsæ—¶ï¼Œç›¸è¾ƒäºåŸºçº¿å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒOpenEMMAåœ¨å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„é©¾é©¶åœºæ™¯ä¸­å±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†æ›´é«˜æ•ˆã€æœ‰æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†æ‰€æœ‰ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/taco-group/OpenEMMA%E3%80%82">https://github.com/taco-group/OpenEMMAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15208v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç­‰å®é™…åº”ç”¨é¢†åŸŸäº§ç”Ÿäº†é‡å¤§å½±å“ã€‚ç„¶è€Œï¼Œå¼€å‘ç«¯åˆ°ç«¯çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹è¿›å±•ç¼“æ…¢ï¼Œå› ä¸ºç°æœ‰çš„å¾®è°ƒæ–¹æ³•éœ€è¦å¤§é‡èµ„æºã€‚å—æ¨ç†è®¡ç®—æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMLLMsçš„å¼€æºç«¯åˆ°ç«¯æ¡†æ¶OpenEMMAã€‚é€šè¿‡èå…¥Chain-of-Thoughtæ¨ç†è¿‡ç¨‹ï¼ŒOpenEMMAåœ¨åˆ©ç”¨å¤šç§MLLMsæ—¶å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šç§æŒ‘æˆ˜é©¾é©¶åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬å·²å°†æ‰€æœ‰ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/taco-group/OpenEMMA%E3%80%82">https://github.com/taco-group/OpenEMMAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>ç°æœ‰å¼€å‘ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„è¿›å±•å› èµ„æºéœ€æ±‚è€Œå—åˆ°é™åˆ¶ã€‚</li>
<li>OpenEMMAæ˜¯ä¸€ä¸ªåŸºäºMLLMsçš„å¼€æºç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶é—®é¢˜ã€‚</li>
<li>OpenEMMAé€šè¿‡èå…¥Chain-of-Thoughtæ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†å¯¹å¤šç§MLLMsçš„åˆ©ç”¨å¹¶è·å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>OpenEMMAåœ¨å¤šç§æŒ‘æˆ˜é©¾é©¶åœºæ™¯ä¸­å±•ç¤ºäº†æœ‰æ•ˆæ€§ã€é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>OpenEMMAä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-89101137c895e8cd0e60deec7bb92f63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dcd9f5d0abcd66d26e917cffd68ba14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-763c31550ca1a724a9aefe485bd156c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a906c4266210343b24bf43aa51fc47c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMLU-CF-A-Contamination-free-Multi-task-Language-Understanding-Benchmark"><a href="#MMLU-CF-A-Contamination-free-Multi-task-Language-Understanding-Benchmark" class="headerlink" title="MMLU-CF: A Contamination-free Multi-task Language Understanding   Benchmark"></a>MMLU-CF: A Contamination-free Multi-task Language Understanding   Benchmark</h2><p><strong>Authors:Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei</strong></p>
<p>Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMsâ€™ understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/MMLU-CF">https://github.com/microsoft/MMLU-CF</a> and the dataset refers to <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/microsoft/MMLU-CF">https://huggingface.co/datasets/microsoft/MMLU-CF</a>. </p>
<blockquote>
<p>ç±»ä¼¼å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰è¿™æ ·çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQï¼‰æ•°æ®é›†å¹¿æ³›ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¸¸è¯†ã€ç†è§£å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•çš„å¼€æºæ€§ä»¥åŠLLMè®­ç»ƒæ•°æ®æ¥æºçš„å¹¿æ³›æ€§ä¸å¯é¿å…åœ°å¯¼è‡´äº†åŸºå‡†æµ‹è¯•æ±¡æŸ“ï¼Œä»è€Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— æ±¡æŸ“ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„MCQåŸºå‡†æµ‹è¯•ï¼Œåä¸ºMMLU-CFã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•é€šè¿‡é¿å…æ— æ„å’Œæ¶æ„çš„æ•°æ®æ³„éœ²æ¥é‡æ–°è¯„ä¼°LLMå¯¹ä¸–ç•ŒçŸ¥è¯†çš„ç†è§£ã€‚ä¸ºé¿å…æ— æ„ä¸­çš„æ•°æ®æ³„éœ²ï¼Œæˆ‘ä»¬ä»æ›´å¹¿æ³›çš„é¢†åŸŸè·å–æ•°æ®ï¼Œå¹¶åˆ¶å®šäº†ä¸‰æ¡å‡€åŒ–è§„åˆ™ã€‚ä¸ºäº†é˜²æ­¢æ¶æ„æ•°æ®æ³„éœ²ï¼Œæˆ‘ä»¬å°†åŸºå‡†æµ‹è¯•åˆ’åˆ†ä¸ºéš¾åº¦å’Œä¸»é¢˜åˆ†å¸ƒç›¸ä¼¼çš„éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æµ‹è¯•é›†ä¿æŒå°é—­æºä»£ç ï¼Œä»¥ç¡®ä¿ç»“æœå¯é ï¼Œè€ŒéªŒè¯é›†é¢å‘å…¬ä¼—ï¼Œä»¥ä¿ƒè¿›é€æ˜åº¦å’Œç‹¬ç«‹éªŒè¯ã€‚æˆ‘ä»¬å¯¹ä¸»æµLLMçš„è¯„ä¼°è¡¨æ˜ï¼Œå¼ºå¤§çš„GPT-4åœ¨æµ‹è¯•é›†ä¸Šä»…è¾¾åˆ°5æ¬¡å°„å‡»çš„73.4%å¾—åˆ†å’Œ0æ¬¡å°„å‡»çš„71.9%å¾—åˆ†ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨åˆ›å»ºæ›´ä¸¥æ ¼å’Œæ— æ±¡æŸ“è¯„ä¼°æ ‡å‡†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚GitHubä»“åº“ä½äº<a target="_blank" rel="noopener" href="https://github.com/microsoft/MMLU-CF%EF%BC%8C%E6%95%B0%E%E6%8D%AE%E9%9B%86%E8%AF%B7%E5%8F%82%E8%A7%81">https://github.com/microsoft/MMLU-CFï¼Œæ•°æ®é›†è¯·å‚è§</a>ã€‚<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/microsoft/MMLU-CF%E3%80%82">https://huggingface.co/datasets/microsoft/MMLU-CFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰ç­‰å¤šé€‰é¢˜ï¼ˆMCQï¼‰æ•°æ®é›†åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¸¸è¯†ã€ç†è§£å’Œé—®é¢˜è§£å†³èƒ½åŠ›æ–¹é¢çš„å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•çš„å¼€æºæ€§ä»¥åŠLLMè®­ç»ƒæ•°æ®çš„å¹¿æ³›æ¥æºå¯¼è‡´äº†ä¸å¯é¿å…çš„åŸºå‡†æµ‹è¯•æ±¡æŸ“ï¼Œä»è€Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†æ— æ±¡æŸ“ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„MCQåŸºå‡†æµ‹è¯•MMLU-CFã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡é¿å…æ— æ„å’Œæ¶æ„çš„æ•°æ®æ³„éœ²ï¼Œé‡æ–°è¯„ä¼°LLMå¯¹ä¸–ç•ŒçŸ¥è¯†çš„ç†è§£ã€‚é€šè¿‡ä»æ›´å¹¿æ³›çš„é¢†åŸŸæ¥æºæ•°æ®å¹¶è®¾è®¡ä¸‰ä¸ªå»æ±¡è§„åˆ™ï¼Œä»¥é¿å…æ— æ„çš„æ•°æ®æ³„éœ²ã€‚åŒæ—¶ï¼Œé€šè¿‡å°†åŸºå‡†æµ‹è¯•åˆ†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†æ¥é˜²æ­¢æ¶æ„æ•°æ®æ³„éœ²ï¼Œä¸¤è€…åœ¨éš¾åº¦å’Œä¸»é¢˜åˆ†å¸ƒä¸Šç›¸ä¼¼ã€‚æµ‹è¯•é›†ä¿æŒå°é—­ä»¥ä¿è¯ç»“æœçš„å¯é æ€§ï¼Œè€ŒéªŒè¯é›†åˆ™å…¬å¼€ä»¥ä¿ƒè¿›é€æ˜åº¦å’Œç‹¬ç«‹éªŒè¯ã€‚å¯¹ä¸»æµLLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¼ºå¤§çš„GPT-4oåœ¨æµ‹è¯•é›†ä¸Šçš„5æ¬¡å°„å‡»å¾—åˆ†ä»…ä¸º73.4%ï¼Œ0æ¬¡å°„å‡»å¾—åˆ†ä¸º71.9%ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬åœ¨åˆ›å»ºæ›´ä¸¥æ ¼å’Œæ— æ±¡æŸ“çš„è¯„ä»·æ ‡å‡†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMLUç­‰MCQæ•°æ®é›†å¹¿æ³›ç”¨äºè¯„ä¼°LLMçš„å¸¸è¯†ã€ç†è§£å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨æ±¡æŸ“é—®é¢˜ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚</li>
<li>æå‡ºæ— æ±¡æŸ“ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„MCQåŸºå‡†æµ‹è¯•MMLU-CFã€‚</li>
<li>MMLU-CFé€šè¿‡é¿å…æ— æ„å’Œæ¶æ„çš„æ•°æ®æ³„éœ²ï¼Œé‡æ–°è¯„ä¼°LLMçš„ä¸–ç•ŒçŸ¥è¯†ç†è§£ã€‚</li>
<li>é€šè¿‡ä»æ›´å¹¿æ³›çš„é¢†åŸŸæ¥æºæ•°æ®å’Œè®¾è®¡å»æ±¡è§„åˆ™æ¥é¿å…æ•°æ®æ³„éœ²ã€‚</li>
<li>æµ‹è¯•é›†ä¿æŒå°é—­ä»¥ä¿è¯ç»“æœå¯é ï¼ŒéªŒè¯é›†å…¬å¼€ä»¥ä¿ƒè¿›é€æ˜åº¦å’Œç‹¬ç«‹éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0ad7e3bbc84a58d88800a19c5bf43db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d901a83216abe6e124a989444e595749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e14e90e7c07c6feb2e234c50bb6023.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a36e0d6d82d55773f44302be849abd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f2f95adc5f21dd16bee941921d62ef.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues"><a href="#EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues" class="headerlink" title="EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues"></a>EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues</h2><p><strong>Authors:Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and resource management. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 37 downstream applications demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks. </p>
<blockquote>
<p>é€šè¿‡äº¤äº’å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹å¤§é‡çš„åœ°çƒè§‚æµ‹æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æï¼Œå¯ä»¥ä¸ºç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œèµ„æºç®¡ç†è§£é”æ–°çš„æœºä¼šã€‚ç°æœ‰çš„é€šç”¨VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œè€Œæœ€è¿‘çš„åœ°ç†ç©ºé—´VLMsä»ç„¶å±€é™äºå›ºå®šçš„åˆ†è¾¨ç‡å’Œå°‘é‡çš„ä¼ æ„Ÿå™¨æ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†EarthDialï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºåœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©æ‰‹ï¼Œå°†å¤æ‚çš„å¤šæ„Ÿå®˜åœ°çƒè§‚æµ‹è½¬åŒ–ä¸ºäº¤äº’å¼çš„è‡ªç„¶è¯­è¨€å¯¹è¯ã€‚EarthDialæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡çš„å½±åƒï¼Œèƒ½å¤Ÿå®Œæˆå¹¿æ³›çš„é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡1111ä¸‡ä¸ªæŒ‡ä»¤å¯¹çš„å¹¿æ³›æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–RGBã€åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå¤šå…‰è°±æ¨¡å¼ï¼Œå¦‚è¿‘çº¢å¤–ï¼ˆNIRï¼‰å’Œçº¢å¤–ã€‚æ­¤å¤–ï¼ŒEarthDialè¿˜å¤„ç†åŒæ—¶ç›¸å’Œå¤šæ—¶ç›¸åºåˆ—åˆ†æï¼Œç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚æˆ‘ä»¬åœ¨37ä¸ªä¸‹æ¸¸åº”ç”¨ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜ï¼ŒEarthDialä¼˜äºç°æœ‰çš„é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œåœ¨å„ç§EOä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15190v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºåœ°çƒè§‚æµ‹çš„å¤§è§„æ¨¡æ•°æ®ï¼Œé€šè¿‡äº¤äº’å¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æï¼Œå¯ä¸ºç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œèµ„æºç®¡ç†å¸¦æ¥æ–°çš„æœºé‡ã€‚ç°æœ‰é€šç”¨VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œè€Œæœ€æ–°çš„åœ°ç†ç©ºé—´VLMsä»å±€é™äºå›ºå®šåˆ†è¾¨ç‡å’Œå°‘æ•°ä¼ æ„Ÿå™¨æ¨¡æ€ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾ä¸“ä¸ºåœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©æ‰‹â€”â€”EarthDialï¼Œå®ƒå°†å¤æ‚çš„å¤šæ„Ÿå®˜åœ°çƒè§‚æµ‹æ•°æ®è½¬åŒ–ä¸ºäº¤äº’å¼è‡ªç„¶è¯­è¨€å¯¹è¯ã€‚EarthDialæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡å½±åƒï¼Œå¯åº”å¯¹å¤šç§é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ç­‰ã€‚ä¸ºè¾¾æˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«è¶…è¿‡1äº¿ä¸€åƒä¸‡æŒ‡ä»¤å¯¹çš„åºå¤§æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–RGBã€åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå¤šå…‰è°±æ¨¡å¼ï¼Œå¦‚è¿‘çº¢å¤–å’Œçº¢å¤–ç­‰ã€‚æ­¤å¤–ï¼ŒEarthDialè¿˜å¤„ç†åŒæ—¶ç›¸å’Œå¤šæ—¶ç›¸åºåˆ—åˆ†æï¼Œé€‚ç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚åœ¨37ä¸ªä¸‹æ¸¸åº”ç”¨ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒEarthDialåœ¨å¤šç§EOä»»åŠ¡ä¸Šä¼˜äºç°æœ‰é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œå®ç°äº†è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°çƒè§‚æµ‹æ•°æ®é€šè¿‡äº¤äº’å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œæœ‰åŠ©äºç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œèµ„æºç®¡ç†ã€‚</li>
<li>å½“å‰VLMsåœ¨é¥æ„Ÿæ•°æ®åº”ç”¨ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦ä¸“é—¨çš„æ¨¡å‹æ¥å¤„ç†åœ°çƒè§‚æµ‹æ•°æ®ã€‚</li>
<li>EarthDialæ˜¯ä¸€æ¬¾ä¸“ä¸ºåœ°çƒè§‚æµ‹æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©æ‰‹ï¼Œæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡å½±åƒåˆ†æã€‚</li>
<li>EarthDialé€šè¿‡å¼•å…¥å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œè¦†ç›–å¤šç§ä¼ æ„Ÿå™¨æ¨¡æ€ï¼Œå®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>EarthDialèƒ½å¤„ç†å¤æ‚çš„é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ç­‰ã€‚</li>
<li>EarthDialè¿˜å…·å¤‡å¤„ç†åŒæ—¶ç›¸å’Œå¤šæ—¶ç›¸åºåˆ—åˆ†æçš„èƒ½åŠ›ï¼Œé€‚ç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89a1b482c8060c0525e5c4207ae63529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ca0e5a40001e6d6bda93656a4ece5dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a6f1eb946288c4b0e889de136f1d2b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24886458342ef2005f55155879b5d0b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b1d45ce232d179cb36fda8ff10ea4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd697d813a0a074d4803710bb9f3d4a2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HPC-Coder-V2-Studying-Code-LLMs-Across-Low-Resource-Parallel-Languages"><a href="#HPC-Coder-V2-Studying-Code-LLMs-Across-Low-Resource-Parallel-Languages" class="headerlink" title="HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages"></a>HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages</h2><p><strong>Authors:Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele</strong></p>
<p>Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼–ç å·¥å…·ä½œä¸ºè½¯ä»¶å¼€å‘åŠ©æ‰‹å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯ä¸ºé€šç”¨ç¼–ç¨‹ä»»åŠ¡è€Œè®¾è®¡çš„ï¼Œå¯¹äºé«˜æ€§èƒ½è®¡ç®—ç­‰ç‰¹æ®Šé¢†åŸŸè¡¨ç°ä¸ä½³ã€‚åœ¨è¿™äº›é¢†åŸŸåˆ›å»ºä¸“ä¸šæ¨¡å‹å’Œç›¸å…³å·¥å…·å¯¹äºåœ¨è¯¸å¦‚é«˜æ€§èƒ½è®¡ç®—ç­‰é¢†åŸŸè·å¾—å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›Šå¤„è‡³å…³é‡è¦ã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†é’ˆå¯¹é«˜æ€§èƒ½è®¡ç®—çš„ä¸“ä¸šæ¨¡å‹ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¹¶è¡Œä»£ç æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼Œå°šä¸æ¸…æ¥šæ˜¯ä»€ä¹ˆéšœç¢ä»ç„¶é˜»ç¢ç€è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä»¥åŠéœ€è¦é‡‡å–ä»€ä¹ˆæªæ–½æ¥å…‹æœè¿™äº›éšœç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ²¿ç€è®¸å¤šè½´å¯¹ä¸“ä¸šé«˜æ€§èƒ½è®¡ç®—å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œä»¥æ›´å¥½åœ°äº†è§£æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬å¯¹ä¸“ä¸šçš„é«˜æ€§èƒ½è®¡ç®—å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒå¹¶è¿›è¡Œäº†è¯„ä¼°ï¼Œè¢«è¯æ˜è¿„ä»Šä¸ºæ­¢æ˜¯å¹¶è¡Œä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°æœ€å¥½çš„å¼€æºä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å¼€å‘åŠ©æ‰‹é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å¯¹äºä¸“ä¸šé¢†åŸŸå¦‚é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ç­‰ç‰¹å®šä»»åŠ¡çš„ç¼–ç¨‹ï¼Œå…¶è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†åœ¨è¿™äº›é¢†åŸŸè·å–LLMçš„ç›Šå¤„ï¼Œåˆ›å»ºä¸“é—¨åŒ–çš„æ¨¡å‹å’Œå·¥å…·è‡³å…³é‡è¦ã€‚å½“å‰å¯¹äºç‰¹å®šäºHPCçš„æ¨¡å‹æ¢ç´¢å°šå­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¹¶è¡Œä»£ç æ–¹é¢ã€‚æœ¬ç ”ç©¶å¯¹ç²¾ç»†è°ƒæ•´HPCç‰¹æ®ŠLLMçš„å¤šä¸ªæ–¹é¢è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œä»¥æ›´å¥½åœ°ç†è§£æŒ‘æˆ˜æ‰€åœ¨ï¼Œå¹¶åŸºäºç ”ç©¶å¯¹LLMè¿›è¡Œäº†ç²¾ç»†è°ƒæ•´ä¸è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºè¿„ä»Šä¸ºæ­¢åœ¨å¹¶è¡Œä»£ç ç”Ÿæˆæ–¹é¢çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä½œä¸ºè½¯ä»¶å¼€å‘åŠ©æ‰‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸå¦‚é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ä¸­çš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>åˆ›å»ºé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„LLMæ¨¡å‹å’Œå·¥å…·ï¼Œæ˜¯æå‡LLMåœ¨HPCç­‰é¢†åŸŸè¡¨ç°çš„å…³é”®ã€‚</li>
<li>ç›®å‰LLMåœ¨ç”Ÿæˆå¹¶è¡Œä»£ç æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡æ·±å…¥ç ”ç©¶å¹¶ç²¾ç»†è°ƒæ•´ä¸€ä¸ªé’ˆå¯¹HPCçš„LLMæ¨¡å‹ï¼Œå–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>æ­¤LLMæ¨¡å‹åœ¨å¹¶è¡Œä»£ç ç”Ÿæˆæ–¹é¢è¢«è¯å®æ˜¯ç›®å‰æœ€ä½³çš„å¼€æºä»£ç LLMã€‚</li>
<li>å¯¹LLMæ¨¡å‹çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›ä»æœ‰å¿…è¦ï¼Œä»¥è§£å†³å…¶åœ¨ç‰¹å®šé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f921286d8ce697078a2295b2459b222.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edbe578400e7ce985a146a02c68e2cd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df82e98812142b1bf44d6f3c6cdc93d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3762c672d32971de228fe23267c67333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed9477542137f861c90d31815be78578.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Critical-Questions-of-Thought-Steering-LLM-reasoning-with-Argumentative-Querying"><a href="#Critical-Questions-of-Thought-Steering-LLM-reasoning-with-Argumentative-Querying" class="headerlink" title="Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative   Querying"></a>Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative   Querying</h2><p><strong>Authors:Federico Castagna, Isabel Sassoon, Simon Parsons</strong></p>
<p>Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulminâ€™s model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the modelsâ€™ reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the modelsâ€™ output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided. </p>
<blockquote>
<p>å°½ç®¡äººå·¥æ™ºèƒ½ç ”ç©¶è¿‘æœŸå–å¾—äº†çªç ´æ€§çš„å¿«é€Ÿè¿›å±•ï¼Œä½†å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€»è¾‘å’Œæ•°å­¦æ¨ç†æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>ç ”ç©¶ç»“æœä¼¼ä¹è¡¨æ˜ï¼ŒLLMä»ç„¶æ›´åƒæ˜¯ï¼ˆé«˜åº¦å…ˆè¿›çš„ï¼‰æ•°æ®æ¨¡å¼è¯†åˆ«å™¨ï¼Œåœ¨å°è¯•æ¨å¹¿å’Œè§£å†³æ¨¡å‹ä¹‹å‰æœªè§æˆ–ä¸å…¶è®­ç»ƒæ•°æ®æ ·æœ¬ä¸ç›¸ä¼¼çš„é—®é¢˜æ—¶ï¼Œè¡¨ç°ä¸ä½³ã€‚</p>
<p>ä¸ºäº†è§£å†³è¿™ä¸€ä»¤äººå…³æ³¨çš„é—®é¢˜ï¼Œæœ¬æ–‡å€Ÿé‰´äº†è®ºè¯ç†è®ºæ–‡çŒ®ä¸­çš„æ‰¹åˆ¤æ€§é—®é¢˜æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯èšç„¦äºå›¾å°”æ•çš„è®ºè¯æ¨¡å‹ã€‚</p>
<p>æˆ‘ä»¬è¡¨æ˜ï¼Œåˆ©ç”¨è¿™äº›æ‰¹åˆ¤æ€§é—®é¢˜å¯ä»¥æ”¹å–„LLMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ¢ç©¶æ¨¡å‹æ¨ç†è¿‡ç¨‹èƒŒåçš„ç†æ€§ï¼ŒLLMå¯ä»¥è¯„ä¼°æ˜¯å¦å‘ç”Ÿäº†é€»è¾‘é”™è¯¯å¹¶åœ¨æä¾›æœ€ç»ˆå›ç­”ä¹‹å‰è¿›è¡Œçº æ­£ã€‚</p>
<p>ä»»ä½•æœ‰æ•ˆçš„è®ºè¯ç¨‹åºçš„é»„é‡‘æ ‡å‡†èƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå¦‚æœç»“è®ºæ˜¯ç”±æ¥å—çš„å‰ææ‰€è•´å«çš„å°±æ˜¯æœ‰æ•ˆçš„ã€‚æˆ–è€…ï¼Œç”¨äºšé‡Œå£«å¤šå¾·åŸç†åœ¨ç°å®ä¸–ç•Œçš„è¿‘ä¼¼å€¼æ¥è½¬è¿°ï¼Œåœ¨å……æ»¡ä¸å®Œå…¨ä¿¡æ¯å’Œé¢„è®¾é€»è¾‘çš„æƒ…å†µä¸‹ï¼Œå¦‚æœæœªè¢«è¯æ˜æ˜¯é”™è¯¯çš„ï¼Œé‚£ä¹ˆç»“è®ºå°±æ˜¯æœ‰æ•ˆçš„ã€‚</p>
<p>è¿™ç§æ–¹æ³•æˆåŠŸåœ°é€šè¿‡æ¨ç†ç®¡é“å¼•å¯¼äº†æ¨¡å‹çš„è¾“å‡ºï¼Œä¸åŸºçº¿åŠå…¶æ€ç»´é“¾ï¼ˆCoTï¼‰å®ç°ç›¸æ¯”ï¼Œåœ¨é€»è¾‘æ¨ç†æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œåœ¨å¤šä¸ªLLMä¸Šå¯¹æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15177v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸAIç ”ç©¶çš„çªç ´å’Œå¿«é€Ÿå‘å±•å¹¶æœªå®Œå…¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘å’Œæ•°å­¦æ¨ç†èƒ½åŠ›é—®é¢˜ã€‚LLMä¸»è¦è¿˜æ˜¯ä½œä¸ºé«˜çº§æ•°æ®æ¨¡å¼è¯†åˆ«å™¨ï¼Œå¯¹äºæœªæ›¾è§è¿‡æˆ–è®­ç»ƒæ•°æ®æ ·æœ¬ä¹‹å¤–çš„æ¨ç†é—®é¢˜ï¼Œå…¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ¬æ–‡å€Ÿé‰´è®ºè¯ç†è®ºæ–‡çŒ®ä¸­çš„æ‰¹åˆ¤æ€§é—®é¢˜æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯æ‰˜å°”æ•çš„è®ºè¯æ¨¡å‹ï¼Œå±•ç¤ºè¿ç”¨è¿™äº›é—®é¢˜å¯ä»¥æ”¹å–„LLMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ¢ç©¶æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ç†æ€§ï¼ŒLLMèƒ½å¤Ÿåœ¨å‡ºç°é€»è¾‘é”™è¯¯æ—¶è¯†åˆ«å¹¶çº æ­£ï¼Œå†å¯¹ç”¨æˆ·æç¤ºç»™å‡ºæœ€ç»ˆç­”å¤ã€‚è¯¥æ–¹æ³•çš„æˆåŠŸå¼•å¯¼äº†æ¨¡å‹é€šè¿‡æ¨ç†ç®¡é“è¾“å‡ºï¼Œæé«˜äº†åœ¨åŸºå‡†æµ‹è¯•å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰å®æ–½ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMå°½ç®¡æœ‰çªç ´æ€§è¿›å±•ï¼Œä½†åœ¨é€»è¾‘å’Œæ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>LLMä¸»è¦ä½œä¸ºé«˜çº§æ•°æ®æ¨¡å¼è¯†åˆ«å™¨ï¼Œå¯¹æœªè§è¿‡çš„æ¨ç†é—®é¢˜æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æ‰¹åˆ¤æ€§é—®é¢˜å¯ä»è®ºè¯ç†è®ºå€Ÿé‰´ï¼Œç”¨ä»¥æ”¹å–„LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ¢ç©¶æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ç†æ€§ï¼ŒLLMèƒ½è¯†åˆ«å¹¶çº æ­£é€»è¾‘é”™è¯¯ã€‚</li>
<li>ä½¿ç”¨æ‰¹åˆ¤æ€§é—®é¢˜èƒ½æˆåŠŸå¼•å¯¼æ¨¡å‹é€šè¿‡æ¨ç†ç®¡é“è¾“å‡ºã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†LLMåœ¨åŸºå‡†æµ‹è¯•å’Œé“¾å¼æ€ç»´å®æ–½ä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f6796247686731d1ebbce4f77220009f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b8c358101f6b2f45058151979ebc960.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-Uncertainty-Estimation-in-Natural-Language-Generation"><a href="#Rethinking-Uncertainty-Estimation-in-Natural-Language-Generation" class="headerlink" title="Rethinking Uncertainty Estimation in Natural Language Generation"></a>Rethinking Uncertainty Estimation in Natural Language Generation</h2><p><strong>Authors:Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter</strong></p>
<p>Large Language Models (LLMs) are increasingly employed in real-world applications, driving the need to evaluate the trustworthiness of their generated text. To this end, reliable uncertainty estimation is essential. Since current LLMs generate text autoregressively through a stochastic process, the same prompt can lead to varying outputs. Consequently, leading uncertainty estimation methods generate and analyze multiple output sequences to determine the LLMâ€™s uncertainty. However, generating output sequences is computationally expensive, making these methods impractical at scale. In this work, we inspect the theoretical foundations of the leading methods and explore new directions to enhance their computational efficiency. Building on the framework of proper scoring rules, we find that the negative log-likelihood of the most likely output sequence constitutes a theoretically grounded uncertainty measure. To approximate this alternative measure, we propose G-NLL, which has the advantage of being obtained using only a single output sequence generated by greedy decoding. This makes uncertainty estimation more efficient and straightforward, while preserving theoretical rigor. Empirical results demonstrate that G-NLL achieves state-of-the-art performance across various LLMs and tasks. Our work lays the foundation for efficient and reliable uncertainty estimation in natural language generation, challenging the necessity of more computationally involved methods currently leading the field. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œè¿™å¼•å‘äº†å¯¹å…¶ç”Ÿæˆæ–‡æœ¬å¯ä¿¡åº¦çš„è¯„ä¼°éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œå¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡è‡³å…³é‡è¦ã€‚ç”±äºå½“å‰çš„LLMé€šè¿‡éšæœºè¿‡ç¨‹è‡ªå›å½’åœ°ç”Ÿæˆæ–‡æœ¬ï¼Œç›¸åŒçš„æç¤ºå¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„è¾“å‡ºã€‚å› æ­¤ï¼Œä¸»æµçš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ä¼šç”Ÿæˆå¹¶åˆ†æå¤šä¸ªè¾“å‡ºåºåˆ—æ¥ç¡®å®šLLMçš„ä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¾“å‡ºåºåˆ—çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ï¼Œä½¿å¾—è¿™äº›æ–¹æ³•åœ¨å¤§è§„æ¨¡åº”ç”¨æ—¶ä¸å¤ªå®ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†ä¸»æµæ–¹æ³•çš„ç†è®ºåŸºçŸ³ï¼Œå¹¶æ¢ç´¢äº†æé«˜è®¡ç®—æ•ˆç‡çš„æ–°æ–¹å‘ã€‚åŸºäºé€‚å½“çš„è¯„åˆ†è§„åˆ™æ¡†æ¶ï¼Œæˆ‘ä»¬å‘ç°æœ€å¯èƒ½çš„è¾“å‡ºåºåˆ—çš„è´Ÿå¯¹æ•°å¯èƒ½æ€§æ„æˆäº†ä¸€ä¸ªæœ‰ç†è®ºä¾æ®çš„ä¸ç¡®å®šæ€§åº¦é‡ã€‚ä¸ºäº†è¿‘ä¼¼è¿™ä¸ªæ›¿ä»£åº¦é‡ï¼Œæˆ‘ä»¬æå‡ºäº†G-NLLï¼Œå®ƒçš„ä¼˜ç‚¹æ˜¯åªä½¿ç”¨è´ªå©ªè§£ç ç”Ÿæˆçš„ä¸€ä¸ªè¾“å‡ºåºåˆ—å³å¯è·å¾—ã€‚è¿™ä½¿å¾—ä¸ç¡®å®šæ€§ä¼°è®¡æ›´åŠ é«˜æ•ˆå’Œç›´è§‚ï¼ŒåŒæ—¶ä¿ç•™äº†ç†è®ºä¸¥è°¨æ€§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒG-NLLåœ¨å„ç§LLMå’Œä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„å·¥ä½œå¥ å®šäº†é«˜æ•ˆå¯é çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸ç¡®å®šæ€§ä¼°è®¡çš„åŸºç¡€ï¼ŒæŒ‘æˆ˜äº†å½“å‰é¢†åŸŸä¸»å¯¼çš„æ›´å¤æ‚è®¡ç®—æ–¹æ³•çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯ä¿¡åº¦è¯„ä¼°è‡³å…³é‡è¦ï¼Œä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯å…³é”®ã€‚å½“å‰LLMé€šè¿‡éšæœºè¿‡ç¨‹è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬ï¼ŒåŒä¸€æç¤ºå¯èƒ½äº§ç”Ÿä¸åŒè¾“å‡ºï¼Œå¯¼è‡´ä¸ç¡®å®šæ€§ã€‚ä¸»æµçš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•é€šè¿‡ç”Ÿæˆå’Œåˆ†æå¤šä¸ªè¾“å‡ºåºåˆ—æ¥è¯„ä¼°LLMçš„ä¸ç¡®å®šæ€§ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å¤§è§„æ¨¡åº”ç”¨ã€‚æœ¬æ–‡æ£€æŸ¥ä¸»æµæ–¹æ³•çš„ç†è®ºåŸºç¡€ï¼Œæ¢ç´¢æé«˜è®¡ç®—æ•ˆç‡çš„æ–°æ–¹å‘ã€‚åŸºäºé€‚å½“çš„è¯„åˆ†è§„åˆ™æ¡†æ¶ï¼Œæˆ‘ä»¬å‘ç°æœ€å¯èƒ½è¾“å‡ºåºåˆ—çš„è´Ÿå¯¹æ•°å¯èƒ½æ€§æ˜¯ä¸€ä¸ªç†è®ºä¸Šå¯é çš„ä¸ç¡®å®šæ€§åº¦é‡ã€‚æˆ‘ä»¬æå‡ºG-NLLæ¥è¿‘ä¼¼è¿™ä¸€åº¦é‡ï¼Œåªéœ€ä½¿ç”¨è´ªå©ªè§£ç ç”Ÿæˆçš„å•ä¸ªè¾“å‡ºåºåˆ—å³å¯è·å¾—ï¼Œæé«˜äº†ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ•ˆç‡å’Œç›´è§‚æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç†è®ºä¸¥è°¨æ€§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒG-NLLåœ¨ä¸åŒLLMå’Œä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯ä¿¡åº¦è¯„ä¼°å¾ˆé‡è¦ï¼Œä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯å…³é”®ã€‚</li>
<li>å½“å‰LLMé€šè¿‡éšæœºè¿‡ç¨‹ç”Ÿæˆæ–‡æœ¬ï¼ŒåŒä¸€æç¤ºå¯èƒ½äº§ç”Ÿä¸åŒè¾“å‡ºã€‚</li>
<li>ä¸»æµçš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥å¤§è§„æ¨¡åº”ç”¨ã€‚</li>
<li>åŸºäºé€‚å½“çš„è¯„åˆ†è§„åˆ™æ¡†æ¶ï¼Œæœ€å¯èƒ½è¾“å‡ºåºåˆ—çš„è´Ÿå¯¹æ•°å¯èƒ½æ€§æ˜¯ç†è®ºä¸Šå¯é çš„ä¸ç¡®å®šæ€§åº¦é‡ã€‚</li>
<li>æå‡ºG-NLLæ–¹æ³•è¿‘ä¼¼è¯¥åº¦é‡ï¼Œåªéœ€å•ä¸ªè¾“å‡ºåºåˆ—ï¼Œæé«˜è®¡ç®—æ•ˆç‡å’Œç›´è§‚æ€§ã€‚</li>
<li>G-NLLåœ¨ä¸åŒLLMå’Œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e8427064fb7ee8915283d18b73028b8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adaptive-Pruning-for-Large-Language-Models-with-Structural-Importance-Awareness"><a href="#Adaptive-Pruning-for-Large-Language-Models-with-Structural-Importance-Awareness" class="headerlink" title="Adaptive Pruning for Large Language Models with Structural Importance   Awareness"></a>Adaptive Pruning for Large Language Models with Structural Importance   Awareness</h2><p><strong>Authors:Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han</strong></p>
<p>The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ˜¾è‘—æé«˜äº†è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¾¹ç¼˜è®¾å¤‡èµ„æºå—é™ï¼ŒLLMçš„éƒ¨ç½²é¢ä¸´è®¡ç®—èµ„æºå’Œå­˜å‚¨èµ„æºéœ€æ±‚é«˜çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹LLMæ¨¡å‹è£å‰ªæ–¹æ³•ï¼Œç§°ä¸ºç»“æ„æ„ŸçŸ¥è‡ªé€‚åº”è£å‰ªï¼ˆSAAPï¼‰ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªè‡ªé€‚åº”é‡è¦æ€§èåˆåº¦é‡æ ‡å‡†ï¼Œé€šè¿‡è€ƒè™‘åŒæ–¹å·®ä¸ç¡®å®šæ€§æ¥è¯„ä¼°LLMä¸­æ‰€æœ‰è€¦åˆç»“æ„çš„é‡è¦æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰æ¨¡å—è¿›è¡Œé‡è¦æ€§æ’åï¼Œä»¥ç¡®å®šä¸ºæ»¡è¶³ç‰¹å®šæ€§èƒ½è¦æ±‚è€Œåº”è£å‰ªçš„ç‰¹å®šå±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ç»„å¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜LLMçš„æ¨ç†æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå¸¸è§ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„SAAPæ–¹æ³•ï¼Œå³é›¶æ ·æœ¬åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SAAPæ–¹æ³•åœ¨LLaMA-7Bã€Vicuna-7Bå’ŒLLaMA-13Bä¸Šä¼˜äºå‡ ç§æœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œåˆ†åˆ«å®ç°äº†2.17%ã€2.37%å’Œ2.39%çš„å‡†ç¡®ç‡æå‡ã€‚æ­¤å¤–ï¼ŒSAAPæé«˜äº†ä»¤ç‰Œç”Ÿæˆé€Ÿåº¦5%ï¼Œåœ¨èµ„æºå—é™åœºæ™¯ä¸­å±•ç°å‡ºå…¶å®ç”¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15127v1">PDF</a> 12 pages, 6 figures, 12 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºLLMså¯¹è®¡ç®—å’Œå­˜å‚¨èµ„æºçš„é«˜éœ€æ±‚ï¼Œå°†å…¶éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºç»“æ„æ„ŸçŸ¥è‡ªé€‚åº”å‰ªæï¼ˆSAAPï¼‰çš„æ–°å‹LLMæ¨¡å‹å‰ªææ–¹æ³•ï¼Œä»¥æ˜¾è‘—é™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡è€ƒè™‘åŒæ„ä¸ç¡®å®šæ€§æ¥å®šä¹‰è‡ªé€‚åº”é‡è¦æ€§èåˆæŒ‡æ ‡ï¼Œè¯„ä¼°LLMsä¸­æ‰€æœ‰è€¦åˆç»“æ„çš„é‡è¦æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®æ¨¡å—é‡è¦æ€§æ’åï¼Œç¡®å®šåº”å‰ªæçš„ç‰¹å®šå±‚ï¼Œä»¥æ»¡è¶³ç‰¹å®šçš„æ€§èƒ½è¦æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ç»„å¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜LLMsçš„æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SAAPæ–¹æ³•åœ¨å¤šä¸ªLLMsä¸Šä¼˜äºå‡ ç§æœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆä¸¤ä¸ªå¸¸è§ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº†2.17%ã€2.37%å’Œ2.39%çš„ç²¾åº¦æå‡ã€‚æ­¤å¤–ï¼ŒSAAPè¿˜æé«˜äº†ä»¤ç‰Œç”Ÿæˆé€Ÿåº¦ï¼Œåœ¨èµ„æºå—é™åœºæ™¯ä¸­è¡¨ç°å‡ºå®é™…ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹LLMæ¨¡å‹å‰ªææ–¹æ³•â€”â€”ç»“æ„æ„ŸçŸ¥è‡ªé€‚åº”å‰ªæï¼ˆSAAPï¼‰ï¼Œä»¥é™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”é‡è¦æ€§èåˆæŒ‡æ ‡è¯„ä¼°LLMsä¸­æ‰€æœ‰è€¦åˆç»“æ„çš„é‡è¦æ€§ã€‚</li>
<li>æ ¹æ®æ¨¡å—é‡è¦æ€§æ’åï¼Œç¡®å®šåº”å‰ªæçš„ç‰¹å®šå±‚ï¼Œä»¥æ»¡è¶³æ€§èƒ½è¦æ±‚ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°çš„ç»„å¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜LLMsçš„æ¨ç†æ•ˆç‡ã€‚</li>
<li>SAAPæ–¹æ³•åœ¨å¤šä¸ªLLMsä¸Šä¼˜äºæœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†ç²¾åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f572959e6d54fba0f388d5d84de2f84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-108e7259b00e853fba8c43a089ae3e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b55d6b852b1ad68416229694493e04a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-561eb86205e53df60819b45ba86e81e0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Outcome-Refining-Process-Supervision-for-Code-Generation"><a href="#Outcome-Refining-Process-Supervision-for-Code-Generation" class="headerlink" title="Outcome-Refining Process Supervision for Code Generation"></a>Outcome-Refining Process Supervision for Code Generation</h2><p><strong>Authors:Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang</strong></p>
<p>Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: <a target="_blank" rel="noopener" href="https://github.com/zhuohaoyu/ORPS">https://github.com/zhuohaoyu/ORPS</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ç®—æ³•æ¨ç†çš„å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸Šå¸¸å¸¸é‡åˆ°å›°éš¾ã€‚è™½ç„¶é€šè¿‡å­¦ä¹ å¥–åŠ±æ¨¡å‹è¿›è¡Œçš„è¿›ç¨‹ç›‘ç£åœ¨å¼•å¯¼æ¨ç†æ­¥éª¤æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒéœ€è¦æ˜‚è´µçš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å­˜åœ¨è¯„ä»·ä¸å¯é çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ç»“æœä¼˜åŒ–è¿›ç¨‹ç›‘ç£ï¼ˆOutcome-Refining Process Supervisionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ç»“æœä¼˜åŒ–æœ¬èº«ä½œä¸ºè¦ç›‘ç£çš„è¿‡ç¨‹çš„æ–°å‹èŒƒå¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨å…·ä½“çš„æ‰§è¡Œä¿¡å·æ¥å¤¯å®æ¨ç†æ­¥éª¤çš„ç›‘ç£ï¼ŒåŒæ—¶ä½¿ç”¨æ ‘å½¢æ¢ç´¢æ¥åŒæ—¶ç»´æŒå¤šä¸ªè§£å†³æ–¹æ¡ˆè½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”šè‡³æ›´å°çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ç«äº‰æ€§ç¼–ç¨‹ä»»åŠ¡ä¸Šå®ç°é«˜æˆåŠŸç‡å’Œæ€§èƒ½æŒ‡æ ‡ï¼Œç›¸æ¯”äºä¼ ç»Ÿå¥–åŠ±æ¨¡å‹åˆ›å»ºæ›´å¯é çš„éªŒè¯ï¼Œæ— éœ€è®­ç»ƒPRMsã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨5ä¸ªæ¨¡å‹å’Œ3ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼šæ­£ç¡®ç‡å¹³å‡æé«˜26.9%ï¼Œæ•ˆç‡æé«˜42.2%ã€‚ç»“æœè¡¨æ˜ï¼Œæä¾›å…·æœ‰å…·ä½“éªŒè¯ä¿¡å·çš„ç»“æ„åŒ–æ¨ç†ç©ºé—´å¯¹äºè§£å†³å¤æ‚ç¼–ç¨‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€äº†æ‰€æœ‰ä»£ç å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://github.com/zhuohaoyu/ORPS">https://github.com/zhuohaoyu/ORPS</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15118v1">PDF</a> 18 pages, 5 figures, Code: <a target="_blank" rel="noopener" href="https://github.com/zhuohaoyu/ORPS">https://github.com/zhuohaoyu/ORPS</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ç®—æ³•æ¨ç†çš„å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸Šå¸¸é‡æŒ‘æˆ˜ã€‚ç°æœ‰æµç¨‹ç›‘ç£æ–¹æ³•è™½æœ‰æœ›å¼•å¯¼æ¨ç†æ­¥éª¤ï¼Œä½†éœ€ä¾èµ–æ˜‚è´µè®­ç»ƒæ•°æ®ä¸”è¯„ä¼°ä¸å¯é ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹ç›‘ç£èŒƒå¼â€”â€”æˆæœä¼˜åŒ–æµç¨‹ç›‘ç£ï¼Œå°†æˆæœä¼˜åŒ–æœ¬èº«ä½œä¸ºç›‘ç£æµç¨‹ã€‚æ­¤æ¡†æ¶ç»“åˆå…·ä½“æ‰§è¡Œä¿¡å·æ¥ç›‘ç£æ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶é‡‡ç”¨æ ‘å½¢æ¢ç´¢æ¥åŒæ—¶ç»´æŠ¤å¤šä¸ªè§£å†³æ–¹æ¡ˆè½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä½¿è¾ƒå°æ¨¡å‹åœ¨ç«èµ›ç¼–ç¨‹ä»»åŠ¡ä¸Šå®ç°äº†é«˜æˆåŠŸç‡å’Œæ€§èƒ½æŒ‡æ ‡ï¼Œè¾ƒä¼ ç»Ÿå¥–åŠ±æ¨¡å‹åˆ›é€ äº†æ›´å¯é çš„éªŒè¯ï¼Œä¸”æ— éœ€è®­ç»ƒPRMsã€‚è¯¥æ–¹æ³•åœ¨5ä¸ªæ¨¡å‹å’Œ3ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æ­£ç¡®æ€§æé«˜äº†26.9%ï¼Œæ•ˆç‡æé«˜äº†42.2%ã€‚ç»“æœè¡¨æ˜ï¼Œæä¾›å…·æœ‰å…·ä½“éªŒè¯ä¿¡å·çš„ç»“æ„åŒ–æ¨ç†ç©ºé—´å¯¹äºè§£å†³å¤æ‚ç¼–ç¨‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä¸Šè¡¨ç°å‡ºä¼˜å¼‚èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„æ·±åº¦ç®—æ³•æ¨ç†èƒ½åŠ›å—é™ã€‚</li>
<li>ä¼ ç»Ÿæµç¨‹ç›‘ç£æ–¹æ³•éœ€å¤§é‡æ˜‚è´µçš„è®­ç»ƒæ•°æ®ä¸”è¯„ä¼°ç»“æœä¸ç¨³å®šã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹ç›‘ç£èŒƒå¼â€”â€”æˆæœä¼˜åŒ–æµç¨‹ç›‘ç£ï¼Œå°†æˆæœä¼˜åŒ–è¿‡ç¨‹ä½œä¸ºç›‘ç£å¯¹è±¡ã€‚</li>
<li>ç»“åˆå…·ä½“æ‰§è¡Œä¿¡å·æ¥ç›‘ç£æ¨ç†æ­¥éª¤ï¼Œå¹¶é‡‡ç”¨æ ‘å½¢æ¢ç´¢ç»´æŠ¤å¤šä¸ªè§£å†³æ–¹æ¡ˆè½¨è¿¹ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸Šçš„æ­£ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åˆ›é€ æ›´å¯é çš„éªŒè¯ä¸”æ— éœ€è®­ç»ƒPRMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c533e46ae19cc805c73a90ab515107a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fab10e850b38e57c9f0bb2f200f2b60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67d00aaba27f36f1f0aaa2172cb816f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f52817428f74faad3843703261fa98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8abfddb9a8a18dc8e71338480d0a396.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03501d1b9019d22bddf64da0c7c3ea5c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Qwen2-5-Technical-Report"><a href="#Qwen2-5-Technical-Report" class="headerlink" title="Qwen2.5 Technical Report"></a>Qwen2.5 Technical Report</h2><p><strong>Authors: Qwen,  :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu</strong></p>
<p>In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Qwen2.5ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å…¨é¢çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æ»¡è¶³ä¸åŒéœ€æ±‚ã€‚ç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬ï¼ŒQwen 2.5åœ¨é¢„è®­ç»ƒå’ŒåæœŸè®­ç»ƒé˜¶æ®µéƒ½å¾—åˆ°äº†æ˜¾è‘—çš„æå‡ã€‚åœ¨é¢„è®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬å°†é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é›†ä»ä¹‹å‰çš„7ä¸‡äº¿æ ‡è®°æ‰©å±•åˆ°18ä¸‡äº¿æ ‡è®°ã€‚è¿™ä¸ºå¸¸è¯†ã€ä¸“ä¸šçŸ¥è¯†æ¨ç†èƒ½åŠ›æä¾›äº†åšå®çš„åŸºç¡€ã€‚åœ¨åæœŸè®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬å®æ–½äº†å¤æ‚çš„ç›‘ç£å¾®è°ƒï¼Œæ ·æœ¬æ•°é‡è¶…è¿‡1ç™¾ä¸‡ï¼Œä»¥åŠå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ã€‚åæœŸè®­ç»ƒæŠ€æœ¯æé«˜äº†äººç±»åå¥½ï¼Œå¹¶æ˜¾è‘—æ”¹å–„äº†é•¿æ–‡æœ¬ç”Ÿæˆã€ç»“æ„åŒ–æ•°æ®åˆ†æå’ŒæŒ‡ä»¤éµå¾ªã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¤„ç†å¤šæ ·åŒ–å’Œå¤šå˜çš„ä½¿ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸°å¯Œçš„Qwen2.5 LLMç³»åˆ—ã€‚å¼€æ”¾æƒé‡çš„äº§å“åŒ…æ‹¬åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œè¿˜æœ‰é‡åŒ–ç‰ˆæœ¬å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤–ï¼Œå¯¹äºæ‰˜ç®¡è§£å†³æ–¹æ¡ˆï¼Œä¸“æœ‰æ¨¡å‹ç›®å‰åŒ…æ‹¬ä¸¤ä¸ªæ··åˆä¸“å®¶ï¼ˆMoEï¼‰ç‰ˆæœ¬ï¼šQwen2.5-Turboå’ŒQwen2.5-Plusï¼Œéƒ½å¯åœ¨é˜¿é‡Œäº‘æ¨¡å‹å·¥ä½œå®¤ä½¿ç”¨ã€‚Qwen2.5å·²åœ¨å¹¿æ³›çš„è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ã€äººç±»åå¥½å¯¹é½ç­‰åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯å¼€æ”¾æƒé‡çš„æ——èˆ°äº§å“Qwen2.5-72B-Instructåœ¨è®¸å¤šå¼€æ”¾å’Œä¸“æœ‰æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€å…ˆè¿›çš„å¼€æ”¾æƒé‡æ¨¡å‹Llama-3-405B-Instructè¡¨ç°ç›¸å½“ï¼Œå°½ç®¡åè€…è§„æ¨¡å¤§çº¦æ˜¯å‰è€…çš„5å€ã€‚Qwen2.5-Turboå’ŒQwen2.5-Pluså…·æœ‰å‡ºè‰²çš„æ€§ä»·æ¯”ï¼Œä¸GPT-4o-miniå’ŒGPT-4oç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œä½œä¸ºåŸºçŸ³ï¼ŒQwen2.5æ¨¡å‹åœ¨è®­ç»ƒä¸“ä¸šåŒ–æ¨¡å‹æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œå¦‚Qwen2.5-Mathã€Qwen2.5-Coderã€QwQå’Œå¤šæ¨¡æ€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15115v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä»‹ç»å…¨é¢çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—Qwen2.5ï¼Œè¯¥ç³»åˆ—æ¨¡å‹åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½æœ‰æ˜¾è‘—æ”¹è¿›ã€‚é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨äº†æ›´å¤§è§„æ¨¡çš„é«˜è´¨æ•°æ®é›†ï¼Œè¾¾åˆ°18ä¸‡äº¿æ ‡è®°ï¼Œå¢å¼ºäº†å¸¸è¯†ã€ä¸“ä¸šçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚å¾®è°ƒé˜¶æ®µé‡‡ç”¨å¤æ‚çš„ç›‘ç£å¾®è°ƒæŠ€æœ¯å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†é•¿æ–‡æœ¬ç”Ÿæˆã€ç»“æ„åŒ–æ•°æ®åˆ†æå’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æä¾›å¤šç§ä¸åŒå¤§å°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä»¥åŠé‡åŒ–ç‰ˆæœ¬ã€‚ä¸“æœ‰æ¨¡å‹åŒ…æ‹¬MoEå˜ä½“Qwen2.5-Turboå’ŒQwen2.5-Plusï¼Œå¯åœ¨é˜¿é‡Œäº‘æ¨¡å‹å·¥ä½œå®¤ä¸­ä½¿ç”¨ã€‚Qwen2.5åœ¨å¤šç§è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¦‚è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ã€äººç±»åå¥½å¯¹é½ç­‰ã€‚å…¶ä¸­ï¼Œå¼€æºæ——èˆ°æ¨¡å‹Qwen2.5-72B-Instructæ€§èƒ½ä¼˜å¼‚ï¼Œä¸å¤§å‹å¼€æºæ¨¡å‹Llama-3-405B-Instructç«äº‰ã€‚Qwen2.5-Turboå’ŒQwen2.5-Pluså…·æœ‰é«˜æ€§ä»·æ¯”ï¼Œä¸GPT-4o-miniå’ŒGPT-4oç›¸å½“ã€‚Qwen2.5è¿˜ä¸ºç‰¹æ®Šæ¨¡å‹å’Œè·¨æ¨¡æ€æ¨¡å‹ï¼ˆå¦‚Qwen2.5-Mathã€Qwen2.5-Coderå’ŒQwQï¼‰çš„åŸ¹è®­å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Qwen2.5æ˜¯ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ»¡è¶³ä¸åŒçš„éœ€æ±‚ã€‚</li>
<li>ä¸ä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒQwen 2.5åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>é¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡ä»7ä¸‡äº¿æ ‡è®°æ‰©å¤§åˆ°18ä¸‡äº¿æ ‡è®°ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¸¸è¯†ã€ä¸“ä¸šçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤æ‚çš„ç›‘ç£å¾®è°ƒå’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æé«˜äº†é•¿æ–‡æœ¬ç”Ÿæˆã€ç»“æ„åŒ–æ•°æ®åˆ†æå’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>Qwen2.5æä¾›å¤šç§ä¸åŒå¤§å°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>Qwen2.5ç³»åˆ—åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦å’Œç¼–ç ç­‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cf4ec0597473eadf289da917a948f25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48058794ecf9bdf047a296b899c8ac64.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Nano-ESG-Extracting-Corporate-Sustainability-Information-from-News-Articles"><a href="#Nano-ESG-Extracting-Corporate-Sustainability-Information-from-News-Articles" class="headerlink" title="Nano-ESG: Extracting Corporate Sustainability Information from News   Articles"></a>Nano-ESG: Extracting Corporate Sustainability Information from News   Articles</h2><p><strong>Authors:Fabian Billert, Stefan Conrad</strong></p>
<p>Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.   An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at <a target="_blank" rel="noopener" href="https://github.com/Bailefan/Nano-ESG">https://github.com/Bailefan/Nano-ESG</a>. </p>
<blockquote>
<p>ç¡®å®šå…¬å¸çš„å¯æŒç»­æ€§å½±å“æ˜¯ä¸€ä¸ªé«˜åº¦å¤æ‚çš„ä¸»é¢˜ï¼Œè¿‡å»å‡ å¹´é‡Œè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚å¦‚ä»Šï¼ŒæŠ•èµ„è€…ä¸»è¦ä¾èµ–å‡ å®¶å·²å»ºç«‹è¯„çº§æœºæ„æä¾›çš„å¯æŒç»­æ€§è¯„çº§æ¥è¯„ä¼°å…¬å¸çš„è´Ÿè´£ä»»ç¨‹åº¦ã€‚ç„¶è€Œï¼Œè¿™äº›è¯„çº§æœ€è¿‘å—åˆ°æ‰¹è¯„ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥ç†è§£ä¸”å‡ ä¹æ— æ³•å¤åˆ¶ã€‚äº†è§£å…¬å¸å¯æŒç»­æ€§å®è·µçš„ç‹¬ç«‹æ–¹å¼åœ¨äºä¸°å¯Œçš„æ–°é—»æ–‡ç« æ•°æ®æ™¯è§‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢ä¸€ç§ä¸åŒçš„æ–¹æ³•æ¥è¯†åˆ«å…¬å¸åœ¨å¯æŒç»­æ€§é¢†åŸŸçš„ä¸»è¦æœºé‡å’ŒæŒ‘æˆ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«2023å¹´1æœˆè‡³2024å¹´9æœˆä¸ºå¾·å›½ä¸»è¦å…¬å¸æ”¶é›†çš„è¶…è¿‡84ä¸‡ç¯‡æ–°é—»æ–‡ç« ã€‚é€šè¿‡åº”ç”¨ä¸€ç³»åˆ—è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºç›¸å…³æ–‡ç« ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ€»ç»“ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå–ä¸å¯æŒç»­æ€§ç›¸å…³çš„æƒ…æ„Ÿå’Œæ–¹é¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹è·å¾—çš„æ•°æ®è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ç¡®å®šLLMç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å‡†ç¡®çš„ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bailefan/Nano-ESG%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/Bailefan/Nano-ESGå‘å¸ƒæ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15093v1">PDF</a> To be published at ECIR 2025. Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…¬å¸å¯æŒç»­æ€§å½±å“è¯„ä¼°æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¸»é¢˜ï¼Œè¿‘å¹´æ¥è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç›®å‰ï¼ŒæŠ•èµ„è€…ä¸»è¦ä¾èµ–æˆç†Ÿçš„è¯„çº§æœºæ„æ¥è¯„ä¼°å…¬å¸çš„å¯æŒç»­æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›è¯„çº§å­˜åœ¨éš¾ä»¥ç†è§£ä¸”éš¾ä»¥å¤åˆ¶çš„é—®é¢˜è€Œå—åˆ°æ‰¹è¯„ã€‚æ–°é—»æ–‡ç« æ•°æ®æä¾›äº†ä¸€ä¸ªäº†è§£å…¬å¸å¯æŒç»­æ€§å®è·µçš„ç‹¬ç«‹é€”å¾„ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼Œä»¥è¯†åˆ«å…¬å¸åœ¨å¯æŒç»­æ€§é¢†åŸŸçš„ä¸»è¦æœºé‡å’ŒæŒ‘æˆ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡84ä¸‡ç¯‡æ–°é—»æ–‡ç« çš„æ–°æ•°æ®é›†ï¼Œè¿™äº›æ–‡ç« æ˜¯å…³äºå¾·å›½ä¸»è¦å…¬å¸åœ¨2023å¹´1æœˆè‡³2024å¹´9æœˆä¹‹é—´çš„ä¿¡æ¯ã€‚é€šè¿‡åº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºç›¸å…³æ–‡ç« ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ€»ç»“ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–ä¸å¯æŒç»­æ€§ç›¸å…³çš„æƒ…æ„Ÿå’Œæ–¹é¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ‰€å¾—æ•°æ®è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ç¡®å®šäº†å¤§å‹è¯­è¨€æ¨¡å‹ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bailefan/Nano-ESG%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E8%BF%99%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/Bailefan/Nano-ESGä¸Šå‘å¸ƒäº†è¿™ä¸¤ä¸ªæ•°æ®é›†ã€‚</a></p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<p>ä¸€ã€ç¡®å®šå…¬å¸å¯æŒç»­æ€§å½±å“çš„é‡è¦æ€§åŠæŠ•èµ„è€…å¯¹æ­¤çš„éœ€æ±‚å¢åŠ ã€‚è™½ç„¶ä¼ ç»Ÿè¯„çº§å¾—åˆ°å¹¿æ³›ä¿¡ä»»ï¼Œä½†å­˜åœ¨çš„éš¾ä»¥ç†è§£å’Œæ— æ³•å¤åˆ¶çš„é—®é¢˜ä¿ƒä½¿å¯»æ±‚å…¶ä»–ç‹¬ç«‹æ–¹æ³•äº†è§£å…¬å¸çš„å¯æŒç»­æ€§å®è·µã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e92ba8eca6174383b4ce861c6957fd70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb333061295202f1e05dcdce4802c37.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AceMath-Advancing-Frontier-Math-Reasoning-with-Post-Training-and-Reward-Modeling"><a href="#AceMath-Advancing-Frontier-Math-Reasoning-with-Post-Training-and-Reward-Modeling" class="headerlink" title="AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward   Modeling"></a>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward   Modeling</h2><p><strong>Authors:Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</strong></p>
<p>In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/acemath">https://research.nvidia.com/labs/adlr/acemath</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†AceMathï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å‰æ²¿çš„æ•°å­¦æ¨¡å‹ï¼Œæ“…é•¿è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜ï¼Œä»¥åŠé«˜æ•ˆçš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¯„ä¼°ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå¹¶å¯é åœ°è¯†åˆ«æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†å¼€å‘æŒ‡ä»¤è°ƒä¼˜çš„æ•°å­¦æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æµç¨‹ï¼Œè¯¥æµç¨‹é¦–å…ˆåœ¨ä¸€èˆ¬é¢†åŸŸå®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œç„¶åä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„æç¤ºå’Œåˆæˆç”Ÿæˆçš„å“åº”è¿›è¡Œé’ˆå¯¹æ•°å­¦é¢†åŸŸçš„å¾®è°ƒã€‚AceMath-72B-Instructæ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¤§å¤§è¶…è¶Šäº†Qwen2.5-Math-72B-Instructã€GPT-4oå’ŒClaude-3.5 Sonnetã€‚ä¸ºäº†å¼€å‘ä¸“é—¨çš„æ•°å­¦å¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†AceMath-RewardBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”ç¨³å¥çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ä¸åŒé—®é¢˜å’Œéš¾åº¦çº§åˆ«çš„æ•°å­¦å¥–åŠ±æ¨¡å‹ã€‚ä¹‹åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„å»ºæ•°å­¦å¥–åŠ±æ¨¡å‹çš„ç³»ç»Ÿæ–¹æ³•ã€‚AceMath-72B-RMæ¨¡å‹å§‹ç»ˆä¼˜äºæœ€æ–°çš„å¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå½“å°†AceMath-72B-Instructä¸AceMath-72B-RMç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†æœ€é«˜çš„å¹³å‡rm@8åˆ†æ•°ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/acemath">https://research.nvidia.com/labs/adlr/acemath</a> å‘å¸ƒæ¨¡å‹æƒé‡ã€è®­ç»ƒæ•°æ®å’Œè¯„ä¼°åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15084v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†AceMathç³»åˆ—çš„å‰æ²¿æ•°å­¦æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ“…é•¿è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜ï¼Œå¹¶é…å¤‡äº†é«˜æ•ˆçš„å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå’Œå‡†ç¡®è¯†åˆ«æ­£ç¡®ç­”æ¡ˆã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ï¼ŒAceMathæ¨¡å‹åœ¨é€šç”¨é¢†åŸŸå–å¾—æœ‰ç«äº‰åŠ›çš„è¡¨ç°åï¼Œä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„æç¤ºå’Œåˆæˆç”Ÿæˆå“åº”è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒã€‚AceMathæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ•°å­¦é—®é¢˜çš„è§£ç­”è´¨é‡ã€‚å°†AceMathæ¨¡å‹ä¸å¥–åŠ±æ¨¡å‹ç»“åˆä½¿ç”¨ï¼Œå¯è·å¾—æœ€ä½³çš„å¹³å‡rm@8åˆ†æ•°ã€‚æ¨¡å‹çš„æƒé‡ã€è®­ç»ƒæ•°æ®å’Œè¯„ä¼°åŸºå‡†å°†åœ¨NVIDIAçš„å®˜æ–¹ç½‘ç«™ä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AceMathæ˜¯ä¸€ä¸ªç”¨äºè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„å‰æ²¿æ•°å­¦æ¨¡å‹å¥—ä»¶ã€‚</li>
<li>AceMathé…å¤‡äº†é«˜æ•ˆçš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¯„ä¼°ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå¹¶å‡†ç¡®è¯†åˆ«æ­£ç¡®ç­”æ¡ˆã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ï¼ŒAceMathæ¨¡å‹é¦–å…ˆåœ¨é€šç”¨é¢†åŸŸå–å¾—ç«äº‰åŠ›ï¼Œç„¶åé’ˆå¯¹æ•°å­¦é¢†åŸŸè¿›è¡Œå¾®è°ƒã€‚</li>
<li>AceMathæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>æ„å»ºäº†AceMath-RewardBenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ•°å­¦å¥–åŠ±æ¨¡å‹åœ¨ä¸åŒé—®é¢˜å’Œéš¾åº¦çº§åˆ«ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†æ„å»ºæ•°å­¦å¥–åŠ±æ¨¡å‹çš„ç³»ç»Ÿæ€§æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†AceMath-72B-RMå¥–åŠ±æ¨¡å‹ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰å¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7133db55a42afc04213fe6c92dc96c0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc895e27d85bfd7915d19a8abc9dd501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a733a8c56f4a9e563f396fd866eaa8e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe373e3fc411bdf79ad86d6dca639f99.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLMs-Lost-in-Translation-M-ALERT-uncovers-Cross-Linguistic-Safety-Gaps"><a href="#LLMs-Lost-in-Translation-M-ALERT-uncovers-Cross-Linguistic-Safety-Gaps" class="headerlink" title="LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps"></a>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</h2><p><strong>Authors:Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting</strong></p>
<p>Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities. </p>
<blockquote>
<p>æ„å»ºå¤šè¯­è¨€å®‰å…¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äºç¡®ä¿å®‰å…¨è®¿é—®å’Œè¯­è¨€å¤šæ ·æ€§è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†M-ALERTï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨äº”ç§è¯­è¨€è¯„ä¼°LLMå®‰å…¨æ€§çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚M-ALERTéµå¾ªè¯¦ç»†çš„ALERTåˆ†ç±»æ³•ï¼Œæ¯ç§è¯­è¨€åŒ…å«1.5ä¸‡ä¸ªé«˜è´¨é‡æç¤ºï¼Œæ€»è®¡7.5ä¸‡ä¸ªã€‚æˆ‘ä»¬å¯¹10ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå‡¸æ˜¾äº†ç‰¹å®šè¯­è¨€çš„å®‰å…¨åˆ†æçš„é‡è¦æ€§ï¼Œç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¯­è¨€å’Œç±»åˆ«ä¸Šçš„å®‰å…¨æ€§èƒ½å­˜åœ¨æ˜¾è‘—çš„ä¸ä¸€è‡´æ€§ã€‚ä¾‹å¦‚ï¼ŒLlama 3.2åœ¨æ„å¤§åˆ©è¯­çŠ¯ç½ªç¨ç±»åˆ«ä¸­å­˜åœ¨è¾ƒé«˜çš„ä¸å®‰å…¨é£é™©ï¼Œä½†åœ¨å…¶ä»–è¯­è¨€ä¸­è¡¨ç°å®‰å…¨ã€‚æ‰€æœ‰æ¨¡å‹ä¹‹é—´éƒ½å¯ä»¥è§‚å¯Ÿåˆ°ç±»ä¼¼çš„å·®å¼‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæŸäº›ç±»åˆ«ï¼ˆå¦‚å¤§éº»å’ŒçŠ¯ç½ªå®£ä¼ ï¼‰åœ¨æ‰€æœ‰æ¨¡å‹å’Œè¯­è¨€ä¸­éƒ½èƒ½è§¦å‘ä¸å®‰å…¨çš„å“åº”ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å»ºç«‹ç¨³å¥çš„å¤šè¯­è¨€å®‰å…¨å®è·µçš„éœ€è¦ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒç”¨æˆ·ç¾¤ä½“ä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»çš„ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15035v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨æ„å»ºå¯¹äºä¿éšœå®‰å…¨è®¿é—®å’Œè¯­è¨€å¤šæ ·æ€§è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†M-ALERTï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨äº”ç§è¯­è¨€çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„å®‰å…¨æ€§ã€‚M-ALERTåŒ…å«æŒ‰è¯¦ç»†è­¦æŠ¥åˆ†ç±»çš„75ké«˜è´¨é‡æç¤ºã€‚å¯¹åç§æœ€æ–°LLMçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯­è¨€ç‰¹å®šçš„å®‰å…¨åˆ†æè‡³å…³é‡è¦ï¼Œå› ä¸ºæ¨¡å‹åœ¨ä¸åŒçš„è¯­è¨€å’Œç±»åˆ«ä¸­ç»å¸¸å‡ºç°æ˜¾è‘—çš„å®‰å…¨ä¸ä¸€è‡´æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒæŸäº›æ¨¡å‹åœ¨æŸäº›è¯­è¨€å’Œç±»åˆ«ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„ä¸å®‰å…¨æ€§ï¼Œè€Œåœ¨å…¶ä»–è¯­è¨€å’Œç±»åˆ«ä¸­åˆ™è¡¨ç°è‰¯å¥½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç¡®ä¿LLMåœ¨å®‰å…¨ç¨³å¥çš„å¤šè¯­ç§å®è·µä¸­çš„å¿…è¦æ€§ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒç”¨æˆ·ç¾¤ä½“ä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M-ALERTæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®‰å…¨æ€§çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº”ç§è¯­è¨€ã€‚</li>
<li>M-ALERTåŒ…å«åŸºäºè¯¦ç»†è­¦æŠ¥åˆ†ç±»çš„75ké«˜è´¨é‡æç¤ºã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œä¸åŒLLMåœ¨ä¸åŒè¯­è¨€å’Œç±»åˆ«ä¸­çš„å®‰å…¨æ€§è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>æŸäº›æ¨¡å‹åœ¨æŸäº›è¯­è¨€å’Œç±»åˆ«ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„ä¸å®‰å…¨æ€§ã€‚</li>
<li>å­˜åœ¨æŸäº›ç±»åˆ«ï¼Œå¦‚ç‰©è´¨æ»¥ç”¨å’ŒçŠ¯ç½ªå®£ä¼ ç­‰ï¼Œå§‹ç»ˆè§¦å‘ä¸å®‰å…¨å“åº”ã€‚</li>
<li>éœ€è¦è¿›è¡Œç¨³å¥çš„å¤šè¯­è¨€å®‰å…¨å®è·µï¼Œä»¥ç¡®ä¿LLMåœ¨ä¸åŒç”¨æˆ·ç¾¤ä½“ä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47af63d086ecd6c042e900fb41e51b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-618be958eace0ccb0c9cbe06cf07c69a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c02313a87794259ed3747c051b06baaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6af95263bd2220fb12bd66979b96cde3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7773ef560f44a8d14afbecca0d0e7f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2eeff8cbc94dca6290d8811f1746323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16ee5eaca9853114d527cb236d88755.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Chain-of-MetaWriting-Linguistic-and-Textual-Analysis-of-How-Small-Language-Models-Write-Young-Students-Texts"><a href="#Chain-of-MetaWriting-Linguistic-and-Textual-Analysis-of-How-Small-Language-Models-Write-Young-Students-Texts" class="headerlink" title="Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small   Language Models Write Young Students Texts"></a>Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small   Language Models Write Young Students Texts</h2><p><strong>Authors:Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu</strong></p>
<p>Large Language Models (LLMs) have been used to generate texts in response to different writing tasks: reports, essays, story telling. However, language models do not have a meta-representation of the text writing process, nor inherent communication learning needs, comparable to those of young human students. This paper introduces a fine-grained linguistic and textual analysis of multilingual Small Language Modelsâ€™ (SLMs) writing. With our method, Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process, such as planning and evaluation. We mainly focused on short story and essay writing tasks in French for schoolchildren and undergraduate students respectively. Our results show that SLMs encounter difficulties in assisting young students on sensitive topics such as violence in the schoolyard, and they sometimes use words too complex for the target audience. In particular, the output is quite different from the human produced texts in term of text cohesion and coherence regarding temporal connectors, topic progression, reference. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«ç”¨äºç”Ÿæˆå›åº”ä¸åŒå†™ä½œä»»åŠ¡çš„æ–‡æœ¬ï¼Œå¦‚æŠ¥å‘Šã€æ–‡ç« ã€è®²æ•…äº‹ã€‚ç„¶è€Œï¼Œè¯­è¨€æ¨¡å‹å¹¶ä¸å…·å¤‡æ–‡æœ¬å†™ä½œè¿‡ç¨‹çš„å…ƒè¡¨ç¤ºï¼Œä¹Ÿæ²¡æœ‰ä¸å¹´è½»äººç±»å­¦ç”Ÿç›¸å½“çš„å†…åœ¨æ²Ÿé€šå­¦ä¹ éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†å¯¹å¤šè¯­ç§å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å†™ä½œçš„ç²¾ç»†è¯­è¨€ä¸æ–‡æœ¬åˆ†æã€‚é€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•â€”â€”MetaWritingé“¾ï¼ŒSLMå¯ä»¥æ¨¡ä»¿äººç±»å†™ä½œè¿‡ç¨‹ä¸­çš„ä¸€äº›æ­¥éª¤ï¼Œå¦‚è§„åˆ’å’Œè¯„ä¼°ã€‚æˆ‘ä»¬ä¸»è¦å…³æ³¨é’ˆå¯¹å„¿ç«¥å’Œæœ¬ç§‘ç”Ÿåˆ†åˆ«è¿›è¡Œçš„çŸ­æ•…äº‹å’Œæ–‡ç« å†™ä½œä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒSLMåœ¨è¾…åŠ©å¹´è½»å­¦ç”Ÿå¤„ç†æ•æ„Ÿè¯é¢˜ï¼ˆå¦‚æ ¡å›­æš´åŠ›ï¼‰æ—¶é‡åˆ°å›°éš¾ï¼Œå®ƒä»¬æœ‰æ—¶ä¼šä½¿ç”¨ç›®æ ‡å—ä¼—éš¾ä»¥ç†è§£çš„å¤ªå¤æ‚çš„è¯æ±‡ã€‚å°¤å…¶ä½“ç°åœ¨è¾“å‡ºæ–‡æœ¬åœ¨æ–‡æœ¬è¿è´¯æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä¸äººç±»äº§ç”Ÿçš„æ–‡æœ¬æœ‰å¾ˆå¤§å·®å¼‚ï¼Œæ¶‰åŠæ—¶é—´è¿æ¥è¯ã€ä¸»é¢˜è¿›å±•å’Œå‚è€ƒç­‰æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14986v1">PDF</a> Accepted at WRAICOGS 2025 (Writing Aids at the Crossroads of AI,   Cognitive Science, and NLP) co-located with COLING 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬è®ºæ–‡ä»‹ç»äº†åŸºäºChain-of-MetaWritingæ–¹æ³•çš„å¯¹å°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„å†™ä½œè¿›è¡Œç²¾ç»†çš„è¯­è¨€å’Œæ–‡æœ¬åˆ†æã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¿æ³›åº”ç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œä½†å°å‹è¯­è¨€æ¨¡å‹æ›´å…³æ³¨ç‰¹å®šå¹´é¾„æ®µå­¦ç”Ÿçš„å†™ä½œéœ€æ±‚ã€‚SLMsèƒ½æ¨¡ä»¿äººç±»å†™ä½œè¿‡ç¨‹çš„éƒ¨åˆ†æ­¥éª¤ï¼Œå¦‚è§„åˆ’å’Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›é’ˆå¯¹å„¿ç«¥å’Œæœ¬ç§‘ç”Ÿçš„çŸ­æ–‡å’Œå†™ä½œä»»åŠ¡ä¸­ï¼Œå°å‹è¯­è¨€æ¨¡å‹é¢ä¸´å¤„ç†æ•æ„Ÿè¯é¢˜çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ ¡å›­æš´åŠ›ï¼Œå¹¶æœ‰æ—¶ä½¿ç”¨å¯¹ç›®æ ‡å—ä¼—è¿‡äºå¤æ‚çš„è¯æ±‡ã€‚å…¶è¾“å‡ºæ–‡æœ¬åœ¨è¿è´¯æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä¸äººç±»æ–‡æœ¬å­˜åœ¨å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å¯æ¨¡ä»¿äººç±»å†™ä½œè¿‡ç¨‹çš„è§„åˆ’ä¸è¯„ä»·æ­¥éª¤ã€‚</li>
<li>SLMsåº”ç”¨äºç‰¹å®šå¹´é¾„æ®µå­¦ç”Ÿçš„å†™ä½œä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å¤„ç†æ•æ„Ÿè¯é¢˜ã€‚</li>
<li>åœ¨å¤„ç†æ ¡å›­æš´åŠ›ç­‰æ•æ„Ÿè¯é¢˜æ—¶ï¼ŒSLMsæœ‰æ—¶ä½¿ç”¨å¯¹ç›®æ ‡å—ä¼—è¿‡äºå¤æ‚çš„è¯æ±‡ã€‚</li>
<li>ä¸äººç±»æ–‡æœ¬ç›¸æ¯”ï¼ŒSLMsè¾“å‡ºçš„æ–‡æœ¬åœ¨è¿è´¯æ€§å’Œä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚ç‰¹åˆ«æ˜¯åœ¨æ—¶æ€è¿è¯ã€è¯é¢˜è¿›å±•å’Œå‚è€ƒç­‰æ–¹é¢æœ‰æ˜æ˜¾å·®è·ã€‚</li>
<li>é€šè¿‡Chain-of-MetaWritingæ–¹æ³•è¿›è¡Œçš„æ–‡æœ¬åˆ†ææ›´åŠ ç»†è‡´æ·±å…¥åœ°äº†è§£SLMsåœ¨å†™ä½œæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ç¼ºä¹å¯¹å®é™…å†™ä½œè¿‡ç¨‹çš„å…ƒè¡¨ç¤ºå’Œæ²Ÿé€šå­¦ä¹ éœ€æ±‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSLMsæ›´æ³¨é‡ç‰¹å®šä»»åŠ¡å’Œç›®æ ‡å—ä¼—çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33e0913ad9017149c891c92c9456f6a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d316df2da0d830a800a6f8d1593bc120.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-21\./crop_LLM/2412.14986v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation"><a href="#TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation" class="headerlink" title="TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"></a>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</h2><p><strong>Authors:Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li</strong></p>
<p>In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5% on TOMG-Bench. Our codes and datasets are available through <a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench">https://github.com/phenixace/TOMG-Bench</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¼€æ”¾åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆTOMG-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾åŸŸåˆ†å­ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚TOMG-BenchåŒ…å«ä¸‰ä¸ªä¸»è¦ä»»åŠ¡çš„æ•°æ®é›†ï¼šåˆ†å­ç¼–è¾‘ï¼ˆMolEditï¼‰ã€åˆ†å­ä¼˜åŒ–ï¼ˆMolOptï¼‰å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆï¼ˆMolCustomï¼‰ã€‚æ¯ä¸ªä»»åŠ¡è¿›ä¸€æ­¥åŒ…å«ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡åŒ…å«5000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚é‰´äºå¼€æ”¾åˆ†å­ç”Ÿæˆçš„å›ºæœ‰å¤æ‚æ€§ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿï¼Œå¸®åŠ©è¡¡é‡ç”Ÿæˆåˆ†å­çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¯¹25ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„å…¨é¢åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ–‡æœ¬å¼•å¯¼åˆ†å­å‘ç°å½“å‰çš„å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚æ­¤å¤–ï¼Œå€ŸåŠ©ä¸“ä¸ºè§£å†³TOMG-Benchæå‡ºçš„æŒ‘æˆ˜è€Œè®¾è®¡çš„ä¸“ç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†OpenMolInsï¼ŒLlama3.1-8Bå¯ä»¥è¶…è¶Šæ‰€æœ‰å¼€æºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”šè‡³åœ¨TOMG-Benchä¸Šçš„è¡¨ç°è¶…è¿‡GPT-3.5-turboçš„46.5%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/phenixace/TOMG-Benchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14642v1">PDF</a> A benchmark for text-based open molecule generation</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„å¼€æ”¾åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆTOMG-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€æ”¾åŸŸåˆ†å­ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚TOMG-BenchåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼šåˆ†å­ç¼–è¾‘ï¼ˆMolEditï¼‰ã€åˆ†å­ä¼˜åŒ–ï¼ˆMolOptï¼‰å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆï¼ˆMolCustomï¼‰ã€‚æ¯ä¸ªä»»åŠ¡åŒ…å«ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡åŒ…å«5000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚æ–‡æœ¬è¿˜ä»‹ç»äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆåˆ†å­çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚å¯¹25ä¸ªLLMçš„å…¨é¢åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ–‡æœ¬å¼•å¯¼åˆ†å­å‘ç°æ–¹é¢çš„å½“å‰å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚ä½¿ç”¨OpenMolInsè¿™ä¸€ä¸“ç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è§£å†³äº†TOMG-Benchæå‡ºçš„æŒ‘æˆ˜ï¼ŒLlama3.1-8Bçš„è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰å¼€æºé€šç”¨LLMï¼Œç”šè‡³è¶…è¿‡äº†GPT-3.5 Turboçš„46.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æå‡ºäº†TOMG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾åŸŸåˆ†å­ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>TOMG-BenchåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼šåˆ†å­ç¼–è¾‘ã€åˆ†å­ä¼˜åŒ–å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆã€‚</li>
<li>è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿç”¨äºè¯„ä¼°ç”Ÿæˆåˆ†å­çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¯¹å¤šä¸ªLLMçš„å…¨é¢åŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚</li>
<li>OpenMolInsæ•°æ®é›†è¢«ç”¨äºè§£å†³TOMG-Benchçš„æŒ‘æˆ˜ã€‚</li>
<li>Llama3.1-8Båœ¨TOMG-Benchä¸Šçš„è¡¨ç°è¶…è¿‡äº†å…¶ä»–LLMï¼ŒåŒ…æ‹¬GPT-3.5 Turboã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d82cb8b8fa1fe22ad856dcbdd8bb87c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3ea1b90efb97df8d551454ed5ab085b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23ed033b9fd9e96bfc0666d948365f59.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models"><a href="#Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models" class="headerlink" title="Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models"></a>Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models</h2><p><strong>Authors:Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai</strong></p>
<p>This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base modelsâ€™ original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Typhoon 2ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹æ³°è¯­ä¼˜åŒ–çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚Typhoon2-Textå»ºç«‹åœ¨æœ€å‰æ²¿çš„å¼€æ”¾æ¨¡å‹ä¸Šï¼Œå¦‚Llama 3å’ŒQwen2ï¼Œæˆ‘ä»¬å¯¹è‹±è¯­å’Œæ³°è¯­æ•°æ®çš„æ··åˆè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚æˆ‘ä»¬é‡‡ç”¨åè®­ç»ƒæŠ€æœ¯ï¼Œåœ¨æé«˜æ³°è¯­æ€§èƒ½çš„åŒæ—¶ï¼Œä¿ç•™åŸºç¡€æ¨¡å‹çš„åŸå§‹åŠŸèƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸åŒå¤§å°çš„æ–‡æœ¬æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»1äº¿åˆ°70äº¿ï¼Œæ—¢æœ‰åŸºç¡€æ¨¡å‹ä¹Ÿæœ‰æŒ‡ä»¤è°ƒæ•´å‹å˜ç§ã€‚ä¸ºäº†å¼•å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œæˆ‘ä»¬å‘å¸ƒäº†Typhoon2-Safetyï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ³°å›½æ–‡åŒ–å’Œè¯­è¨€å¢å¼ºçš„åˆ†ç±»å™¨ã€‚Typhoon2-Visionåœ¨ä¿ç•™é€šç”¨è§†è§‰åŠŸèƒ½ï¼ˆå¦‚å›¾åƒæè¿°ï¼‰çš„åŒæ—¶ï¼Œæé«˜äº†å¯¹æ³°è¯­æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ã€‚Typhoon2-Audioå¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€è¯­éŸ³å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13702v2">PDF</a> technical report, 55 pages</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†Typhoon 2ç³»åˆ—æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥ç³»åˆ—æ¨¡å‹é’ˆå¯¹æ³°è¯­è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚Typhoon2-TextåŸºäºæœ€å‰æ²¿çš„å¼€æ”¾æ¨¡å‹ï¼Œå¦‚Llama 3å’ŒQwen2ï¼Œè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œä½¿ç”¨æ··åˆçš„è‹±è¯­å’Œæ³°è¯­æ•°æ®ã€‚é‡‡ç”¨åè®­ç»ƒæŠ€æœ¯æé«˜æ³°è¯­æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸åŒè§„æ¨¡çš„æ–‡æœ¬æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒæ•´å‹ã€‚ä¸ºå¼•å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œæ¨å‡ºäº†Typhoon2-Safetyï¼Œä¸€ä¸ªå¢å¼ºæ³°è¯­æ–‡åŒ–å’Œè¯­è¨€çš„åˆ†ç±»å™¨ã€‚Typhoon2-Visionæé«˜äº†å¯¹æ³°è¯­æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†å›¾åƒçš„ä¸€èˆ¬åŠŸèƒ½ï¼Œå¦‚å›¾åƒæ ‡é¢˜ç”Ÿæˆã€‚Typhoon2-Audioå¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€è¯­éŸ³å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Typhoon 2ç³»åˆ—æ¨¡å‹æ˜¯é’ˆå¯¹æ³°è¯­ä¼˜åŒ–çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>Typhoon 2åŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ï¼Œæ—¨åœ¨æ»¡è¶³å¤šç§éœ€æ±‚ã€‚</li>
<li>Typhoon2-TextåŸºäºå‰æ²¿å¼€æ”¾æ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ··åˆçš„è‹±è¯­å’Œæ³°è¯­æ•°æ®ã€‚</li>
<li>åè®­ç»ƒæŠ€æœ¯ç”¨äºæé«˜æ³°è¯­æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚</li>
<li>å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸åŒè§„æ¨¡çš„æ–‡æœ¬æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸åŒå°ºå¯¸å’Œç±»å‹çš„åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒæ•´å‹æ¨¡å‹ã€‚</li>
<li>Typhoon2-Safetyåˆ†ç±»å™¨ç”¨äºå¼•å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œé€‚åº”æ³°è¯­æ–‡åŒ–å’Œè¯­è¨€ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-827c73ec6d26ad52cff0c2a413bda65b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="G-VEval-A-Versatile-Metric-for-Evaluating-Image-and-Video-Captions-Using-GPT-4o"><a href="#G-VEval-A-Versatile-Metric-for-Evaluating-Image-and-Video-Captions-Using-GPT-4o" class="headerlink" title="G-VEval: A Versatile Metric for Evaluating Image and Video Captions   Using GPT-4o"></a>G-VEval: A Versatile Metric for Evaluating Image and Video Captions   Using GPT-4o</h2><p><strong>Authors:Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung</strong></p>
<p>Evaluation metric of visual captioning is important yet not thoroughly explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are limited in zero-shot scenarios. Advanced Language Model-based metrics also struggle with aligning to nuanced human preferences. To address these issues, we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and supports three modes: reference-free, reference-only, and combined, accommodating both video and image inputs. We also propose MSVD-Eval, a new dataset for video captioning evaluation, to establish a more transparent and consistent framework for both human experts and evaluation metrics. It is designed to address the lack of clear criteria in existing datasets by introducing distinct dimensions of Accuracy, Completeness, Conciseness, and Relevance (ACCR). Extensive results show that G-VEval outperforms existing methods in correlation with human annotations, as measured by Kendall tau-b and Kendall tau-c. This provides a flexible solution for diverse captioning tasks and suggests a straightforward yet effective approach for large language models to understand video content, paving the way for advancements in automated captioning. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/ztangaj/gveval">https://github.com/ztangaj/gveval</a> </p>
<blockquote>
<p>è§†è§‰æ ‡é¢˜çš„è¯„ä»·æŒ‡æ ‡éå¸¸é‡è¦ï¼Œä½†å°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚ä¼ ç»Ÿçš„è¯„ä»·æŒ‡æ ‡ï¼Œå¦‚BLEUã€METEORã€CIDErå’ŒROUGEï¼Œå¾€å¾€å¿½ç•¥äº†è¯­ä¹‰æ·±åº¦ï¼Œè€Œè®­ç»ƒæŒ‡æ ‡å¦‚CLIP-Scoreã€PAC-Så’ŒPolosåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­å—åˆ°é™åˆ¶ã€‚å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åŸºç¡€æŒ‡æ ‡ä¹Ÿå¾ˆéš¾ä¸å¾®å¦™çš„äººç±»åå¥½å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†G-VEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå—G-Evalå¯å‘çš„æ–°æŒ‡æ ‡ï¼Œç”±æ–°çš„GPT-4oæä¾›æ”¯æŒã€‚G-VEvalä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æ€ç»´é“¾æ¨ç†ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼ï¼šæ— å‚è€ƒã€ä»…æœ‰å‚è€ƒå’Œç»„åˆï¼Œé€‚åº”è§†é¢‘å’Œå›¾åƒè¾“å…¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MSVD-Evalï¼Œä¸€ä¸ªæ–°çš„è§†é¢‘æ ‡é¢˜è¯„ä»·æ•°æ®é›†ï¼Œä¸ºäººå·¥ä¸“å®¶å’Œè¯„ä»·æŒ‡æ ‡å»ºç«‹ä¸€ä¸ªæ›´é€æ˜å’Œä¸€è‡´çš„è¯„ä»·æ¡†æ¶ã€‚å®ƒé€šè¿‡å¼•å…¥å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ç®€æ´æ€§å’Œç›¸å…³æ€§ï¼ˆACCRï¼‰çš„ä¸åŒç»´åº¦ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹æ˜ç¡®æ ‡å‡†çš„é—®é¢˜ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼ŒG-VEvalåœ¨ä¸äººç±»æ³¨é‡Šçš„ç›¸å…³æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¦‚Kendall tau-bå’ŒKendall tau-cæ‰€ç¤ºã€‚è¿™ä¸ºå„ç§æ ‡é¢˜ä»»åŠ¡æä¾›äº†çµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç†è§£è§†é¢‘å†…å®¹æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨æ ‡é¢˜åˆ¶ä½œçš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ztangaj/gveval">https://github.com/ztangaj/gveval</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13647v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰æè¿°è¯„ä¼°çš„é‡è¦æ€§åŠç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†æ–°å‹è¯„ä¼°æ–¹æ³•G-VEvalï¼Œå¹¶ä»‹ç»äº†å…¶ä¸‰ç§æ¨¡å¼ï¼Œè¯¥è¯„ä¼°æ–¹æ³•ä»¥GPT-4oä¸ºé©±åŠ¨æ”¯æŒé“¾å¼æ¨ç†æ–¹å¼è¯„ä¼°å›¾åƒå’Œè§†é¢‘æè¿°ï¼Œæ•ˆæœæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’Œç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚åŒæ—¶ä»‹ç»äº†MSVD-Evalæ•°æ®é›†çš„è®¾è®¡å’Œç‰¹æ€§ã€‚è¿™ä¸€æ”¹è¿›æ–¹æ¡ˆä¸ºæœªæ¥è‡ªåŠ¨æè¿°è¯„ä¼°é¢†åŸŸæä¾›äº†æ–°çš„æ–¹å‘ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰æè¿°è¯„ä¼°æ–¹æ³•çš„å±€é™ï¼šä¼ ç»Ÿçš„BLEUç­‰æ–¹æ³•å¿½è§†äº†è¯­ä¹‰æ·±åº¦ï¼›è®­ç»ƒçš„åº¦é‡æ ‡å‡†åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹å­˜åœ¨å±€é™æ€§ï¼›é«˜çº§è¯­è¨€æ¨¡å‹åº¦é‡éš¾ä»¥ä¸å¾®å¦™çš„äººç±»åå¥½å¯¹é½ã€‚</li>
<li>æ–°æå‡ºçš„è¯„ä¼°æ–¹æ³•ï¼šä»‹ç»G-VEvalæŒ‡æ ‡åŠå…¶ä¸»è¦ç‰¹æ€§ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é“¾å¼æ¨ç†æŠ€æœ¯ã€‚æä¾›ä¸‰ç§æ¨¡å¼ä»¥æ»¡è¶³ä¸åŒçš„éœ€æ±‚ï¼šæ— å‚è€ƒæ¨¡å¼ã€ä»…å‚è€ƒæ¨¡å¼å’Œç»„åˆæ¨¡å¼ã€‚æ”¯æŒè§†é¢‘å’Œå›¾åƒè¾“å…¥ã€‚</li>
<li>æ–°æ•°æ®é›†MSVD-Evalçš„ä»‹ç»ï¼šä¸“ä¸ºè§†é¢‘æè¿°è¯„ä¼°è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†ç¼ºä¹æ˜ç¡®æ ‡å‡†çš„é—®é¢˜ã€‚å¼•å…¥å››ä¸ªç»´åº¦ï¼šå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ç®€æ´æ€§å’Œç›¸å…³æ€§ï¼ˆACCRï¼‰ã€‚</li>
<li>G-VEvalæ€§èƒ½è¡¨ç°ï¼šä¸äººç±»æ³¨é‡Šç›¸æ¯”ï¼ŒG-VEvalåœ¨ç›¸å…³æ€§è¯„ä»·æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¦‚Kendall tau-bå’ŒKendall tau-cçš„è¯„ä»·ç»“æœæ‰€ç¤ºã€‚å®ƒä¸ºå¤šæ ·åŒ–çš„æè¿°ä»»åŠ¡æä¾›äº†çµæ´»è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LLMå¯¹è§†é¢‘å†…å®¹çš„ç†è§£èƒ½åŠ›æå‡ï¼šä½¿ç”¨æ–°å‹è¯„ä¼°æ–¹å¼å¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„ç†è§£ï¼Œä¸ºæœªæ¥è‡ªåŠ¨åŒ–æè¿°é¢†åŸŸçš„å‘å±•é“ºå¹³é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40b8583c074dad2ce2d61f48f2b27ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb206cb892842077b56340e9b1d86e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39e064d904cb63025dbc1ca392b3a98d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b49aff845cc408d02d4d73e4980d4b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-facbbae06cae95267265a876be4c932f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Does-VLM-Classification-Benefit-from-LLM-Description-Semantics"><a href="#Does-VLM-Classification-Benefit-from-LLM-Description-Semantics" class="headerlink" title="Does VLM Classification Benefit from LLM Description Semantics?"></a>Does VLM Classification Benefit from LLM Description Semantics?</h2><p><strong>Authors:Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, BjÃ¶rn Ommer</strong></p>
<p>Accurately describing images with text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect, where multiple modified text prompts act as a noisy test-time augmentation for the original one. We propose an alternative evaluation scenario to decide if a performance boost of LLM-generated descriptions is caused by such a noise augmentation effect or rather by genuine description semantics. The proposed scenario avoids noisy test-time augmentation and ensures that genuine, distinctive descriptions cause the performance boost. Furthermore, we propose a training-free method for selecting discriminative descriptions that work independently of classname-ensembling effects. Our approach identifies descriptions that effectively differentiate classes within a local CLIP label neighborhood, improving classification accuracy across seven datasets. Additionally, we provide insights into the explainability of description-based image classification with VLMs. </p>
<blockquote>
<p>ç”¨æ–‡æœ¬å‡†ç¡®æè¿°å›¾åƒæ˜¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½çš„åŸºç¡€ã€‚åƒCLIPè¿™æ ·çš„è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼Œè¡¨è¾¾è§†è§‰å’Œè¯­è¨€åµŒå…¥ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æè¿°å¯ä»¥æ”¹å–„VLMåˆ†ç±»ã€‚ç„¶è€Œï¼Œç”±äºæ€§èƒ½æå‡ä¹Ÿå¯èƒ½æºäºä¸€ç§è¯­ä¹‰æ— å…³çš„é›†æˆæ•ˆåº”ï¼Œå³å¤šä¸ªä¿®æ”¹åçš„æ–‡æœ¬æç¤ºä½œä¸ºåŸå§‹æç¤ºçš„å™ªå£°æµ‹è¯•æ—¶é—´å¢å¼ºï¼Œå› æ­¤å¾ˆéš¾ç¡®å®šå®é™…æè¿°è¯­ä¹‰çš„è´¡çŒ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£çš„è¯„ä»·åœºæ™¯ï¼Œä»¥ç¡®å®šLLMç”Ÿæˆçš„æè¿°çš„æ€§èƒ½æå‡æ˜¯ç”±äºå™ªå£°å¢å¼ºæ•ˆåº”è¿˜æ˜¯çœŸæ­£çš„æè¿°è¯­ä¹‰æ‰€é€ æˆçš„ã€‚æ‰€æå‡ºçš„åœºæ™¯é¿å…äº†å™ªå£°æµ‹è¯•æ—¶é—´å¢å¼ºï¼Œå¹¶ç¡®ä¿çœŸæ­£çš„ã€æœ‰ç‰¹è‰²çš„æè¿°ä¼šå¼•èµ·æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„é€‰æ‹©æœ‰è¾¨åˆ«åŠ›çš„æè¿°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç‹¬ç«‹äºç±»åˆ«é›†æˆæ•ˆåº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«åœ¨CLIPæ ‡ç­¾é‚»åŸŸå†…æœ‰æ•ˆåŒºåˆ†ç±»åˆ«çš„æè¿°ï¼Œä»è€Œæé«˜ä¸ƒä¸ªæ•°æ®é›†çš„åˆ†ç±»ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ·±å…¥æ¢è®¨äº†åŸºäºæè¿°çš„å›¾åƒåˆ†ç±»ä¸VLMçš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11917v3">PDF</a> AAAI-25 (extended version), Code: <a target="_blank" rel="noopener" href="https://github.com/CompVis/DisCLIP">https://github.com/CompVis/DisCLIP</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬çš„å›¾åƒå‡†ç¡®æè¿°æ˜¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½çš„åŸºç¡€ã€‚æœ¬æ–‡æå‡ºä¸€ç§è¯„ä¼°åœºæ™¯ï¼Œæ—¨åœ¨ç¡®å®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æè¿°æ€§èƒ½æå‡æ˜¯ç”±äºå™ªå£°æµ‹è¯•æ—¶é—´å¢å¼ºæ•ˆåº”è¿˜æ˜¯çœŸæ­£çš„æè¿°è¯­ä¹‰å¼•èµ·çš„ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ç‹¬ç«‹äºè®­ç»ƒçš„é€‰æ‹©æ€§é‰´åˆ«æè¿°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½è¯†åˆ«æœ¬åœ°CLIPæ ‡ç­¾é‚»åŸŸå†…æœ‰æ•ˆåŒºåˆ†ä¸åŒç±»åˆ«çš„æè¿°ï¼Œå¹¶åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šæé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†åŸºäºæè¿°çš„å›¾åƒåˆ†ç±»çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision-Language Models (VLMs)å¦‚CLIPé€šè¿‡å¯¹å›¾åƒå’Œæ–‡æœ¬åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„å¯¹é½ï¼Œè¡¨è¾¾äº†è§†è§‰å’Œè¯­è¨€åµŒå…¥ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå®ç°äº†å›¾åƒçš„æ–‡æœ¬æè¿°ã€‚</li>
<li>LLMç”Ÿæˆçš„æè¿°å¯ä»¥æ”¹å–„VLMåˆ†ç±»ï¼Œä½†ç¡®å®šæ€§èƒ½æå‡çš„çœŸæ­£æ¥æºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¯„ä¼°åœºæ™¯æ¥ç¡®å®šLLMç”Ÿæˆçš„æè¿°æ€§èƒ½æå‡æ˜¯å¦æºäºçœŸæ­£çš„æè¿°è¯­ä¹‰ï¼Œé¿å…äº†å™ªå£°æµ‹è¯•æ—¶é—´å¢å¼ºæ•ˆåº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç‹¬ç«‹äºè®­ç»ƒçš„é€‰æ‹©æ€§é‰´åˆ«æè¿°æ–¹æ³•ï¼Œèƒ½è¯†åˆ«åœ¨æœ¬åœ°CLIPæ ‡ç­¾é‚»åŸŸå†…æœ‰æ•ˆåŒºåˆ†ä¸åŒç±»åˆ«çš„æè¿°ï¼Œæé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29d64018c8376442da19635bbbac8d53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efdcfdda3be02c28fec68a831d4f111f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37c0827cceac6c14ae9606ecddf3511c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b1318707ade6f3c01af6c13f774a599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee3f9109dfac30ac6e5e3f246440a8f2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by â€œselecting sparsely and computing denselyâ€. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹LLMçš„PEFTæ–¹æ³•åªèƒ½åŒæ—¶å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒæˆ–å¯æ‰©å±•çš„æœåŠ¡ï¼Œè€Œæ— æ³•ä¸‰è€…å…¼é¡¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç¨€ç–å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨è¿™ä¸€å…³é”®è§è§£ï¼Œæˆ‘ä»¬é’ˆå¯¹LLMæå‡ºäº†ä¸€ç³»åˆ—ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆS$^{2}$FTï¼‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§ã€‚S$^{2}$FTé€šè¿‡â€œç¨€ç–é€‰æ‹©ã€å¯†é›†è®¡ç®—â€æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®ƒåˆ†åˆ«é€‰æ‹©MHAå’ŒFFNæ¨¡å—ä¸­æ¯ä¸ªTransformerå—çš„å‡ ä¸ªå¤´å’Œé€šé“ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå¯¹LLMä¸­è€¦åˆç»“æ„ä¸¤ä¾§çš„æƒé‡çŸ©é˜µè¿›è¡Œå…±ç½®æ¢ï¼Œä»¥å°†æ¯å±‚çš„æ‰€é€‰ç»„ä»¶è¿æ¥æˆå¯†é›†çš„å­çŸ©é˜µã€‚æœ€åï¼ŒS$^{2}$FTå¯¹æ‰€æœ‰å­çŸ©é˜µæ‰§è¡ŒåŸåœ°æ¢¯åº¦æ›´æ–°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢é˜²æ­¢é—å¿˜åˆç®€åŒ–äº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¸LoRAç›¸æ¯”å¹³å‡æé«˜äº†4.6%å’Œ1.3%ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒåæ¨å¹¿åˆ°ä¸åŒé¢†åŸŸæ—¶ï¼Œè¾ƒå…¨é‡å¾®è°ƒæé«˜äº†11.5%ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTåœ¨è®­ç»ƒå†…å­˜æ–¹é¢èŠ‚çœäº†é«˜è¾¾3å€ï¼Œä¸å…¨é‡å¾®è°ƒç›¸æ¯”ï¼Œå»¶è¿Ÿæé«˜äº†1.5-2.7å€ï¼ŒåŒæ—¶åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šéƒ½è¾ƒLoRAå¹³å‡æé«˜äº†10%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼ŒS$^{2}$FTä¸­çš„æƒé‡æ›´æ–°å¯ä»¥è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¸ºå®ç°å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆèåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡ŒæœåŠ¡æä¾›æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¯¹LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆSÂ²FTï¼‰çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®ç°å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§çš„åŒæ—¶è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚SÂ²FTé€šè¿‡é€‰æ‹©ç¨€ç–è®¡ç®—å¯†é›†çš„æ–¹å¼å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå®ƒé€šè¿‡é€‰æ‹©Transformerå—ä¸­MHAå’ŒFFNæ¨¡å—çš„å°‘æ•°å¤´éƒ¨å’Œé€šé“ï¼Œå¹¶å¯¹LLMä¸­è€¦åˆç»“æ„çš„æƒé‡çŸ©é˜µè¿›è¡Œå…±æ’åˆ—ï¼Œå°†æ‰€é€‰ç»„ä»¶åœ¨æ¯å±‚ä¸­è¿æ¥æˆå¯†é›†å­çŸ©é˜µã€‚æœ€åï¼ŒSÂ²FTå¯¹æ‰€æœ‰å­çŸ©é˜µè¿›è¡ŒåŸåœ°æ¢¯åº¦æ›´æ–°ã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•é˜²æ­¢é—å¿˜ï¼Œç®€åŒ–ä¼˜åŒ–ï¼Œåœ¨å¸¸è¯†æ¨ç†å’Œç®—æœ¯æ¨ç†æ–¹é¢å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSÂ²FTè¿˜é€šè¿‡éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•èŠ‚çœè®­ç»ƒå†…å­˜ï¼Œæé«˜å»¶è¿Ÿï¼ŒåŒæ—¶æé«˜æ€§èƒ½ã€‚æœ€åï¼Œæ–‡ç« å±•ç¤ºäº†SÂ²FTä¸­çš„æƒé‡æ›´æ–°å¯ä»¥è¢«è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¸ºèåˆå¤šä¸ªå¾®è°ƒæ¨¡å‹æä¾›äº†æœ‰æ•ˆã€å¿«é€Ÿå’Œé«˜æ•ˆå¹¶è¡Œçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰PEFTæ–¹æ³•å¯¹LLMså­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•åŒæ—¶å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯æ‰©å±•æœåŠ¡ã€‚</li>
<li>å¼•å…¥ç¨€ç–å¾®è°ƒå¹¶è§‚å¯Ÿåˆ°å…¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆSÂ²FTï¼‰æ–¹æ³•ï¼ŒåŒæ—¶å®ç°æœ€ä½³å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§ã€‚</li>
<li>SÂ²FTé€šè¿‡é€‰æ‹©ç¨€ç–éƒ¨åˆ†å¹¶è¿›è¡Œå¯†é›†è®¡ç®—æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>SÂ²FTåœ¨ç†è®ºåˆ†æå’Œå®éªŒç»“æœä¸Šè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢ã€‚</li>
<li>SÂ²FTé‡‡ç”¨éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒèŠ‚çœè®­ç»ƒå†…å­˜å¹¶æé«˜å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c5cc0d9ede3c4970f3bcc409080770a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BayLing-2-A-Multilingual-Large-Language-Model-with-Efficient-Language-Alignment"><a href="#BayLing-2-A-Multilingual-Large-Language-Model-with-Efficient-Language-Alignment" class="headerlink" title="BayLing 2: A Multilingual Large Language Model with Efficient Language   Alignment"></a>BayLing 2: A Multilingual Large Language Model with Efficient Language   Alignment</h2><p><strong>Authors:Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng</strong></p>
<p>Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life. However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide. An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly. In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment. To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages. Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale. For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages. Furthermore, results on English benchmarks indicate that BayLing maintains high performance in highresource languages while enhancing the performance in low-resource languages. Demo, homepage, code and models of BayLing are available. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å’Œä¸°å¯Œçš„çŸ¥è¯†ï¼Œèƒ½å¤Ÿæ”¯æŒæ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›èƒ½åŠ›ä¸»è¦é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€ä¸Šï¼Œå¯¼è‡´ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆèƒ½åŠ›è¾ƒå¼±ï¼ŒçŸ¥è¯†ç›¸å¯¹æœ‰é™ã€‚å› æ­¤ï¼Œå¢å¼ºLLMçš„å¤šè¯­è¨€èƒ½åŠ›å¯¹äºæœåŠ¡äºå…¨çƒ100å¤šç§è¯­è¨€ç¤¾åŒºè‡³å…³é‡è¦ã€‚å¢å¼ºå¤šè¯­è¨€èƒ½åŠ›çš„ä¸€ç§ç›´è§‚æ–¹æ³•æ˜¯ä¸ºå„ç§è¯­è¨€æ„å»ºæŒ‡ä»¤æ•°æ®ï¼Œä½†æ˜¯ä¸ºè¶…è¿‡100ç§è¯­è¨€æ„å»ºæŒ‡ä»¤æ•°æ®çš„æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†BayLing 2ï¼Œå®ƒé€šè¿‡è¯­è¨€å¯¹é½ï¼Œæœ‰æ•ˆåœ°å°†ä»é«˜èµ„æºè¯­è¨€è½¬ç§»åˆ°ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆèƒ½åŠ›å’ŒçŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«320ä¸‡æ¡æŒ‡ä»¤çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬é«˜èµ„æºè¯­è¨€æŒ‡ä»¤ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰å’Œ100å¤šç§è¯­è¨€çš„è·¨è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åŸºäºè¯¥æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥ä¿ƒè¿›è¯­è¨€ä¹‹é—´çš„èƒ½åŠ›è½¬ç§»ã€‚æˆ‘ä»¬ä»¥Llamaä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¼€å‘äº†BayLing-2-7Bã€BayLing-2-13Bå’ŒBayLing-2-8Bï¼Œå¹¶å¯¹BayLingè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚åœ¨100å¤šç§è¯­è¨€çš„è·¨è¯­è¨€ç¿»è¯‘æ–¹é¢ï¼ŒBayLingä¸ç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ç›¸æ¯”è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åœ¨å¤šè¯­è¨€çŸ¥è¯†å’Œç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBayLingåœ¨è¶…è¿‡20ç§ä½èµ„æºè¯­è¨€ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¿™è¯æ˜äº†å…¶ä»é«˜èµ„æºè¯­è¨€åˆ°ä½èµ„æºè¯­è¨€çš„æœ‰æ•ˆçŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨è‹±è¯­åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨è¡¨æ˜ï¼ŒBayLingåœ¨é«˜èµ„æºè¯­è¨€ä¸Šä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚BayLingçš„æ¼”ç¤ºã€ä¸»é¡µã€ä»£ç å’Œæ¨¡å‹éƒ½å·²æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16300v3">PDF</a> BayLing 2â€™s online demo: <a target="_blank" rel="noopener" href="http://nlp.ict.ac.cn/bayling/demo">http://nlp.ict.ac.cn/bayling/demo</a>. BayLing   2â€™s code and models: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/BayLing">https://github.com/ictnlp/BayLing</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ç§ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å’Œå¹¿æ³›çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›èƒ½åŠ›ä¸»è¦é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€ä¸Šï¼Œå¯¼è‡´ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆèƒ½åŠ›è¾ƒå¼±ï¼ŒçŸ¥è¯†ç›¸å¯¹æœ‰é™ã€‚å¢å¼ºLLMçš„å¤šè¯­è¨€èƒ½åŠ›å¯¹äºæœåŠ¡å…¨çƒ100å¤šç§è¯­è¨€ç¤¾åŒºè‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†BayLing 2ï¼Œå®ƒé€šè¿‡è¯­è¨€å¯¹é½ï¼Œé«˜æ•ˆåœ°ä»é«˜èµ„æºè¯­è¨€å‘ä½èµ„æºè¯­è¨€è½¬ç§»ç”Ÿæˆèƒ½åŠ›å’ŒçŸ¥è¯†ã€‚æˆ‘ä»¬æ„å»ºäº†åŒ…å«320ä¸‡æ¡æŒ‡ä»¤çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«é«˜èµ„æºè¯­è¨€æŒ‡ä»¤ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰ä»¥åŠé€‚ç”¨äº100å¤šç§è¯­è¨€çš„è·¨è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åŸºäºè¯¥æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥ä¿ƒè¿›è¯­è¨€é—´çš„èƒ½åŠ›è½¬ç§»ã€‚ä½¿ç”¨Llamaä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†BayLing-2-7Bã€BayLing-2-13Bå’ŒBayLing-2-8Bï¼Œå¹¶å¯¹BayLingè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚åœ¨å¤šè¯­ç§ç¿»è¯‘å’Œå¤šç§è¯­è¨€çŸ¥è¯†ç†è§£æ–¹é¢ï¼ŒBayLingè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å’ŒçŸ¥è¯†ï¼Œä½†ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆèƒ½åŠ›è¾ƒå¼±ã€‚</li>
<li>å¢å¼ºLLMçš„å¤šè¯­è¨€èƒ½åŠ›å¯¹æœåŠ¡å…¨çƒå„ç§è¯­è¨€ç¤¾åŒºè‡³å…³é‡è¦ã€‚</li>
<li>BayLing 2é€šè¿‡è¯­è¨€å¯¹é½ï¼Œä»é«˜èµ„æºè¯­è¨€å‘ä½èµ„æºè¯­è¨€è½¬ç§»ç”Ÿæˆèƒ½åŠ›å’ŒçŸ¥è¯†ã€‚</li>
<li>BayLingæ„å»ºäº†åŒ…å«é«˜èµ„æºè¯­è¨€å’Œè·¨è¯­è¨€æŒ‡ä»¤çš„æ•°æ®é›†ï¼Œç”¨äºæŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>BayLing-2æ¨¡å‹ç³»åˆ—æ˜¯åŸºäºLlamaåŸºç¡€æ¨¡å‹å¼€å‘çš„ã€‚</li>
<li>BayLingåœ¨å¤šè¯­ç§ç¿»è¯‘å’Œå¤šç§è¯­è¨€çŸ¥è¯†ç†è§£æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14b21ae1803c025f67dafec51a75efaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a41976009602db1b1a7c55c4aacf103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94c3f0552a2c59041ddaf6111b1f3128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f84ffff19295258ea4fe9f1d467e8cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b88bd7042014a6d52ca28cb26744a84c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65cb59c2aaaf5042c9ea47db332b61b7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CodeLutra-Boosting-LLM-Code-Generation-via-Preference-Guided-Refinement"><a href="#CodeLutra-Boosting-LLM-Code-Generation-via-Preference-Guided-Refinement" class="headerlink" title="CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement"></a>CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement</h2><p><strong>Authors:Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra</strong></p>
<p>Large Language Models (LLMs) have revolutionized code generation but require significant resources and often over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective alternative. However, standard supervised approaches rely only on correct examples, missing valuable insights from failures. We introduce CodeLutra, a framework that leverages both correct and incorrect code attempts. Instead of using only correct solutions, CodeLutra applies iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This approach narrows the performance gap with state-of-the-art larger models without requiring massive datasets or auxiliary models. For instance, on a challenging data science coding task, using only 500 samples improved Llama-3-8Bâ€™s accuracy from 28.2% to 48.6%, approaching GPT-4â€™s level. By learning from both successes and mistakes, CodeLutra provides a scalable and efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å½»åº•æ”¹å˜äº†ä»£ç ç”Ÿæˆçš„é¢è²Œï¼Œä½†å®ƒä»¬éœ€è¦å·¨å¤§çš„èµ„æºï¼Œå¹¶ä¸”ç»å¸¸è¿‡åº¦æ³›åŒ–ï¼Œé™åˆ¶äº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ•ˆç‡ã€‚å¾®è°ƒè¾ƒå°ã€å¼€æºçš„LLMsæä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„ç›‘ç£æ–¹æ³•åªä¾èµ–äºæ­£ç¡®çš„ä¾‹å­ï¼Œå¿½ç•¥äº†å¤±è´¥ä¸­å®è´µçš„è§è§£ã€‚æˆ‘ä»¬å¼•å…¥äº†CodeLutraæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ­£ç¡®çš„å’Œé”™è¯¯çš„ä»£ç å°è¯•ã€‚CodeLutraå¹¶ä¸åªä½¿ç”¨æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯é‡‡ç”¨åŸºäºåå¥½çš„è¿­ä»£ä¼˜åŒ–ï¼Œæ¯”è¾ƒæˆåŠŸå’Œå¤±è´¥çš„è¾“å‡ºæ¥æ›´å¥½åœ°è¿‘ä¼¼æœŸæœ›çš„ç»“æœã€‚è¿™ç§æ–¹æ³•åœ¨ä¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†æˆ–è¾…åŠ©æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œç¼©å°äº†ä¸æœ€æ–°å…ˆè¿›å¤§å‹æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®ç§‘å­¦ç¼–ç ä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨500ä¸ªæ ·æœ¬å°±æé«˜äº†Llama-3-8Bçš„å‡†ç¡®ç‡ï¼Œä»28.2%æé«˜åˆ°48.6%ï¼Œæ¥è¿‘GPT-4çš„æ°´å¹³ã€‚é€šè¿‡ä»æˆåŠŸå’Œé”™è¯¯ä¸­å­¦ä¹ ï¼ŒCodeLutraä¸ºé«˜è´¨é‡ä»£ç ç”Ÿæˆæä¾›äº†ä¸€æ¡å¯æ‰©å±•å’Œé«˜æ•ˆçš„è·¯å¾„ï¼Œä½¿å¾—è¾ƒå°çš„å¼€æºæ¨¡å‹ä¸é¢†å…ˆçš„é—­æºæ›¿ä»£å“æ›´å…·ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05199v2">PDF</a> 16 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>CodeLutraæ¡†æ¶æ”¹å˜äº†ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆæ–¹å¼ã€‚å®ƒé€šè¿‡ç»“åˆæ­£ç¡®å’Œé”™è¯¯çš„ä»£ç å°è¯•ï¼Œåˆ©ç”¨åå¥½åŸºç¡€çš„è¿­ä»£ä¼˜åŒ–æ–¹æ³•ï¼Œç¼©å°äº†ä¸é¡¶å°–å¤§å‹æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚CodeLutraä½¿å¾—å°å‹å¼€æºæ¨¡å‹ä¹Ÿèƒ½ç”Ÿæˆé«˜è´¨é‡ä»£ç ï¼Œæé«˜äº†ä»»åŠ¡ç‰¹å®šæ•ˆç‡ï¼ŒåŒæ—¶é™ä½äº†æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CodeLutraåˆ©ç”¨æ­£ç¡®å’Œé”™è¯¯çš„ä»£ç å°è¯•ï¼Œæé«˜äº†ä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>CodeLutraé€šè¿‡æ¯”è¾ƒæˆåŠŸå’Œå¤±è´¥çš„è¾“å‡ºï¼Œæ›´å¥½åœ°è¿‘ä¼¼æœŸæœ›ç»“æœã€‚</li>
<li>CodeLutraèƒ½å¤Ÿç¼©å°å°å‹å¼€æºæ¨¡å‹ä¸é¡¶å°–å¤§å‹æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
<li>CodeLutraæé«˜äº†å°å‹å¼€æºæ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šæ•ˆç‡ã€‚</li>
<li>CodeLutraé™ä½äº†ä»£ç ç”Ÿæˆçš„æˆæœ¬ï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨èµ„æºã€‚</li>
<li>CodeLutraåœ¨ä»…ä½¿ç”¨500ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½æ˜¾è‘—æé«˜LLama-3-8Bæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51622908dfd1088bcc83b49721f6b855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e4f1b0ba9a0ff8e8b6e3f6fc891e9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cc729737df900fb38e8b36a15260a68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d759c4738fd132fbc28570c43f0b4d9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e52f299d172a0c3ea8966cf353feefcd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e80b19e2c7dc8e34c906e815957f9ebe.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-20  CAD-Recode Reverse Engineering CAD Code from Point Clouds
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
