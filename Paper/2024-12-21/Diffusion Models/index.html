<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  LeviTor 3D Trajectory Oriented Image-to-Video Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-750202e1a98a34f8ad6157aefde1aebd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-21-æ›´æ–°"><a href="#2024-12-21-æ›´æ–°" class="headerlink" title="2024-12-21 æ›´æ–°"></a>2024-12-21 æ›´æ–°</h1><h2 id="LeviTor-3D-Trajectory-Oriented-Image-to-Video-Synthesis"><a href="#LeviTor-3D-Trajectory-Oriented-Image-to-Video-Synthesis" class="headerlink" title="LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis"></a>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</h2><p><strong>Authors:Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang</strong></p>
<p>The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: <a target="_blank" rel="noopener" href="https://ppetrichor.github.io/levitor.github.io/">https://ppetrichor.github.io/levitor.github.io/</a> </p>
<blockquote>
<p>åŸºäºæ‹–åŠ¨çš„äº¤äº’æ–¹å¼çš„ç›´è§‚æ€§ä½¿å…¶åœ¨å›¾åƒåˆ°è§†é¢‘åˆæˆä¸­æ§åˆ¶ç‰©ä½“è½¨è¿¹çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œåœ¨äºŒç»´ç©ºé—´æ‰§è¡Œæ‹–åŠ¨æ“ä½œçš„ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹å¹³é¢å¤–ç§»åŠ¨æ—¶é€šå¸¸é¢ä¸´æ¨¡ç³Šæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦æ¥å¢å¼ºäº¤äº’ï¼Œå³æ·±åº¦ç»´åº¦ï¼Œä½¿ç”¨æˆ·å¯ä»¥ä¸ºè½¨è¿¹ä¸Šçš„æ¯ä¸ªç‚¹åˆ†é…ç›¸å¯¹æ·±åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–°äº¤äº’èŒƒå¼ä¸ä»…ç»§æ‰¿äº†äºŒç»´æ‹–åŠ¨çš„ä¾¿åˆ©æ€§ï¼Œè¿˜ä¿ƒè¿›äº†åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„è½¨è¿¹æ§åˆ¶ï¼Œä»è€Œæ‰©å¤§äº†åˆ›æ„èŒƒå›´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨å›¾åƒåˆ°è§†é¢‘åˆæˆä¸­è¿›è¡Œä¸‰ç»´è½¨è¿¹æ§åˆ¶çš„å¼€åˆ›æ€§æ–¹æ³•ï¼Œé€šè¿‡å°†å¯¹è±¡è’™ç‰ˆæŠ½è±¡ä¸ºå‡ ä¸ªèšç±»ç‚¹æ¥å®ç°ã€‚è¿™äº›ç‚¹ä¼´éšæ·±åº¦ä¿¡æ¯å’Œå®ä¾‹ä¿¡æ¯ï¼Œæœ€ç»ˆä½œä¸ºæ§åˆ¶ä¿¡å·è¾“å…¥åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆç§°ä¸ºLeviTorï¼‰åœ¨ç”Ÿæˆä»é™æ€å›¾åƒç”Ÿæˆçš„é€¼çœŸè§†é¢‘æ—¶ç²¾ç¡®æ§åˆ¶ç‰©ä½“è¿åŠ¨çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ppetrichor.github.io/levitor.github.io/">https://ppetrichor.github.io/levitor.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15214v1">PDF</a> Project page available at   <a target="_blank" rel="noopener" href="https://ppetrichor.github.io/levitor.github.io/">https://ppetrichor.github.io/levitor.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>é‡‡ç”¨æ‹–åŠ¨æ‰‹åŠ¿äº¤äº’ï¼Œç”¨æˆ·èƒ½å¤Ÿä¸ºå›¾åƒåˆæˆè§†é¢‘ä¸­å¯¹è±¡çš„è¿åŠ¨è½¨è¿¹æŒ‡å®šç›¸å¯¹æ·±åº¦ä¿¡æ¯ï¼Œçªç ´äº†åŸæœ‰äºŒç»´æ‹–åŠ¨æ–¹æ³•çš„å±€é™æ€§ï¼Œæ‹“å®½äº†å›¾åƒåˆ°è§†é¢‘åˆæˆçš„åˆ›é€ åŠ›èŒƒç•´ã€‚LeviToræ–¹æ³•é€šè¿‡æŠ½è±¡å¯¹è±¡æ©è†œä¸ºå‡ ä¸ªç°‡ç‚¹ï¼Œç»“åˆæ·±åº¦ä¿¡æ¯å’Œå®ä¾‹ä¿¡æ¯ï¼Œä½œä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ§åˆ¶ä¿¡å·ï¼Œæœ‰æ•ˆç²¾ç¡®æ“æ§å¯¹è±¡è¿åŠ¨ï¼Œç”Ÿæˆé€¼çœŸçš„è§†é¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‹–åŠ¨æ‰‹åŠ¿äº¤äº’åœ¨å›¾åƒåˆ°è§†é¢‘åˆæˆä¸­å¹¿æ³›åº”ç”¨äºæ§åˆ¶å¯¹è±¡è½¨è¿¹ã€‚</li>
<li>åœ¨å¤„ç†å‡ºå¹³é¢ç§»åŠ¨æ—¶ï¼Œç°æœ‰äºŒç»´ç©ºé—´æ‹–åŠ¨æ–¹æ³•é¢ä¸´æ¨¡ç³Šæ€§æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ·±åº¦ç»´åº¦ä¿¡æ¯ï¼Œç”¨æˆ·å¯ä»¥ä¸ºè½¨è¿¹ä¸Šçš„æ¯ä¸ªç‚¹æŒ‡å®šç›¸å¯¹æ·±åº¦ã€‚</li>
<li>æ–°é¢–çš„äº¤äº’æ¨¡å¼ç»§æ‰¿äº†äºŒç»´æ‹–åŠ¨çš„ä¾¿æ·æ€§ï¼Œå¹¶ä¿ƒè¿›äº†ä¸‰ç»´ç©ºé—´ä¸­çš„è½¨è¿¹æ§åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºLeviTorçš„æ–¹æ³•ï¼Œé€šè¿‡æŠ½è±¡å¯¹è±¡æ©è†œä¸ºå‡ ä¸ªç°‡ç‚¹è¿›è¡Œä¸‰ç»´è½¨è¿¹æ§åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ·±åº¦ä¿¡æ¯å’Œå®ä¾‹ä¿¡æ¯ï¼Œæé«˜äº†ç²¾ç¡®æ“æ§å¯¹è±¡è¿åŠ¨çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-918cd2a01cde1ccf70eb84368563135b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f7b2587b9c41e443b5edd7fe948d58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0166670a70a57ad0673a0faec21cce21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2b87dd0c32b8518ed0cbabcad25315.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca83068fa1595d950978c819f177930b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Flowing-from-Words-to-Pixels-A-Framework-for-Cross-Modality-Evolution"><a href="#Flowing-from-Words-to-Pixels-A-Framework-for-Cross-Modality-Evolution" class="headerlink" title="Flowing from Words to Pixels: A Framework for Cross-Modality Evolution"></a>Flowing from Words to Pixels: A Framework for Cross-Modality Evolution</h2><p><strong>Authors:Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh</strong></p>
<p>Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal &#x2F; intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åŠå…¶æ³›åŒ–å½¢å¼â€”â€”æµåŒ¹é…ï¼Œåœ¨åª’ä½“ç”Ÿæˆé¢†åŸŸäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚åœ¨è¿™é‡Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•æ˜¯å­¦ä¹ ä»ç®€å•çš„é«˜æ–¯å™ªå£°æºåˆ†å¸ƒåˆ°ç›®æ ‡åª’ä½“åˆ†å¸ƒçš„å¤æ‚æ˜ å°„ã€‚å¯¹äºè·¨æ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼‰ï¼Œåœ¨æ¨¡å‹ä¸­å¼•å…¥æ¡ä»¶æœºåˆ¶çš„åŒæ—¶ï¼Œä¹Ÿå­¦ä¹ ä»å™ªå£°åˆ°å›¾åƒçš„ç›¸åŒæ˜ å°„ã€‚æµåŒ¹é…çš„ä¸€ä¸ªå…³é”®ä¸”è¿„ä»Šä¸ºæ­¢ç›¸å¯¹æœªè¢«æ¢ç´¢çš„ç‰¹ç‚¹æ˜¯ï¼Œä¸æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œå®ƒä¸å—æºåˆ†å¸ƒå¿…é¡»æ˜¯å™ªå£°çš„é™åˆ¶ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªèŒƒå¼è½¬å˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦èƒ½è®­ç»ƒæµåŒ¹é…æ¨¡å‹æ¥å­¦ä¹ ä»ä¸€ç§æ¨¡æ€çš„åˆ†å¸ƒåˆ°å¦ä¸€ç§æ¨¡æ€çš„åˆ†å¸ƒçš„ç›´æ¥æ˜ å°„ï¼Œä»è€Œé¿å…å¯¹å™ªå£°åˆ†å¸ƒå’Œæ¡ä»¶æœºåˆ¶çš„éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨ä¸”ç®€å•çš„è·¨æ¨¡æ€æµåŒ¹é…æ¡†æ¶CrossFlowã€‚æˆ‘ä»¬å¼ºè°ƒäº†å°†å˜ç¼–ç å™¨åº”ç”¨äºè¾“å…¥æ•°æ®çš„é‡è¦æ€§ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§å®ç°æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹æ³•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¯¹äºæ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ï¼Œä½¿ç”¨æ²¡æœ‰äº¤å‰æ³¨æ„åŠ›çš„æ™®é€šå˜å‹å™¨çš„CrossFlowç•¥å¾®ä¼˜äºæ ‡å‡†æµåŒ¹é…ï¼Œæˆ‘ä»¬è¯æ˜å®ƒåœ¨è®­ç»ƒæ­¥éª¤å’Œæ¨¡å‹è§„æ¨¡ä¸Šè¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶å…è®¸æœ‰è¶£çš„æ½œåœ¨è¿ç®—ï¼Œä»è€Œåœ¨è¾“å‡ºç©ºé—´äº§ç”Ÿè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç¼–è¾‘ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€èˆ¬æ€§ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜CrossFlowåœ¨å„ç§è·¨æ¨¡æ€&#x2F;åŒæ¨¡æ€æ˜ å°„ä»»åŠ¡ä¸Šè¡¨ç°ä¸æœ€æ–°æŠ€æœ¯ä¸ç›¸ä¸Šä¸‹æˆ–è¡¨ç°æ›´å¥½ï¼Œå¦‚å›¾åƒæè¿°ã€æ·±åº¦ä¼°è®¡å’Œå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡èƒ½ä¿ƒè¿›è·¨æ¨¡æ€åª’ä½“ç”Ÿæˆçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15213v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cross-flow.github.io/">https://cross-flow.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åŠå…¶æ³›åŒ–â€”â€”æµåŒ¹é…åœ¨åª’ä½“ç”Ÿæˆé¢†åŸŸäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚æœ¬æ–‡çªç ´äº†ä¼ ç»Ÿæ–¹æ³•ï¼Œä¸å†å±€é™äºä»ç®€å•çš„é«˜æ–¯å™ªå£°æºåˆ†å¸ƒåˆ°ç›®æ ‡åª’ä½“åˆ†å¸ƒçš„å­¦ä¹ æ˜ å°„ã€‚é’ˆå¯¹è·¨æ¨¡æ€ä»»åŠ¡å¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå³æ˜¯å¦å¯ä»¥ç›´æ¥è®­ç»ƒæµåŒ¹é…æ¨¡å‹ä»ä¸€ç§æ¨¡æ€çš„åˆ†å¸ƒå­¦ä¹ åˆ°å¦ä¸€ç§æ¨¡æ€çš„åˆ†å¸ƒï¼Œè€Œæ— éœ€å™ªå£°åˆ†å¸ƒå’Œè°ƒèŠ‚æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„ç®€å•æ¡†æ¶CrossFlowè¿›è¡Œè·¨æ¨¡æ€æµåŒ¹é…ï¼Œå¹¶å¼ºè°ƒäº†åº”ç”¨å˜åˆ†ç¼–ç å™¨äºè¾“å…¥æ•°æ®çš„é‡è¦æ€§ï¼Œå¼•å…¥äº†ä¸€ç§æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¯¹äºæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ï¼Œä½¿ç”¨å¸¸è§„Transformerçš„CrossFlowåœ¨ä¸ä½¿ç”¨äº¤å‰æ³¨æ„çš„æƒ…å†µä¸‹ç•¥å¾®ä¼˜äºæ ‡å‡†æµåŒ¹é…ï¼Œä¸”éšç€è®­ç»ƒæ­¥éª¤å’Œæ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œè¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸æœ‰è¶£çš„æ½œåœ¨ç®—æœ¯è¿ç®—ï¼Œåœ¨è¾“å‡ºç©ºé—´ä¸­äº§ç”Ÿè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç¼–è¾‘ã€‚ä¸ºè¯æ˜æˆ‘ä»¬æ–¹æ³•çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†CrossFlowåœ¨å„ç§è·¨æ¨¡æ€&#x2F;å•æ¨¡æ€æ˜ å°„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æˆ–ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ï¼Œå¦‚å›¾åƒæè¿°ã€æ·±åº¦ä¼°è®¡å’Œå›¾åƒè¶…åˆ†è¾¨ç‡ç­‰ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºè·¨æ¨¡æ€åª’ä½“ç”Ÿæˆçš„å‘å±•åŠ é€Ÿåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…åœ¨åª’ä½“ç”Ÿæˆé¢†åŸŸå…·æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”CrossFlowæ¡†æ¶è¿›è¡Œè·¨æ¨¡æ€æµåŒ¹é…ã€‚</li>
<li>CrossFlowçªç ´äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„é™åˆ¶ï¼Œå¯ä»¥ç›´æ¥å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ˜ å°„ï¼Œæ— éœ€å™ªå£°åˆ†å¸ƒå’Œè°ƒèŠ‚æœºåˆ¶ã€‚</li>
<li>å˜åˆ†ç¼–ç å™¨åœ¨è¾“å…¥æ•°æ®ä¸­çš„åº”ç”¨å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>CrossFlowå…·æœ‰æ— åˆ†ç±»å™¨å¼•å¯¼çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸Šï¼ŒCrossFlowçš„è¡¨ç°ç•¥ä¼˜äºæ ‡å‡†æµåŒ¹é…ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚</li>
<li>CrossFlowåœ¨å„ç§è·¨æ¨¡æ€å’Œå•æ¨¡æ€æ˜ å°„ä»»åŠ¡ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›æˆ–ä¼˜äºå½“å‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e040faeb01cb2a62876b64ee7c397abd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f838c057d689c4e5dd2b0ac92f6aa99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d9d7f6a2a700a42db59e1ed3e3d2e1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1b2da9c68466911298144044693595d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-Multiview-Relighting-for-3D-Reconstruction-under-Extreme-Illumination-Variation"><a href="#Generative-Multiview-Relighting-for-3D-Reconstruction-under-Extreme-Illumination-Variation" class="headerlink" title="Generative Multiview Relighting for 3D Reconstruction under Extreme   Illumination Variation"></a>Generative Multiview Relighting for 3D Reconstruction under Extreme   Illumination Variation</h2><p><strong>Authors:Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin</strong></p>
<p>Reconstructing the geometry and appearance of objects from photographs taken in different environments is difficult as the illumination and therefore the object appearance vary across captured images. This is particularly challenging for more specular objects whose appearance strongly depends on the viewing direction. Some prior approaches model appearance variation across images using a per-image embedding vector, while others use physically-based rendering to recover the materials and per-image illumination. Such approaches fail at faithfully recovering view-dependent appearance given significant variation in input illumination and tend to produce mostly diffuse results. We present an approach that reconstructs objects from images taken under different illuminations by first relighting the images under a single reference illumination with a multiview relighting diffusion model and then reconstructing the objectâ€™s geometry and appearance with a radiance field architecture that is robust to the small remaining inconsistencies among the relit images. We validate our proposed approach on both synthetic and real datasets and demonstrate that it greatly outperforms existing techniques at reconstructing high-fidelity appearance from images taken under extreme illumination variation. Moreover, our approach is particularly effective at recovering view-dependent â€œshinyâ€ appearance which cannot be reconstructed by prior methods. </p>
<blockquote>
<p>ä»åœ¨ä¸åŒç¯å¢ƒä¸‹æ‹æ‘„çš„ç…§ç‰‡é‡å»ºç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œå› ä¸ºå…‰ç…§å’Œå› æ­¤ç‰©ä½“çš„å¤–è§‚åœ¨ä¸åŒæ•è·çš„å›¾åƒä¸­ä¼šæœ‰æ‰€å˜åŒ–ã€‚è¿™å¯¹äºå…·æœ‰å¼ºçƒˆæ–¹å‘æ€§å¤–è§‚çš„ç‰©ä½“æ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸€äº›å…ˆå‰çš„æ–¹æ³•ä½¿ç”¨æ¯å¼ å›¾åƒçš„åµŒå…¥å‘é‡å¯¹å›¾åƒä¸­çš„å¤–è§‚å˜åŒ–è¿›è¡Œå»ºæ¨¡ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™ä½¿ç”¨åŸºäºç‰©ç†çš„æ¸²æŸ“æ¥æ¢å¤ææ–™å’Œæ¯å¼ å›¾åƒçš„å…‰ç…§ã€‚è¿™äº›æ–¹æ³•åœ¨è¾“å…¥å…‰ç…§å­˜åœ¨æ˜¾è‘—å·®å¼‚çš„æƒ…å†µä¸‹æ— æ³•å¿ å®æ¢å¤ä¸è§†å›¾ç›¸å…³çš„å¤–è§‚ï¼Œå¹¶ä¸”å¾€å¾€äº§ç”Ÿå¤§éƒ¨åˆ†æ‰©æ•£ç»“æœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡é¦–å…ˆä½¿ç”¨å¤šè§†è§’é‡æ–°ç…§æ˜æ‰©æ•£æ¨¡å‹åœ¨å•ä¸€å‚è€ƒå…‰ç…§ä¸‹é‡æ–°ç…§æ˜å›¾åƒï¼Œç„¶åä»ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„çš„ç…§ç‰‡é‡å»ºç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§å¯¹é‡æ–°ç…§æ˜å›¾åƒä¹‹é—´å‰©ä½™çš„å°ä¸ä¸€è‡´æ€§å…·æœ‰é²æ£’æ€§çš„è¾å°„åœºæ¶æ„æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜å®ƒåœ¨ä»æç«¯å…‰ç…§å˜åŒ–æ¡ä»¶ä¸‹æ‹æ‘„çš„ç…§ç‰‡é‡å»ºé«˜ä¿çœŸå¤–è§‚æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¢å¤æ— æ³•ç”±å…ˆå‰æ–¹æ³•é‡å»ºçš„ä¸è§†å›¾ç›¸å…³çš„â€œé—ªäº®â€å¤–è§‚æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15211v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://relight-to-reconstruct.github.io/">https://relight-to-reconstruct.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æè¿°äº†ä¸€ç§åˆ©ç”¨å¤šè§†è§’é‡å…‰ç…§æ‰©æ•£æ¨¡å‹ï¼ˆmultiview relighting diffusion modelï¼‰ä»ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„çš„ç…§ç‰‡é‡å»ºç‰©ä½“å‡ ä½•å’Œå¤–è§‚çš„æ–¹æ³•ã€‚é€šè¿‡é¦–å…ˆåœ¨å•ä¸€å‚è€ƒå…‰ç…§ä¸‹é‡æ–°ç…§æ˜å›¾åƒï¼Œç„¶åé‡‡ç”¨è¾å°„åœºæ¶æ„ï¼ˆradiance field architectureï¼‰æ¢å¤ç‰©ä½“å‡ ä½•å’Œå¤–è§‚ï¼Œæé«˜äº†åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„ç‰©ä½“å›¾åƒçš„é«˜ä¿çœŸé‡å»ºèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºâ€œé•œé¢å…‰æ³½â€å¤–è§‚çš„é‡å»ºæ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§ä»ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„ç…§ç‰‡é‡å»ºç‰©ä½“å‡ ä½•å’Œå¤–è§‚çš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åŸºäºå¤šè§†è§’é‡å…‰ç…§æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡æ–°ç…§æ˜ã€‚</li>
<li>é€šè¿‡åœ¨å•ä¸€å‚è€ƒå…‰ç…§ä¸‹é‡æ–°ç…§æ˜å›¾åƒï¼Œå‡å°‘äº†å…‰ç…§å˜åŒ–å¯¹ç‰©ä½“å¤–è§‚é‡å»ºçš„å½±å“ã€‚</li>
<li>é‡‡ç”¨è¾å°„åœºæ¶æ„æ¢å¤ç‰©ä½“å‡ ä½•å’Œå¤–è§‚ï¼Œå¢å¼ºäº†é‡å»ºç»“æœçš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹æç«¯å…‰ç…§å˜åŒ–ä¸‹çš„å›¾åƒé‡å»ºæ•ˆæœæ˜¾è‘—æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºé‡å»ºâ€œé•œé¢å…‰æ³½â€å¤–è§‚ï¼Œè¿™æ˜¯å…ˆå‰æ–¹æ³•æ— æ³•å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f52fe6a1a861a9700748ab9899a4438.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f265a88d8d882d78493ebada027aa56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd92031639e05668efb6a0d04bc83dd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b96613d67d132d999ae62350d2916e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2636fc6037dca8f4086a136162415fb1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Jet-A-Modern-Transformer-Based-Normalizing-Flow"><a href="#Jet-A-Modern-Transformer-Based-Normalizing-Flow" class="headerlink" title="Jet: A Modern Transformer-Based Normalizing Flow"></a>Jet: A Modern Transformer-Based Normalizing Flow</h2><p><strong>Authors:Alexander Kolesnikov, AndrÃ© Susano Pinto, Michael Tschannen</strong></p>
<p>In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models. </p>
<blockquote>
<p>è¿‡å»ï¼Œæ ‡å‡†åŒ–ç”Ÿæˆæµå·²ç»æ¶Œç°ä¸ºä¸€ç±»æœ‰å‰æ™¯çš„è‡ªç„¶å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¿™ç§æ¨¡å‹å…·æœ‰è®¸å¤šå»ºæ¨¡ä¼˜åŠ¿ï¼šèƒ½å¤Ÿé«˜æ•ˆåœ°è®¡ç®—è¾“å…¥æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ï¼Œç”Ÿæˆé€Ÿåº¦å¿«ï¼Œæ•´ä½“ç»“æ„ç®€å•ã€‚æ ‡å‡†åŒ–æµä¸€ç›´æ˜¯æ´»è·ƒç ”ç©¶çš„ä¸»é¢˜ï¼Œä½†åæ¥å› å…¶æ ·æœ¬çš„è§†è§‰è´¨é‡ä¸å…¶ä»–æ¨¡å‹ç±»åˆ«ç›¸æ¯”ä¸å…·ç«äº‰åŠ›è€Œä¸å†å—æ¬¢è¿ï¼Œå¦‚GANã€åŸºäºVQ-VAEçš„æ–¹æ³•æˆ–æ‰©æ•£æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†åŸºäºè€¦åˆçš„æ ‡å‡†åŒ–æµæ¨¡å‹çš„è®¾è®¡ï¼Œé€šè¿‡ä»”ç»†æ¶ˆé™¤å…ˆå‰çš„è®¾è®¡é€‰æ‹©å¹¶ä½¿ç”¨åŸºäºè§†è§‰è½¬æ¢å™¨æ¶æ„çš„è®¡ç®—å—ï¼ˆè€Œä¸æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‡­å€Ÿæ›´ç®€å•çš„æ¶æ„å®ç°äº†æœ€å…ˆè¿›çš„å®šé‡å’Œå®šæ€§æ€§èƒ½ã€‚è™½ç„¶æ•´ä½“è§†è§‰è´¨é‡ä»ç„¶è½åäºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæˆ‘ä»¬è®¤ä¸ºå¼ºå¤§çš„æ ‡å‡†åŒ–æµæ¨¡å‹å¯ä»¥ä½œä¸ºæ›´å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹çš„æ„å»ºç»„ä»¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨ç ”ç©¶å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15129v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é‡æ–°å®¡è§†äº†åŸºäºè€¦åˆçš„å½’ä¸€åŒ–æµæ¨¡å‹çš„è®¾è®¡ï¼Œé€šè¿‡å¯¹å…ˆå‰è®¾è®¡é€‰æ‹©è¿›è¡Œä»”ç»†æ¶ˆèï¼Œå¹¶ä½¿ç”¨åŸºäºVision Transformeræ¶æ„çš„è®¡ç®—å—ï¼Œè€Œéå·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ç°äº†å…ˆè¿›çš„è´¨é‡å’Œæ€§èƒ½ã€‚è™½ç„¶æ•´ä½“è§†è§‰è´¨é‡ä»è½åäºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºå¼ºå¤§çš„å½’ä¸€åŒ–æµæ¨¡å‹å¯ä»¥ä½œä¸ºæ›´å¼ºå¤§ç”Ÿæˆæ¨¡å‹çš„æ„å»ºç»„ä»¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨ç ”ç©¶å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½’ä¸€åŒ–æµæ¨¡å‹æ˜¯ä¸€ç§å…·æœ‰å¤šä¸ªå»ºæ¨¡ä¼˜åŠ¿çš„ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬è®¡ç®—è¾“å…¥æ•°æ®å¯¹æ•°ä¼¼ç„¶çš„é«˜æ•ˆæ€§ã€å¿«é€Ÿç”Ÿæˆå’Œç®€å•ç»“æ„ã€‚</li>
<li>ä¹‹å‰çš„å½’ä¸€åŒ–æµæ¨¡å‹åœ¨æ ·æœ¬çš„è§†è§‰è´¨é‡ä¸Šä¸å…¶ä»–æ¨¡å‹ç±»ç›¸æ¯”ä¸å…·ç«äº‰åŠ›ï¼Œå¦‚GANsã€VQ-VAEæ–¹æ³•å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡é‡æ–°å®¡è§†åŸºäºè€¦åˆçš„å½’ä¸€åŒ–æµæ¨¡å‹çš„è®¾è®¡ï¼Œä½¿ç”¨Vision Transformeræ¶æ„çš„è®¡ç®—å—æ›¿ä»£å·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ç°äº†å…ˆè¿›çš„è´¨é‡å’Œæ€§èƒ½ã€‚</li>
<li>å°½ç®¡æ•´ä½“è§†è§‰è´¨é‡ä»å¾…æé«˜ï¼Œä½†ä½œè€…è®¤ä¸ºå¼ºå¤§çš„å½’ä¸€åŒ–æµæ¨¡å‹å¯ä»¥ä½œä¸ºæ„å»ºæ›´å¼ºå¤§ç”Ÿæˆæ¨¡å‹çš„ç»„ä»¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨ç ”ç©¶å‰æ²¿ã€‚</li>
<li>æ¶ˆèå…ˆå‰çš„è®¾è®¡é€‰æ‹©æ˜¯æ”¹è¿›å½’ä¸€åŒ–æµæ¨¡å‹çš„å…³é”®æ­¥éª¤ä¹‹ä¸€ã€‚</li>
<li>ä½¿ç”¨åŸºäºVision Transformerçš„è®¡ç®—å—åœ¨å½’ä¸€åŒ–æµæ¨¡å‹ä¸­äº§ç”Ÿäº†ç§¯æçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11360f92701d5b6c72bbfc37e5461247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb05969f79f097db1ff2bf24cb65f840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c239ce6bc86d40cec105998ab124cd08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-248c7a67d0d6e4e31f776a83a7e5ba0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b371b3a81df48fcd55c960d1e4c6b92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1613bb08e98c13c95a34f9af5e8d9b9d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space"><a href="#DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space" class="headerlink" title="DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space"></a>DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space</h2><p><strong>Authors:Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin BeneÅ¡, Albert Ali Salah, Itir Onal Ertugrul</strong></p>
<p>This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to high-resolution generation without using the latent diffusion paradigm. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why &#96;image diffusion can be seen as spectral autoregressionâ€™, bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is at \url{<a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff%7D">https://github.com/forever208/DCTdiff}</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†é¢‘ç‡ç©ºé—´çš„å›¾åƒå»ºæ¨¡ï¼Œå¹¶ä»‹ç»äº†DCTdiffè¿™ä¸€ç«¯åˆ°ç«¯çš„æ‰©æ•£ç”ŸæˆèŒƒå¼ï¼Œè¯¥èŒƒå¼åœ¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç©ºé—´æœ‰æ•ˆåœ°å»ºæ¨¡å›¾åƒã€‚æˆ‘ä»¬ç ”ç©¶äº†DCTdiffçš„è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºäº†å…³é”®çš„è®¾è®¡å› ç´ ã€‚åœ¨ä¸åŒæ¡†æ¶ï¼ˆUViTã€DiTï¼‰ã€ç”Ÿæˆä»»åŠ¡å’Œå¤šç§æ‰©æ•£é‡‡æ ·å™¨ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDCTdiffåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºåƒç´ çš„æ‰©æ•£æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDCTdiffå¯ä»¥æ— ç¼åœ°æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼Œè€Œæ— éœ€ä½¿ç”¨æ½œåœ¨æ‰©æ•£èŒƒå¼ã€‚æœ€åï¼Œæˆ‘ä»¬è¯´æ˜äº†DCTå›¾åƒå»ºæ¨¡çš„å‡ ä¸ªæœ‰è¶£ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æä¾›äº†ç†è®ºè¯æ˜ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆâ€œå›¾åƒæ‰©æ•£å¯ä»¥è¢«è§†ä¸ºè°±è‡ªå›å½’â€ï¼Œä»è€Œç¼©å°äº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚DCTdiffçš„æœ‰æ•ˆæ€§å’Œæ‰€ä»‹ç»çš„ç‰¹æ€§è¡¨æ˜ï¼Œé¢‘ç‡ç©ºé—´çš„å›¾åƒå»ºæ¨¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚ä»£ç ä½äº\url{<a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff%7D%E3%80%82">https://github.com/forever208/DCTdiff}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15032v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DCTdiffï¼Œè¿™æ˜¯ä¸€ç§åœ¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç©ºé—´ä¸­è¿›è¡Œå›¾åƒå»ºæ¨¡çš„ç«¯åˆ°ç«¯æ‰©æ•£ç”ŸæˆèŒƒå¼ã€‚æ–‡ç« æ¢è®¨äº†DCTdiffçš„è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºäº†å…³é”®è®¾è®¡å› ç´ ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶åœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ä¸Šä¼˜äºåŸºäºåƒç´ çš„æ‰©æ•£æ¨¡å‹ã€‚DCTdiffèƒ½å¤Ÿæ— ç¼åœ°æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼Œè€Œæ— éœ€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜é˜è¿°äº†DCTå›¾åƒå»ºæ¨¡çš„å‡ ä¸ªæœ‰è¶£ç‰¹æ€§ï¼Œå¦‚ä»ç†è®ºä¸Šè¯æ˜äº†â€œå›¾åƒæ‰©æ•£å¯ä»¥è¢«è§†ä¸ºè°±è‡ªå›å½’â€ï¼Œä¸ºæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ä¹‹é—´å»ºç«‹äº†è”ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCTdiffæ˜¯ä¸€ç§åœ¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç©ºé—´è¿›è¡Œå›¾åƒå»ºæ¨¡çš„æ‰©æ•£ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>DCTdiffçš„è®¾è®¡ç©ºé—´åŒ…æ‹¬å…³é”®è®¾è®¡å› ç´ ï¼Œè¿™äº›å› ç´ å¯¹äºæé«˜ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>DCTdiffåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ä¸Šä¼˜äºåŸºäºåƒç´ çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>DCTdiffèƒ½å¤Ÿæ— ç¼æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚</li>
<li>DCTdiffä¸ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ–¹æ³•ã€‚</li>
<li>æ–‡ç« ä»ç†è®ºä¸Šè¯æ˜äº†â€œå›¾åƒæ‰©æ•£å¯ä»¥è¢«è§†ä¸ºè°±è‡ªå›å½’â€ï¼Œè¿™æ˜¯æ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„ä¸€ä¸ªé‡è¦è”ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0a05d4c2a356d0f975da60371dafc30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fae6abffbea645dc4cb970676696d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4de03d9b84fbf2f542072fb216860b3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MagicNaming-Consistent-Identity-Generation-by-Finding-a-â€œName-Spaceâ€-in-T2I-Diffusion-Models"><a href="#MagicNaming-Consistent-Identity-Generation-by-Finding-a-â€œName-Spaceâ€-in-T2I-Diffusion-Models" class="headerlink" title="MagicNaming: Consistent Identity Generation by Finding a â€œName Spaceâ€ in   T2I Diffusion Models"></a>MagicNaming: Consistent Identity Generation by Finding a â€œName Spaceâ€ in   T2I Diffusion Models</h2><p><strong>Authors:Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang</strong></p>
<p>Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable of generating famous persons by simply referring to their names. Is it possible to make such models generate generic identities as simple as the famous ones, e.g., just use a name? In this paper, we explore the existence of a â€œName Spaceâ€, where any point in the space corresponds to a specific identity. Fortunately, we find some clues in the feature space spanned by text embedding of celebritiesâ€™ names. Specifically, we first extract the embeddings of celebritiesâ€™ names in the Laion5B dataset with the text encoder of diffusion models. Such embeddings are used as supervision to learn an encoder that can predict the name (actually an embedding) of a given face image. We experimentally find that such name embeddings work well in promising the generated image with good identity consistency. Note that like the names of celebrities, our predicted name embeddings are disentangled from the semantics of text inputs, making the original generation capability of text-to-image models well-preserved. Moreover, by simply plugging such name embeddings, all variants (e.g., from Civitai) derived from the same base model (i.e., SDXL) readily become identity-aware text-to-image models. Project homepage: \url{<a target="_blank" rel="noopener" href="https://magicfusion.github.io/MagicNaming/%7D">https://magicfusion.github.io/MagicNaming/}</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚DALL-Eã€SDXLï¼‰èƒ½å¤Ÿé€šè¿‡ç®€å•åœ°æåŠåå­—ç”Ÿæˆåäººå›¾åƒã€‚æ˜¯å¦å¯èƒ½ä½¿è¿™ç±»æ¨¡å‹ç”Ÿæˆåƒåäººä¸€æ ·ç®€å•çš„æ™®é€šèº«ä»½å‘¢ï¼Ÿä¾‹å¦‚ï¼Œä»…ä»…ä½¿ç”¨ä¸€ä¸ªåå­—ï¼Ÿåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†â€œåç§°ç©ºé—´â€çš„å­˜åœ¨ï¼Œè¯¥ç©ºé—´ä¸­çš„æ¯ä¸ªç‚¹éƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„èº«ä»½ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ç”±åäººåå­—çš„æ–‡æœ¬åµŒå…¥æ‰€æ„æˆçš„ç‰¹å¾ç©ºé—´ä¸­æ‰¾åˆ°äº†ä¸€äº›çº¿ç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨æå–Laion5Bæ•°æ®é›†çš„åäººåå­—çš„åµŒå…¥ã€‚è¿™äº›åµŒå…¥ç”¨ä½œç›‘ç£å­¦ä¹ ï¼Œä»¥è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿå¯¹ç»™å®šçš„é¢éƒ¨å›¾åƒè¿›è¡Œåç§°é¢„æµ‹ï¼ˆå®é™…ä¸Šæ˜¯åµŒå…¥ï¼‰çš„ç¼–ç å™¨ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå‘ç°ï¼Œè¿™æ ·çš„åç§°åµŒå…¥åœ¨ä¿æŒç”Ÿæˆçš„å›¾åƒå…·æœ‰è‰¯å¥½çš„èº«ä»½ä¸€è‡´æ€§æ–¹é¢æ•ˆæœå¾ˆå¥½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸åäººçš„åå­—ä¸€æ ·ï¼Œæˆ‘ä»¬é¢„æµ‹çš„åç§°åµŒå…¥ä¸æ–‡æœ¬è¾“å…¥çš„è¯­ä¹‰æ˜¯åˆ†å¼€çš„ï¼Œä½¿å¾—æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›å¾—åˆ°äº†å¾ˆå¥½çš„ä¿ç•™ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç®€å•åœ°æ’å…¥è¿™æ ·çš„åç§°åµŒå…¥ï¼ŒåŸºäºåŒä¸€åŸºç¡€æ¨¡å‹ï¼ˆå³SDXLï¼‰çš„æ‰€æœ‰å˜ä½“ï¼ˆä¾‹å¦‚æ¥è‡ªCivitaiï¼‰éƒ½èƒ½è½»æ˜“æˆä¸ºå…·æœ‰èº«ä»½æ„è¯†çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚é¡¹ç›®ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://magicfusion.github.io/MagicNaming/">https://magicfusion.github.io/MagicNaming/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14902v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†æ–‡æœ¬åˆ°å›¾åƒçš„å¤§å°ºåº¦æ‰©æ•£æ¨¡å‹ä¸­çš„â€œåç§°ç©ºé—´â€çš„å­˜åœ¨æ€§ã€‚ç ”ç©¶é€šè¿‡æå–Laion5Bæ•°æ®é›†ä¸­åäººåç§°çš„æ–‡æœ¬åµŒå…¥ï¼Œå­¦ä¹ äº†ä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®é¢éƒ¨å›¾åƒé¢„æµ‹åç§°çš„ç¼–ç å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›åç§°åµŒå…¥èƒ½å¤Ÿç¡®ä¿ç”Ÿæˆçš„å›¾åƒå…·æœ‰è‰¯å¥½çš„èº«ä»½ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ–¹æ³•èƒ½å¤Ÿä½¿åŒä¸€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SDXLï¼‰çš„æ‰€æœ‰å˜ä½“ï¼ˆå¦‚Civitaiï¼‰è½»æ˜“æˆä¸ºå…·æœ‰èº«ä»½è¯†åˆ«åŠŸèƒ½çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„å¤§å°ºåº¦æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆç‰¹å®šåç§°çš„å›¾åƒï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†æ˜¯å¦ä¹Ÿèƒ½ç”Ÿæˆæ™®é€šåç§°çš„å›¾åƒã€‚</li>
<li>ç ”ç©¶å‘ç°äº†â€œåç§°ç©ºé—´â€ï¼Œå…¶ä¸­ç©ºé—´çš„æ¯ä¸ªç‚¹éƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„èº«ä»½ã€‚</li>
<li>é€šè¿‡æå–Laion5Bæ•°æ®é›†ä¸­åäººåç§°çš„æ–‡æœ¬åµŒå…¥ï¼Œå­¦ä¹ äº†ä¸€ä¸ªé¢„æµ‹ç»™å®šé¢éƒ¨å›¾åƒåç§°çš„ç¼–ç å™¨ã€‚</li>
<li>åç§°åµŒå…¥åœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œä¸æ”¹å˜æ¨¡å‹çš„åŸå§‹æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åç§°åµŒå…¥æŠ€æœ¯å¯ä»¥åº”ç”¨äºåŒä¸€åŸºç¡€æ¨¡å‹çš„å˜ä½“ï¼Œä½¿å…¶æˆä¸ºå…·æœ‰èº«ä»½è¯†åˆ«åŠŸèƒ½çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¿™ç§æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b834d87b4759bce05bfe6cc44a2e2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29d7fad3c6446210891aff905765cd9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f0c2768bc886f60032f8053ffa9fe7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239bcacd81f3cbb65c621368cc818388.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations"><a href="#Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations" class="headerlink" title="Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations"></a>Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations</h2><p><strong>Authors:Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen</strong></p>
<p>Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8% increase in success rates for complex real-world dexterous manipulation tasks. </p>
<blockquote>
<p>è¿‘æœŸæœºå™¨äººæŠ€æœ¯çš„è¿›æ­¥ä¸»è¦é›†ä¸­åœ¨å¼€å‘èƒ½å¤Ÿæ‰§è¡Œå¤šç§ä»»åŠ¡çš„ä¸€èˆ¬ç­–ç•¥ä¸Šã€‚é€šå¸¸ï¼Œè¿™äº›ç­–ç•¥ä¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ¥æ•è·å½“å‰è§‚å¯Ÿä¸­çš„å…³é”®ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä»¥å‰è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸»è¦ä¾èµ–äºä¸¤å›¾åƒå¯¹æ¯”å­¦ä¹ æˆ–å•å›¾åƒé‡å»ºï¼Œæ— æ³•å®Œç¾æ•è·å¯¹å®ä½“ä»»åŠ¡è‡³å…³é‡è¦çš„åºåˆ—ä¿¡æ¯ã€‚æœ€è¿‘ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰å·²ç»å±•ç°å‡ºå‡†ç¡®é¢„æµ‹æœªæ¥å›¾åƒåºåˆ—çš„èƒ½åŠ›ï¼Œè¡¨ç°å‡ºå¯¹ç‰©ç†åŠ¨æ€çš„è‰¯å¥½ç†è§£ã€‚å—åˆ°VDMså¼ºå¤§è§†è§‰é¢„æµ‹èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾å®ƒä»¬å¤©ç”Ÿå°±å…·æœ‰åæ˜ ç‰©ç†ä¸–ç•Œæ¼”å˜çš„è§†è§‰è¡¨å¾ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºé¢„æµ‹æ€§è§†è§‰è¡¨å¾ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºäº†è§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä»¥VDMsçš„é¢„æµ‹æ€§è§†è§‰è¡¨å¾ä¸ºæ¡ä»¶çš„ä¸€èˆ¬æ€§æœºå™¨äººç­–ç•¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿™äº›è¡¨å¾ï¼Œæˆ‘ä»¬èå…¥äº†å¤šæ ·åŒ–çš„äººç±»æˆ–æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆè®­ç»ƒç›®æ ‡ã€‚VPPåœ¨ä¸¤ä¸ªæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å‡å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä¸ä¹‹å‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†28.1%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œç²¾ç»†æ“ä½œä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜äº†28.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14803v1">PDF</a> The first two authors contribute equally. Project Page at   <a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io/">https://video-prediction-policy.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰çš„é¢„æµ‹è§†è§‰è¡¨å¾ï¼Œæå‡ºè§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œç»“åˆå¤šç§æ•°æ®é›†å’Œç›®æ ‡ï¼Œæå‡æœºå™¨äººæ‰§è¡Œå¤šç§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸæœºå™¨äººæŠ€æœ¯è¿›å±•é›†ä¸­äºå¼€å‘èƒ½æ‰§è¡Œå¤šç§ä»»åŠ¡çš„é€šç”¨ç­–ç•¥ã€‚</li>
<li>é€šç”¨ç­–ç•¥å¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ•æ‰å½“å‰è§‚å¯Ÿä¿¡æ¯ã€‚</li>
<li>ä»¥å¾€çš„è§†è§‰ç¼–ç å™¨åœ¨æ•æ‰å®ä½“ä»»åŠ¡æ‰€éœ€çš„åºåˆ—ä¿¡æ¯æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½å‡†ç¡®é¢„æµ‹æœªæ¥å›¾åƒåºåˆ—ï¼Œç†è§£ç‰©ç†åŠ¨æ€ã€‚</li>
<li>VDMså…·æœ‰é¢„æµ‹è§†è§‰è¡¨å¾ï¼Œå³åæ˜ ç‰©ç†ä¸–ç•Œæ¼”å˜çš„å†…åœ¨èƒ½åŠ›ã€‚</li>
<li>åŸºäºæ­¤ï¼Œæå‡ºè§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œç»“åˆVDMsçš„é¢„æµ‹è§†è§‰è¡¨å¾ï¼Œæ„å»ºé€šç”¨æœºå™¨äººç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a0633cab27a1dab8bc8deae652cb614b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd64e25d03d7b9d8221e2740f77f2d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746a0ed8f218c569b507d8cde6f6f142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ec1419d9d6845568c183d648993c172.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Event-assisted-12-stop-HDR-Imaging-of-Dynamic-Scene"><a href="#Event-assisted-12-stop-HDR-Imaging-of-Dynamic-Scene" class="headerlink" title="Event-assisted 12-stop HDR Imaging of Dynamic Scene"></a>Event-assisted 12-stop HDR Imaging of Dynamic Scene</h2><p><strong>Authors:Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue</strong></p>
<p>High dynamic range (HDR) imaging is a crucial task in computational photography, which captures details across diverse lighting conditions. Traditional HDR fusion methods face limitations in dynamic scenes with extreme exposure differences, as aligning low dynamic range (LDR) frames becomes challenging due to motion and brightness variation. In this work, we propose a novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera system with an event camera and an RGB camera. The event camera provides temporally dense, high dynamic range signals that improve alignment between LDR frames with large exposure differences, reducing ghosting artifacts caused by motion. Also, a real-world finetuning strategy is proposed to increase the generalization of alignment module on real-world events. Additionally, we introduce a diffusion-based fusion module that incorporates image priors from pre-trained diffusion models to address artifacts in high-contrast regions and minimize errors from the alignment process. To support this work, we developed the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized event signals, and validated our approach on both simulated and real-world data. Extensive experiments demonstrate that our method achieves state-of-the-art performance, successfully extending HDR imaging to 12 stops in dynamic scenes. </p>
<blockquote>
<p>é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æˆåƒæ˜¯è®¡ç®—æ‘„å½±ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹æ•æ‰ç»†èŠ‚ã€‚ä¼ ç»Ÿçš„HDRèåˆæ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯ä¸­å­˜åœ¨æé™ï¼Œç‰¹åˆ«æ˜¯åœ¨æç«¯æ›å…‰å·®å¼‚çš„æƒ…å†µä¸‹ï¼Œç”±äºè¿åŠ¨å’Œäº®åº¦å˜åŒ–ï¼Œå¯¹é½ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å¸§å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŠ¨æ€åœºæ™¯çš„12æ¡£HDRæˆåƒæ–°æ–¹æ³•ï¼Œåˆ©ç”¨é…å¤‡äº‹ä»¶ç›¸æœºå’ŒRGBç›¸æœºçš„åŒç›¸æœºç³»ç»Ÿè¿›è¡Œå®ç°ã€‚äº‹ä»¶ç›¸æœºæä¾›æ—¶é—´å¯†é›†ã€é«˜åŠ¨æ€èŒƒå›´çš„ä¿¡å·ï¼Œæ”¹å–„äº†å…·æœ‰è¾ƒå¤§æ›å…‰å·®å¼‚çš„LDRå¸§ä¹‹é—´çš„å¯¹é½ï¼Œå‡å°‘äº†ç”±äºè¿åŠ¨é€ æˆçš„é¬¼å½±ä¼ªå½±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç°å®ä¸–ç•Œçš„å¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜å¯¹é½æ¨¡å—åœ¨ç°å®äº‹ä»¶ä¸­çš„é€šç”¨æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒå…ˆéªŒä¿¡æ¯ï¼Œä»¥è§£å†³é«˜å¯¹æ¯”åº¦åŒºåŸŸçš„ä¼ªå½±é—®é¢˜ï¼Œå¹¶æœ€å°åŒ–å¯¹é½è¿‡ç¨‹ä¸­çš„è¯¯å·®ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬å¼€å‘äº†ESHDRæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰åŒæ­¥äº‹ä»¶ä¿¡å·çš„12æ¡£HDRæˆåƒæ•°æ®é›†ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒæˆåŠŸåœ°å°†HDRæˆåƒæ‰©å±•åˆ°åŠ¨æ€çš„12æ¡£åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14705v1">PDF</a> Project page:   <a target="_blank" rel="noopener" href="https://openimaginglab.github.io/Event-Assisted-12stops-HDR/">https://openimaginglab.github.io/Event-Assisted-12stops-HDR/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŒæ‘„åƒå¤´ç³»ç»Ÿï¼ˆäº‹ä»¶æ‘„åƒå¤´å’ŒRGBæ‘„åƒå¤´ï¼‰çš„12æ¡£é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æˆåƒæ–°æ–¹æ³•ï¼Œç”¨äºåŠ¨æ€åœºæ™¯çš„HDRæˆåƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº‹ä»¶æ‘„åƒå¤´æä¾›çš„æ—¶é—´å¯†é›†ã€é«˜åŠ¨æ€èŒƒå›´çš„ä¿¡å·ï¼Œæ”¹è¿›äº†ä¸åŒæ›å…‰åº¦ä¹‹é—´çš„LDRå¸§å¯¹é½ï¼Œå‡å°‘äº†å› è¿åŠ¨äº§ç”Ÿçš„é¬¼å½±ä¼ªå½±ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„èåˆæ¨¡å—ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„å›¾åƒå…ˆéªŒä¿¡æ¯æ¥è§£å†³é«˜å¯¹æ¯”åº¦åŒºåŸŸçš„ä¼ªå½±é—®é¢˜å¹¶å‡å°‘å¯¹é½è¿‡ç¨‹ä¸­çš„è¯¯å·®ã€‚ä¸ºäº†æ”¯æŒè¯¥ç ”ç©¶ï¼Œå¼€å‘äº†åŒæ­¥äº‹ä»¶ä¿¡å·çš„ESHDRæ•°æ®é›†ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•æˆåŠŸå°†HDRæˆåƒæ‰©å±•åˆ°åŠ¨æ€åœºæ™¯çš„12æ¡£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†åŸºäºåŒæ‘„åƒå¤´ç³»ç»Ÿï¼ˆäº‹ä»¶æ‘„åƒå¤´å’ŒRGBæ‘„åƒå¤´ï¼‰çš„12æ¡£é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æˆåƒæ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨äº‹ä»¶æ‘„åƒå¤´æä¾›çš„æ—¶é—´å¯†é›†ã€é«˜åŠ¨æ€èŒƒå›´çš„ä¿¡å·æ”¹è¿›LDRå¸§å¯¹é½ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨æ‰©æ•£èåˆæ¨¡å—è§£å†³äº†é«˜å¯¹æ¯”åº¦åŒºåŸŸçš„ä¼ªå½±é—®é¢˜å¹¶å‡å°‘äº†è¯¯å·®ã€‚</li>
<li>é‡‡ç”¨åŒæ­¥äº‹ä»¶ä¿¡å·çš„ESHDRæ•°æ®é›†ç”¨äºéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ–¹æ³•æˆåŠŸå°†HDRæˆåƒæ‰©å±•åˆ°åŠ¨æ€åœºæ™¯çš„12æ¡£æ€§èƒ½è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç°å®ä¸–ç•Œçš„å¾®è°ƒç­–ç•¥ï¼Œæé«˜äº†å¯¹é½æ¨¡å—åœ¨ç°å®äº‹ä»¶ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f94c8462581460e81a710b59cd576899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-986fd87a39cad9732ad3ea5e237d3230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ccdb37ec8b303ecf0f87382f372f1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e53ee01757d4f411c3eb2f99f8aff29.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unified-Image-Restoration-and-Enhancement-Degradation-Calibrated-Cycle-Reconstruction-Diffusion-Model"><a href="#Unified-Image-Restoration-and-Enhancement-Degradation-Calibrated-Cycle-Reconstruction-Diffusion-Model" class="headerlink" title="Unified Image Restoration and Enhancement: Degradation Calibrated Cycle   Reconstruction Diffusion Model"></a>Unified Image Restoration and Enhancement: Degradation Calibrated Cycle   Reconstruction Diffusion Model</h2><p><strong>Authors:Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou</strong></p>
<p>Image restoration and enhancement are pivotal for numerous computer vision applications, yet unifying these tasks efficiently remains a significant challenge. Inspired by the iterative refinement capabilities of diffusion models, we propose CycleRDM, a novel framework designed to unify restoration and enhancement tasks while achieving high-quality mapping. Specifically, CycleRDM first learns the mapping relationships among the degraded domain, the rough normal domain, and the normal domain through a two-stage diffusion inference process. Subsequently, we transfer the final calibration process to the wavelet low-frequency domain using discrete wavelet transform, performing fine-grained calibration from a frequency domain perspective by leveraging task-specific frequency spaces. To improve restoration quality, we design a feature gain module for the decomposed wavelet high-frequency domain to eliminate redundant features. Additionally, we employ multimodal textual prompts and Fourier transform to drive stable denoising and reduce randomness during the inference process. After extensive validation, CycleRDM can be effectively generalized to a wide range of image restoration and enhancement tasks while requiring only a small number of training samples to be significantly superior on various benchmarks of reconstruction quality and perceptual quality. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/hejh8/CycleRDM">https://github.com/hejh8/CycleRDM</a>. </p>
<blockquote>
<p>å›¾åƒä¿®å¤å’Œå¢å¼ºå¯¹äºè®¸å¤šè®¡ç®—æœºè§†è§‰åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°ç»Ÿä¸€è¿™äº›ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚å—æ‰©æ•£æ¨¡å‹è¿­ä»£ç»†åŒ–èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†CycleRDMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç»Ÿä¸€ä¿®å¤å’Œå¢å¼ºä»»åŠ¡çš„åŒæ—¶å®ç°é«˜è´¨é‡æ˜ å°„çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒCycleRDMé¦–å…ˆé€šè¿‡ä¸¤é˜¶æ®µæ‰©æ•£æ¨ç†è¿‡ç¨‹å­¦ä¹ é€€åŒ–åŸŸã€ç²—ç•¥æ­£å¸¸åŸŸå’Œæ­£å¸¸åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚éšåï¼Œæˆ‘ä»¬é€šè¿‡ç¦»æ•£å°æ³¢å˜æ¢å°†æœ€ç»ˆçš„æ ¡å‡†è¿‡ç¨‹è½¬ç§»åˆ°å°æ³¢ä½é¢‘åŸŸï¼Œåˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„é¢‘ç‡ç©ºé—´ï¼Œä»é¢‘ç‡åŸŸçš„è§’åº¦è¿›è¡Œç²¾ç»†æ ¡å‡†ã€‚ä¸ºäº†æé«˜æ¢å¤è´¨é‡ï¼Œæˆ‘ä»¬ä¸ºåˆ†è§£åçš„å°æ³¢é«˜é¢‘åŸŸè®¾è®¡äº†ç‰¹å¾å¢ç›Šæ¨¡å—ï¼Œä»¥æ¶ˆé™¤å†—ä½™ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨å¤šæ¨¡å¼æ–‡æœ¬æç¤ºå’Œå‚…é‡Œå¶å˜æ¢æ¥é©±åŠ¨ç¨³å®šçš„å»å™ªï¼Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„éšæœºæ€§ã€‚ç»è¿‡å¹¿æ³›éªŒè¯ï¼ŒCycleRDMå¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°å„ç§å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ï¼Œå¹¶ä¸”åªéœ€è¦å°‘é‡è®­ç»ƒæ ·æœ¬å°±èƒ½åœ¨é‡å»ºè´¨é‡å’Œæ„ŸçŸ¥è´¨é‡çš„å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/hejh8/CycleRDM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/hejh8/CycleRDMä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¾ªç¯RDMæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¿­ä»£ä¼˜åŒ–èƒ½åŠ›ï¼Œå®ç°äº†å›¾åƒä¿®å¤ä¸å¢å¼ºçš„ç»Ÿä¸€ã€‚å®ƒé€šè¿‡ä¸¤é˜¶æ®µæ‰©æ•£æ¨ç†å­¦ä¹ é€€åŒ–åŸŸã€ç²—ç•¥æ­£å¸¸åŸŸå’Œæ­£å¸¸åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¹¶åœ¨å°æ³¢ä½é¢‘åŸŸè¿›è¡Œç²¾ç»†æ ¡å‡†ï¼ŒåŒæ—¶æ¶ˆé™¤å°æ³¢é«˜é¢‘åŸŸä¸­çš„å†—ä½™ç‰¹å¾ï¼Œæé«˜å›¾åƒä¿®å¤è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šæ¨¡å¼æ–‡æœ¬æç¤ºå’Œå‚…é‡Œå¶å˜æ¢ï¼Œä»¥å®ç°ç¨³å®šçš„å»å™ªå’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„éšæœºæ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒCycleRDMå¯å¹¿æ³›åº”ç”¨äºå¤šç§å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ï¼Œå¹¶åœ¨é‡å»ºè´¨é‡å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CycleRDMæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¿­ä»£ä¼˜åŒ–èƒ½åŠ›ï¼Œæœ‰æ•ˆç»Ÿä¸€äº†å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µæ‰©æ•£æ¨ç†ï¼Œå­¦ä¹ é€€åŒ–åŸŸã€ç²—ç•¥æ­£å¸¸åŸŸå’Œæ­£å¸¸åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚</li>
<li>åœ¨å°æ³¢ä½é¢‘åŸŸè¿›è¡Œç²¾ç»†æ ¡å‡†ï¼Œåˆ©ç”¨ä»»åŠ¡ç‰¹å®šé¢‘ç‡ç©ºé—´æé«˜ä¿®å¤è´¨é‡ã€‚</li>
<li>è®¾è®¡ç‰¹å¾å¢ç›Šæ¨¡å—ï¼Œæ¶ˆé™¤å°æ³¢é«˜é¢‘åŸŸä¸­çš„å†—ä½™ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨å¤šæ¨¡å¼æ–‡æœ¬æç¤ºå’Œå‚…é‡Œå¶å˜æ¢ï¼Œå®ç°ç¨³å®šå»å™ªï¼Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„éšæœºæ€§ã€‚</li>
<li>CycleRDMå¯å¹¿æ³›åº”ç”¨äºå¤šç§å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-531fc125da524005318142c970a1894d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7578ae45a38113ab0777eea0ce68ee45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4710a646aa48bd335d45bf0bd35b8a8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Qua-2-SeDiMo-Quantifiable-Quantization-Sensitivity-of-Diffusion-Models"><a href="#Qua-2-SeDiMo-Quantifiable-Quantization-Sensitivity-of-Diffusion-Models" class="headerlink" title="Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models"></a>Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models</h2><p><strong>Authors:Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu</strong></p>
<p>Diffusion Models (DM) have democratized AI image generation through an iterative denoising process. Quantization is a major technique to alleviate the inference cost and reduce the size of DM denoiser networks. However, as denoisers evolve from variants of convolutional U-Nets toward newer Transformer architectures, it is of growing importance to understand the quantization sensitivity of different weight layers, operations and architecture types to performance. In this work, we address this challenge with Qua$^2$SeDiMo, a mixed-precision Post-Training Quantization framework that generates explainable insights on the cost-effectiveness of various model weight quantization methods for different denoiser operation types and block structures. We leverage these insights to make high-quality mixed-precision quantization decisions for a myriad of diffusion models ranging from foundational U-Nets to state-of-the-art Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit, 3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$, PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our weight-quantization configurations with 6-bit activation quantization and outperform existing approaches in terms of quantitative metrics and generative image quality. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹å®ç°äº†AIå›¾åƒç”Ÿæˆçš„æ™®åŠã€‚é‡åŒ–æ˜¯ä¸€ç§ä¸»è¦æŠ€æœ¯ï¼Œç”¨äºé™ä½æ¨ç†æˆæœ¬å¹¶ç¼©å°DMå»å™ªç½‘ç»œçš„è§„æ¨¡ã€‚ç„¶è€Œï¼Œéšç€å»å™ªå™¨ä»å·ç§¯U-Netçš„å˜ä½“å‘æ›´æ–°çš„Transformeræ¶æ„å‘å±•ï¼Œäº†è§£ä¸åŒæƒé‡å±‚ã€æ“ä½œå’Œæ¶æ„ç±»å‹å¯¹æ€§èƒ½çš„é‡åŒ–æ•æ„Ÿæ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡Qua$^2$SeDiMoæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆç²¾åº¦è®­ç»ƒåé‡åŒ–æ¡†æ¶ï¼Œä¸ºå„ç§æ¨¡å‹æƒé‡é‡åŒ–æ–¹æ³•åœ¨å„ç§å»å™ªå™¨æ“ä½œç±»å‹å’Œå—ç»“æ„ä¸Šçš„æˆæœ¬æ•ˆç›Šç”Ÿæˆå¯è§£é‡Šçš„è§è§£ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™äº›è§è§£æ¥ä¸ºä»åŸºç¡€U-Netåˆ°æœ€æ–°Transformerçš„ä¼—å¤šæ‰©æ•£æ¨¡å‹åšå‡ºé«˜è´¨é‡çš„æ··åˆç²¾åº¦é‡åŒ–å†³ç­–ã€‚ç»“æœï¼ŒQuasDiMoå¯ä»¥åœ¨PixArt-${\alpha}$ã€PixArt-${\Sigma}$ã€Hunyuan-DiTå’ŒSDXLä¸Šåˆ†åˆ«æ„å»º3.4ä½ã€3.9ä½ã€3.65ä½å’Œ3.7ä½çš„æƒé‡é‡åŒ–ã€‚æˆ‘ä»¬è¿˜å°†æˆ‘ä»¬çš„æƒé‡é‡åŒ–é…ç½®ä¸6ä½æ¿€æ´»é‡åŒ–ç›¸ç»“åˆï¼Œå¹¶åœ¨å®šé‡æŒ‡æ ‡å’Œç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14628v1">PDF</a> AAAI 2025; version includes supplementary material; 22 Pages, 18   Figures, 8 Tables</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹å®ç°äº†AIå›¾åƒç”Ÿæˆçš„æ™®åŠã€‚é‡åŒ–æ˜¯é™ä½æ¨ç†æˆæœ¬å¹¶å‡å°DMå»å™ªç½‘ç»œå¤§å°çš„ä¸»è¦æŠ€æœ¯ã€‚éšç€å»å™ªå™¨ä»å·ç§¯U-Netå˜ä½“å‘æ›´æ–°çš„Transformeræ¶æ„å‘å±•ï¼Œäº†è§£ä¸åŒæƒé‡å±‚ã€æ“ä½œå’Œæ¶æ„ç±»å‹å¯¹æ€§èƒ½çš„é‡åŒ–æ•æ„Ÿæ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å€ŸåŠ©Quamixed-precisionPost-TrainingQuantizationæ¡†æ¶è§£å†³æ­¤æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆæœ‰å…³ä¸åŒæ¨¡å‹æƒé‡é‡åŒ–æ–¹æ³•çš„æˆæœ¬æ•ˆç›Šçš„å¯è§£é‡Šè§è§£ï¼Œé€‚ç”¨äºä¸åŒçš„å»å™ªå™¨æ“ä½œç±»å‹å’Œå—ç»“æ„ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™äº›è§è§£ä¸ºä»åŸºç¡€U-Netåˆ°æœ€æ–°Transformerçš„å„ç§æ‰©æ•£æ¨¡å‹åšå‡ºé«˜è´¨é‡æ··åˆç²¾åº¦é‡åŒ–å†³ç­–ã€‚ç»“æœï¼ŒQuamixedå¯ä»¥åœ¨PixArt-Î±ã€PixArt-Î£ã€Hunyuan-DiTå’ŒSDXLä¸Šåˆ†åˆ«è¿›è¡Œ3.4ä½ã€3.9ä½ã€3.65ä½å’Œ3.7ä½æƒé‡é‡åŒ–ã€‚æˆ‘ä»¬å°†æƒé‡é‡åŒ–é…ç½®ä¸6ä½æ¿€æ´»é‡åŒ–ç›¸ç»“åˆï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹æ¨åŠ¨äº†AIå›¾åƒç”Ÿæˆçš„æ™®åŠã€‚</li>
<li>é‡åŒ–æ˜¯é™ä½æ‰©æ•£æ¨¡å‹æ¨ç†æˆæœ¬å’Œå‡å°ç½‘ç»œå¤§å°çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>ç†è§£ä¸åŒæƒé‡å±‚ã€æ“ä½œå’Œæ¶æ„ç±»å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­çš„æ•æ„Ÿæ€§å¯¹äºä¼˜åŒ–æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>Qua$^2$SeDiMoæ¡†æ¶æä¾›äº†å¯¹ä¸åŒæ¨¡å‹æƒé‡é‡åŒ–æ–¹æ³•çš„æˆæœ¬æ•ˆç›Šçš„å¯è§£é‡Šè§è§£ã€‚</li>
<li>Qua$^2$SeDiMoæ”¯æŒä»åŸºç¡€U-Netåˆ°é«˜çº§Transformerçš„å¤šç§æ‰©æ•£æ¨¡å‹çš„æ··åˆç²¾åº¦é‡åŒ–ã€‚</li>
<li>åœ¨ç‰¹å®šçš„æ‰©æ•£æ¨¡å‹ä¸Šï¼ŒQuamixedå®ç°äº†ä½æƒé‡æ¯”ç‰¹ç‡çš„é‡åŒ–ï¼Œå¦‚PixArtç³»åˆ—æ¨¡å‹çš„3.4-3.9ä½æƒé‡é‡åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17001be23bf41716496f74ae161df747.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e044badbcb79261a2824d04024f68cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3924dd7796ccc97da6a2035eee48172d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa8c63183e332f577d9a23cd473cd232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3dfc24d6b21a4315b3d87c3343c999b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6864aaeca1422061e4bab9c3355a5cd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LDP-Generalizing-to-Multilingual-Visual-Information-Extraction-by-Language-Decoupled-Pretraining"><a href="#LDP-Generalizing-to-Multilingual-Visual-Information-Extraction-by-Language-Decoupled-Pretraining" class="headerlink" title="LDP: Generalizing to Multilingual Visual Information Extraction by   Language Decoupled Pretraining"></a>LDP: Generalizing to Multilingual Visual Information Extraction by   Language Decoupled Pretraining</h2><p><strong>Authors:Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou</strong></p>
<p>Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hold invariance among images with different languages. If decoupling language bias from document images, a vision-layout-based model can achieve impressive cross-lingual generalization. Accordingly, we present a simple but effective multilingual training paradigm LDP (Language Decoupled Pre-training) for better utilization of monolingual pre-training data. Our proposed model LDM (Language Decoupled Model) is first pre-trained on the language-independent data, where the language knowledge is decoupled by a diffusion model, and then the LDM is fine-tuned on the downstream languages. Extensive experiments show that the LDM outperformed all SOTA multilingual pre-trained models, and also maintains competitiveness on downstream monolingual&#x2F;English benchmarks. </p>
<blockquote>
<p>è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰åœ¨åŠç»“æ„åŒ–æ–‡æ¡£ç†è§£ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå·²ç»å¼€å‘äº†ä¸€äº›é¢„è®­ç»ƒæ¨¡å‹æ¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥ä½œå¤§å¤šæ•°æ˜¯å•è¯­è¨€çš„ï¼ˆé€šå¸¸æ˜¯è‹±è¯­ï¼‰ã€‚ç”±äºè‹±è¯­å’Œå…¶ä»–è¯­è¨€ä¹‹é—´é¢„è®­ç»ƒè¯­æ–™åº“çš„æ•°é‡å’Œè´¨é‡æä¸å¹³è¡¡ï¼Œå¾ˆå°‘æœ‰å·¥ä½œèƒ½æ‰©å±•åˆ°éè‹±è¯­åœºæ™¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸åŒè¯­è¨€çš„å›¾åƒåœ¨è§†è§‰å’Œå¸ƒå±€æ¨¡å¼ä¸Šå…·æœ‰ä¸å˜æ€§ã€‚å¦‚æœå°†è¯­è¨€åè§ä»æ–‡æ¡£å›¾åƒä¸­è§£è€¦å‡ºæ¥ï¼ŒåŸºäºè§†è§‰å¸ƒå±€æ¨¡å‹å¯ä»¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤šè¯­è¨€è®­ç»ƒèŒƒå¼LDPï¼ˆè¯­è¨€è§£è€¦é¢„è®­ç»ƒï¼‰ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨å•è¯­è¨€é¢„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºçš„LDMï¼ˆè¯­è¨€è§£è€¦æ¨¡å‹ï¼‰é¦–å…ˆæ˜¯åœ¨ç‹¬ç«‹äºè¯­è¨€çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå…¶ä¸­è¯­è¨€çŸ¥è¯†é€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œè§£è€¦ï¼Œç„¶åLDMåœ¨ä¸‹æ¸¸è¯­è¨€ä¸Šè¿›è¡Œå¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLDMåœ¨æ‰€æœ‰æœ€å…ˆè¿›çš„å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸å•è¯­è¨€&#x2F;è‹±è¯­åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿä¿æŒç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14596v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰åœ¨åŠç»“æ„åŒ–æ–‡æ¡£ç†è§£ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä¸ºæå‡æ€§èƒ½ï¼Œå·²å¼€å‘å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥ä½œå¤šæ•°ä¸ºå•è¯­è¨€ï¼ˆé€šå¸¸ä¸ºè‹±è¯­ï¼‰ã€‚ç”±äºè‹±è¯­ä¸å…¶ä»–è¯­è¨€çš„é¢„è®­ç»ƒè¯­æ–™åº“åœ¨æ•°é‡å’Œå“è´¨ä¸Šå­˜åœ¨æä¸å‡è¡¡ï¼Œå°‘æœ‰å·¥ä½œèƒ½å»¶ä¼¸è‡³éè‹±è¯­åœºæ™¯ã€‚æœ¬æ–‡è¿›è¡Œç³»ç»Ÿå®éªŒï¼Œè¯æ˜è§†è§‰å’Œå¸ƒå±€æ¨¡æ€åœ¨ä¸åŒè¯­è¨€çš„å›¾åƒä¸­å…·æœ‰ä¸å˜æ€§ã€‚è‹¥ä»æ–‡æ¡£å›¾åƒä¸­è§£è€¦è¯­è¨€åè§ï¼ŒåŸºäºè§†è§‰å¸ƒå±€æ¨¡å‹å¯å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„è·¨è¯­è¨€æ³›åŒ–ã€‚æ®æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤šè¯­è¨€è®­ç»ƒèŒƒå¼LDPï¼ˆè¯­è¨€è§£è€¦é¢„è®­ç»ƒï¼‰ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨å•è¯­è¨€é¢„è®­ç»ƒæ•°æ®ã€‚æ‰€æå‡ºçš„LDMï¼ˆè¯­è¨€è§£è€¦æ¨¡å‹ï¼‰é¦–å…ˆåœ¨ç‹¬ç«‹äºè¯­è¨€çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶ä¸­è¯­è¨€çŸ¥è¯†é€šè¿‡æ‰©æ•£æ¨¡å‹è§£è€¦ï¼Œç„¶ååœ¨ä¸‹æ¸¸è¯­è¨€ä¸Šè¿›è¡Œå¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLDMåœ¨å¤šç§è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸å•è¯­è¨€&#x2F;è‹±è¯­åŸºå‡†æµ‹è¯•ä¸­äº¦ä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰åœ¨åŠç»“æ„åŒ–æ–‡æ¡£ç†è§£ä¸­éå¸¸é‡è¦ï¼Œå¹¶ä¸”å·²æœ‰å¤šç§é¢„è®­ç»ƒæ¨¡å‹ç”¨äºæå‡æ€§èƒ½ã€‚</li>
<li>å¤§å¤šæ•°ç›¸å…³å·¥ä½œä¸ºå•è¯­è¨€ï¼ˆé€šå¸¸æ˜¯è‹±è¯­ï¼‰ï¼Œç”±äºè¯­æ–™åº“çš„æ•°é‡å’Œè´¨é‡é—®é¢˜ï¼Œè¿™äº›æ¨¡å‹åœ¨éè‹±è¯­åœºæ™¯çš„åº”ç”¨å—é™ã€‚</li>
<li>å®éªŒè¡¨æ˜è§†è§‰å’Œå¸ƒå±€æ¨¡æ€åœ¨ä¸åŒè¯­è¨€çš„å›¾åƒä¸­å…·æœ‰ä¸å˜æ€§ã€‚</li>
<li>è§£è€¦è¯­è¨€åè§çš„åŸºäºè§†è§‰å¸ƒå±€çš„æ¨¡å‹å¯ä»¥å®ç°è·¨è¯­è¨€æ³›åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šè¯­è¨€è®­ç»ƒèŒƒå¼LDPï¼Œæ—¨åœ¨æ›´å¥½åœ°åˆ©ç”¨å•è¯­è¨€é¢„è®­ç»ƒæ•°æ®ã€‚</li>
<li>LDMæ¨¡å‹é¦–å…ˆåœ¨ç‹¬ç«‹äºè¯­è¨€çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡æ‰©æ•£æ¨¡å‹è§£è€¦è¯­è¨€çŸ¥è¯†ï¼Œå¹¶åœ¨ä¸‹æ¸¸è¯­è¨€ä¸Šè¿›è¡Œå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2718c6014987cc38fd24ada84412f5a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f63a78252fce44b152e61c47d2727ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c0f59c56a539335b3beb737c7481481.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4193d1d078a9799636e265935916555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-829edf06debb3d92497923c8cda760b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd087f8732346b4589e984a8fdc57875.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DiffSim-Taming-Diffusion-Models-for-Evaluating-Visual-Similarity"><a href="#DiffSim-Taming-Diffusion-Models-for-Evaluating-Visual-Similarity" class="headerlink" title="DiffSim: Taming Diffusion Models for Evaluating Visual Similarity"></a>DiffSim: Taming Diffusion Models for Evaluating Visual Similarity</h2><p><strong>Authors:Yiren Song, Xiaokang Liu, Mike Zheng Shou</strong></p>
<p>Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†ç”Ÿæˆæ¨¡å‹é¢†åŸŸï¼Œè¯„ä¼°å®šåˆ¶æ¨¡å‹è¾“å‡ºå’Œå‚è€ƒè¾“å…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ„ŸçŸ¥ç›¸ä¼¼æ€§åº¦é‡ä¸»è¦å·¥ä½œåœ¨åƒç´ å’Œè¡¥ä¸çº§åˆ«ï¼Œæ¯”è¾ƒä½çº§çš„é¢œè‰²å’Œçº¹ç†ï¼Œä½†æ— æ³•æ•æ‰ä¸­çº§ç›¸ä¼¼æ€§ä»¥åŠå›¾åƒå¸ƒå±€ã€ç‰©ä½“å§¿æ€å’Œè¯­ä¹‰å†…å®¹çš„å·®å¼‚ã€‚åŸºäºå¯¹æ¯”å­¦ä¹ çš„CLIPå’ŒåŸºäºè‡ªç›‘ç£å­¦ä¹ çš„DINOé€šå¸¸ç”¨äºæµ‹é‡è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½†å®ƒä»¬ä¼šé«˜åº¦å‹ç¼©å›¾åƒç‰¹å¾ï¼Œå¯¹å¤–è§‚ç»†èŠ‚è¯„ä¼°ä¸è¶³ã€‚æœ¬æ–‡é¦–æ¬¡å‘ç°å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥æµ‹é‡è§†è§‰ç›¸ä¼¼æ€§ï¼Œå¹¶å¼•å…¥äº†DiffSimæ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»ŸæŒ‡æ ‡åœ¨æ•è·å®šåˆ¶ç”Ÿæˆä»»åŠ¡ä¸­çš„æ„ŸçŸ¥ä¸€è‡´æ€§æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å¯¹é½å»å™ªU-Netæ³¨æ„å±‚ä¸­çš„ç‰¹å¾ï¼ŒDiffSimè¯„ä¼°å¤–è§‚å’Œé£æ ¼ç›¸ä¼¼æ€§ï¼Œä¸äººç±»è§†è§‰åå¥½é«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Srefå’ŒIPåŸºå‡†æ¥åˆ†åˆ«åœ¨é£æ ¼å’Œå®ä¾‹çº§åˆ«è¯„ä¼°è§†è§‰ç›¸ä¼¼æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒDiffSimå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºç”Ÿæˆæ¨¡å‹ä¸­è§†è§‰ä¸€è‡´æ€§çš„æµ‹é‡æä¾›äº†å¯é çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14580v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†ç”Ÿæˆæ¨¡å‹é¢†åŸŸï¼Œå› æ­¤å¯¹å®šåˆ¶åŒ–æ¨¡å‹è¾“å‡ºä¸å‚è€ƒè¾“å…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§è¯„ä¼°å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ„ŸçŸ¥ç›¸ä¼¼æ€§åº¦é‡ä¸»è¦ä¾§é‡äºåƒç´ å’Œæ–‘å—å±‚é¢ï¼Œæ¯”è¾ƒä½çº§çš„é¢œè‰²å’Œçº¹ç†ï¼Œè€Œæ— æ³•æ•æ‰ä¸­çº§çš„ç›¸ä¼¼æ€§ä»¥åŠå›¾åƒå¸ƒå±€ã€ç‰©ä½“å§¿æ€å’Œè¯­ä¹‰å†…å®¹çš„å·®å¼‚ã€‚è™½ç„¶å¯¹æ¯”å­¦ä¹ åŸºç¡€ä¸Šçš„CLIPå’Œè‡ªç›‘ç£å­¦ä¹ åŸºç¡€ä¸Šçš„DINOå¸¸ç”¨äºè¡¡é‡è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½†å®ƒä»¬é«˜åº¦å‹ç¼©å›¾åƒç‰¹å¾ï¼Œå¯¹å¤–è§‚ç»†èŠ‚è¯„ä¼°ä¸è¶³ã€‚æœ¬æ–‡é¦–æ¬¡å‘ç°å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥æµ‹é‡è§†è§‰ç›¸ä¼¼æ€§ï¼Œå¹¶å¼•å…¥äº†DiffSimæ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿåº¦é‡æ–¹æ³•åœ¨å®šåˆ¶ç”Ÿæˆä»»åŠ¡ä¸­æ•æ‰æ„ŸçŸ¥ä¸€è‡´æ€§çš„å±€é™æ€§ã€‚é€šè¿‡å¯¹é½å»å™ªU-Netæ³¨æ„åŠ›å±‚ä¸­çš„ç‰¹å¾ï¼ŒDiffSimè¯„ä¼°å¤–è§‚å’Œé£æ ¼ç›¸ä¼¼æ€§ï¼Œä¸äººç±»è§†è§‰åå¥½é«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Srefå’ŒIPåŸºå‡†æ¥åˆ†åˆ«åœ¨é£æ ¼å’Œå®ä¾‹å±‚é¢è¯„ä¼°è§†è§‰ç›¸ä¼¼æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†çš„ç»¼åˆæ€§è¯„ä¼°ä¸­ï¼ŒDiffSimå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºç”Ÿæˆæ¨¡å‹ä¸­è§†è§‰ä¸€è‡´æ€§çš„æµ‹é‡æä¾›äº†ç¨³å¥çš„å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯¹ç”Ÿæˆæ¨¡å‹é¢†åŸŸäº§ç”Ÿäº†æ ¹æœ¬æ€§å½±å“ï¼Œä½¿å¾—è¯„ä¼°å®šåˆ¶åŒ–æ¨¡å‹è¾“å‡ºä¸å‚è€ƒè¾“å…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ„ŸçŸ¥ç›¸ä¼¼æ€§åº¦é‡ä¸»è¦å…³æ³¨åƒç´ å’Œæ–‘å—å±‚é¢ï¼Œéš¾ä»¥æ•æ‰ä¸­çº§ç›¸ä¼¼æ€§åŠå›¾åƒå¸ƒå±€ã€ç‰©ä½“å§¿æ€å’Œè¯­ä¹‰å†…å®¹çš„å·®å¼‚ã€‚</li>
<li>é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯ç”¨äºæµ‹é‡è§†è§‰ç›¸ä¼¼æ€§ï¼Œå¼•å…¥çš„DiffSimæ–¹æ³•è§£å†³äº†ä¼ ç»Ÿåº¦é‡æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>DiffSimé€šè¿‡å¯¹é½å»å™ªU-Netæ³¨æ„åŠ›å±‚ä¸­çš„ç‰¹å¾ï¼Œèƒ½å¤Ÿè¯„ä¼°å¤–è§‚å’Œé£æ ¼ç›¸ä¼¼æ€§ï¼Œä¸äººç±»è§†è§‰åå¥½é«˜åº¦ä¸€è‡´ã€‚</li>
<li>Srefå’ŒIPåŸºå‡†çš„å¼•å…¥åˆ†åˆ«ç”¨äºè¯„ä¼°è§†è§‰ç›¸ä¼¼æ€§çš„é£æ ¼å’Œå®ä¾‹å±‚é¢ã€‚</li>
<li>DiffSimåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä¸ºç”Ÿæˆæ¨¡å‹ä¸­è§†è§‰ä¸€è‡´æ€§çš„æµ‹é‡æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚</li>
<li>DiffSimçš„å¼•å…¥æœ‰æœ›æ¨åŠ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„å‘å±•ï¼Œä¿ƒè¿›æ›´å‡†ç¡®çš„æ¨¡å‹è¯„ä¼°å’Œå®šåˆ¶ç”Ÿæˆä»»åŠ¡çš„ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be8dfd9612efc2d6d0cc232ebff03c60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d246278b6e83b90ca7ddf66a6a802a7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1e6f2582c2d1e7f0fc2b1a42df0be6a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Content-style-disentangled-representation-for-controllable-artistic-image-stylization-and-generation"><a href="#Content-style-disentangled-representation-for-controllable-artistic-image-stylization-and-generation" class="headerlink" title="Content-style disentangled representation for controllable artistic   image stylization and generation"></a>Content-style disentangled representation for controllable artistic   image stylization and generation</h2><p><strong>Authors:Ma Zhuoqi, Zhang Yixuan, You Zejun, Tian Long, Liu Xiyang</strong></p>
<p>Controllable artistic image stylization and generation aims to render the content provided by text or image with the learned artistic style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image information for supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in semantic interference from the reference image. To address the above issues, this paper proposes a content-style representation disentangling method for controllable artistic image stylization and generation. We construct a WikiStyle+ dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled content and style representations guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers for better controllable stylization. This approach allows model to accommodate inputs from different modalities. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling a harmonious integration of content and style in the generated outputs, successfully producing style-consistent and expressive stylized images. </p>
<blockquote>
<p>å¯æ§çš„è‰ºæœ¯å›¾åƒé£æ ¼åŒ–å’Œç”Ÿæˆæ—¨åœ¨ç”¨æä¾›çš„æ–‡æœ¬æˆ–å›¾åƒå†…å®¹å‘ˆç°æ‰€å­¦çš„è‰ºæœ¯é£æ ¼ï¼Œè€Œå†…å®¹å’Œé£æ ¼çš„è§£è€¦æ˜¯è·å¾—æ»¡æ„ç»“æœçš„å…³é”®ã€‚ç„¶è€Œï¼Œå½“å‰çš„å†…å®¹å’Œé£æ ¼åˆ†ç¦»æ–¹æ³•ä¸»è¦ä¾èµ–äºå›¾åƒä¿¡æ¯è¿›è¡Œç›‘ç£ï¼Œè¿™å¯¼è‡´äº†ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰æ¨¡å‹åªèƒ½æ”¯æŒä¸€ç§é£æ ¼æˆ–å†…å®¹è¾“å…¥çš„æ¨¡æ€ï¼›2ï¼‰ç”±äºå‚è€ƒå›¾åƒå­˜åœ¨è¯­ä¹‰å¹²æ‰°ï¼Œå¯¼è‡´è§£è€¦ä¸å®Œå…¨ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ§è‰ºæœ¯å›¾åƒé£æ ¼åŒ–å’Œç”Ÿæˆçš„å†…å®¹ä¸é£æ ¼è¡¨ç¤ºåˆ†ç¦»æ–¹æ³•ã€‚æˆ‘ä»¬æ„å»ºäº†WikiStyle+æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è‰ºæœ¯å“åŠå…¶ç›¸åº”çš„æ–‡æœ¬æè¿°é£æ ¼å’Œå†…å®¹ã€‚åŸºäºå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç¦»çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºæŒ‡å¯¼çš„æ‰©æ•£æ¨¡å‹ã€‚é¦–å…ˆé€šè¿‡Q-Formerså­¦ä¹ è§£è€¦çš„è¡¨ç¤ºï¼Œç„¶åå°†å…¶æ³¨å…¥é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„å¤šæ­¥äº¤å‰æ³¨æ„å±‚å®ç°æ›´å¥½çš„å¯æ§é£æ ¼åŒ–ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹æ¥å—æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ¨¡æ€ç›‘ç£ä¸‹å®ç°äº†å‚è€ƒå›¾åƒå†…å®¹å’Œé£æ ¼çš„å½»åº•åˆ†ç¦»ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ç”Ÿæˆçš„è¾“å‡ºä¸­å’Œè°åœ°èåˆå†…å®¹å’Œé£æ ¼ï¼ŒæˆåŠŸç”Ÿæˆé£æ ¼ä¸€è‡´ã€è¡¨ç°åŠ›å¼ºçš„å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14496v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€æ•°æ®é›†çš„å†…å®¹ä¸é£æ ¼è¡¨ç¤ºåˆ†ç¦»æ–¹æ³•ï¼Œç”¨äºå¯æ§çš„è‰ºæœ¯å›¾åƒé£æ ¼åŒ–å’Œç”Ÿæˆã€‚é€šè¿‡æ„å»ºåŒ…å«è‰ºæœ¯ä½œå“åŠå…¶ç›¸åº”æ–‡æœ¬æè¿°çš„WikiStyle+æ•°æ®é›†ï¼Œä»¥åŠé‡‡ç”¨åˆ†ç¦»çš„å†…å®¹ä¸é£æ ¼è¡¨ç¤ºå¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¯¹ä¸åŒæ¨¡æ€è¾“å…¥çš„æ”¯æŒï¼Œå¹¶å½»åº•è§£è€¦äº†å‚è€ƒå›¾åƒä¸­çš„å†…å®¹å’Œé£æ ¼ï¼Œä»è€Œç”Ÿæˆé£æ ¼ä¸€è‡´ã€å¯Œæœ‰è¡¨ç°åŠ›çš„å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ—¨åœ¨é€šè¿‡å†…å®¹ä¸é£æ ¼è§£è€¦çš„æ–¹æ³•å®ç°è‰ºæœ¯å›¾åƒçš„å¯æ§é£æ ¼åŒ–å’Œç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒä¿¡æ¯è¿›è¡Œç›‘ç£ï¼Œå¯¼è‡´åªèƒ½æ”¯æŒå•ä¸€æ¨¡æ€çš„è¾“å…¥å’Œä¸å®Œæ•´çš„è§£è€¦ã€‚</li>
<li>è®ºæ–‡æ„å»ºäº†WikiStyle+æ•°æ®é›†ï¼ŒåŒ…å«è‰ºæœ¯ä½œå“åŠå…¶ç›¸åº”çš„æ–‡æœ¬æè¿°ï¼Œç”¨äºé£æ ¼å’Œå†…å®¹çš„è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†åŸºäºå¤šæ¨¡æ€æ•°æ®é›†çš„å†…å®¹ä¸é£æ ¼è¡¨ç¤ºåˆ†ç¦»æ–¹æ³•ã€‚</li>
<li>é€šè¿‡Q-Formerså­¦ä¹ è§£è€¦åçš„è¡¨ç¤ºï¼Œç„¶åæ³¨å…¥é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„å¤šæ­¥äº¤å‰æ³¨æ„å±‚å®ç°æ›´å¥½çš„å¯æ§é£æ ¼åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€ç›‘ç£ä¸‹å½»åº•è§£è€¦äº†å‚è€ƒå›¾åƒä¸­çš„å†…å®¹å’Œé£æ ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e989c1b0feb71926ee7511824ac3964c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc40e86c3c647c7a89f9f00efc968117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44897723a308df0bc607aa2f4d74e3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-309dc704bd3f7ce3388da8fe2b13fccb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æ–°æ™‹çƒ­é—¨ï¼Œåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“ç”¨äºç›®æ ‡ç§»é™¤ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚äº§ç”Ÿéšæœºä¼ªå½±å’Œåœ¨ç§»é™¤åæ— æ³•é‡æ–°ç»˜åˆ¶å‰æ™¯å¯¹è±¡åŒºåŸŸä»¥å¡«å……é€‚å½“çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œAttentive Eraserâ€æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´å³å¯å¢å¼ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œç¨³å®šå’Œæœ‰æ•ˆç›®æ ‡ç§»é™¤çš„æ–¹æ³•ã€‚é¦–å…ˆï¼ŒåŸºäºè§‚å¯Ÿåˆ°è‡ªæ³¨æ„åŠ›å›¾ä¼šå½±å“ç”Ÿæˆå›¾åƒçš„ç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›æ¿€æ´»å’ŒæŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œè¯¥æ–¹æ³•æ ¹æ®ç»™å®šçš„æ©è†œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œåœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯ç›®æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæ³¨æ„åŠ›é‡å®šå‘æŒ‡å¯¼ï¼ˆSARGï¼‰ï¼Œå®ƒåˆ©ç”¨ASSå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œåœ¨æ©è†œå†…æœ‰æ•ˆåœ°ç§»é™¤å‰æ™¯ç›®æ ‡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨ä¸åŒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ç¨³å®šä¸”æœ‰æ•ˆï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯åº”ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser%E3%80%82">https://github.com/Anonym0u3/AttentiveEraserã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v3">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸå´­éœ²å¤´è§’ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å¯¹è±¡ç§»é™¤ä»»åŠ¡æ—¶ä»å­˜åœ¨é—®é¢˜ï¼Œå¦‚ç”Ÿæˆéšæœºä¼ªå½±å’Œåœ¨ç§»é™¤å‰æ™¯å¯¹è±¡åæ— æ³•é‡æ–°ç»˜åˆ¶é€‚å½“å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ— éœ€è°ƒæ•´çš„â€œAttentive Eraserâ€æ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›é‡å®šå‘å¼•å¯¼ï¼ˆSARGï¼‰å’Œæ³¨æ„åŠ›æ¿€æ´»ä¸æŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ã€‚è¯¥æ–¹æ³•åœ¨å¯¹è±¡ç§»é™¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†è®¸å¤šè®­ç»ƒå‹æ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹é¢ä¸´ç”Ÿæˆéšæœºä¼ªå½±å’Œæ— æ³•é‡æ–°ç»˜åˆ¶ç§»é™¤å¯¹è±¡åçš„é€‚å½“å†…å®¹çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œAttentive Eraserâ€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿèµ‹èƒ½é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚</li>
<li>é€šè¿‡Attention Activation and Suppressionï¼ˆASSï¼‰æŠ€æœ¯ï¼Œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>å¼•å…¥Self-Attention Redirection Guidanceï¼ˆSARGï¼‰ï¼Œæœ‰æ•ˆå»é™¤å‰æ™¯å¯¹è±¡å¹¶ç”Ÿæˆåˆç†è¿è´¯çš„å†…å®¹ã€‚</li>
<li>Attentive Eraseræ–¹æ³•åœ¨å„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°å‡ºç¨³å®šæ€§ä¸é«˜æ•ˆæ€§ï¼Œä¸”ä¼˜äºè®¸å¤šè®­ç»ƒå‹æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c05a74d5fc8f7c7fe4df9ee99c061548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2072630d561b95396afa914c2d8e33b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d9b80c7275fc5978631c2565f9043ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daf18f400271dc04dfd192a7b462c011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3fe8a0715cad758c3774271ce823ca6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c81810756ea7a7a3ba441ce5a37a1067.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RAD-Region-Aware-Diffusion-Models-for-Image-Inpainting"><a href="#RAD-Region-Aware-Diffusion-Models-for-Image-Inpainting" class="headerlink" title="RAD: Region-Aware Diffusion Models for Image Inpainting"></a>RAD: Region-Aware Diffusion Models for Image Inpainting</h2><p><strong>Authors:Sora Kim, Sungho Suh, Minsik Lee</strong></p>
<p>Diffusion models have achieved remarkable success in image generation, with applications broadening across various domains. Inpainting is one such application that can benefit significantly from diffusion models. Existing methods either hijack the reverse process of a pretrained diffusion model or cast the problem into a larger framework, \ie, conditioned generation. However, these approaches often require nested loops in the generation process or additional components for conditioning. In this paper, we present region-aware diffusion models (RAD) for inpainting with a simple yet effective reformulation of the vanilla diffusion models. RAD utilizes a different noise schedule for each pixel, which allows local regions to be generated asynchronously while considering the global image context. A plain reverse process requires no additional components, enabling RAD to achieve inference time up to 100 times faster than the state-of-the-art approaches. Moreover, we employ low-rank adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models, reducing computational burdens in training as well. Experiments demonstrated that RAD provides state-of-the-art results both qualitatively and quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå…¶åº”ç”¨æ­£åœ¨å„ä¸ªé¢†åŸŸä¸æ–­æ‰©å±•ã€‚å›¾åƒä¿®å¤æ˜¯å…¶ä¸­ä¸€ä¸ªå¯ä»¥ä»æ‰©æ•£æ¨¡å‹ä¸­å¤§å¤§å—ç›Šçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆåŠ«æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åå‘è¿‡ç¨‹ï¼Œè¦ä¹ˆå°†é—®é¢˜è½¬åŒ–ä¸ºæ›´å¤§çš„æ¡†æ¶ï¼Œå³æ¡ä»¶ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨åµŒå¥—å¾ªç¯æˆ–é¢å¤–çš„ç»„ä»¶æ¥è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå›¾åƒä¿®å¤çš„åŒºåŸŸæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼ˆRADï¼‰ï¼Œè¿™æ˜¯å¯¹åŸå§‹æ‰©æ•£æ¨¡å‹çš„ç®€å•è€Œæœ‰æ•ˆçš„é‡æ–°è¡¨è¿°ã€‚RADä¸ºæ¯ä¸ªåƒç´ ä½¿ç”¨ä¸åŒçš„å™ªå£°æ–¹æ¡ˆï¼Œè¿™å…è®¸å±€éƒ¨åŒºåŸŸåœ¨è€ƒè™‘å…¨å±€å›¾åƒä¸Šä¸‹æ–‡çš„åŒæ—¶è¿›è¡Œå¼‚æ­¥ç”Ÿæˆã€‚ç®€å•çš„åå‘è¿‡ç¨‹ä¸éœ€è¦é¢å¤–çš„ç»„ä»¶ï¼Œä½¿RADçš„æ¨ç†æ—¶é—´è¾¾åˆ°æœ€æ–°æ–¹æ³•çš„100å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•æ¥å¯¹RADè¿›è¡Œå¾®è°ƒï¼ŒåŸºäºå…¶ä»–é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå‡è½»äº†è®­ç»ƒä¸­çš„è®¡ç®—è´Ÿæ‹…ã€‚å®éªŒè¡¨æ˜ï¼ŒRADåœ¨FFHQã€LSUNå§å®¤å’ŒImageNetæ•°æ®é›†ä¸Šåœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½æä¾›äº†æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09191v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒºåŸŸæ„ŸçŸ¥å›¾åƒä¿®å¤æ–¹æ³•ï¼ˆRADï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç®€å•çš„è°ƒæ•´å®ç°äº†æ‰©æ•£æ¨¡å‹çš„å¼‚æ­¥å±€éƒ¨åŒºåŸŸç”Ÿæˆï¼Œå¹¶åˆ©ç”¨ä¸åŒçš„å™ªå£°è°ƒåº¦ç­–ç•¥è€ƒè™‘äº†å…¨å±€å›¾åƒä¸Šä¸‹æ–‡ã€‚RADæ— éœ€é¢å¤–çš„ç»„ä»¶ï¼Œåªéœ€ä½¿ç”¨æ™®é€šçš„åå‘è¿‡ç¨‹ï¼Œå³å¯å®ç°æ¯”ç°æœ‰æŠ€æœ¯æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¾®è°ƒRADæ¨¡å‹ï¼Œé™ä½äº†è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…ã€‚å®éªŒè¡¨æ˜ï¼ŒRADåœ¨FFHQã€LSUNå§å®¤å’ŒImageNetæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå…¶åº”ç”¨é¢†åŸŸæ­£åœ¨ä¸æ–­æ‰©å±•ï¼ŒåŒ…æ‹¬å›¾åƒä¿®å¤ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚çš„ç”Ÿæˆè¿‡ç¨‹æˆ–é¢å¤–çš„ç»„ä»¶æ¥å®ç°æ¡ä»¶ç”Ÿæˆã€‚</li>
<li>RADé€šè¿‡ç®€å•çš„è°ƒæ•´å®ç°äº†æ‰©æ•£æ¨¡å‹çš„åŒºåŸŸæ„ŸçŸ¥å›¾åƒä¿®å¤ï¼Œåˆ©ç”¨ä¸åŒçš„å™ªå£°è°ƒåº¦ç­–ç•¥ä¸ºæ¯ä¸ªåƒç´ ç”Ÿæˆå±€éƒ¨åŒºåŸŸï¼ŒåŒæ—¶è€ƒè™‘å…¨å±€å›¾åƒä¸Šä¸‹æ–‡ã€‚</li>
<li>RADåœ¨æ¨ç†é€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæ¯”ç°æœ‰æŠ€æœ¯å¿«100å€ã€‚</li>
<li>RADæ¨¡å‹é€šè¿‡ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRADåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb27e2f9609a195d0967e868517d9563.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd8c396472f1aa11c05ef604534c1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d6dfcc7130bc7eb44a19ba72b755367.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰çš„æœ€æ–°è¿›å±•ä¸ºç«¯åˆ°ç«¯çš„è¯­ä¹‰å›¾åƒä¼ è¾“å±•ç¤ºäº†å‰æ™¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ¡ˆä¸»è¦å…³æ³¨åƒç´ çº§çš„æŒ‡æ ‡ä¼˜åŒ–ï¼Œè¿™é€šå¸¸ä¸äººç±»æ„ŸçŸ¥ä¸å»åˆï¼Œå¯¼è‡´æ„ŸçŸ¥è´¨é‡ä¸‹é™ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼Œä»¥æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ç†µæ¨¡å‹ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µæœ‰æ•ˆåœ°ç®¡ç†ä¼ è¾“å¸¦å®½ã€‚è¿™äº›ç¬¦å·ç„¶åä½œä¸ºæ¥æ”¶ç«¯æ—¶çš„æ¡ä»¶ä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼å›¾åƒé‡å»ºä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æ–°å…´çš„é©¬å§†å·´å¼çº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ä¹‹ä¸Šï¼Œè¯¥éª¨æ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å¹¶æé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ·±åº¦è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç†µæ¨¡å‹ä¼°ç®—ä¼ è¾“ç¬¦å·çš„ç†µï¼Œå®ç°æœ‰æ•ˆå¸¦å®½ç®¡ç†ï¼Œå¹¶åœ¨æ¥æ”¶ç«¯ä½¿ç”¨è¿™äº›ç¬¦å·ä½œä¸ºæ¡ä»¶ä¿¡æ¯æ¥æŒ‡å¯¼å›¾åƒé‡å»ºçš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚è¯¥æ¨¡å‹åŸºäºå…ˆè¿›çš„mamba-likeçº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ï¼Œå…·æœ‰å›¾åƒå¤„ç†ä»»åŠ¡å‡ºè‰²å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦çš„ä¼˜ç‚¹ã€‚åŒæ—¶ï¼Œå¼•å…¥å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å’Œæé«˜æ•´ä½“æ€§èƒ½ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹æ·±åº¦è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹å¢å¼ºå›¾åƒä¼ è¾“çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>åˆ©ç”¨ç†µæ¨¡å‹ä¼°ç®—ä¼ è¾“ç¬¦å·çš„ç†µï¼Œå®ç°å¸¦å®½çš„æœ‰æ•ˆç®¡ç†ã€‚</li>
<li>åœ¨æ¥æ”¶ç«¯ä½¿ç”¨æ¡ä»¶ä¿¡æ¯æ¥æŒ‡å¯¼å›¾åƒé‡å»ºçš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚</li>
<li>æ¨¡å‹åŸºäºå…ˆè¿›çš„mamba-likeçº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ï¼Œå…·æœ‰å¿«é€Ÿæ¨ç†é€Ÿåº¦å’Œä¼˜ç§€çš„å›¾åƒå¤„ç†æ€§èƒ½ã€‚</li>
<li>å¼•å…¥å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ•´ä½“æ€§èƒ½ã€‚</li>
<li>ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90b3a0658014bd140d80a5609c0caf99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d6eb91831daa0dcbcae7edc72d4ae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20e02759ca70cd725aed915d7feac78f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SkyDiffusion-Ground-to-Aerial-Image-Synthesis-with-Diffusion-Models-and-BEV-Paradigm"><a href="#SkyDiffusion-Ground-to-Aerial-Image-Synthesis-with-Diffusion-Models-and-BEV-Paradigm" class="headerlink" title="SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and   BEV Paradigm"></a>SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and   BEV Paradigm</h2><p><strong>Authors:Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He</strong></p>
<p>Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Birdâ€™s-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a â€œmulti-to-oneâ€ mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, historical high-resolution satellite image synthesis, and low-altitude UAV image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. More result and dataset information can be found at <a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/">https://opendatalab.github.io/skydiffusion/</a> . </p>
<blockquote>
<p>åœ°é¢åˆ°ç©ºä¸­çš„å›¾åƒåˆæˆä¸“æ³¨äºä»ç›¸åº”çš„åœ°é¢è¡—æ™¯å›¾åƒç”Ÿæˆé€¼çœŸçš„ç©ºä¸­å›¾åƒï¼ŒåŒæ—¶ä¿æŒå†…å®¹å¸ƒå±€çš„ä¸€è‡´æ€§ï¼Œæ¨¡æ‹Ÿä»ä¸Šåˆ°ä¸‹çš„è§†è§’ã€‚æ˜¾è‘—çš„è§†ç‚¹å·®å¼‚å¯¼è‡´äº†ä¸åŒè§†è§’ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè€Œå¯†é›†çš„åŸåŒºåœºæ™¯é™åˆ¶äº†è¡—æ™¯çš„å¯è§èŒƒå›´ï¼Œè¿™ä½¿å¾—è·¨è§†å›¾ç”Ÿæˆä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SkyDiffusionï¼Œè¿™æ˜¯ä¸€ç§ä»è¡—æ™¯å›¾åƒåˆæˆç©ºä¸­å›¾åƒçš„æ–°å‹è·¨è§†å›¾ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œé¸Ÿç°å›¾ï¼ˆBEVï¼‰èŒƒå¼ã€‚SkyDiffusionä¸­çš„Curved-BEVæ–¹æ³•å°†è¡—æ™¯å›¾åƒè½¬æ¢ä¸ºBEVé€è§†ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†é¢†åŸŸå·®è·ï¼Œå¹¶é‡‡ç”¨â€œå¤šåˆ°ä¸€â€çš„æ˜ å°„ç­–ç•¥æ¥è§£å†³å¯†é›†åŸå¸‚åœºæ™¯ä¸­çš„é®æŒ¡é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼ŒSkyDiffusionè®¾è®¡äº†ä¸€ä¸ªBEVå¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆå†…å®¹ä¸€è‡´ä¸”é€¼çœŸçš„ç©ºä¸­å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†Ground2Aerial-3ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºå¤šæ ·åŒ–çš„åœ°é¢åˆ°ç©ºä¸­å›¾åƒåˆæˆåº”ç”¨è€Œè®¾è®¡ï¼ŒåŒ…æ‹¬ç¾å®³åœºæ™¯ç©ºä¸­åˆæˆã€é«˜åˆ†è¾¨ç‡å†å²å«æ˜Ÿå›¾åƒåˆæˆä»¥åŠä½ç©ºæ— äººæœºå›¾åƒåˆæˆä»»åŠ¡ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSkyDiffusionåœ¨è·¨è§†å›¾æ•°æ®é›†ï¼ˆå¦‚è‡ªç„¶åœºæ™¯ï¼ˆCVUSAï¼‰ã€éƒŠåŒºåœºæ™¯ï¼ˆCVACTï¼‰ã€åŸå¸‚åœºæ™¯ï¼ˆVIGOR-Chicagoï¼‰ä»¥åŠå¤šç§åº”ç”¨åœºæ™¯ï¼ˆG2A-3ï¼‰ï¼‰ä¸Šçš„è¡¨ç°å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†é€¼çœŸä¸”å†…å®¹ä¸€è‡´çš„å¤©ç©ºå›¾åƒç”Ÿæˆã€‚æ›´å¤šç»“æœå’Œæ•°æ®é›†ä¿¡æ¯å¯åœ¨<a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/%E6%89%BE%E5%88%B0%E3%80%82">https://opendatalab.github.io/skydiffusion/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01812v3">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåœ°é¢è§†è§’ç”ŸæˆçœŸå®æ„Ÿå¼ºçƒˆçš„ç©ºä¸­å›¾åƒæ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œå› ä¸ºè§†ç‚¹å·®å¼‚å¯¼è‡´é¢†åŸŸé¸¿æ²Ÿä»¥åŠåŸå¸‚æ™¯è§‚çš„é®æŒ¡é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†SkyDiffusionæ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å’Œé¸Ÿç°å›¾ï¼ˆBEVï¼‰èŒƒå¼å®ç°è¿™ä¸€ä»»åŠ¡ã€‚SkyDiffusionåˆ›æ–°æ€§åœ°å¼•å…¥äº†Curved-BEVæ–¹æ³•ï¼Œæœ‰æ•ˆç¼©çŸ­äº†é¢†åŸŸå·®è·å¹¶è§£å†³äº†é®æŒ¡é—®é¢˜ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§å—é¸Ÿç°å›¾æŒ‡å¯¼çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå†…å®¹è¿è´¯ä¸”çœŸå®çš„ç©ºä¸­å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Ground2Aerial-3ï¼Œç”¨äºåœ°é¢åˆ°ç©ºä¸­çš„å›¾åƒåˆæˆåº”ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkyDiffusionåœ¨è‡ªç„¶ã€éƒŠåŒºã€åŸå¸‚å’Œå¤šç§åº”ç”¨æƒ…å¢ƒçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯ã€‚æœ‰å…³ç»“æœå’Œè¯¦ç»†ä¿¡æ¯å¯å‚è§ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/">https://opendatalab.github.io/skydiffusion/</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SkyDiffusionæ˜¯ä¸€ç§æ–°é¢–çš„è·¨è§†è§’ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºä»è¡—å¤´è§†è§’çš„å›¾åƒåˆæˆç©ºä¸­å›¾åƒã€‚</li>
<li>å¼•å…¥Curved-BEVæ–¹æ³•ï¼Œæœ‰æ•ˆæ¡¥æ¥åœ°é¢ä¸ç©ºä¸­è§†è§’çš„åŸŸå·®è·ã€‚</li>
<li>é‡‡ç”¨â€œå¤šåˆ°ä¸€â€çš„æ˜ å°„ç­–ç•¥æ¥è§£å†³å¯†é›†åŸå¸‚æ™¯è§‚ä¸­çš„é®æŒ¡é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†å—é¸Ÿç°å›¾æŒ‡å¯¼çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆè¿è´¯ä¸”çœŸå®çš„ç©ºä¸­å›¾åƒã€‚</li>
<li>æ¨å‡ºæ–°çš„æ•°æ®é›†Ground2Aerial-3ï¼Œé€‚ç”¨äºå¤šç§åœ°é¢åˆ°ç©ºä¸­çš„å›¾åƒåˆæˆåº”ç”¨ã€‚</li>
<li>SkyDiffusionåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59528443fe4476fe95bff216ced5f1e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-750202e1a98a34f8ad6157aefde1aebd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fc89567385bbfb3d6dc5d45fc48b5f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790ed603eda1320e53b2640b00dd3a71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac69890ed119bb0ddd03c0d9c0b1bbcd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diff-Shadow-Global-guided-Diffusion-Model-for-Shadow-Removal"><a href="#Diff-Shadow-Global-guided-Diffusion-Model-for-Shadow-Removal" class="headerlink" title="Diff-Shadow: Global-guided Diffusion Model for Shadow Removal"></a>Diff-Shadow: Global-guided Diffusion Model for Shadow Removal</h2><p><strong>Authors:Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu</strong></p>
<p>We propose Diff-Shadow, a global-guided diffusion model for shadow removal. Previous transformer-based approaches can utilize global information to relate shadow and non-shadow regions but are limited in their synthesis ability and recover images with obvious boundaries. In contrast, diffusion-based methods can generate better content but they are not exempt from issues related to inconsistent illumination. In this work, we combine the advantages of diffusion models and global guidance to achieve shadow-free restoration. Specifically, we propose a parallel UNets architecture: 1) the local branch performs the patch-based noise estimation in the diffusion process, and 2) the global branch recovers the low-resolution shadow-free images. A Reweight Cross Attention (RCA) module is designed to integrate global contextual information of non-shadow regions into the local branch. We further design a Global-guided Sampling Strategy (GSS) that mitigates patch boundary issues and ensures consistent illumination across shaded and unshaded regions in the recovered image. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have demonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art methods, our method achieves a significant improvement in terms of PSNR, increasing from 32.33dB to 33.69dB on the ISTD dataset. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Diff-Shadowï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé˜´å½±å»é™¤çš„å…¨å±€å¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚ä¹‹å‰çš„åŸºäºtransformerçš„æ–¹æ³•å¯ä»¥åˆ©ç”¨å…¨å±€ä¿¡æ¯æ¥å…³è”é˜´å½±å’Œéé˜´å½±åŒºåŸŸï¼Œä½†å®ƒä»¬åœ¨åˆæˆèƒ½åŠ›æ–¹é¢æœ‰é™ï¼Œæ¢å¤çš„å›¾åƒå…·æœ‰æ˜æ˜¾çš„è¾¹ç•Œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆæ›´å¥½çš„å†…å®¹ï¼Œä½†å®ƒä»¬ä¹Ÿå­˜åœ¨å…‰ç…§ä¸ä¸€è‡´çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå…¨å±€å¼•å¯¼çš„ä¼˜ç‚¹ï¼Œå®ç°äº†æ— é˜´å½±çš„æ¢å¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¹¶è¡ŒU-Netæ¶æ„ï¼š1ï¼‰å±€éƒ¨åˆ†æ”¯æ‰§è¡Œæ‰©æ•£è¿‡ç¨‹ä¸­çš„åŸºäºè¡¥ä¸çš„å™ªå£°ä¼°è®¡ï¼›2ï¼‰å…¨å±€åˆ†æ”¯æ¢å¤æ— é˜´å½±çš„ä½åˆ†è¾¨ç‡å›¾åƒã€‚è®¾è®¡äº†ä¸€ç§é‡åŠ æƒäº¤å‰æ³¨æ„åŠ›ï¼ˆRCAï¼‰æ¨¡å—ï¼Œå°†éé˜´å½±åŒºåŸŸçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯é›†æˆåˆ°å±€éƒ¨åˆ†æ”¯ä¸­ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§å…¨å±€å¼•å¯¼é‡‡æ ·ç­–ç•¥ï¼ˆGSSï¼‰ï¼Œå¯ä»¥ç¼“è§£è¡¥ä¸è¾¹ç•Œé—®é¢˜ï¼Œå¹¶ç¡®ä¿æ¢å¤å›¾åƒä¸­çš„é˜´å½±å’Œéé˜´å½±åŒºåŸŸä¹‹é—´å…‰ç…§ä¸€è‡´ã€‚åœ¨ISTDã€ISTD+å’ŒSRDæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†Diff-Shadowçš„æœ‰æ•ˆæ€§ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PSNRæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨ISTDæ•°æ®é›†ä¸Šä»32.33dBæé«˜åˆ°33.69dBã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16214v2">PDF</a> Proceedings of the 39th Annual AAAI Conference on Artificial   Intelligence</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„é˜´å½±å»é™¤æŠ€æœ¯Diff-Shadowç ”ç©¶ã€‚ç»“åˆæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹å’Œå…¨å±€æŒ‡å¯¼ç­–ç•¥ï¼Œå®ç°äº†é˜´å½±å»é™¤çš„å›¾åƒæ¢å¤ã€‚é‡‡ç”¨å¹¶è¡ŒUNetsæ¶æ„ï¼Œå±€éƒ¨åˆ†æ”¯è¿›è¡ŒåŸºäºè¡¥ä¸çš„å™ªå£°ä¼°è®¡ï¼Œå…¨å±€åˆ†æ”¯æ¢å¤æ— é˜´å½±çš„ä½åˆ†è¾¨ç‡å›¾åƒã€‚è®¾è®¡é‡æƒäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆRCAï¼‰æ•´åˆéé˜´å½±åŒºåŸŸçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°å±€éƒ¨åˆ†æ”¯ï¼Œå¹¶è®¾è®¡å…¨å±€å¼•å¯¼é‡‡æ ·ç­–ç•¥ï¼ˆGSSï¼‰å‡å°‘è¡¥ä¸è¾¹ç•Œé—®é¢˜å¹¶ç¡®ä¿æ¢å¤å›¾åƒä¸­çš„é˜´å½±å’Œæœªé˜´å½±åŒºåŸŸçš„ç…§æ˜ä¸€è‡´æ€§ã€‚åœ¨ISTDã€ISTD+å’ŒSRDæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†Diff-Shadowçš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºæœ€æ–°æ–¹æ³•ï¼Œåœ¨ISTDæ•°æ®é›†ä¸Šçš„PSNRå€¼ä»32.33dBæ˜¾è‘—æé«˜è‡³33.69dBã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é˜´å½±å»é™¤æ–¹æ³•Diff-Shadowï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå…¨å±€æŒ‡å¯¼ç­–ç•¥çš„ä¼˜åŠ¿ã€‚</li>
<li>é‡‡ç”¨å¹¶è¡ŒUNetsæ¶æ„ï¼ŒåŒ…æ‹¬å±€éƒ¨å’Œå…¨å±€ä¸¤ä¸ªåˆ†æ”¯ï¼Œåˆ†åˆ«è¿›è¡Œå™ªå£°ä¼°è®¡å’Œæ— é˜´å½±å›¾åƒæ¢å¤ã€‚</li>
<li>è®¾è®¡äº†é‡æƒäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆRCAï¼‰ï¼Œç”¨äºæ•´åˆå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†å…¨å±€å¼•å¯¼é‡‡æ ·ç­–ç•¥ï¼ˆGSSï¼‰ï¼Œè§£å†³è¡¥ä¸è¾¹ç•Œé—®é¢˜ï¼Œç¡®ä¿ç…§æ˜ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†Diff-Shadowçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDiff-Shadowåœ¨ISTDæ•°æ®é›†ä¸Šçš„PSNRå€¼æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21cffbcdf280bbb4ae925b7841f9443c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d18380365954dc0e6678be5f91282d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-153ac815315c0be74528c371c6178cb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8d92231f049231c18e83eb699c5c33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-475125131725026b086e94094f3b962d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1e0713c8695103e4823d1d65078ed10.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ID-Sculpt-ID-aware-3D-Head-Generation-from-Single-In-the-wild-Portrait-Image"><a href="#ID-Sculpt-ID-aware-3D-Head-Generation-from-Single-In-the-wild-Portrait-Image" class="headerlink" title="ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait   Image"></a>ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait   Image</h2><p><strong>Authors:Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma</strong></p>
<p>While recent works have achieved great success on image-to-3D object generation, high quality and fidelity 3D head generation from a single image remains a great challenge. Previous text-based methods for generating 3D heads were limited by text descriptions and image-based methods struggled to produce high-quality head geometry. To handle this challenging problem, we propose a novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving their identities. Our work incorporates the identity information of the portrait image into three parts: 1) geometry initialization, 2) geometry sculpting, and 3) texture generation stages. Given a reference portrait image, we first align the identity features with text features to realize ID-aware guidance enhancement, which contains the control signals representing the face information. We then use the canny map, ID features of the portrait image, and a pre-trained text-to-normal&#x2F;depth diffusion model to generate ID-aware geometry supervision, and 3D-GAN inversion is employed to generate ID-aware geometry initialization. Furthermore, with the ability to inject identity information into 3D head generation, we use ID-aware guidance to calculate ID-aware Score Distillation (ISD) for geometry sculpting. For texture generation, we adopt the ID Consistent Texture Inpainting and Refinement which progressively expands the view for texture inpainting to obtain an initialization UV texture map. We then use the ID-aware guidance to provide image-level supervision for noisy multi-view images to obtain a refined texture map. Extensive experiments demonstrate that we can generate high-quality 3D heads with accurate geometry and texture from a single in-the-wild portrait image. </p>
<blockquote>
<p>è™½ç„¶è¿‘æœŸçš„å·¥ä½œåœ¨å›¾åƒåˆ°3Dç‰©ä½“çš„ç”Ÿæˆä¸Šå–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡å’Œé«˜ä¿çœŸåº¦çš„3Då¤´åƒä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¹‹å‰åŸºäºæ–‡æœ¬çš„æ–¹æ³•ç”Ÿæˆ3Då¤´åƒå—é™äºæ–‡æœ¬æè¿°ï¼Œè€ŒåŸºäºå›¾åƒçš„æ–¹æ³•éš¾ä»¥äº§ç”Ÿé«˜è´¨é‡çš„å¤´åƒå‡ ä½•ç»“æ„ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ID-Sculptï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„3Då¤´åƒï¼ŒåŒæ—¶ä¿ç•™å…¶èº«ä»½ç‰¹å¾ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†è‚–åƒå›¾åƒçš„èº«ä»½ä¿¡æ¯èå…¥ä¸‰ä¸ªé˜¶æ®µï¼š1ï¼‰å‡ ä½•åˆå§‹åŒ–ï¼Œ2ï¼‰å‡ ä½•é›•å¡‘ï¼Œ3ï¼‰çº¹ç†ç”Ÿæˆã€‚ç»™å®šä¸€ä¸ªå‚è€ƒè‚–åƒå›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æ–‡æœ¬ç‰¹å¾ä¸èº«ä»½ç‰¹å¾çš„å¯¹é½ï¼Œå®ç°IDæ„ŸçŸ¥å¼•å¯¼å¢å¼ºï¼Œå…¶ä¸­åŒ…å«ä»£è¡¨é¢éƒ¨ä¿¡æ¯çš„æ§åˆ¶ä¿¡å·ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Cannyåœ°å›¾ã€è‚–åƒå›¾åƒçš„èº«ä»½ç‰¹å¾ä»¥åŠé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°æ³•çº¿&#x2F;æ·±åº¦æ‰©æ•£æ¨¡å‹æ¥ç”ŸæˆIDæ„ŸçŸ¥çš„å‡ ä½•ç›‘ç£ï¼Œå¹¶åˆ©ç”¨3D-GANåè½¬æ¥ç”ŸæˆIDæ„ŸçŸ¥çš„å‡ ä½•åˆå§‹åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‘3Då¤´åƒç”Ÿæˆä¸­æ³¨å…¥èº«ä»½ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨IDæ„ŸçŸ¥æŒ‡å¯¼æ¥è®¡ç®—ç”¨äºå‡ ä½•é›•å¡‘çš„IDæ„ŸçŸ¥åˆ†æ•°è’¸é¦ï¼ˆISDï¼‰ã€‚å¯¹äºçº¹ç†ç”Ÿæˆï¼Œæˆ‘ä»¬é‡‡ç”¨IDä¸€è‡´çº¹ç†å¡«å……å’Œç»†åŒ–æ–¹æ³•ï¼Œé€æ­¥æ‰©å±•è§†å›¾ä»¥è¿›è¡Œçº¹ç†å¡«å……ï¼Œä»¥è·å¾—åˆå§‹UVçº¹ç†è´´å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨IDæ„ŸçŸ¥æŒ‡å¯¼ä¸ºå™ªå£°å¤šè§†è§’å›¾åƒæä¾›å›¾åƒçº§ç›‘ç£ï¼Œä»¥è·å¾—ç²¾ç»†çš„çº¹ç†è´´å›¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸€å¼ é‡å¤–çš„è‚–åƒå›¾åƒç”Ÿæˆé«˜è´¨é‡ã€å‡ ä½•å’Œçº¹ç†å‡†ç¡®çš„3Då¤´åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16710v2">PDF</a> Accepted by AAAI 2025; Project page:   <a target="_blank" rel="noopener" href="https://jinkun-hao.github.io/ID-Sculpt/">https://jinkun-hao.github.io/ID-Sculpt/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶ID-Sculptï¼Œç”¨äºä»å•å¹…å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´å¤´åƒï¼ŒåŒæ—¶ä¿ç•™å…¶èº«ä»½ç‰¹å¾ã€‚è¯¥æ¡†æ¶å°†èº«ä»½ä¿¡æ¯èå…¥ä¸‰ä¸ªé˜¶æ®µï¼šå‡ ä½•åˆå§‹åŒ–ã€å‡ ä½•é›•åˆ»å’Œçº¹ç†ç”Ÿæˆã€‚é€šè¿‡IDæ„ŸçŸ¥æŒ‡å¯¼å¢å¼ºã€Cannyåœ°å›¾ã€é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°æ³•çº¿&#x2F;æ·±åº¦æ‰©æ•£æ¨¡å‹ç­‰æŠ€æœ¯ï¼Œå®ç°äº†é«˜è´¨é‡çš„ä¸‰ç»´å¤´åƒç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ID-Sculptï¼Œæ—¨åœ¨è§£å†³ä»å•å¹…å›¾åƒç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´å¤´åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶å°†èº«ä»½ä¿¡æ¯èå…¥ä¸‰ä¸ªé˜¶æ®µï¼šå‡ ä½•åˆå§‹åŒ–ã€å‡ ä½•é›•åˆ»å’Œçº¹ç†ç”Ÿæˆã€‚</li>
<li>é€šè¿‡IDæ„ŸçŸ¥æŒ‡å¯¼å¢å¼ºï¼Œå®ç°äº†èº«ä»½ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾çš„èåˆã€‚</li>
<li>åˆ©ç”¨Cannyåœ°å›¾ã€é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°æ³•çº¿&#x2F;æ·±åº¦æ‰©æ•£æ¨¡å‹å’Œ3D-GANåè½¬ï¼Œç”ŸæˆIDæ„ŸçŸ¥çš„å‡ ä½•ç›‘ç£ã€‚</li>
<li>IDæ„ŸçŸ¥æŒ‡å¯¼ç”¨äºè®¡ç®—å‡ ä½•é›•åˆ»çš„IDæ„ŸçŸ¥åˆ†æ•°è’¸é¦ï¼ˆISDï¼‰ã€‚</li>
<li>é‡‡ç”¨IDä¸€è‡´çº¹ç†è¡¥å…¨ä¸ç»†åŒ–æ–¹æ³•ï¼Œé€æ­¥æ‰©å±•è§†å›¾è¿›è¡Œçº¹ç†è¡¥å…¨ï¼Œè·å¾—åˆå§‹UVçº¹ç†è´´å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba9419e23b446e089160b023d1057f2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c16ea30d0c3612840684653ecc9e653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-304dd237598c4c95fac0e6a7a6bcabe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42b4bb5107fd5bf1b41a3707a2f0fa03.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself"><a href="#Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself" class="headerlink" title="Guiding a Diffusion Model with a Bad Version of Itself"></a>Guiding a Diffusion Model with a Bad Version of Itself</h2><p><strong>Authors:Tero Karras, Miika Aittala, Tuomas KynkÃ¤Ã¤nniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine</strong></p>
<p>The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality. </p>
<blockquote>
<p>åœ¨å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä¸»è¦çš„å…³æ³¨ç‚¹åœ¨äºå›¾åƒè´¨é‡ã€ç»“æœå˜åŒ–çš„ç¨‹åº¦ä»¥åŠç»“æœå¦‚ä½•ç¬¦åˆç»™å®šçš„æ¡ä»¶ï¼Œä¾‹å¦‚ç±»åˆ«æ ‡ç­¾æˆ–æ–‡æœ¬æç¤ºã€‚æµè¡Œçš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹æ¥å¼•å¯¼æœ‰æ¡ä»¶æ¨¡å‹ï¼Œè¿™å¯¼è‡´äº†æç¤ºå¯¹é½æ›´å¥½å’Œå›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†ä»£ä»·æ˜¯å˜åŒ–å‡å°‘ã€‚è¿™äº›æ•ˆæœä¼¼ä¹å›ºæœ‰åœ°çº ç¼ åœ¨ä¸€èµ·ï¼Œå› æ­¤éš¾ä»¥æ§åˆ¶ã€‚æˆ‘ä»¬æ„å¤–åœ°å‘ç°ï¼Œé€šè¿‡ä½¿ç”¨æ¨¡å‹æœ¬èº«çš„æ›´å°ã€è®­ç»ƒè¾ƒå°‘çš„ç‰ˆæœ¬æ¥æŒ‡å¯¼ç”Ÿæˆï¼Œå¯ä»¥åœ¨ä¸æŸå®³å˜åŒ–é‡çš„åŒæ—¶è·å¾—å¯¹å›¾åƒè´¨é‡çš„åˆ†ç¦»æ§åˆ¶ã€‚è¿™å¯¼è‡´åœ¨ImageNetç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½¿ç”¨å…¬å¼€å¯ç”¨çš„ç½‘ç»œï¼Œä»¥64x64å’Œ512x512çš„åˆ†è¾¨ç‡åˆ†åˆ«è®¾ç½®äº†è®°å½•FIDå¾—åˆ†1.01å’Œ1.25ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿé€‚ç”¨äºæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¤§å¤§æé«˜äº†å…¶è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02507v3">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­çš„å…³é”®æ–¹é¢ï¼Œå¦‚å›¾åƒè´¨é‡ã€ç»“æœå¤šæ ·æ€§å’Œå¯¹é½ç‰¹å®šæ¡ä»¶çš„èƒ½åŠ›ã€‚é€šè¿‡æŒ‡å¯¼æ¨¡å‹çš„æ–¹æ³•å¯ä»¥æ”¹è¿›æ¨¡å‹è¡¨ç°ï¼Œå³åˆ©ç”¨æ— æ¡ä»¶çš„æ¨¡å‹å¼•å¯¼æ¡ä»¶æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æç¤ºå¯¹é½å’Œæ›´é«˜çš„å›¾åƒè´¨é‡ï¼Œä½†åŒæ—¶ä¹Ÿé™ä½äº†å¤šæ ·æ€§ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°å¯ä»¥é€šè¿‡åˆ©ç”¨è¾ƒå°çš„æœªå®Œå…¨è®­ç»ƒç‰ˆæœ¬æ¨¡å‹è¿›è¡Œå¼•å¯¼æ¥åˆ†ç¦»æ§åˆ¶å›¾åƒè´¨é‡è€Œä¸ç‰ºç‰²å¤šæ ·æ€§ï¼Œè¿™åœ¨ImageNetç”Ÿæˆä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨å…¬å¼€ç½‘ç»œä¸Šè®¾ç½®äº†æ–°çš„FIDè®°å½•ã€‚æ­¤æ–¹æ³•åŒæ ·é€‚ç”¨äºæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¯å¤§å¹…æé«˜æ¨¡å‹è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒå…³æ³¨ç‚¹åŒ…æ‹¬å›¾åƒè´¨é‡ã€ç»“æœå¤šæ ·æ€§å’Œä¸ç»™å®šæ¡ä»¶å¯¹é½çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹å¼•å¯¼æ¡ä»¶æ¨¡å‹çš„æ–¹æ³•å¯ä»¥åŒæ—¶æé«˜æç¤ºå¯¹é½å’Œå›¾åƒè´¨é‡ï¼Œä½†ä¼šé™ä½å¤šæ ·æ€§ã€‚</li>
<li>åˆ©ç”¨è¾ƒå°çš„æœªå®Œå…¨è®­ç»ƒç‰ˆæœ¬æ¨¡å‹è¿›è¡Œå¼•å¯¼å¯ä»¥åˆ†ç¦»æ§åˆ¶å›¾åƒè´¨é‡è€Œä¸ç‰ºç‰²å¤šæ ·æ€§ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨ImageNetç”Ÿæˆä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶è®¾ç½®äº†æ–°çš„FIDè®°å½•ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥å¤§å¹…æé«˜æ¨¡å‹è´¨é‡ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥æ¥æ§åˆ¶å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³é”®å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffbb79443270ac0d20c8ec13422e441e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99fffc68294fd93fed46bad9b1126352.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  Preventing Local Pitfalls in Vector Quantization via Optimal Transport
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d49716e6574422c1cdbd5ff1916f0448.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  LiDAR-RT Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
