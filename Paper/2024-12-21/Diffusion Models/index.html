<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-21  LeviTor 3D Trajectory Oriented Image-to-Video Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-750202e1a98a34f8ad6157aefde1aebd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-21-更新"><a href="#2024-12-21-更新" class="headerlink" title="2024-12-21 更新"></a>2024-12-21 更新</h1><h2 id="LeviTor-3D-Trajectory-Oriented-Image-to-Video-Synthesis"><a href="#LeviTor-3D-Trajectory-Oriented-Image-to-Video-Synthesis" class="headerlink" title="LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis"></a>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</h2><p><strong>Authors:Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang</strong></p>
<p>The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: <a target="_blank" rel="noopener" href="https://ppetrichor.github.io/levitor.github.io/">https://ppetrichor.github.io/levitor.github.io/</a> </p>
<blockquote>
<p>基于拖动的交互方式的直观性使其在图像到视频合成中控制物体轨迹的应用越来越广泛。然而，在二维空间执行拖动操作的现有方法在应对平面外移动时通常面临模糊性。在这项工作中，我们通过引入一个新的维度来增强交互，即深度维度，使用户可以为轨迹上的每个点分配相对深度。通过这种方式，我们的新交互范式不仅继承了二维拖动的便利性，还促进了在三维空间中的轨迹控制，从而扩大了创意范围。我们提出了一种在图像到视频合成中进行三维轨迹控制的开创性方法，通过将对象蒙版抽象为几个聚类点来实现。这些点伴随深度信息和实例信息，最终作为控制信号输入到视频扩散模型中。大量实验验证了我们的方法（称为LeviTor）在生成从静态图像生成的逼真视频时精确控制物体运动的有效性。项目页面：<a target="_blank" rel="noopener" href="https://ppetrichor.github.io/levitor.github.io/">https://ppetrichor.github.io/levitor.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15214v1">PDF</a> Project page available at   <a target="_blank" rel="noopener" href="https://ppetrichor.github.io/levitor.github.io/">https://ppetrichor.github.io/levitor.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>采用拖动手势交互，用户能够为图像合成视频中对象的运动轨迹指定相对深度信息，突破了原有二维拖动方法的局限性，拓宽了图像到视频合成的创造力范畴。LeviTor方法通过抽象对象掩膜为几个簇点，结合深度信息和实例信息，作为视频扩散模型的控制信号，有效精确操控对象运动，生成逼真的视频。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>拖动手势交互在图像到视频合成中广泛应用于控制对象轨迹。</li>
<li>在处理出平面移动时，现有二维空间拖动方法面临模糊性挑战。</li>
<li>通过引入深度维度信息，用户可以为轨迹上的每个点指定相对深度。</li>
<li>新颖的交互模式继承了二维拖动的便捷性，并促进了三维空间中的轨迹控制。</li>
<li>提出了一种名为LeviTor的方法，通过抽象对象掩膜为几个簇点进行三维轨迹控制。</li>
<li>该方法结合了深度信息和实例信息，提高了精确操控对象运动的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-918cd2a01cde1ccf70eb84368563135b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f7b2587b9c41e443b5edd7fe948d58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0166670a70a57ad0673a0faec21cce21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2b87dd0c32b8518ed0cbabcad25315.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca83068fa1595d950978c819f177930b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Flowing-from-Words-to-Pixels-A-Framework-for-Cross-Modality-Evolution"><a href="#Flowing-from-Words-to-Pixels-A-Framework-for-Cross-Modality-Evolution" class="headerlink" title="Flowing from Words to Pixels: A Framework for Cross-Modality Evolution"></a>Flowing from Words to Pixels: A Framework for Cross-Modality Evolution</h2><p><strong>Authors:Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh</strong></p>
<p>Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal &#x2F; intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation. </p>
<blockquote>
<p>扩散模型及其泛化形式——流匹配，在媒体生成领域产生了显著影响。在这里，传统的方法是学习从简单的高斯噪声源分布到目标媒体分布的复杂映射。对于跨模态任务（如文本到图像生成），在模型中引入条件机制的同时，也学习从噪声到图像的相同映射。流匹配的一个关键且迄今为止相对未被探索的特点是，与扩散模型不同，它不受源分布必须是噪声的限制。因此，在本文中，我们提出了一个范式转变，并提出了一个问题：我们是否能训练流匹配模型来学习从一种模态的分布到另一种模态的分布的直接映射，从而避免对噪声分布和条件机制的需求。我们提出了一个通用且简单的跨模态流匹配框架CrossFlow。我们强调了将变编码器应用于输入数据的重要性，并介绍了一种实现无分类器引导的方法。令人惊讶的是，对于文本到图像的任务，使用没有交叉注意力的普通变压器的CrossFlow略微优于标准流匹配，我们证明它在训练步骤和模型规模上表现更好，同时允许有趣的潜在运算，从而在输出空间产生语义上有意义的编辑。为了证明我们的方法的一般性，我们还表明CrossFlow在各种跨模态&#x2F;同模态映射任务上表现与最新技术不相上下或表现更好，如图像描述、深度估计和图像超分辨率。我们希望这篇论文能促进跨模态媒体生成的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15213v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cross-flow.github.io/">https://cross-flow.github.io/</a></p>
<p><strong>摘要</strong></p>
<p>扩散模型及其泛化——流匹配在媒体生成领域产生了显著影响。本文突破了传统方法，不再局限于从简单的高斯噪声源分布到目标媒体分布的学习映射。针对跨模态任务如文本到图像生成，本文提出了一个新的视角，即是否可以直接训练流匹配模型从一种模态的分布学习到另一种模态的分布，而无需噪声分布和调节机制。本文提出了一个通用的简单框架CrossFlow进行跨模态流匹配，并强调了应用变分编码器于输入数据的重要性，引入了一种无分类器引导方法。令人惊讶的是，对于文本到图像任务，使用常规Transformer的CrossFlow在不使用交叉注意的情况下略微优于标准流匹配，且随着训练步骤和模型规模的扩大，表现更好。此外，它还允许有趣的潜在算术运算，在输出空间中产生语义上有意义的编辑。为证明我们方法的通用性，我们还展示了CrossFlow在各种跨模态&#x2F;单模态映射任务上的表现与或优于当前最新技术，如图像描述、深度估计和图像超分辨率等。本文旨在为跨模态媒体生成的发展加速做出贡献。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型和流匹配在媒体生成领域具有显著影响。</li>
<li>本文提出了一种新的方法——CrossFlow框架进行跨模态流匹配。</li>
<li>CrossFlow突破了传统扩散模型的限制，可以直接学习不同模态之间的映射，无需噪声分布和调节机制。</li>
<li>变分编码器在输入数据中的应用对于提高模型性能至关重要。</li>
<li>CrossFlow具有无分类器引导的能力。</li>
<li>在文本到图像任务上，CrossFlow的表现略优于标准流匹配，且具有良好的扩展性。</li>
<li>CrossFlow在各种跨模态和单模态映射任务上的表现具有竞争力或优于当前技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e040faeb01cb2a62876b64ee7c397abd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f838c057d689c4e5dd2b0ac92f6aa99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d9d7f6a2a700a42db59e1ed3e3d2e1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1b2da9c68466911298144044693595d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-Multiview-Relighting-for-3D-Reconstruction-under-Extreme-Illumination-Variation"><a href="#Generative-Multiview-Relighting-for-3D-Reconstruction-under-Extreme-Illumination-Variation" class="headerlink" title="Generative Multiview Relighting for 3D Reconstruction under Extreme   Illumination Variation"></a>Generative Multiview Relighting for 3D Reconstruction under Extreme   Illumination Variation</h2><p><strong>Authors:Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin</strong></p>
<p>Reconstructing the geometry and appearance of objects from photographs taken in different environments is difficult as the illumination and therefore the object appearance vary across captured images. This is particularly challenging for more specular objects whose appearance strongly depends on the viewing direction. Some prior approaches model appearance variation across images using a per-image embedding vector, while others use physically-based rendering to recover the materials and per-image illumination. Such approaches fail at faithfully recovering view-dependent appearance given significant variation in input illumination and tend to produce mostly diffuse results. We present an approach that reconstructs objects from images taken under different illuminations by first relighting the images under a single reference illumination with a multiview relighting diffusion model and then reconstructing the object’s geometry and appearance with a radiance field architecture that is robust to the small remaining inconsistencies among the relit images. We validate our proposed approach on both synthetic and real datasets and demonstrate that it greatly outperforms existing techniques at reconstructing high-fidelity appearance from images taken under extreme illumination variation. Moreover, our approach is particularly effective at recovering view-dependent “shiny” appearance which cannot be reconstructed by prior methods. </p>
<blockquote>
<p>从在不同环境下拍摄的照片重建物体的几何形状和外观是一项艰巨的任务，因为光照和因此物体的外观在不同捕获的图像中会有所变化。这对于具有强烈方向性外观的物体来说尤其具有挑战性。一些先前的方法使用每张图像的嵌入向量对图像中的外观变化进行建模，而其他方法则使用基于物理的渲染来恢复材料和每张图像的光照。这些方法在输入光照存在显著差异的情况下无法忠实恢复与视图相关的外观，并且往往产生大部分扩散结果。我们提出了一种方法，通过首先使用多视角重新照明扩散模型在单一参考光照下重新照明图像，然后从不同的光照条件下拍摄的照片重建物体的几何形状和外观。我们使用一种对重新照明图像之间剩余的小不一致性具有鲁棒性的辐射场架构来实现这一目标。我们在合成数据集和真实数据集上验证了我们的方法，并证明它在从极端光照变化条件下拍摄的照片重建高保真外观方面大大优于现有技术。此外，我们的方法在恢复无法由先前方法重建的与视图相关的“闪亮”外观方面特别有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15211v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://relight-to-reconstruct.github.io/">https://relight-to-reconstruct.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>该文描述了一种利用多视角重光照扩散模型（multiview relighting diffusion model）从不同光照条件下拍摄的照片重建物体几何和外观的方法。通过首先在单一参考光照下重新照明图像，然后采用辐射场架构（radiance field architecture）恢复物体几何和外观，提高了在不同光照条件下拍摄物体图像的高保真重建能力，特别是对于“镜面光泽”外观的重建效果尤为显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文中提出了一种从不同光照条件下的照片重建物体几何和外观的新方法。</li>
<li>方法基于多视角重光照扩散模型进行图像重新照明。</li>
<li>通过在单一参考光照下重新照明图像，减少了光照变化对物体外观重建的影响。</li>
<li>采用辐射场架构恢复物体几何和外观，增强了重建结果的稳健性。</li>
<li>该方法对极端光照变化下的图像重建效果显著提升。</li>
<li>该方法特别适用于重建“镜面光泽”外观，这是先前方法无法实现的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15211">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9f52fe6a1a861a9700748ab9899a4438.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f265a88d8d882d78493ebada027aa56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd92031639e05668efb6a0d04bc83dd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b96613d67d132d999ae62350d2916e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2636fc6037dca8f4086a136162415fb1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Jet-A-Modern-Transformer-Based-Normalizing-Flow"><a href="#Jet-A-Modern-Transformer-Based-Normalizing-Flow" class="headerlink" title="Jet: A Modern Transformer-Based Normalizing Flow"></a>Jet: A Modern Transformer-Based Normalizing Flow</h2><p><strong>Authors:Alexander Kolesnikov, André Susano Pinto, Michael Tschannen</strong></p>
<p>In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models. </p>
<blockquote>
<p>过去，标准化生成流已经涌现为一类有前景的自然图像生成模型。这种模型具有许多建模优势：能够高效地计算输入数据的对数似然，生成速度快，整体结构简单。标准化流一直是活跃研究的主题，但后来因其样本的视觉质量与其他模型类别相比不具竞争力而不再受欢迎，如GAN、基于VQ-VAE的方法或扩散模型。在本文中，我们重新审视基于耦合的标准化流模型的设计，通过仔细消除先前的设计选择并使用基于视觉转换器架构的计算块（而不是卷积神经网络）。因此，我们凭借更简单的架构实现了最先进的定量和定性性能。虽然整体视觉质量仍然落后于当前最先进的模型，我们认为强大的标准化流模型可以作为更强大的生成模型的构建组件，有助于推动研究前沿。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15129v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文重新审视了基于耦合的归一化流模型的设计，通过对先前设计选择进行仔细消融，并使用基于Vision Transformer架构的计算块，而非卷积神经网络，实现了先进的质量和性能。虽然整体视觉质量仍落后于当前最先进的模型，但我们认为强大的归一化流模型可以作为更强大生成模型的构建组件，有助于推动研究前沿。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>归一化流模型是一种具有多个建模优势的生成模型，包括计算输入数据对数似然的高效性、快速生成和简单结构。</li>
<li>之前的归一化流模型在样本的视觉质量上与其他模型类相比不具竞争力，如GANs、VQ-VAE方法和扩散模型。</li>
<li>本文通过重新审视基于耦合的归一化流模型的设计，使用Vision Transformer架构的计算块替代卷积神经网络，实现了先进的质量和性能。</li>
<li>尽管整体视觉质量仍待提高，但作者认为强大的归一化流模型可以作为构建更强大生成模型的组件，有助于推动研究前沿。</li>
<li>消融先前的设计选择是改进归一化流模型的关键步骤之一。</li>
<li>使用基于Vision Transformer的计算块在归一化流模型中产生了积极的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-11360f92701d5b6c72bbfc37e5461247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb05969f79f097db1ff2bf24cb65f840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c239ce6bc86d40cec105998ab124cd08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-248c7a67d0d6e4e31f776a83a7e5ba0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b371b3a81df48fcd55c960d1e4c6b92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1613bb08e98c13c95a34f9af5e8d9b9d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space"><a href="#DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space" class="headerlink" title="DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space"></a>DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space</h2><p><strong>Authors:Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Albert Ali Salah, Itir Onal Ertugrul</strong></p>
<p>This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to high-resolution generation without using the latent diffusion paradigm. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why &#96;image diffusion can be seen as spectral autoregression’, bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is at \url{<a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff%7D">https://github.com/forever208/DCTdiff}</a>. </p>
<blockquote>
<p>本文探讨了频率空间的图像建模，并介绍了DCTdiff这一端到端的扩散生成范式，该范式在离散余弦变换（DCT）空间有效地建模图像。我们研究了DCTdiff的设计空间，揭示了关键的设计因素。在不同框架（UViT、DiT）、生成任务和多种扩散采样器上的实验表明，DCTdiff在生成质量和训练效率方面优于基于像素的扩散模型。值得注意的是，DCTdiff可以无缝地扩展到高分辨率生成，而无需使用潜在扩散范式。最后，我们说明了DCT图像建模的几个有趣特性。例如，我们提供了理论证明，解释了为什么“图像扩散可以被视为谱自回归”，从而缩小了扩散模型和自回归模型之间的差距。DCTdiff的有效性和所介绍的特性表明，频率空间的图像建模具有广阔的发展前景。代码位于\url{<a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff%7D%E3%80%82">https://github.com/forever208/DCTdiff}。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15032v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DCTdiff，这是一种在离散余弦变换（DCT）空间中进行图像建模的端到端扩散生成范式。文章探讨了DCTdiff的设计空间，揭示了关键设计因素，并通过实验证明其在生成质量和训练效率上优于基于像素的扩散模型。DCTdiff能够无缝地扩展到高分辨率生成，而无需使用潜在扩散方法。此外，文章还阐述了DCT图像建模的几个有趣特性，如从理论上证明了“图像扩散可以被视为谱自回归”，为扩散和自回归模型之间建立了联系。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCTdiff是一种在离散余弦变换（DCT）空间进行图像建模的扩散生成方法。</li>
<li>DCTdiff的设计空间包括关键设计因素，这些因素对于提高生成质量和训练效率至关重要。</li>
<li>DCTdiff在生成质量和训练效率上优于基于像素的扩散模型。</li>
<li>DCTdiff能够无缝扩展到高分辨率图像生成。</li>
<li>DCTdiff不使用潜在扩散方法。</li>
<li>文章从理论上证明了“图像扩散可以被视为谱自回归”，这是扩散和自回归模型之间的一个重要联系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a0a05d4c2a356d0f975da60371dafc30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fae6abffbea645dc4cb970676696d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4de03d9b84fbf2f542072fb216860b3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MagicNaming-Consistent-Identity-Generation-by-Finding-a-“Name-Space”-in-T2I-Diffusion-Models"><a href="#MagicNaming-Consistent-Identity-Generation-by-Finding-a-“Name-Space”-in-T2I-Diffusion-Models" class="headerlink" title="MagicNaming: Consistent Identity Generation by Finding a “Name Space” in   T2I Diffusion Models"></a>MagicNaming: Consistent Identity Generation by Finding a “Name Space” in   T2I Diffusion Models</h2><p><strong>Authors:Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang</strong></p>
<p>Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable of generating famous persons by simply referring to their names. Is it possible to make such models generate generic identities as simple as the famous ones, e.g., just use a name? In this paper, we explore the existence of a “Name Space”, where any point in the space corresponds to a specific identity. Fortunately, we find some clues in the feature space spanned by text embedding of celebrities’ names. Specifically, we first extract the embeddings of celebrities’ names in the Laion5B dataset with the text encoder of diffusion models. Such embeddings are used as supervision to learn an encoder that can predict the name (actually an embedding) of a given face image. We experimentally find that such name embeddings work well in promising the generated image with good identity consistency. Note that like the names of celebrities, our predicted name embeddings are disentangled from the semantics of text inputs, making the original generation capability of text-to-image models well-preserved. Moreover, by simply plugging such name embeddings, all variants (e.g., from Civitai) derived from the same base model (i.e., SDXL) readily become identity-aware text-to-image models. Project homepage: \url{<a target="_blank" rel="noopener" href="https://magicfusion.github.io/MagicNaming/%7D">https://magicfusion.github.io/MagicNaming/}</a>. </p>
<blockquote>
<p>大规模文本到图像的扩散模型（例如DALL-E、SDXL）能够通过简单地提及名字生成名人图像。是否可能使这类模型生成像名人一样简单的普通身份呢？例如，仅仅使用一个名字？在这篇论文中，我们探索了“名称空间”的存在，该空间中的每个点都对应一个特定的身份。幸运的是，我们在由名人名字的文本嵌入所构成的特征空间中找到了一些线索。具体来说，我们首先使用扩散模型的文本编码器提取Laion5B数据集的名人名字的嵌入。这些嵌入用作监督学习，以训练一个能够对给定的面部图像进行名称预测（实际上是嵌入）的编码器。我们通过实验发现，这样的名称嵌入在保持生成的图像具有良好的身份一致性方面效果很好。值得注意的是，与名人的名字一样，我们预测的名称嵌入与文本输入的语义是分开的，使得文本到图像模型的原始生成能力得到了很好的保留。此外，通过简单地插入这样的名称嵌入，基于同一基础模型（即SDXL）的所有变体（例如来自Civitai）都能轻易成为具有身份意识的文本到图像模型。项目主页：<a target="_blank" rel="noopener" href="https://magicfusion.github.io/MagicNaming/">https://magicfusion.github.io/MagicNaming/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14902v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文探索了文本到图像的大尺度扩散模型中的“名称空间”的存在性。研究通过提取Laion5B数据集中名人名称的文本嵌入，学习了一个能够根据面部图像预测名称的编码器。实验表明，这些名称嵌入能够确保生成的图像具有良好的身份一致性。此外，该研究方法能够使同一基础模型（如SDXL）的所有变体（如Civitai）轻易成为具有身份识别功能的文本到图像模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像的大尺度扩散模型能够生成特定名称的图像，本研究探讨了是否也能生成普通名称的图像。</li>
<li>研究发现了“名称空间”，其中空间的每个点都对应一个特定的身份。</li>
<li>通过提取Laion5B数据集中名人名称的文本嵌入，学习了一个预测给定面部图像名称的编码器。</li>
<li>名称嵌入在保持身份一致性的同时，不改变模型的原始文本到图像的生成能力。</li>
<li>名称嵌入技术可以应用于同一基础模型的变体，使其成为具有身份识别功能的文本到图像模型。</li>
<li>实验表明，这种技术在实际应用中表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14902">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8b834d87b4759bce05bfe6cc44a2e2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29d7fad3c6446210891aff905765cd9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f0c2768bc886f60032f8053ffa9fe7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239bcacd81f3cbb65c621368cc818388.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations"><a href="#Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations" class="headerlink" title="Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations"></a>Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations</h2><p><strong>Authors:Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen</strong></p>
<p>Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8% increase in success rates for complex real-world dexterous manipulation tasks. </p>
<blockquote>
<p>近期机器人技术的进步主要集中在开发能够执行多种任务的一般策略上。通常，这些策略会使用预训练的视觉编码器来捕获当前观察中的关键信息。然而，以前训练的视觉编码器主要依赖于两图像对比学习或单图像重建，无法完美捕获对实体任务至关重要的序列信息。最近，视频扩散模型（VDMs）已经展现出准确预测未来图像序列的能力，表现出对物理动态的良好理解。受到VDMs强大视觉预测能力的启发，我们假设它们天生就具有反映物理世界演变的视觉表征，我们称之为预测性视觉表征。基于这一假设，我们提出了视频预测策略（VPP），这是一种以VDMs的预测性视觉表征为条件的一般性机器人策略。为了进一步增强这些表征，我们融入了多样化的人类或机器人操作数据集，采用统一的视频生成训练目标。VPP在两个模拟基准测试和两个真实世界基准测试中均始终优于现有方法。值得一提的是，与之前的最新技术相比，它在Calvin ABC-D基准测试中实现了28.1%的相对改进，并在复杂的真实世界精细操作任务中成功率提高了28.8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14803v1">PDF</a> The first two authors contribute equally. Project Page at   <a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io/">https://video-prediction-policy.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>基于视频扩散模型（VDMs）的预测视觉表征，提出视频预测策略（VPP），结合多种数据集和目标，提升机器人执行多种任务的能力，在模拟和真实世界基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期机器人技术进展集中于开发能执行多种任务的通用策略。</li>
<li>通用策略常使用预训练的视觉编码器捕捉当前观察信息。</li>
<li>以往的视觉编码器在捕捉实体任务所需的序列信息方面存在缺陷。</li>
<li>视频扩散模型（VDMs）能准确预测未来图像序列，理解物理动态。</li>
<li>VDMs具有预测视觉表征，即反映物理世界演变的内在能力。</li>
<li>基于此，提出视频预测策略（VPP），结合VDMs的预测视觉表征，构建通用机器人策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a0633cab27a1dab8bc8deae652cb614b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd64e25d03d7b9d8221e2740f77f2d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746a0ed8f218c569b507d8cde6f6f142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ec1419d9d6845568c183d648993c172.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Event-assisted-12-stop-HDR-Imaging-of-Dynamic-Scene"><a href="#Event-assisted-12-stop-HDR-Imaging-of-Dynamic-Scene" class="headerlink" title="Event-assisted 12-stop HDR Imaging of Dynamic Scene"></a>Event-assisted 12-stop HDR Imaging of Dynamic Scene</h2><p><strong>Authors:Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue</strong></p>
<p>High dynamic range (HDR) imaging is a crucial task in computational photography, which captures details across diverse lighting conditions. Traditional HDR fusion methods face limitations in dynamic scenes with extreme exposure differences, as aligning low dynamic range (LDR) frames becomes challenging due to motion and brightness variation. In this work, we propose a novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera system with an event camera and an RGB camera. The event camera provides temporally dense, high dynamic range signals that improve alignment between LDR frames with large exposure differences, reducing ghosting artifacts caused by motion. Also, a real-world finetuning strategy is proposed to increase the generalization of alignment module on real-world events. Additionally, we introduce a diffusion-based fusion module that incorporates image priors from pre-trained diffusion models to address artifacts in high-contrast regions and minimize errors from the alignment process. To support this work, we developed the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized event signals, and validated our approach on both simulated and real-world data. Extensive experiments demonstrate that our method achieves state-of-the-art performance, successfully extending HDR imaging to 12 stops in dynamic scenes. </p>
<blockquote>
<p>高动态范围（HDR）成像是计算摄影中的一项重要任务，能够在不同的光照条件下捕捉细节。传统的HDR融合方法在动态场景中存在极限，特别是在极端曝光差异的情况下，由于运动和亮度变化，对齐低动态范围（LDR）帧变得具有挑战性。在这项工作中，我们提出了一种用于动态场景的12档HDR成像新方法，利用配备事件相机和RGB相机的双相机系统进行实现。事件相机提供时间密集、高动态范围的信号，改善了具有较大曝光差异的LDR帧之间的对齐，减少了由于运动造成的鬼影伪影。此外，我们还提出了一种现实世界的微调策略，以提高对齐模块在现实事件中的通用性。另外，我们引入了一个基于扩散的融合模块，该模块结合了预训练扩散模型的图像先验信息，以解决高对比度区域的伪影问题，并最小化对齐过程中的误差。为了支持这项工作，我们开发了ESHDR数据集，这是第一个具有同步事件信号的12档HDR成像数据集，并在模拟和真实数据上验证了我们的方法。大量实验表明，我们的方法达到了最先进的性能，成功地将HDR成像扩展到动态的12档场景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14705v1">PDF</a> Project page:   <a target="_blank" rel="noopener" href="https://openimaginglab.github.io/Event-Assisted-12stops-HDR/">https://openimaginglab.github.io/Event-Assisted-12stops-HDR/</a></p>
<p><strong>Summary</strong>：</p>
<p>本文提出了一种基于双摄像头系统（事件摄像头和RGB摄像头）的12档高动态范围（HDR）成像新方法，用于动态场景的HDR成像。该方法利用事件摄像头提供的时间密集、高动态范围的信号，改进了不同曝光度之间的LDR帧对齐，减少了因运动产生的鬼影伪影。此外，引入了一个基于扩散的融合模块，利用预训练的扩散模型的图像先验信息来解决高对比度区域的伪影问题并减少对齐过程中的误差。为了支持该研究，开发了同步事件信号的ESHDR数据集，并在模拟和真实数据上验证了该方法的有效性。该方法成功将HDR成像扩展到动态场景的12档。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了基于双摄像头系统（事件摄像头和RGB摄像头）的12档高动态范围（HDR）成像方法。</li>
<li>利用事件摄像头提供的时间密集、高动态范围的信号改进LDR帧对齐。</li>
<li>通过采用扩散融合模块解决了高对比度区域的伪影问题并减少了误差。</li>
<li>采用同步事件信号的ESHDR数据集用于验证方法的有效性。</li>
<li>方法成功将HDR成像扩展到动态场景的12档性能表现优异。</li>
<li>提出了一种现实世界的微调策略，提高了对齐模块在现实事件中的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14705">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f94c8462581460e81a710b59cd576899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-986fd87a39cad9732ad3ea5e237d3230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ccdb37ec8b303ecf0f87382f372f1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e53ee01757d4f411c3eb2f99f8aff29.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unified-Image-Restoration-and-Enhancement-Degradation-Calibrated-Cycle-Reconstruction-Diffusion-Model"><a href="#Unified-Image-Restoration-and-Enhancement-Degradation-Calibrated-Cycle-Reconstruction-Diffusion-Model" class="headerlink" title="Unified Image Restoration and Enhancement: Degradation Calibrated Cycle   Reconstruction Diffusion Model"></a>Unified Image Restoration and Enhancement: Degradation Calibrated Cycle   Reconstruction Diffusion Model</h2><p><strong>Authors:Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou</strong></p>
<p>Image restoration and enhancement are pivotal for numerous computer vision applications, yet unifying these tasks efficiently remains a significant challenge. Inspired by the iterative refinement capabilities of diffusion models, we propose CycleRDM, a novel framework designed to unify restoration and enhancement tasks while achieving high-quality mapping. Specifically, CycleRDM first learns the mapping relationships among the degraded domain, the rough normal domain, and the normal domain through a two-stage diffusion inference process. Subsequently, we transfer the final calibration process to the wavelet low-frequency domain using discrete wavelet transform, performing fine-grained calibration from a frequency domain perspective by leveraging task-specific frequency spaces. To improve restoration quality, we design a feature gain module for the decomposed wavelet high-frequency domain to eliminate redundant features. Additionally, we employ multimodal textual prompts and Fourier transform to drive stable denoising and reduce randomness during the inference process. After extensive validation, CycleRDM can be effectively generalized to a wide range of image restoration and enhancement tasks while requiring only a small number of training samples to be significantly superior on various benchmarks of reconstruction quality and perceptual quality. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/hejh8/CycleRDM">https://github.com/hejh8/CycleRDM</a>. </p>
<blockquote>
<p>图像修复和增强对于许多计算机视觉应用至关重要，但是如何有效地统一这些任务仍然是一个巨大挑战。受扩散模型迭代细化能力的启发，我们提出了CycleRDM，这是一个旨在统一修复和增强任务的同时实现高质量映射的新型框架。具体来说，CycleRDM首先通过两阶段扩散推理过程学习退化域、粗略正常域和正常域之间的映射关系。随后，我们通过离散小波变换将最终的校准过程转移到小波低频域，利用任务特定的频率空间，从频率域的角度进行精细校准。为了提高恢复质量，我们为分解后的小波高频域设计了特征增益模块，以消除冗余特征。此外，我们还采用多模式文本提示和傅里叶变换来驱动稳定的去噪，减少推理过程中的随机性。经过广泛验证，CycleRDM可以有效地推广到各种图像修复和增强任务，并且只需要少量训练样本就能在重建质量和感知质量的各种基准测试中表现出卓越性能。源代码将在<a target="_blank" rel="noopener" href="https://github.com/hejh8/CycleRDM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/hejh8/CycleRDM上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>循环RDM框架利用扩散模型的迭代优化能力，实现了图像修复与增强的统一。它通过两阶段扩散推理学习退化域、粗略正常域和正常域之间的映射关系，并在小波低频域进行精细校准，同时消除小波高频域中的冗余特征，提高图像修复质量。此外，该框架采用多模式文本提示和傅里叶变换，以实现稳定的去噪和减少推理过程中的随机性。总体而言，CycleRDM可广泛应用于多种图像修复和增强任务，并在重建质量和感知质量方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CycleRDM框架利用扩散模型的迭代优化能力，有效统一了图像修复和增强任务。</li>
<li>通过两阶段扩散推理，学习退化域、粗略正常域和正常域之间的映射关系。</li>
<li>在小波低频域进行精细校准，利用任务特定频率空间提高修复质量。</li>
<li>设计特征增益模块，消除小波高频域中的冗余特征。</li>
<li>采用多模式文本提示和傅里叶变换，实现稳定去噪，减少推理过程中的随机性。</li>
<li>CycleRDM可广泛应用于多种图像修复和增强任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-531fc125da524005318142c970a1894d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7578ae45a38113ab0777eea0ce68ee45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4710a646aa48bd335d45bf0bd35b8a8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Qua-2-SeDiMo-Quantifiable-Quantization-Sensitivity-of-Diffusion-Models"><a href="#Qua-2-SeDiMo-Quantifiable-Quantization-Sensitivity-of-Diffusion-Models" class="headerlink" title="Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models"></a>Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models</h2><p><strong>Authors:Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu</strong></p>
<p>Diffusion Models (DM) have democratized AI image generation through an iterative denoising process. Quantization is a major technique to alleviate the inference cost and reduce the size of DM denoiser networks. However, as denoisers evolve from variants of convolutional U-Nets toward newer Transformer architectures, it is of growing importance to understand the quantization sensitivity of different weight layers, operations and architecture types to performance. In this work, we address this challenge with Qua$^2$SeDiMo, a mixed-precision Post-Training Quantization framework that generates explainable insights on the cost-effectiveness of various model weight quantization methods for different denoiser operation types and block structures. We leverage these insights to make high-quality mixed-precision quantization decisions for a myriad of diffusion models ranging from foundational U-Nets to state-of-the-art Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit, 3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$, PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our weight-quantization configurations with 6-bit activation quantization and outperform existing approaches in terms of quantitative metrics and generative image quality. </p>
<blockquote>
<p>扩散模型（DM）通过迭代去噪过程实现了AI图像生成的普及。量化是一种主要技术，用于降低推理成本并缩小DM去噪网络的规模。然而，随着去噪器从卷积U-Net的变体向更新的Transformer架构发展，了解不同权重层、操作和架构类型对性能的量化敏感性变得越来越重要。在这项工作中，我们通过Qua$^2$SeDiMo来解决这一挑战，这是一个混合精度训练后量化框架，为各种模型权重量化方法在各种去噪器操作类型和块结构上的成本效益生成可解释的见解。我们利用这些见解来为从基础U-Net到最新Transformer的众多扩散模型做出高质量的混合精度量化决策。结果，QuasDiMo可以在PixArt-${\alpha}$、PixArt-${\Sigma}$、Hunyuan-DiT和SDXL上分别构建3.4位、3.9位、3.65位和3.7位的权重量化。我们还将我们的权重量化配置与6位激活量化相结合，并在定量指标和生成图像质量方面超越了现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14628v1">PDF</a> AAAI 2025; version includes supplementary material; 22 Pages, 18   Figures, 8 Tables</p>
<p><strong>Summary</strong><br>     扩散模型（DM）通过迭代去噪过程实现了AI图像生成的普及。量化是降低推理成本并减小DM去噪网络大小的主要技术。随着去噪器从卷积U-Net变体向更新的Transformer架构发展，了解不同权重层、操作和架构类型对性能的量化敏感性变得越来越重要。在此研究中，我们借助Quamixed-precisionPost-TrainingQuantization框架解决此挑战，该框架生成有关不同模型权重量化方法的成本效益的可解释见解，适用于不同的去噪器操作类型和块结构。我们利用这些见解为从基础U-Net到最新Transformer的各种扩散模型做出高质量混合精度量化决策。结果，Quamixed可以在PixArt-α、PixArt-Σ、Hunyuan-DiT和SDXL上分别进行3.4位、3.9位、3.65位和3.7位权重量化。我们将权重量化配置与6位激活量化相结合，在定量指标和生成图像质量方面超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过迭代去噪过程推动了AI图像生成的普及。</li>
<li>量化是降低扩散模型推理成本和减小网络大小的关键技术。</li>
<li>理解不同权重层、操作和架构类型在量化过程中的敏感性对于优化性能至关重要。</li>
<li>Qua$^2$SeDiMo框架提供了对不同模型权重量化方法的成本效益的可解释见解。</li>
<li>Qua$^2$SeDiMo支持从基础U-Net到高级Transformer的多种扩散模型的混合精度量化。</li>
<li>在特定的扩散模型上，Quamixed实现了低权重比特率的量化，如PixArt系列模型的3.4-3.9位权重量化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-17001be23bf41716496f74ae161df747.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e044badbcb79261a2824d04024f68cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3924dd7796ccc97da6a2035eee48172d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa8c63183e332f577d9a23cd473cd232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3dfc24d6b21a4315b3d87c3343c999b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6864aaeca1422061e4bab9c3355a5cd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LDP-Generalizing-to-Multilingual-Visual-Information-Extraction-by-Language-Decoupled-Pretraining"><a href="#LDP-Generalizing-to-Multilingual-Visual-Information-Extraction-by-Language-Decoupled-Pretraining" class="headerlink" title="LDP: Generalizing to Multilingual Visual Information Extraction by   Language Decoupled Pretraining"></a>LDP: Generalizing to Multilingual Visual Information Extraction by   Language Decoupled Pretraining</h2><p><strong>Authors:Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou</strong></p>
<p>Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hold invariance among images with different languages. If decoupling language bias from document images, a vision-layout-based model can achieve impressive cross-lingual generalization. Accordingly, we present a simple but effective multilingual training paradigm LDP (Language Decoupled Pre-training) for better utilization of monolingual pre-training data. Our proposed model LDM (Language Decoupled Model) is first pre-trained on the language-independent data, where the language knowledge is decoupled by a diffusion model, and then the LDM is fine-tuned on the downstream languages. Extensive experiments show that the LDM outperformed all SOTA multilingual pre-trained models, and also maintains competitiveness on downstream monolingual&#x2F;English benchmarks. </p>
<blockquote>
<p>视觉信息提取（VIE）在半结构化文档理解中起着至关重要的作用，已经开发了一些预训练模型来提高性能。然而，这些工作大多数是单语言的（通常是英语）。由于英语和其他语言之间预训练语料库的数量和质量极不平衡，很少有工作能扩展到非英语场景。在本文中，我们进行了系统的实验，结果表明，不同语言的图像在视觉和布局模式上具有不变性。如果将语言偏见从文档图像中解耦出来，基于视觉布局模型可以实现令人印象深刻的跨语言泛化能力。因此，我们提出了一种简单有效的多语言训练范式LDP（语言解耦预训练），以更好地利用单语言预训练数据。我们提出的LDM（语言解耦模型）首先是在独立于语言的数据上进行预训练的，其中语言知识通过扩散模型进行解耦，然后LDM在下游语言上进行微调。大量实验表明，LDM在所有最先进的多语言预训练模型中表现最佳，同时在下游单语言&#x2F;英语基准测试中也保持竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14596v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>视觉信息提取（VIE）在半结构化文档理解中扮演着至关重要的角色，为提升性能，已开发多个预训练模型。然而，这些工作多数为单语言（通常为英语）。由于英语与其他语言的预训练语料库在数量和品质上存在极不均衡，少有工作能延伸至非英语场景。本文进行系统实验，证明视觉和布局模态在不同语言的图像中具有不变性。若从文档图像中解耦语言偏见，基于视觉布局模型可实现令人印象深刻的跨语言泛化。据此，本文提出一种简单有效的多语言训练范式LDP（语言解耦预训练），以更好地利用单语言预训练数据。所提出的LDM（语言解耦模型）首先在独立于语言的数据上进行预训练，其中语言知识通过扩散模型解耦，然后在下游语言上进行微调。大量实验表明，LDM在多种语言预训练模型中表现最佳，同时在下游单语言&#x2F;英语基准测试中亦保持竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉信息提取（VIE）在半结构化文档理解中非常重要，并且已有多种预训练模型用于提升性能。</li>
<li>大多数相关工作为单语言（通常是英语），由于语料库的数量和质量问题，这些模型在非英语场景的应用受限。</li>
<li>实验表明视觉和布局模态在不同语言的图像中具有不变性。</li>
<li>解耦语言偏见的基于视觉布局的模型可以实现跨语言泛化。</li>
<li>提出了一种新的多语言训练范式LDP，旨在更好地利用单语言预训练数据。</li>
<li>LDM模型首先在独立于语言的数据上进行预训练，然后通过扩散模型解耦语言知识，并在下游语言上进行微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2718c6014987cc38fd24ada84412f5a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f63a78252fce44b152e61c47d2727ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c0f59c56a539335b3beb737c7481481.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4193d1d078a9799636e265935916555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-829edf06debb3d92497923c8cda760b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd087f8732346b4589e984a8fdc57875.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DiffSim-Taming-Diffusion-Models-for-Evaluating-Visual-Similarity"><a href="#DiffSim-Taming-Diffusion-Models-for-Evaluating-Visual-Similarity" class="headerlink" title="DiffSim: Taming Diffusion Models for Evaluating Visual Similarity"></a>DiffSim: Taming Diffusion Models for Evaluating Visual Similarity</h2><p><strong>Authors:Yiren Song, Xiaokang Liu, Mike Zheng Shou</strong></p>
<p>Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models. </p>
<blockquote>
<p>扩散模型已经从根本上改变了生成模型领域，评估定制模型输出和参考输入之间的相似性变得至关重要。然而，传统的感知相似性度量主要工作在像素和补丁级别，比较低级的颜色和纹理，但无法捕捉中级相似性以及图像布局、物体姿态和语义内容的差异。基于对比学习的CLIP和基于自监督学习的DINO通常用于测量语义相似性，但它们会高度压缩图像特征，对外观细节评估不足。本文首次发现可以利用预训练的扩散模型来测量视觉相似性，并引入了DiffSim方法，解决了传统指标在捕获定制生成任务中的感知一致性方面的局限性。通过对齐去噪U-Net注意层中的特征，DiffSim评估外观和风格相似性，与人类视觉偏好高度一致。此外，我们还引入了Sref和IP基准来分别在风格和实例级别评估视觉相似性。在多个基准上的综合评估表明，DiffSim实现了最先进的性能，为生成模型中视觉一致性的测量提供了可靠的工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14580v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型已经彻底改变了生成模型领域，因此对定制化模型输出与参考输入之间的相似性评估变得至关重要。然而，传统的感知相似性度量主要侧重于像素和斑块层面，比较低级的颜色和纹理，而无法捕捉中级的相似性以及图像布局、物体姿态和语义内容的差异。虽然对比学习基础上的CLIP和自监督学习基础上的DINO常用于衡量语义相似性，但它们高度压缩图像特征，对外观细节评估不足。本文首次发现可以利用预训练的扩散模型来测量视觉相似性，并引入了DiffSim方法，解决了传统度量方法在定制生成任务中捕捉感知一致性的局限性。通过对齐去噪U-Net注意力层中的特征，DiffSim评估外观和风格相似性，与人类视觉偏好高度一致。此外，我们还引入了Sref和IP基准来分别在风格和实例层面评估视觉相似性。在多个基准的综合性评估中，DiffSim实现了卓越的性能，为生成模型中视觉一致性的测量提供了稳健的工具。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型对生成模型领域产生了根本性影响，使得评估定制化模型输出与参考输入之间的相似性至关重要。</li>
<li>传统感知相似性度量主要关注像素和斑块层面，难以捕捉中级相似性及图像布局、物体姿态和语义内容的差异。</li>
<li>预训练的扩散模型可用于测量视觉相似性，引入的DiffSim方法解决了传统度量方法的局限性。</li>
<li>DiffSim通过对齐去噪U-Net注意力层中的特征，能够评估外观和风格相似性，与人类视觉偏好高度一致。</li>
<li>Sref和IP基准的引入分别用于评估视觉相似性的风格和实例层面。</li>
<li>DiffSim在多个基准测试中实现了卓越性能，为生成模型中视觉一致性的测量提供了有力工具。</li>
<li>DiffSim的引入有望推动生成模型领域的发展，促进更准确的模型评估和定制生成任务的优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-be8dfd9612efc2d6d0cc232ebff03c60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d246278b6e83b90ca7ddf66a6a802a7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1e6f2582c2d1e7f0fc2b1a42df0be6a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Content-style-disentangled-representation-for-controllable-artistic-image-stylization-and-generation"><a href="#Content-style-disentangled-representation-for-controllable-artistic-image-stylization-and-generation" class="headerlink" title="Content-style disentangled representation for controllable artistic   image stylization and generation"></a>Content-style disentangled representation for controllable artistic   image stylization and generation</h2><p><strong>Authors:Ma Zhuoqi, Zhang Yixuan, You Zejun, Tian Long, Liu Xiyang</strong></p>
<p>Controllable artistic image stylization and generation aims to render the content provided by text or image with the learned artistic style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image information for supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in semantic interference from the reference image. To address the above issues, this paper proposes a content-style representation disentangling method for controllable artistic image stylization and generation. We construct a WikiStyle+ dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled content and style representations guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers for better controllable stylization. This approach allows model to accommodate inputs from different modalities. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling a harmonious integration of content and style in the generated outputs, successfully producing style-consistent and expressive stylized images. </p>
<blockquote>
<p>可控的艺术图像风格化和生成旨在用提供的文本或图像内容呈现所学的艺术风格，而内容和风格的解耦是获得满意结果的关键。然而，当前的内容和风格分离方法主要依赖于图像信息进行监督，这导致了两个问题：1）模型只能支持一种风格或内容输入的模态；2）由于参考图像存在语义干扰，导致解耦不完全。为了解决上述问题，本文提出了一种可控艺术图像风格化和生成的内容与风格表示分离方法。我们构建了WikiStyle+数据集，其中包含艺术品及其相应的文本描述风格和内容。基于多模态数据集，我们提出了一个分离的内容和风格表示指导的扩散模型。首先通过Q-Formers学习解耦的表示，然后将其注入预训练的扩散模型中，使用可学习的多步交叉注意层实现更好的可控风格化。这种方法允许模型接受来自不同模态的输入。实验结果表明，我们的方法在多种模态监督下实现了参考图像内容和风格的彻底分离，从而能够在生成的输出中和谐地融合内容和风格，成功生成风格一致、表现力强的图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14496v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于多模态数据集的内容与风格表示分离方法，用于可控的艺术图像风格化和生成。通过构建包含艺术作品及其相应文本描述的WikiStyle+数据集，以及采用分离的内容与风格表示引导扩散模型，实现了对不同模态输入的支持，并彻底解耦了参考图像中的内容和风格，从而生成风格一致、富有表现力的图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文旨在通过内容与风格解耦的方法实现艺术图像的可控风格化和生成。</li>
<li>现有方法主要依赖图像信息进行监督，导致只能支持单一模态的输入和不完整的解耦。</li>
<li>论文构建了WikiStyle+数据集，包含艺术作品及其相应的文本描述，用于风格和内容的表示。</li>
<li>提出了基于多模态数据集的内容与风格表示分离方法。</li>
<li>通过Q-Formers学习解耦后的表示，然后注入预训练的扩散模型中，使用可学习的多步交叉注意层实现更好的可控风格化。</li>
<li>实验结果表明，该方法在多模态监督下彻底解耦了参考图像中的内容和风格。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14496">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e989c1b0feb71926ee7511824ac3964c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc40e86c3c647c7a89f9f00efc968117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44897723a308df0bc607aa2f4d74e3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-309dc704bd3f7ce3388da8fe2b13fccb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Model’s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Model’s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Model’s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Model’s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>近期，扩散模型作为生成模型领域的新晋热门，在图像生成方面表现出色。然而，当用于目标移除任务时，它们仍面临一些问题，例如产生随机伪影和在移除后无法重新绘制前景对象区域以填充适当的内容。为了解决这些问题，我们提出了“Attentive Eraser”方法，这是一种无需调整即可增强预训练扩散模型进行稳定和有效目标移除的方法。首先，基于观察到自注意力图会影响生成图像的结构和形状细节，我们提出了注意力激活和抑制（ASS），该方法根据给定的掩膜重新设计预训练扩散模型内的自注意力机制，从而在反向生成过程中优先处理背景而非前景目标。此外，我们引入了自注意力重定向指导（SARG），它利用ASS引导的自注意力来指导生成过程，从而在掩膜内有效地移除前景目标，同时生成既合理又连贯的内容。实验表明，Attentive Eraser在不同预训练的扩散模型中表现稳定且有效，甚至超越了基于训练的方法。此外，Attentive Eraser可应用于各种扩散模型架构和检查点，具有良好的可扩展性。相关代码可访问<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser%E3%80%82">https://github.com/Anonym0u3/AttentiveEraser。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v3">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     扩散模型在生成模型领域崭露头角，尤其在图像生成方面表现出色。然而，在处理对象移除任务时仍存在问题，如生成随机伪影和在移除前景对象后无法重新绘制适当内容。为此，我们提出无需调整的“Attentive Eraser”方法，使预训练的扩散模型能够进行稳定有效的对象移除。我们通过利用自注意力重定向引导（SARG）和注意力激活与抑制（ASS），重新设计预训练扩散模型内的自注意力机制，优先处理背景而非前景对象。该方法在对象移除方面表现出色，超越了许多训练型方法，适用于各种预训练扩散模型和检查点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在生成模型领域具有广阔的应用前景，尤其在图像生成方面表现优异。</li>
<li>对象移除任务中，扩散模型面临生成随机伪影和无法重新绘制移除对象后的适当内容的问题。</li>
<li>提出了一种名为“Attentive Eraser”的方法，该方法能够赋能预训练的扩散模型进行稳定有效的对象移除。</li>
<li>通过Attention Activation and Suppression（ASS）技术，重新设计预训练扩散模型内的自注意力机制。</li>
<li>引入Self-Attention Redirection Guidance（SARG），有效去除前景对象并生成合理连贯的内容。</li>
<li>Attentive Eraser方法在各种预训练扩散模型中表现出稳定性与高效性，且优于许多训练型方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c05a74d5fc8f7c7fe4df9ee99c061548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2072630d561b95396afa914c2d8e33b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d9b80c7275fc5978631c2565f9043ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daf18f400271dc04dfd192a7b462c011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3fe8a0715cad758c3774271ce823ca6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c81810756ea7a7a3ba441ce5a37a1067.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RAD-Region-Aware-Diffusion-Models-for-Image-Inpainting"><a href="#RAD-Region-Aware-Diffusion-Models-for-Image-Inpainting" class="headerlink" title="RAD: Region-Aware Diffusion Models for Image Inpainting"></a>RAD: Region-Aware Diffusion Models for Image Inpainting</h2><p><strong>Authors:Sora Kim, Sungho Suh, Minsik Lee</strong></p>
<p>Diffusion models have achieved remarkable success in image generation, with applications broadening across various domains. Inpainting is one such application that can benefit significantly from diffusion models. Existing methods either hijack the reverse process of a pretrained diffusion model or cast the problem into a larger framework, \ie, conditioned generation. However, these approaches often require nested loops in the generation process or additional components for conditioning. In this paper, we present region-aware diffusion models (RAD) for inpainting with a simple yet effective reformulation of the vanilla diffusion models. RAD utilizes a different noise schedule for each pixel, which allows local regions to be generated asynchronously while considering the global image context. A plain reverse process requires no additional components, enabling RAD to achieve inference time up to 100 times faster than the state-of-the-art approaches. Moreover, we employ low-rank adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models, reducing computational burdens in training as well. Experiments demonstrated that RAD provides state-of-the-art results both qualitatively and quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets. </p>
<blockquote>
<p>扩散模型在图像生成方面取得了显著的成功，其应用正在各个领域不断扩展。图像修复是其中一个可以从扩散模型中大大受益的应用。现有方法要么劫持预训练扩散模型的反向过程，要么将问题转化为更大的框架，即条件生成。然而，这些方法通常需要在生成过程中使用嵌套循环或额外的组件来进行条件处理。在本文中，我们提出了用于图像修复的区域感知扩散模型（RAD），这是对原始扩散模型的简单而有效的重新表述。RAD为每个像素使用不同的噪声方案，这允许局部区域在考虑全局图像上下文的同时进行异步生成。简单的反向过程不需要额外的组件，使RAD的推理时间达到最新方法的100倍。此外，我们采用了低秩适应（LoRA）方法来对RAD进行微调，基于其他预训练的扩散模型，减轻了训练中的计算负担。实验表明，RAD在FFHQ、LSUN卧室和ImageNet数据集上在定性和定量方面都提供了最新结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09191v3">PDF</a> </p>
<p><strong>摘要</strong><br>    本文提出了基于扩散模型的区域感知图像修复方法（RAD）。该方法通过简单的调整实现了扩散模型的异步局部区域生成，并利用不同的噪声调度策略考虑了全局图像上下文。RAD无需额外的组件，只需使用普通的反向过程，即可实现比现有技术更快的推理速度。此外，通过使用低秩适应（LoRA）技术微调RAD模型，降低了训练的计算负担。实验表明，RAD在FFHQ、LSUN卧室和ImageNet数据集上均达到了最先进的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在图像生成方面取得了显著的成功，其应用领域正在不断扩展，包括图像修复。</li>
<li>现有方法通常需要复杂的生成过程或额外的组件来实现条件生成。</li>
<li>RAD通过简单的调整实现了扩散模型的区域感知图像修复，利用不同的噪声调度策略为每个像素生成局部区域，同时考虑全局图像上下文。</li>
<li>RAD在推理速度方面表现出显著的优势，比现有技术快100倍。</li>
<li>RAD模型通过使用低秩适应（LoRA）技术进行微调，降低了训练的计算负担。</li>
<li>实验结果表明，RAD在多个数据集上实现了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09191">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb27e2f9609a195d0967e868517d9563.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd8c396472f1aa11c05ef604534c1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d6dfcc7130bc7eb44a19ba72b755367.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>近年来，基于深度学习的联合源信道编码（DJSCC）的最新进展为端到端的语义图像传输展示了前景。然而，大多数现有方案主要关注像素级的指标优化，这通常与人类感知不吻合，导致感知质量下降。在这篇文章中，我们提出了一种新的基于条件扩散模型的生成式DJSCC方法，以提高传输图像的感知质量。具体来说，我们利用熵模型，根据传输符号的估计熵有效地管理传输带宽。这些符号然后作为接收端时的条件信息，用于指导图像重建中的条件扩散解码器。我们的模型建立在新兴的马姆巴式线性注意力（MLLA）骨架之上，该骨架在图像处理任务中表现出色，同时提供快速的推理速度。此外，我们引入了一种多阶段训练策略，以确保模型的稳定性并提高其整体性能。仿真结果表明，我们提出的方法在感知质量方面显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于条件扩散模型的新型深度联合源信道编码（DJSCC）方法，旨在提高传输图像的感知质量。该方法利用熵模型估算传输符号的熵，实现有效带宽管理，并在接收端使用这些符号作为条件信息来指导图像重建的条件扩散解码器。该模型基于先进的mamba-like线性注意力（MLLA）骨架，具有图像处理任务出色和快速推理速度的优点。同时，引入多阶段训练策略以确保模型的稳定性和提高整体性能。仿真结果表明，该方法在感知质量方面显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种新型深度联合源信道编码（DJSCC）方法，利用条件扩散模型增强图像传输的感知质量。</li>
<li>利用熵模型估算传输符号的熵，实现带宽的有效管理。</li>
<li>在接收端使用条件信息来指导图像重建的条件扩散解码器。</li>
<li>模型基于先进的mamba-like线性注意力（MLLA）骨架，具有快速推理速度和优秀的图像处理性能。</li>
<li>引入多阶段训练策略以提高模型的稳定性和整体性能。</li>
<li>仿真结果表明，该方法在感知质量上显著优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-90b3a0658014bd140d80a5609c0caf99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d6eb91831daa0dcbcae7edc72d4ae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20e02759ca70cd725aed915d7feac78f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SkyDiffusion-Ground-to-Aerial-Image-Synthesis-with-Diffusion-Models-and-BEV-Paradigm"><a href="#SkyDiffusion-Ground-to-Aerial-Image-Synthesis-with-Diffusion-Models-and-BEV-Paradigm" class="headerlink" title="SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and   BEV Paradigm"></a>SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and   BEV Paradigm</h2><p><strong>Authors:Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He</strong></p>
<p>Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Bird’s-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a “multi-to-one” mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, historical high-resolution satellite image synthesis, and low-altitude UAV image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. More result and dataset information can be found at <a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/">https://opendatalab.github.io/skydiffusion/</a> . </p>
<blockquote>
<p>地面到空中的图像合成专注于从相应的地面街景图像生成逼真的空中图像，同时保持内容布局的一致性，模拟从上到下的视角。显著的视点差异导致了不同视角之间的领域差距，而密集的城区场景限制了街景的可见范围，这使得跨视图生成任务特别具有挑战性。在本文中，我们介绍了SkyDiffusion，这是一种从街景图像合成空中图像的新型跨视图生成方法，它利用扩散模型和鸟瞰图（BEV）范式。SkyDiffusion中的Curved-BEV方法将街景图像转换为BEV透视，有效地弥补了领域差距，并采用“多到一”的映射策略来解决密集城市场景中的遮挡问题。接下来，SkyDiffusion设计了一个BEV引导的扩散模型，以生成内容一致且逼真的空中图像。此外，我们还引入了一个新型数据集Ground2Aerial-3，该数据集专为多样化的地面到空中图像合成应用而设计，包括灾害场景空中合成、高分辨率历史卫星图像合成以及低空无人机图像合成任务等。实验结果表明，SkyDiffusion在跨视图数据集（如自然场景（CVUSA）、郊区场景（CVACT）、城市场景（VIGOR-Chicago）以及多种应用场景（G2A-3））上的表现均优于最先进的方法，实现了逼真且内容一致的天空图像生成。更多结果和数据集信息可在<a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/%E6%89%BE%E5%88%B0%E3%80%82">https://opendatalab.github.io/skydiffusion/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01812v3">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>基于地面视角生成真实感强烈的空中图像是一个充满挑战的任务，因为视点差异导致领域鸿沟以及城市景观的遮挡问题。本文提出了SkyDiffusion方法，通过扩散模型和鸟瞰图（BEV）范式实现这一任务。SkyDiffusion创新性地引入了Curved-BEV方法，有效缩短了领域差距并解决了遮挡问题。同时，设计了一种受鸟瞰图指导的扩散模型来生成内容连贯且真实的空中图像。此外，还推出了一个新的数据集Ground2Aerial-3，用于地面到空中的图像合成应用。实验结果显示，SkyDiffusion在自然、郊区、城市和多种应用情境的数据集上表现均优于其他先进技术。有关结果和详细信息可参见网址：<a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/">https://opendatalab.github.io/skydiffusion/</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SkyDiffusion是一种新颖的跨视角生成方法，用于从街头视角的图像合成空中图像。</li>
<li>引入Curved-BEV方法，有效桥接地面与空中视角的域差距。</li>
<li>采用“多到一”的映射策略来解决密集城市景观中的遮挡问题。</li>
<li>设计了受鸟瞰图指导的扩散模型来生成连贯且真实的空中图像。</li>
<li>推出新的数据集Ground2Aerial-3，适用于多种地面到空中的图像合成应用。</li>
<li>SkyDiffusion在多个数据集上的表现优于其他先进技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01812">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-59528443fe4476fe95bff216ced5f1e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-750202e1a98a34f8ad6157aefde1aebd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fc89567385bbfb3d6dc5d45fc48b5f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790ed603eda1320e53b2640b00dd3a71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac69890ed119bb0ddd03c0d9c0b1bbcd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diff-Shadow-Global-guided-Diffusion-Model-for-Shadow-Removal"><a href="#Diff-Shadow-Global-guided-Diffusion-Model-for-Shadow-Removal" class="headerlink" title="Diff-Shadow: Global-guided Diffusion Model for Shadow Removal"></a>Diff-Shadow: Global-guided Diffusion Model for Shadow Removal</h2><p><strong>Authors:Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu</strong></p>
<p>We propose Diff-Shadow, a global-guided diffusion model for shadow removal. Previous transformer-based approaches can utilize global information to relate shadow and non-shadow regions but are limited in their synthesis ability and recover images with obvious boundaries. In contrast, diffusion-based methods can generate better content but they are not exempt from issues related to inconsistent illumination. In this work, we combine the advantages of diffusion models and global guidance to achieve shadow-free restoration. Specifically, we propose a parallel UNets architecture: 1) the local branch performs the patch-based noise estimation in the diffusion process, and 2) the global branch recovers the low-resolution shadow-free images. A Reweight Cross Attention (RCA) module is designed to integrate global contextual information of non-shadow regions into the local branch. We further design a Global-guided Sampling Strategy (GSS) that mitigates patch boundary issues and ensures consistent illumination across shaded and unshaded regions in the recovered image. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have demonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art methods, our method achieves a significant improvement in terms of PSNR, increasing from 32.33dB to 33.69dB on the ISTD dataset. </p>
<blockquote>
<p>我们提出了Diff-Shadow，这是一种用于阴影去除的全局引导扩散模型。之前的基于transformer的方法可以利用全局信息来关联阴影和非阴影区域，但它们在合成能力方面有限，恢复的图像具有明显的边界。相比之下，基于扩散的方法可以生成更好的内容，但它们也存在光照不一致的问题。在这项工作中，我们结合了扩散模型和全局引导的优点，实现了无阴影的恢复。具体来说，我们提出了一种并行U-Net架构：1）局部分支执行扩散过程中的基于补丁的噪声估计；2）全局分支恢复无阴影的低分辨率图像。设计了一种重加权交叉注意力（RCA）模块，将非阴影区域的全局上下文信息集成到局部分支中。我们还设计了一种全局引导采样策略（GSS），可以缓解补丁边界问题，并确保恢复图像中的阴影和非阴影区域之间光照一致。在ISTD、ISTD+和SRD数据集上的综合实验证明了Diff-Shadow的有效性。与最先进的方法相比，我们的方法在PSNR方面取得了显著改进，在ISTD数据集上从32.33dB提高到33.69dB。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16214v2">PDF</a> Proceedings of the 39th Annual AAAI Conference on Artificial   Intelligence</p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的阴影去除技术Diff-Shadow研究。结合扩散模型的优点和全局指导策略，实现了阴影去除的图像恢复。采用并行UNets架构，局部分支进行基于补丁的噪声估计，全局分支恢复无阴影的低分辨率图像。设计重权交叉注意力模块（RCA）整合非阴影区域的全局上下文信息到局部分支，并设计全局引导采样策略（GSS）减少补丁边界问题并确保恢复图像中的阴影和未阴影区域的照明一致性。在ISTD、ISTD+和SRD数据集上的实验验证了Diff-Shadow的有效性，相较于最新方法，在ISTD数据集上的PSNR值从32.33dB显著提高至33.69dB。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于扩散模型的阴影去除方法Diff-Shadow，结合了扩散模型和全局指导策略的优势。</li>
<li>采用并行UNets架构，包括局部和全局两个分支，分别进行噪声估计和无阴影图像恢复。</li>
<li>设计了重权交叉注意力模块（RCA），用于整合全局上下文信息。</li>
<li>提出了全局引导采样策略（GSS），解决补丁边界问题，确保照明一致性。</li>
<li>在多个数据集上的实验验证了Diff-Shadow的有效性。</li>
<li>与现有方法相比，Diff-Shadow在ISTD数据集上的PSNR值有显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-21cffbcdf280bbb4ae925b7841f9443c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d18380365954dc0e6678be5f91282d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-153ac815315c0be74528c371c6178cb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8d92231f049231c18e83eb699c5c33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-475125131725026b086e94094f3b962d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1e0713c8695103e4823d1d65078ed10.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ID-Sculpt-ID-aware-3D-Head-Generation-from-Single-In-the-wild-Portrait-Image"><a href="#ID-Sculpt-ID-aware-3D-Head-Generation-from-Single-In-the-wild-Portrait-Image" class="headerlink" title="ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait   Image"></a>ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait   Image</h2><p><strong>Authors:Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma</strong></p>
<p>While recent works have achieved great success on image-to-3D object generation, high quality and fidelity 3D head generation from a single image remains a great challenge. Previous text-based methods for generating 3D heads were limited by text descriptions and image-based methods struggled to produce high-quality head geometry. To handle this challenging problem, we propose a novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving their identities. Our work incorporates the identity information of the portrait image into three parts: 1) geometry initialization, 2) geometry sculpting, and 3) texture generation stages. Given a reference portrait image, we first align the identity features with text features to realize ID-aware guidance enhancement, which contains the control signals representing the face information. We then use the canny map, ID features of the portrait image, and a pre-trained text-to-normal&#x2F;depth diffusion model to generate ID-aware geometry supervision, and 3D-GAN inversion is employed to generate ID-aware geometry initialization. Furthermore, with the ability to inject identity information into 3D head generation, we use ID-aware guidance to calculate ID-aware Score Distillation (ISD) for geometry sculpting. For texture generation, we adopt the ID Consistent Texture Inpainting and Refinement which progressively expands the view for texture inpainting to obtain an initialization UV texture map. We then use the ID-aware guidance to provide image-level supervision for noisy multi-view images to obtain a refined texture map. Extensive experiments demonstrate that we can generate high-quality 3D heads with accurate geometry and texture from a single in-the-wild portrait image. </p>
<blockquote>
<p>虽然近期的工作在图像到3D物体的生成上取得了巨大的成功，但从单张图像生成高质量和高保真度的3D头像仍然是一个巨大的挑战。之前基于文本的方法生成3D头像受限于文本描述，而基于图像的方法难以产生高质量的头像几何结构。为了处理这个具有挑战性的问题，我们提出了一种新型框架ID-Sculpt，用于生成高质量的3D头像，同时保留其身份特征。我们的工作将肖像图像的身份信息融入三个阶段：1）几何初始化，2）几何雕塑，3）纹理生成。给定一个参考肖像图像，我们首先通过文本特征与身份特征的对齐，实现ID感知引导增强，其中包含代表面部信息的控制信号。然后，我们使用Canny地图、肖像图像的身份特征以及预训练的文本到法线&#x2F;深度扩散模型来生成ID感知的几何监督，并利用3D-GAN反转来生成ID感知的几何初始化。此外，通过向3D头像生成中注入身份信息的能力，我们使用ID感知指导来计算用于几何雕塑的ID感知分数蒸馏（ISD）。对于纹理生成，我们采用ID一致纹理填充和细化方法，逐步扩展视图以进行纹理填充，以获得初始UV纹理贴图。然后，我们使用ID感知指导为噪声多视角图像提供图像级监督，以获得精细的纹理贴图。大量实验表明，我们可以从一张野外的肖像图像生成高质量、几何和纹理准确的3D头像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16710v2">PDF</a> Accepted by AAAI 2025; Project page:   <a target="_blank" rel="noopener" href="https://jinkun-hao.github.io/ID-Sculpt/">https://jinkun-hao.github.io/ID-Sculpt/</a></p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种新型框架ID-Sculpt，用于从单幅图像生成高质量的三维头像，同时保留其身份特征。该框架将身份信息融入三个阶段：几何初始化、几何雕刻和纹理生成。通过ID感知指导增强、Canny地图、预训练的文本到法线&#x2F;深度扩散模型等技术，实现了高质量的三维头像生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新型框架ID-Sculpt，旨在解决从单幅图像生成高质量三维头像的挑战。</li>
<li>框架将身份信息融入三个阶段：几何初始化、几何雕刻和纹理生成。</li>
<li>通过ID感知指导增强，实现了身份特征和文本特征的融合。</li>
<li>利用Canny地图、预训练的文本到法线&#x2F;深度扩散模型和3D-GAN反转，生成ID感知的几何监督。</li>
<li>ID感知指导用于计算几何雕刻的ID感知分数蒸馏（ISD）。</li>
<li>采用ID一致纹理补全与细化方法，逐步扩展视图进行纹理补全，获得初始UV纹理贴图。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ba9419e23b446e089160b023d1057f2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c16ea30d0c3612840684653ecc9e653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-304dd237598c4c95fac0e6a7a6bcabe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42b4bb5107fd5bf1b41a3707a2f0fa03.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself"><a href="#Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself" class="headerlink" title="Guiding a Diffusion Model with a Bad Version of Itself"></a>Guiding a Diffusion Model with a Bad Version of Itself</h2><p><strong>Authors:Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine</strong></p>
<p>The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality. </p>
<blockquote>
<p>在图像生成扩散模型中，主要的关注点在于图像质量、结果变化的程度以及结果如何符合给定的条件，例如类别标签或文本提示。流行的无分类器引导方法使用无条件模型来引导有条件模型，这导致了提示对齐更好和图像质量更高，但代价是变化减少。这些效果似乎固有地纠缠在一起，因此难以控制。我们意外地发现，通过使用模型本身的更小、训练较少的版本来指导生成，可以在不损害变化量的同时获得对图像质量的分离控制。这导致在ImageNet生成方面取得了显著改进，使用公开可用的网络，以64x64和512x512的分辨率分别设置了记录FID得分1.01和1.25。此外，该方法也适用于无条件扩散模型，大大提高了其质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02507v3">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文探讨了图像生成扩散模型中的关键方面，如图像质量、结果多样性和对齐特定条件的能力。通过指导模型的方法可以改进模型表现，即利用无条件的模型引导条件模型以获得更好的提示对齐和更高的图像质量，但同时也降低了多样性。本文观察到可以通过利用较小的未完全训练版本模型进行引导来分离控制图像质量而不牺牲多样性，这在ImageNet生成上取得了显著改进，并在公开网络上设置了新的FID记录。此方法同样适用于无条件扩散模型，可大幅提高模型质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成扩散模型的核心关注点包括图像质量、结果多样性和与给定条件对齐的能力。</li>
<li>使用无条件模型引导条件模型的方法可以同时提高提示对齐和图像质量，但会降低多样性。</li>
<li>利用较小的未完全训练版本模型进行引导可以分离控制图像质量而不牺牲多样性。</li>
<li>此方法在ImageNet生成上取得了显著改进，并设置了新的FID记录。</li>
<li>该方法适用于无条件扩散模型，可以大幅提高模型质量。</li>
<li>该研究提供了一种有效的策略来控制图像生成过程中的关键因素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ffbb79443270ac0d20c8ec13422e441e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99fffc68294fd93fed46bad9b1126352.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e73508f23bde92aaa21cf7932d77219f.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-21  Preventing Local Pitfalls in Vector Quantization via Optimal Transport
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d49716e6574422c1cdbd5ff1916f0448.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-21  LiDAR-RT Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
