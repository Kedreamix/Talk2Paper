<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Prometheus 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c606b27e21365bf72d1c862888c3d944.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Prometheus-3D-Aware-Latent-Diffusion-Models-for-Feed-Forward-Text-to-3D-Scene-Generation"><a href="#Prometheus-3D-Aware-Latent-Diffusion-Models-for-Feed-Forward-Text-to-3D-Scene-Generation" class="headerlink" title="Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation"></a>Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation</h2><p><strong>Authors:Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, Yiyi Liao</strong></p>
<p>In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation. Project page: <a target="_blank" rel="noopener" href="https://freemty.github.io/project-prometheus/">https://freemty.github.io/project-prometheus/</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Prometheusï¼Œè¿™æ˜¯ä¸€ä¸ª3Dæ„ŸçŸ¥æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¯åœ¨å¯¹è±¡å’Œåœºæ™¯çº§åˆ«è¿›è¡Œæ–‡æœ¬åˆ°3Dçš„å³æ—¶ç”Ÿæˆã€‚æˆ‘ä»¬å°†3Dåœºæ™¯ç”Ÿæˆå…¬å¼åŒ–ä¸ºæ½œåœ¨æ‰©æ•£èŒƒå¼å†…çš„å¤šè§†å›¾ã€å‰é¦ˆã€åƒç´ å¯¹é½çš„3Dé«˜æ–¯ç”Ÿæˆã€‚ä¸ºäº†ç¡®ä¿é€šç”¨æ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¹‹ä¸Šï¼Œåªéœ€è¿›è¡Œæœ€å°çš„è°ƒæ•´ï¼Œå¹¶ä½¿ç”¨å¤§é‡æ¥è‡ªå•è§†å›¾å’Œå¤šè§†å›¾æ•°æ®é›†çš„å›¾ç‰‡è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†RGB-Dæ½œåœ¨ç©ºé—´å¼•å…¥åˆ°3Dé«˜æ–¯ç”Ÿæˆä¸­ï¼Œä»¥åˆ†ç¦»å¤–è§‚å’Œå‡ ä½•ä¿¡æ¯ï¼Œå®ç°é«˜æ•ˆçš„å‰é¦ˆ3Dé«˜æ–¯ç”Ÿæˆï¼Œå…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦å’Œå‡ ä½•æ€§ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‰é¦ˆ3Dé«˜æ–¯é‡å»ºå’Œæ–‡æœ¬åˆ°3Dç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://freemty.github.io/project-prometheus/">https://freemty.github.io/project-prometheus/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21117v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†Prometheusï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºæ–‡æœ¬åˆ°ä¸‰ç»´åœºæ™¯çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½åœ¨å‡ ç§’å†…å®Œæˆç‰©ä½“å’Œåœºæ™¯çº§åˆ«çš„ä¸‰ç»´æ„ŸçŸ¥æ½œåœ¨æ‰©æ•£æ¨¡å‹æ„å»ºã€‚æ¨¡å‹åœ¨å·²æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ä¸Šç¨ä½œè°ƒæ•´è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ï¼Œç”¨äºå¤„ç†å•è§†è§’å’Œå¤šè§†è§’æ•°æ®é›†çš„åºå¤§å›¾åƒåº“ã€‚æˆ‘ä»¬è¿˜å°†RGB-Dæ½œåœ¨ç©ºé—´å¼•å…¥åˆ°ä¸‰ç»´é«˜æ–¯ç”Ÿæˆè¿‡ç¨‹ä¸­ä»¥åŒºåˆ†è¡¨é¢çº¹ç†ä¿¡æ¯å’Œå‡ ä½•ä¿¡æ¯ï¼Œä»è€Œå®ç°åœ¨ä¸‰ç»´é«˜æ–¯æ¨¡å‹ä¸­çš„é«˜æ•ˆæ­£å‘ç”Ÿæˆï¼Œå…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦å’Œå‡ ä½•ç‰¹æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ­£å‘ä¸‰ç»´é«˜æ–¯é‡å»ºå’Œæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆæ–¹é¢å‡å…·æœ‰è‰¯å¥½çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prometheusæ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬åˆ°ä¸‰ç»´åœºæ™¯çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥åœ¨å‡ ç§’å†…å®Œæˆç‰©ä½“å’Œåœºæ™¯çº§åˆ«çš„æ„å»ºã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºå•è§†è§’å’Œå¤šè§†è§’æ•°æ®é›†çš„åºå¤§å›¾åƒåº“å¤„ç†ã€‚</li>
<li>RGB-Dæ½œåœ¨ç©ºé—´è¢«å¼•å…¥ä»¥åŒºåˆ†è¡¨é¢çº¹ç†ä¿¡æ¯å’Œå‡ ä½•ä¿¡æ¯ï¼Œæœ‰åŠ©äºåœ¨ä¸‰ç»´é«˜æ–¯æ¨¡å‹ä¸­æ›´é«˜æ•ˆæ­£å‘ç”Ÿæˆé«˜è´¨é‡åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æé«˜æ­£å‘ä¸‰ç»´é«˜æ–¯é‡å»ºçš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨å·²æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œå…¼å®¹æ€§ã€‚</li>
<li>æ¨¡å‹å±•ç¤ºäº†å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šç§ç±»å‹çš„æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2778e0db5bc3638b2cd58683b762ed75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21f302ba85971abb997416d889cd19ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa23e3ff59e9121764b32b3887734fd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7045da7717bf68e670514bd038da7d62.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Quantum-Diffusion-Model-for-Quark-and-Gluon-Jet-Generation"><a href="#Quantum-Diffusion-Model-for-Quark-and-Gluon-Jet-Generation" class="headerlink" title="Quantum Diffusion Model for Quark and Gluon Jet Generation"></a>Quantum Diffusion Model for Quark and Gluon Jet Generation</h2><p><strong>Authors:Mariia Baidachna, Rey Guadarrama, Gopal Ramesh Dahale, Tom Magorsch, Isabel Pedraza, Konstantin T. Matchev, Katia Matcheva, Kyoungchul Kong, Sergei Gleyzer</strong></p>
<p>Diffusion models have demonstrated remarkable success in image generation, but they are computationally intensive and time-consuming to train. In this paper, we introduce a novel diffusion model that benefits from quantum computing techniques in order to mitigate computational challenges and enhance generative performance within high energy physics data. The fully quantum diffusion model replaces Gaussian noise with random unitary matrices in the forward process and incorporates a variational quantum circuit within the U-Net in the denoising architecture. We run evaluations on the structurally complex quark and gluon jets dataset from the Large Hadron Collider. The results demonstrate that the fully quantum and hybrid models are competitive with a similar classical model for jet generation, highlighting the potential of using quantum techniques for machine learning problems. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æ–¹é¢å¯†é›†ä¸”è®­ç»ƒè€—æ—¶è¾ƒé•¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œå—ç›Šäºé‡å­è®¡ç®—æŠ€æœ¯æ¥ç¼“è§£è®¡ç®—æŒ‘æˆ˜ï¼Œæé«˜é«˜èƒ½ç‰©ç†æ•°æ®ä¸­çš„ç”Ÿæˆæ€§èƒ½ã€‚å…¨é‡å­æ‰©æ•£æ¨¡å‹ç”¨éšæœºé…‰çŸ©é˜µæ›¿æ¢æ­£å‘è¿‡ç¨‹ä¸­çš„é«˜æ–¯å™ªå£°ï¼Œå¹¶åœ¨é™å™ªæ¶æ„ä¸­çš„U-Netä¸­èå…¥å˜åˆ†é‡å­ç”µè·¯ã€‚æˆ‘ä»¬åœ¨å¤§å‹å¼ºå­å¯¹æ’æœºç»“æ„å¤æ‚çš„å¤¸å…‹å’Œèƒ¶å­å–·å°„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå…¨é‡å­æ¨¡å‹å’Œæ··åˆæ¨¡å‹åœ¨ä¸å–·å°„ç”Ÿæˆçš„ç±»ä¼¼ç»å…¸æ¨¡å‹çš„ç«äº‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œçªæ˜¾äº†ä½¿ç”¨é‡å­æŠ€æœ¯è§£å†³æœºå™¨å­¦ä¹ é—®é¢˜çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21082v1">PDF</a> Accepted for the NeurIPS 2024 MLNCP workshop</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨é‡å­è®¡ç®—æŠ€æœ¯æ¥ç¼“è§£è®¡ç®—æŒ‘æˆ˜ï¼Œå¹¶æé«˜é«˜èƒ½ç‰©ç†æ•°æ®çš„ç”Ÿæˆæ€§èƒ½ã€‚é€šè¿‡éšæœºå•ä½çŸ©é˜µæ›¿æ¢é«˜æ–¯å™ªå£°è¿›è¡Œæ­£å‘è¿‡ç¨‹ï¼Œå¹¶åœ¨å»å™ªæ¶æ„çš„U-Netä¸­å¼•å…¥å˜åˆ†é‡å­ç”µè·¯ã€‚åœ¨å¤§å‹å¼ºå­å¯¹æ’æœºç»“æ„å¤æ‚çš„å¤¸å…‹å’Œèƒ¶å­å°„æµæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼Œå®Œå…¨é‡å­æ¨¡å‹å’Œæ··åˆæ¨¡å‹åœ¨å°„æµç”Ÿæˆæ–¹é¢ä¸ç±»ä¼¼çš„ç»å…¸æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œçªæ˜¾äº†é‡å­æŠ€æœ¯åœ¨æœºå™¨å­¦ä¹ é—®é¢˜ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹æ‰©æ•£æ¨¡å‹ç»“åˆé‡å­è®¡ç®—æŠ€æœ¯ä»¥æé«˜ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>ç”¨éšæœºå•ä½çŸ©é˜µæ›¿æ¢é«˜æ–¯å™ªå£°æ¥è¿›è¡Œæ­£å‘è¿‡ç¨‹ã€‚</li>
<li>åœ¨å»å™ªæ¶æ„U-Netä¸­å¼•å…¥å˜åˆ†é‡å­ç”µè·¯ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§å‹å¼ºå­å¯¹æ’æœºçš„å¤æ‚æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>å®Œå…¨é‡å­æ¨¡å‹å’Œæ··åˆæ¨¡å‹åœ¨å°„æµç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>é‡å­æŠ€æœ¯åœ¨è§£å†³æœºå™¨å­¦ä¹ é—®é¢˜ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc16433b601a0f1d7992b518f2d3fb9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c1fad4c44210a6ebdec66083d23e5f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a268263283177f7b3c088001dc44fe49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-342ed2f48170a4cbe4c17c66b06e7e0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7fbb2479510bc0471982937a836b6ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c392d3e1bebb372a38a29abbcc08ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9d9312175680acb0ff440c125077f2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Varformer-Adapting-VARâ€™s-Generative-Prior-for-Image-Restoration"><a href="#Varformer-Adapting-VARâ€™s-Generative-Prior-for-Image-Restoration" class="headerlink" title="Varformer: Adapting VARâ€™s Generative Prior for Image Restoration"></a>Varformer: Adapting VARâ€™s Generative Prior for Image Restoration</h2><p><strong>Authors:Siyang Wang, Feng Zhao</strong></p>
<p>Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VARâ€™s adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨å¤§é‡é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰æ¸…æ´å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå°†é€€åŒ–ç‰¹å¾è½¬æ¢ä¸ºæ¸…æ´ç‰¹å¾è¿›è¡Œå›¾åƒä¿®å¤çš„å¼ºå¤§å…ˆéªŒã€‚VARä½œä¸ºä¸€ç§æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡åº”ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡è‡ªå›å½’è¿‡ç¨‹é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œè¿™ä¸ä¿®å¤ç¤¾åŒºå¹¿æ³›è®¤å¯çš„å¤šå°ºåº¦ä¿®å¤åŸåˆ™ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°ï¼Œå°ºåº¦é¢„æµ‹ä¼šè‡ªåŠ¨è°ƒæ•´è¾“å…¥ï¼Œä¾¿äºåç»­å°ºåº¦ä¸Šçš„è¡¨ç¤ºä¸æ¸…æ´å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚ä¸ºäº†åˆ©ç”¨VARåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†VARå†…çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒï¼Œä»è€Œæ¨è¿›äº†æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„VarFormeræ¡†æ¶ã€‚è¿™äº›å…ˆéªŒçš„æˆ˜ç•¥åº”ç”¨ä½¿æˆ‘ä»¬çš„VarFormeråœ¨æœªè§ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ³›åŒ–ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„VarFormeråœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¤šä»»åŠ¡å›¾åƒä¿®å¤æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21063v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæ–°å‹å›¾åƒç”ŸæˆèŒƒå¼VARé€šè¿‡åº”ç”¨å¤šå°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œæœ‰æ•ˆæ•æ‰å›¾åƒçš„å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šçš„è¡¨ç°ã€‚VARåœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´å°ºåº¦é¢„æµ‹ï¼Œå®ç°åç»­å°ºåº¦çš„è¡¨ç¤ºä¸å¹²å‡€å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚åŸºäºVARçš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºæ¢å¤å…ˆéªŒï¼Œæå‡ºçš„VarFormeræ¡†æ¶åœ¨å¤šé¡¹å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œè¾ƒä½çš„åŸ¹è®­è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼Œèƒ½æœ‰æ•ˆæ•æ‰å¹²å‡€å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡å±æ€§ï¼Œä¸ºå›¾åƒæ¢å¤ä¸­é€€åŒ–ç‰¹å¾è½¬åŒ–ä¸ºå¹²å‡€ç‰¹å¾æä¾›å¼ºå¤§å…ˆéªŒã€‚</li>
<li>VARä½œä¸ºä¸€ç§æ–°å‹å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>VARèƒ½å¤Ÿæ¸è¿›æ•æ‰å›¾åƒçš„å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œç¬¦åˆæ¢å¤ç¤¾åŒºå¹¿æ³›è®¤å¯çš„å¤šå°ºåº¦æ¢å¤åŸåˆ™ã€‚</li>
<li>åœ¨ä½¿ç”¨VARçš„å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­ï¼Œå°ºåº¦é¢„æµ‹èƒ½è‡ªåŠ¨è°ƒæ•´è¾“å…¥ï¼Œä¿ƒè¿›åç»­å°ºåº¦çš„è¡¨ç¤ºä¸å¹²å‡€å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚</li>
<li>VarFormeræ¡†æ¶åˆ©ç”¨VARçš„è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œåœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>VarFormeré€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºæ¢å¤å…ˆéªŒï¼Œå®ç°äº†åœ¨æœªè§ä»»åŠ¡ä¸Šçš„å“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-047e0b1ea1ffd6f557c3611c4cf56c0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5f0425205ce9beaa2989c326e55abfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3511e2679316454b668466915f771e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-379997d28d2bc021f95d178576011611.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d4c3d485f3d32ba6a149661c2425bba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5372e9d2fb99038ba2b27a4174366b6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VMix-Improving-Text-to-Image-Diffusion-Model-with-Cross-Attention-Mixing-Control"><a href="#VMix-Improving-Text-to-Image-Diffusion-Model-with-Cross-Attention-Mixing-Control" class="headerlink" title="VMix: Improving Text-to-Image Diffusion Model with Cross-Attention   Mixing Control"></a>VMix: Improving Text-to-Image Diffusion Model with Cross-Attention   Mixing Control</h2><p><strong>Authors:Shaojin Wu, Fei Ding, Mengqi Huang, Wei Liu, Qian He</strong></p>
<p>While diffusion models show extraordinary talents in text-to-image generation, they may still fail to generate highly aesthetic images. More specifically, there is still a gap between the generated images and the real-world aesthetic images in finer-grained dimensions including color, lighting, composition, etc. In this paper, we propose Cross-Attention Value Mixing Control (VMix) Adapter, a plug-and-play aesthetics adapter, to upgrade the quality of generated images while maintaining generality across visual concepts by (1) disentangling the input text prompt into the content description and aesthetic description by the initialization of aesthetic embedding, and (2) integrating aesthetic conditions into the denoising process through value-mixed cross-attention, with the network connected by zero-initialized linear layers. Our key insight is to enhance the aesthetic presentation of existing diffusion models by designing a superior condition control method, all while preserving the image-text alignment. Through our meticulous design, VMix is flexible enough to be applied to community models for better visual performance without retraining. To validate the effectiveness of our method, we conducted extensive experiments, showing that VMix outperforms other state-of-the-art methods and is compatible with other community modules (e.g., LoRA, ControlNet, and IPAdapter) for image generation. The project page is <a target="_blank" rel="noopener" href="https://vmix-diffusion.github.io/VMix/">https://vmix-diffusion.github.io/VMix/</a>. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„å¤©èµ‹ï¼Œä½†å®ƒä»¬ä»ç„¶å¯èƒ½æ— æ³•ç”Ÿæˆå…·æœ‰é«˜åº¦å®¡ç¾æ„Ÿçš„å›¾åƒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨æ›´ç²¾ç»†çš„ç»´åº¦ï¼ˆåŒ…æ‹¬é¢œè‰²ã€å…‰çº¿ã€æ„å›¾ç­‰ï¼‰ä¸Šï¼Œç”Ÿæˆçš„å›¾åƒä¸çœŸå®ä¸–ç•Œçš„å®¡ç¾å›¾åƒä¹‹é—´ä»ç„¶å­˜åœ¨å·®è·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Cross-Attention Value Mixing Controlï¼ˆVMixï¼‰é€‚é…å™¨ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„ç¾å­¦é€‚é…å™¨ï¼Œæ—¨åœ¨é€šè¿‡ï¼ˆ1ï¼‰é€šè¿‡åˆå§‹åŒ–ç¾å­¦åµŒå…¥å°†è¾“å…¥æ–‡æœ¬æç¤ºè§£è€¦ä¸ºå†…å®¹æè¿°å’Œç¾å­¦æè¿°ï¼Œï¼ˆ2ï¼‰é€šè¿‡å€¼æ··åˆäº¤å‰æ³¨æ„åŠ›å°†ç¾å­¦æ¡ä»¶é›†æˆåˆ°å»å™ªè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ç½‘ç»œä¸é›¶åˆå§‹åŒ–çº¿æ€§å±‚è¿æ¥ï¼Œæ¥å‡çº§ç”Ÿæˆçš„å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒè§†è§‰æ¦‚å¿µçš„æ™®éæ€§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯é€šè¿‡è®¾è®¡ä¸€ç§ä¼˜è¶Šçš„æ¡ä»¶æ§åˆ¶æ–¹æ³•æ¥æé«˜ç°æœ‰æ‰©æ•£æ¨¡å‹çš„ç¾å­¦è¡¨ç°ï¼ŒåŒæ—¶ä¿ç•™å›¾åƒæ–‡æœ¬çš„å¯¹é½ã€‚é€šè¿‡æˆ‘ä»¬çš„ç²¾å¿ƒè®¾è®¡ï¼ŒVMixè¶³å¤Ÿçµæ´»ï¼Œå¯åº”ç”¨äºç¤¾åŒºæ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„è§†è§‰æ€§èƒ½è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜VMixä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸ç”¨äºå›¾åƒç”Ÿæˆçš„å…¶ä»–ç¤¾åŒºæ¨¡å—ï¼ˆä¾‹å¦‚LoRAã€ControlNetå’ŒIPAdapterï¼‰å…¼å®¹ã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://vmix-diffusion.github.io/VMix/%E3%80%82">https://vmix-diffusion.github.io/VMix/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20800v1">PDF</a> Codes and models are available at   <a target="_blank" rel="noopener" href="https://github.com/fenfenfenfan/VMix">https://github.com/fenfenfenfan/VMix</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCross-Attention Value Mixing Controlï¼ˆVMixï¼‰Adapterçš„ç¾å­¦é€‚é…å™¨ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„è´¨é‡ï¼ŒåŒæ—¶ä¿æŒå¯¹è§†è§‰æ¦‚å¿µçš„æ™®éæ€§ã€‚å®ƒé€šè¿‡åˆå§‹åŒ–ç¾å­¦åµŒå…¥æ¥å°†è¾“å…¥æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºå†…å®¹æè¿°å’Œç¾å­¦æè¿°ï¼Œå¹¶é€šè¿‡å€¼æ··åˆäº¤å‰æ³¨æ„åŠ›å°†ç¾å­¦æ¡ä»¶èå…¥å»å™ªè¿‡ç¨‹ã€‚VMixè®¾è®¡äº†ä¸€ç§ä¼˜è¶Šçš„æ¡ä»¶æ§åˆ¶æ–¹æ³•ï¼Œæé«˜äº†ç°æœ‰æ‰©æ•£æ¨¡å‹çš„ç¾å­¦è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒä¸æ–‡æœ¬çš„å¯¹åº”æ€§ã€‚æ­¤å¤–ï¼ŒVMixçµæ´»é€‚ç”¨äºç¤¾åŒºæ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯æé«˜è§†è§‰æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒVMixåœ¨å›¾åƒç”Ÿæˆæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸å…¶ä»–ç¤¾åŒºæ¨¡å—ï¼ˆå¦‚LoRAã€ControlNetå’ŒIPAdapterï¼‰å…¼å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆå…·æœ‰é«˜åº¦ç¾å­¦ä»·å€¼çš„å›¾åƒæ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç¾å­¦é€‚é…å™¨Cross-Attention Value Mixing Controlï¼ˆVMixï¼‰Adapterï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
<li>VMixé€šè¿‡åˆå§‹åŒ–ç¾å­¦åµŒå…¥æ¥åˆ†è§£è¾“å…¥æ–‡æœ¬æç¤ºï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡å€¼æ··åˆäº¤å‰æ³¨æ„åŠ›å°†ç¾å­¦æ¡ä»¶èå…¥å»å™ªè¿‡ç¨‹ã€‚</li>
<li>VMixè®¾è®¡äº†ä¸€ç§ä¼˜è¶Šçš„æ¡ä»¶æ§åˆ¶æ–¹æ³•ï¼Œæé«˜äº†ç°æœ‰æ‰©æ•£æ¨¡å‹çš„ç¾å­¦è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå›¾åƒä¸æ–‡æœ¬çš„å¯¹åº”æ€§ã€‚</li>
<li>VMixå¯çµæ´»åº”ç”¨äºç¤¾åŒºæ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯æå‡è§†è§‰æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜VMixåœ¨å›¾åƒç”Ÿæˆæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>VMixä¸å…¶ä»–ç¤¾åŒºæ¨¡å—å…¼å®¹ï¼Œå¦‚LoRAã€ControlNetå’ŒIPAdapterã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-efa59c6122a0e3836a542db697e4055a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05785662cb139dcee6058e40d8f62af9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e0ff1ef939b3ef372cd6caa86ff0021.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ea5ce07844c87c5e250c47dd2436b75.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HFI-A-unified-framework-for-training-free-detection-and-implicit-watermarking-of-latent-diffusion-model-generated-images"><a href="#HFI-A-unified-framework-for-training-free-detection-and-implicit-watermarking-of-latent-diffusion-model-generated-images" class="headerlink" title="HFI: A unified framework for training-free detection and implicit   watermarking of latent diffusion model generated images"></a>HFI: A unified framework for training-free detection and implicit   watermarking of latent diffusion model generated images</h2><p><strong>Authors:Sungik Choi, Sungwoo Park, Jaehoon Lee, Seunghyun Kim, Stanley Jungkyu Choi, Moontae Lee</strong></p>
<p>Dramatic advances in the quality of the latent diffusion models (LDMs) also led to the malicious use of AI-generated images. While current AI-generated image detection methods assume the availability of real&#x2F;AI-generated images for training, this is practically limited given the vast expressibility of LDMs. This motivates the training-free detection setup where no related data are available in advance. The existing LDM-generated image detection method assumes that images generated by LDM are easier to reconstruct using an autoencoder than real images. However, we observe that this reconstruction distance is overfitted to background information, leading the current method to underperform in detecting images with simple backgrounds. To address this, we propose a novel method called HFI. Specifically, by viewing the autoencoder of LDM as a downsampling-upsampling kernel, HFI measures the extent of aliasing, a distortion of high-frequency information that appears in the reconstructed image. HFI is training-free, efficient, and consistently outperforms other training-free methods in detecting challenging images generated by various generative models. We also show that HFI can successfully detect the images generated from the specified LDM as a means of implicit watermarking. HFI outperforms the best baseline method while achieving magnitudes of </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è´¨é‡çš„æ˜¾è‘—è¿›æ­¥ä¹Ÿå¯¼è‡´äº†AIç”Ÿæˆå›¾åƒè¢«æ¶æ„ä½¿ç”¨ã€‚å½“å‰AIç”Ÿæˆçš„å›¾åƒæ£€æµ‹æ–¹æ³•å‡è®¾æœ‰çœŸå®&#x2F;AIç”Ÿæˆçš„å›¾åƒå¯ç”¨äºè®­ç»ƒï¼Œä½†è€ƒè™‘åˆ°LDMçš„å·¨å¤§è¡¨è¾¾èƒ½åŠ›ï¼Œè¿™åœ¨å®è·µä¸­æ˜¯æœ‰é™çš„ã€‚è¿™ä¿ƒä½¿äº†æ— éœ€è®­ç»ƒçš„æ£€æµ‹è®¾ç½®çš„å‡ºç°ï¼Œå³æå‰æ²¡æœ‰ç›¸å…³æ•°æ®å¯ç”¨ã€‚ç°æœ‰çš„LDMç”Ÿæˆçš„å›¾åƒæ£€æµ‹æ–¹æ³•å‡è®¾ä½¿ç”¨è‡ªç¼–ç å™¨é‡å»ºLDMç”Ÿæˆçš„å›¾åƒæ¯”é‡å»ºçœŸå®å›¾åƒæ›´å®¹æ˜“ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ç§é‡å»ºè·ç¦»æ˜¯è¿‡åº¦æ‹ŸåˆèƒŒæ™¯ä¿¡æ¯çš„ï¼Œå¯¼è‡´å½“å‰æ–¹æ³•åœ¨æ£€æµ‹å…·æœ‰ç®€å•èƒŒæ™¯çš„å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºHFIçš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å°†LDMçš„è‡ªç¼–ç å™¨è§†ä¸ºä¸‹é‡‡æ ·-ä¸Šé‡‡æ ·å†…æ ¸ï¼ŒHFIæµ‹é‡é‡å»ºå›¾åƒä¸­å‡ºç°çš„æ··å ç¨‹åº¦ï¼Œå³é«˜é¢‘ä¿¡æ¯çš„å¤±çœŸã€‚HFIæ— éœ€è®­ç»ƒï¼Œæ•ˆç‡é«˜ï¼Œå¹¶ä¸”åœ¨æ£€æµ‹ç”±å„ç§ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒæ—¶å§‹ç»ˆä¼˜äºå…¶ä»–æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†HFIå¯ä»¥æˆåŠŸæ£€æµ‹ä»ç‰¹å®šLDMç”Ÿæˆçš„å›¾åƒï¼Œä½œä¸ºä¸€ç§éšå¼æ°´å°æ‰‹æ®µã€‚HFIåœ¨æ— éœ€è®­ç»ƒçš„åŸºçº¿æ–¹æ³•ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20704v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è¿›å±•åŠå…¶è¢«æ¶æ„åˆ©ç”¨ç”Ÿæˆå›¾åƒçš„é—®é¢˜ã€‚ç°æœ‰AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹æ–¹æ³•å—é™äºè®­ç»ƒæ•°æ®ï¼Œè€ŒLDMç”Ÿæˆçš„å›¾åƒæ£€æµ‹ä¾èµ–äºå›¾åƒé‡æ„è·ç¦»æ¥åˆ¤æ–­ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“å¿½ç•¥ç®€å•èƒŒæ™¯å›¾åƒçš„æ£€æµ‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€æµ‹æ–¹æ³•â€”â€”HFIã€‚å®ƒé€šè¿‡æµ‹é‡é‡å»ºå›¾åƒä¸­çš„æ··å ç¨‹åº¦æ¥è¡¡é‡é«˜é¢‘ä¿¡æ¯çš„å¤±çœŸç¨‹åº¦ï¼Œæ— éœ€è®­ç»ƒå³å¯é«˜æ•ˆæ£€æµ‹å„ç§ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„æŒ‘æˆ˜å›¾åƒã€‚æ­¤å¤–ï¼ŒHFIè¿˜å¯ç”¨äºæ£€æµ‹ç‰¹å®šLDMç”Ÿæˆçš„å›¾åƒä½œä¸ºéšæ€§æ°´å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMçš„è¿›å±•å¯¼è‡´AIç”Ÿæˆå›¾åƒçš„æ¶æ„ä½¿ç”¨é—®é¢˜ã€‚</li>
<li>ç°æœ‰AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•å—é™äºè®­ç»ƒæ•°æ®ï¼Œä¸”éš¾ä»¥æ£€æµ‹ç®€å•èƒŒæ™¯å›¾åƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€æµ‹æ–¹æ³•HFIï¼ŒåŸºäºé«˜é¢‘ä¿¡æ¯çš„å¤±çœŸç¨‹åº¦æ¥è¡¡é‡æ··å ç¨‹åº¦ã€‚</li>
<li>HFIæ— éœ€è®­ç»ƒå³å¯é«˜æ•ˆæ£€æµ‹å„ç§ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„æŒ‘æˆ˜å›¾åƒã€‚</li>
<li>HFIå¯æˆåŠŸæ£€æµ‹ç‰¹å®šLDMç”Ÿæˆçš„å›¾åƒä½œä¸ºéšæ€§æ°´å°ã€‚</li>
<li>HFIç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5da4b5631f6e9805d0a6c85e4836d21b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e18c44f7d3b0187426a78c7732973532.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcb143695ca4d6768dc7eba7b86ab88b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0747eb1704ac50984c1a19a6ee81fd96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e502a1ba1327584f2dd79f3de4cba300.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee59924bc8e6c4efe944e68418fcbed4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation Agent for Weakly Supervised Semantic Segmentation"></a>Image Augmentation Agent for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„WSSSæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°æ¥ç”Ÿæˆæ›´å‡†ç¡®çš„å¯†é›†æ ‡ç­¾ï¼Œè€Œå¿½ç•¥äº†å›ºå®šæ•°æ®é›†æ‰€å¸¦æ¥çš„é™åˆ¶ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½çš„æå‡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæä¾›æ›´å¤šå¯è®­ç»ƒçš„å›¾åƒå¯ä»¥ä¸ºWSSSæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶å¸®åŠ©æ¨¡å‹ç†è§£æ›´å…¨é¢çš„è¯­ä¹‰æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºå›¾åƒå¢å¼ºä»£ç†ï¼ˆIAAï¼‰ï¼Œå®ƒè¡¨æ˜ä»æ•°æ®ç”Ÿæˆçš„è§’åº¦å¢å¼ºWSSSæ˜¯å¯èƒ½çš„ã€‚IAAä¸»è¦è®¾è®¡äº†ä¸€ä¸ªå¢å¼ºä»£ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ä¸ºWSSSç”Ÿæˆé¢å¤–çš„å›¾åƒã€‚åœ¨å®è·µä¸­ï¼Œä¸ºäº†è§£å†³LLMæç¤ºç”Ÿæˆä¸­çš„ä¸ç¨³å®šé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æç¤ºè‡ªæˆ‘å®Œå–„æœºåˆ¶ã€‚å®ƒå…è®¸LLMé‡æ–°è¯„ä¼°ç”Ÿæˆæç¤ºçš„åˆç†æ€§ï¼Œä»¥äº§ç”Ÿæ›´è¿è´¯çš„æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥äº†ä¸€ä¸ªåœ¨çº¿è¿‡æ»¤å™¨ï¼Œä»¥åŠ¨æ€ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„WSSSæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20439v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºImage Augmentation Agentï¼ˆIAAï¼‰çš„æ–¹æ³•ï¼Œä»æ•°æ®ç”Ÿæˆçš„è§’åº¦æå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ä¸ºWSSSç”Ÿæˆé¢å¤–å›¾åƒï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æç¤ºè‡ªæˆ‘å®Œå–„æœºåˆ¶å’Œåœ¨çº¿è¿‡æ»¤å™¨ï¼Œè§£å†³äº†è¯­è¨€æ¨¡å‹æç¤ºç”Ÿæˆçš„ä¸ç¨³å®šæ€§å’Œå›¾åƒç”Ÿæˆè´¨é‡çš„é—®é¢˜ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœä¼˜äºç°æœ‰WSSSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹æå‡ºä¸€ç§æ–°é¢–æ–¹æ³•Image Augmentation Agentï¼ˆIAAï¼‰ï¼Œä»æ•°æ®ç”Ÿæˆè§’åº¦æå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ€§èƒ½ã€‚</li>
<li>IAAè®¾è®¡äº†ä¸€ä¸ªæç¤ºè‡ªæˆ‘å®Œå–„æœºåˆ¶ï¼Œä½¿LLMsèƒ½å¤Ÿé‡æ–°è¯„ä¼°ç”Ÿæˆçš„æç¤ºçš„åˆç†æ€§ï¼Œäº§ç”Ÿæ›´è¿è´¯çš„æç¤ºã€‚</li>
<li>ä¸ºäº†ä¿è¯å›¾åƒçš„è´¨é‡ä¸å¹³è¡¡ï¼Œåœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥äº†åœ¨çº¿è¿‡æ»¤å™¨ã€‚</li>
<li>æ–¹æ³•ä¸“æ³¨äºè§£å†³å›ºå®šæ•°æ®é›†å¸¦æ¥çš„å±€é™æ€§ï¼Œé€šè¿‡æä¾›æ›´å¤šå¤šæ ·åŒ–çš„è®­ç»ƒå›¾åƒï¼Œä½¿WSSSè·å¾—æ›´ä¸°å¯Œä¿¡æ¯å’Œæ›´å…¨é¢çš„è¯­ä¹‰æ¨¡å¼ç†è§£ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„WSSSæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œç”Ÿæˆå›¾åƒè´¨é‡åœ¨æå‡WSSSæ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af2a3ebd09a1bf1b97a1d039283df260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d420af9d7c6e4b553c221ca8efb92b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50df88fe53c498b6f64d28ef4ad06123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7d6bc9d163130f0f670b8ebe70adb5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4acc489d52ac34b0db3e21364468b2a6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bringing-Objects-to-Life-4D-generation-from-3D-objects"><a href="#Bringing-Objects-to-Life-4D-generation-from-3D-objects" class="headerlink" title="Bringing Objects to Life: 4D generation from 3D objects"></a>Bringing Objects to Life: 4D generation from 3D objects</h2><p><strong>Authors:Ohad Rahamim, Ori Malca, Dvir Samuel, Gal Chechik</strong></p>
<p>Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry of generated content. In this work, we introduce a method for animating user-provided 3D objects by conditioning on textual prompts to guide 4D generation, enabling custom animations while maintaining the identity of the original object. We first convert a 3D mesh into a &#96;&#96;staticâ€ 4D Neural Radiance Field (NeRF) that preserves the visual attributes of the input object. Then, we animate the object using an Image-to-Video diffusion model driven by text. To improve motion realism, we introduce an incremental viewpoint selection protocol for sampling perspectives to promote lifelike movement and a masked Score Distillation Sampling (SDS) loss, which leverages attention maps to focus optimization on relevant regions. We evaluate our model in terms of temporal coherence, prompt adherence, and visual fidelity and find that our method outperforms baselines that are based on other approaches, achieving up to threefold improvements in identity preservation measured using LPIPS scores, and effectively balancing visual quality with dynamic content. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—é€šè¿‡æ–‡æœ¬æç¤ºåˆ›å»º4Då†…å®¹ï¼ˆåŠ¨æ€3Dç‰©ä½“ï¼‰æˆä¸ºå¯èƒ½ã€‚4Dç”Ÿæˆåœ¨è™šæ‹Ÿä¸–ç•Œã€åª’ä½“å’Œæ¸¸æˆç­‰é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶ç”Ÿæˆå†…å®¹çš„å¤–è§‚å’Œå‡ ä½•å½¢çŠ¶æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºæ¥å¼•å¯¼åŠ¨ç”»çš„4Dç”Ÿæˆæ–¹æ³•ï¼Œä½¿ç”¨æˆ·æä¾›çš„3Dç‰©ä½“åŠ¨ç”»åŒ–ï¼ŒåŒæ—¶ä¿æŒåŸå§‹ç‰©ä½“çš„èº«ä»½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªä¸‰ç»´ç½‘æ ¼è½¬æ¢æˆä¸€ä¸ªä¿ç•™è¾“å…¥ç‰©ä½“è§†è§‰å±æ€§çš„â€œé™æ€â€å››ç»´ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥åˆ¶ä½œåŠ¨ç”»ã€‚ä¸ºäº†æé«˜è¿åŠ¨é€¼çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¢é‡è§†ç‚¹é€‰æ‹©åè®®ï¼Œç”¨äºé‡‡æ ·è§†è§’ä»¥ä¿ƒè¿›é€¼çœŸçš„è¿åŠ¨ï¼Œä»¥åŠå¸¦æœ‰å±è”½çš„åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾æ¥ä¸“æ³¨äºä¼˜åŒ–ç›¸å…³åŒºåŸŸã€‚æˆ‘ä»¬æ ¹æ®æ—¶é—´ä¸€è‡´æ€§ã€æç¤ºç¬¦åˆåº¦å’Œè§†è§‰ä¿çœŸåº¦æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå‘ç°æˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½ä¿ç•™æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•çš„ä¸‰å€å·¦å³ï¼Œå¹¶åœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶æœ‰æ•ˆåœ°å¹³è¡¡äº†åŠ¨æ€å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºé©±åŠ¨åŠ¨ç”»ç”¨æˆ·æä¾›çš„3Då¯¹è±¡çš„æ–¹æ³•ï¼Œå®ç°äº†4Då†…å®¹ç”Ÿæˆã€‚è¯¥æ–¹æ³•å°†3Dç½‘æ ¼è½¬æ¢ä¸ºé™æ€çš„4Dç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œç„¶åä½¿ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŠ¨ç”»åŒ–ã€‚ä¸ºæé«˜è¿åŠ¨çœŸå®æ„Ÿï¼Œå¼•å…¥äº†å¢é‡è§†è§’é€‰æ‹©åè®®å’Œæ©æ¨¡å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒç‰©ä½“èº«ä»½ã€æ—¶é—´è¿è´¯æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯å·²å‘å±•åˆ°å¯ä»¥åˆ›å»ºå—æ–‡æœ¬æç¤ºæ§åˆ¶çš„4Då†…å®¹ï¼ˆåŠ¨æ€3Då¯¹è±¡ï¼‰ã€‚</li>
<li>4Dç”Ÿæˆåœ¨è™šæ‹Ÿä¸–ç•Œã€åª’ä½“å’Œæ¸¸æˆç­‰é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºé©±åŠ¨åŠ¨ç”»ç”¨æˆ·æä¾›çš„3Då¯¹è±¡çš„æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬å°†3Dç½‘æ ¼è½¬æ¢ä¸ºé™æ€çš„4Dç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œç„¶åä½¿ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŠ¨ç”»åŒ–ã€‚</li>
<li>ä¸ºæé«˜è¿åŠ¨çœŸå®æ„Ÿï¼Œå¼•å…¥äº†å¢é‡è§†è§’é€‰æ‹©åè®®å’Œæ©æ¨¡å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿æŒç‰©ä½“èº«ä»½ã€æ—¶é—´è¿è´¯æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-438771f144c153b43a01cfe97fab3ae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e9598f4e678da3d65c4be1085d22a3c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diff4MMLiTS-Advanced-Multimodal-Liver-Tumor-Segmentation-via-Diffusion-Based-Image-Synthesis-and-Alignment"><a href="#Diff4MMLiTS-Advanced-Multimodal-Liver-Tumor-Segmentation-via-Diffusion-Based-Image-Synthesis-and-Alignment" class="headerlink" title="Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via   Diffusion-Based Image Synthesis and Alignment"></a>Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via   Diffusion-Based Image Synthesis and Alignment</h2><p><strong>Authors:Shiyun Chen, Li Lin, Pujin Cheng, ZhiCheng Jin, JianJian Chen, HaiDong Zhu, Kenneth K. Y. Wong, Xiaoying Tang</strong></p>
<p>Multimodal learning has been demonstrated to enhance performance across various clinical tasks, owing to the diverse perspectives offered by different modalities of data. However, existing multimodal segmentation methods rely on well-registered multimodal data, which is unrealistic for real-world clinical images, particularly for indistinct and diffuse regions such as liver tumors. In this paper, we introduce Diff4MMLiTS, a four-stage multimodal liver tumor segmentation pipeline: pre-registration of the target organs in multimodal CTs; dilation of the annotated modalityâ€™s mask and followed by its use in inpainting to obtain multimodal normal CTs without tumors; synthesis of strictly aligned multimodal CTs with tumors using the latent diffusion model based on multimodal CT features and randomly generated tumor masks; and finally, training the segmentation model, thus eliminating the need for strictly aligned multimodal data. Extensive experiments on public and internal datasets demonstrate the superiority of Diff4MMLiTS over other state-of-the-art multimodal segmentation methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€å­¦ä¹ å·²è¯æ˜å¯ä»¥å¢å¼ºå„ç§ä¸´åºŠä»»åŠ¡çš„è¡¨ç°ï¼Œè¿™æ˜¯ç”±äºä¸åŒæ¨¡æ€çš„æ•°æ®æä¾›äº†ä¸åŒçš„è§†è§’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ä¾èµ–äºå·²æ³¨å†Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠå›¾åƒæ¥è¯´å¹¶ä¸ç°å®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¨¡ç³Šå’Œæ•£æ¼«çš„åŒºåŸŸï¼ˆå¦‚è‚è„è‚¿ç˜¤ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Diff4MMLiTSï¼Œè¿™æ˜¯ä¸€ä¸ªå››é˜¶æ®µçš„å¤šæ¨¡æ€è‚è„è‚¿ç˜¤åˆ†å‰²æµç¨‹ï¼šå¤šæ¨¡æ€CTä¸­ç›®æ ‡å™¨å®˜çš„é¢„æ³¨å†Œï¼›å¯¹æ³¨é‡Šæ¨¡æ€çš„æ©è†œè¿›è¡Œè†¨èƒ€ï¼Œç„¶åç”¨äºä¿®å¤ä»¥è·å¾—ä¸å«è‚¿ç˜¤çš„å¤šæ¨¡æ€æ­£å¸¸CTï¼›ä½¿ç”¨åŸºäºå¤šæ¨¡æ€CTç‰¹å¾å’Œéšæœºç”Ÿæˆçš„è‚¿ç˜¤æ©è†œè¿›è¡Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åˆæˆï¼Œç”Ÿæˆå«æœ‰è‚¿ç˜¤çš„å¤šæ¨¡æ€CTï¼›æœ€åï¼Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ã€‚åœ¨å…¬å…±å’Œå†…éƒ¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiff4MMLiTSä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diff4MMLiTSï¼Œä¸€ç§å››é˜¶æ®µçš„å¤šæ¨¡æ€è‚è„è‚¿ç˜¤åˆ†å‰²ç®¡é“ï¼Œæ—¨åœ¨è§£å†³çœŸå®ä¸–ç•Œä¸´åºŠå›¾åƒä¸­å¤šæ¨¡æ€æ•°æ®åˆ†å‰²çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æ³¨å†Œç›®æ ‡å™¨å®˜ã€è†¨èƒ€æ ‡æ³¨æ¨¡æ€çš„æ©è†œå¹¶è¿›è¡Œè¡¥å…¨æ“ä½œï¼Œè·å¾—ä¸å«è‚¿ç˜¤çš„å¤šæ¨¡æ€æ­£å¸¸CTå›¾åƒã€‚éšåï¼ŒåŸºäºå¤šæ¨¡æ€CTç‰¹å¾å’Œéšæœºç”Ÿæˆçš„è‚¿ç˜¤æ©è†œï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹åˆæˆä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€CTå›¾åƒã€‚æœ€åï¼Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œæ— éœ€ä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®ã€‚åœ¨å…¬å…±å’Œå†…éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiff4MMLiTSä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ å¯ä»¥æå‡å„ç§ä¸´åºŠä»»åŠ¡çš„æ€§èƒ½ï¼Œå› ä¸ºä¸åŒæ¨¡æ€çš„æ•°æ®æä¾›äº†ä¸åŒçš„è§†è§’ã€‚</li>
<li>ç°æœ‰å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ä¾èµ–äºè‰¯å¥½æ³¨å†Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™åœ¨çœŸå®ä¸–ç•Œçš„ä¸´åºŠå›¾åƒä¸­æ˜¯ä¸ç°å®çš„ã€‚</li>
<li>Diff4MMLiTSæ˜¯ä¸€ç§å››é˜¶æ®µçš„å¤šæ¨¡æ€è‚è„è‚¿ç˜¤åˆ†å‰²ç®¡é“ï¼ŒåŒ…æ‹¬é¢„æ³¨å†Œã€æ©è†œè†¨èƒ€ä¸è¡¥å…¨ã€åˆæˆä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€CTå›¾åƒå’Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>Diff4MMLiTSè§£å†³äº†å¯¹ä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®çš„éœ€è¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…¬å…±å’Œå†…éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹åˆæˆä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€CTå›¾åƒæ˜¯æ­¤æ–¹æ³•çš„ä¸€å¤§äº®ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cace16f62ce7fcb8ed89257406ea95c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87cc73f2ef25bbe1ad1e75395b73ba02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88849d3feb6fa4fbac26cb6d7f16bb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27f0d44d208b0a67c86b9c51315a3542.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbd40a0c7d26fb792740f6692b1abc2b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EraseAnything-Enabling-Concept-Erasure-in-Rectified-Flow-Transformers"><a href="#EraseAnything-Enabling-Concept-Erasure-in-Rectified-Flow-Transformers" class="headerlink" title="EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers"></a>EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers</h2><p><strong>Authors:Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, Weiming Zhang</strong></p>
<p>Removing unwanted concepts from large-scale text-to-image (T2I) diffusion models while maintaining their overall generative quality remains an open challenge. This difficulty is especially pronounced in emerging paradigms, such as Stable Diffusion (SD) v3 and Flux, which incorporate flow matching and transformer-based architectures. These advancements limit the transferability of existing concept-erasure techniques that were originally designed for the previous T2I paradigm (\textit{e.g.}, SD v1.4). In this work, we introduce \logopic \textbf{EraseAnything}, the first method specifically developed to address concept erasure within the latest flow-based T2I framework. We formulate concept erasure as a bi-level optimization problem, employing LoRA-based parameter tuning and an attention map regularizer to selectively suppress undesirable activations. Furthermore, we propose a self-contrastive learning strategy to ensure that removing unwanted concepts does not inadvertently harm performance on unrelated ones. Experimental results demonstrate that EraseAnything successfully fills the research gap left by earlier methods in this new T2I paradigm, achieving state-of-the-art performance across a wide range of concept erasure tasks. </p>
<blockquote>
<p>ä»å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸­ç§»é™¤ä¸éœ€è¦çš„æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“çš„ç”Ÿæˆè´¨é‡ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¾…è§£å†³çš„éš¾é¢˜ã€‚ç‰¹åˆ«æ˜¯åœ¨æ–°å…´çš„æ¨¡å¼ä¸­ï¼Œå¦‚ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰v3å’ŒFluxï¼Œè¿™äº›æ¨¡å¼èå…¥äº†æµåŒ¹é…å’ŒåŸºäºtransformerçš„æ¶æ„ï¼Œè¿™ä¸€éš¾é¢˜å˜å¾—æ›´åŠ çªå‡ºã€‚è¿™äº›è¿›å±•é™åˆ¶äº†åŸå…ˆä¸ºæ—§T2Iæ¨¡å¼ï¼ˆä¾‹å¦‚SD v1.4ï¼‰è®¾è®¡çš„æ¦‚å¿µæ“¦é™¤æŠ€æœ¯çš„å¯è½¬ç§»æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“é—¨ä¸ºæœ€æ–°æµå¼T2Iæ¡†æ¶ä¸­çš„æ¦‚å¿µæ“¦é™¤è€Œå¼€å‘çš„æ–¹æ³•â€”â€”LoPic EraseAnythingã€‚æˆ‘ä»¬å°†æ¦‚å¿µæ“¦é™¤å…¬å¼åŒ–ä¸ºä¸€ä¸ªä¸¤çº§ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨åŸºäºLoRAçš„å‚æ•°è°ƒæ•´å’Œæ³¨æ„åŠ›å›¾æ­£åˆ™åŒ–æ¥é€‰æ‹©æ€§æŠ‘åˆ¶ä¸éœ€è¦çš„æ¿€æ´»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç§»é™¤ä¸éœ€è¦çš„æ¦‚å¿µä¸ä¼šæ— æ„ä¸­æŸå®³å…¶ä»–ä¸ç›¸å…³æ¦‚å¿µçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEraseAnythingæˆåŠŸå¡«è¡¥äº†æ—©æœŸæ–¹æ³•åœ¨æ–°T2Iæ¨¡å¼ä¸­çš„ç ”ç©¶ç©ºç™½ï¼Œåœ¨å¹¿æ³›çš„æ¦‚å¿µæ“¦é™¤ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20413v1">PDF</a> 24 pages, 18 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µç§»é™¤æŠ€æœ¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°çš„æ¨¡å‹å¦‚Stable Diffusion v3å’ŒFluxä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†é’ˆå¯¹æœ€æ–°æµT2Iæ¡†æ¶çš„æ¦‚å¿µç§»é™¤æ–¹æ³•â€”â€”EraseAnythingã€‚é€šè¿‡é‡‡ç”¨åŸºäºLoRAçš„å‚æ•°è°ƒä¼˜å’Œæ³¨æ„åŠ›å›¾æ­£åˆ™åŒ–ï¼Œå®ç°é€‰æ‹©æ€§æŠ‘åˆ¶ä¸éœ€è¦çš„æ¿€æ´»ï¼ŒåŒæ—¶é€šè¿‡è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥ç¡®ä¿ä¸å½±å“æ— å…³æ¦‚å¿µçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEraseAnythingå¡«è¡¥äº†å½“å‰T2Iæ¨¡å‹ç§»é™¤æŠ€æœ¯çš„ç©ºç™½ï¼Œå¹¶åœ¨æ¦‚å¿µç§»é™¤ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µç§»é™¤æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨æ–°å…´æ¨¡å‹ä¸­ï¼Œå¦‚Stable Diffusion v3å’ŒFluxä¸­é¢ä¸´æ›´å¤šæŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹èå…¥äº†æ–°çš„ç‰¹æ€§å¦‚æµåŒ¹é…å’ŒåŸºäºTransformerçš„æ¶æ„ã€‚</li>
<li>EraseAnythingæ˜¯é¦–ä¸ªé’ˆå¯¹æœ€æ–°æµå¼T2Iæ¡†æ¶è®¾è®¡çš„æ¦‚å¿µç§»é™¤æ–¹æ³•ã€‚è§£å†³äº†ä¹‹å‰ä¸ºæ—§T2IèŒƒå¼è®¾è®¡çš„æ¦‚å¿µç§»é™¤æŠ€æœ¯åœ¨æœ€æ–°æ¨¡å‹ä¸­çš„ä¸é€‚ç”¨é—®é¢˜ã€‚</li>
<li>EraseAnythingé€šè¿‡å°†æ¦‚å¿µç§»é™¤å»ºæ¨¡ä¸ºåŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨LoRAæŠ€æœ¯è¿›è¡Œå‚æ•°è°ƒæ•´æ¥é€‰æ‹©æ€§æŠ‘åˆ¶ä¸éœ€è¦çš„æ¿€æ´»ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨æ³¨æ„åŠ›å›¾æ­£åˆ™åŒ–ï¼Œç¡®ä¿ä¸éœ€è¦çš„æ¦‚å¿µåœ¨æ¨¡å‹ä¸­å—åˆ°æœ‰æ•ˆæŠ‘åˆ¶ã€‚</li>
<li>EraseAnythingå¼•å…¥äº†ä¸€ç§è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç§»é™¤ä¸éœ€è¦çš„æ¦‚å¿µæ—¶ä¸ä¼šæŸå®³æ¨¡å‹åœ¨å…¶ä»–æ¦‚å¿µä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEraseAnythingåœ¨å¤šç§æ¦‚å¿µç§»é™¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¡«è¡¥äº†å½“å‰ç ”ç©¶çš„ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c690d53acb23fc460e5c07668463fd3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f237d295f55413419b8f4bfb2cb1056.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9acf677d168d89c91a7cfb5c1be74884.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FairDiffusion-Enhancing-Equity-in-Latent-Diffusion-Models-via-Fair-Bayesian-Perturbation"><a href="#FairDiffusion-Enhancing-Equity-in-Latent-Diffusion-Models-via-Fair-Bayesian-Perturbation" class="headerlink" title="FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair   Bayesian Perturbation"></a>FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair   Bayesian Perturbation</h2><p><strong>Authors:Yan Luo, Muhammad Osama Khan, Congcong Wen, Muhammad Muneeb Afzal, Titus Fidelis Wuermeling, Min Shi, Yu Tian, Yi Fang, Mengyu Wang</strong></p>
<p>Recent progress in generative AI, especially diffusion models, has demonstrated significant utility in text-to-image synthesis. Particularly in healthcare, these models offer immense potential in generating synthetic datasets and training medical students. However, despite these strong performances, it remains uncertain if the image generation quality is consistent across different demographic subgroups. To address this critical concern, we present the first comprehensive study on the fairness of medical text-to-image diffusion models. Our extensive evaluations of the popular Stable Diffusion model reveal significant disparities across gender, race, and ethnicity. To mitigate these biases, we introduce FairDiffusion, an equity-aware latent diffusion model that enhances fairness in both image generation quality as well as the semantic correlation of clinical features. In addition, we also design and curate FairGenMed, the first dataset for studying the fairness of medical generative models. Complementing this effort, we further evaluate FairDiffusion on two widely-used external medical datasets: HAM10000 (dermatoscopic images) and CheXpert (chest X-rays) to demonstrate FairDiffusionâ€™s effectiveness in addressing fairness concerns across diverse medical imaging modalities. Together, FairDiffusion and FairGenMed significantly advance research in fair generative learning, promoting equitable benefits of generative AI in healthcare. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆäººå·¥æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢çš„è¿›å±•ï¼Œå·²ç»æ˜¾ç¤ºå‡ºå·¨å¤§çš„å®ç”¨ä»·å€¼ã€‚ç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®é›†å’ŒåŸ¹è®­åŒ»å­¦ç”Ÿæ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›è¡¨ç°å¼ºåŠ²ï¼Œä½†ä¸åŒäººå£äºšç»„ä¹‹é—´çš„å›¾åƒç”Ÿæˆè´¨é‡æ˜¯å¦ä¸€è‡´ä»ç„¶ä¸ç¡®å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®æ‹…å¿§ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚æˆ‘ä»¬å¯¹æµè¡Œçš„Stable Diffusionæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œå…¶åœ¨æ€§åˆ«ã€ç§æ—å’Œæ°‘æ—æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†ç¼“è§£è¿™äº›åè§ï¼Œæˆ‘ä»¬å¼•å…¥äº†FairDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªæ³¨é‡å…¬å¹³æ€§çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ƒæé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡å’Œä¸´åºŠç‰¹å¾è¯­ä¹‰ç›¸å…³æ€§çš„å…¬å¹³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡å’Œæ•´ç†äº†FairGenMedï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç ”ç©¶åŒ»ç–—ç”Ÿæˆæ¨¡å‹å…¬å¹³æ€§çš„æ•°æ®é›†ã€‚ä½œä¸ºè¡¥å……ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤–éƒ¨åŒ»ç–—æ•°æ®é›†ä¸Šå¯¹FairDiffusionè¿›è¡Œäº†è¯„ä¼°ï¼šHAM10000ï¼ˆçš®è‚¤ç§‘å›¾åƒï¼‰å’ŒCheXpertï¼ˆèƒ¸éƒ¨Xå°„çº¿ï¼‰ï¼Œä»¥è¯æ˜FairDiffusionåœ¨ä¸åŒåŒ»ç–—æˆåƒæ¨¡å¼è§£å†³å…¬å¹³æ€§é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚æ€»ä¹‹ï¼ŒFairDiffusionå’ŒFairGenMedçš„ç›¸ç»“åˆï¼Œæ˜¾è‘—æ¨åŠ¨äº†å…¬å¹³ç”Ÿæˆå­¦ä¹ çš„ç ”ç©¶ï¼Œä¿ƒè¿›äº†ç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å…¬å¹³å—ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20374v1">PDF</a> The data and code are made publicly available at   <a target="_blank" rel="noopener" href="https://github.com/Harvard-Ophthalmology-AI-Lab/FairDiffusion">https://github.com/Harvard-Ophthalmology-AI-Lab/FairDiffusion</a></p>
<p><strong>Summary</strong><br>     æ–‡æœ¬ä»‹ç»äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®é›†å’ŒåŸ¹è®­åŒ»å­¦ç”Ÿæ–¹é¢çš„æ½œåŠ›ã€‚æ–‡ç« è¿˜æå‡ºäº†å¯¹åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å…¬å¹³æ€§çš„é¦–æ¬¡å…¨é¢ç ”ç©¶ï¼Œæ­ç¤ºäº†æ€§åˆ«ã€ç§æ—å’Œæ°‘æ—ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶ä»‹ç»äº†æ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆè´¨é‡å’Œä¸´åºŠç‰¹å¾è¯­ä¹‰ç›¸å…³æ€§çš„å…¬å¹³æ‰©æ•£æ¨¡å‹ï¼ˆFairDiffusionï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡å’Œæ•´ç†äº†ç”¨äºç ”ç©¶åŒ»ç–—ç”Ÿæˆæ¨¡å‹å…¬å¹³æ€§çš„FairGenMedæ•°æ®é›†ï¼Œå¹¶åœ¨ä¸¤ä¸ªå¸¸ç”¨çš„å¤–éƒ¨åŒ»ç–—æ•°æ®é›†ä¸Šè¯„ä¼°äº†FairDiffusionçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®é›†å’ŒåŸ¹è®­åŒ»å­¦ç”Ÿæ–¹é¢è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚</li>
<li>åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>å¯¹æµè¡Œçš„Stable Diffusionæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨ä¸åŒäººç¾¤ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>å¼•å…¥FairDiffusionæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆè´¨é‡å’Œä¸´åºŠç‰¹å¾çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚</li>
<li>è®¾è®¡å’Œæ•´ç†äº†ç”¨äºç ”ç©¶åŒ»ç–—ç”Ÿæˆæ¨¡å‹å…¬å¹³æ€§çš„FairGenMedæ•°æ®é›†ã€‚</li>
<li>FairDiffusionåœ¨ä¸¤ä¸ªå¸¸ç”¨çš„å¤–éƒ¨åŒ»ç–—æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5069eadf372c302db852c07d630d762a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multi-Modality-Driven-LoRA-for-Adverse-Condition-Depth-Estimation"><a href="#Multi-Modality-Driven-LoRA-for-Adverse-Condition-Depth-Estimation" class="headerlink" title="Multi-Modality Driven LoRA for Adverse Condition Depth Estimation"></a>Multi-Modality Driven LoRA for Adverse Condition Depth Estimation</h2><p><strong>Authors:Guanglei Yang, Rui Tian, Yongqiang Zhang, Zhun Zhong, Yongqiang Li, Wangmeng Zuo</strong></p>
<p>The autonomous driving community is increasingly focused on addressing corner case problems, particularly those related to ensuring driving safety under adverse conditions (e.g., nighttime, fog, rain). To this end, the task of Adverse Condition Depth Estimation (ACDE) has gained significant attention. Previous approaches in ACDE have primarily relied on generative models, which necessitate additional target images to convert the sunny condition into adverse weather, or learnable parameters for feature augmentation to adapt domain gaps, resulting in increased model complexity and tuning efforts. Furthermore, unlike CLIP-based methods where textual and visual features have been pre-aligned, depth estimation models lack sufficient alignment between multimodal features, hindering coherent understanding under adverse conditions. To address these limitations, we propose Multi-Modality Driven LoRA (MMD-LoRA), which leverages low-rank adaptation matrices for efficient fine-tuning from source-domain to target-domain. It consists of two core components: Prompt Driven Domain Alignment (PDDA) and Visual-Text Consistent Contrastive Learning(VTCCL). During PDDA, the image encoder with MMD-LoRA generates target-domain visual representations, supervised by alignment loss that the source-target difference between language and image should be equal. Meanwhile, VTCCL bridges the gap between textual features from CLIP and visual features from diffusion model, pushing apart different weather representations (vision and text) and bringing together similar ones. Through extensive experiments, the proposed method achieves state-of-the-art performance on the nuScenes and Oxford RobotCar datasets, underscoring robustness and efficiency in adapting to varied adverse environments. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶ç¤¾åŒºè¶Šæ¥è¶Šå…³æ³¨è¾¹ç¼˜æƒ…å†µé—®é¢˜çš„å¤„ç†ï¼Œå°¤å…¶æ˜¯é‚£äº›ä¸æ¶åŠ£æ¡ä»¶ä¸‹çš„é©¾é©¶å®‰å…¨æœ‰å…³çš„æƒ…å¢ƒï¼ˆä¾‹å¦‚å¤œé—´ã€é›¾å¤©å’Œé›¨å¤©ï¼‰ã€‚ä¸ºæ­¤ï¼Œæ¶åŠ£æ¡ä»¶ä¸‹çš„æ·±åº¦ä¼°è®¡ï¼ˆACDEï¼‰ä»»åŠ¡å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ä»¥å‰çš„ACDEæ–¹æ³•ä¸»è¦ä¾èµ–äºç”Ÿæˆæ¨¡å‹ï¼Œéœ€è¦é¢å¤–çš„ç›®æ ‡å›¾åƒå°†æ™´æœ—æ¡ä»¶è½¬æ¢ä¸ºæ¶åŠ£å¤©æ°”ï¼Œæˆ–è€…ä½¿ç”¨å¯å­¦ä¹ çš„å‚æ•°è¿›è¡Œç‰¹å¾å¢å¼ºä»¥é€‚åº”é¢†åŸŸå·®å¼‚ï¼Œè¿™å¢åŠ äº†æ¨¡å‹å¤æ‚æ€§å’Œè°ƒæ•´å·¥ä½œé‡ã€‚æ­¤å¤–ï¼Œä¸åŸºäºCLIPçš„æ–¹æ³•ä¸åŒï¼Œå…¶ä¸­æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å·²ç»é¢„å…ˆå¯¹é½ï¼Œæ·±åº¦ä¼°è®¡æ¨¡å‹åœ¨å¤šç§æ¨¡æ€ç‰¹å¾ä¹‹é—´ç¼ºä¹è¶³å¤Ÿçš„å¯¹é½ï¼Œé˜»ç¢äº†æ¶åŠ£æ¡ä»¶ä¸‹çš„è¿è´¯ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€é©±åŠ¨çš„LoRAï¼ˆMMD-LoRAï¼‰ï¼Œå®ƒåˆ©ç”¨ä½ç§©é€‚åº”çŸ©é˜µè¿›è¡Œé«˜æ•ˆçš„æºåŸŸåˆ°ç›®æ ‡åŸŸçš„å¾®è°ƒã€‚å®ƒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæç¤ºé©±åŠ¨çš„åŸŸå¯¹é½ï¼ˆPDDAï¼‰å’Œè§†è§‰æ–‡æœ¬ä¸€è‡´å¯¹æ¯”å­¦ä¹ ï¼ˆVTCCLï¼‰ã€‚åœ¨PDDAæœŸé—´ï¼Œå¸¦æœ‰MMD-LoRAçš„å›¾åƒç¼–ç å™¨ç”Ÿæˆç›®æ ‡åŸŸçš„è§†è§‰è¡¨ç¤ºï¼Œç”±å¯¹é½æŸå¤±è¿›è¡Œç›‘ç£ï¼Œè¯¥æŸå¤±ä½¿æºè¯­è¨€å’Œç›®æ ‡å›¾åƒä¹‹é—´çš„å·®å¼‚ç›¸ç­‰ã€‚åŒæ—¶ï¼ŒVTCCLæ¡¥æ¥äº†CLIPçš„æ–‡æœ¬ç‰¹å¾å’Œæ‰©æ•£æ¨¡å‹çš„è§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œå°†ä¸åŒçš„å¤©æ°”è¡¨ç¤ºï¼ˆè§†è§‰å’Œæ–‡æœ¬ï¼‰åŒºåˆ†å¼€æ¥å¹¶å°†ç›¸ä¼¼çš„è¡¨ç¤ºèšé›†åœ¨ä¸€èµ·ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨nuSceneså’ŒOxford RobotCaræ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†åœ¨é€‚åº”å„ç§æ¶åŠ£ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20162v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­çš„æç«¯æƒ…å†µé—®é¢˜å¤„ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„é©¾é©¶å®‰å…¨é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºAdverse Condition Depth Estimationï¼ˆACDEï¼‰ä»»åŠ¡å¹¶ä»‹ç»äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼šMulti-Modality Driven LoRAï¼ˆMMD-LoRAï¼‰ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä½ç§©é€‚é…çŸ©é˜µï¼Œä¼˜åŒ–äº†æºåŸŸåˆ°ç›®æ ‡åŸŸçš„ç²¾ç»†è°ƒæ•´ï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šPrompt Driven Domain Alignmentï¼ˆPDDAï¼‰å’ŒVisual-Text Consistent Contrastive Learningï¼ˆVTCCLï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨nuSceneså’ŒOxford RobotCaræ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨é€‚åº”ä¸åŒæ¶åŠ£ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨é©¾é©¶é¢†åŸŸæ—¥ç›Šå…³æ³¨æç«¯æƒ…å†µé—®é¢˜å¤„ç†ï¼Œç‰¹åˆ«æ˜¯æ¶åŠ£æ¡ä»¶ä¸‹çš„é©¾é©¶å®‰å…¨ã€‚</li>
<li>Adverse Condition Depth Estimation (ACDE) ä»»åŠ¡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç”Ÿæˆæ¨¡å‹ï¼Œéœ€è¦é¢å¤–çš„ç›®æ ‡å›¾åƒæˆ–å­¦ä¹ å‚æ•°è¿›è¡Œç‰¹å¾å¢å¼ºä»¥é€‚åº”é¢†åŸŸå·®å¼‚ï¼Œå¯¼è‡´æ¨¡å‹å¤æ‚å’Œè°ƒè¯•å·¥ä½œé‡å¤§ã€‚</li>
<li>Multi-Modality Driven LoRA (MMD-LoRA) è¢«æå‡ºä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ƒä½¿ç”¨ä½ç§©é€‚é…çŸ©é˜µè¿›è¡Œé«˜æ•ˆçš„æºåŸŸåˆ°ç›®æ ‡åŸŸçš„å¾®è°ƒã€‚</li>
<li>MMD-LoRA åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šPDDA å’Œ VTCCLã€‚PDDA è´Ÿè´£ç”Ÿæˆç›®æ ‡åŸŸè§†è§‰è¡¨ç¤ºï¼Œè€Œ VTCCL åˆ™å¼¥åˆäº†æ–‡æœ¬ç‰¹å¾å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d6ba7e737ca1fd7784d8845ded42829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-110a999a5ed86e6cfa01e5eb58c42228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41e9dfb01cefe108ebdd6fcd1acf3820.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78889cb0db855720c42c5d2c166f4375.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15a4f6cdc42ed92a1bd3ccf2ec1032c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MADiff-Text-Guided-Fashion-Image-Editing-with-Mask-Prediction-and-Attention-Enhanced-Diffusion"><a href="#MADiff-Text-Guided-Fashion-Image-Editing-with-Mask-Prediction-and-Attention-Enhanced-Diffusion" class="headerlink" title="MADiff: Text-Guided Fashion Image Editing with Mask Prediction and   Attention-Enhanced Diffusion"></a>MADiff: Text-Guided Fashion Image Editing with Mask Prediction and   Attention-Enhanced Diffusion</h2><p><strong>Authors:Zechao Zhan, Dehong Gao, Jinxia Zhang, Jiale Huang, Yang Hu, Xin Wang</strong></p>
<p>Text-guided image editing model has achieved great success in general domain. However, directly applying these models to the fashion domain may encounter two issues: (1) Inaccurate localization of editing region; (2) Weak editing magnitude. To address these issues, the MADiff model is proposed. Specifically, to more accurately identify editing region, the MaskNet is proposed, in which the foreground region, densepose and mask prompts from large language model are fed into a lightweight UNet to predict the mask for editing region. To strengthen the editing magnitude, the Attention-Enhanced Diffusion Model is proposed, where the noise map, attention map, and the mask from MaskNet are fed into the proposed Attention Processor to produce a refined noise map. By integrating the refined noise map into the diffusion model, the edited image can better align with the target prompt. Given the absence of benchmarks in fashion image editing, we constructed a dataset named Fashion-E, comprising 28390 image-text pairs in the training set, and 2639 image-text pairs for four types of fashion tasks in the evaluation set. Extensive experiments on Fashion-E demonstrate that our proposed method can accurately predict the mask of editing region and significantly enhance editing magnitude in fashion image editing compared to the state-of-the-art methods. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸå·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç›´æ¥å°†è¿™äº›æ¨¡å‹åº”ç”¨äºæ—¶å°šé¢†åŸŸå¯èƒ½ä¼šé‡åˆ°ä¸¤ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰ç¼–è¾‘åŒºåŸŸå®šä½ä¸å‡†ç¡®ï¼›ï¼ˆ2ï¼‰ç¼–è¾‘å¹…åº¦è¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†MADiffæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æ›´å‡†ç¡®åœ°è¯†åˆ«ç¼–è¾‘åŒºåŸŸï¼Œæå‡ºäº†MaskNetï¼Œå…¶ä¸­å°†å‰æ™¯åŒºåŸŸã€denseposeå’Œå¤§è¯­è¨€æ¨¡å‹çš„æ©ç æç¤ºè¾“å…¥åˆ°è½»é‡çº§çš„UNetä¸­ï¼Œä»¥é¢„æµ‹ç¼–è¾‘åŒºåŸŸçš„æ©ç ã€‚ä¸ºäº†å¢å¼ºç¼–è¾‘å¹…åº¦ï¼Œæå‡ºäº†æ³¨æ„åŠ›å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­å°†å™ªå£°å›¾ã€æ³¨æ„åŠ›å›¾å’ŒMaskNetçš„æ©ç è¾“å…¥åˆ°æå‡ºçš„æ³¨æ„åŠ›å¤„ç†å™¨ä¸­ï¼Œä»¥äº§ç”Ÿç²¾ç»†çš„å™ªå£°å›¾ã€‚é€šè¿‡å°†ç²¾ç»†çš„å™ªå£°å›¾é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç¼–è¾‘åçš„å›¾åƒå¯ä»¥æ›´å¥½åœ°ä¸ç›®æ ‡æç¤ºå¯¹é½ã€‚é‰´äºæ—¶å°šå›¾åƒç¼–è¾‘ç¼ºä¹åŸºå‡†æµ‹è¯•é›†ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºFashion-Eçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬è®­ç»ƒé›†ä¸­çš„28390ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œä»¥åŠè¯„ä¼°é›†ä¸­çš„2639ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œç”¨äºå››ç§æ—¶å°šä»»åŠ¡ã€‚åœ¨Fashion-Eä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç¼–è¾‘åŒºåŸŸçš„æ©ç ï¼Œå¹¶åœ¨æ—¶å°šå›¾åƒç¼–è¾‘ä¸­æ˜¾è‘—å¢å¼ºç¼–è¾‘å¹…åº¦ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20062v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨æ—¶å°šé¢†åŸŸåº”ç”¨æ—¶é¢ä¸´åŒºåŸŸå®šä½ä¸å‡†ç¡®å’Œç¼–è¾‘å¼ºåº¦å¼±çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†MADiffæ¨¡å‹ï¼ŒåŒ…æ‹¬MaskNetå’ŒAttention-Enhanced Diffusion Modelã€‚MaskNeté€šè¿‡å‰æ™¯åŒºåŸŸã€denseposeå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ©è†œæç¤ºæ¥é¢„æµ‹ç¼–è¾‘åŒºåŸŸæ©è†œï¼Œæé«˜åŒºåŸŸå®šä½å‡†ç¡®æ€§ã€‚Attention-Enhanced Diffusion Modelåˆ™é€šè¿‡å™ªå£°å›¾ã€æ³¨æ„åŠ›å›¾å’ŒMaskNetçš„æ©è†œæ¥äº§ç”Ÿç²¾ç»†çš„å™ªå£°å›¾ï¼Œå¢å¼ºç¼–è¾‘å¼ºåº¦ã€‚å› æ—¶å°šå›¾åƒç¼–è¾‘ç¼ºä¹åŸºå‡†æµ‹è¯•é›†ï¼Œæ•…æ„å»ºäº†Fashion-Eæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å‡†ç¡®é¢„æµ‹ç¼–è¾‘åŒºåŸŸæ©è†œï¼Œæ˜¾è‘—æé«˜æ—¶å°šå›¾åƒç¼–è¾‘çš„ç¼–è¾‘å¼ºåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸæˆåŠŸï¼Œä½†åœ¨æ—¶å°šé¢†åŸŸé¢ä¸´åŒºåŸŸå®šä½ä¸å‡†ç¡®å’Œç¼–è¾‘å¼ºåº¦å¼±çš„é—®é¢˜ã€‚</li>
<li>MADiffæ¨¡å‹åŒ…æ‹¬MaskNetå’ŒAttention-Enhanced Diffusion Modelï¼Œåˆ†åˆ«è§£å†³åŒºåŸŸå®šä½å’Œç¼–è¾‘å¼ºåº¦é—®é¢˜ã€‚</li>
<li>MaskNetä½¿ç”¨å‰æ™¯åŒºåŸŸã€denseposeå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ©è†œæç¤ºæ¥é¢„æµ‹ç¼–è¾‘åŒºåŸŸæ©è†œã€‚</li>
<li>Attention-Enhanced Diffusion Modelé€šè¿‡å™ªå£°å›¾ã€æ³¨æ„åŠ›å›¾å’ŒMaskNetçš„æ©è†œäº§ç”Ÿç²¾ç»†çš„å™ªå£°å›¾ï¼Œå¢å¼ºç¼–è¾‘å¼ºåº¦ã€‚</li>
<li>æ—¶å°šå›¾åƒç¼–è¾‘é¢†åŸŸç¼ºä¹åŸºå‡†æµ‹è¯•é›†ï¼Œå› æ­¤æ„å»ºäº†Fashion-Eæ•°æ®é›†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMADiffæ¨¡å‹èƒ½å‡†ç¡®é¢„æµ‹ç¼–è¾‘åŒºåŸŸæ©è†œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cf184a82602fc94107da8b467fecb04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c907c746d73aad0896920241206e03ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66f0ebf2a421aee0d399e09ec5808b32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ec4956246822859578b32b0dfa3353b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c012f3b3dbd75c337c0ea00a5b6eaf9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6d3cef7914b88c975cfe2400103130a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="An-Ordinary-Differential-Equation-Sampler-with-Stochastic-Start-for-Diffusion-Bridge-Models"><a href="#An-Ordinary-Differential-Equation-Sampler-with-Stochastic-Start-for-Diffusion-Bridge-Models" class="headerlink" title="An Ordinary Differential Equation Sampler with Stochastic Start for   Diffusion Bridge Models"></a>An Ordinary Differential Equation Sampler with Stochastic Start for   Diffusion Bridge Models</h2><p><strong>Authors:Yuang Wang, Pengfei Jin, Li Zhang, Quanzheng Li, Zhiqiang Chen, Dufan Wu</strong></p>
<p>Diffusion bridge models have demonstrated promising performance in conditional image generation tasks, such as image restoration and translation, by initializing the generative process from corrupted images instead of pure Gaussian noise. However, existing diffusion bridge models often rely on Stochastic Differential Equation (SDE) samplers, which result in slower inference speed compared to diffusion models that employ high-order Ordinary Differential Equation (ODE) solvers for acceleration. To mitigate this gap, we propose a high-order ODE sampler with a stochastic start for diffusion bridge models. To overcome the singular behavior of the probability flow ODE (PF-ODE) at the beginning of the reverse process, a posterior sampling approach was introduced at the first reverse step. The sampling was designed to ensure a smooth transition from corrupted images to the generative trajectory while reducing discretization errors. Following this stochastic start, Heunâ€™s second-order solver is applied to solve the PF-ODE, achieving high perceptual quality with significantly reduced neural function evaluations (NFEs). Our method is fully compatible with pretrained diffusion bridge models and requires no additional training. Extensive experiments on image restoration and translation tasks, including super-resolution, JPEG restoration, Edges-to-Handbags, and DIODE-Outdoor, demonstrated that our sampler outperforms state-of-the-art methods in both visual quality and Frechet Inception Distance (FID). </p>
<blockquote>
<p>æ‰©æ•£æ¡¥æ¨¡å‹åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚å›¾åƒæ¢å¤å’Œç¿»è¯‘ï¼Œå®ƒé€šè¿‡ä»å—æŸå›¾åƒè€Œä¸æ˜¯çº¯é«˜æ–¯å™ªå£°å¼€å§‹ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¡¥æ¨¡å‹é€šå¸¸ä¾èµ–äºéšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰é‡‡æ ·å™¨ï¼Œä¸é‡‡ç”¨é«˜é˜¶å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨è¿›è¡ŒåŠ é€Ÿçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä¸ºæ‰©æ•£æ¡¥æ¨¡å‹æå‡ºäº†ä¸€ç§å…·æœ‰éšæœºèµ·å§‹çš„é«˜é˜¶ODEé‡‡æ ·å™¨ã€‚ä¸ºäº†è§£å†³åå‘è¿‡ç¨‹ä¸­æ¦‚ç‡æµODEï¼ˆPF-ODEï¼‰åœ¨èµ·å§‹æ—¶çš„å¥‡å¼‚è¡Œä¸ºï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸€æ¬¡åå‘æ­¥éª¤ä¸­å¼•å…¥äº†åé‡‡æ ·æ–¹æ³•ã€‚é‡‡æ ·çš„è®¾è®¡æ˜¯ä¸ºäº†ç¡®ä¿ä»å—æŸå›¾åƒåˆ°ç”Ÿæˆè½¨è¿¹çš„å¹³ç¨³è¿‡æ¸¡ï¼ŒåŒæ—¶å‡å°‘ç¦»æ•£åŒ–è¯¯å·®ã€‚éšåé‡‡ç”¨Heunçš„äºŒé˜¶æ±‚è§£å™¨è§£å†³PF-ODEï¼Œåœ¨ä¿è¯æ„ŸçŸ¥è´¨é‡é«˜çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†ç¥ç»åŠŸèƒ½è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸é¢„è®­ç»ƒçš„æ‰©æ•£æ¡¥æ¨¡å‹å®Œå…¨å…¼å®¹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚åœ¨å›¾åƒæ¢å¤å’Œç¿»è¯‘ä»»åŠ¡çš„å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€JPEGæ¢å¤ã€Edges-to-Handbagså’ŒDIODE-Outdoorç­‰å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é‡‡æ ·å™¨åœ¨è§†è§‰è´¨é‡å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰æ–¹é¢éƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19992v1">PDF</a> 9 pages, 5 figures, This work has been submitted to the IEEE for   possible publication</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¡¥æ¨¡å‹åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¦‚å›¾åƒæ¢å¤å’Œç¿»è¯‘ã€‚å®ƒé€šè¿‡ä»æŸåçš„å›¾åƒå¼€å§‹ç”Ÿæˆè¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨çº¯é«˜æ–¯å™ªå£°ï¼Œå®ç°æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ‰©æ•£æ¡¥æ¨¡å‹é€šå¸¸ä¾èµ–éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰é‡‡æ ·å™¨ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆéšæœºèµ·å§‹çš„é«˜é˜¶å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰é‡‡æ ·å™¨ã€‚åœ¨åå‘è¿‡ç¨‹å¼€å§‹æ—¶ï¼Œå¼•å…¥æ¦‚ç‡æµODEï¼ˆPF-ODEï¼‰çš„åé‡‡æ ·æ–¹æ³•ï¼Œç¡®ä¿ä»æŸåå›¾åƒåˆ°ç”Ÿæˆè½¨è¿¹çš„å¹³ç¨³è¿‡æ¸¡ï¼ŒåŒæ—¶å‡å°‘ç¦»æ•£åŒ–è¯¯å·®ã€‚éšåä½¿ç”¨Heunçš„äºŒé˜¶æ±‚è§£å™¨è§£å†³PF-ODEé—®é¢˜ï¼Œå®ç°äº†æ„ŸçŸ¥è´¨é‡çš„æå‡å’Œç¥ç»å‡½æ•°è¯„ä¼°ï¼ˆNFEsï¼‰çš„æ˜¾è‘—é™ä½ã€‚è¯¥æ–¹æ³•ä¸é¢„è®­ç»ƒçš„æ‰©æ•£æ¡¥æ¨¡å‹å®Œå…¨å…¼å®¹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„é‡‡æ ·å™¨åœ¨è§†è§‰è´¨é‡å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¡¥æ¨¡å‹é€šè¿‡ä»æŸåå›¾åƒå¼€å§‹ç”Ÿæˆè¿‡ç¨‹å®ç°æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡æ€§èƒ½çš„æå‡ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¡¥æ¨¡å‹ä¾èµ–SDEé‡‡æ ·å™¨å¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆéšæœºèµ·å§‹çš„é«˜é˜¶ODEé‡‡æ ·å™¨æ¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶æå‡ç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ¦‚ç‡æµODEçš„åé‡‡æ ·æ–¹æ³•ç¡®ä¿ä»æŸåå›¾åƒåˆ°ç”Ÿæˆè½¨è¿¹çš„å¹³ç¨³è¿‡æ¸¡ã€‚</li>
<li>ä½¿ç”¨Heunçš„äºŒé˜¶æ±‚è§£å™¨è§£å†³PF-ODEé—®é¢˜ï¼Œæé«˜æ„ŸçŸ¥è´¨é‡å¹¶é™ä½ç¥ç»å‡½æ•°è¯„ä¼°ã€‚</li>
<li>æ–¹æ³•ä¸é¢„è®­ç»ƒçš„æ‰©æ•£æ¡¥æ¨¡å‹å…¼å®¹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d504ad7b18576e30cf4469496809785d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56a401fb76b378fef40dfd7e3e54dc7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f61c5e8bf64391e3fa8af73e628ceeea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64524790ee4ae086d5e1458c7522ca5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c606b27e21365bf72d1c862888c3d944.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a254d83c13a4e1dc207184c99e805ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de6ea93b7372d8c63d3c7d20d06f1aa2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a8c203cf03eeb977522b9b30ca524e1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DriveEditor-A-Unified-3D-Information-Guided-Framework-for-Controllable-Object-Editing-in-Driving-Scenes"><a href="#DriveEditor-A-Unified-3D-Information-Guided-Framework-for-Controllable-Object-Editing-in-Driving-Scenes" class="headerlink" title="DriveEditor: A Unified 3D Information-Guided Framework for Controllable   Object Editing in Driving Scenes"></a>DriveEditor: A Unified 3D Information-Guided Framework for Controllable   Object Editing in Driving Scenes</h2><p><strong>Authors:Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou</strong></p>
<p>Vision-centric autonomous driving systems require diverse data for robust training and evaluation, which can be augmented by manipulating object positions and appearances within existing scene captures. While recent advancements in diffusion models have shown promise in video editing, their application to object manipulation in driving scenarios remains challenging due to imprecise positional control and difficulties in preserving high-fidelity object appearances. To address these challenges in position and appearance control, we introduce DriveEditor, a diffusion-based framework for object editing in driving videos. DriveEditor offers a unified framework for comprehensive object editing operations, including repositioning, replacement, deletion, and insertion. These diverse manipulations are all achieved through a shared set of varying inputs, processed by identical position control and appearance maintenance modules. The position control module projects the given 3D bounding box while preserving depth information and hierarchically injects it into the diffusion process, enabling precise control over object position and orientation. The appearance maintenance module preserves consistent attributes with a single reference image by employing a three-tiered approach: low-level detail preservation, high-level semantic maintenance, and the integration of 3D priors from a novel view synthesis model. Extensive qualitative and quantitative evaluations on the nuScenes dataset demonstrate DriveEditorâ€™s exceptional fidelity and controllability in generating diverse driving scene edits, as well as its remarkable ability to facilitate downstream tasks. Project page: <a target="_blank" rel="noopener" href="https://yvanliang.github.io/DriveEditor">https://yvanliang.github.io/DriveEditor</a>. </p>
<blockquote>
<p>è§†è§‰ä¸ºä¸­å¿ƒçš„è‡ªé©¾ç³»ç»Ÿéœ€è¦å¤šæ ·åŒ–çš„æ•°æ®è¿›è¡Œç¨³å¥çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œè¿™äº›æ•°æ®å¯ä»¥é€šè¿‡æ“ä½œç°æœ‰åœºæ™¯æ•æ‰ä¸­çš„ç‰©ä½“ä½ç½®å’Œå¤–è§‚æ¥æ‰©å……ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„æœ€æ–°è¿›å±•åœ¨è§†é¢‘ç¼–è¾‘æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨é©¾é©¶åœºæ™¯ä¸­çš„ç‰©ä½“æ“ä½œåº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦æ˜¯ç”±äºä½ç½®æ§åˆ¶ä¸ç²¾ç¡®ä»¥åŠä¿æŒé«˜ä¿çœŸç‰©ä½“å¤–è§‚çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›ä½ç½®å’Œå¤–è§‚æ§åˆ¶çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DriveEditorï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„é©¾é©¶è§†é¢‘ç‰©ä½“ç¼–è¾‘æ¡†æ¶ã€‚DriveEditoræä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºå…¨é¢çš„ç‰©ä½“ç¼–è¾‘æ“ä½œï¼ŒåŒ…æ‹¬é‡æ–°å®šä½ã€æ›¿æ¢ã€åˆ é™¤å’Œæ’å…¥ã€‚è¿™äº›ä¸åŒçš„æ“ä½œéƒ½æ˜¯é€šè¿‡ä¸€ç»„å¯å˜çš„è¾“å…¥å®ç°çš„ï¼Œè¿™äº›è¾“å…¥ç”±ç›¸åŒçš„å®šä½æ§åˆ¶å’Œå¤–è§‚ç»´æŠ¤æ¨¡å—å¤„ç†ã€‚å®šä½æ§åˆ¶æ¨¡å—æŠ•å½±ç»™å®šçš„3Dè¾¹ç•Œæ¡†ï¼ŒåŒæ—¶ä¿ç•™æ·±åº¦ä¿¡æ¯å¹¶æŒ‰å±‚æ¬¡ç»“æ„æ³¨å…¥æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°å¯¹ç‰©ä½“ä½ç½®å’Œæ–¹å‘çš„ç²¾ç¡®æ§åˆ¶ã€‚å¤–è§‚ç»´æŠ¤æ¨¡å—é€šè¿‡é‡‡ç”¨ä¸‰çº§æ–¹æ³•ï¼šä¿ç•™ä½çº§åˆ«ç»†èŠ‚ã€ä¿æŒé«˜çº§è¯­ä¹‰ä»¥åŠä»æ–°å‹è§†å›¾åˆæˆæ¨¡å‹ä¸­æ•´åˆ3Då…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œä»¥å•ä¸€å‚è€ƒå›¾åƒä¿æŒä¸€è‡´çš„å±æ€§ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®šæ€§å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒDriveEditoråœ¨ç”Ÿæˆå„ç§é©¾é©¶åœºæ™¯ç¼–è¾‘æ–¹é¢å…·æœ‰å‡ºè‰²çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ï¼Œå¹¶ä¸”åœ¨ä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢å…·æœ‰æ˜¾è‘—çš„èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://yvanliang.github.io/DriveEditor%E3%80%82">https://yvanliang.github.io/DriveEditorã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19458v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é©¾é©¶è§†é¢‘å¯¹è±¡ç¼–è¾‘æ¡†æ¶DriveEditorã€‚è¯¥æ¡†æ¶å¯å®ç°é©¾é©¶è§†é¢‘ä¸­å¯¹è±¡çš„å…¨é¢ç¼–è¾‘æ“ä½œï¼ŒåŒ…æ‹¬é‡æ–°å®šä½ã€æ›¿æ¢ã€åˆ é™¤å’Œæ’å…¥ç­‰ã€‚é€šè¿‡ç²¾å‡†çš„ä½ç½®æ§åˆ¶æ¨¡å—å’Œå¤–è§‚ç»´æŠ¤æ¨¡å—ï¼ŒDriveEditorèƒ½å¤Ÿåœ¨ç¼–è¾‘å¯¹è±¡æ—¶ä¿æŒé«˜ç²¾åº¦å’Œé«˜ä¿çœŸåº¦ã€‚å…¶åœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDriveEditoråœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„é©¾é©¶åœºæ™¯ç¼–è¾‘æ–¹é¢å…·æœ‰å‡ºè‰²çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é©¾é©¶è§†é¢‘å¯¹è±¡ç¼–è¾‘ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>DriveEditoræ¡†æ¶æ”¯æŒå¤šç§é©¾é©¶è§†é¢‘ä¸­çš„å¯¹è±¡ç¼–è¾‘æ“ä½œã€‚</li>
<li>ä½ç½®æ§åˆ¶æ¨¡å—èƒ½å¤Ÿå®ç°å¯¹è±¡ä½ç½®çš„ç²¾ç¡®æ§åˆ¶ï¼ŒåŒæ—¶ä¿ç•™æ·±åº¦ä¿¡æ¯ã€‚</li>
<li>å¤–è§‚ç»´æŠ¤æ¨¡å—é‡‡ç”¨ä¸‰å±‚æ–¹æ³•ï¼Œä¿æŒä¸å‚è€ƒå›¾åƒä¸€è‡´çš„å±æ€§ã€‚</li>
<li>DriveEditoråœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰é«˜çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ã€‚</li>
<li>DriveEditoræœ‰åŠ©äºç”Ÿæˆå¤šæ ·åŒ–çš„é©¾é©¶åœºæ™¯ç¼–è¾‘ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ä¿ƒè¿›ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3c1755b2b6754461fd6dad0d72d6337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42287799f0b288d93d0ca6dfcefd0ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02500440dec6e711fe234d097a42ba06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6265359a359d848ee503a1bab3618f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfca1b5d19e04e4423b3d8c2a4e40aad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff2354afead1740dc42b5da862815338.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PartGen-Part-level-3D-Generation-and-Reconstruction-with-Multi-View-Diffusion-Models"><a href="#PartGen-Part-level-3D-Generation-and-Reconstruction-with-Multi-View-Diffusion-Models" class="headerlink" title="PartGen: Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models"></a>PartGen: Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models</h2><p><strong>Authors:Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, Andrea Vedaldi</strong></p>
<p>Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure. However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce PartGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing. </p>
<blockquote>
<p>æ–‡æœ¬æˆ–å›¾åƒåˆ°3Dç”Ÿæˆå™¨å’Œ3Dæ‰«æä»ªç°åœ¨å¯ä»¥ç”Ÿæˆå…·æœ‰é«˜è´¨é‡å½¢çŠ¶å’Œçº¹ç†çš„3Dèµ„äº§ã€‚è¿™äº›èµ„äº§é€šå¸¸ç”±å•ä¸€èåˆè¡¨ç¤ºç»„æˆï¼Œå¦‚éšå¼ç¥ç»ç½‘ç»œã€é«˜æ–¯æ··åˆæˆ–ç½‘æ ¼ï¼Œä½†æ²¡æœ‰æœ‰ç”¨çš„ç»“æ„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åº”ç”¨ç¨‹åºå’Œåˆ›æ„å·¥ä½œæµç¨‹è¦æ±‚èµ„äº§ç”±å¯ä»¥ç‹¬ç«‹æ“ä½œçš„å¤šä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ç»„æˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PartGenï¼Œè¿™æ˜¯ä¸€ç§ä»æ–‡æœ¬ã€å›¾åƒæˆ–æ— åºçš„3Då¯¹è±¡å¼€å§‹ç”Ÿæˆç”±æœ‰æ„ä¹‰çš„éƒ¨åˆ†ç»„æˆçš„3Dç‰©ä½“çš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œç»™å®šä¸€ä¸ª3Dç‰©ä½“çš„å¤šä¸ªè§†å›¾ï¼ˆæ— è®ºæ˜¯ç”Ÿæˆçš„è¿˜æ˜¯æ¸²æŸ“çš„ï¼‰ï¼Œå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¼šä»è¿™äº›è§†å›¾ä¸­æå–ä¸€å¥—åˆç†ä¸”è§†è§’ä¸€è‡´çš„éƒ¨åˆ†åˆ†å‰²ï¼Œå°†ç‰©ä½“åˆ†å‰²æˆå„ä¸ªéƒ¨åˆ†ã€‚ç„¶åï¼Œç¬¬äºŒä¸ªå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¼šåˆ†åˆ«å¤„ç†æ¯ä¸ªéƒ¨åˆ†ï¼Œå¡«å……é®æŒ¡ç‰©ï¼Œå¹¶ä½¿ç”¨è¿™äº›å®Œæˆçš„è§†å›¾è¿›è¡Œ3Dé‡å»ºï¼Œæ–¹æ³•æ˜¯å°†å…¶è¾“å…¥åˆ°3Dé‡å»ºç½‘ç»œä¸­ã€‚è¿™ä¸ªå®Œæˆè¿‡ç¨‹ä¼šè€ƒè™‘æ•´ä¸ªç‰©ä½“çš„ä¸Šä¸‹æ–‡ï¼Œä»¥ç¡®ä¿å„éƒ¨åˆ†èƒ½å¤Ÿç´§å¯†é›†æˆã€‚ç”Ÿæˆå®Œæˆæ¨¡å‹å¯ä»¥å¼¥è¡¥å› é®æŒ¡è€Œç¼ºå¤±çš„ä¿¡æ¯ï¼›åœ¨æç«¯æƒ…å†µä¸‹ï¼Œå®ƒå¯ä»¥æ ¹æ®è¾“å…¥çš„3Dèµ„äº§å®Œå…¨è™šæ„å‡ºä¸å¯è§çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨ç”Ÿæˆçš„å’ŒçœŸå®çš„3Dèµ„äº§ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶æ˜¾ç¤ºå‡ºå®ƒå¤§å¤§è¶…è¶Šäº†åˆ†å‰²å’Œéƒ¨ä»¶æå–çš„åŸºçº¿ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚3Déƒ¨ä»¶ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18608v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://silent-chen.github.io/PartGen/">https://silent-chen.github.io/PartGen/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPartGençš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»æ–‡æœ¬ã€å›¾åƒæˆ–æ— åºçš„3Då¯¹è±¡ä¸­ç”Ÿæˆç”±æœ‰æ„ä¹‰çš„éƒ¨ä»¶ç»„æˆçš„3Dç‰©ä½“ã€‚å®ƒé‡‡ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›è¡Œéƒ¨ä»¶åˆ†å‰²ï¼Œå¹¶å¡«å……é®æŒ¡éƒ¨åˆ†ï¼Œç„¶åé€šè¿‡3Dé‡å»ºç½‘ç»œè¿›è¡Œé‡å»ºã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿå¼¥è¡¥å› é®æŒ¡é€ æˆçš„ä¿¡æ¯ç¼ºå¤±ï¼Œç”šè‡³åœ¨æç«¯æƒ…å†µä¸‹å¯ä»¥åŸºäºè¾“å…¥çš„3Dèµ„äº§ç”Ÿæˆå®Œå…¨ä¸å¯è§çš„éƒ¨ä»¶ã€‚PartGenåœ¨ç”Ÿæˆå’ŒçœŸå®çš„3Dèµ„äº§ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„åˆ†å‰²å’Œéƒ¨ä»¶æå–æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†åœ¨3Déƒ¨ä»¶ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartGenæ˜¯ä¸€ç§èƒ½å¤Ÿä»æ–‡æœ¬ã€å›¾åƒæˆ–æ— åºçš„3Då¯¹è±¡ç”Ÿæˆç”±æœ‰æ„ä¹‰éƒ¨ä»¶ç»„æˆçš„3Dç‰©ä½“çš„æ–°æ–¹æ³•ã€‚</li>
<li>PartGené‡‡ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›è¡Œéƒ¨ä»¶åˆ†å‰²ï¼Œèƒ½å¤Ÿå°†ç‰©ä½“åˆ†æˆå¤šä¸ªå¯ç‹¬ç«‹æ“ä½œçš„éƒ¨ä»¶ã€‚</li>
<li>æ–¹æ³•ä¸­çš„é®æŒ¡å¡«å……å’Œ3Dé‡å»ºè¿‡ç¨‹èƒ½å¤ŸåŸºäºæ•´ä¸ªç‰©ä½“çš„ä¸Šä¸‹æ–‡è¿›è¡Œï¼Œç¡®ä¿éƒ¨ä»¶çš„æ•´åˆæ€§ã€‚</li>
<li>ç”Ÿæˆå®Œæˆæ¨¡å‹å¯ä»¥å¼¥è¡¥å› é®æŒ¡é€ æˆçš„ä¿¡æ¯ç¼ºå¤±ï¼Œç”šè‡³èƒ½å¤ŸåŸºäºè¾“å…¥çš„3Dèµ„äº§ç”Ÿæˆå®Œå…¨ä¸å¯è§çš„éƒ¨ä»¶ã€‚</li>
<li>PartGenåœ¨ç”Ÿæˆå’ŒçœŸå®3Dèµ„äº§ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„åˆ†å‰²å’Œéƒ¨ä»¶æå–æ–¹æ³•ã€‚</li>
<li>PartGenå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨3Dç¼–è¾‘ã€æ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a0c2ad11d9d056888d6b0cbc427340d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a4484bed5c2f49df6684085dd8cb8c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a04b43ae9c468b0c9a10571b740f65a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec6164f2b484e5b17e41a37dff6fa50.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Free-viewpoint-Human-Animation-with-Pose-correlated-Reference-Selection"><a href="#Free-viewpoint-Human-Animation-with-Pose-correlated-Reference-Selection" class="headerlink" title="Free-viewpoint Human Animation with Pose-correlated Reference Selection"></a>Free-viewpoint Human Animation with Pose-correlated Reference Selection</h2><p><strong>Authors:Fa-Ting Hong, Zhan Xu, Haiyang Liu, Qinjie Lin, Luchuan Song, Zhixin Shu, Yang Zhou, Duygu Ceylan, Dan Xu</strong></p>
<p>Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in&#x2F;zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„äººä½“åŠ¨ç”»æ—¨åœ¨æ ¹æ®æºäººä½“å›¾åƒä»¥åŠé©±åŠ¨ä¿¡å·ï¼ˆå¦‚ä¸€ç³»åˆ—å§¿åŠ¿ï¼‰æ¥ä½¿äººç‰©è§’è‰²åŠ¨ç”»åŒ–ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å§¿åŠ¿ï¼Œä½†åœ¨è§†ç‚¹å˜åŒ–è¾ƒå¤§æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼©æ”¾åœºæ™¯ï¼ˆå¦‚æ‘„åƒæœºä¸è§’è‰²çš„è·ç¦»å˜åŒ–ï¼‰ä¸­å°¤ä¸ºå¦‚æ­¤ã€‚è¿™é™åˆ¶äº†å…¶åœ¨ç”µå½±æ‹æ‘„è®¡åˆ’æˆ–ç›¸æœºæ§åˆ¶ç­‰æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å§¿æ€ç›¸å…³å‚è€ƒé€‰æ‹©æ‰©æ•£ç½‘ç»œï¼Œæ”¯æŒäººä½“åŠ¨ç”»ä¸­çš„å¤§å¹…è§†ç‚¹å˜åŒ–ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©ç½‘ç»œä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå› ä¸ºè§†ç‚¹çš„å¤§å¹…å˜åŒ–é€šå¸¸ä¼šå¯¼è‡´äººä½“å¤–è§‚ç»†èŠ‚ç¼ºå¤±ã€‚ä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å§¿æ€ç›¸å…³æ€§æ¨¡å—ï¼Œç”¨äºè®¡ç®—æœªå¯¹é½çš„ç›®æ ‡å’Œæºå§¿æ€ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œç„¶åæå‡ºäº†ä¸€ç§è‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«åŠ¨ç”»ç”Ÿæˆçš„å…³é”®åŒºåŸŸã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä»å…¬å…±TEDæ¼”è®²ä¸­æ•´ç†äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼Œå±•ç¤ºäº†åŒä¸€è§’è‰²çš„ä¸åŒé•œå¤´ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ ä¸åŒè§†è§’çš„åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ•°é‡çš„å‚è€ƒå›¾åƒä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§è§†ç‚¹å˜åŒ–æ–¹é¢ä¸å½“å‰æœ€ä½³æ–¹æ³•ç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œè‡ªé€‚åº”å‚è€ƒé€‰æ‹©èƒ½å¤Ÿé€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒåŒºåŸŸæ¥åœ¨è‡ªç”±è§†è§’ä¸‹ç”Ÿæˆäººä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17290v2">PDF</a> Under review; Project page:   <a target="_blank" rel="noopener" href="https://harlanhong.github.io/publications/fvhuman/index.html">https://harlanhong.github.io/publications/fvhuman/index.html</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„äººä½“åŠ¨ç”»æŠ€æœ¯èƒ½é€šè¿‡æºäººä½“å›¾åƒå’Œé©±åŠ¨ä¿¡å·ï¼ˆå¦‚ä¸€ç³»åˆ—å§¿åŠ¿ï¼‰æ¥åŠ¨ç”»åŒ–äººç‰©è§’è‰²ã€‚ç°æœ‰æ–¹æ³•è™½èƒ½ç”Ÿæˆé«˜ä¿çœŸå§¿åŠ¿ï¼Œä½†åœ¨è§†è§’å˜åŒ–è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå¤´æ¨æ‹‰ï¼ˆzoom-in&#x2F;zoom-outï¼‰åœºæ™¯ä¸­ï¼Œç”±äºç›¸æœºä¸è§’è‰²çš„è·ç¦»å˜åŒ–å¯¼è‡´çš„è§†è§’å˜åŒ–è¾ƒå¤§ï¼Œåº”ç”¨å—é™ï¼Œå¦‚ç”µå½±æ‹æ‘„è®¡åˆ’æˆ–ç›¸æœºæ§åˆ¶ç­‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å§¿æ€ç›¸å…³çš„å‚è€ƒé€‰æ‹©æ‰©æ•£ç½‘ç»œï¼Œæ”¯æŒäººä½“åŠ¨ç”»ä¸­çš„å¤§å¹…è§†è§’å˜åŒ–ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©ç½‘ç»œèƒ½å¤Ÿä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå› ä¸ºè§†è§’çš„é‡å¤§å˜åŒ–é€šå¸¸ä¼šå¯¼è‡´äººä½“å¤–è§‚ç»†èŠ‚ç¼ºå¤±ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„å§¿æ€å…³è”æ¨¡å—æ¥è®¡ç®—éå¯¹é½ç›®æ ‡å§¿æ€å’Œæºå§¿æ€ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«åŠ¨ç”»ç”Ÿæˆçš„å…³é”®åŒºåŸŸã€‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å½“å‰æœ€ä½³æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç›¸åŒæ•°é‡çš„å‚è€ƒå›¾åƒä¸‹ï¼Œåœ¨å¤§è§†è§’å˜åŒ–çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚è¿›ä¸€æ­¥å±•ç¤ºäº†è‡ªé€‚åº”å‚è€ƒé€‰æ‹©èƒ½å¤Ÿé€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒåŒºåŸŸï¼Œåœ¨è‡ªç”±è§†è§’ä¸‹ç”Ÿæˆäººä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºåŸºäºæºå›¾åƒå’Œé©±åŠ¨ä¿¡å·çš„äººä½“åŠ¨ç”»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹è§†è§’å˜åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå¤´æ¨æ‹‰åœºæ™¯ä¸­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å§¿æ€ç›¸å…³çš„å‚è€ƒé€‰æ‹©æ‰©æ•£ç½‘ç»œï¼Œä»¥å¤„ç†å¤§å¹…è§†è§’å˜åŒ–ã€‚</li>
<li>å¼•å…¥å§¿æ€å…³è”æ¨¡å—è®¡ç®—ç›®æ ‡å§¿æ€å’Œæºå§¿æ€çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾è¯†åˆ«åŠ¨ç”»ç”Ÿæˆçš„å…³é”®åŒºåŸŸã€‚</li>
<li>ä½¿ç”¨å…¬å…±TEDè®²åº§çš„å¤§å‹æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå­¦ä¹ ä¸åŒè§†è§’çš„åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-885aa3801c2fea4efb35dc565458304d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4aa96ab1286313582d8aa32d9c51cfb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af19359905018557983a2edd79155003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6248515f46fc609214fda8d912a6e7fc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DiMSUM-Diffusion-Mamba-â€“-A-Scalable-and-Unified-Spatial-Frequency-Method-for-Image-Generation"><a href="#DiMSUM-Diffusion-Mamba-â€“-A-Scalable-and-Unified-Spatial-Frequency-Method-for-Image-Generation" class="headerlink" title="DiMSUM: Diffusion Mamba â€“ A Scalable and Unified Spatial-Frequency   Method for Image Generation"></a>DiMSUM: Diffusion Mamba â€“ A Scalable and Unified Spatial-Frequency   Method for Image Generation</h2><p><strong>Authors:Hao Phung, Quan Dao, Trung Dao, Hoang Phan, Dimitris Metaxas, Anh Tran</strong></p>
<p>We introduce a novel state-space architecture for diffusion models, effectively harnessing spatial and frequency information to enhance the inductive bias towards local features in input images for image generation tasks. While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data. Our method demonstrates that integrating wavelet transformation into Mamba enhances the local structure awareness of visual inputs and better captures long-range relations of frequencies by disentangling them into wavelet subbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation. Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships. Through extensive experiments on standard benchmarks, our method demonstrates superior results compared to DiT and DIFFUSSM, achieving faster training convergence and delivering high-quality outputs. The codes and pretrained models are released at <a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/DiMSUM.git">https://github.com/VinAIResearch/DiMSUM.git</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºæ‰©æ•£æ¨¡å‹å¼•å…¥äº†ä¸€ç§æ–°å‹çŠ¶æ€ç©ºé—´æ¶æ„ï¼Œæœ‰æ•ˆåˆ©ç”¨ç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯ï¼Œå¢å¼ºå¯¹è¾“å…¥å›¾åƒå±€éƒ¨ç‰¹å¾çš„å½’çº³åè§ï¼Œä»¥ç”¨äºå›¾åƒç”Ÿæˆä»»åŠ¡ã€‚çŠ¶æ€ç©ºé—´ç½‘ç»œï¼ˆåŒ…æ‹¬é©å‘½æ€§çš„å¾ªç¯ç¥ç»ç½‘ç»œMambaï¼‰é€šå¸¸ä»å·¦åˆ°å³æ‰«æè¾“å…¥åºåˆ—ï¼Œä½†åœ¨è®¾è®¡æœ‰æ•ˆçš„æ‰«æç­–ç•¥æ—¶é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¯æ˜ï¼Œé€šè¿‡å°†å°æ³¢å˜æ¢æ•´åˆåˆ°Mambaä¸­ï¼Œå¯ä»¥å¢å¼ºå¯¹è§†è§‰è¾“å…¥çš„å±€éƒ¨ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å°†é¢‘ç‡åˆ†è§£ä¸ºå°æ³¢å­å¸¦æ›´å¥½åœ°æ•è·é•¿æœŸé¢‘ç‡å…³ç³»ï¼Œè¿™äº›å­å¸¦ä»£è¡¨ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ã€‚è¿™äº›åŸºäºå°æ³¢çš„è¾“å‡ºç»è¿‡å¤„ç†ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„èåˆå±‚æ— ç¼èåˆåˆ°åŸå§‹Mambaè¾“å‡ºä¸­ï¼Œç»“åˆç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯ä¼˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„é¡ºåºæ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿™å¯¹äºå›¾åƒç”Ÿæˆçš„ç»†èŠ‚å’Œæ•´ä½“è´¨é‡è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨å±€å…±äº«å˜å‹å™¨æ¥æå‡Mambaçš„æ€§èƒ½ï¼Œåˆ©ç”¨å…¶æ•æ‰å…¨å±€å…³ç³»çš„å‡ºè‰²èƒ½åŠ›ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºDiTå’ŒDIFFUSSMå±•ç°å‡ºä¼˜è¶Šçš„ç»“æœï¼Œå®ç°æ›´å¿«çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦å¹¶äº§ç”Ÿé«˜è´¨é‡è¾“å‡ºã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/DiMSUM.git%E4%B8%8A%E3%80%82">https://github.com/VinAIResearch/DiMSUM.gitä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04168v2">PDF</a> Accepted to NeurIPS 2024. Project page:   <a target="_blank" rel="noopener" href="https://vinairesearch.github.io/DiMSUM/">https://vinairesearch.github.io/DiMSUM/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çŠ¶æ€ç©ºé—´æ¶æ„çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨å°æ³¢å˜æ¢å¼ºåŒ–å±€éƒ¨ç‰¹å¾æ„ŸçŸ¥ï¼Œå¹¶ç»“åˆMambaç½‘ç»œï¼Œæœ‰æ•ˆç»“åˆç©ºé—´ä¸é¢‘ç‡ä¿¡æ¯ï¼Œæå‡å›¾åƒç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥å…¨å±€å…±äº«å˜å‹å™¨ï¼Œè¿›ä¸€æ­¥æå‡äº†Mambaçš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸DiTå’ŒDIFFUSSMç›¸æ¯”ï¼Œè®­ç»ƒæ”¶æ•›æ›´å¿«ï¼Œè¾“å‡ºè´¨é‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹çŠ¶æ€ç©ºé—´æ¶æ„æ‰©æ•£æ¨¡å‹ç»“åˆäº†ç©ºé—´ä¸é¢‘ç‡ä¿¡æ¯ï¼Œä»¥å¢å¼ºå›¾åƒç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¼•å…¥å°æ³¢å˜æ¢ï¼Œæå‡æ¨¡å‹å¯¹å±€éƒ¨ç»“æ„çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶æ›´å¥½åœ°æ•æ‰é¢‘ç‡çš„é•¿æœŸå…³ç³»ã€‚</li>
<li>ç»“åˆå°æ³¢è¾“å‡ºå’ŒMambaè¾“å‡ºï¼Œé€šè¿‡äº¤å‰æ³¨æ„èåˆå±‚ä¼˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„é¡ºåºæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å…¨å±€å…±äº«å˜å‹å™¨ä»¥è¿›ä¸€æ­¥æå‡Mambaæ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæ”¶æ•›æ›´å¿«ï¼Œè¾“å‡ºè´¨é‡æ›´é«˜ã€‚</li>
<li>æ¨¡å‹ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/DiMSUM.git%E3%80%82">https://github.com/VinAIResearch/DiMSUM.gitã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb9c737f78a1eeb52558e064eb77b431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-542ff209479a1befc4deb1246a19e0b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40666d434a45d6a39f2ad0e9dee38a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ab1fbc097095a3bb76141706cdcecb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CustomCrafter-Customized-Video-Generation-with-Preserving-Motion-and-Concept-Composition-Abilities"><a href="#CustomCrafter-Customized-Video-Generation-with-Preserving-Motion-and-Concept-Composition-Abilities" class="headerlink" title="CustomCrafter: Customized Video Generation with Preserving Motion and   Concept Composition Abilities"></a>CustomCrafter: Customized Video Generation with Preserving Motion and   Concept Composition Abilities</h2><p><strong>Authors:Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, Xi Li</strong></p>
<p>Customized video generation aims to generate high-quality videos guided by text prompts and subjectâ€™s reference images. However, since it is only trained on static images, the fine-tuning process of subject learning disrupts abilities of video diffusion models (VDMs) to combine concepts and generate motions. To restore these abilities, some methods use additional video similar to the prompt to fine-tune or guide the model. This requires frequent changes of guiding videos and even re-tuning of the model when generating different motions, which is very inconvenient for users. In this paper, we propose CustomCrafter, a novel framework that preserves the modelâ€™s motion generation and conceptual combination abilities without additional video and fine-tuning to recovery. For preserving conceptual combination ability, we design a plug-and-play module to update few parameters in VDMs, enhancing the modelâ€™s ability to capture the appearance details and the ability of concept combinations for new subjects. For motion generation, we observed that VDMs tend to restore the motion of video in the early stage of denoising, while focusing on the recovery of subject details in the later stage. Therefore, we propose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our subject learning modules, we reduce the impact of this module on motion generation in the early stage of denoising, preserving the ability to generate motion of VDMs. In the later stage of denoising, we restore this module to repair the appearance details of the specified subject, thereby ensuring the fidelity of the subjectâ€™s appearance. Experimental results show that our method has a significant improvement compared to previous methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/WuTao-CS/CustomCrafter">https://github.com/WuTao-CS/CustomCrafter</a> </p>
<blockquote>
<p>å®šåˆ¶åŒ–è§†é¢‘ç”Ÿæˆæ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºå’Œå‚è€ƒå›¾åƒç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚ç„¶è€Œï¼Œç”±äºå…¶ä»…å¯¹é™æ€å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä¸»ä½“å­¦ä¹ çš„å¾®è°ƒè¿‡ç¨‹ä¼šç ´åè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰ç»“åˆæ¦‚å¿µå’Œç”Ÿæˆè¿åŠ¨çš„èƒ½åŠ›ã€‚ä¸ºäº†æ¢å¤è¿™äº›èƒ½åŠ›ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨ä¸æç¤ºç›¸ä¼¼çš„é¢å¤–è§†é¢‘æ¥å¾®è°ƒæˆ–æŒ‡å¯¼æ¨¡å‹ã€‚è¿™éœ€è¦åœ¨ç”Ÿæˆä¸åŒè¿åŠ¨æ—¶é¢‘ç¹æ›´æ¢æŒ‡å¯¼è§†é¢‘ï¼Œç”šè‡³é‡æ–°è°ƒæ•´æ¨¡å‹ï¼Œéå¸¸ä¸æ–¹ä¾¿ç”¨æˆ·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CustomCrafterï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ— éœ€é¢å¤–çš„è§†é¢‘å’Œå¾®è°ƒå³å¯ä¿ç•™æ¨¡å‹çš„è¿åŠ¨ç”Ÿæˆå’Œæ¦‚å¿µç»“åˆèƒ½åŠ›ã€‚ä¸ºäº†ä¿ç•™æ¦‚å¿µç»“åˆèƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå³æ’å³ç”¨æ¨¡å—æ¥æ›´æ–°VDMä¸­çš„å°‘æ•°å‚æ•°ï¼Œå¢å¼ºæ¨¡å‹æ•æ‰å¤–è§‚ç»†èŠ‚å’Œç»“åˆæ–°æ¦‚å¿µçš„èƒ½åŠ›ã€‚å¯¹äºè¿åŠ¨ç”Ÿæˆï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°VDMå€¾å‘äºåœ¨é™å™ªçš„æ—©æœŸé˜¶æ®µæ¢å¤è§†é¢‘çš„è¿åŠ¨ï¼ŒåŒæ—¶ä¸“æ³¨äºåæœŸæ¢å¤ä¸»ä½“çš„ç»†èŠ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€åŠ æƒè§†é¢‘é‡‡æ ·ç­–ç•¥ã€‚åˆ©ç”¨æˆ‘ä»¬ä¸»ä½“å­¦ä¹ æ¨¡å—çš„å³æ’å³ç”¨ç‰¹æ€§ï¼Œæˆ‘ä»¬åœ¨é™å™ªçš„æ—©æœŸé˜¶æ®µå‡å°‘è¯¥æ¨¡å—å¯¹è¿åŠ¨ç”Ÿæˆçš„å½±å“ï¼Œä¿ç•™VDMsç”Ÿæˆè¿åŠ¨çš„èƒ½åŠ›ã€‚åœ¨é™å™ªçš„åæœŸé˜¶æ®µï¼Œæˆ‘ä»¬æ¢å¤æ­¤æ¨¡å—ä»¥ä¿®å¤æŒ‡å®šä¸»ä½“çš„å¤–è§‚ç»†èŠ‚ï¼Œä»è€Œç¡®ä¿ä¸»ä½“å¤–è§‚çš„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—æ”¹è¿›ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/WuTao-CS/CustomCrafter">https://github.com/WuTao-CS/CustomCrafter</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13239v2">PDF</a> Accepted by AAAI 2025. Project page: <a target="_blank" rel="noopener" href="https://customcrafter.github.io/">https://customcrafter.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ä¸ªåä¸ºCustomCrafterçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨å®šåˆ¶åŒ–è§†é¢‘ç”Ÿæˆæ—¶é¢ä¸´çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–è§†é¢‘å’ŒæŒ‡å¯¼è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œä¿æŒæ¨¡å‹çš„è¿åŠ¨ç”Ÿæˆå’Œæ¦‚å¿µç»„åˆèƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡å³æ’å³ç”¨æ¨¡å—æ›´æ–°VDMsä¸­çš„å°‘æ•°å‚æ•°ï¼Œæå‡æ¨¡å‹å¯¹æ–°ä¸»é¢˜çš„å¤–è§‚ç»†èŠ‚æ•æ‰å’Œæ¦‚å¿µç»„åˆèƒ½åŠ›ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åŠ¨æ€åŠ æƒè§†é¢‘é‡‡æ ·ç­–ç•¥ï¼Œåœ¨é™å™ªçš„æ—©æœŸé˜¶æ®µå‡å°‘æ¨¡å—å¯¹è¿åŠ¨ç”Ÿæˆçš„å½±å“ï¼Œç¡®ä¿VDMsçš„è¿åŠ¨ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨åæœŸä¿®å¤æŒ‡å®šä¸»é¢˜çš„å¤–è§‚ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾æ–°æ¡†æ¶ç›¸æ¯”ä»¥å¾€æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CustomCrafteræ¡†æ¶è§£å†³äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨å®šåˆ¶åŒ–è§†é¢‘ç”Ÿæˆæ—¶é¢ä¸´çš„æ¦‚å¿µç»„åˆå’Œè¿åŠ¨ç”Ÿæˆèƒ½åŠ›å—æŸçš„é—®é¢˜ã€‚</li>
<li>æ— éœ€é¢å¤–è§†é¢‘å’ŒæŒ‡å¯¼è°ƒæ•´ï¼Œå°±èƒ½ä¿æŒVDMsçš„è¿åŠ¨ç”Ÿæˆå’Œæ¦‚å¿µç»„åˆèƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†å³æ’å³ç”¨æ¨¡å—ï¼Œé€šè¿‡æ›´æ–°å°‘æ•°å‚æ•°æå‡æ¨¡å‹å¯¹æ–°ä¸»é¢˜çš„å¤–è§‚ç»†èŠ‚æ•æ‰å’Œæ¦‚å¿µç»„åˆèƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€åŠ æƒè§†é¢‘é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿VDMsåœ¨é™å™ªè¿‡ç¨‹ä¸­æ—¢èƒ½ç”Ÿæˆæµç•…è¿åŠ¨ï¼Œåˆèƒ½ä¿®å¤ä¸»é¢˜çš„å¤–è§‚ç»†èŠ‚ã€‚</li>
<li>åœ¨å®éªŒç¯èŠ‚ï¼ŒCustomCrafterç›¸æ¯”ä»¥å¾€æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>CustomCrafteræ¡†æ¶çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæé«˜è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡å…·æœ‰æ½œåœ¨çš„é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1621cd3761361ea314cebe7df8e5767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-059ca56cd42c5518b06998c456a46e00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c31b39a9c147f862a1db242a3c8c15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ad97a78e62968dcac90f05386c5b96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea4d193b566c166a40e899a2bd86dcf8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c73a55dcafebd294dc288a412561fb7e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ssProp-Energy-Efficient-Training-for-Convolutional-Neural-Networks-with-Scheduled-Sparse-Back-Propagation"><a href="#ssProp-Energy-Efficient-Training-for-Convolutional-Neural-Networks-with-Scheduled-Sparse-Back-Propagation" class="headerlink" title="ssProp: Energy-Efficient Training for Convolutional Neural Networks with   Scheduled Sparse Back Propagation"></a>ssProp: Energy-Efficient Training for Convolutional Neural Networks with   Scheduled Sparse Back Propagation</h2><p><strong>Authors:Lujia Zhong, Shuo Huang, Yonggang Shi</strong></p>
<p>Recently, deep learning has made remarkable strides, especially with generative modeling, such as large language models and probabilistic diffusion models. However, training these models often involves significant computational resources, requiring billions of petaFLOPs. This high resource consumption results in substantial energy usage and a large carbon footprint, raising critical environmental concerns. Back-propagation (BP) is a major source of computational expense during training deep learning models. To advance research on energy-efficient training and allow for sparse learning on any machine and device, we propose a general, energy-efficient convolution module that can be seamlessly integrated into any deep learning architecture. Specifically, we introduce channel-wise sparsity with additional gradient selection schedulers during backward based on the assumption that BP is often dense and inefficient, which can lead to over-fitting and high computational consumption. Our experiments demonstrate that our approach reduces 40% computations while potentially improving model performance, validated on image classification and generation tasks. This reduction can lead to significant energy savings and a lower carbon footprint during the research and development phases of large-scale AI systems. Additionally, our method mitigates over-fitting in a manner distinct from Dropout, allowing it to be combined with Dropout to further enhance model performance and reduce computational resource usage. Extensive experiments validate that our method generalizes to a variety of datasets and tasks and is compatible with a wide range of deep learning architectures and modules. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/lujiazho/ssProp">https://github.com/lujiazho/ssProp</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆæ¨¡å‹æ–¹é¢ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¦‚ç‡æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œéœ€è¦æ•°ä»¥äº¿è®¡çš„petaFLOPsã€‚è¿™ç§é«˜èµ„æºæ¶ˆè€—å¯¼è‡´å¤§é‡èƒ½æºæ¶ˆè€—å’Œè¾ƒå¤§çš„ç¢³è¶³è¿¹ï¼Œå¼•å‘äº†å…³é”®çš„ç¯å¢ƒé—®é¢˜ã€‚åå‘ä¼ æ’­ï¼ˆBPï¼‰æ˜¯è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶è®¡ç®—æˆæœ¬çš„ä¸»è¦æ¥æºã€‚ä¸ºäº†æ¨è¿›èŠ‚èƒ½è®­ç»ƒçš„ç ”ç©¶ï¼Œå¹¶å®ç°åœ¨ä»»ä½•æœºå™¨å’Œè®¾å¤‡ä¸Šè¿›è¡Œç¨€ç–å­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„èŠ‚èƒ½å·ç§¯æ¨¡å—ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ä»»ä½•æ·±åº¦å­¦ä¹ æ¶æ„ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šé“ç¨€ç–æ€§å’ŒåŸºäºåå‘ä¼ æ’­ä¸­å¸¸ç”¨çš„æ¢¯åº¦é€‰æ‹©è°ƒåº¦å™¨ï¼Œå‡è®¾åå‘ä¼ æ’­é€šå¸¸æ˜¯å¯†é›†ä¸”ä½æ•ˆçš„ï¼Œè¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œé«˜è®¡ç®—æ¶ˆè€—ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡å°‘äº†40%çš„è®¡ç®—é‡ï¼ŒåŒæ—¶å¯èƒ½æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™åœ¨å›¾åƒåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚è¿™ç§å‡å°‘å¯ä»¥èŠ‚çœå¤§é‡èƒ½æºå¹¶é™ä½å¤§å‹äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç ”å‘é˜¶æ®µçš„ç¢³è¶³è¿¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥ä¸Dropoutä¸åŒçš„æ–¹å¼å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå…è®¸å…¶ä¸Dropoutç»“åˆä½¿ç”¨ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½å’Œé™ä½è®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å„ç§æ•°æ®é›†å’Œä»»åŠ¡ï¼Œå¹¶ä¸”ä¸å„ç§æ·±åº¦å­¦ä¹ æ¶æ„å’Œæ¨¡å—å…¼å®¹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lujiazho/ssProp%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lujiazho/ssPropå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12561v2">PDF</a> Accepted by AAAI24 Workshop: Scalable and Efficient Artificial   Intelligence Systems</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é€šç”¨çš„ã€èŠ‚èƒ½çš„å·ç§¯æ¨¡å—ï¼Œå¯æ— ç¼é›†æˆåˆ°ä»»ä½•æ·±åº¦å­¦ä¹ æ¶æ„ä¸­ï¼Œä»¥æé«˜èƒ½æºæ•ˆç‡å¹¶å®ç°åœ¨ä»»ä½•æœºå™¨å’Œè®¾å¤‡ä¸Šçš„ç¨€ç–å­¦ä¹ ã€‚é€šè¿‡å¼•å…¥åŸºäºé€šé“ç¨€ç–æ€§å’Œé™„åŠ æ¢¯åº¦é€‰æ‹©è°ƒåº¦å™¨çš„åå‘ä¼ æ’­æ–¹æ³•ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿåœ¨å›¾åƒåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šå‡å°‘è®¡ç®—é‡å¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•å¯æ˜¾è‘—é™ä½å¤§å‹AIç³»ç»Ÿåœ¨ç ”å‘é˜¶æ®µçš„èƒ½è€—å’Œç¢³æ’æ”¾ï¼Œå¹¶ä»¥ä¸åŒäºDropoutçš„æ–¹å¼ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯ä¸Dropoutç»“åˆä½¿ç”¨ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½å’Œé™ä½è®¡ç®—èµ„æºä½¿ç”¨é‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå„ç§æ•°æ®é›†å’Œä»»åŠ¡ï¼Œå…¼å®¹å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„å’Œæ¨¡å—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ä¸­çš„ç”Ÿæˆå»ºæ¨¡å¦‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¦‚ç‡æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œèƒ½æºã€‚</li>
<li>åå‘ä¼ æ’­ï¼ˆBPï¼‰æ˜¯è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„ä¸»è¦è®¡ç®—æˆæœ¬æ¥æºã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šç”¨çš„ã€èŠ‚èƒ½çš„å·ç§¯æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æé«˜èƒ½æºæ•ˆç‡å¹¶å®ç°åœ¨ä»»ä½•æœºå™¨å’Œè®¾å¤‡ä¸Šçš„ç¨€ç–å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºé€šé“ç¨€ç–æ€§å’Œé™„åŠ æ¢¯åº¦é€‰æ‹©è°ƒåº¦å™¨çš„æŠ€æœ¯ï¼Œè¯¥æ¨¡å—åœ¨å›¾åƒåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†è®¡ç®—é‡çš„å‡å°‘å’Œæ¨¡å‹æ€§èƒ½çš„æ½œåœ¨æé«˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå‡å°‘å¤§å‹AIç³»ç»Ÿåœ¨ç ”å‘é˜¶æ®µçš„èƒ½è€—å’Œç¢³æ’æ”¾ã€‚</li>
<li>ä¸Dropoutä¸åŒï¼Œè¯¥æ–¹æ³•ä»¥ç‹¬ç‰¹çš„æ–¹å¼ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶ä¸Dropoutç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½å’Œé™ä½è®¡ç®—èµ„æºä½¿ç”¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67275699c8d25ca02d9f82801c8604b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a0ddc25023d437674e46d5763c3d2e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e97f595af60dfa876cfe4d67251c2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e2188d7e3e0860a9d88d6916c511595.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6cc73dfbc7c0025c497dfba3acd910f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e47a5f088ce151e6c112ab6a369f537.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Grid-Diffusion-Models-for-Text-to-Video-Generation"><a href="#Grid-Diffusion-Models-for-Text-to-Video-Generation" class="headerlink" title="Grid Diffusion Models for Text-to-Video Generation"></a>Grid Diffusion Models for Text-to-Video Generation</h2><p><strong>Authors:Taegyeong Lee, Soyeong Kwon, Taehwan Kim</strong></p>
<p>Recent advances in the diffusion models have significantly improved text-to-image generation. However, generating videos from text is a more challenging task than generating images from text, due to the much larger dataset and higher computational cost required. Most existing video generation methods use either a 3D U-Net architecture that considers the temporal dimension or autoregressive generation. These methods require large datasets and are limited in terms of computational costs compared to text-to-image generation. To tackle these challenges, we propose a simple but effective novel grid diffusion for text-to-video generation without temporal dimension in architecture and a large text-video paired dataset. We can generate a high-quality video using a fixed amount of GPU memory regardless of the number of frames by representing the video as a grid image. Additionally, since our method reduces the dimensions of the video to the dimensions of the image, various image-based methods can be applied to videos, such as text-guided video manipulation from image manipulation. Our proposed method outperforms the existing methods in both quantitative and qualitative evaluations, demonstrating the suitability of our model for real-world video generation. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œä»æ–‡æœ¬ç”Ÿæˆè§†é¢‘æ˜¯ä¸€é¡¹æ¯”ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ‰€éœ€çš„æ•°æ®é›†æ›´å¤§å’Œè®¡ç®—æˆæœ¬æ›´é«˜ã€‚ç›®å‰å¤§å¤šæ•°è§†é¢‘ç”Ÿæˆæ–¹æ³•ä½¿ç”¨è€ƒè™‘æ—¶é—´ç»´åº¦çš„3D U-Netæ¶æ„æˆ–è‡ªå›å½’ç”Ÿæˆã€‚è¿™äº›æ–¹æ³•éœ€è¦å¤§é‡æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æˆæœ¬æ–¹é¢ä¸æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç›¸æ¯”å­˜åœ¨å±€é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–°å‹ç½‘æ ¼æ‰©æ•£æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼Œæ— éœ€è€ƒè™‘æ¶æ„ä¸­çš„æ—¶é—´ç»´åº¦å’Œå¤§å‹æ–‡æœ¬-è§†é¢‘é…å¯¹æ•°æ®é›†ã€‚é€šè¿‡å°†è§†é¢‘è¡¨ç¤ºä¸ºç½‘æ ¼å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å›ºå®šé‡çš„GPUå†…å­˜ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œæ— è®ºå¸§æ•°å¦‚ä½•ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ–¹æ³•å°†è§†é¢‘çš„ç»´åº¦å‡å°‘åˆ°å›¾åƒçš„ç»´åº¦ï¼Œå› æ­¤å„ç§åŸºäºå›¾åƒçš„æ–¹æ³•å¯ä»¥åº”ç”¨äºè§†é¢‘ï¼Œä¾‹å¦‚åŸºäºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œè¿›è¡Œè§†é¢‘æ“ä½œã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†æˆ‘ä»¬æ¨¡å‹åœ¨ç°å®ä¸–ç•Œè§†é¢‘ç”Ÿæˆä¸­çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.00234v2">PDF</a> This paper is being withdrawn due to issues of misconduct in the   experiments presented in Table 1 and 5. We recognize this as an ethical   concern and sincerely apologize to the research community for any   inconvenience it may have caused</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°æ‰©æ•£æ¨¡å‹è¿›å±•æ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ–‡æœ¬ç”Ÿæˆè§†é¢‘çš„ä»»åŠ¡æ¯”æ–‡æœ¬ç”Ÿæˆå›¾åƒæ›´å…·æŒ‘æˆ˜æ€§ï¼Œæ¶‰åŠæ›´å¤§çš„æ•°æ®é›†å’Œæ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•å¤šé‡‡ç”¨è€ƒè™‘æ—¶é—´ç»´åº¦çš„3D U-Netæ¶æ„æˆ–è‡ªå›å½’ç”Ÿæˆã€‚è¿™äº›æ–¹æ³•éœ€è¦å¤§é‡æ•°æ®é›†ï¼Œä¸æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç½‘æ ¼æ‰©æ•£æ–°æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼Œæ— éœ€è€ƒè™‘æ—¶é—´ç»´åº¦å’Œå¤§å‹æ–‡æœ¬è§†é¢‘é…å¯¹æ•°æ®é›†ã€‚é€šè¿‡æŠŠè§†é¢‘è¡¨ç¤ºä¸ºç½‘æ ¼å›¾åƒï¼Œæˆ‘ä»¬èƒ½ä»¥å›ºå®šGPUå†…å­˜ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œæ— è®ºå¸§æ•°å¤šå°‘ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ–¹æ³•å°†è§†é¢‘ç»´åº¦é™è‡³å›¾åƒç»´åº¦ï¼Œå„ç§åŸºäºå›¾åƒçš„æ–¹æ³•å¯åº”ç”¨äºè§†é¢‘ï¼Œå¦‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘æ“ä½œæ¥è‡ªå›¾åƒæ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†æˆ‘ä»¬æ¨¡å‹åœ¨çœŸå®è§†é¢‘ç”Ÿæˆä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²ç»æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ–‡æœ¬ç”Ÿæˆè§†é¢‘ä»»åŠ¡ç”±äºéœ€è¦æ›´å¤§çš„æ•°æ®é›†å’Œæ›´é«˜çš„è®¡ç®—æˆæœ¬è€Œæ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸»è¦é‡‡ç”¨3D U-Netæ¶æ„æˆ–è‡ªå›å½’ç”Ÿæˆï¼Œå­˜åœ¨è®¡ç®—æˆæœ¬é«˜çš„å±€é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç½‘æ ¼æ‰©æ•£æ–¹æ³•ï¼Œæ— éœ€è€ƒè™‘æ—¶é—´ç»´åº¦å’Œå¤§å‹æ–‡æœ¬è§†é¢‘é…å¯¹æ•°æ®é›†ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆã€‚</li>
<li>é€šè¿‡å°†è§†é¢‘è¡¨ç¤ºä¸ºç½‘æ ¼å›¾åƒï¼Œèƒ½åœ¨å›ºå®šGPUå†…å­˜ä¸‹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œæ— è®ºå¸§æ•°å¤šå°‘ã€‚</li>
<li>æ–¹æ³•é™ä½äº†è§†é¢‘ç»´åº¦è‡³å›¾åƒç»´åº¦ï¼Œä½¿å¾—å›¾åƒå¤„ç†æ–¹æ³•å¯åº”ç”¨äºè§†é¢‘æ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.00234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-72af0a8c8363aa88c45ff7385523414d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-836ac1823e7c054752c453b6fa6058b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa762bf1f5b89cd04b21c9e77c5e3d4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8f44fbeea68ba9f9d259538b059266a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-004bfd1021ff97c19dd3af30b051e242.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4c8b84956abf848ebd779adfca2d313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b6d7ad3aaec971277e421ef4ee9f8f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d93f74034282349cb9c977e84f2d9c33.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image   Modeling for CBCT Tooth Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e0d61a3ec03e4f212d16afb4fead0a57.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Bringing Objects to Life 4D generation from 3D objects
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18863.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
