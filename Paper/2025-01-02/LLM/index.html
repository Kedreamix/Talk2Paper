<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Distributed Mixture-of-Agents for Edge Inference with Large Language   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c7827bb6f2f165dd9d946225924095f9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Distributed-Mixture-of-Agents-for-Edge-Inference-with-Large-Language-Models"><a href="#Distributed-Mixture-of-Agents-for-Edge-Inference-with-Large-Language-Models" class="headerlink" title="Distributed Mixture-of-Agents for Edge Inference with Large Language   Models"></a>Distributed Mixture-of-Agents for Edge Inference with Large Language   Models</h2><p><strong>Authors:Purbesh Mitra, Priyanka Kaswan, Sennur Ulukus</strong></p>
<p>Mixture-of-Agents (MoA) has recently been proposed as a method to enhance performance of large language models (LLMs), enabling multiple individual LLMs to work together for collaborative inference. This collaborative approach results in improved responses to user prompts compared to relying on a single LLM. In this paper, we consider such an MoA architecture in a distributed setting, where LLMs operate on individual edge devices, each uniquely associated with a user and equipped with its own distributed computing power. These devices exchange information using decentralized gossip algorithms, allowing different device nodes to talk without the supervision of a centralized server. In the considered setup, different users have their own LLM models to address user prompts. Additionally, the devices gossip either their own user-specific prompts or augmented prompts to generate more refined answers to certain queries. User prompts are temporarily stored in the device queues when their corresponding LLMs are busy. Given the memory limitations of edge devices, it is crucial to ensure that the average queue sizes in the system remain bounded. In this paper, we address this by theoretically calculating the queuing stability conditions for the device queues under reasonable assumptions, which we validate experimentally as well. Further, we demonstrate through experiments, leveraging open-source LLMs for the implementation of distributed MoA, that certain MoA configurations produce higher-quality responses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/distributed_moa">https://github.com/purbeshmitra/distributed_moa</a>. </p>
<blockquote>
<p>æ··åˆä»£ç†ï¼ˆMoAï¼‰æ–¹æ³•æœ€è¿‘è¢«æå‡ºä½œä¸ºä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿä½¿å¤šä¸ªç‹¬ç«‹çš„LLMååŒå·¥ä½œï¼Œè¿›è¡ŒååŒæ¨ç†ã€‚è¿™ç§åä½œæ–¹æ³•ç›¸è¾ƒäºä¾èµ–å•ä¸ªLLMï¼Œèƒ½æ”¹è¿›å¯¹ç”¨æˆ·æç¤ºçš„å“åº”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è€ƒè™‘è¿™æ ·ä¸€ç§MoAæ¶æ„ï¼Œå…¶ä¸­LLMåœ¨ä¸ªåˆ«è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œï¼Œæ¯ä¸ªè®¾å¤‡éƒ½ä¸ç”¨æˆ·ç›¸å…³è”ï¼Œå¹¶é…å¤‡å…¶è‡ªå·±çš„åˆ†å¸ƒå¼è®¡ç®—èƒ½åŠ›ã€‚è¿™äº›è®¾å¤‡ä½¿ç”¨å»ä¸­å¿ƒåŒ–çš„é—²èŠç®—æ³•äº¤æ¢ä¿¡æ¯ï¼Œä½¿å¾—ä¸åŒçš„è®¾å¤‡èŠ‚ç‚¹èƒ½å¤Ÿåœ¨æ— éœ€ä¸­å¤®æœåŠ¡å™¨ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œäº¤æµã€‚åœ¨è€ƒè™‘çš„è®¾å®šä¸­ï¼Œä¸åŒç”¨æˆ·æ‹¥æœ‰ä»–ä»¬è‡ªå·±çš„LLMæ¨¡å‹æ¥åº”å¯¹ç”¨æˆ·æç¤ºã€‚æ­¤å¤–ï¼Œè®¾å¤‡ä¼šé—²èŠå®ƒä»¬è‡ªå·±çš„ç”¨æˆ·ç‰¹å®šæç¤ºæˆ–å¢å¼ºæç¤ºï¼Œä»¥ç”Ÿæˆå¯¹ç‰¹å®šæŸ¥è¯¢çš„æ›´ç²¾ç»†ç­”æ¡ˆã€‚å½“ç”¨æˆ·å¯¹åº”çš„LLMå¿™ç¢Œæ—¶ï¼Œç”¨æˆ·æç¤ºä¼šæš‚æ—¶å­˜å‚¨åœ¨è®¾å¤‡é˜Ÿåˆ—ä¸­ã€‚é‰´äºè¾¹ç¼˜è®¾å¤‡çš„å†…å­˜é™åˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿä¸­çš„å¹³å‡é˜Ÿåˆ—å¤§å°ä¿æŒæœ‰ç•Œè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç†è®ºè®¡ç®—è®¾å¤‡é˜Ÿåˆ—çš„æ’é˜Ÿç¨³å®šæ€§æ¡ä»¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™äº›å‡è®¾æ˜¯åˆç†çš„ï¼Œæˆ‘ä»¬ä¹Ÿé€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›å‡è®¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å€ŸåŠ©å¼€æºLLMå®ç°åˆ†å¸ƒå¼MoAï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜ï¼ŒæŸäº›MoAé…ç½®äº§ç”Ÿçš„å“åº”è´¨é‡è¾ƒé«˜ï¼Œè¿™æ˜¯åœ¨AlpacaEval 2.0åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°å¾—å‡ºçš„ã€‚ç›¸å…³å®ç°å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/distributed_moa%E3%80%82">https://github.com/purbeshmitra/distributed_moaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21200v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºMixture-of-Agentsï¼ˆMoAï¼‰æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ååŒæ¨ç†æ¶æ„ã€‚è¯¥æ¶æ„å…è®¸æ¯ä¸ªè¾¹ç¼˜è®¾å¤‡ä¸Šçš„LLMæ¨¡å‹é€šè¿‡å»ä¸­å¿ƒåŒ–çš„é—²èŠç®—æ³•è¿›è¡Œä¿¡æ¯äº¤æµï¼Œè€Œæ— éœ€ä¸­å¤®æœåŠ¡å™¨çš„ç›‘ç£ã€‚åœ¨å†…å­˜æœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†ç†è®ºä¸Šè®¡ç®—è®¾å¤‡é˜Ÿåˆ—çš„æ’é˜Ÿç¨³å®šæ€§æ¡ä»¶ï¼Œå¹¶éªŒè¯äº†å…¶å¯¹æé«˜ç³»ç»Ÿå“åº”è´¨é‡çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ©ç”¨å¼€æºLLMå®ç°åˆ†å¸ƒå¼MoAçš„å®éªŒè¡¨æ˜ï¼ŒæŸäº›MoAé…ç½®èƒ½å¤Ÿäº§ç”Ÿæ›´é«˜è´¨é‡çš„å“åº”ã€‚ç›¸å…³å®ç°å¯å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/distributed_moa%E3%80%82">https://github.com/purbeshmitra/distributed_moaã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoAæ–¹æ³•ç”¨äºå¢å¼ºLLMæ€§èƒ½ï¼Œå®ç°å¤šLLMååŒæ¨ç†ã€‚</li>
<li>åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼ŒLLMåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ“ä½œï¼Œé‡‡ç”¨å»ä¸­å¿ƒåŒ–çš„é—²èŠç®—æ³•è¿›è¡Œä¿¡æ¯äº¤æµã€‚</li>
<li>ç”¨æˆ·æœ‰è‡ªå·±çš„LLMæ¨¡å‹ä»¥å“åº”ç”¨æˆ·æç¤ºï¼Œå¹¶é€šè¿‡é—²èŠç®—æ³•åˆ†äº«ä¿¡æ¯ã€‚</li>
<li>è®¾å¤‡é˜Ÿåˆ—ç”¨äºæš‚å­˜ç”¨æˆ·æç¤ºï¼Œåœ¨LLMå¿™ç¢Œæ—¶ä¿è¯å“åº”çš„è¿ç»­æ€§ã€‚</li>
<li>åœ¨å†…å­˜æœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°è®¾å¤‡é˜Ÿåˆ—çš„æ’é˜Ÿç¨³å®šæ€§æ¡ä»¶çš„ç†è®ºè®¡ç®—ã€‚</li>
<li>å®éªŒéªŒè¯æ’é˜Ÿç¨³å®šæ€§æ¡ä»¶çš„æœ‰æ•ˆæ€§å¯¹æé«˜ç³»ç»Ÿå“åº”è´¨é‡çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92053527df11ccaa7e9bdcc19f358342.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-732a93b239ac84b7c361335645ed46e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f45f698f5949fd64088a763e35d91036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd799638a4a194b36da3fffb687aaf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb71ec41e1afe2fd81f0dae2d6ac5f4b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Do-NOT-Think-That-Much-for-2-3-On-the-Overthinking-of-o1-Like-LLMs"><a href="#Do-NOT-Think-That-Much-for-2-3-On-the-Overthinking-of-o1-Like-LLMs" class="headerlink" title="Do NOT Think That Much for 2+3&#x3D;? On the Overthinking of o1-Like LLMs"></a>Do NOT Think That Much for 2+3&#x3D;? On the Overthinking of o1-Like LLMs</h2><p><strong>Authors:Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</strong></p>
<p>The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME. </p>
<blockquote>
<p>æ¨¡å‹çš„å‡ºè‰²è¡¨ç°ï¼Œå¦‚OpenAI o1ï¼Œå¯å½’åŠŸäºå…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¨¡æ‹Ÿäººç±»é•¿æ—¶é—´æ€è€ƒçš„èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨æ‰©å±•çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è¿‡ç¨‹ï¼Œæ¢ç´¢å¤šç§ç­–ç•¥ä»¥å¢å¼ºè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šå¦‚ä½•åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­æ™ºèƒ½é«˜æ•ˆåœ°æ‰©å±•è®¡ç®—èµ„æºã€‚æœ¬æ–‡å¯¹æ¨¡å‹ä¸­æ™®éå­˜åœ¨çš„è¿‡åº¦æ€è€ƒé—®é¢˜è¿›è¡Œäº†é¦–æ¬¡å…¨é¢ç ”ç©¶ï¼Œå³è¿‡å¤šè®¡ç®—èµ„æºè¢«åˆ†é…ç»™ç®€å•é—®é¢˜ï¼Œè€Œè¿™äº›é—®é¢˜å¸¦æ¥çš„ç›Šå¤„å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬ä»ç»“æœå’Œè¿‡ç¨‹ä¸¤ä¸ªè§’åº¦å¼•å…¥äº†æ–°çš„æ•ˆç‡æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°o1ç±»æ¨¡å‹å¯¹è®¡ç®—èµ„æºçš„åˆç†ä½¿ç”¨ã€‚é€šè¿‡è‡ªæˆ‘è®­ç»ƒèŒƒå¼ï¼Œæˆ‘ä»¬æå‡ºäº†ç¼“è§£è¿‡åº¦æ€è€ƒçš„ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹è€Œä¸æŸå®³å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸå‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹åœ¨ä¸åŒéš¾åº¦æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œå¦‚GSM8Kã€MATH500ã€GPQAå’ŒAIMEã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21187v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç±»ä¼¼OpenAI o1çš„æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•æ¨¡æ‹Ÿäººç±»çš„é•¿æ—¶æ€è€ƒï¼Œä»è€Œæå‡é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è®ºæ–‡çš„é‡ç‚¹åœ¨äºç ”ç©¶è¿™äº›æ¨¡å‹ä¸­çš„è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œå³å¯¹äºç®€å•é—®é¢˜åˆ†é…è¿‡å¤šè®¡ç®—èµ„æºçš„é—®é¢˜ã€‚è®ºæ–‡ä»ç»“æœå’Œè¿‡ç¨‹ä¸¤ä¸ªè§’åº¦æå‡ºäº†è¯„ä¼°è®¡ç®—èµ„æºåˆç†åˆ©ç”¨çš„æ–°æ•ˆç‡æŒ‡æ ‡ï¼Œå¹¶æå‡ºäº†é€šè¿‡è‡ªæˆ‘è®­ç»ƒç­–ç•¥æ¥å‡å°‘è¿‡åº¦æ€è€ƒçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæ¨¡å‹åœ¨ä¸åŒéš¾åº¦æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç±»ä¼¼OpenAI o1çš„æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„é•¿æ—¶æ€è€ƒï¼Œæå‡é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œå³å¯¹ç®€å•é—®é¢˜åˆ†é…è¿‡å¤šè®¡ç®—èµ„æºã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä»ç»“æœå’Œè¿‡ç¨‹ä¸¤ä¸ªè§’åº¦è¯„ä¼°è®¡ç®—èµ„æºåˆ©ç”¨çš„æ–°æ•ˆç‡æŒ‡æ ‡ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹çš„è¿‡åº¦æ€è€ƒï¼Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¿™ç§æ–¹æ³•åœ¨ä¸åŒéš¾åº¦çš„æµ‹è¯•é›†ï¼ˆå¦‚GSM8K, MATH500, GPQAå’ŒAIMEï¼‰ä¸Šå‡æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4335e8c0eb903f840211cdc7bb85f6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9194f6a530349fe47e96f0e5d7ce8ab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c941e5aafe7f9f9c5c2c60e47604ce64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-062b12a69b5fe0afdface79769ed3274.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Facilitating-large-language-model-Russian-adaptation-with-Learned-Embedding-Propagation"><a href="#Facilitating-large-language-model-Russian-adaptation-with-Learned-Embedding-Propagation" class="headerlink" title="Facilitating large language model Russian adaptation with Learned   Embedding Propagation"></a>Facilitating large language model Russian adaptation with Learned   Embedding Propagation</h2><p><strong>Authors:Mikhail Tikhomirov, Daniil Chernyshev</strong></p>
<p>Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a language specific LLMs as improved inference computation efficiency becomes the only guaranteed advantage of such costly procedure. More cost-efficient options such as vocabulary extension and subsequent continued pre-training are also inhibited by the lack of access to high-quality instruction-tuning data since it is the major factor behind the resulting LLM task-solving capabilities. To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP). Unlike existing approaches our method has lower training data size requirements due to minimal impact on existing LLM knowledge which we reinforce using novel ad-hoc embedding propagation procedure that allows to skip the instruction-tuning step and instead implant the new language knowledge directly into any existing instruct-tuned variant. We evaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B, showing that LEP is competitive with traditional instruction-tuning methods, achieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with further improvements via self-calibration and continued tuning enhancing task-solving capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ¨åŠ¨äº†å¼ºå¤§çš„å¼€æºæŒ‡ä»¤è°ƒä¼˜LLMçš„å¼•å…¥ï¼Œå…¶æ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸æœ€æ–°åŒè¡Œå¦‚GPT-4ç›¸å½“ã€‚è™½ç„¶æ­¤ç±»æ¨¡å‹çš„æ¶Œç°åŠ é€Ÿäº†LLMæŠ€æœ¯åœ¨æ•æ„Ÿä¿¡æ¯ç¯å¢ƒä¸­çš„é‡‡ç”¨ï¼Œä½†è¿™äº›æ¨¡å‹çš„ä½œè€…å¹¶æœªå…¬å¼€å¤åˆ¶ç»“æœæ‰€éœ€çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œä½¿å¾—è¿™äº›æˆå°±æˆä¸ºæ¨¡å‹ä¸“æœ‰ã€‚ç”±äºè¿™äº›å¼€æºæ¨¡å‹ä¹Ÿæ˜¯å¤šè¯­è¨€çš„ï¼Œè¿™åè¿‡æ¥åˆå‡å°‘äº†è®­ç»ƒç‰¹å®šè¯­è¨€LLMçš„æ•ˆç›Šï¼Œå› ä¸ºæé«˜æ¨ç†è®¡ç®—æ•ˆç‡æˆä¸ºè¿™ç§æ˜‚è´µç¨‹åºå”¯ä¸€ä¿è¯çš„ä¼˜åŠ¿ã€‚è¯æ±‡æ‰©å±•å’Œéšåçš„æŒç»­é¢„è®­ç»ƒç­‰æ›´å…·æˆæœ¬æ•ˆç›Šçš„é€‰é¡¹ä¹Ÿå—åˆ°é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®æ— æ³•è·å–çš„é™åˆ¶çš„é˜»ç¢ï¼Œå› ä¸ºè¿™æ˜¯å®ç°LLMè§£å†³é—®é¢˜èƒ½åŠ›çš„å…³é”®å› ç´ ã€‚ä¸ºäº†è§£å†³è¯­è¨€é€‚åº”ç®¡é“çš„é™åˆ¶å¹¶é™ä½æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦åˆ°çš„åµŒå…¥ä¼ æ’­ï¼ˆLEPï¼‰ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”±äºå¯¹ç°æœ‰LLMçŸ¥è¯†çš„å½±å“æœ€å°ï¼Œå…·æœ‰è¾ƒä½çš„è®­ç»ƒæ•°æ®å¤§å°è¦æ±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°å‹å³æ—¶åµŒå…¥ä¼ æ’­ç¨‹åºæ¥åŠ å¼ºè¿™ä¸€ç‚¹ï¼Œå¯ä»¥è·³è¿‡æŒ‡ä»¤è°ƒæ•´æ­¥éª¤ï¼Œè€Œæ˜¯ç›´æ¥å°†æ–°è¯­è¨€çŸ¥è¯†æ¤å…¥ä»»ä½•ç°æœ‰çš„æŒ‡ä»¤è°ƒæ•´å˜ä½“ã€‚æˆ‘ä»¬å¯¹LLaMa-3-8Bå’ŒMistral-7Bçš„å››ç§ä¿„è¯­è¯æ±‡é€‚åº”è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºLEPä¸ä¼ ç»ŸæŒ‡ä»¤è°ƒæ•´æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œæ€§èƒ½å¯ä¸OpenChat 3.5å’ŒLLaMa-3-8B-Instructç›¸åª²ç¾ï¼Œé€šè¿‡è‡ªæˆ‘æ ¡å‡†å’ŒæŒç»­è°ƒæ•´å¯è¿›ä¸€æ­¥æé«˜è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21140v1">PDF</a> Preprint version of an article published in the Journal of Language   and Education. Copyright held by the owner&#x2F;author(s). Publication rights   licensed to the Journal of Language and Education</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå‡ºç°äº†å…·æœ‰ä¸æœ€å‰æ²¿æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ç›¸åŒæ–‡æœ¬ç”Ÿæˆè´¨é‡çš„å¼ºå¤§å¼€æºæŒ‡ä»¤ä¼˜åŒ–LLMã€‚è™½ç„¶è¿™äº›æ¨¡å‹çš„å‡ºç°åœ¨åŠ é€ŸLLMæŠ€æœ¯åœ¨æ•æ„Ÿä¿¡æ¯ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œä½†æ¨¡å‹çš„ä½œè€…æ²¡æœ‰å…¬å¼€å¤åˆ¶ç»“æœæ‰€éœ€çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä½¿å¾—æ¨¡å‹çš„æˆæœå˜å¾—ç‹¬å®¶ä¸“æœ‰ã€‚ç”±äºè¿™äº›å¼€æºæ¨¡å‹ä¹Ÿæ˜¯å¤šè¯­è¨€çš„ï¼Œè¿™åè¿‡æ¥åˆå‡å°‘äº†è®­ç»ƒç‰¹å®šè¯­è¨€LLMçš„å¥½å¤„ï¼Œå› ä¸ºæé«˜æ¨ç†è®¡ç®—æ•ˆç‡æˆä¸ºè¿™ç§æ˜‚è´µç¨‹åºå”¯ä¸€ä¿è¯çš„ä¼˜åŠ¿ã€‚æ›´ç»æµé«˜æ•ˆçš„é€‰æ‹©ï¼Œå¦‚è¯æ±‡æ‰©å±•å’Œéšåçš„æŒç»­é¢„è®­ç»ƒï¼Œä¹Ÿå—åˆ°é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®è®¿é—®çš„é™åˆ¶çš„é˜»ç¢ï¼Œè¿™æ˜¯å®ç°LLMä»»åŠ¡è§£å†³èƒ½åŠ›çš„å…³é”®å› ç´ ã€‚ä¸ºäº†è§£å†³è¯­è¨€é€‚åº”æµç¨‹ä¸­çš„å±€é™æ€§å¹¶é™ä½æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ åµŒå…¥ä¼ æ’­ï¼ˆLEPï¼‰ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰è¾ƒä½çš„è®­ç»ƒæ•°æ®å¤§å°è¦æ±‚ï¼Œå¯¹ç°æœ‰LLMçŸ¥è¯†çš„å½±å“è¾ƒå°ã€‚æˆ‘ä»¬ä½¿ç”¨æ–°å‹ä¸“ç”¨åµŒå…¥ä¼ æ’­ç¨‹åºæ¥åŠ å¼ºè¿™ä¸€ç‚¹ï¼Œå¯ä»¥è·³è¿‡æŒ‡ä»¤è°ƒæ•´æ­¥éª¤ï¼Œè€Œæ˜¯ç›´æ¥å°†æ–°è¯­è¨€çŸ¥è¯†æ¤å…¥ä»»ä½•ç°æœ‰çš„æŒ‡ä»¤ä¼˜åŒ–å˜ä½“ã€‚æˆ‘ä»¬å¯¹LLaMa-3-8Bå’ŒMistral-7Bçš„å››ä¸ªä¿„è¯­è¯æ±‡é€‚åº”è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºLEPä¸ä¼ ç»ŸæŒ‡ä»¤è°ƒæ•´æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œæ€§èƒ½å¯ä¸OpenChat 3.5å’ŒLLaMa-3-8B-Instructç›¸åª²ç¾ï¼Œé€šè¿‡è‡ªæˆ‘æ ¡å‡†å’ŒæŒç»­è°ƒæ•´å¯è¿›ä¸€æ­¥ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œå‡ºç°äº†å¼€æºæŒ‡ä»¤ä¼˜åŒ–LLMã€‚è¿™äº›æ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œè¿™äº›æ¨¡å‹çš„çŸ¥è¯†äº§æƒå¹¶ä¸å…¬å¼€é€æ˜ï¼Œä½¿å¾—å…¶ä»–äººéš¾ä»¥å¤åˆ¶æˆ–æ”¹è¿›å…¶æˆæœã€‚è¿™ä½¿å¾—ä¸€äº›è¯­è¨€ç‰¹å®šæ¨¡å‹çš„è®­ç»ƒæˆæœ¬æ•ˆç›Šé™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f5e93935dc4571208f85c37a779c29a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cd7ce1ed73cd1195999ef0b41ddd68f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-and-Controlling-Diversity-in-LLM-Agent-Conversation"><a href="#Exploring-and-Controlling-Diversity-in-LLM-Agent-Conversation" class="headerlink" title="Exploring and Controlling Diversity in LLM-Agent Conversation"></a>Exploring and Controlling Diversity in LLM-Agent Conversation</h2><p><strong>Authors:KuanChao Chu, Yi-Pei Chen, Hideki Nakayama</strong></p>
<p>Diversity is a critical aspect of multi-agent communication. In this paper, we focus on controlling and exploring diversity in the context of open-domain multi-agent conversations, particularly for world simulation applications. We propose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts the content of the utterance generation prompt to control diversity using a single parameter, lambda. Through extensive experiments, we show that APP effectively controls the output diversity across models and datasets, with pruning more information leading to more diverse output. We comprehensively analyze the relationship between prompt content and conversational diversity. Our findings reveal that information from all components of the prompt generally constrains the diversity of the output, with the Memory block exerting the most significant influence. APP is compatible with established techniques like temperature sampling and top-p sampling, providing a versatile tool for diversity management. To address the trade-offs of increased diversity, such as inconsistencies with omitted information, we incorporate a post-generation correction step, which effectively balances diversity enhancement with output consistency. Additionally, we examine how prompt structure, including component order and length, impacts diversity. This study addresses key questions surrounding diversity in multi-agent world simulation, offering insights into its control, influencing factors, and associated trade-offs. Our contributions lay the foundation for systematically engineering diversity in LLM-based multi-agent collaborations, advancing their effectiveness in real-world applications. </p>
<blockquote>
<p>å¤šæ ·æ€§æ˜¯å¤šæ™ºèƒ½ä½“é€šä¿¡çš„å…³é”®æ–¹é¢ã€‚æœ¬æ–‡ä¸“æ³¨äºåœ¨å¼€æ”¾åŸŸå¤šæ™ºèƒ½ä½“å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¸­æ§åˆ¶å’Œæ¢ç´¢å¤šæ ·æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸–ç•Œæ¨¡æ‹Ÿåº”ç”¨æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æç¤ºä¿®å‰ªï¼ˆAPPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡å•ä¸ªå‚æ•°Î»åŠ¨æ€è°ƒæ•´è¯è¯­ç”Ÿæˆæç¤ºçš„å†…å®¹æ¥æ§åˆ¶å¤šæ ·æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†APPå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶æ¨¡å‹å’Œæ•°æ®é›†ä¹‹é—´çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œä¿®å‰ªæ›´å¤šä¿¡æ¯ä¼šå¯¼è‡´è¾“å‡ºæ›´åŠ å¤šæ ·åŒ–ã€‚æˆ‘ä»¬å…¨é¢åˆ†æäº†æç¤ºå†…å®¹ä¸å¯¹è¯å¤šæ ·æ€§ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæç¤ºçš„æ‰€æœ‰ç»„æˆéƒ¨åˆ†çš„ä¿¡æ¯é€šå¸¸éƒ½é™åˆ¶äº†è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œå…¶ä¸­è®°å¿†å—çš„å½±å“æœ€ä¸ºæ˜¾è‘—ã€‚APPå¯ä»¥ä¸æ¸©åº¦é‡‡æ ·å’Œtop-pé‡‡æ ·ç­‰ç°æœ‰æŠ€æœ¯ç›¸ç»“åˆï¼Œä¸ºå¤šæ ·æ€§ç®¡ç†æä¾›äº†é€šç”¨çš„å·¥å…·ã€‚ä¸ºäº†è§£å†³å¢åŠ å¤šæ ·æ€§æ‰€å¸¦æ¥çš„æƒè¡¡é—®é¢˜ï¼Œå¦‚çœç•¥ä¿¡æ¯çš„çŸ›ç›¾ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç”Ÿæˆåçš„æ ¡æ­£æ­¥éª¤ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†å¤šæ ·æ€§å¢å¼ºä¸è¾“å‡ºä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†æç¤ºç»“æ„ï¼ŒåŒ…æ‹¬ç»„ä»¶çš„é¡ºåºå’Œé•¿åº¦ï¼Œå¯¹å¤šæ ·æ€§çš„å½±å“ã€‚æœ¬ç ”ç©¶è§£å†³äº†å›´ç»•å¤šæ™ºèƒ½ä½“ä¸–ç•Œæ¨¡æ‹Ÿä¸­å¤šæ ·æ€§çš„å…³é”®é—®é¢˜ï¼Œä¸ºæ§åˆ¶ã€å½±å“å› ç´ å’Œç›¸å…³çš„æƒè¡¡æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸ºåœ¨åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“åä½œä¸­ç³»ç»Ÿåœ°æ„å»ºå¤šæ ·æ€§å¥ å®šäº†åŸºç¡€ï¼Œæé«˜äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21102v1">PDF</a> Accepted for the AAAI 2025 Workshop on Advancing LLM-Based   Multi-Agent Collaboration</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡å…³æ³¨å¼€æ”¾åŸŸå¤šæ™ºèƒ½ä½“å¯¹è¯ä¸­çš„å¤šæ ·æ€§æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸–ç•Œæ¨¡æ‹Ÿåº”ç”¨ä¸­çš„å¤šæ ·æ€§æ¢ç´¢ã€‚æå‡ºä¸€ç§åä¸ºè‡ªé€‚åº”æç¤ºä¿®å‰ªï¼ˆAPPï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è¯è¯­ç”Ÿæˆæç¤ºçš„å†…å®¹æ¥æ§åˆ¶å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAPPèƒ½æœ‰æ•ˆæ§åˆ¶æ¨¡å‹å’Œæ•°æ®é›†çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œä¿®å‰ªæ›´å¤šä¿¡æ¯ä¼šå¯¼è‡´è¾“å‡ºæ›´å¤šæ ·åŒ–ã€‚ç ”ç©¶å‘ç°æç¤ºå†…å®¹ä¸å¯¹è¯å¤šæ ·æ€§çš„å…³ç³»ï¼Œå…¶ä¸­è®°å¿†å—å¯¹è¾“å‡ºå¤šæ ·æ€§çš„å½±å“æœ€ä¸ºæ˜¾è‘—ã€‚APPä¸ç°æœ‰æŠ€æœ¯å…¼å®¹ï¼Œå¦‚æ¸©åº¦é‡‡æ ·å’Œtop-pé‡‡æ ·ï¼Œä¸ºå¤šæ ·æ€§ç®¡ç†æä¾›é€šç”¨å·¥å…·ã€‚ä¸ºè§£å†³å¢åŠ å¤šæ ·æ€§å¸¦æ¥çš„æƒè¡¡é—®é¢˜ï¼Œå¦‚çœç•¥ä¿¡æ¯çš„ä¸ä¸€è‡´æ€§ï¼Œå¼•å…¥äº†ä¸€ä¸ªç”Ÿæˆåæ ¡æ­£æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†æç¤ºç»“æ„ï¼ŒåŒ…æ‹¬ç»„ä»¶é¡ºåºå’Œé•¿åº¦ï¼Œå¯¹å¤šæ ·æ€§çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å…³æ³¨å¼€æ”¾åŸŸå¤šæ™ºèƒ½ä½“å¯¹è¯ä¸­çš„å¤šæ ·æ€§æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸–ç•Œæ¨¡æ‹Ÿåº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†åä¸ºè‡ªé€‚åº”æç¤ºä¿®å‰ªï¼ˆAPPï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´ç”Ÿæˆæç¤ºçš„å†…å®¹æ¥åŠ¨æ€æ§åˆ¶å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAPPèƒ½æœ‰æ•ˆæ§åˆ¶æ¨¡å‹å’Œæ•°æ®é›†çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œä¿®å‰ªæ›´å¤šä¿¡æ¯ä¼šå¯¼è‡´è¾“å‡ºæ›´ä¸°å¯Œã€å¤šæ ·åŒ–ã€‚</li>
<li>ç ”ç©¶å‘ç°æç¤ºå†…å®¹ï¼Œç‰¹åˆ«æ˜¯è®°å¿†å—ï¼Œå¯¹å¯¹è¯çš„å¤šæ ·æ€§æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>APPä¸ç°æœ‰æŠ€æœ¯å…¼å®¹ï¼Œä¸ºå¤šæ ·æ€§ç®¡ç†æä¾›é€šç”¨å·¥å…·ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç”Ÿæˆåæ ¡æ­£æ­¥éª¤ï¼Œè§£å†³äº†å¢åŠ å¤šæ ·æ€§å¯èƒ½å¸¦æ¥çš„ä¿¡æ¯ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è¿˜æ¢è®¨äº†æç¤ºç»“æ„å¯¹å¤šæ ·æ€§çš„å½±å“ï¼ŒåŒ…æ‹¬ç»„ä»¶é¡ºåºå’Œé•¿åº¦çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84874cdeb5b43d0781163379c24afb21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e2d0c62c317ffbf5f7cbf1590cc572e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-264e3d1ab662c4c493ad0b25d3a709bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5b89dedbeac7b19498f4d415aa13e14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05573722ada88a5827569313e18bd23e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2acd99ae4e6de1b4e7dcc261f794d98c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97f9571e4d3cd0d28fabf6fbe7193496.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Toward-Intelligent-and-Secure-Cloud-Large-Language-Model-Empowered-Proactive-Defense"><a href="#Toward-Intelligent-and-Secure-Cloud-Large-Language-Model-Empowered-Proactive-Defense" class="headerlink" title="Toward Intelligent and Secure Cloud: Large Language Model Empowered   Proactive Defense"></a>Toward Intelligent and Secure Cloud: Large Language Model Empowered   Proactive Defense</h2><p><strong>Authors:Yuyang Zhou, Guang Cheng, Kang Du, Zihan Chen</strong></p>
<p>The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided a large number of benefits in daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks. Recent advancements in generative foundation models (GFMs), particularly in the large language models (LLMs), offer promising solutions for security intelligence. By exploiting the powerful abilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel proactive defense architecture that defeats various threats in a proactive manner. LLM-PD can efficiently make a decision through comprehensive data analysis and sequential reasoning, as well as dynamically creating and deploying actionable defense mechanisms on the target cloud. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. The experimental results demonstrate its remarkable ability in terms of defense effectiveness and efficiency, particularly highlighting an outstanding success rate when compared with other existing methods. </p>
<blockquote>
<p>äº‘è®¡ç®—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å’Œäº‘åº”ç”¨çš„æ•°é‡å¢åŠ åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å¸¦æ¥äº†è®¸å¤šå¥½å¤„ã€‚ç„¶è€Œï¼Œä¸åŒç»„ä»¶çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§å¯¹äº‘å®‰å…¨æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å¤æ‚ä¸”å…ˆè¿›çš„ç½‘ç»œæ”»å‡»æ—¶ã€‚æœ€è¿‘ï¼Œç”Ÿæˆå¼åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢ï¼Œä¸ºå®‰å…¨æ™ºèƒ½æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€ç†è§£ã€æ•°æ®åˆ†æã€ä»»åŠ¡æ¨ç†ã€è¡ŒåŠ¨è§„åˆ’å’Œä»£ç ç”Ÿæˆç­‰å¼ºå¤§èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-PDï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸»åŠ¨é˜²å¾¡æ¶æ„ï¼Œèƒ½ä»¥ä¸»åŠ¨æ–¹å¼æŠµå¾¡å„ç§å¨èƒã€‚LLM-PDå¯ä»¥é€šè¿‡ç»¼åˆæ•°æ®åˆ†æå’Œåºåˆ—æ¨ç†æ¥æœ‰æ•ˆåšå‡ºå†³ç­–ï¼Œå¹¶èƒ½åœ¨ç›®æ ‡äº‘ä¸ŠåŠ¨æ€åˆ›å»ºå’Œéƒ¨ç½²å¯è¡Œçš„é˜²å¾¡æœºåˆ¶ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥åŸºäºä»ä»¥å¾€äº¤äº’ä¸­å­¦åˆ°çš„ç»éªŒè¿›è¡Œçµæ´»çš„è‡ªé€‚åº”è¿›åŒ–ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯é€‚åº”æ–°çš„æ”»å‡»åœºæ™¯ã€‚å®éªŒç»“æœè¯æ˜äº†å…¶åœ¨é˜²å¾¡æ•ˆæœå’Œæ•ˆç‡æ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…¶æˆåŠŸç‡å°¤ä¸ºçªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21051v1">PDF</a> 7 pages; In submission</p>
<p><strong>Summary</strong><br>äº‘è®¡ç®—æŠ€æœ¯çš„è¿…é€Ÿå‘å±•å’Œäº‘åº”ç”¨çš„ä¸æ–­å¢åŠ ä¸ºæ—¥å¸¸ç”Ÿæ´»å¸¦æ¥äº†è®¸å¤šå¥½å¤„ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†äº‘å®‰å…¨æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é¢å¯¹å¤æ‚çš„é«˜çº§ç½‘ç»œæ”»å‡»ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼ˆGFMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå®‰å…¨æ™ºèƒ½æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ä¸»åŠ¨é˜²å¾¡æ¶æ„LLM-PDï¼Œè¯¥æ¶æ„èƒ½å¤Ÿé€šè¿‡ç†è§£è¯­è¨€ã€æ•°æ®åˆ†æã€ä»»åŠ¡æ¨ç†ã€è¡ŒåŠ¨è§„åˆ’å’Œä»£ç ç”Ÿæˆç­‰åŠŸèƒ½ï¼Œä¸»åŠ¨åº”å¯¹å„ç§å¨èƒã€‚LLM-PDå¯ä»¥é€šè¿‡ç»¼åˆæ•°æ®åˆ†æå’Œæ¨ç†åšå‡ºæœ‰æ•ˆå†³ç­–ï¼Œå¹¶åŠ¨æ€åœ°åœ¨ç›®æ ‡äº‘ä¸Šåˆ›å»ºå’Œéƒ¨ç½²å¯è¡Œçš„é˜²å¾¡æœºåˆ¶ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥åŸºäºè¿‡å»çš„ç»éªŒè¿›è¡Œè‡ªæˆ‘è¿›åŒ–ï¼Œé€‚åº”æ–°çš„æ”»å‡»åœºæ™¯è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨é˜²å¾¡æ•ˆæœå’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯ä¸å…¶ä»–ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒä¸­å–å¾—äº†å‡ºè‰²çš„æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‘è®¡ç®—çš„å‘å±•å’Œäº‘åº”ç”¨å¢åŠ å¸¦æ¥äº†äº‘å®‰å…¨æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„è§£å†³æ–¹æ¡ˆæ¥åº”å¯¹å¤æ‚ç½‘ç»œæ”»å‡»ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼ˆGFMï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•ä¸ºäº‘å®‰å…¨æä¾›äº†æ–°çš„å¸Œæœ›ã€‚</li>
<li>LLM-PDæ˜¯ä¸€ç§æ–°å‹ä¸»åŠ¨é˜²å¾¡æ¶æ„ï¼Œåˆ©ç”¨LLMçš„åŠŸèƒ½è¿›è¡Œè¯­è¨€ç†è§£ã€æ•°æ®åˆ†æç­‰ï¼Œä»¥ä¸»åŠ¨åº”å¯¹äº‘å®‰å…¨å¨èƒã€‚</li>
<li>LLM-PDå¯ä»¥é€šè¿‡ç»¼åˆæ•°æ®åˆ†æå’Œæ¨ç†åšå‡ºæœ‰æ•ˆå†³ç­–ï¼Œå¹¶åŠ¨æ€éƒ¨ç½²é˜²å¾¡æœºåˆ¶ã€‚</li>
<li>LLM-PDå…·æœ‰è‡ªæˆ‘è¿›åŒ–çš„èƒ½åŠ›ï¼Œå¯ä»¥æ ¹æ®è¿‡å»çš„ç»éªŒé€‚åº”æ–°çš„æ”»å‡»åœºæ™¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜LLM-PDåœ¨é˜²å¾¡æ•ˆæœå’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0733d3c1a95c122d27cd52ba6aa9b20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d2343c5671c4a958cd3a3406bf09af4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba689db797b552211e651ee166f89149.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b8250f455e29d908d8accf4afb931e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TangoFlux-Super-Fast-and-Faithful-Text-to-Audio-Generation-with-Flow-Matching-and-Clap-Ranked-Preference-Optimization"><a href="#TangoFlux-Super-Fast-and-Faithful-Text-to-Audio-Generation-with-Flow-Matching-and-Clap-Ranked-Preference-Optimization" class="headerlink" title="TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow   Matching and Clap-Ranked Preference Optimization"></a>TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow   Matching and Clap-Ranked Preference Optimization</h2><p><strong>Authors:Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Rafael Valle, Bryan Catanzaro, Soujanya Poria</strong></p>
<p>We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†TangoFluxï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡æœ¬åˆ°éŸ³é¢‘ï¼ˆTTAï¼‰ç”Ÿæˆæ¨¡å‹ï¼Œæ‹¥æœ‰5.15äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªA40 GPUä¸Šåœ¨çŸ­çŸ­3.7ç§’å†…ç”Ÿæˆé•¿è¾¾30ç§’çš„44.1kHzéŸ³é¢‘ã€‚TTAæ¨¡å‹å¯¹é½çš„å…³é”®æŒ‘æˆ˜åœ¨äºåˆ›å»ºåå¥½å¯¹çš„éš¾åº¦ï¼Œå› ä¸ºTTAç¼ºä¹å¯éªŒè¯çš„å¥–åŠ±æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨çš„é»„é‡‘æ ‡å‡†ç­”æ¡ˆç­‰ç»“æ„åŒ–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLAPæ’ååå¥½ä¼˜åŒ–ï¼ˆCRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå¯ä»¥è¿­ä»£ç”Ÿæˆå’Œä¼˜åŒ–åå¥½æ•°æ®ï¼Œä»¥æé«˜TTAçš„å¯¹é½æ€§ã€‚æˆ‘ä»¬è¯æ˜ä½¿ç”¨CRPOç”Ÿæˆçš„éŸ³é¢‘åå¥½æ•°æ®é›†ä¼˜äºç°æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚å€ŸåŠ©æ­¤æ¡†æ¶ï¼ŒTangoFluxåœ¨å®¢è§‚å’Œä¸»è§‚åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬å¼€æºæ‰€æœ‰ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„TTAç”Ÿæˆç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21037v1">PDF</a> <a target="_blank" rel="noopener" href="https://tangoflux.github.io/">https://tangoflux.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>TangoFluxæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ–‡æœ¬è½¬éŸ³é¢‘ï¼ˆTTAï¼‰ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰5.15äº¿å‚æ•°ï¼Œèƒ½åœ¨å•ä¸ªA40 GPUä¸Šå®ç°ä»¥æ¯ç§’ç”Ÿæˆé•¿è¾¾30ç§’çš„éŸ³é¢‘ï¼Œæ‹¥æœ‰æ°å‡ºæ€§èƒ½è¡¨ç°ã€‚ä¸ºäº†æå‡TTAæ¨¡å‹çš„å¯¹é½ç¨‹åº¦ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€é¡¹æ–°æ¡†æ¶CRPOï¼ˆæ‹æ‰‹æ’ååå¥½ä¼˜åŒ–ï¼‰ï¼Œç”Ÿæˆå¹¶ä¼˜åŒ–åå¥½æ•°æ®ä»¥æ”¹è¿›TTAå¯¹é½æ•ˆæœã€‚æœ¬ç ”ç©¶è¯æ˜ï¼Œä½¿ç”¨CRPOç”Ÿæˆçš„éŸ³é¢‘åå¥½æ•°æ®é›†ä¼˜äºç°æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬å…¬å¼€æ‰€æœ‰ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„TTAç”Ÿæˆç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TangoFluxæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ–‡æœ¬è½¬éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>TangoFluxèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘ï¼Œæ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ã€‚</li>
<li>å¯¹é½TTAæ¨¡å‹çš„å…³é”®æŒ‘æˆ˜åœ¨äºåˆ›å»ºåå¥½å¯¹çš„éš¾åº¦ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†CRPOæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå’Œä¼˜åŒ–åå¥½æ•°æ®æ¥æé«˜TTAå¯¹é½æ•ˆæœã€‚</li>
<li>ä½¿ç”¨CRPOç”Ÿæˆçš„éŸ³é¢‘åå¥½æ•°æ®é›†åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>TangoFluxåœ¨å®¢è§‚å’Œä¸»è§‚åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4004e2dca1b974d821b45412b281f5f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2bf305745140b5195eb31370c15bba4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GePBench-Evaluating-Fundamental-Geometric-Perception-for-Multimodal-Large-Language-Models"><a href="#GePBench-Evaluating-Fundamental-Geometric-Perception-for-Multimodal-Large-Language-Models" class="headerlink" title="GePBench: Evaluating Fundamental Geometric Perception for Multimodal   Large Language Models"></a>GePBench: Evaluating Fundamental Geometric Perception for Multimodal   Large Language Models</h2><p><strong>Authors:Shangyu Xing, Changhao Xiang, Yuteng Han, Yifan Yue, Zhen Wu, Xinyu Liu, Zhangtai Wu, Fei Zhao, Xinyu Dai</strong></p>
<p>Multimodal large language models (MLLMs) have achieved significant advancements in integrating visual and linguistic understanding. While existing benchmarks evaluate these models in context-rich, real-life scenarios, they often overlook fundamental perceptual skills essential for environments deviating from everyday realism. In particular, geometric perception, the ability to interpret spatial relationships and abstract visual patterns, remains underexplored. To address this limitation, we introduce GePBench, a novel benchmark designed to assess the geometric perception capabilities of MLLMs. Results from extensive evaluations reveal that current state-of-the-art MLLMs exhibit significant deficiencies in such tasks. Additionally, we demonstrate that models trained with data sourced from GePBench show notable improvements on a wide range of downstream tasks, underscoring the importance of geometric perception as a foundation for advanced multimodal applications. Our code and datasets will be publicly available. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆè§†è§‰å’Œè¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚è™½ç„¶ç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨ä¸°å¯Œè¯­å¢ƒçš„ç°å®ç”Ÿæ´»ä¸­è¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†åœ¨éæ—¥å¸¸ç°å®ç¯å¢ƒä¸­è‡³å…³é‡è¦çš„åŸºæœ¬æ„ŸçŸ¥æŠ€èƒ½ã€‚ç‰¹åˆ«æ˜¯å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œå³è§£é‡Šç©ºé—´å…³ç³»å’ŒæŠ½è±¡è§†è§‰æ¨¡å¼çš„èƒ½åŠ›ï¼Œä»ç„¶è¢«ç ”ç©¶å¾—ä¸å¤Ÿæ·±å…¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GePBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ã€‚å¹¿æ³›çš„è¯„ä¼°ç»“æœæ­ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜ï¼Œä½¿ç”¨GePBenchæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ï¼Œçªæ˜¾äº†åŸºç¡€å‡ ä½•æ„ŸçŸ¥å¯¹äºé«˜çº§å¤šæ¨¡æ€åº”ç”¨çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21036v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆè§†è§‰å’Œè¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨ä¸°å¯Œè¯­å¢ƒçš„ç°å®åœºæ™¯ä¸­å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†åç¦»æ—¥å¸¸ç°å®çš„ç¯å¢ƒä¸­æ‰€å¿…éœ€çš„åŸºæœ¬æ„ŸçŸ¥æŠ€èƒ½ã€‚ç‰¹åˆ«æ˜¯å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œå³è§£é‡Šç©ºé—´å…³ç³»å’ŒæŠ½è±¡è§†è§‰æ¨¡å¼çš„èƒ½åŠ›ï¼Œä»ç„¶è¢«æ¢ç´¢å¾—å¾ˆå°‘ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GePBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ã€‚æ¥è‡ªå¹¿æ³›è¯„ä¼°çš„ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„MLLMsåœ¨æ­¤ç±»ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜ï¼Œä½¿ç”¨GePBenchæ•°æ®æºè®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹å–„ï¼Œè¿™çªå‡ºäº†å‡ ä½•æ„ŸçŸ¥ä½œä¸ºå…ˆè¿›å¤šæ¨¡æ€åº”ç”¨åŸºç¡€çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆè§†è§‰å’Œè¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¿½è§†äº†å¯¹åç¦»æ—¥å¸¸ç°å®çš„ç¯å¢ƒä¸­çš„åŸºæœ¬æ„ŸçŸ¥æŠ€èƒ½çš„è¯„ä¼°ã€‚</li>
<li>å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼ˆè§£é‡Šç©ºé—´å…³ç³»å’ŒæŠ½è±¡è§†è§‰æ¨¡å¼çš„èƒ½åŠ›ï¼‰åœ¨MLLMsä¸­ä»ç„¶è¢«æ¢ç´¢å¾—å¾ˆå°‘ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•GePBenchï¼Œç”¨äºè¯„ä¼°MLLMsçš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„MLLMsåœ¨å‡ ä½•æ„ŸçŸ¥ä»»åŠ¡çš„è¯„ä¼°ä¸­å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚</li>
<li>ä½¿ç”¨GePBenchæ•°æ®æºè®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ”¹å–„ã€‚</li>
<li>å‡ ä½•æ„ŸçŸ¥å¯¹äºå…ˆè¿›çš„å¤šæ¨¡æ€åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a35b574f718cf5d61ed6dea6bb1e1f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71afd877d793ce1c6a38e645ed36cd14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23f616a3e3a061ce494f0a5ee6432358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66fba02ca0e9494e047fbb25937ea2ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebfbfe7a15213a570656fa1a2f19ca58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e5ad2da4bbc5148dcb2f123c3db655.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Automated-Robustness-Testing-for-LLM-based-NLP-Software"><a href="#Automated-Robustness-Testing-for-LLM-based-NLP-Software" class="headerlink" title="Automated Robustness Testing for LLM-based NLP Software"></a>Automated Robustness Testing for LLM-based NLP Software</h2><p><strong>Authors:Mingxuan Xiao, Yan Xiao, Shunhui Ji, Hanbo Cai, Lei Xue, Pengcheng Zhang</strong></p>
<p>Benefiting from the advancements in LLMs, NLP software has undergone rapid development. Such software is widely employed in various safety-critical tasks, such as financial sentiment analysis, toxic content moderation, and log generation. To our knowledge, there are no known automated robustness testing methods specifically designed for LLM-based NLP software. Given the complexity of LLMs and the unpredictability of real-world inputs (including prompts and examples), it is essential to examine the robustness of overall inputs to ensure the safety of such software.   To this end, this paper introduces the first AutOmated Robustness Testing frAmework, AORTA, which reconceptualizes the testing process into a combinatorial optimization problem. Existing testing methods designed for DNN-based software can be applied to LLM-based software by AORTA, but their effectiveness is limited. To address this, we propose a novel testing method for LLM-based software within AORTA called Adaptive Beam Search. ABS is tailored for the expansive feature space of LLMs and improves testing effectiveness through an adaptive beam width and the capability for backtracking.   We successfully embed 18 test methods in the designed framework AORTA and compared the test validity of ABS with three datasets and five threat models. ABS facilitates a more comprehensive and accurate robustness assessment before software deployment, with an average test success rate of 86.138%. Compared to the currently best-performing baseline PWWS, ABS significantly reduces the computational overhead by up to 3441.895 seconds per successful test case and decreases the number of queries by 218.762 times on average. Furthermore, test cases generated by ABS exhibit greater naturalness and transferability. </p>
<blockquote>
<p>å¾—ç›ŠäºLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„è¿›æ­¥ï¼ŒNLPè½¯ä»¶å¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚æ­¤ç±»è½¯ä»¶å¹¿æ³›åº”ç”¨äºå„ç§å®‰å…¨å…³é”®ä»»åŠ¡ï¼Œå¦‚é‡‘èæƒ…æ„Ÿåˆ†æã€æœ‰æ¯’å†…å®¹å®¡æ ¸å’Œæ—¥å¿—ç”Ÿæˆç­‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰å°šæ— ä¸“é—¨ä¸ºåŸºäºLLMçš„NLPè½¯ä»¶è®¾è®¡çš„è‡ªåŠ¨åŒ–é²æ£’æ€§æµ‹è¯•æ–¹æ³•ã€‚è€ƒè™‘åˆ°LLMçš„å¤æ‚æ€§å’ŒçœŸå®ä¸–ç•Œè¾“å…¥çš„ä¸å¯é¢„æµ‹æ€§ï¼ˆåŒ…æ‹¬æç¤ºå’Œç¤ºä¾‹ï¼‰ï¼Œå¿…é¡»æ£€æŸ¥æ€»ä½“è¾“å…¥çš„é²æ£’æ€§ä»¥ç¡®ä¿æ­¤ç±»è½¯ä»¶çš„å®‰å…¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ç¬¬ä¸€ä¸ªè‡ªåŠ¨åŒ–é²æ£’æ€§æµ‹è¯•æ¡†æ¶AORTAï¼Œå®ƒå°†æµ‹è¯•è¿‡ç¨‹é‡æ–°æ¦‚å¿µåŒ–ä¸ºç»„åˆä¼˜åŒ–é—®é¢˜ã€‚AORTAå¯ä»¥å°†ä¸ºDNNï¼ˆæ·±åº¦å­¦ä¹ ç½‘ç»œï¼‰è½¯ä»¶è®¾è®¡çš„ç°æœ‰æµ‹è¯•æ–¹æ³•åº”ç”¨äºLLMè½¯ä»¶ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AORTAä¸­ä¸ºåŸºäºLLMçš„è½¯ä»¶æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ–¹æ³•ï¼Œç§°ä¸ºè‡ªé€‚åº”å…‰æŸæœç´¢ï¼ˆABSï¼‰ã€‚ABSé’ˆå¯¹LLMçš„åºå¤§ç‰¹å¾ç©ºé—´é‡èº«å®šåˆ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å…‰æŸå®½åº¦å’Œåå‘è·Ÿè¸ªèƒ½åŠ›æé«˜äº†æµ‹è¯•æ•ˆç‡ã€‚æˆ‘ä»¬æˆåŠŸåœ°åœ¨è®¾è®¡çš„æ¡†æ¶AORTAä¸­åµŒå…¥äº†18ç§æµ‹è¯•æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ä¸‰ä¸ªæ•°æ®é›†å’Œäº”ç§å¨èƒæ¨¡å‹æ¯”è¾ƒäº†ABSçš„æµ‹è¯•æœ‰æ•ˆæ€§ã€‚ABSåœ¨è½¯ä»¶éƒ¨ç½²å‰ä¿ƒè¿›äº†æ›´å…¨é¢å’Œå‡†ç¡®çš„é²æ£’æ€§è¯„ä¼°ï¼Œå¹³å‡æµ‹è¯•æˆåŠŸç‡ä¸º86.138%ã€‚ä¸å½“å‰æ€§èƒ½æœ€ä½³çš„åŸºçº¿PWWSç›¸æ¯”ï¼ŒABSå°†æ¯ä¸ªæˆåŠŸæµ‹è¯•ç”¨ä¾‹çš„è®¡ç®—å¼€é”€å‡å°‘äº†é«˜è¾¾3441.895ç§’ï¼Œå¹³å‡æŸ¥è¯¢æ¬¡æ•°å‡å°‘äº†218.762æ¬¡ã€‚æ­¤å¤–ï¼ŒABSç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹è¡¨ç°å‡ºæ›´é«˜çš„è‡ªç„¶æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21016v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¾—ç›ŠäºLLMçš„è¿›æ­¥ï¼ŒNLPè½¯ä»¶å¾—åˆ°å¿«é€Ÿå‘å±•å¹¶å¹¿æ³›åº”ç”¨äºå®‰å…¨å…³é”®ä»»åŠ¡ã€‚ç›®å‰å°šæ— å¯ç”¨äºLLM-based NLPè½¯ä»¶çš„è‡ªåŠ¨åŒ–é²æ£’æ€§æµ‹è¯•æ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»é¦–ä¸ªè‡ªåŠ¨åŒ–é²æ£’æ€§æµ‹è¯•æ¡†æ¶AORTAï¼Œå°†æµ‹è¯•è¿‡ç¨‹é‡æ„ä¸ºç»„åˆä¼˜åŒ–é—®é¢˜ã€‚AORTAè™½ç„¶å¯ä»¥åº”ç”¨ç°æœ‰é’ˆå¯¹DNNè½¯ä»¶çš„æµ‹è¯•æ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AORTAä¸­çš„æ–°å‹LLMè½¯ä»¶æµ‹è¯•æ–¹æ³•â€”â€”è‡ªé€‚åº”å…‰æŸæœç´¢ï¼ˆABSï¼‰ã€‚ABSé’ˆå¯¹LLMçš„åºå¤§ç‰¹å¾ç©ºé—´è¿›è¡Œå®šåˆ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å…‰æŸå®½åº¦å’Œå›æº¯èƒ½åŠ›æé«˜æµ‹è¯•æ•ˆç‡ã€‚åœ¨åµŒå…¥çš„18ç§æµ‹è¯•æ–¹æ³•ä¸­ï¼ŒABSåœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œäº”ä¸ªå¨èƒæ¨¡å‹ä¸­çš„æµ‹è¯•æœ‰æ•ˆæ€§è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æµ‹è¯•æˆåŠŸç‡ä¸º86.138%ã€‚ä¸å½“å‰æ€§èƒ½æœ€ä½³çš„PWWSç›¸æ¯”ï¼ŒABSæ¯æˆåŠŸæµ‹è¯•ç”¨ä¾‹çš„è®¡ç®—å¼€é”€å‡å°‘é«˜è¾¾3441.895ç§’ï¼Œå¹³å‡æŸ¥è¯¢æ¬¡æ•°å‡å°‘218.762æ¬¡ã€‚æ­¤å¤–ï¼ŒABSç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹å…·æœ‰æ›´é«˜çš„è‡ªç„¶æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMçš„è¿›æ­¥æ¨åŠ¨äº†NLPè½¯ä»¶çš„å¿«é€Ÿå‘å±•ï¼Œå¹¿æ³›åº”ç”¨äºå®‰å…¨å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç›®å‰ç¼ºä¹é’ˆå¯¹LLM-based NLPè½¯ä»¶çš„è‡ªåŠ¨åŒ–é²æ£’æ€§æµ‹è¯•æ–¹æ³•ã€‚</li>
<li>AORTAæ¡†æ¶å°†LLMè½¯ä»¶æµ‹è¯•è¿‡ç¨‹é‡æ„ä¸ºç»„åˆä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>ç°æœ‰æµ‹è¯•æ–¹æ³•åœ¨AORTAä¸­çš„åº”ç”¨è™½ç„¶å¯è¡Œï¼Œä½†æœ‰æ•ˆæ€§å—é™ã€‚</li>
<li>æå‡ºæ–°å‹LLMè½¯ä»¶æµ‹è¯•æ–¹æ³•â€”â€”è‡ªé€‚åº”å…‰æŸæœç´¢ï¼ˆABSï¼‰ï¼Œé’ˆå¯¹LLMçš„åºå¤§ç‰¹å¾ç©ºé—´å®šåˆ¶ã€‚</li>
<li>ABSé€šè¿‡è‡ªé€‚åº”å…‰æŸå®½åº¦å’Œå›æº¯èƒ½åŠ›æé«˜æµ‹è¯•æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dd74190717f1e62dc671fc0c09f79e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4811c1107cc920a9a6da937a8819aa5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MapQaTor-A-System-for-Efficient-Annotation-of-Map-Query-Datasets"><a href="#MapQaTor-A-System-for-Efficient-Annotation-of-Map-Query-Datasets" class="headerlink" title="MapQaTor: A System for Efficient Annotation of Map Query Datasets"></a>MapQaTor: A System for Efficient Annotation of Map Query Datasets</h2><p><strong>Authors:Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez</strong></p>
<p>Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: <a target="_blank" rel="noopener" href="https://mapqator.github.io/">https://mapqator.github.io/</a> and a demo video is available at: <a target="_blank" rel="noopener" href="https://youtu.be/7_aV9Wmhs6Q">https://youtu.be/7_aV9Wmhs6Q</a>. </p>
<blockquote>
<p>åƒè°·æ­Œåœ°å›¾ã€è‹¹æœåœ°å›¾ã€Openstreet Mapsè¿™æ ·çš„æ˜ å°„å’Œå¯¼èˆªæœåŠ¡å¯¹äºè®¿é—®å„ç§åŸºäºä½ç½®çš„æ•°æ®è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†è‡ªç„¶è¯­è¨€åœ°ç†ç©ºé—´æŸ¥è¯¢æ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ï¼ˆQAï¼‰æ–¹é¢çš„è¿›å±•æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†ä»åœ°å›¾æœåŠ¡åˆ›å»ºå¯é çš„åœ°ç†ç©ºé—´QAæ•°æ®é›†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†MapQaTorï¼Œè¿™æ˜¯ä¸€ä¸ªç½‘ç»œåº”ç”¨ç¨‹åºï¼Œèƒ½å¤Ÿç®€åŒ–å¯å¤ç°å’Œå¯è¿½æº¯çš„åŸºäºåœ°å›¾çš„QAæ•°æ®é›†çš„åˆ¶ä½œè¿‡ç¨‹ã€‚å‡­å€Ÿå³æ’å³ç”¨çš„æ¶æ„ï¼ŒMapQaTorèƒ½å¤Ÿæ— ç¼åœ°ä¸ä»»ä½•åœ°å›¾APIé›†æˆï¼Œå…è®¸ç”¨æˆ·ä»å„ç§æ¥æºæ”¶é›†å¹¶å¯è§†åŒ–æ•°æ®ï¼Œè€Œä¸”å‡ ä¹ä¸éœ€è¦è¿›è¡Œè®¾ç½®ã€‚é€šè¿‡ç¼“å­˜APIå“åº”ï¼Œè¯¥å¹³å°ç¡®ä¿äº†çœŸå®çš„åœ°é¢ä¿¡æ¯ä¿æŒä¸€è‡´ï¼Œå³ä½¿åœ¨ç°å®ä¸–ç•Œçš„ä¿¡æ¯ä¸æ–­å‘å±•å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æé«˜æ•°æ®çš„å¯é æ€§ã€‚MapQaToråœ¨å•ä¸ªå¹³å°ä¸Šé›†ä¸­æ•°æ®æ£€ç´¢ã€æ³¨é‡Šå’Œå¯è§†åŒ–ï¼Œæä¾›äº†ä¸€ä¸ªè¯„ä¼°åŸºäºLLMçš„åœ°ç†ç©ºé—´æ¨ç†å½“å‰çŠ¶æ€çš„ç‹¬ç‰¹æœºä¼šï¼ŒåŒæ—¶æ¨åŠ¨å…¶èƒ½åŠ›ä»¥æ”¹å–„åœ°ç†ç©ºé—´ç†è§£ã€‚è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºï¼ŒMapQaTorç›¸æ¯”æ‰‹åŠ¨æ–¹æ³•è‡³å°‘èƒ½åŠ å¿«30å€çš„æ³¨é‡Šè¿‡ç¨‹ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨å¼€å‘å¤æ‚åœ°å›¾æ¨ç†æ•°æ®é›†ç­‰åœ°ç†ç©ºé—´èµ„æºæ–¹é¢çš„æ½œåŠ›ã€‚ç½‘ç«™ç°å·²ä¸Šçº¿ï¼š<a target="_blank" rel="noopener" href="https://mapqator.github.io/">https://mapqator.github.io/</a> å¹¶ä¸”æ¼”ç¤ºè§†é¢‘å¯é€šè¿‡ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/7_aV9Wmhs6Q%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://youtu.be/7_aV9Wmhs6Qè¿›è¡Œè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21015v1">PDF</a> 13 pages, 35 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºGoogle Mapsã€Apple Mapsã€Openstreet Mapsç­‰æ˜ å°„å’Œå¯¼èˆªæœåŠ¡åœ¨å¤„ç†è‡ªç„¶è¯­è¨€åœ°ç†ç©ºé—´æŸ¥è¯¢æ—¶çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ï¼ˆQAï¼‰æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MapQaTorç½‘é¡µåº”ç”¨ç¨‹åºã€‚è¯¥ç¨‹åºå¯ç®€åŒ–å¯é‡å¤çš„ã€å¯è¿½æº¯çš„åœ°å›¾QAæ•°æ®é›†çš„åˆ¶ä½œæµç¨‹ã€‚å®ƒé‡‡ç”¨å³æ’å³ç”¨æ¶æ„ï¼Œæ”¯æŒæ— ç¼é›†æˆä»»ä½•åœ°å›¾APIï¼Œå¹¶ç¼“å­˜APIå“åº”ä»¥ç¡®ä¿çœŸå®æ•°æ®çš„ä¸€è‡´æ€§ã€‚MapQaToré›†ä¸­æ•°æ®æ£€ç´¢ã€æ³¨é‡Šå’Œå¯è§†åŒ–åœ¨ä¸€ä¸ªå¹³å°ä¸Šï¼Œä¸ä»…ä¸ºè¯„ä¼°å½“å‰LLMåœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†ç‹¬ç‰¹æœºä¼šï¼Œè€Œä¸”ä¸ºå…¶æ”¹è¿›åœ°ç†ç©ºé—´ç†è§£èƒ½åŠ›æä¾›äº†åŠ¨åŠ›ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ‰‹åŠ¨æ–¹æ³•ï¼ŒMapQaToræ ‡æ³¨é€Ÿåº¦æé«˜äº†è‡³å°‘ä¸‰åå€ã€‚MapQaTorå¹³å°å¯åŠ å¿«åœ°ç†ç©ºé—´èµ„æºå¼€å‘ï¼Œä¾‹å¦‚å¤æ‚åœ°å›¾æ¨ç†æ•°æ®é›†çš„å¼€å‘é€Ÿåº¦ã€‚å¹³å°ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://mapqator.github.io/">ç½‘ç«™é“¾æ¥</a>ã€‚æ¼”ç¤ºè§†é¢‘åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/7_aV9Wmhs6Q">è§†é¢‘é“¾æ¥</a>ã€‚æ­¤å·¥å…·æœ‰æœ›æˆä¸ºåœ°å›¾ä¿¡æ¯æŸ¥è¯¢æ–°æ¨¡å¼çš„èµ·ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MapQaToræ˜¯ä¸€ä¸ªåˆ›æ–°çš„webåº”ç”¨ç¨‹åºï¼Œç”¨äºç®€åŒ–åŸºäºåœ°å›¾çš„QAæ•°æ®é›†çš„åˆ¶ä½œæµç¨‹ã€‚</li>
<li>MapQaToræ”¯æŒæ— ç¼é›†æˆå„ç§åœ°å›¾APIï¼Œå®ç°å¤šç§æ¥æºæ•°æ®çš„è½»æ¾æ”¶é›†å’Œå¯è§†åŒ–ã€‚</li>
<li>å¹³å°ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å¹¶æå‡å…¶å¯é æ€§ï¼Œé€šè¿‡ç¼“å­˜APIå“åº”åº”å¯¹ç°å®ä¸–ç•Œä¿¡æ¯çš„å˜åŒ–ã€‚</li>
<li>MapQaTorå®ç°äº†æ•°æ®çš„æ£€ç´¢ã€æ ‡æ³¨å’Œå¯è§†åŒ–çš„é›†ä¸­å¤„ç†åœ¨ä¸€ä¸ªå¹³å°ä¸Šï¼Œæ¨è¿›LLMçš„åœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›è¯„ä¼°å’Œæå‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ‰‹åŠ¨æ–¹æ³•ç›¸æ¯”ï¼ŒMapQaTorè‡³å°‘æé«˜äº†ä¸‰åå€çš„æ ‡æ³¨é€Ÿåº¦ã€‚</li>
<li>MapQaTorå¯¹äºå¼€å‘å¤æ‚åœ°å›¾æ¨ç†æ•°æ®é›†ç­‰åœ°ç†ç©ºé—´èµ„æºå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb5d7583794826964ade546fc7ce114b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e819a061a0a2e08b93d020e505f7518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53d7d6f9d7804c6e3dba95fd5926ae7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-554f21c4919bcdb842c1c4048c107694.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3db550773fbd6e074c77101bf0262b98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7898d77edf995392fb211139d903641a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KARPA-A-Training-free-Method-of-Adapting-Knowledge-Graph-as-References-for-Large-Language-Modelâ€™s-Reasoning-Path-Aggregation"><a href="#KARPA-A-Training-free-Method-of-Adapting-Knowledge-Graph-as-References-for-Large-Language-Modelâ€™s-Reasoning-Path-Aggregation" class="headerlink" title="KARPA: A Training-free Method of Adapting Knowledge Graph as References   for Large Language Modelâ€™s Reasoning Path Aggregation"></a>KARPA: A Training-free Method of Adapting Knowledge Graph as References   for Large Language Modelâ€™s Reasoning Path Aggregation</h2><p><strong>Authors:Siyuan Fang, Kaijing Ma, Tianyu Zheng, Xinrun Du, Ningxuan Lu, Ge Zhang, Qingkun Tang</strong></p>
<p>Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs (KGs) as external knowledge sources has emerged as a viable solution, but existing methods for LLM-based knowledge graph question answering (KGQA) are often limited by step-by-step decision-making on KGs, restricting the global planning and reasoning capabilities of LLMs, or they require fine-tuning or pre-training on specific KGs. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning. KARPA operates in three steps: pre-planning relation paths using the LLMâ€™s global planning capabilities, matching semantically relevant paths via an embedding model, and reasoning over these paths to generate answers. Unlike existing KGQA methods, KARPA avoids stepwise traversal, requires no additional training, and is adaptable to various LLM architectures. Extensive experimental results show that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both high efficiency and accuracy. Our code will be available on Github. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€å—åˆ°å¹»è§‰å’ŒçŸ¥è¯†æ—¶æ•ˆæ€§çš„å½±å“ã€‚åˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä½œä¸ºå¤–éƒ¨çŸ¥è¯†æºå·²æˆä¸ºä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„åŸºäºLLMçš„çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ–¹æ³•å¾€å¾€å—åˆ°çŸ¥è¯†å›¾è°±ä¸Šçš„é€æ­¥å†³ç­–åˆ¶å®šçš„é™åˆ¶ï¼Œé™åˆ¶äº†LLMçš„å…¨å±€è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆ–è€…å®ƒä»¬éœ€è¦åœ¨ç‰¹å®šçŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œå¾®è°ƒæˆ–é¢„è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å›¾è°±è¾…åŠ©æ¨ç†è·¯å¾„èšåˆï¼ˆKARPAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLMçš„å…¨å±€è§„åˆ’èƒ½åŠ›è¿›è¡Œé«˜æ•ˆä¸”å‡†ç¡®çš„KGæ¨ç†ã€‚KARPAåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šåˆ©ç”¨LLMçš„å…¨å±€è§„åˆ’èƒ½åŠ›é¢„å…ˆè§„åˆ’å…³ç³»è·¯å¾„ã€é€šè¿‡åµŒå…¥æ¨¡å‹åŒ¹é…è¯­ä¹‰ç›¸å…³è·¯å¾„ã€ä»¥åŠåœ¨è¿™äº›è·¯å¾„ä¸Šè¿›è¡Œæ¨ç†ä»¥ç”Ÿæˆç­”æ¡ˆã€‚ä¸ç°æœ‰KGQAæ–¹æ³•ä¸åŒï¼ŒKARPAé¿å…äº†é€æ­¥éå†ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¹¶ä¸”å¯é€‚åº”å„ç§LLMæ¶æ„ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒKARPAåœ¨KGQAä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå®ç°äº†é«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨Githubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20995v1">PDF</a> 23 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å—åˆ°çŸ¥è¯†æ—¶æ•ˆæ€§å’Œå¹»è§‰çš„å½±å“ã€‚åˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä½œä¸ºå¤–éƒ¨çŸ¥è¯†æºæ˜¯ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„LLMçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ–¹æ³•å—é™äºçŸ¥è¯†å›¾è°±ä¸Šçš„é€æ­¥å†³ç­–åˆ¶å®šï¼Œé™åˆ¶äº†LLMçš„å…¨å±€è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆ–è€…éœ€è¦å¯¹ç‰¹å®šçŸ¥è¯†å›¾è°±è¿›è¡Œå¾®è°ƒæˆ–é¢„è®­ç»ƒã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å›¾è°±è¾…åŠ©æ¨ç†è·¯å¾„èšåˆï¼ˆKARPAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLMçš„å…¨å±€è§„åˆ’èƒ½åŠ›è¿›è¡Œé«˜æ•ˆã€å‡†ç¡®çš„KGæ¨ç†ã€‚KARPAåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šåˆ©ç”¨LLMçš„å…¨å±€è§„åˆ’èƒ½åŠ›é¢„å…ˆè§„åˆ’å…³ç³»è·¯å¾„ã€é€šè¿‡åµŒå…¥æ¨¡å‹åŒ¹é…è¯­ä¹‰ç›¸å…³è·¯å¾„ä»¥åŠåœ¨è¿™äº›è·¯å¾„ä¸Šè¿›è¡Œæ¨ç†ä»¥ç”Ÿæˆç­”æ¡ˆã€‚ä¸åŒäºç°æœ‰çš„KGQAæ–¹æ³•ï¼ŒKARPAé¿å…äº†é€æ­¥éå†ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¹¶ä¸”å¯é€‚åº”å„ç§LLMæ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKARPAåœ¨KGQAä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®æ€§çš„åŒé‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs å±•ç°å‡ºå“è¶Šçš„å¤šä»»åŠ¡æ€§èƒ½ï¼Œä½†ä»å—çŸ¥è¯†æ—¶æ•ˆæ€§å’Œå¹»è§‰çš„å½±å“ã€‚</li>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä½œä¸ºå¤–éƒ¨çŸ¥è¯†æºä¸º LLMs æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰LLM-based KGQAæ–¹æ³•å­˜åœ¨é€æ­¥å†³ç­–çš„é™åˆ¶ï¼Œå½±å“å…¨å±€è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>KARPAæ¡†æ¶åˆ©ç”¨LLMçš„å…¨å±€è§„åˆ’èƒ½åŠ›è¿›è¡Œé«˜æ•ˆã€å‡†ç¡®çš„KGæ¨ç†ã€‚</li>
<li>KARPAåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šé¢„è§„åˆ’å…³ç³»è·¯å¾„ã€è¯­ä¹‰è·¯å¾„åŒ¹é…å’Œè·¯å¾„æ¨ç†ã€‚</li>
<li>KARPAåŒºåˆ«äºå…¶ä»–KGQAæ–¹æ³•ï¼Œæ— éœ€é€æ­¥éå†ã€é¢å¤–è®­ç»ƒï¼Œé€‚åº”å¤šç§LLMæ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c3d595724b10132a8a7685a45f55c459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db661eb406fc2985c8cd07da1c27f6ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3287f6e9c8d3546dbcc196aa44a0e8de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-118be992e940980220b04fe3dbc3c2cc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Ontology-grounded-Automatic-Knowledge-Graph-Construction-by-LLM-under-Wikidata-schema"><a href="#Ontology-grounded-Automatic-Knowledge-Graph-Construction-by-LLM-under-Wikidata-schema" class="headerlink" title="Ontology-grounded Automatic Knowledge Graph Construction by LLM under   Wikidata schema"></a>Ontology-grounded Automatic Knowledge Graph Construction by LLM under   Wikidata schema</h2><p><strong>Authors:Xiaohan Feng, Xixin Wu, Helen Meng</strong></p>
<p>We propose an ontology-grounded approach to Knowledge Graph (KG) construction using Large Language Models (LLMs) on a knowledge base. An ontology is authored by generating Competency Questions (CQ) on knowledge base to discover knowledge scope, extracting relations from CQs, and attempt to replace equivalent relations by their counterpart in Wikidata. To ensure consistency and interpretability in the resulting KG, we ground generation of KG with the authored ontology based on extracted relations. Evaluation on benchmark datasets demonstrates competitive performance in knowledge graph construction task. Our work presents a promising direction for scalable KG construction pipeline with minimal human intervention, that yields high quality and human-interpretable KGs, which are interoperable with Wikidata semantics for potential knowledge base expansion. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥æœ¬ä½“ä¸ºåŸºç¡€ï¼Œåœ¨çŸ¥è¯†åº“ä¸Šè¿›è¡Œæ“ä½œã€‚æœ¬ä½“æ˜¯é€šè¿‡åœ¨çŸ¥è¯†åº“ä¸Šç”Ÿæˆèƒ½åŠ›é—®é¢˜ï¼ˆCQï¼‰æ¥å‘ç°çŸ¥è¯†èŒƒå›´ï¼Œä»èƒ½åŠ›é—®é¢˜ä¸­æå–å…³ç³»ï¼Œå¹¶å°è¯•ç”¨ç»´åŸºç™¾ç§‘ä¸­çš„å¯¹åº”ç‰©æ›¿æ¢ç­‰ä»·å…³ç³»è€Œåˆ›å»ºçš„ã€‚ä¸ºç¡®ä¿ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬æ ¹æ®æå–çš„å…³ç³»å°†çŸ¥è¯†å›¾è°±ä¸å·²æ„å»ºçš„æœ¬ä½“ç›¸ç»“åˆã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå³æ„å»ºä¸€ä¸ªå¯æ‰©å±•çš„çŸ¥è¯†å›¾è°±æ„å»ºç®¡é“ï¼Œä»¥æœ€å°çš„æ‰‹å·¥å¹²é¢„ï¼Œç”Ÿæˆé«˜è´¨é‡å’Œå¯è§£é‡Šçš„çŸ¥è¯†å›¾è°±ï¼Œè¿™äº›å›¾è°±å¯ä»¥ä¸ç»´åŸºç™¾ç§‘è¯­ä¹‰è¿›è¡Œäº’æ“ä½œï¼Œä»¥å®ç°æ½œåœ¨çš„çŸ¥è¯†åº“æ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20942v1">PDF</a> Presented at HI-AI@KDD, Human-Interpretable AI Workshop at the KDD   2024, 26th of August 2024, Barcelona, Spain</p>
<p><strong>Summary</strong></p>
<p>é‡‡ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºæ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆèƒ½åŠ›é—®é¢˜ï¼ˆCQï¼‰å‘æ˜çŸ¥è¯†èŒƒå›´ï¼Œæå–å…³ç³»å¹¶ä¸Wikidataä¸­çš„å¯¹åº”ç‰©è¿›è¡Œæ›¿æ¢ï¼Œç¡®ä¿çŸ¥è¯†å›¾è°±çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜åœ¨çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å·¥ä½œå±•ç°å‡ºå¤§è§„æ¨¡çŸ¥è¯†å›¾è°±æ„å»ºç®¡é“çš„å‰æ™¯ï¼Œå¯æœ€å°åŒ–äººå·¥å¹²é¢„ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”äººç±»å¯è§£é‡Šçš„çŸ¥è¯†å›¾è°±ï¼Œå¯ä¸Wikidataè¯­ä¹‰è¿›è¡Œäº’æ“ä½œä»¥å®ç°æ½œåœ¨çŸ¥è¯†åº“çš„æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºçš„ontology-groundedæ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆèƒ½åŠ›é—®é¢˜ï¼ˆCQï¼‰å‘æ˜çŸ¥è¯†èŒƒå›´ï¼Œå¹¶æå–å…³ç³»ã€‚</li>
<li>å°†æå–çš„å…³ç³»ä¸Wikidataä¸­çš„å¯¹åº”ç‰©è¿›è¡Œæ›¿æ¢ï¼Œç¡®ä¿çŸ¥è¯†å›¾è°±çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•çš„è¯„ä¼°ç»“æœå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºå®ç°å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±æ„å»ºç®¡é“ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚</li>
<li>æ„å»ºçš„çŸ¥è¯†å›¾è°±è´¨é‡é«˜ä¸”äººç±»å¯è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b88ff41f801bee72426b7f0c96bbea8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72e89aeba6421aed96b184ad9e9dbb85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00c9e609ed99ab7fc5f6613c24d5aced.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhanced-Multimodal-RAG-LLM-for-Accurate-Visual-Question-Answering"><a href="#Enhanced-Multimodal-RAG-LLM-for-Accurate-Visual-Question-Answering" class="headerlink" title="Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering"></a>Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering</h2><p><strong>Authors:Junxiao Xue, Quan Deng, Fei Yu, Yanhao Wang, Jun Wang, Yuehua Li</strong></p>
<p>Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and Flamingo, have made significant progress in integrating visual and textual modalities, excelling in tasks like visual question answering (VQA), image captioning, and content retrieval. They can generate coherent and contextually relevant descriptions of images. However, they still face challenges in accurately identifying and counting objects and determining their spatial locations, particularly in complex scenes with overlapping or small objects. To address these limitations, we propose a novel framework based on multimodal retrieval-augmented generation (RAG), which introduces structured scene graphs to enhance object recognition, relationship identification, and spatial understanding within images. Our framework improves the MLLMâ€™s capacity to handle tasks requiring precise visual descriptions, especially in scenarios with challenging perspectives, such as aerial views or scenes with dense object arrangements. Finally, we conduct extensive experiments on the VG-150 dataset that focuses on first-person visual understanding and the AUG dataset that involves aerial imagery. The results show that our approach consistently outperforms existing MLLMs in VQA tasks, which stands out in recognizing, localizing, and quantifying objects in different spatial contexts and provides more accurate visual descriptions. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå¦‚GPT-4oã€Geminiã€LLaVAå’ŒFlamingoï¼Œåœ¨æ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€å›¾åƒæ ‡é¢˜å’Œå†…å®¹æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚å®ƒä»¬å¯ä»¥ç”Ÿæˆè¿è´¯ä¸”ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„å›¾åƒæè¿°ã€‚ç„¶è€Œï¼Œåœ¨å‡†ç¡®è¯†åˆ«ã€è®¡æ•°ç‰©ä½“å¹¶ç¡®å®šå…¶ç©ºé—´ä½ç½®æ–¹é¢ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­å­˜åœ¨é‡å æˆ–å°å‹ç‰©ä½“çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥ç»“æ„åŒ–åœºæ™¯å›¾ï¼Œä»¥å¢å¼ºå›¾åƒä¸­çš„å¯¹è±¡è¯†åˆ«ã€å…³ç³»è¯†åˆ«å’Œç©ºé—´ç†è§£ã€‚æˆ‘ä»¬çš„æ¡†æ¶æé«˜äº†MLLMåœ¨å¤„ç†éœ€è¦ç²¾ç¡®è§†è§‰æè¿°çš„ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜è§†è§’çš„åœºæ™¯ä¸­ï¼Œå¦‚ç©ºä¸­ä¿¯è§†å›¾æˆ–åœºæ™¯ä¸­çš„å¯†é›†å¯¹è±¡æ’åˆ—ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä¸“æ³¨äºç¬¬ä¸€äººç§°è§†è§‰ç†è§£çš„VG-150æ•°æ®é›†å’Œæ¶‰åŠç©ºä¸­å›¾åƒçš„AUGæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VQAä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„MLLMsï¼Œç‰¹åˆ«æ“…é•¿åœ¨ä¸åŒç©ºé—´ä¸Šä¸‹æ–‡ä¸­è¯†åˆ«ã€å®šä½å’Œé‡åŒ–å¯¹è±¡ï¼Œå¹¶æä¾›æ›´å‡†ç¡®çš„è§†è§‰æè¿°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20927v1">PDF</a> 6 pages, 3 figures, under review</p>
<p><strong>Summary</strong><br>å¤§æ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å’Œæ–‡æœ¬èåˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä¾‹å¦‚åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€å›¾åƒæ ‡æ³¨å’Œå†…å®¹æ£€ç´¢ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤æ‚åœºæ™¯ä¸­è¯†åˆ«ã€è®¡æ•°å’Œå®šä½ç‰©ä½“æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œå¼•å…¥ç»“æ„åŒ–åœºæ™¯å›¾å¢å¼ºå›¾åƒä¸­çš„ç‰©ä½“è¯†åˆ«ã€å…³ç³»è¯†åˆ«å’Œç©ºé—´ç†è§£ã€‚è¯¥æ¡†æ¶æé«˜äº†MLLMå¤„ç†éœ€è¦ç²¾ç¡®è§†è§‰æè¿°çš„ä»»åŠ¡çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§è§†è§’çš„åœºæ™¯ä¸­ï¼Œå¦‚ç©ºä¸­è§†è§’æˆ–å¯†é›†ç‰©ä½“æ’åˆ—çš„åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨VQAä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºç°æœ‰MLLMsï¼Œå¹¶åœ¨ä¸åŒç©ºé—´ä¸Šä¸‹æ–‡ä¸­è¯†åˆ«ã€å®šä½å’Œé‡åŒ–ç‰©ä½“æ—¶è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oã€Geminiã€LLaVAå’ŒFlamingoåœ¨è§†è§‰å’Œæ–‡æœ¬èåˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>MLLMsåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€å›¾åƒæ ‡æ³¨å’Œå†…å®¹æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†ç‰©ä½“è¯†åˆ«ã€è®¡æ•°å’Œå®šä½æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥ç»“æ„åŒ–åœºæ™¯å›¾ä»¥å¢å¼ºå›¾åƒä¸­çš„ç‰©ä½“è¯†åˆ«ã€å…³ç³»è¯†åˆ«å’Œç©ºé—´ç†è§£ã€‚</li>
<li>æ–°å‹æ¡†æ¶æé«˜äº†MLLMå¤„ç†å¤æ‚å’Œå…·æœ‰æŒ‘æˆ˜æ€§è§†è§’åœºæ™¯ä¸­çš„ç²¾ç¡®è§†è§‰æè¿°ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨VQAä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰MLLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-731c7718bf0fa3f401b083ae2842817f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccf63b21a52227d5c358e84227f3b5a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab6d0efc264d3da0433bc9406f9c76a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ee5e1d68be674ebc63447ca13983cfc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DoTA-Weight-Decomposed-Tensor-Adaptation-for-Large-Language-Models"><a href="#DoTA-Weight-Decomposed-Tensor-Adaptation-for-Large-Language-Models" class="headerlink" title="DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models"></a>DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models</h2><p><strong>Authors:Xiaolin Hu, Xiang Cheng, Peiyu Liu, Wei Liu, Jian Luan, Bin Wang, Yong Liu</strong></p>
<p>Low-rank adaptation (LoRA) reduces the computational and memory demands of fine-tuning large language models (LLMs) by approximating updates with low-rank matrices. However, low-rank approximation in two-dimensional space fails to capture high-dimensional structures within the target matrix. Recently, tensor decomposition methods have been explored for fine-tuning LLMs, leveraging their ability to extract structured information. Yet, these approaches primarily rely on random initialization, and the impact of initialization on tensor adaptation remains underexplored. In this paper, we reveal that random initialization significantly diverges from the validation loss achieved by full fine-tuning. To address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which leverages the Matrix Product Operator (MPO) decomposition of pre-trained weights for effective initialization in fine-tuning LLMs. Additionally, we introduce QDoTA, a quantized version of DoTA designed for 4-bit quantization. Experiments on commonsense and arithmetic reasoning tasks show that DoTA outperforms random initialization methods with fewer parameters. QDoTA further reduces memory consumption and achieves comparable performance to DoTA on commonsense reasoning tasks. We will release our code to support future research. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰é€šè¿‡ä½ç§©çŸ©é˜µè¿‘ä¼¼æ›´æ–°æ¥é™ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒçš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨äºŒç»´ç©ºé—´ä¸­çš„ä½ç§©è¿‘ä¼¼æ— æ³•æ•è·ç›®æ ‡çŸ©é˜µä¸­çš„é«˜ç»´ç»“æ„ã€‚æœ€è¿‘ï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢ä½¿ç”¨å¼ é‡åˆ†è§£æ–¹æ³•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨å®ƒä»¬æå–ç»“æ„åŒ–ä¿¡æ¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦ä¾èµ–äºéšæœºåˆå§‹åŒ–ï¼Œè€Œå…³äºåˆå§‹åŒ–å¯¹å¼ é‡é€‚åº”çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°éšæœºåˆå§‹åŒ–ä¸é€šè¿‡å®Œå…¨å¾®è°ƒå®ç°çš„éªŒè¯æŸå¤±ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæƒé‡åˆ†è§£çš„å¼ é‡é€‚åº”ï¼ˆDoTAï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæƒé‡çš„çŸ©é˜µä¹˜ç§¯è¿ç®—ç¬¦ï¼ˆMPOï¼‰åˆ†è§£ï¼Œä»¥åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶è¿›è¡Œæœ‰æ•ˆåˆå§‹åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸“ä¸º4ä½é‡åŒ–è®¾è®¡çš„DoTAçš„é‡åŒ–ç‰ˆæœ¬QDoTAã€‚åœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDoTAåœ¨å…·æœ‰æ›´å°‘å‚æ•°çš„æƒ…å†µä¸‹ä¼˜äºéšæœºåˆå§‹åŒ–æ–¹æ³•ã€‚QDoTAè¿›ä¸€æ­¥é™ä½äº†å†…å­˜æ¶ˆè€—ï¼Œå¹¶åœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†ä¸DoTAç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20891v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong><br>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰é€šè¿‡ä½ç§©çŸ©é˜µè¿‘ä¼¼æ›´æ–°æ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒè®¡ç®—ä¸å†…å­˜éœ€æ±‚ã€‚ç„¶è€Œï¼ŒäºŒç»´ç©ºé—´çš„ä½ç§©è¿‘ä¼¼æ— æ³•æ•æ‰ç›®æ ‡çŸ©é˜µçš„é«˜ç»´ç»“æ„ã€‚è¿‘æœŸï¼Œç ”ç©¶è€…å¼€å§‹æ¢ç´¢åˆ©ç”¨å¼ é‡åˆ†è§£æ–¹æ³•å¾®è°ƒLLMï¼Œå…¶èƒ½æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚ä½†è¿™äº›æ–¹æ³•ä¸»è¦ä¾èµ–éšæœºåˆå§‹åŒ–ï¼Œè€Œå¼ é‡é€‚åº”å¯¹åˆå§‹åŒ–çš„å½±å“å°šæœªè¢«å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡æ­ç¤ºéšæœºåˆå§‹åŒ–ä¸å®Œå…¨å¾®è°ƒå®ç°çš„éªŒè¯æŸå¤±å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºWeight-Decomposed Tensor Adaptationï¼ˆDoTAï¼‰ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæƒé‡çš„çŸ©é˜µä¹˜ç§¯è¿ç®—ç¬¦ï¼ˆMPOï¼‰åˆ†è§£å®ç°æœ‰æ•ˆåˆå§‹åŒ–ï¼Œä»¥å¾®è°ƒLLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸º4ä½é‡åŒ–è®¾è®¡çš„DoTAçš„é‡åŒ–ç‰ˆQDoTAã€‚åœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDoTAåœ¨å‚æ•°æ›´å°‘çš„æƒ…å†µä¸‹ä¼˜äºéšæœºåˆå§‹åŒ–æ–¹æ³•ã€‚QDoTAè¿›ä¸€æ­¥é™ä½äº†å†…å­˜æ¶ˆè€—ï¼Œå¹¶åœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†ä¸DoTAç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAé€šè¿‡ä½ç§©çŸ©é˜µè¿‘ä¼¼å‡å°‘LLMå¾®è°ƒçš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œä½†æ— æ³•æ•æ‰é«˜ç»´ç»“æ„ã€‚</li>
<li>å¼ é‡åˆ†è§£æ–¹æ³•ç”¨äºå¾®è°ƒLLMï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯ã€‚</li>
<li>éšæœºåˆå§‹åŒ–åœ¨å¼ é‡é€‚åº”æ–¹æ³•ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å…¶å½±å“å°šæœªè¢«å……åˆ†ç ”ç©¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºDoTAæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæƒé‡çš„çŸ©é˜µä¹˜ç§¯è¿ç®—ç¬¦åˆ†è§£å®ç°æœ‰æ•ˆåˆå§‹åŒ–ã€‚</li>
<li>DoTAåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºéšæœºåˆå§‹åŒ–æ–¹æ³•ã€‚</li>
<li>QDoTAæ˜¯DoTAçš„é‡åŒ–ç‰ˆï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ï¼Œå®ç°äº†ä¸DoTAç›¸å½“çš„å¸¸è¯†æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-043082d24c49f29831cd5a1e3955375e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-116490072147b4059b8e4c70b5f333d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c67bc69b55f39819fe0551d4e0f697f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e89fa304f5580d032cd5c6bef5e12464.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GFormer-Accelerating-Large-Language-Models-with-Optimized-Transformers-on-Gaudi-Processors"><a href="#GFormer-Accelerating-Large-Language-Models-with-Optimized-Transformers-on-Gaudi-Processors" class="headerlink" title="GFormer: Accelerating Large Language Models with Optimized Transformers   on Gaudi Processors"></a>GFormer: Accelerating Large Language Models with Optimized Transformers   on Gaudi Processors</h2><p><strong>Authors:Chengming Zhang, Xinheng Ding, Baixi Sun, Xiaodong Yu, Weijian Zheng, Zhen Xie, Dingwen Tao</strong></p>
<p>Heterogeneous hardware like Gaudi processor has been developed to enhance computations, especially matrix operations for Transformer-based large language models (LLMs) for generative AI tasks. However, our analysis indicates that Transformers are not fully optimized on such emerging hardware, primarily due to inadequate optimizations in non-matrix computational kernels like Softmax and in heterogeneous resource utilization, particularly when processing long sequences. To address these issues, we propose an integrated approach (called GFormer) that merges sparse and linear attention mechanisms. GFormer aims to maximize the computational capabilities of the Gaudi processorâ€™s Matrix Multiplication Engine (MME) and Tensor Processing Cores (TPC) without compromising model quality. GFormer includes a windowed self-attention kernel and an efficient outer product kernel for causal linear attention, aiming to optimize LLM inference on Gaudi processors. Evaluation shows that GFormer significantly improves efficiency and model performance across various tasks on the Gaudi processor and outperforms state-of-the-art GPUs. </p>
<blockquote>
<p>é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä»»åŠ¡ï¼Œå·²ç»å¼€å‘äº†å¦‚é«˜æ–¯å¤„ç†å™¨ç­‰å¼‚æ„ç¡¬ä»¶æ¥å¢å¼ºè®¡ç®—èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ©é˜µè¿ç®—ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒTransformeråœ¨è¿™ç§æ–°å…´ç¡¬ä»¶ä¸Šå¹¶æœªå®Œå…¨ä¼˜åŒ–ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºSoftmaxç­‰éçŸ©é˜µè®¡ç®—å†…æ ¸çš„ä¼˜åŒ–ä¸è¶³ä»¥åŠå¼‚æ„èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é›†æˆæ–¹æ³•ï¼ˆç§°ä¸ºGformerï¼‰ï¼Œå®ƒç»“åˆäº†ç¨€ç–å’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ã€‚Gformeræ—¨åœ¨æœ€å¤§åŒ–é«˜æ–¯å¤„ç†å™¨çŸ©é˜µä¹˜æ³•å¼•æ“ï¼ˆMMEï¼‰å’Œå¼ é‡å¤„ç†æ ¸å¿ƒï¼ˆTPCï¼‰çš„è®¡ç®—èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå®³æ¨¡å‹è´¨é‡ã€‚GformeråŒ…æ‹¬ä¸€ä¸ªçª—å£è‡ªæ³¨æ„åŠ›å†…æ ¸å’Œä¸€ä¸ªé«˜æ•ˆçš„å¤–éƒ¨ä¹˜ç§¯å†…æ ¸ï¼Œç”¨äºå› æœçº¿æ€§æ³¨æ„åŠ›ï¼Œæ—¨åœ¨ä¼˜åŒ–åœ¨é«˜æ–¯å¤„ç†å™¨ä¸Šçš„LLMæ¨ç†ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒGformeråœ¨é«˜æ–¯å¤„ç†å™¨ä¸Šå¤§å¤§æé«˜äº†æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¼˜äºæœ€æ–°çš„GPUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19829v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹æ–°å…´ç¡¬ä»¶å¦‚Gaudiå¤„ç†å™¨ï¼Œæœ¬æ–‡ç ”ç©¶äº†å…¶åœ¨Transformerå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¡ç®—æ–¹é¢çš„ä¼˜åŒ–é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç”±äºéçŸ©é˜µè®¡ç®—å†…æ ¸ï¼ˆå¦‚Softmaxï¼‰å’Œå¼‚æ„èµ„æºåˆ©ç”¨ç‡ç­‰æ–¹é¢çš„ä¸è¶³ï¼ŒTransformerçš„ä¼˜åŒ–å¹¶ä¸å……åˆ†ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§èåˆç¨€ç–å’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶çš„ç»¼åˆæ–¹æ³•ï¼ˆç§°ä¸ºGFormerï¼‰ï¼Œæ—¨åœ¨æœ€å¤§åŒ–Gaudiå¤„ç†å™¨çš„çŸ©é˜µä¹˜æ³•å¼•æ“ï¼ˆMMEï¼‰å’Œå¼ é‡å¤„ç†æ ¸å¿ƒï¼ˆTPCï¼‰çš„è®¡ç®—èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå®³æ¨¡å‹è´¨é‡ã€‚é€šè¿‡å¼•å…¥çª—å£è‡ªæ³¨æ„åŠ›å†…æ ¸å’Œé«˜æ•ˆçš„å› æœçº¿æ€§æ³¨æ„åŠ›å¤–ç§¯å†…æ ¸ï¼ŒGFormeræ—¨åœ¨ä¼˜åŒ–Gaudiå¤„ç†å™¨ä¸Šçš„LLMæ¨ç†ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒGFormeråœ¨Gaudiå¤„ç†å™¨ä¸Šå¤§å¤§æé«˜äº†æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¼˜äºæœ€æ–°çš„GPUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Gaudiå¤„ç†å™¨è¢«å¼€å‘ç”¨äºå¢å¼ºè®¡ç®—ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹Transformerå¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ©é˜µæ“ä½œã€‚</li>
<li>ç›®å‰Transformeråœ¨Gaudiå¤„ç†å™¨ä¸Šçš„ä¼˜åŒ–å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦è¡¨ç°åœ¨éçŸ©é˜µè®¡ç®—å†…æ ¸å’Œå¼‚æ„èµ„æºåˆ©ç”¨ç‡æ–¹é¢ã€‚</li>
<li>GFormeræ˜¯ä¸€ç§ç»¼åˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡èåˆç¨€ç–å’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æ¥ä¼˜åŒ–è®¡ç®—ã€‚</li>
<li>GFormerèƒ½å¤Ÿæœ€å¤§åŒ–Gaudiå¤„ç†å™¨çš„è®¡ç®—èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹è´¨é‡ã€‚</li>
<li>GFormeråŒ…æ‹¬çª—å£è‡ªæ³¨æ„åŠ›å†…æ ¸å’Œå› æœçº¿æ€§æ³¨æ„åŠ›çš„å¤–ç§¯å†…æ ¸ï¼Œä»¥ä¼˜åŒ–Gaudiå¤„ç†å™¨ä¸Šçš„LLMæ¨ç†ã€‚</li>
<li>è¯„ä¼°è¡¨æ˜ï¼ŒGFormeråœ¨Gaudiå¤„ç†å™¨ä¸Šæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db4bfdfb03e6e8bd6043601d0bf83f73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c00b4f8327e6df20a014b8b74af845fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-779183b8ff7f2504227972c0898a2b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9195078e0cbb0af659d8d24db5bad98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb84d349cbcd99cda54d46328e511148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9becd06051752c0c7810e6831b2e1dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b827ba13e27b730661f6ba815189ce6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98d475546cf323752cc2f17b0a3e098f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e28f4a6661e2834191994bc88a1d519c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DrivingWorld-Constructing-World-Model-for-Autonomous-Driving-via-Video-GPT"><a href="#DrivingWorld-Constructing-World-Model-for-Autonomous-Driving-via-Video-GPT" class="headerlink" title="DrivingWorld: Constructing World Model for Autonomous Driving via Video   GPT"></a>DrivingWorld: Constructing World Model for Autonomous Driving via Video   GPT</h2><p><strong>Authors:Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan</strong></p>
<p>Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token prediction strategy to capture spatial information within each frame. To further enhance generalization ability, we propose a novel masking strategy and reweighting strategy for token prediction to mitigate long-term drifting issues and enable precise control. Our work demonstrates the ability to produce high-fidelity and consistent video clips of over 40 seconds in duration, which is over 2 times longer than state-of-the-art driving world models. Experiments show that, in contrast to prior works, our method achieves superior visual quality and significantly more accurate controllable future video generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YvanYin/DrivingWorld">https://github.com/YvanYin/DrivingWorld</a>. </p>
<blockquote>
<p>è¿‘æœŸåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„GPTç³»åˆ—ç­‰æˆæœå±•ç¤ºäº†è‡ªå›å½’ï¼ˆARï¼‰ç”Ÿæˆæ¨¡å‹çš„æˆåŠŸï¼Œè¿™æ¿€å‘äº†äººä»¬å°†è¿™ç§æˆåŠŸå¤åˆ¶åˆ°è§†è§‰ä»»åŠ¡çš„åŠªåŠ›ã€‚ä¸€äº›å·¥ä½œå°è¯•å°†è¿™ç§æŠ€æœ¯æ‰©å±•åˆ°è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œé€šè¿‡å»ºç«‹åŸºäºè§†é¢‘çš„èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„æœªæ¥è§†é¢‘åºåˆ—å¹¶é¢„æµ‹è‡ªæˆ‘æ„è¯†çŠ¶æ€çš„ä¸–ç•Œæ¨¡å‹ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œå¾€å¾€äº§ç”Ÿä»¤äººä¸æ»¡æ„çš„ç»“æœï¼Œå› ä¸ºç»å…¸çš„GPTæ¡†æ¶è®¾è®¡ç”¨äºå¤„ç†ä¸€ç»´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ï¼‰ï¼Œç¼ºä¹å»ºæ¨¡è§†é¢‘ç”Ÿæˆæ‰€éœ€çš„å›ºæœ‰ç©ºé—´å’Œæ—¶é—´åŠ¨æ€èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè‡ªåŠ¨é©¾é©¶çš„DrivingWorldä¸–ç•Œæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å¤šç§æ—¶ç©ºèåˆæœºåˆ¶çš„GPTé£æ ¼è®¾è®¡ã€‚è¿™ç§è®¾è®¡ä½¿ç©ºé—´å’Œæ—¶é—´åŠ¨æ€çš„å»ºæ¨¡æœ‰æ•ˆï¼Œå®ç°äº†é«˜ä¿çœŸã€é•¿æ—¶é—´çš„è§†é¢‘ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‹ä¸€çŠ¶æ€é¢„æµ‹ç­–ç•¥æ¥æ¨¡æ‹Ÿè¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ï¼Œå¹¶é‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç­–ç•¥æ¥æ•è·æ¯ä¸ªå¸§å†…çš„ç©ºé—´ä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰Œé¢„æµ‹æ©ç ç­–ç•¥å’Œé‡æ–°åŠ æƒç­–ç•¥ï¼Œä»¥å‡è½»é•¿æœŸæ¼‚ç§»é—®é¢˜å¹¶å®ç°ç²¾ç¡®æ§åˆ¶ã€‚æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿäº§ç”ŸæŒç»­è¶…è¿‡40ç§’çš„é«˜ä¿çœŸä¸”è¿è´¯çš„è§†é¢‘ç‰‡æ®µï¼Œè¿™æ¯”æœ€å…ˆè¿›çš„é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„æ—¶é—´é•¿ä¸¤å€å¤šã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œå¯æ§çš„æœªæ¥è§†é¢‘ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æ›´é«˜çš„æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YvanYin/DrivingWorld%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YvanYin/DrivingWorldä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19505v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„GPTç³»åˆ—ç­‰è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„æˆåŠŸï¼Œæ¿€å‘äº†å°†å…¶åº”ç”¨äºè§†è§‰ä»»åŠ¡çš„åŠªåŠ›ã€‚æœ¬æ–‡å°è¯•å°†è¿™ä¸€æ–¹æ³•åº”ç”¨äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘çš„è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹DrivingWorldï¼Œæ—¨åœ¨ç”Ÿæˆé€¼çœŸçš„æœªæ¥è§†é¢‘åºåˆ—å¹¶é¢„æµ‹è‡ªæˆ‘çŠ¶æ€ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒDrivingWorldè®¾è®¡äº†å¤šç§æ—¶ç©ºèåˆæœºåˆ¶ï¼Œå®ç°äº†å¯¹è§†é¢‘æ—¶ç©ºåŠ¨æ€çš„æœ‰æ•ˆå»ºæ¨¡ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€é•¿æ—¶é—´çš„è§†é¢‘ã€‚é€šè¿‡æå‡ºä¸€ç§çŠ¶æ€é¢„æµ‹ç­–ç•¥å’Œä¸€ç§æ ‡è®°é¢„æµ‹ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„æ—¶ç©ºè¿è´¯æ€§å’Œç©ºé—´ä¿¡æ¯æ•æ‰èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ©ç ç­–ç•¥å’Œé‡æƒç­–ç•¥æ¥å‡è½»é•¿æœŸæ¼‚ç§»é—®é¢˜å¹¶å®ç°ç²¾ç¡®æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä¹‹å‰çš„å·¥ä½œï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œå¯æ§çš„æœªæ¥è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/YvanYin/DrivingWorld%E3%80%82">https://github.com/YvanYin/DrivingWorldã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªå›å½’ç”Ÿæˆæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨å—åˆ°å…³æ³¨ã€‚</li>
<li>DrivingWorldæ˜¯ä¸€ç§åŸºäºè§†é¢‘çš„è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ï¼Œé‡‡ç”¨GPTé£æ ¼è®¾è®¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¤šç§æ—¶ç©ºèåˆæœºåˆ¶å®ç°è§†é¢‘æ—¶ç©ºåŠ¨æ€çš„æœ‰æ•ˆå»ºæ¨¡ã€‚</li>
<li>é©¾é©¶ä¸–ç•Œæ¨¡å‹èƒ½ç”Ÿæˆé«˜ä¿çœŸã€é•¿æ—¶é—´çš„è§†é¢‘ï¼Œæ—¶é•¿è¶…è¿‡ç°æœ‰æ¨¡å‹çš„ä¸¤å€ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨çŠ¶æ€é¢„æµ‹ç­–ç•¥å’Œæ ‡è®°é¢„æµ‹ç­–ç•¥æ¥æé«˜æ—¶ç©ºè¿è´¯æ€§å’Œç©ºé—´ä¿¡æ¯æ•æ‰ã€‚</li>
<li>å¼•å…¥æ©ç ç­–ç•¥å’Œé‡æƒç­–ç•¥ä»¥è§£å†³é•¿æœŸæ¼‚ç§»é—®é¢˜å¹¶å®ç°ç²¾ç¡®æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-45c7f7f73dab907c9be5dff0ed475f9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11728948abd4ac8205cacd260867ec51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5413fd286d6d72ddf078e948ffac5d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2238ee99590c6550977b01c26c06773e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RecLM-Recommendation-Instruction-Tuning"><a href="#RecLM-Recommendation-Instruction-Tuning" class="headerlink" title="RecLM: Recommendation Instruction Tuning"></a>RecLM: Recommendation Instruction Tuning</h2><p><strong>Authors:Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang</strong></p>
<p>Modern recommender systems aim to deeply understand usersâ€™ complex preferences through their past interactions. While deep collaborative filtering approaches using Graph Neural Networks (GNNs) excel at capturing user-item relationships, their effectiveness is limited when handling sparse data or zero-shot scenarios, primarily due to constraints in ID-based embedding functions. To address these challenges, we propose a model-agnostic recommendation instruction-tuning paradigm that seamlessly integrates large language models with collaborative filtering. Our proposed Recommendation Language Model (RecLM) enhances the capture of user preference diversity through a carefully designed reinforcement learning reward function that facilitates self-augmentation of language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings, and its plug-and-play compatibility with state-of-the-art recommender systems results in notable performance enhancements. </p>
<blockquote>
<p>ç°ä»£æ¨èç³»ç»Ÿæ—¨åœ¨é€šè¿‡ç”¨æˆ·è¿‡å»çš„äº¤äº’æ¥æ·±å…¥äº†è§£ç”¨æˆ·çš„å¤æ‚åå¥½ã€‚è™½ç„¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æ·±åº¦ååŒè¿‡æ»¤æ–¹æ³•åœ¨æ•æ‰ç”¨æˆ·-é¡¹ç›®å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å½“å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶å°„å‡»åœºæ™¯æ—¶ï¼Œå®ƒä»¬çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºåŸºäºIDçš„åµŒå…¥å‡½æ•°çš„çº¦æŸã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤å¾®è°ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼æ— ç¼é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤ã€‚æˆ‘ä»¬æå‡ºçš„æ¨èè¯­è¨€æ¨¡å‹ï¼ˆRecLMï¼‰é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°æé«˜äº†å¯¹ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰ï¼Œè¯¥å¥–åŠ±å‡½æ•°ä¿ƒè¿›äº†è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä¸”å®ƒä¸æœ€æ–°æ¨èç³»ç»Ÿçš„å³æ’å³ç”¨å…¼å®¹æ€§å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç°ä»£æ¨èç³»ç»Ÿè‡´åŠ›äºé€šè¿‡ç”¨æˆ·è¿‡å»çš„äº’åŠ¨æ¥æ·±å…¥ç†è§£å…¶å¤æ‚çš„åå¥½ã€‚é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œåœ¨å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶æ ·æœ¬åœºæ™¯æ—¶çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒæ•´èŒƒå¼ï¼Œè¯¥èŒƒå¼èƒ½å¤Ÿæ— ç¼åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤é›†æˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬çš„æ¨èè¯­è¨€æ¨¡å‹ï¼ˆRecLMï¼‰é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæé«˜äº†å¯¹ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰èƒ½åŠ›ï¼Œä¿ƒè¿›äº†è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä¸”ä¸æœ€å…ˆè¿›çš„æ¨èç³»ç»Ÿçš„å³æ’å³ç”¨å…¼å®¹æ€§å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æ¨èç³»ç»Ÿè‡´åŠ›äºé€šè¿‡ç”¨æˆ·è¿‡å»çš„è¡Œä¸ºç†è§£å…¶å¤æ‚åå¥½ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œåœ¨å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶æ ·æœ¬åœºæ™¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼Œé›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤ã€‚</li>
<li>æ¨èè¯­è¨€æ¨¡å‹ï¼ˆRecLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°æé«˜ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰ã€‚</li>
<li>RecLMèƒ½ä¿ƒè¿›è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºRecLMåœ¨å„ç§è®¾ç½®ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c397df8ccfcff0dd609df4884a82f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28ed79cf78bcfa943e4ad56ac97c5ce3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Biology-Instructions-A-Dataset-and-Benchmark-for-Multi-Omics-Sequence-Understanding-Capability-of-Large-Language-Models"><a href="#Biology-Instructions-A-Dataset-and-Benchmark-for-Multi-Omics-Sequence-Understanding-Capability-of-Large-Language-Models" class="headerlink" title="Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence   Understanding Capability of Large Language Models"></a>Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence   Understanding Capability of Large Language Models</h2><p><strong>Authors:Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye</strong></p>
<p>Large language models have already demonstrated their formidable capabilities in general domains, ushering in a revolutionary transformation. However, exploring and exploiting the extensive knowledge of these models to comprehend multi-omics biology remains underexplored. To fill this research gap, we first introduce Biology-Instructions, the first large-scale multi-omics biological sequences-related instruction-tuning dataset including DNA, RNA, proteins, and multi-molecules, designed to bridge the gap between large language models (LLMs) and complex biological sequences-related tasks. This dataset can enhance the versatility of LLMs by integrating diverse biological sequenced-based prediction tasks with advanced reasoning capabilities, while maintaining conversational fluency. Additionally, we reveal significant performance limitations in even state-of-the-art LLMs on biological sequence-related multi-omics tasks without specialized pre-training and instruction-tuning. We further develop a strong baseline called ChatMultiOmics with a novel three-stage training pipeline, demonstrating the powerful ability to understand biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics are publicly available and crucial resources for enabling more effective integration of LLMs with multi-omics sequence analysis. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»åœ¨é€šç”¨é¢†åŸŸå±•ç°äº†å…¶å¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶å¼•é¢†äº†ä¸€åœºé©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œæ¢ç´¢è¿™äº›æ¨¡å‹ä¸°å¯Œçš„çŸ¥è¯†ä»¥ç†è§£å¤šç»„å­¦ç”Ÿç‰©å­¦ä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªå……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ç”Ÿç‰©å­¦æŒ‡ä»¤æ•°æ®é›†ï¼ˆBiology-Instructionsï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šç»„å­¦ç”Ÿç‰©åºåˆ—ç›¸å…³æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬DNAã€RNAã€è›‹ç™½è´¨å’Œå¤šç§åˆ†å­ï¼Œæ—¨åœ¨å¼¥è¡¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤æ‚çš„ç”Ÿç‰©åºåˆ—ç›¸å…³ä»»åŠ¡ä¹‹é—´çš„å·®è·ã€‚è¯¥æ•°æ®é›†é€šè¿‡æ•´åˆå¤šæ ·åŒ–çš„ç”Ÿç‰©åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹è¯çš„æµç•…æ€§ï¼Œå¢å¼ºäº†LLMçš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ­ç¤ºäº†å³ä½¿åœ¨æœ€å…ˆè¿›çš„LLMä¸Šï¼Œåœ¨æ²¡æœ‰ä¸“é—¨çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åœ¨ç”Ÿç‰©åºåˆ—ç›¸å…³çš„å¤šç»„å­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ChatMultiOmicsï¼Œé‡‡ç”¨æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨ç”Ÿç‰©å­¦æŒ‡ä»¤å±•ç¤ºäº†å¼ºå¤§çš„ç”Ÿç‰©å­¦ç†è§£èƒ½åŠ›ã€‚ç”Ÿç‰©å­¦æŒ‡ä»¤å’ŒChatMultiOmicsæ˜¯å…¬å¼€å¯ç”¨çš„é‡è¦èµ„æºï¼Œå¯¹äºæ›´æœ‰æ•ˆåœ°å°†LLMä¸å¤šç»„å­¦åºåˆ—åˆ†æç›¸ç»“åˆè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19191v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸå±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨ç†è§£å¤šç»„å­¦ç”Ÿç‰©å­¦æ–¹é¢åº”ç”¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºBiology-Instructionsæ•°æ®é›†ï¼ŒåŒ…å«DNAã€RNAã€è›‹ç™½è´¨ç­‰å¤šåˆ†å­ç”Ÿç‰©åºåˆ—ç›¸å…³æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œæ—¨åœ¨å¡«è¡¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤æ‚ç”Ÿç‰©åºåˆ—ä»»åŠ¡ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥æ•°æ®é›†å¢å¼ºLLMçš„é€šç”¨æ€§ï¼Œé€šè¿‡é›†æˆå¤šæ ·ç”Ÿç‰©åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸é«˜çº§æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€æµç•…ã€‚æˆ‘ä»¬æ­ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMsåœ¨å¤„ç†ç”Ÿç‰©åºåˆ—ç›¸å…³å¤šç»„å­¦ä»»åŠ¡æ—¶ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½å±€é™ï¼Œéœ€ä¸“é—¨é¢„è®­ç»ƒä¸æŒ‡ä»¤å¾®è°ƒã€‚æˆ‘ä»¬å¼€å‘å‡ºChatMultiOmicsåŸºçº¿æ¨¡å‹ï¼Œé‡‡ç”¨åˆ›æ–°çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œå±•ç¤ºä½¿ç”¨Biology-Instructionsç†è§£ç”Ÿç‰©å­¦çš„å¼ºå¤§èƒ½åŠ›ã€‚ä¸¤è€…å‡ä¸ºå…¬å¼€èµ„æºï¼Œæœ‰åŠ©äºLLMsä¸å¤šç»„å­¦åºåˆ—åˆ†æçš„æœ‰æ•ˆç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç»„å­¦ç”Ÿç‰©å­¦é¢†åŸŸçš„è¿ç”¨å°šå¾…æ¢ç´¢ã€‚</li>
<li>Biology-Instructionsæ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡å¤šç»„å­¦ç”Ÿç‰©åºåˆ—ç›¸å…³æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œæ—¨åœ¨è¿æ¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤æ‚ç”Ÿç‰©åºåˆ—ä»»åŠ¡ã€‚</li>
<li>è¯¥æ•°æ®é›†èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨æ€§ï¼Œç»“åˆç”Ÿç‰©åºåˆ—é¢„æµ‹ä»»åŠ¡å’Œé«˜çº§æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€æµç•…ã€‚</li>
<li>å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç”Ÿç‰©åºåˆ—ç›¸å…³å¤šç»„å­¦ä»»åŠ¡æ—¶å­˜åœ¨æ€§èƒ½å±€é™ï¼Œéœ€è¦ä¸“é—¨çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>ChatMultiOmicsåŸºçº¿æ¨¡å‹é‡‡ç”¨åˆ›æ–°çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œå±•ç°å¼ºå¤§çš„ç”Ÿç‰©å­¦ç†è§£èƒ½åŠ›ã€‚</li>
<li>Biology-Instructionså’ŒChatMultiOmicséƒ½æ˜¯å…¬å¼€èµ„æºï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç»„å­¦åºåˆ—åˆ†æçš„ç»“åˆæä¾›äº†é‡è¦æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-307ab993b1e8a7b917893eb629a29f3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f152fbdefe19317626e04126a872f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dd6abe295c8908e61b33d1187aa65fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4285c759c086a1ad0876dbc14d1854bd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Open-Vocabulary-Panoptic-Segmentation-Using-BERT-Pre-Training-of-Vision-Language-Multiway-Transformer-Model"><a href="#Open-Vocabulary-Panoptic-Segmentation-Using-BERT-Pre-Training-of-Vision-Language-Multiway-Transformer-Model" class="headerlink" title="Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of   Vision-Language Multiway Transformer Model"></a>Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of   Vision-Language Multiway Transformer Model</h2><p><strong>Authors:Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen</strong></p>
<p>Open-vocabulary panoptic segmentation remains a challenging problem. One of the biggest difficulties lies in training models to generalize to an unlimited number of classes using limited categorized training data. Recent popular methods involve large-scale vision-language pre-trained foundation models, such as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation using another large-scale vision-language pre-trained model called BEiT-3 and leveraging the cross-modal attention between visual and linguistic features in BEiT-3 to achieve better performance. Experiments result demonstrates that OMTSeg performs favorably against state-of-the-art models. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æœ€å¤§çš„å›°éš¾ä¹‹ä¸€æ˜¯ï¼Œåœ¨æœ‰é™çš„åˆ†ç±»è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨å¹¿åˆ°æ— é™æ•°é‡çš„ç±»åˆ«ã€‚æœ€è¿‘æµè¡Œçš„æ–¹æ³•æ¶‰åŠå¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¦‚CLIPã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¦ä¸€ç§å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹Beit-3è¿›è¡Œå¼€æ”¾è¯æ±‡åˆ†å‰²çš„OMTSegæ–¹æ³•ï¼Œå¹¶å€ŸåŠ©Beit-3ä¸­çš„è§†è§‰å’Œè¯­è¨€ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOMTSegçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18917v1">PDF</a> ICIP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹BEiT-3çš„å¼€æ”¾è¯æ±‡è¡¨åˆ†å‰²æ–¹æ³•OMTSegã€‚è¯¥æ–¹æ³•åˆ©ç”¨BEiT-3ä¸­è§†è§‰å’Œè¯­è¨€ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æœ‰é™åˆ†ç±»çš„è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ³›åŒ–åˆ°æ— é™æ•°é‡çš„ç±»åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOMTSegç›¸è¾ƒäºå½“å‰å…ˆè¿›æ¨¡å‹è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¡¨åˆ†å‰²æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œä¸»è¦åœ¨äºå¦‚ä½•åˆ©ç”¨æœ‰é™çš„åˆ†ç±»è®­ç»ƒæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ä»¥æ³›åŒ–åˆ°æ— é™æ•°é‡çš„ç±»åˆ«ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•OMTSegï¼ŒåŸºäºå¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹BEiT-3è¿›è¡Œå¼€æ”¾è¯æ±‡è¡¨åˆ†å‰²ã€‚</li>
<li>OMTSegåˆ©ç”¨BEiT-3ä¸­çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆè§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œä»¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOMTSegç›¸è¾ƒäºå½“å‰å…ˆè¿›æ¨¡å‹å…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºå¤„ç†å¤§è§„æ¨¡ã€å¤šæ ·åŒ–æ•°æ®çš„åœºæ™¯å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>OMTSegçš„æå‡ºå±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eff71f92c2d9e2a0a95553369477761a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5586bb57b1040f537bfccd82c235750.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a6c933ff3940f58f39c3837db6453d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3075e68b6e7e7d86ed8b747d76871765.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Bootstrap-Your-Own-Context-Length"><a href="#Bootstrap-Your-Own-Context-Length" class="headerlink" title="Bootstrap Your Own Context Length"></a>Bootstrap Your Own Context Length</h2><p><strong>Authors:Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei</strong></p>
<p>We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„çŸ­ä¸Šä¸‹æ–‡èƒ½åŠ›è¿›è¡Œè®­ç»ƒçš„è‡ªä¸¾æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸€ä¸ªç®€å•çš„ä»£ç†å·¥ä½œæµç¨‹æ¥åˆæˆå¤šæ ·åŒ–çš„é•¿ä¸Šä¸‹æ–‡æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œä»è€Œæ¶ˆé™¤äº†æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨æ•°æ®çš„å¿…è¦æ€§ã€‚æ‰€æå‡ºçš„æ•°æ®åˆæˆå·¥ä½œæµç¨‹åªéœ€è¦çŸ­ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ£€ç´¢å™¨å’Œæ–‡æ¡£é›†åˆï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥åœ¨å¼€æºç”Ÿæ€ç³»ç»Ÿä¸­è½»æ¾è®¿é—®ã€‚éšåï¼Œä½¿ç”¨åˆæˆæ•°æ®å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªä¸¾è¿‡ç¨‹æœ‰æ•ˆåœ°å°†è¯­è¨€æ¨¡å‹çš„çŸ­ä¸Šä¸‹æ–‡èƒ½åŠ›è½¬ç§»åˆ°é•¿ä¸Šä¸‹æ–‡åœºæ™¯ã€‚æˆ‘ä»¬å¯¹å¼€æºLlama-3ç³»åˆ—æ¨¡å‹è¿›è¡Œå®éªŒï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†ä¸Šä¸‹æ–‡é•¿åº¦æˆåŠŸæ‰©å±•åˆ°é«˜è¾¾1Mä¸ªä»¤ç‰Œï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18860v1">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>ä»‹ç»äº†ä¸€ç§åˆ©ç”¨çŸ­è¯­å¢ƒèƒ½åŠ›è®­ç»ƒé•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹çš„è‡ªä¸¾æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç®€å•çš„ä»£ç†å·¥ä½œæµç¨‹åˆæˆå¤šæ ·åŒ–çš„é•¿è¯­å¢ƒæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæ¶ˆé™¤äº†å¯¹æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„éœ€æ±‚ã€‚ä½¿ç”¨çŸ­è¯­å¢ƒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ£€ç´¢å™¨å’Œæ–‡æ¡£é›†åˆç­‰å¼€æºç”Ÿæ€ç³»ç»Ÿä¸­çš„èµ„æºå³å¯å®ç°æ•°æ®åˆæˆã€‚éšåï¼Œä½¿ç”¨åˆæˆæ•°æ®å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å°†é€šè¿‡è‡ªä¸¾è¿‡ç¨‹å°†è¯­è¨€æ¨¡å‹çš„çŸ­è¯­å¢ƒèƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­å¢ƒåœºæ™¯ã€‚å¯¹å¼€æºLlama-3ç³»åˆ—æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯å°†ä¸Šä¸‹æ–‡é•¿åº¦æˆåŠŸæ‰©å±•åˆ°100ä¸‡ä»¤ç‰Œï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åˆ©ç”¨çŸ­è¯­å¢ƒèƒ½åŠ›è®­ç»ƒé•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹çš„è‡ªä¸¾æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç®€å•çš„ä»£ç†å·¥ä½œæµç¨‹åˆæˆå¤šæ ·åŒ–çš„é•¿è¯­å¢ƒæŒ‡ä»¤è°ƒæ•´æ•°æ®ã€‚</li>
<li>æ¶ˆé™¤äº†å¯¹æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„éœ€æ±‚ã€‚</li>
<li>ä½¿ç”¨çŸ­è¯­å¢ƒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ£€ç´¢å™¨å’Œæ–‡æ¡£é›†åˆç­‰å¼€æºèµ„æºå®ç°æ•°æ®åˆæˆã€‚</li>
<li>é€šè¿‡å¾®è°ƒè¯­è¨€æ¨¡å‹ä½¿ç”¨åˆæˆæ•°æ®æ¥æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</li>
<li>æˆåŠŸå°†è¯­è¨€æ¨¡å‹çš„çŸ­è¯­å¢ƒèƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­å¢ƒåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-595a55b4baaea4d3548588841f0e0ac4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f8afff45778b7f77fedeb8c51144691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53262c78cc2486ca7afcd44ced590d4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7827bb6f2f165dd9d946225924095f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aece8d05b53d0866f2cb3f69a3c161cf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GPT-or-BERT-why-not-both"><a href="#GPT-or-BERT-why-not-both" class="headerlink" title="GPT or BERT: why not both?"></a>GPT or BERT: why not both?</h2><p><strong>Authors:Lucas Georges Gabriel Charpentier, David Samuel</strong></p>
<p>We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack: GPT-BERT can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå°†æ©ç è¯­è¨€å»ºæ¨¡ä¸å› æœè¯­è¨€å»ºæ¨¡ç›¸ç»“åˆã€‚è¿™ç§æ··åˆè®­ç»ƒç›®æ ‡å¯¼è‡´äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¸€ä¸ªå•ä¸€çš„å˜å‹å™¨å †æ ˆå†…ç»“åˆäº†ä¸¤ç§å»ºæ¨¡æ–¹æ³•çš„ä¼˜ç‚¹ï¼šGPT-BERTå¯ä»¥åƒä»»ä½•æ ‡å‡†çš„å› æœæˆ–æ©ç è¯­è¨€æ¨¡å‹ä¸€æ ·é€æ˜åœ°ä½¿ç”¨ã€‚æˆ‘ä»¬åœ¨BabyLM Challenge 2024ä¸Šæµ‹è¯•äº†è¿™ç§çµæ´»è¡Œä¸ºçš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚ç»“æœè¡¨æ˜ï¼Œæ··åˆé¢„è®­ç»ƒä¼˜äºä»…ä½¿ç”¨æ©ç æˆ–ä»…ä½¿ç”¨å› æœçš„æ¨¡å‹ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ¨¡å‹ã€è®­ç»ƒè¯­æ–™åº“å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.24159v2">PDF</a> 22 pages; submission to the BabyLM Challenge 2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ©ç è¯­è¨€å»ºæ¨¡ä¸å› æœè¯­è¨€å»ºæ¨¡ç›¸ç»“åˆçš„æ–¹æ³•ã€‚è¿™ç§æ··åˆè®­ç»ƒç›®æ ‡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªå˜å‹å™¨å †æ ˆä¸­ç»“åˆä¸¤ç§å»ºæ¨¡æ–¹æ³•çš„ä¼˜ç‚¹ï¼šGPT-BERTå¯ä»¥åƒä»»ä½•æ ‡å‡†å› æœæˆ–æ©ç è¯­è¨€æ¨¡å‹ä¸€æ ·é€æ˜åœ°ä½¿ç”¨ã€‚åœ¨BabyLM Challenge 2024ä¸Šæµ‹è¯•äº†è¿™ç§çµæ´»è¡Œä¸ºçš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œç»“æœæ˜¾ç¤ºæ··åˆé¢„è®­ç»ƒä¼˜äºä»…ä½¿ç”¨æ©ç æˆ–ä»…ä½¿ç”¨å› æœçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ©ç è¯­è¨€å»ºæ¨¡ä¸å› æœè¯­è¨€å»ºæ¨¡çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸æ¨¡å‹åœ¨åŒä¸€è½¬æ¢å™¨å †æ ˆä¸­ç»“åˆä¸¤ç§å»ºæ¨¡æ–¹æ³•çš„ä¼˜ç‚¹ã€‚</li>
<li>GPT-BERTå¯ä»¥åƒæ ‡å‡†å› æœæˆ–æ©ç è¯­è¨€æ¨¡å‹ä¸€æ ·ä½¿ç”¨ã€‚</li>
<li>åœ¨BabyLM Challenge 2024ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒè¿‡ç¨‹æµ‹è¯•ã€‚</li>
<li>æ··åˆé¢„è®­ç»ƒè¡¨ç°ä¼˜äºä»…ä½¿ç”¨æ©ç æˆ–ä»…ä½¿ç”¨å› æœçš„æ¨¡å‹ã€‚</li>
<li>å…¬å¼€å‘å¸ƒäº†æ¨¡å‹ã€è®­ç»ƒè¯­æ–™åº“å’Œä»£ç ã€‚</li>
<li>è¿™ç§ç»“åˆæ–¹æ³•æœ‰åŠ©äºæé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.24159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2d9d917933ae86f9264d048f362d15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c882672540379f90944db9139ce567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db73435f61e9f4e937da2d7649a2bf69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-049c55801cac9d5ef86dee3a7176fe14.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d420af9d7c6e4b553c221ca8efb92b25.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Distributed Mixture-of-Agents for Edge Inference with Large Language   Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-27  Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">10329.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
