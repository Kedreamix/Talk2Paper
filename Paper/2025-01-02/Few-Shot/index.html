<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b01de80e41cf94f03899138149fddebb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning"><a href="#Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning" class="headerlink" title="Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning"></a>Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning</h2><p><strong>Authors:Zhifang Zhang, Shuo He, Bingquan Shen, Lei Feng</strong></p>
<p>Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, yet they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we disclose that CLIPâ€™s vulnerabilities primarily stem from its excessive encoding of class-irrelevant features, which can compromise the modelâ€™s visual feature resistivity to input perturbations, making it more susceptible to capturing the trigger patterns inserted by backdoor attacks. Inspired by this finding, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs specially designed deep visual prompt tuning and feature-repelling loss to eliminate excessive class-irrelevant features while simultaneously optimizing cross-entropy loss to maintain clean accuracy. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27% of the parameters relative to CLIP, yet it significantly outperforms state-of-the-art baselines, reducing the attack success rate from 67.53% to 2.76% against SoTA attacks and effectively generalizing its defensive capabilities across multiple datasets. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰å¯ä»¥ä»å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸­å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºï¼Œä½†å®ƒä»¬å¯¹åé—¨æ”»å‡»è¡¨ç°å‡ºæ˜¾è‘—è„†å¼±æ€§ï¼Œè¿™å¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŠ«éœ²CLIPçš„è„†å¼±æ€§ä¸»è¦æºäºå…¶å¯¹ä¸ç±»åˆ«æ— å…³ç‰¹å¾çš„è¿‡åº¦ç¼–ç ï¼Œè¿™å¯èƒ½ä¼šæŸå®³æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾ç”µé˜»ï¼Œä½¿å…¶æ›´å®¹æ˜“æ•è·ç”±åé—¨æ”»å‡»æ’å…¥çš„è§¦å‘æ¨¡å¼ã€‚å—è¿™ä¸€å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œæ’æ–¥è§†è§‰æç¤ºè°ƒæ•´â€ï¼ˆRVPTï¼‰çš„æ–°å‹é˜²å¾¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ä¸“é—¨è®¾è®¡çš„æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±ï¼Œä»¥æ¶ˆé™¤è¿‡å¤šçš„ä¸ç±»åˆ«æ— å…³çš„ç‰¹å¾ï¼ŒåŒæ—¶ä¼˜åŒ–äº¤å‰ç†µæŸå¤±ä»¥ä¿æŒæ¸…æ´å‡†ç¡®æ€§ã€‚ä¸é€šå¸¸éœ€è¦ä¸­æ¯’æ•°æ®æˆ–æ¶‰åŠå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ä¼ ç»Ÿå¤šæ¨¡æ€åé—¨é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒRVPTåˆ©ç”¨å°‘é‡ä¸‹æ¸¸æ¸…æ´æ ·æœ¬ï¼Œå¹¶ä¸”åªè°ƒæ•´å°‘é‡å‚æ•°ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒRVPTä»…è°ƒæ•´CLIPçš„0.27%å‚æ•°ï¼Œå´æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå°†æ”»å‡»æˆåŠŸç‡ä»67.53%é™ä½åˆ°2.76%ï¼Œæœ‰æ•ˆæŠµå¾¡äº†æœ€å…ˆè¿›çš„æ”»å‡»ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é˜²å¾¡èƒ½åŠ›çš„æœ‰æ•ˆæ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20392v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºCLIPç­‰å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹çš„å›¾åƒæ–‡æœ¬æ•°æ®é›†èƒ½å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºï¼Œä½†å…¶å­˜åœ¨æ˜“å—åé—¨æ”»å‡»å½±å“çš„è„†å¼±æ€§ï¼Œå¼•èµ·ä¸¥é‡çš„å®‰å…¨é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒCLIPçš„è„†å¼±æ€§ä¸»è¦æºäºå…¶å¯¹éç±»åˆ«ç‰¹å¾çš„è¿‡åº¦ç¼–ç ï¼Œå‰Šå¼±äº†æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾æŠ—æ€§ï¼Œæ˜“äºæ•è·åé—¨æ”»å‡»æ’å…¥çš„è§¦å‘æ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºé‡‡ç”¨æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±çš„é˜²å¾¡æ–¹æ³•â€”â€”RVPTã€‚RVPTåœ¨ä¸è¦æ±‚ä¸­æ¯’æ•°æ®å¯ç”¨æ€§æˆ–æ¶‰åŠæ•´ä¸ªæ¨¡å‹å¾®è°ƒçš„å‰æä¸‹ï¼Œé€šè¿‡ä¼˜åŒ–å°‘é‡å‚æ•°å®ç°äº†è‰¯å¥½çš„é˜²å¾¡æ•ˆæœã€‚å®éªŒç»“æœè¯æ˜ï¼ŒRVPTä»…è°ƒæ•´CLIPçš„0.27%å‚æ•°ï¼Œå³å¯æ˜¾è‘—è¶…è¶Šç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œæ”»å‡»æˆåŠŸç‡ä»åŸæ¥çš„67.53%é™ä½åˆ°ä»…å­˜çš„2.76%ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ‰æ•ˆçš„é˜²å¾¡èƒ½åŠ›æ¨å¹¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹å¦‚CLIPé¢ä¸´åé—¨æ”»å‡»çš„é£é™©ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>CLIPçš„è„†å¼±æ€§æºäºå…¶å¯¹éç±»åˆ«ç‰¹å¾çš„è¿‡åº¦ç¼–ç ï¼Œä½¿å…¶æ˜“å—è§¦å‘æ¨¡å¼æ’å…¥æ”»å‡»å½±å“ã€‚</li>
<li>æ–°æå‡ºçš„é˜²å¾¡æ–¹æ³•RVPTï¼Œæ—¨åœ¨æ¶ˆé™¤å¤šä½™çš„ç±»åˆ«æ— å…³ç‰¹å¾ã€‚é€šè¿‡æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±å®ç°ä¼˜åŒ–ã€‚</li>
<li>RVPTä»…è°ƒæ•´å°‘é‡å‚æ•°å³å¯å®ç°é«˜æ•ˆé˜²å¾¡ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒè¯æ˜å…¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šé˜²å¾¡æ•ˆæœå¹¿æ³›é€‚ç”¨ã€‚</li>
<li>RVPTé’ˆå¯¹åé—¨æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›å¼ºå¤§ï¼Œæ”»å‡»æˆåŠŸç‡æ˜¾è‘—é™ä½ã€‚</li>
<li>RVPTæ–¹æ³•å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆæŠµå¾¡æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-731c10ba9cb98a5f1c9571bf1e10724e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b660138eeac2701eb1a32cf69813426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96ea090cdcfae8508eace173ec375f29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bdecdca900ace72072661824a0df709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd4349bc7cf61837d6670e0b0525b7b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e2c0e1397e7179408b75735154a2d7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-shot-Algorithm-Assurance"><a href="#Few-shot-Algorithm-Assurance" class="headerlink" title="Few-shot Algorithm Assurance"></a>Few-shot Algorithm Assurance</h2><p><strong>Authors:Dang Nguyen, Sunil Gupta</strong></p>
<p>In image classification tasks, deep learning models are vulnerable to image distortion. For successful deployment, it is important to identify distortion levels under which the model is usable i.e. its accuracy stays above a stipulated threshold. We refer to this problem as Model Assurance under Image Distortion, and formulate it as a classification task. Given a distortion level, our goal is to predict if the modelâ€™s accuracy on the set of distorted images is greater than a threshold. We propose a novel classifier based on a Level Set Estimation (LSE) algorithm, which uses the LSEâ€™s mean and variance functions to form the classification rule. We further extend our method to a â€œfew sampleâ€ setting where we can only acquire few real images to perform the model assurance process. Our idea is to generate extra synthetic images using a novel Conditional Variational Autoencoder model with two new loss functions. We conduct extensive experiments to show that our classification method significantly outperforms strong baselines on five benchmark image datasets. </p>
<blockquote>
<p>åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å®¹æ˜“å—åˆ°å›¾åƒå¤±çœŸçš„å½±å“ã€‚ä¸ºäº†ç¡®ä¿æˆåŠŸéƒ¨ç½²ï¼Œé‡è¦çš„æ˜¯è¦ç¡®å®šæ¨¡å‹åœ¨ä½•ç§å¤±çœŸæ°´å¹³ä¸‹æ˜¯å¯ç”¨çš„ï¼Œå³å…¶å‡†ç¡®æ€§ä¿æŒåœ¨è§„å®šçš„é˜ˆå€¼ä¹‹ä¸Šã€‚æˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜ç§°ä¸ºå›¾åƒå¤±çœŸä¸‹çš„æ¨¡å‹ä¿è¯ï¼Œå¹¶å°†å…¶åˆ¶å®šä¸ºä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ã€‚ç»™å®šå¤±çœŸæ°´å¹³ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é¢„æµ‹æ¨¡å‹åœ¨å¤±çœŸå›¾åƒé›†ä¸Šçš„å‡†ç¡®æ€§æ˜¯å¦é«˜äºé˜ˆå€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ°´å¹³é›†ä¼°è®¡ï¼ˆLSEï¼‰ç®—æ³•çš„æ–°å‹åˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨ä½¿ç”¨LSEçš„å‡å€¼å’Œæ–¹å·®å‡½æ•°æ¥å½¢æˆåˆ†ç±»è§„åˆ™ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•è¿›ä¸€æ­¥æ‰©å±•åˆ°â€œå°‘é‡æ ·æœ¬â€è®¾ç½®ï¼Œå…¶ä¸­æˆ‘ä»¬åªèƒ½è·å–å°‘é‡çœŸå®å›¾åƒæ¥æ‰§è¡Œæ¨¡å‹ä¿è¯è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æƒ³æ³•æ˜¯åˆ©ç”¨æ–°å‹æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹ç”Ÿæˆé¢å¤–çš„åˆæˆå›¾åƒï¼Œè¯¥æ¨¡å‹å…·æœ‰ä¸¤ä¸ªæ–°çš„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„åˆ†ç±»æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†å›¾åƒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºæ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å¯¹å›¾åƒå¤±çœŸçš„è„†å¼±æ€§é—®é¢˜ï¼Œå¹¶å®šä¹‰æ­¤ä¸ºæ¨¡å‹åœ¨å¤±çœŸå›¾åƒä¸Šçš„å‡†ç¡®ç‡é«˜äºè§„å®šé˜ˆå€¼çš„é—®é¢˜ï¼Œç§°ä¸ºæ¨¡å‹ä¿è¯ä¸‹çš„å›¾åƒå¤±çœŸé—®é¢˜ã€‚æ–‡ç« å»ºè®®ä½¿ç”¨åŸºäºæ°´å¹³é›†ä¼°è®¡ï¼ˆLSEï¼‰ç®—æ³•çš„å…¨æ–°åˆ†ç±»å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚å½“é¢ä¸´çœŸå®å›¾åƒæ ·æœ¬æœ‰é™çš„æƒ…å†µæ—¶ï¼Œé€šè¿‡é‡‡ç”¨æ–°å‹çš„æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹åŠä¸¤ä¸ªæ–°æŸå¤±å‡½æ•°ç”Ÿæˆé¢å¤–åˆæˆå›¾åƒæ¥åº”å¯¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­é¢ä¸´å›¾åƒå¤±çœŸçš„è„†å¼±æ€§æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ¨¡å‹ä¿è¯ä¸‹çš„å›¾åƒå¤±çœŸé—®é¢˜ï¼Œå®šä¹‰ä¸ºæ¨¡å‹åœ¨å¤±çœŸå›¾åƒä¸Šçš„å‡†ç¡®ç‡é«˜äºæŸä¸€é˜ˆå€¼ã€‚</li>
<li>æå‡ºåŸºäºæ°´å¹³é›†ä¼°è®¡ï¼ˆLSEï¼‰ç®—æ³•çš„æ–°å‹åˆ†ç±»å™¨æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>LSEç®—æ³•åˆ©ç”¨å‡å€¼å’Œæ–¹å·®å‡½æ•°å½¢æˆåˆ†ç±»è§„åˆ™ã€‚</li>
<li>å½“çœŸå®å›¾åƒæ ·æœ¬æœ‰é™æ—¶ï¼Œé€šè¿‡æ–°å‹çš„æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹åŠä¸¤ä¸ªæ–°æŸå¤±å‡½æ•°ç”Ÿæˆåˆæˆå›¾åƒä»¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4402b28add3f9740cff0b72677a57892.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-948f51b51f427cc2e107e59c16d5cde7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8023517cf815e5310a2066c60b1bda0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Time-Series-Foundational-Models-Their-Role-in-Anomaly-Detection-and-Prediction"><a href="#Time-Series-Foundational-Models-Their-Role-in-Anomaly-Detection-and-Prediction" class="headerlink" title="Time Series Foundational Models: Their Role in Anomaly Detection and   Prediction"></a>Time Series Foundational Models: Their Role in Anomaly Detection and   Prediction</h2><p><strong>Authors:Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth</strong></p>
<p>Time series foundational models (TSFM) have gained prominence in time series forecasting, promising state-of-the-art performance across various applications. However, their application in anomaly detection and prediction remains underexplored, with growing concerns regarding their black-box nature, lack of interpretability and applicability. This paper critically evaluates the efficacy of TSFM in anomaly detection and prediction tasks. We systematically analyze TSFM across multiple datasets, including those characterized by the absence of discernible patterns, trends and seasonality. Our analysis shows that while TSFMs can be extended for anomaly detection and prediction, traditional statistical and deep learning models often match or outperform TSFM in these tasks. Additionally, TSFMs require high computational resources but fail to capture sequential dependencies effectively or improve performance in few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to reproduce the results and supplementary materials are available at <a target="_blank" rel="noopener" href="https://github.com/smtmnfg/TSFM">https://github.com/smtmnfg/TSFM</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸå·²ç»å´­éœ²å¤´è§’ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶åœ¨å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹æ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ï¼Œå¹¶ä¸”ç”±äºå®ƒä»¬é»‘ç®±æ€§è´¨ã€ç¼ºä¹å¯è§£é‡Šæ€§å’Œé€‚ç”¨æ€§çš„æ‹…å¿§æ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡å¯¹TSFMåœ¨å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†å¤šä¸ªæ•°æ®é›†ä¸Šçš„TSFMï¼ŒåŒ…æ‹¬é‚£äº›ä»¥ç¼ºä¹å¯è¾¨è¯†çš„æ¨¡å¼ã€è¶‹åŠ¿å’Œå­£èŠ‚æ€§ä¸ºç‰¹å¾çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶å¯ä»¥å°†TSFMæ‰©å±•åˆ°å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä½†ä¼ ç»Ÿçš„ç»Ÿè®¡æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­å¾€å¾€ä¸TSFMç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼ŒTSFMéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºï¼Œä½†åœ¨å°æ ·æœ¬æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸­æœªèƒ½æœ‰æ•ˆåœ°æ•æ‰åºåˆ—ä¾èµ–æ€§æˆ–æé«˜æ€§èƒ½ã€‚é¢„å¤„ç†è¿‡çš„æ•°æ®é›†ã€ç”¨äºé‡ç°ç»“æœçš„ä»£ç å’Œè¡¥å……ææ–™å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/smtmnfg/TSFM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/smtmnfg/TSFMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19286v1">PDF</a> 12 pages, 6 figures, 5 tables. Accepted at AAAI2025 Anomaly Detection   in Scientific Domains Workshop</p>
<p><strong>Summary</strong></p>
<p>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹æ–¹é¢çš„åº”ç”¨ä»ç„¶æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡å¯¹TSFMåœ¨å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°è™½ç„¶TSFMå¯ä»¥æ‰©å±•åˆ°å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä½†åœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿçš„ç»Ÿè®¡æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¾€å¾€ä¸TSFMç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼ŒTSFMéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºï¼Œä½†åœ¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸­æ— æ³•æœ‰æ•ˆæ•æ‰åºåˆ—ä¾èµ–æ€§æˆ–æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>TSFMåœ¨å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹æ–¹é¢çš„åº”ç”¨ä»ç„¶æ¢ç´¢ä¸è¶³ã€‚</li>
<li>TSFMå¯ä»¥æ‰©å±•åˆ°å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸­ã€‚</li>
<li>åœ¨å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿçš„ç»Ÿè®¡æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¾€å¾€ä¸TSFMç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚</li>
<li>TSFMéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</li>
<li>åœ¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸­ï¼ŒTSFMæ— æ³•æœ‰æ•ˆæ•æ‰åºåˆ—ä¾èµ–æ€§æˆ–æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-027eac48ab82e6bb20e6403fe3594a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-154237e835aed34809f1f39c2806389a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b01de80e41cf94f03899138149fddebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42804a18c405e484be138c91414487a3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLIP-GS-Unifying-Vision-Language-Representation-with-3D-Gaussian-Splatting"><a href="#CLIP-GS-Unifying-Vision-Language-Representation-with-3D-Gaussian-Splatting" class="headerlink" title="CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian   Splatting"></a>CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian   Splatting</h2><p><strong>Authors:Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei</strong></p>
<p>Recent works in 3D multimodal learning have made remarkable progress. However, typically 3D multimodal models are only capable of handling point clouds. Compared to the emerging 3D representation technique, 3D Gaussian Splatting (3DGS), the spatially sparse point cloud cannot depict the texture information of 3D objects, resulting in inferior reconstruction capabilities. This limitation constrains the potential of point cloud-based 3D multimodal representation learning. In this paper, we present CLIP-GS, a novel multimodal representation learning framework grounded in 3DGS. We introduce the GS Tokenizer to generate serialized gaussian tokens, which are then processed through transformer layers pre-initialized with weights from point cloud models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss between 3DGS and the visual-text embeddings of CLIP, and we introduce an image voting loss to guide the directionality and convergence of gradient optimization. Furthermore, we develop an efficient way to generate triplets of 3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal representations. Leveraging the well-aligned multimodal representations, CLIP-GS demonstrates versatility and outperforms point cloud-based models on various 3D tasks, including multimodal retrieval, zero-shot, and few-shot classification. </p>
<blockquote>
<p>è¿‘æœŸï¼Œåœ¨ä¸‰ç»´å¤šæ¨¡æ€å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå…¸å‹çš„ä¸‰ç»´å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸åªèƒ½å¤„ç†ç‚¹äº‘æ•°æ®ã€‚ä¸æ–°å…´çš„ä¸‰ç»´è¡¨ç¤ºæŠ€æœ¯ç›¸æ¯”ï¼Œä¸‰ç»´é«˜æ–¯å±•å¼€ï¼ˆ3DGSï¼‰åœ¨è¡¨è¾¾ä¸‰ç»´ç‰©ä½“çš„çº¹ç†ä¿¡æ¯æ–¹é¢ï¼Œç¨€ç–çš„ç‚¹äº‘æ— æ³•å¾ˆå¥½åœ°æè¿°çº¹ç†ä¿¡æ¯ï¼Œå¯¼è‡´é‡å»ºèƒ½åŠ›å—é™ã€‚è¿™ä¸€å±€é™æ€§åˆ¶çº¦äº†åŸºäºç‚¹äº‘çš„ä¸‰ç»´å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸‰ç»´é«˜æ–¯å±•å¼€ï¼ˆCLIP-GSï¼‰çš„æ–°å‹å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†GSåˆ†è¯å™¨ç”Ÿæˆåºåˆ—åŒ–é«˜æ–¯ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œç»è¿‡ä½¿ç”¨ç‚¹äº‘æ¨¡å‹çš„æƒé‡è¿›è¡Œé¢„åˆå§‹åŒ–çš„å˜å‹å™¨å±‚å¤„ç†ï¼Œç”Ÿæˆä¸‰ç»´é«˜æ–¯åµŒå…¥ã€‚CLIP-GSåˆ©ç”¨ä¸‰ç»´é«˜æ–¯åµŒå…¥ä¸CLIPçš„è§†è§‰æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å¯¹æ¯”æŸå¤±ï¼Œå¹¶å¼•å…¥äº†å›¾åƒæŠ•ç¥¨æŸå¤±æ¥æŒ‡å¯¼æ¢¯åº¦ä¼˜åŒ–çš„æ–¹å‘æ€§å’Œæ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥ç”Ÿæˆä¸‰ç»´é«˜æ–¯åµŒå…¥ã€å›¾åƒå’Œæ–‡æœ¬çš„ä¸‰å…ƒç»„ï¼Œä¿ƒè¿›äº†CLIP-GSåœ¨å­¦ä¹ ç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨ç¤ºæ—¶çš„åº”ç”¨ã€‚åˆ©ç”¨å¯¹é½è‰¯å¥½çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼ŒCLIP-GSè¡¨ç°å‡ºå¤šåŠŸèƒ½æ€§å¹¶åœ¨å¤šç§ä¸‰ç»´ä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºäºç‚¹äº‘çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ£€ç´¢ã€é›¶æ ·æœ¬å­¦ä¹ å’Œå°æ ·æœ¬æ–‡æœ¬åˆ†ç±»ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäº3DGSçš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶CLIP-GSçš„ç ”ç©¶ã€‚é’ˆå¯¹ç‚¹äº‘åœ¨æè¿°çº¹ç†ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†GS Tokenizerç”Ÿæˆé«˜æ–¯ä»¤ç‰Œï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ç‚¹äº‘æ¨¡å‹ä¸Transformerå±‚ç”Ÿæˆ3DGSåµŒå…¥ã€‚é€šè¿‡å¯¹æ¯”CLIPçš„è§†è§‰æ–‡æœ¬åµŒå…¥å’Œå›¾åƒæŠ•ç¥¨æŸå¤±å®ç°ä¼˜åŒ–ã€‚èƒ½å¤Ÿç”Ÿæˆç»Ÿä¸€çš„è·¨æ¨¡æ€ä¸‰å…ƒç»„è¡¨ç¤ºï¼Œæœ‰åŠ©äºåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°è¶…è¶Šç‚¹äº‘æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSæŠ€æœ¯ç›¸è¾ƒäºä¼ ç»Ÿçš„ç‚¹äº‘æŠ€æœ¯èƒ½æ›´å¥½åœ°æè¿°çº¹ç†ä¿¡æ¯ï¼Œæå‡é‡å»ºèƒ½åŠ›ã€‚</li>
<li>CLIP-GSæ˜¯ä¸€ä¸ªåŸºäº3DGSçš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‚¹äº‘å¤„ç†æ¨¡å‹çš„é—®é¢˜ã€‚</li>
<li>GS Tokenizerå¯ä»¥ç”Ÿæˆåºåˆ—åŒ–é«˜æ–¯ä»¤ç‰Œå¹¶å¤„ç†ä»¥å¾—åˆ°æ›´æœ‰æ•ˆçš„3DGSåµŒå…¥ã€‚</li>
<li>CLIP-GSä½¿ç”¨å¯¹æ¯”æŸå¤±å’Œå›¾åƒæŠ•ç¥¨æŸå¤±è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–æ–¹å‘æŒ‡å¯¼ã€‚</li>
<li>CLIP-GSèƒ½å¤Ÿç”Ÿæˆè·¨æ¨¡æ€çš„ä¸‰å…ƒç»„è¡¨ç¤ºï¼ŒåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬å’Œ3DGSæ•°æ®ã€‚</li>
<li>CLIP-GSåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°è¶…è¶Šç‚¹äº‘æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ£€ç´¢ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcbc138552113253a621cb50d857496d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d55fb43103e5b6f0ffd9fbf5a090d464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f564696445487fc2b998cee8e8e25454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003a21e2cc14126fa6e89a6254a0e537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ebab259c5516d6d9a6c0cc4040baad7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reconstruction-Target-Matters-in-Masked-Image-Modeling-for-Cross-Domain-Few-Shot-Learning"><a href="#Reconstruction-Target-Matters-in-Masked-Image-Modeling-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Reconstruction Target Matters in Masked Image Modeling for Cross-Domain   Few-Shot Learning"></a>Reconstruction Target Matters in Masked Image Modeling for Cross-Domain   Few-Shot Learning</h2><p><strong>Authors:Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer knowledge from the data-abundant source domain to data-scarce target domains for fast adaptation, where the large domain gap makes CDFSL a challenging problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data and learning imageâ€™s global structures, enhancing model generalization and robustness. However, in the CDFSL task with significant domain shifts, we find MAE even shows lower performance than the baseline supervised models. In this paper, we first delve into this phenomenon for an interpretation. We find that MAE tends to focus on low-level domain information during reconstructing pixels while changing the reconstruction target to token features could mitigate this problem. However, not all features are beneficial, as we then find reconstructing high-level features can hardly improve the modelâ€™s transferability, indicating a trade-off between filtering domain information and preserving the imageâ€™s global structure. In all, the reconstruction target matters for the CDFSL task. Based on the above findings and interpretations, we further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL task. DAMIM includes an Aggregated Feature Reconstruction module to automatically aggregate features for reconstruction, with balanced learning of domain-agnostic information and imagesâ€™ global structure, and a Lightweight Decoder module to further benefit the encoderâ€™s generalizability. Experiments on four CDFSL datasets demonstrate that our method achieves state-of-the-art performance. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰è¦æ±‚æ¨¡å‹ä»æ•°æ®ä¸°å¯Œçš„æºåŸŸè½¬ç§»çŸ¥è¯†åˆ°æ•°æ®ç¨€ç¼ºçš„ç›®æ ‡åŸŸï¼Œä»¥å®ç°å¿«é€Ÿé€‚åº”ï¼Œè€Œè¾ƒå¤§çš„åŸŸå·®è·ä½¿å¾—CDFSLæˆä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰æ“…é•¿æœ‰æ•ˆåœ°ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®å¹¶å­¦ä¹ å›¾åƒçš„å…¨å±€ç»“æ„ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰æ˜¾è‘—åŸŸè½¬ç§»ç‰¹æ€§çš„CDFSLä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç°MAEçš„æ€§èƒ½ç”šè‡³ä½äºåŸºçº¿ç›‘ç£æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ·±å…¥ç ”ç©¶è¿™ä¸€ç°è±¡è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å‘ç°MAEåœ¨é‡å»ºåƒç´ æ—¶å€¾å‘äºå…³æ³¨ä½çº§åˆ«çš„åŸŸä¿¡æ¯ï¼Œè€Œå°†é‡å»ºç›®æ ‡æ”¹ä¸ºä»¤ç‰Œç‰¹å¾å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰ç‰¹æ€§éƒ½æ˜¯æœ‰ç›Šçš„ï¼Œå› ä¸ºæˆ‘ä»¬è¿˜å‘ç°é‡å»ºé«˜çº§ç‰¹æ€§å‡ ä¹ä¸èƒ½æé«˜æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ï¼Œè¿™è¡¨æ˜åœ¨è¿‡æ»¤åŸŸä¿¡æ¯å’Œä¿ç•™å›¾åƒå…¨å±€ç»“æ„ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚æ€»ä¹‹ï¼Œé‡å»ºç›®æ ‡å¯¹äºCDFSLä»»åŠ¡å¾ˆé‡è¦ã€‚åŸºäºä¸Šè¿°å‘ç°å’Œè§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºCDFSLä»»åŠ¡æå‡ºäº†é¢†åŸŸæ— å…³æ©ç å›¾åƒå»ºæ¨¡ï¼ˆDAMIMï¼‰ã€‚DAMIMåŒ…æ‹¬ä¸€ä¸ªèšåˆç‰¹å¾é‡å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯è‡ªåŠ¨èšåˆç‰¹å¾è¿›è¡Œé‡å»ºï¼Œå¹³è¡¡é¢†åŸŸæ— å…³ä¿¡æ¯çš„å­¦ä¹ ä¸å›¾åƒå…¨å±€ç»“æ„çš„å¹³è¡¡å­¦ä¹ ï¼Œä»¥åŠä¸€ä¸ªè½»é‡çº§è§£ç å™¨æ¨¡å—ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›ç¼–ç å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å››ä¸ªCDFSLæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19101v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œå‘ç°Masked Autoencoderï¼ˆMAEï¼‰åœ¨å¤„ç†æ˜¾è‘—åŸŸè½¬ç§»çš„CDFSLä»»åŠ¡æ—¶æ€§èƒ½ä¸‹é™ã€‚é€šè¿‡å¯¹ç°è±¡çš„åˆ†æï¼Œæœ¬æ–‡å‘ç°MAEåœ¨é‡å»ºåƒç´ æ—¶è¿‡äºå…³æ³¨ä½çº§åŸŸä¿¡æ¯ï¼Œè€Œæ”¹å˜é‡å»ºç›®æ ‡ä¸ºä»¤ç‰Œç‰¹å¾å¯ä»¥ç¼“è§£æ­¤é—®é¢˜ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰ç‰¹å¾éƒ½æœ‰ç›Šï¼Œé‡å»ºé«˜çº§ç‰¹å¾å¯¹æ¨¡å‹å¯è¿ç§»æ€§çš„æå‡æœ‰é™ï¼Œè¿™æ˜¾ç¤ºå‡ºåœ¨è¿‡æ»¤åŸŸä¿¡æ¯å’Œä¿ç•™å›¾åƒå…¨å±€ç»“æ„ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†é¢å‘CDFSLä»»åŠ¡çš„Domain-Agnostic Masked Image Modelingï¼ˆDAMIMï¼‰ã€‚DAMIMåŒ…æ‹¬ä¸€ä¸ªèšåˆç‰¹å¾é‡å»ºæ¨¡å—ï¼Œç”¨äºè‡ªåŠ¨èšåˆç‰¹å¾è¿›è¡Œé‡å»ºï¼Œå¹¶å¹³è¡¡å­¦ä¹ é¢†åŸŸæ— å…³ä¿¡æ¯å’Œå›¾åƒå…¨å±€ç»“æ„çš„å­¦ä¹ ï¼›è¿˜åŒ…æ‹¬ä¸€ä¸ªè½»é‡çº§è§£ç å™¨æ¨¡å—ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›ç¼–ç å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªCDFSLæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰é¢ä¸´ä»æ•°æ®ä¸°å¯Œçš„æºåŸŸè½¬ç§»åˆ°æ•°æ®ç¨€ç¼ºçš„ç›®æ ‡åŸŸçš„æŒ‘æˆ˜ï¼Œå…¶ä¸­æ˜¾è‘—çš„åŸŸå·®è·ä½¿å¾—ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>Masked Autoencoderï¼ˆMAEï¼‰åœ¨å¤„ç†CDFSLä»»åŠ¡æ—¶æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨æ˜¾è‘—åŸŸè½¬ç§»çš„æƒ…å†µä¸‹ã€‚</li>
<li>MAEåœ¨é‡å»ºåƒç´ æ—¶è¿‡äºå…³æ³¨ä½çº§åˆ«åŸŸä¿¡æ¯ï¼Œæ”¹å˜é‡å»ºç›®æ ‡è‡³ä»¤ç‰Œç‰¹å¾èƒ½å¤Ÿç¼“è§£è¯¥é—®é¢˜ã€‚</li>
<li>åœ¨è¿‡æ»¤åŸŸä¿¡æ¯å’Œä¿ç•™å›¾åƒå…¨å±€ç»“æ„ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé«˜çº§ç‰¹å¾çš„é‡å»ºå¯¹æ¨¡å‹å¯è¿ç§»æ€§çš„æå‡æœ‰é™ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†Domain-Agnostic Masked Image Modelingï¼ˆDAMIMï¼‰æ–¹æ³•ï¼Œç”¨äºCDFSLä»»åŠ¡ã€‚</li>
<li>DAMIMåŒ…æ‹¬ä¸€ä¸ªèšåˆç‰¹å¾é‡å»ºæ¨¡å—ï¼Œèƒ½è‡ªåŠ¨èšåˆç‰¹å¾å¹¶å¹³è¡¡å­¦ä¹ é¢†åŸŸæ— å…³ä¿¡æ¯å’Œå›¾åƒå…¨å±€ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1d2500a6aed9a0acf51d3e27772555c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1889972c98a31bbbc717052202aa1307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e1b20b71e864ef56cbf6f7ede65d8d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867e157b0c9c467034a7e2919e299677.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cbc2ef192b8519c41654a1a0127ec25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60a33590efd256ace0d2dba1710a9708.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-shot-Metric-Domain-Adaptation-Practical-Learning-Strategies-for-an-Automated-Plant-Disease-Diagnosis"><a href="#Few-shot-Metric-Domain-Adaptation-Practical-Learning-Strategies-for-an-Automated-Plant-Disease-Diagnosis" class="headerlink" title="Few-shot Metric Domain Adaptation: Practical Learning Strategies for an   Automated Plant Disease Diagnosis"></a>Few-shot Metric Domain Adaptation: Practical Learning Strategies for an   Automated Plant Disease Diagnosis</h2><p><strong>Authors:Shoma Kudo, Satoshi Kagiwada, Hitoshi Iyatomi</strong></p>
<p>Numerous studies have explored image-based automated systems for plant disease diagnosis, demonstrating impressive diagnostic capabilities. However, recent large-scale analyses have revealed a critical limitation: that the diagnostic capability suffers significantly when validated on images captured in environments (domains) differing from those used during training. This shortfall stems from the inherently limited dataset size and the diverse manifestation of disease symptoms, combined with substantial variations in cultivation environments and imaging conditions, such as equipment and composition. These factors lead to insufficient variety in training data, ultimately constraining the systemâ€™s robustness and generalization. To address these challenges, we propose Few-shot Metric Domain Adaptation (FMDA), a flexible and effective approach for enhancing diagnostic accuracy in practical systems, even when only limited target data is available. FMDA reduces domain discrepancies by introducing a constraint to the diagnostic model that minimizes the â€œdistanceâ€ between feature spaces of source (training) data and target data with limited samples. FMDA is computationally efficient, requiring only basic feature distance calculations and backpropagation, and can be seamlessly integrated into any machine learning (ML) pipeline. In large-scale experiments, involving 223,015 leaf images across 20 fields and 3 crop species, FMDA achieved F1 score improvements of 11.1 to 29.3 points compared to cases without target data, using only 10 images per disease from the target domain. Moreover, FMDA consistently outperformed fine-tuning methods utilizing the same data, with an average improvement of 8.5 points. </p>
<blockquote>
<p>è®¸å¤šç ”ç©¶å·²ç»æ¢è®¨äº†åŸºäºå›¾åƒçš„æ¤ç‰©ç–¾ç—…è¯Šæ–­è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œå¹¶å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¯Šæ–­èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å¤§è§„æ¨¡åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„å±€é™æ€§ï¼šå½“åœ¨ä¸åŒäºè®­ç»ƒé˜¶æ®µä½¿ç”¨çš„ç¯å¢ƒï¼ˆé¢†åŸŸï¼‰ä¸­æ•è·çš„å›¾åƒä¸Šè¿›è¡ŒéªŒè¯æ—¶ï¼Œè¯Šæ–­èƒ½åŠ›ä¼šå—åˆ°æ˜¾è‘—å½±å“ã€‚è¿™ä¸€ç¼ºé™·æºäºæ•°æ®é›†å¤§å°æœ¬èº«çš„é™åˆ¶ã€ç–¾ç—…ç—‡çŠ¶è¡¨ç°çš„å¤šæ ·æ€§ï¼Œä»¥åŠæ ½åŸ¹ç¯å¢ƒã€æˆåƒæ¡ä»¶ï¼ˆå¦‚è®¾å¤‡å’Œç»„æˆï¼‰çš„æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å› ç´ å¯¼è‡´è®­ç»ƒæ•°æ®ç¼ºä¹è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼Œæœ€ç»ˆé™åˆ¶ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† Few-shot Metric Domain Adaptationï¼ˆFMDAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§çµæ´»æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå³ä½¿åœ¨åªæœ‰æœ‰é™çš„ç›®æ ‡æ•°æ®å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥æé«˜å®é™…ç³»ç»Ÿä¸­çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚FMDA é€šè¿‡åœ¨è¯Šæ–­æ¨¡å‹ä¸­å¼•å…¥ä¸€ä¸ªçº¦æŸæ¥å‡å°‘é¢†åŸŸå·®å¼‚ï¼Œè¯¥çº¦æŸæœ€å°åŒ–äº†æºï¼ˆè®­ç»ƒï¼‰æ•°æ®ç›®æ ‡æ•°æ®ç‰¹å¾ç©ºé—´ä¹‹é—´çš„â€œè·ç¦»â€ï¼Œè€Œç›®æ ‡æ•°æ®ä»…åŒ…å«æœ‰é™æ ·æœ¬ã€‚FMDA è®¡ç®—æ•ˆç‡é«˜ï¼Œåªéœ€è¿›è¡ŒåŸºæœ¬ç‰¹å¾è·ç¦»è®¡ç®—å’Œåå‘ä¼ æ’­ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®¡é“ä¸­ã€‚åœ¨å¤§è§„æ¨¡å®éªŒä¸­ï¼Œæ¶‰åŠ 20 ä¸ªé¢†åŸŸå’Œ 3 ä¸ªä½œç‰©ç§ç±»çš„ 223,015 å¼ å¶ç‰‡å›¾åƒï¼ŒFMDA åœ¨ä»…ä½¿ç”¨ç›®æ ‡åŸŸæ¯ç–¾ç—… 10 å¼ å›¾åƒçš„æƒ…å†µä¸‹ï¼Œä¸æ— ç›®æ ‡æ•°æ®çš„æƒ…å†µç›¸æ¯”ï¼ŒF1 åˆ†æ•°çš„æ”¹è¿›èŒƒå›´ä¸º 11.1 è‡³ 29.3 ç‚¹ã€‚æ­¤å¤–ï¼ŒFMDA å§‹ç»ˆä¼˜äºä½¿ç”¨ç›¸åŒæ•°æ®çš„å¾®è°ƒæ–¹æ³•ï¼Œå¹³å‡æé«˜äº† 8.5 ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18859v1">PDF</a> 8 pages, 4 figures, 3 tables. Accepted at 4th Annual AAAI Workshop on   AI to Accelerate Science and Engineering (AI2ASE)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå›¾åƒçš„æ¤ç‰©ç–¾ç—…è¯Šæ–­è‡ªåŠ¨åŒ–ç³»ç»Ÿåœ¨ç¯å¢ƒå·®å¼‚ä¸‹çš„è¯Šæ–­èƒ½åŠ›å—é™é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Few-shot Metric Domain Adaptationï¼ˆFMDAï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æºï¼ˆè®­ç»ƒï¼‰æ•°æ®ç‰¹å¾ç©ºé—´ä¸ç›®æ ‡æ•°æ®ç‰¹å¾ç©ºé—´çš„â€œè·ç¦»â€ï¼Œæé«˜è¯Šæ–­æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚FMDAæ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ï¼Œå¯æ— ç¼èå…¥ä»»ä½•æœºå™¨å­¦ä¹ ç®¡é“ã€‚åœ¨å¤§å‹å®éªŒä¸­ï¼ŒFMDAä½¿ç”¨ä»…æ¥è‡ªç›®æ ‡åŸŸçš„å°‘é‡å›¾åƒï¼ˆæ¯ç–¾ç—…ä»…10å¼ ï¼‰å®ç°äº†ç›¸è¾ƒäºæ— ç›®æ ‡æ•°æ®æƒ…å†µçš„F1åˆ†æ•°æé«˜11.1è‡³29.3ç‚¹ï¼Œä¸”ä¼˜äºä½¿ç”¨ç›¸åŒæ•°æ®çš„å¾®è°ƒæ–¹æ³•ï¼Œå¹³å‡æé«˜8.5ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè‡ªåŠ¨åŒ–æ¤ç‰©ç–¾ç—…è¯Šæ–­ç³»ç»Ÿåœ¨ç¯å¢ƒå·®å¼‚ä¸‹å­˜åœ¨è¯Šæ–­èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚</li>
<li>é—®é¢˜çš„æ ¹æºåœ¨äºæ•°æ®é›†å¤§å°æœ‰é™ã€ç–¾ç—…ç—‡çŠ¶è¡¨ç°å¤šæ ·ä»¥åŠæ ½åŸ¹ç¯å¢ƒå’Œæˆåƒæ¡ä»¶çš„å˜åŒ–ã€‚</li>
<li>å¼•å…¥Few-shot Metric Domain Adaptationï¼ˆFMDAï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æºï¼ˆè®­ç»ƒï¼‰æ•°æ®ç‰¹å¾ç©ºé—´ä¸ç›®æ ‡æ•°æ®ç‰¹å¾ç©ºé—´çš„â€œè·ç¦»â€ï¼Œæé«˜è¯Šæ–­æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>FMDAæ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ï¼Œå¯èå…¥ä»»ä½•æœºå™¨å­¦ä¹ ç®¡é“ã€‚</li>
<li>åœ¨å¤§å‹å®éªŒä¸­ï¼ŒFMDAåœ¨ä»…ä½¿ç”¨ç›®æ ‡åŸŸçš„å°‘é‡å›¾åƒæƒ…å†µä¸‹ï¼Œå®ç°äº†æ˜¾è‘—çš„è¯Šæ–­å‡†ç¡®æ€§æé«˜ã€‚</li>
<li>FMDAç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨ç¼ºä¹å¤§é‡ç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f059d1fe7da7799819ef769f9e75ad9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d33cb090f6168ffe840dd16fe03171.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e738260b7c63d7918cb4ce5447db45eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5686fa923cb3e7afc57292af2ed0432a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ac8b0c09505b9520421bdd1e2d0e512.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-241355c945529d05cd24e2a5931a8ddb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation"><a href="#AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation" class="headerlink" title="AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation"></a>AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation</h2><p><strong>Authors:Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li</strong></p>
<p>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet">https://github.com/jarch-ma/AFANet</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ æ—¨åœ¨é€šè¿‡ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥è¯†åˆ«æ–°æ¦‚å¿µã€‚ç„¶è€Œï¼Œå¯¹äºè§†è§‰å¯†é›†å‹ä»»åŠ¡ï¼Œå¦‚å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼Œåƒç´ çº§æ³¨é‡Šæ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œæœ¬æ–‡åˆ©ç”¨æ›´å…·æŒ‘æˆ˜æ€§çš„å›¾åƒçº§æ³¨é‡Šï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰ç”¨äºå¼±ç›‘ç£å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼ˆWFSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†è·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œå®ƒå°†RGBå›¾åƒåˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶é€šè¿‡é‡æ–°å¯¹é½è¿›ä¸€æ­¥ä¼˜åŒ–è¯­ä¹‰ç»“æ„ä¿¡æ¯ã€‚ä¸å¤§å¤šæ•°ç°æœ‰WFSSæ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä»¥ç¦»çº¿å­¦ä¹ æ–¹å¼ä½¿ç”¨å¤šæ¨¡æ€è¯­è¨€è§†è§‰æ¨¡å‹çš„æ–‡æœ¬ä¿¡æ¯ï¼ˆä¾‹å¦‚CLIPï¼‰ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†CLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡åœ¨çº¿å­¦ä¹ å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ï¼Œä»è€Œä¸ºCFMæä¾›ä¸°å¯Œçš„è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAFANetå·²è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jarch-ma/AFANetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17601v2">PDF</a> Accepted by TMM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰çš„å¼±ç›‘ç£å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆWFSSï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒçº§æ ‡æ³¨ï¼Œé€šè¿‡è·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰è§£è€¦å›¾åƒçš„é«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶é€šè¿‡CLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰è¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼Œå®ç°ç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ï¼Œæé«˜äº†è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAFANetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­¦ä¹ æ—¨åœ¨é€šè¿‡ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥è¯†åˆ«æ–°æ¦‚å¿µã€‚</li>
<li>é’ˆå¯¹è§†è§‰å¯†é›†å‹ä»»åŠ¡å¦‚å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼Œåƒç´ çº§æ ‡æ³¨æ˜¯è€—æ—¶ä¸”æ˜‚è´µçš„ã€‚</li>
<li>æœ¬æ–‡ä½¿ç”¨æ›´å…·æŒ‘æˆ˜æ€§çš„å›¾åƒçº§æ ‡æ³¨ï¼Œå¹¶æå‡ºè‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰è¿›è¡Œå¼±ç›‘ç£å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆWFSSï¼‰ã€‚</li>
<li>AFANetåŒ…æ‹¬è·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œå°†å›¾åƒè§£è€¦ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶é€šè¿‡å¯¹é½è¯­ä¹‰ç»“æ„ä¿¡æ¯æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</li>
<li>ä¸å¤§å¤šæ•°ä½¿ç”¨å¤šæ¨¡æ€è¯­è¨€è§†è§‰æ¨¡å‹çš„ç¦»çº¿å­¦ä¹ æ–¹å¼ä¸åŒï¼Œæœ¬æ–‡æå‡ºäº†CLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œé€šè¿‡åœ¨çº¿å­¦ä¹ å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAFANetåœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e3c275123d7cc725bbdac793e2b1c4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92b12f4ef3edec5343b33d63af5f2e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3c89546934a25924e4a0910914e013.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-912009cb34665b008f15f63397f9a37e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the modelâ€™s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>ç°æœ‰çš„å°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ¨¡å‹æ— æ³•è§£å†³åŒ»å­¦æˆåƒä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼šç”±ä¸åŒæˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸåç§»ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å½“å‰FSMISä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè·¨åŸŸå°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ–°çš„ç›®æ ‡åŸŸä¸­æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ã€‚æˆ‘ä»¬å—åˆ°ä¸åŒé¢†åŸŸé¢‘ç‡åŸŸç›¸ä¼¼æ€§ç‰¹å¾çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç”±äºå™¨å®˜å’Œç—…å˜çš„ä¸åŒå¤–è§‚å¼•èµ·çš„åŸŸå†…æ–¹å·®å¯¼è‡´çš„å›ºæœ‰æ”¯æŒæŸ¥è¯¢åè§ï¼›ä»¥åŠ2ï¼‰ç”±äºä¸åŒçš„åŒ»å­¦æˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸé—´æ–¹å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªMSFæ¨¡å—ï¼Œä»¥æ•´åˆFAMæ¨¡å—è§£è€¦çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥å‡è½»åŸŸé—´æ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚ç»“åˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬çš„FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v4">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ–°ç›®æ ‡åŸŸæ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæå‡ºé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…æ‹¬é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚è¯¥ç½‘ç»œè§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„åŸŸå†…å’ŒåŸŸé—´æ–¹å·®é—®é¢˜ï¼Œå¹¶åœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ¨¡å‹é¢ä¸´å› ä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„åŸŸåç§»é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºè·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½é€‚åº”æ›´å¹¿æ³›åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯çš„é€šç”¨æ¨¡å‹ï¼Œåœ¨æœ‰é™çš„æ–°ç›®æ ‡åŸŸæ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…å«é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä»¥è§£å†³å…ƒå­¦ä¹ é˜¶æ®µçš„åŸŸå†…å’ŒåŸŸé—´æ–¹å·®é—®é¢˜ã€‚</li>
<li>FAMæ¨¡å—è§£å†³äº†å› ä¸åŒå™¨å®˜å’Œç—…å˜çš„å¤–è§‚é€ æˆçš„åŸŸå†…æ–¹å·®ä»¥åŠå› ä¸åŒåŒ»ç–—æˆåƒæŠ€æœ¯é€ æˆçš„åŸŸé—´æ–¹å·®é—®é¢˜ã€‚</li>
<li>MSFæ¨¡å—ç”¨äºæ•´åˆç”±FAMæ¨¡å—è§£è€¦çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å‡è½»åŸŸé—´æ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚</li>
<li>FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa98f3e63dd1dfa34a0fe8c49c6c6931.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0796f64fbec939deef49b781663f0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def190b9c5343be34695a04abbf24490.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Context-aware-Inductive-Knowledge-Graph-Completion-with-Latent-Type-Constraints-and-Subgraph-Reasoning"><a href="#Context-aware-Inductive-Knowledge-Graph-Completion-with-Latent-Type-Constraints-and-Subgraph-Reasoning" class="headerlink" title="Context-aware Inductive Knowledge Graph Completion with Latent Type   Constraints and Subgraph Reasoning"></a>Context-aware Inductive Knowledge Graph Completion with Latent Type   Constraints and Subgraph Reasoning</h2><p><strong>Authors:Muzhi Li, Cehao Yang, Chengjin Xu, Zixing Song, Xuhui Jiang, Jian Guo, Ho-fung Leung, Irwin King</strong></p>
<p>Inductive knowledge graph completion (KGC) aims to predict missing triples with unseen entities. Recent works focus on modeling reasoning paths between the head and tail entity as direct supporting evidence. However, these methods depend heavily on the existence and quality of reasoning paths, which limits their general applicability in different scenarios. In addition, we observe that latent type constraints and neighboring facts inherent in KGs are also vital in inferring missing triples. To effectively utilize all useful information in KGs, we introduce CATS, a novel context-aware inductive KGC solution. With sufficient guidance from proper prompts and supervised fine-tuning, CATS activates the strong semantic understanding and reasoning capabilities of large language models to assess the existence of query triples, which consist of two modules. First, the type-aware reasoning module evaluates whether the candidate entity matches the latent entity type as required by the query relation. Then, the subgraph reasoning module selects relevant reasoning paths and neighboring facts, and evaluates their correlation to the query triple. Experiment results on three widely used datasets demonstrate that CATS significantly outperforms state-of-the-art methods in 16 out of 18 transductive, inductive, and few-shot settings with an average absolute MRR improvement of 7.2%. </p>
<blockquote>
<p>å½’çº³å¼çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨é¢„æµ‹ç¼ºå¤±çš„ä¸‰å…ƒç»„ä¸æœªè§è¿‡çš„å®ä½“ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œé›†ä¸­åœ¨å°†å¤´å®ä½“å’Œå°¾å®ä½“ä¹‹é—´çš„æ¨ç†è·¯å¾„å»ºæ¨¡ä¸ºç›´æ¥æ”¯æŒè¯æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ¨ç†è·¯å¾„çš„å­˜åœ¨å’Œå“è´¨ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°çŸ¥è¯†å›¾è°±ä¸­éšå«çš„ç±»å‹çº¦æŸå’Œé‚»è¿‘äº‹å®åœ¨æ¨æ–­ç¼ºå¤±ä¸‰å…ƒç»„ä¸­ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰æœ‰ç”¨ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†CATSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥å½’çº³KGCè§£å†³æ–¹æ¡ˆã€‚åœ¨é€‚å½“çš„æç¤ºå’Œç»è¿‡ç›‘ç£ç²¾ç»†è°ƒæ•´çš„å……è¶³æŒ‡å¯¼ä¸‹ï¼ŒCATSæ¿€æ´»äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥è¯„ä¼°æŸ¥è¯¢ä¸‰å…ƒç»„çš„å­˜åœ¨ä¸å¦ï¼Œå…¶ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆã€‚é¦–å…ˆï¼Œç±»å‹æ„ŸçŸ¥æ¨ç†æ¨¡å—è¯„ä¼°å€™é€‰å®ä½“æ˜¯å¦ç¬¦åˆæŸ¥è¯¢å…³ç³»æ‰€éœ€çš„æ½œåœ¨å®ä½“ç±»å‹ã€‚ç„¶åï¼Œå­å›¾æ¨ç†æ¨¡å—é€‰æ‹©ç›¸å…³çš„æ¨ç†è·¯å¾„å’Œé‚»è¿‘äº‹å®ï¼Œå¹¶è¯„ä¼°å®ƒä»¬ä¸æŸ¥è¯¢ä¸‰å…ƒç»„çš„å…³è”åº¦ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCATSåœ¨18ç§è½¬æ¢ã€å½’çº³å’Œå°æ ·æœ¬è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹³å‡ç»å¯¹MRRå€¼æé«˜äº†7.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16803v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå½’çº³çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰çš„ç›®æ ‡æ˜¯å¯¹ç¼ºå¤±çš„ä¸‰å…ƒç»„è¿›è¡Œé¢„æµ‹ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹æœªè§è¿‡çš„å®ä½“ã€‚å½“å‰æ–¹æ³•ä¸»è¦èšç„¦äºå¯¹å¤´å®ä½“å’Œå°¾å®ä½“ä¹‹é—´çš„æ¨ç†è·¯å¾„è¿›è¡Œå»ºæ¨¡ï¼Œä½œä¸ºç›´æ¥çš„æ”¯æŒè¯æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ¨ç†è·¯å¾„çš„å­˜åœ¨å’Œè´¨é‡ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼ŒçŸ¥è¯†å›¾è°±ä¸­éšæ€§çš„ç±»å‹çº¦æŸå’Œé‚»è¿‘äº‹å®å¯¹äºæ¨æ–­ç¼ºå¤±ä¸‰å…ƒç»„ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰æœ‰ç”¨ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†CATSè¿™ä¸€æ–°å‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥å½’çº³KGCè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡é€‚å½“çš„æç¤ºå’Œç²¾ç»†ç›‘ç£å¾®è°ƒï¼ŒCATSèƒ½å¤Ÿæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥è¯„ä¼°æŸ¥è¯¢ä¸‰å…ƒç»„çš„å­˜åœ¨æ€§ã€‚è¯¥æ–¹æ¡ˆåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šç±»å‹æ„ŸçŸ¥æ¨ç†æ¨¡å—è¯„ä¼°å€™é€‰å®ä½“æ˜¯å¦ç¬¦åˆæŸ¥è¯¢å…³ç³»æ‰€éœ€çš„æ½œåœ¨å®ä½“ç±»å‹ï¼›å­å›¾æ¨ç†æ¨¡å—é€‰æ‹©ç›¸å…³çš„æ¨ç†è·¯å¾„å’Œé‚»è¿‘äº‹å®ï¼Œå¹¶è¯„ä¼°å…¶ä¸æŸ¥è¯¢ä¸‰å…ƒç»„çš„å…³è”åº¦ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCATSåœ¨è½¬å¯¼æ€§ã€å½’çº³æ€§å’Œå°‘æ ·æœ¬è®¾ç½®çš„16ç§åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹³å‡ç»å¯¹MRRå€¼æé«˜äº†7.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½’çº³çŸ¥è¯†å›¾è°±è¡¥å…¨æ—¨åœ¨é¢„æµ‹ç¼ºå¤±çš„ä¸‰å…ƒç»„ï¼Œå°¤å…¶æ˜¯æ¶‰åŠæœªè§å®ä½“çš„æƒ…å½¢ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–æ¨ç†è·¯å¾„è¿›è¡Œå»ºæ¨¡ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚</li>
<li>éšæ€§ç±»å‹çº¦æŸå’Œé‚»è¿‘äº‹å®åœ¨çŸ¥è¯†å›¾è°±ä¸­åŒæ ·é‡è¦ï¼Œå¯¹äºæ¨æ–­ç¼ºå¤±ä¸‰å…ƒç»„è‡³å…³é‡è¦ã€‚</li>
<li>CATSæ˜¯ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥å½’çº³KGCè§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰ä¿¡æ¯ã€‚</li>
<li>CATSåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šç±»å‹æ„ŸçŸ¥æ¨ç†æ¨¡å—å’Œå­å›¾æ¨ç†æ¨¡å—ã€‚</li>
<li>CATSé€šè¿‡é€‚å½“çš„æç¤ºå’Œç²¾ç»†ç›‘ç£å¾®è°ƒï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-945d84a6fc8a21118bc24438c8c5ce1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7dc9fdbe0bba78e1d9fd0c66a58f746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d3efa9d673e71717571bd345e914873.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9eea04177ab1b66fed7452ee8fc1b9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca287de3fc61a9c15db73dfe1c40683e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e077bfaa1a5970deb41fa4194d98bd67.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Federated-Learning-with-MMD-based-Early-Stopping-for-Adaptive-GNSS-Interference-Classification"><a href="#Federated-Learning-with-MMD-based-Early-Stopping-for-Adaptive-GNSS-Interference-Classification" class="headerlink" title="Federated Learning with MMD-based Early Stopping for Adaptive GNSS   Interference Classification"></a>Federated Learning with MMD-based Early Stopping for Adaptive GNSS   Interference Classification</h2><p><strong>Authors:Nishant S. Gaikwad, Lucas Heublein, Nisha L. Raichur, Tobias Feigl, Christopher Mutschler, Felix Ott</strong></p>
<p>Federated learning (FL) enables multiple devices to collaboratively train a global model while maintaining data on local servers. Each device trains the model on its local server and shares only the model updates (i.e., gradient weights) during the aggregation step. A significant challenge in FL is managing the feature distribution of novel and unbalanced data across devices. In this paper, we propose an FL approach using few-shot learning and aggregation of the model weights on a global server. We introduce a dynamic early stopping method to balance out-of-distribution classes based on representation learning, specifically utilizing the maximum mean discrepancy of feature embeddings between local and global models. An exemplary application of FL is to orchestrate machine learning models along highways for interference classification based on snapshots from global navigation satellite system (GNSS) receivers. Extensive experiments on four GNSS datasets from two real-world highways and controlled environments demonstrate that our FL method surpasses state-of-the-art techniques in adapting to both novel interference classes and multipath scenarios. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å…è®¸å¤šä¸ªè®¾å¤‡ååŒè®­ç»ƒå…¨å±€æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæ•°æ®å­˜å‚¨åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šã€‚æ¯ä¸ªè®¾å¤‡éƒ½åœ¨å…¶æœ¬åœ°æœåŠ¡å™¨ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä»…åœ¨èšåˆæ­¥éª¤ä¸­å…±äº«æ¨¡å‹æ›´æ–°ï¼ˆå³æ¢¯åº¦æƒé‡ï¼‰ã€‚è”é‚¦å­¦ä¹ é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ç®¡ç†è·¨è®¾å¤‡çš„æ–°é¢–ä¸”ä¸å¹³è¡¡æ•°æ®çš„ç‰¹å¾åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ å’Œå…¨å±€æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹æƒé‡èšåˆçš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ—©æœŸåœæ­¢æ–¹æ³•æ¥å¹³è¡¡è¶…å‡ºåˆ†å¸ƒç±»åˆ«çš„è¡¨ç¤ºå­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åˆ©ç”¨æœ¬åœ°å’Œå…¨å±€æ¨¡å‹ä¹‹é—´çš„ç‰¹å¾åµŒå…¥çš„æœ€å¤§å¹³å‡å·®å¼‚å€¼æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è”é‚¦å­¦ä¹ çš„ä¸€ä¸ªå…¸å‹åº”ç”¨æ˜¯åœ¨é«˜é€Ÿå…¬è·¯ä¸ŠååŒç»„ç»‡æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ŒåŸºäºå…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰æ¥æ”¶å™¨çš„å¿«ç…§è¿›è¡Œå¹²æ‰°åˆ†ç±»ã€‚åœ¨ä¸¤æ¡å®é™…é«˜é€Ÿå…¬è·¯å’Œæ§åˆ¶ç¯å¢ƒä¸­çš„å››ä¸ªGNSSæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨é€‚åº”æ–°é¢–å¹²æ‰°ç±»åˆ«å’Œå¤šè·¯å¾„åœºæ™¯æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ‰‹æ®µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15681v2">PDF</a> Git repository:   <a target="_blank" rel="noopener" href="https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/federated_learning">https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/federated_learning</a></p>
<p><strong>Summary</strong></p>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å…è®¸å¤šä¸ªè®¾å¤‡åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šåä½œè®­ç»ƒå…¨å±€æ¨¡å‹çš„åŒæ—¶ä¿æŒæœ¬åœ°æ•°æ®çš„å®‰å…¨ã€‚æ¨¡å‹åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šè®­ç»ƒï¼Œä»…åœ¨èšåˆé˜¶æ®µå…±äº«æ¨¡å‹æ›´æ–°ï¼ˆå³æ¢¯åº¦æƒé‡ï¼‰ã€‚åœ¨FLä¸­ï¼Œç®¡ç†æ–°å‹å’Œä¸å¹³è¡¡æ•°æ®åœ¨è®¾å¤‡é—´çš„ç‰¹å¾åˆ†å¸ƒæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè”é‚¦å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æ¨¡å‹æƒé‡èšåˆæ–¹æ³•ï¼Œå¼•å…¥äº†ä¸€ç§åŠ¨æ€æ—©æœŸåœæ­¢æ–¹æ³•ä»¥å¹³è¡¡å¼‚å¸¸ç±»ç‰¹å¾åˆ†å¸ƒã€‚å…·ä½“é‡‡ç”¨ä»£è¡¨å­¦ä¹ ï¼ŒåŸºäºå±€éƒ¨å’Œå…¨å±€æ¨¡å‹ç‰¹å¾åµŒå…¥çš„æœ€å¤§å‡å€¼å·®å¼‚æ¥åŒºåˆ†ã€‚æ­¤å¤–ï¼Œä»¥å…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰æ¥æ”¶æœºçš„é«˜é€Ÿå…¬è·¯å¹²æ‰°åˆ†ç±»ä¸ºä¾‹å±•ç¤ºäº†è”é‚¦å­¦ä¹ çš„åº”ç”¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨é€‚åº”æ–°å‹å¹²æ‰°ç±»å’Œå¤šå¾„åœºæ™¯æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è”é‚¦å­¦ä¹ å…è®¸è®¾å¤‡åœ¨æœ¬åœ°è®­ç»ƒæ¨¡å‹å¹¶å…±äº«æ¨¡å‹æ›´æ–°ï¼Œä»¥ä¿æŠ¤æœ¬åœ°æ•°æ®çš„å®‰å…¨ã€‚</li>
<li>ç®¡ç†æ–°å‹å’Œä¸å¹³è¡¡æ•°æ®åœ¨è®¾å¤‡é—´çš„ç‰¹å¾åˆ†å¸ƒæ˜¯è”é‚¦å­¦ä¹ çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè”é‚¦å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æ¨¡å‹æƒé‡èšåˆæ–¹æ³•ã€‚</li>
<li>åŠ¨æ€æ—©æœŸåœæ­¢æ–¹æ³•è¢«ç”¨æ¥å¹³è¡¡å¼‚å¸¸ç±»ç‰¹å¾åˆ†å¸ƒã€‚è¯¥æ–¹æ³•é€šè¿‡åŸºäºä»£è¡¨å­¦ä¹ çš„æœ€å¤§å‡å€¼å·®å¼‚æ¥åŒºåˆ†å±€éƒ¨å’Œå…¨å±€æ¨¡å‹çš„ç‰¹å¾åµŒå…¥ã€‚</li>
<li>å…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰æ¥æ”¶æœºçš„å¹²æ‰°åˆ†ç±»æ˜¯è”é‚¦å­¦ä¹ çš„ä¸€ä¸ªå…¸å‹åº”ç”¨ç¤ºä¾‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-810090252909222be0b982630b65ac54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28bc092471cc9f294d534630e40005d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94cca7ebe7e473b704344d837d53be2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-448ab72adb3883080ade398f11e42a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4a5e361166d5466db13c6e20eac2bbe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi-scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve state-of-the-art results on benchmark datasets such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ—¨åœ¨è§£å†³ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒä¸­çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè®¸å¤šä¹‹å‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•è¦ä¹ˆä¸å¾—ä¸æ”¾å¼ƒå¤æ‚çš„å±€éƒ¨è¯­ä¹‰ç‰¹å¾ï¼Œè¦ä¹ˆé¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformeræ¶æ„çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©ç ç”Ÿæˆæ¨¡å—ï¼Œä»¥æé«˜æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥åˆ†å±‚çš„æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©ç ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èåˆäº†ä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨å±€ç‰¹å¾ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚åº¦ã€‚æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„è¿™ç§å¹³è¡¡ä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL-5iå’ŒCOCO-20iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„1-shotå’Œ5-shotç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…æœ‰150ä¸‡ä¸ªå‚æ•°ï¼Œå±•ç¤ºäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet%E4%BA%86%E8%A7%A3%E8%AF%A6%E6%83%85%E3%80%82">https://github.com/amirrezafateh/MSDNetäº†è§£è¯¦æƒ…ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformeræ¶æ„çš„Few-shotè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨ã€ä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—å’Œå¤šå°ºåº¦è§£ç å™¨ï¼Œæé«˜äº†å¯¹æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´å…³ç³»çš„ç†è§£ï¼Œå®ç°äº†ç²¾ç»†çš„è¯­ä¹‰åˆ†å‰²ï¼Œåœ¨PASCAL-5iå’ŒCOCO-20iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶ï¼Œå®ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Few-shotè¯­ä¹‰åˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œå¯¹è±¡åˆ†å‰²ã€‚</li>
<li>æå‡ºçš„åŸºäºTransformeræ¶æ„çš„Few-shotè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨ã€ä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—æ”¹å–„å…³ç³»ç†è§£ã€‚</li>
<li>å¤šå°ºåº¦è§£ç å™¨ç”¨äºé€šè¿‡åˆ†å±‚æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾ï¼Œä»¥ä¼˜åŒ–åˆ†å‰²æ©è†œã€‚</li>
<li>é›†æˆä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨çƒç‰¹å¾ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æˆæœï¼Œå¦‚PASCAL-5iå’ŒCOCO-20iï¼Œåœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹ä»…1.5ç™¾ä¸‡å‚æ•°ï¼Œåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dfbf8a50110d45135537d58989963cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86f9eb0a4c352a4a2a0765cb5bfc2f60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4fa677efdc0fbf4dddf9a40b66fae11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55d5166f5ce99d76a8cae11e3ae5aff8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TAVP-Task-Adaptive-Visual-Prompt-for-Cross-domain-Few-shot-Segmentation"><a href="#TAVP-Task-Adaptive-Visual-Prompt-for-Cross-domain-Few-shot-Segmentation" class="headerlink" title="TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation"></a>TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation</h2><p><strong>Authors:Jiaqi Yang, Yaning Zhang, Jingxi Hu, Xiangjian He, Linlin Shen, Guoping Qiu</strong></p>
<p>While large visual models (LVM) demonstrated significant potential in image understanding, due to the application of large-scale pre-training, the Segment Anything Model (SAM) has also achieved great success in the field of image segmentation, supporting flexible interactive cues and strong learning capabilities. However, SAMâ€™s performance often falls short in cross-domain and few-shot applications. Previous work has performed poorly in transferring prior knowledge from base models to new applications. To tackle this issue, we propose a task-adaptive auto-visual prompt framework, a new paradigm for Cross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature Fusion (MFF) was used for integrated feature extraction as prior knowledge. Besides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module to enable class-domain agnostic feature extraction and generate high-quality, learnable visual prompts. This significant advancement uses a unique generative approach to prompts alongside a comprehensive model structure and specialized prototype computation. While ensuring that the prior knowledge of SAM is not discarded, the new branch disentangles category and domain information through prototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across four cross-domain datasets demonstrate that our model outperforms the state-of-the-art CD-FSS approach, achieving an average accuracy improvement of 1.3% in the 1-shot setting and 11.76% in the 5-shot setting. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶å¾—ç›Šäºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„åº”ç”¨ï¼Œä½†åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸï¼ŒSegment Anything Modelï¼ˆSAMï¼‰ä¹Ÿå–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œå®ƒæ”¯æŒçµæ´»çš„äº¤äº’æç¤ºå’Œå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSAMåœ¨è·¨åŸŸå’Œå°‘æ ·æœ¬åº”ç”¨ä¸­çš„è¡¨ç°å¾€å¾€ä¸å°½å¦‚äººæ„ã€‚ä»¥å‰çš„å·¥ä½œåœ¨å°†å…ˆéªŒçŸ¥è¯†ä»åŸºç¡€æ¨¡å‹è½¬ç§»åˆ°æ–°åº”ç”¨ä¸Šçš„æ•ˆæœå¾ˆå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨è§†è§‰æç¤ºæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè·¨åŸŸå°‘æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„æ–°èŒƒå¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šçº§ç‰¹å¾èåˆï¼ˆMFFï¼‰è¿›è¡Œé›†æˆç‰¹å¾æå–ä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†ç±»åŸŸä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨æç¤ºï¼ˆCDTAPï¼‰æ¨¡å—ï¼Œä»¥å®ç°ç±»åŸŸæ— å…³çš„ç‰¹å¾æå–ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡ã€å¯å­¦ä¹ çš„è§†è§‰æç¤ºã€‚è¿™é¡¹é‡å¤§è¿›å±•é‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„ç”Ÿæˆæç¤ºæ–¹æ³•ï¼Œå¹¶é…å¤‡äº†å…¨é¢çš„æ¨¡å‹ç»“æ„å’Œä¸“é—¨çš„åŸå‹è®¡ç®—ã€‚åœ¨ä¿ç•™SAMçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶ï¼Œæ–°åˆ†æ”¯é€šè¿‡åŸå‹è§£çº ç¼ ç±»åˆ«å’ŒåŸŸä¿¡æ¯ï¼Œå¼•å¯¼å…¶é€‚åº”CD-FSSã€‚åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„CD-FSSæ–¹æ³•ï¼Œåœ¨1æ¬¡æ‹æ‘„çš„æƒ…å†µä¸‹å¹³å‡å‡†ç¡®ç‡æé«˜1.3%ï¼Œåœ¨5æ¬¡æ‹æ‘„çš„æƒ…å†µä¸‹å¹³å‡å‡†ç¡®ç‡æé«˜11.76%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05393v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œè€ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸä¹Ÿå–å¾—äº†æˆåŠŸï¼Œå…·æœ‰çµæ´»äº¤äº’æç¤ºå’Œå¼ºå¤§å­¦ä¹ èƒ½åŠ›ã€‚ä½†åœ¨è·¨åŸŸå’Œå°‘é•œå¤´åº”ç”¨ä¸­ï¼ŒSAMè¡¨ç°æ¬ ä½³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨è§†è§‰æç¤ºæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è·¨åŸŸå°‘é•œå¤´åˆ†å‰²ï¼ˆCD-FSSï¼‰èŒƒå¼ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šçº§åˆ«ç‰¹å¾èåˆï¼ˆMFFï¼‰è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ç»“åˆç±»åŸŸä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨æç¤ºï¼ˆCDTAPï¼‰æ¨¡å—ï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¯å­¦ä¹ çš„è§†è§‰æç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›CD-FSSæ–¹æ³•ï¼Œåœ¨å•é•œå¤´è®¾ç½®ä¸‹å¹³å‡ç²¾åº¦æé«˜1.3%ï¼Œåœ¨äº”é•œå¤´è®¾ç½®ä¸‹æé«˜11.76%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†å…¶åœ¨è·¨åŸŸå’Œå°‘é•œå¤´åº”ç”¨ä¸­è¡¨ç°ä¸è¶³ã€‚</li>
<li>æå‡ºçš„ä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨è§†è§‰æç¤ºæ¡†æ¶æ˜¯ä¸€ç§æ–°çš„è·¨åŸŸå°‘é•œå¤´åˆ†å‰²ï¼ˆCD-FSSï¼‰æ–¹æ³•ã€‚</li>
<li>å¤šçº§åˆ«ç‰¹å¾èåˆï¼ˆMFFï¼‰ç”¨äºé›†æˆç‰¹å¾æå–ä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ç±»åŸŸä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨æç¤ºï¼ˆCDTAPï¼‰æ¨¡å—å¯ç”Ÿæˆé«˜è´¨é‡ã€å¯å­¦ä¹ çš„è§†è§‰æç¤ºã€‚</li>
<li>æ¨¡å‹åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰CD-FSSæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4541d57fc7cef23263f894fdb4ab19c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10b5094632539bd64ce887a7ddf2dfae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1a58eb1d51be19d717063965e48a6d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c92f6aeb75f67bae2eec0ead5ca1128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b07a91a16c6948fedbdf391cc9cd89fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EHRCon-Dataset-for-Checking-Consistency-between-Unstructured-Notes-and-Structured-Tables-in-Electronic-Health-Records"><a href="#EHRCon-Dataset-for-Checking-Consistency-between-Unstructured-Notes-and-Structured-Tables-in-Electronic-Health-Records" class="headerlink" title="EHRCon: Dataset for Checking Consistency between Unstructured Notes and   Structured Tables in Electronic Health Records"></a>EHRCon: Dataset for Checking Consistency between Unstructured Notes and   Structured Tables in Electronic Health Records</h2><p><strong>Authors:Yeonsu Kwon, Jiho Kim, Gyubok Lee, Seongsu Bae, Daeun Kyung, Wonchul Cha, Tom Pollard, Alistair Johnson, Edward Choi</strong></p>
<p>Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 4,101 entities across 105 clinical notes checked against database entries for consistency. EHRCon has two versions, one using the original MIMIC-III schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/dustn1259/EHRCon">https://github.com/dustn1259/EHRCon</a>. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰å¯¹äºå­˜å‚¨å…¨é¢çš„æ‚£è€…åŒ»ç–—è®°å½•è‡³å…³é‡è¦ï¼Œå®ƒç»“åˆäº†ç»“æ„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚è¯ç‰©ä¿¡æ¯ï¼‰å’Œè¯¦ç»†çš„ä¸´åºŠç¬”è®°ï¼ˆä¾‹å¦‚åŒ»ç”Ÿç¬”è®°ï¼‰ã€‚è¿™äº›è¦ç´ å¯¹äºç›´æ¥çš„æ•°æ®æ£€ç´¢è‡³å…³é‡è¦ï¼Œå¹¶ä¸ºæ‚£è€…æŠ¤ç†æä¾›äº†æ·±å…¥ã€å…·ä½“çš„è§è§£ã€‚ç„¶è€Œï¼Œç”±äºç”µå­å¥åº·è®°å½•ç³»ç»Ÿçš„ä¸ç›´è§‚è®¾è®¡å’Œäººä¸ºé”™è¯¯ï¼Œå®ƒä»¬å¸¸å¸¸ä¼šå‡ºç°å·®å¼‚ï¼Œç»™æ‚£è€…çš„å®‰å…¨å¸¦æ¥ä¸¥é‡é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†EHRConï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æ–°æ•°æ®é›†å’Œä»»åŠ¡ï¼Œæ—¨åœ¨ç¡®ä¿ç”µå­å¥åº·è®°å½•ä¸­ç»“æ„åŒ–è¡¨æ ¼å’Œæœªç»“æ„åŒ–ç¬”è®°ä¹‹é—´çš„æ•°æ®ä¸€è‡´æ€§ã€‚EHRConæ˜¯ä¸åŒ»ç–—ä¿å¥ä¸“ä¸šäººå‘˜åˆä½œï¼Œåˆ©ç”¨MIMIC-IIIç”µå­å¥åº·è®°å½•æ•°æ®é›†åˆ¶ä½œçš„ï¼Œå…¶ä¸­åŒ…æ‹¬å¯¹æ•°æ®åº“æ¡ç›®çš„ä¸€è‡´æ€§æ£€æŸ¥çš„105ä¸ªä¸´åºŠç¬”è®°ä¸­çš„4,101ä¸ªå®ä½“çš„æ‰‹åŠ¨æ³¨é‡Šã€‚EHRConæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ªä½¿ç”¨åŸå§‹çš„MIMIC-IIIæ¨¡å¼ï¼Œå¦ä¸€ä¸ªä½¿ç”¨OMOP CDMæ¨¡å¼ï¼Œä»¥å¢åŠ å…¶é€‚ç”¨æ€§å’Œé€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CheckEHRï¼Œè¿™æ˜¯ä¸€ä¸ªéªŒè¯ä¸´åºŠç¬”è®°å’Œæ•°æ®åº“è¡¨æ ¼ä¹‹é—´ä¸€è‡´æ€§çš„æ–°æ¡†æ¶ã€‚CheckEHRé‡‡ç”¨ä¸€ä¸ªå…«é˜¶æ®µçš„æµç¨‹ï¼Œå¹¶åœ¨å°æ ·å’Œé›¶æ ·æœ¬ç¯å¢ƒä¸­éƒ½æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dustn1259/EHRCon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dustn1259/EHRConæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16341v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰èåˆäº†ç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–ä¸´åºŠç¬”è®°ï¼Œæ˜¯æ‚£è€…åŒ»ç–—è®°å½•çš„å…³é”®å­˜å‚¨å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºEHRç³»ç»Ÿè®¾è®¡çš„éç›´è§‚æ€§å’Œäººä¸ºé”™è¯¯ï¼Œæ•°æ®ä¸€è‡´æ€§å­˜åœ¨é£é™©ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†EHRConæ•°æ®é›†å’Œä»»åŠ¡ï¼Œç¡®ä¿EHRä¸­ç»“æ„åŒ–è¡¨æ ¼ä¸æœªç»“æ„åŒ–ç¬”è®°ä¹‹é—´çš„æ•°æ®ä¸€è‡´æ€§ã€‚EHRConä¸åŒ»ç–—ä¿å¥ä¸“ä¸šäººå£«åˆä½œåˆ›å»ºï¼ŒåŒ…æ‹¬åœ¨æ•°æ®åº“æ¡ç›®ä¸­æ‰‹åŠ¨æ³¨é‡Šçš„4,101ä¸ªå®ä½“çš„ä¸€è‡´æ€§æ£€æŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¨å‡ºäº†CheckEHRæ¡†æ¶ï¼Œç”¨äºéªŒè¯ä¸´åºŠç¬”è®°å’Œæ•°æ®åº“è¡¨æ ¼ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚CheckEHRå±•ç¤ºå‡ºåœ¨å°‘é‡ç”šè‡³æ— æ ·æœ¬çš„æƒ…å†µä¸‹å…·æœ‰åº”ç”¨å‰æ™¯ã€‚å…¶ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ç»“åˆäº†ç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–ä¸´åºŠç¬”è®°ï¼Œä¸ºæ‚£è€…åŒ»ç–—è®°å½•æä¾›äº†å…¨é¢å­˜å‚¨æ–¹æ¡ˆã€‚</li>
<li>EHRså› ç³»ç»Ÿè®¾è®¡çš„éç›´è§‚æ€§å’Œäººä¸ºé”™è¯¯å¸¸å¸¸å‡ºç°æ•°æ®ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç»™æ‚£è€…å®‰å…¨å¸¦æ¥é£é™©ã€‚</li>
<li>å¼€å‘EHRConæ•°æ®é›†å’Œä»»åŠ¡æ˜¯ä¸ºäº†ç¡®ä¿EHRä¸­æ•°æ®çš„ä¸€è‡´æ€§ã€‚è¯¥æ•°æ®é›†ç”±åŒ»ç–—ä¿å¥ä¸“ä¸šäººå£«åˆä½œåˆ›å»ºï¼Œå¹¶åŒ…æ‹¬åœ¨æ•°æ®åº“æ¡ç›®ä¸­æ‰‹åŠ¨æ³¨é‡Šçš„å®ä½“ä¸€è‡´æ€§æ£€æŸ¥ã€‚</li>
<li>EHRConæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œåˆ†åˆ«ä½¿ç”¨åŸå§‹çš„MIMIC-IIIæ¨¡å¼å’ŒOMOP CDMæ¨¡å¼ï¼Œä»¥æé«˜å…¶é€‚ç”¨æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>CheckEHRæ¡†æ¶è¢«ç”¨äºéªŒè¯ä¸´åºŠç¬”è®°å’Œæ•°æ®åº“è¡¨æ ¼ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>CheckEHRåœ¨å°‘é‡æ ·æœ¬ç”šè‡³æ— æ ·æœ¬çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dafe4f9617c6741023c4ed99613be7cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971937f7c85511d62c62ea0a01eea71b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-323158506eca6384d54f1930487b5e30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a286af03a871bc25b5962f6a81fa0f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f532996c41e8376424156660f3dfaacc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰é€šè¿‡ä½¿ä¸åŒæœ¬ä½“ä¹‹é—´å®ç°è¯­ä¹‰äº’æ“ä½œæ€§ï¼Œå¹¶é€šè¿‡å¯¹é½ç›¸å…³å®ä½“è§£å†³å…¶æ¦‚å¿µä¸Šçš„å¼‚è´¨æ€§ã€‚ç›®å‰ï¼ŒOMç³»ç»Ÿä¸»è¦æœ‰ä¸¤ç§æµè¡Œçš„è®¾è®¡èŒƒå¼ï¼šä¼ ç»Ÿçš„åŸºäºçŸ¥è¯†çš„ä¸“å®¶ç³»ç»Ÿå’Œè¾ƒæ–°çš„åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ç³»ç»Ÿã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†å·²ç»å½»åº•æ”¹å˜äº†æ•°æ®å·¥ç¨‹ï¼Œå¹¶åœ¨è®¸å¤šé¢†åŸŸå¾—åˆ°äº†åˆ›é€ æ€§çš„åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨OMä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åŸºäºä»£ç†çš„LLMè®¾è®¡èŒƒå¼çš„OMç³»ç»Ÿã€‚è€ƒè™‘åˆ°åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMé¢ä¸´çš„è‹¥å¹²ç‰¹å®šæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå³Agent-OMï¼ˆç”¨äºæœ¬ä½“åŒ¹é…çš„ä»£ç†ï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†ä»¥åŠä¸€ç»„OMå·¥å…·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸€ä¸ªæ¦‚å¿µéªŒè¯ç³»ç»Ÿä¸­å®ç°ã€‚å¯¹ä¸‰ä¸ªæœ¬ä½“å¯¹é½è¯„ä¼°å€¡è®®ï¼ˆOAEIï¼‰èµ›é“ä¸Šçš„æœ€æ–°OMç³»ç»Ÿçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šçš„ç»“æœéå¸¸æ¥è¿‘é•¿æœŸä»¥æ¥çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨å¤æ‚å’Œå°‘æ ·æœ¬OMä»»åŠ¡ä¸Šå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v6">PDF</a> 19 pages, 12 figures, 3 tables</p>
<p><strong>æ€»ç»“</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹åŸºäºLLMä»£ç†çš„OMç³»ç»Ÿè®¾è®¡èŒƒå¼ã€‚é’ˆå¯¹åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMçš„ç‰¹å®šæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªé€šç”¨çš„Agent-OMæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†å’Œä¸€ç»„OMå·¥å…·ã€‚åœ¨è¯æ˜æ¦‚å¿µçš„ç³»ç»Ÿä¸Šå®ç°è¯¥æ¡†æ¶ï¼Œå¹¶é€šè¿‡ä¸å…ˆè¿›çš„OMç³»ç»Ÿçš„è¯„ä»·å¯¹æ¯”ï¼Œç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šè¡¨ç°æ¥è¿‘æœ€ä½³æ°´å¹³ï¼Œå¹¶åœ¨å¤æ‚å’Œå°‘æ ·æœ¬OMä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºLLMä»£ç†çš„OMç³»ç»Ÿè®¾è®¡èŒƒå¼ã€‚</li>
<li>ä»‹ç»äº†é’ˆå¯¹åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMçš„ç‰¹å®šæŒ‘æˆ˜è€Œè®¾è®¡çš„é€šç”¨Agent-OMæ¡†æ¶ã€‚</li>
<li>Agent-OMæ¡†æ¶åŒ…å«ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†å’Œä¸€ç»„OMå·¥å…·ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨è¯æ˜æ¦‚å¿µçš„ç³»ç»Ÿä¸Šè¿›è¡Œäº†å®ç°ã€‚</li>
<li>ä¸å…ˆè¿›çš„OMç³»ç»Ÿçš„è¯„ä»·å¯¹æ¯”æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šè¡¨ç°æ¥è¿‘æœ€ä½³æ°´å¹³ã€‚</li>
<li>åœ¨å¤æ‚å’Œå°‘æ ·æœ¬OMä»»åŠ¡ä¸Šï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e94f310dc617e93193eaa255421342af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3be1f670b4b3e5e54e8dc998918fda9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cde61049b12372f9a36b0b53054ed8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e27c225a871ec7ed4833fed181d2f8e7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Knowledge-Graph-Construction-and-Reasoning-Recent-Capabilities-and-Future-Opportunities"><a href="#LLMs-for-Knowledge-Graph-Construction-and-Reasoning-Recent-Capabilities-and-Future-Opportunities" class="headerlink" title="LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities   and Future Opportunities"></a>LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities   and Future Opportunities</h2><p><strong>Authors:Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang</strong></p>
<p>This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMsâ€™ performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs. The code and datasets are in <a target="_blank" rel="noopener" href="https://github.com/zjunlp/AutoKG">https://github.com/zjunlp/AutoKG</a>. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢å®šé‡å’Œå®šæ€§åœ°è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºå’Œæ¨ç†ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œé‡ç‚¹ç ”ç©¶å››ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å®ä½“å’Œå…³ç³»æŠ½å–ã€äº‹ä»¶æŠ½å–ã€é“¾æ¥é¢„æµ‹å’Œé—®ç­”ï¼Œä»è€Œå…¨é¢æ¢ç´¢LLMåœ¨æ„å»ºå’Œæ¨ç†é¢†åŸŸçš„æ€§èƒ½ã€‚å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»¥GPT-4ä¸ºä»£è¡¨çš„å¤§å‹è¯­è¨€æ¨¡å‹æ›´é€‚åˆä½œä¸ºæ¨ç†åŠ©æ‰‹ï¼Œè€Œéå°‘æ•°æƒ…å†µä¸‹çš„ä¿¡æ¯æå–å™¨ã€‚å…·ä½“è€Œè¨€ï¼ŒGPT-4åœ¨çŸ¥è¯†å›¾è°±æ„å»ºç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šæ›´æ˜¯è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†å¾®è°ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥è¿˜æ‰©å±•åˆ°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æå–æ–¹é¢çš„æ½œåœ¨æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œæå‡ºäº†è™šæ‹ŸçŸ¥è¯†æå–ä»»åŠ¡å¹¶å¼€å‘äº†ç›¸åº”çš„VINEæ•°æ®é›†ã€‚åŸºäºè¿™äº›å®è¯å‘ç°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†AutoKGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šä»£ç†çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨èµ„æºæ¥è¿›è¡ŒçŸ¥è¯†å›¾è°±çš„æ„å»ºå’Œæ¨ç†ã€‚æˆ‘ä»¬é¢„æœŸè¿™é¡¹ç ”ç©¶èƒ½ä¸ºæœªæ¥çŸ¥è¯†å›¾è°±é¢†åŸŸçš„ç ”ç©¶æä¾›å®è´µçš„è§è§£ã€‚ä»£ç å’Œæ•°æ®é›†ä½äº<a target="_blank" rel="noopener" href="https://github.com/zjunlp">https://github.com/zjunlp</a> å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.13168v4">PDF</a> World Wide Web Journal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºå’Œæ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒæ¶‰åŠå…«ä¸ªä¸åŒæ•°æ®é›†ï¼Œæ¶µç›–å®ä½“å’Œå…³ç³»æŠ½å–ã€äº‹ä»¶æŠ½å–ã€é“¾æ¥é¢„æµ‹å’Œé—®ç­”ç­‰å››ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼Œæ·±å…¥æ¢è®¨äº†LLMsåœ¨æ„å»ºå’Œæ¨ç†é¢†åŸŸçš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-4ç­‰LLMsæ›´é€‚åˆä½œä¸ºæ¨ç†åŠ©æ‰‹è€Œéå°‘æ ·æœ¬ä¿¡æ¯æå–å™¨ã€‚åœ¨KGæ„å»ºç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸­æ›´æ˜¯è¡¨ç°å‡ºè¶…è¶Šå¾®è°ƒæ¨¡å‹çš„å®åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ‰©å±•äº†LLMsåœ¨ä¿¡æ¯æå–æ–¹é¢çš„æ½œåœ¨æ³›åŒ–èƒ½åŠ›ï¼Œæå‡ºäº†è™šæ‹ŸçŸ¥è¯†æå–ä»»åŠ¡åŠç›¸åº”çš„VINEæ•°æ®é›†ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†AutoKGï¼Œä¸€ç§åŸºäºå¤šä»£ç†çš„æ–¹æ³•ï¼Œåˆ©ç”¨LLMså’Œå¤–éƒ¨èµ„æºç”¨äºKGæ„å»ºå’Œæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ¨ç†æ–¹é¢è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>å®éªŒæ¶‰åŠå¤šä¸ªæ•°æ®é›†å’Œä»£è¡¨æ€§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å®ä½“å’Œå…³ç³»æŠ½å–ã€äº‹ä»¶æŠ½å–ã€é“¾æ¥é¢„æµ‹å’Œé—®ç­”ã€‚</li>
<li>GPT-4ç­‰LLMsæ›´é€‚åˆä½œä¸ºæ¨ç†åŠ©æ‰‹ï¼Œè€Œéå°‘æ ·æœ¬ä¿¡æ¯æå–å™¨ã€‚</li>
<li>åœ¨çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡ä¸­ï¼ŒGPT-4è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œæœ‰æ—¶è¶…è¶Šå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æ‰©å±•äº†LLMsåœ¨ä¿¡æ¯æå–æ–¹é¢çš„æ½œåœ¨æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†è™šæ‹ŸçŸ¥è¯†æå–ä»»åŠ¡åŠç›¸åº”çš„VINEæ•°æ®é›†ã€‚</li>
<li>åŸºäºå®è¯å‘ç°ï¼Œè®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†AutoKGï¼Œä¸€ç§åŸºäºå¤šä»£ç†çš„æ–¹æ³•ï¼Œåˆ©ç”¨LLMså’Œå¤–éƒ¨èµ„æºè¿›è¡ŒKGæ„å»ºå’Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.13168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76d0f5184449764f1a13ec16feb72f11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88165b4ff15dda86312ad14f379c880f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ac2e5b25845df57f9f7ae6eacab784d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90557d01fd4d2e8de91b2951f475dc79.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a8c203cf03eeb977522b9b30ca524e1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  ReFlow6D Refraction-Guided Transparent Object 6D Pose Estimation via   Intermediate Representation Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d420af9d7c6e4b553c221ca8efb92b25.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Distributed Mixture-of-Agents for Edge Inference with Large Language   Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
