<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-02  Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b01de80e41cf94f03899138149fddebb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    61 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-02-更新"><a href="#2025-01-02-更新" class="headerlink" title="2025-01-02 更新"></a>2025-01-02 更新</h1><h2 id="Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning"><a href="#Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning" class="headerlink" title="Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning"></a>Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning</h2><p><strong>Authors:Zhifang Zhang, Shuo He, Bingquan Shen, Lei Feng</strong></p>
<p>Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, yet they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we disclose that CLIP’s vulnerabilities primarily stem from its excessive encoding of class-irrelevant features, which can compromise the model’s visual feature resistivity to input perturbations, making it more susceptible to capturing the trigger patterns inserted by backdoor attacks. Inspired by this finding, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs specially designed deep visual prompt tuning and feature-repelling loss to eliminate excessive class-irrelevant features while simultaneously optimizing cross-entropy loss to maintain clean accuracy. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27% of the parameters relative to CLIP, yet it significantly outperforms state-of-the-art baselines, reducing the attack success rate from 67.53% to 2.76% against SoTA attacks and effectively generalizing its defensive capabilities across multiple datasets. </p>
<blockquote>
<p>多模态对比学习模型（例如CLIP）可以从大规模图像文本数据集中学习高质量表示，但它们对后门攻击表现出显著脆弱性，这引发了严重的安全担忧。在本文中，我们披露CLIP的脆弱性主要源于其对与类别无关特征的过度编码，这可能会损害模型对输入扰动的视觉特征电阻，使其更容易捕获由后门攻击插入的触发模式。受这一发现的启发，我们提出了名为“排斥视觉提示调整”（RVPT）的新型防御方法，该方法采用专门设计的深度视觉提示调整和特征排斥损失，以消除过多的与类别无关的特征，同时优化交叉熵损失以保持清洁准确性。与通常需要中毒数据或涉及对整个模型进行微调的传统多模态后门防御方法不同，RVPT利用少量下游清洁样本，并且只调整少量参数。经验结果表明，RVPT仅调整CLIP的0.27%参数，却显著优于最新基线技术，将攻击成功率从67.53%降低到2.76%，有效抵御了最先进的攻击，并在多个数据集上实现了防御能力的有效泛化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20392v1">PDF</a> </p>
<p><strong>Summary</strong><br>     基于CLIP等多模态对比学习模型的图像文本数据集能学习高质量表示，但其存在易受后门攻击影响的脆弱性，引起严重的安全问题。研究发现，CLIP的脆弱性主要源于其对非类别特征的过度编码，削弱了模型对输入扰动的视觉特征抗性，易于捕获后门攻击插入的触发模式。为此，本文提出采用深度视觉提示调整和特征排斥损失的防御方法——RVPT。RVPT在不要求中毒数据可用性或涉及整个模型微调的前提下，通过优化少量参数实现了良好的防御效果。实验结果证明，RVPT仅调整CLIP的0.27%参数，即可显著超越现有技术基线，攻击成功率从原来的67.53%降低到仅存的2.76%，并且在多个数据集上实现了有效的防御能力推广。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态对比学习模型如CLIP面临后门攻击的风险，存在安全隐患。</li>
<li>CLIP的脆弱性源于其对非类别特征的过度编码，使其易受触发模式插入攻击影响。</li>
<li>新提出的防御方法RVPT，旨在消除多余的类别无关特征。通过深度视觉提示调整和特征排斥损失实现优化。</li>
<li>RVPT仅调整少量参数即可实现高效防御，与现有方法相比有显著优势。实验证明其在不同数据集上防御效果广泛适用。</li>
<li>RVPT针对后门攻击的防御能力强大，攻击成功率显著降低。</li>
<li>RVPT方法具有强大的泛化能力，能在多个数据集上有效抵御攻击。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20392">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-731c10ba9cb98a5f1c9571bf1e10724e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b660138eeac2701eb1a32cf69813426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96ea090cdcfae8508eace173ec375f29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bdecdca900ace72072661824a0df709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd4349bc7cf61837d6670e0b0525b7b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e2c0e1397e7179408b75735154a2d7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-shot-Algorithm-Assurance"><a href="#Few-shot-Algorithm-Assurance" class="headerlink" title="Few-shot Algorithm Assurance"></a>Few-shot Algorithm Assurance</h2><p><strong>Authors:Dang Nguyen, Sunil Gupta</strong></p>
<p>In image classification tasks, deep learning models are vulnerable to image distortion. For successful deployment, it is important to identify distortion levels under which the model is usable i.e. its accuracy stays above a stipulated threshold. We refer to this problem as Model Assurance under Image Distortion, and formulate it as a classification task. Given a distortion level, our goal is to predict if the model’s accuracy on the set of distorted images is greater than a threshold. We propose a novel classifier based on a Level Set Estimation (LSE) algorithm, which uses the LSE’s mean and variance functions to form the classification rule. We further extend our method to a “few sample” setting where we can only acquire few real images to perform the model assurance process. Our idea is to generate extra synthetic images using a novel Conditional Variational Autoencoder model with two new loss functions. We conduct extensive experiments to show that our classification method significantly outperforms strong baselines on five benchmark image datasets. </p>
<blockquote>
<p>在图像分类任务中，深度学习模型容易受到图像失真的影响。为了确保成功部署，重要的是要确定模型在何种失真水平下是可用的，即其准确性保持在规定的阈值之上。我们将这个问题称为图像失真下的模型保证，并将其制定为一个分类任务。给定失真水平，我们的目标是预测模型在失真图像集上的准确性是否高于阈值。我们提出了一种基于水平集估计（LSE）算法的新型分类器，该分类器使用LSE的均值和方差函数来形成分类规则。我们将该方法进一步扩展到“少量样本”设置，其中我们只能获取少量真实图像来执行模型保证过程。我们的想法是利用新型条件变分自动编码器模型生成额外的合成图像，该模型具有两个新的损失函数。我们进行了大量实验，结果表明我们的分类方法在五个基准图像数据集上显著优于强大的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出模型在图像分类任务中对图像失真的脆弱性问题，并定义此为模型在失真图像上的准确率高于规定阈值的问题，称为模型保证下的图像失真问题。文章建议使用基于水平集估计（LSE）算法的全新分类器来解决这一问题，并通过实验证明该方法在五个基准图像数据集上的表现显著优于其他方法。当面临真实图像样本有限的情况时，通过采用新型的条件变分自动编码器模型及两个新损失函数生成额外合成图像来应对。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型在图像分类任务中面临图像失真的脆弱性挑战。</li>
<li>引入模型保证下的图像失真问题，定义为模型在失真图像上的准确率高于某一阈值。</li>
<li>提出基于水平集估计（LSE）算法的新型分类器来解决此问题。</li>
<li>LSE算法利用均值和方差函数形成分类规则。</li>
<li>当真实图像样本有限时，通过新型的条件变分自动编码器模型及两个新损失函数生成合成图像以应对挑战。</li>
<li>方法在五个基准图像数据集上的表现显著优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4402b28add3f9740cff0b72677a57892.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-948f51b51f427cc2e107e59c16d5cde7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8023517cf815e5310a2066c60b1bda0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Time-Series-Foundational-Models-Their-Role-in-Anomaly-Detection-and-Prediction"><a href="#Time-Series-Foundational-Models-Their-Role-in-Anomaly-Detection-and-Prediction" class="headerlink" title="Time Series Foundational Models: Their Role in Anomaly Detection and   Prediction"></a>Time Series Foundational Models: Their Role in Anomaly Detection and   Prediction</h2><p><strong>Authors:Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth</strong></p>
<p>Time series foundational models (TSFM) have gained prominence in time series forecasting, promising state-of-the-art performance across various applications. However, their application in anomaly detection and prediction remains underexplored, with growing concerns regarding their black-box nature, lack of interpretability and applicability. This paper critically evaluates the efficacy of TSFM in anomaly detection and prediction tasks. We systematically analyze TSFM across multiple datasets, including those characterized by the absence of discernible patterns, trends and seasonality. Our analysis shows that while TSFMs can be extended for anomaly detection and prediction, traditional statistical and deep learning models often match or outperform TSFM in these tasks. Additionally, TSFMs require high computational resources but fail to capture sequential dependencies effectively or improve performance in few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to reproduce the results and supplementary materials are available at <a target="_blank" rel="noopener" href="https://github.com/smtmnfg/TSFM">https://github.com/smtmnfg/TSFM</a>. </p>
<blockquote>
<p>时间序列基础模型（TSFM）在时间序列预测领域已经崭露头角，并在各种应用中展现出卓越的性能。然而，其在异常检测和预测方面的应用仍然被较少探索，并且由于它们黑箱性质、缺乏可解释性和适用性的担忧日益增加。本文对TSFM在异常检测和预测任务中的有效性进行了深入评估。我们系统地分析了多个数据集上的TSFM，包括那些以缺乏可辨识的模式、趋势和季节性为特征的数据集。我们的分析表明，虽然可以将TSFM扩展到异常检测和预测任务中，但传统的统计模型和深度学习模型在这些任务中往往与TSFM相匹配或表现更好。此外，TSFM需要较高的计算资源，但在小样本或零样本场景中未能有效地捕捉序列依赖性或提高性能。预处理过的数据集、用于重现结果的代码和补充材料可通过<a target="_blank" rel="noopener" href="https://github.com/smtmnfg/TSFM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/smtmnfg/TSFM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19286v1">PDF</a> 12 pages, 6 figures, 5 tables. Accepted at AAAI2025 Anomaly Detection   in Scientific Domains Workshop</p>
<p><strong>Summary</strong></p>
<p>时间序列基础模型（TSFM）在时间序列预测中受到广泛关注，但在异常检测和预测方面的应用仍然探索不足。本文对TSFM在异常检测和预测任务中的有效性进行了评估，发现虽然TSFM可以扩展到异常检测和预测任务中，但在这些任务中，传统的统计模型和深度学习模型往往与TSFM相匹配或表现更好。此外，TSFM需要较高的计算资源，但在少样本或零样本场景中无法有效捕捉序列依赖性或提高性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列基础模型（TSFM）在时间序列预测领域受到广泛关注。</li>
<li>TSFM在异常检测和预测方面的应用仍然探索不足。</li>
<li>TSFM可以扩展到异常检测和预测任务中。</li>
<li>在异常检测和预测任务中，传统的统计模型和深度学习模型往往与TSFM相匹配或表现更好。</li>
<li>TSFM需要较高的计算资源。</li>
<li>在少样本或零样本场景中，TSFM无法有效捕捉序列依赖性或提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19286">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-027eac48ab82e6bb20e6403fe3594a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-154237e835aed34809f1f39c2806389a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b01de80e41cf94f03899138149fddebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42804a18c405e484be138c91414487a3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLIP-GS-Unifying-Vision-Language-Representation-with-3D-Gaussian-Splatting"><a href="#CLIP-GS-Unifying-Vision-Language-Representation-with-3D-Gaussian-Splatting" class="headerlink" title="CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian   Splatting"></a>CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian   Splatting</h2><p><strong>Authors:Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei</strong></p>
<p>Recent works in 3D multimodal learning have made remarkable progress. However, typically 3D multimodal models are only capable of handling point clouds. Compared to the emerging 3D representation technique, 3D Gaussian Splatting (3DGS), the spatially sparse point cloud cannot depict the texture information of 3D objects, resulting in inferior reconstruction capabilities. This limitation constrains the potential of point cloud-based 3D multimodal representation learning. In this paper, we present CLIP-GS, a novel multimodal representation learning framework grounded in 3DGS. We introduce the GS Tokenizer to generate serialized gaussian tokens, which are then processed through transformer layers pre-initialized with weights from point cloud models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss between 3DGS and the visual-text embeddings of CLIP, and we introduce an image voting loss to guide the directionality and convergence of gradient optimization. Furthermore, we develop an efficient way to generate triplets of 3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal representations. Leveraging the well-aligned multimodal representations, CLIP-GS demonstrates versatility and outperforms point cloud-based models on various 3D tasks, including multimodal retrieval, zero-shot, and few-shot classification. </p>
<blockquote>
<p>近期，在三维多模态学习领域取得了显著进展。然而，典型的三维多模态模型通常只能处理点云数据。与新兴的三维表示技术相比，三维高斯展开（3DGS）在表达三维物体的纹理信息方面，稀疏的点云无法很好地描述纹理信息，导致重建能力受限。这一局限性制约了基于点云的三维多模态表示学习的潜力。在本文中，我们提出了基于三维高斯展开（CLIP-GS）的新型多模态表示学习框架。我们引入了GS分词器生成序列化高斯令牌，这些令牌经过使用点云模型的权重进行预初始化的变压器层处理，生成三维高斯嵌入。CLIP-GS利用三维高斯嵌入与CLIP的视觉文本嵌入之间的对比损失，并引入了图像投票损失来指导梯度优化的方向性和收敛性。此外，我们开发了一种有效的方法来生成三维高斯嵌入、图像和文本的三元组，促进了CLIP-GS在学习统一的多模态表示时的应用。利用对齐良好的多模态表示，CLIP-GS表现出多功能性并在多种三维任务上超越了基于点云的模型，包括多模态检索、零样本学习和小样本文本分类等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于3DGS的多模态表示学习框架CLIP-GS的研究。针对点云在描述纹理信息方面的不足，提出了GS Tokenizer生成高斯令牌，通过预训练的点云模型与Transformer层生成3DGS嵌入。通过对比CLIP的视觉文本嵌入和图像投票损失实现优化。能够生成统一的跨模态三元组表示，有助于在多种任务上表现超越点云模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS技术相较于传统的点云技术能更好地描述纹理信息，提升重建能力。</li>
<li>CLIP-GS是一个基于3DGS的多模态表示学习框架，旨在解决点云处理模型的问题。</li>
<li>GS Tokenizer可以生成序列化高斯令牌并处理以得到更有效的3DGS嵌入。</li>
<li>CLIP-GS使用对比损失和图像投票损失进行梯度优化方向指导。</li>
<li>CLIP-GS能够生成跨模态的三元组表示，包括图像、文本和3DGS数据。</li>
<li>CLIP-GS在多种任务上表现超越点云模型，包括多模态检索、零样本和少样本分类等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dcbc138552113253a621cb50d857496d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d55fb43103e5b6f0ffd9fbf5a090d464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f564696445487fc2b998cee8e8e25454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003a21e2cc14126fa6e89a6254a0e537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ebab259c5516d6d9a6c0cc4040baad7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reconstruction-Target-Matters-in-Masked-Image-Modeling-for-Cross-Domain-Few-Shot-Learning"><a href="#Reconstruction-Target-Matters-in-Masked-Image-Modeling-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Reconstruction Target Matters in Masked Image Modeling for Cross-Domain   Few-Shot Learning"></a>Reconstruction Target Matters in Masked Image Modeling for Cross-Domain   Few-Shot Learning</h2><p><strong>Authors:Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer knowledge from the data-abundant source domain to data-scarce target domains for fast adaptation, where the large domain gap makes CDFSL a challenging problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data and learning image’s global structures, enhancing model generalization and robustness. However, in the CDFSL task with significant domain shifts, we find MAE even shows lower performance than the baseline supervised models. In this paper, we first delve into this phenomenon for an interpretation. We find that MAE tends to focus on low-level domain information during reconstructing pixels while changing the reconstruction target to token features could mitigate this problem. However, not all features are beneficial, as we then find reconstructing high-level features can hardly improve the model’s transferability, indicating a trade-off between filtering domain information and preserving the image’s global structure. In all, the reconstruction target matters for the CDFSL task. Based on the above findings and interpretations, we further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL task. DAMIM includes an Aggregated Feature Reconstruction module to automatically aggregate features for reconstruction, with balanced learning of domain-agnostic information and images’ global structure, and a Lightweight Decoder module to further benefit the encoder’s generalizability. Experiments on four CDFSL datasets demonstrate that our method achieves state-of-the-art performance. </p>
<blockquote>
<p>跨域小样本学习（CDFSL）要求模型从数据丰富的源域转移知识到数据稀缺的目标域，以实现快速适应，而较大的域差距使得CDFSL成为一个具有挑战性的问题。掩码自动编码器（MAE）擅长有效地使用无标签数据并学习图像的全局结构，增强了模型的通用性和稳健性。然而，在具有显著域转移特性的CDFSL任务中，我们发现MAE的性能甚至低于基线监督模型。在本文中，我们首先深入研究这一现象进行解释。我们发现MAE在重建像素时倾向于关注低级别的域信息，而将重建目标改为令牌特征可以缓解这个问题。然而，并非所有特性都是有益的，因为我们还发现重建高级特性几乎不能提高模型的迁移能力，这表明在过滤域信息和保留图像全局结构之间存在权衡。总之，重建目标对于CDFSL任务很重要。基于上述发现和解释，我们进一步为CDFSL任务提出了领域无关掩码图像建模（DAMIM）。DAMIM包括一个聚合特征重建模块，该模块可自动聚合特征进行重建，平衡领域无关信息的学习与图像全局结构的平衡学习，以及一个轻量级解码器模块，进一步促进编码器的泛化能力。在四个CDFSL数据集上的实验表明，我们的方法达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19101v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探讨了跨域小样本学习（CDFSL）中的挑战，发现Masked Autoencoder（MAE）在处理显著域转移的CDFSL任务时性能下降。通过对现象的分析，本文发现MAE在重建像素时过于关注低级域信息，而改变重建目标为令牌特征可以缓解此问题。然而，并非所有特征都有益，重建高级特征对模型可迁移性的提升有限，这显示出在过滤域信息和保留图像全局结构之间存在权衡。基于此，本文提出了面向CDFSL任务的Domain-Agnostic Masked Image Modeling（DAMIM）。DAMIM包括一个聚合特征重建模块，用于自动聚合特征进行重建，并平衡学习领域无关信息和图像全局结构的学习；还包括一个轻量级解码器模块，以进一步促进编码器的泛化能力。实验证明，该方法在四个CDFSL数据集上取得了最新性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨域小样本学习（CDFSL）面临从数据丰富的源域转移到数据稀缺的目标域的挑战，其中显著的域差距使得任务更具挑战性。</li>
<li>Masked Autoencoder（MAE）在处理CDFSL任务时性能下降，尤其是在显著域转移的情况下。</li>
<li>MAE在重建像素时过于关注低级别域信息，改变重建目标至令牌特征能够缓解该问题。</li>
<li>在过滤域信息和保留图像全局结构之间存在权衡，高级特征的重建对模型可迁移性的提升有限。</li>
<li>论文提出了Domain-Agnostic Masked Image Modeling（DAMIM）方法，用于CDFSL任务。</li>
<li>DAMIM包括一个聚合特征重建模块，能自动聚合特征并平衡学习领域无关信息和图像全局结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1d2500a6aed9a0acf51d3e27772555c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1889972c98a31bbbc717052202aa1307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e1b20b71e864ef56cbf6f7ede65d8d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867e157b0c9c467034a7e2919e299677.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cbc2ef192b8519c41654a1a0127ec25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60a33590efd256ace0d2dba1710a9708.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-shot-Metric-Domain-Adaptation-Practical-Learning-Strategies-for-an-Automated-Plant-Disease-Diagnosis"><a href="#Few-shot-Metric-Domain-Adaptation-Practical-Learning-Strategies-for-an-Automated-Plant-Disease-Diagnosis" class="headerlink" title="Few-shot Metric Domain Adaptation: Practical Learning Strategies for an   Automated Plant Disease Diagnosis"></a>Few-shot Metric Domain Adaptation: Practical Learning Strategies for an   Automated Plant Disease Diagnosis</h2><p><strong>Authors:Shoma Kudo, Satoshi Kagiwada, Hitoshi Iyatomi</strong></p>
<p>Numerous studies have explored image-based automated systems for plant disease diagnosis, demonstrating impressive diagnostic capabilities. However, recent large-scale analyses have revealed a critical limitation: that the diagnostic capability suffers significantly when validated on images captured in environments (domains) differing from those used during training. This shortfall stems from the inherently limited dataset size and the diverse manifestation of disease symptoms, combined with substantial variations in cultivation environments and imaging conditions, such as equipment and composition. These factors lead to insufficient variety in training data, ultimately constraining the system’s robustness and generalization. To address these challenges, we propose Few-shot Metric Domain Adaptation (FMDA), a flexible and effective approach for enhancing diagnostic accuracy in practical systems, even when only limited target data is available. FMDA reduces domain discrepancies by introducing a constraint to the diagnostic model that minimizes the “distance” between feature spaces of source (training) data and target data with limited samples. FMDA is computationally efficient, requiring only basic feature distance calculations and backpropagation, and can be seamlessly integrated into any machine learning (ML) pipeline. In large-scale experiments, involving 223,015 leaf images across 20 fields and 3 crop species, FMDA achieved F1 score improvements of 11.1 to 29.3 points compared to cases without target data, using only 10 images per disease from the target domain. Moreover, FMDA consistently outperformed fine-tuning methods utilizing the same data, with an average improvement of 8.5 points. </p>
<blockquote>
<p>许多研究已经探讨了基于图像的植物疾病诊断自动化系统，并展示了令人印象深刻的诊断能力。然而，最近的大规模分析揭示了一个关键的局限性：当在不同于训练阶段使用的环境（领域）中捕获的图像上进行验证时，诊断能力会受到显著影响。这一缺陷源于数据集大小本身的限制、疾病症状表现的多样性，以及栽培环境、成像条件（如设备和组成）的显著差异。这些因素导致训练数据缺乏足够的多样性，最终限制系统的稳健性和泛化能力。为了解决这些挑战，我们提出了 Few-shot Metric Domain Adaptation（FMDA）方法，这是一种灵活有效的方法，即使在只有有限的目标数据可用的情况下，也可以提高实际系统中的诊断准确性。FMDA 通过在诊断模型中引入一个约束来减少领域差异，该约束最小化了源（训练）数据目标数据特征空间之间的“距离”，而目标数据仅包含有限样本。FMDA 计算效率高，只需进行基本特征距离计算和反向传播，可以无缝集成到任何机器学习（ML）管道中。在大规模实验中，涉及 20 个领域和 3 个作物种类的 223,015 张叶片图像，FMDA 在仅使用目标域每疾病 10 张图像的情况下，与无目标数据的情况相比，F1 分数的改进范围为 11.1 至 29.3 点。此外，FMDA 始终优于使用相同数据的微调方法，平均提高了 8.5 点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18859v1">PDF</a> 8 pages, 4 figures, 3 tables. Accepted at 4th Annual AAAI Workshop on   AI to Accelerate Science and Engineering (AI2ASE)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于图像的植物疾病诊断自动化系统在环境差异下的诊断能力受限问题。为此，提出了Few-shot Metric Domain Adaptation（FMDA）方法，通过最小化源（训练）数据特征空间与目标数据特征空间的“距离”，提高诊断模型的鲁棒性和泛化能力。FMDA方法计算效率高，可无缝融入任何机器学习管道。在大型实验中，FMDA使用仅来自目标域的少量图像（每疾病仅10张）实现了相较于无目标数据情况的F1分数提高11.1至29.3点，且优于使用相同数据的微调方法，平均提高8.5点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像自动化植物疾病诊断系统在环境差异下存在诊断能力受限的问题。</li>
<li>问题的根源在于数据集大小有限、疾病症状表现多样以及栽培环境和成像条件的变化。</li>
<li>引入Few-shot Metric Domain Adaptation（FMDA）方法，通过最小化源（训练）数据特征空间与目标数据特征空间的“距离”，提高诊断模型的鲁棒性和泛化能力。</li>
<li>FMDA方法计算效率高，可融入任何机器学习管道。</li>
<li>在大型实验中，FMDA在仅使用目标域的少量图像情况下，实现了显著的诊断准确性提高。</li>
<li>FMDA相较于传统方法有明显优势，尤其在缺乏大量目标数据的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f059d1fe7da7799819ef769f9e75ad9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d33cb090f6168ffe840dd16fe03171.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e738260b7c63d7918cb4ce5447db45eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5686fa923cb3e7afc57292af2ed0432a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ac8b0c09505b9520421bdd1e2d0e512.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-241355c945529d05cd24e2a5931a8ddb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation"><a href="#AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation" class="headerlink" title="AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation"></a>AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation</h2><p><strong>Authors:Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li</strong></p>
<p>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet">https://github.com/jarch-ma/AFANet</a>. </p>
<blockquote>
<p>少量学习旨在通过从少量样本中学习到的先验知识来识别新概念。然而，对于视觉密集型任务，如少量语义分割，像素级注释既耗时又成本高昂。因此，本文利用更具挑战性的图像级注释，并提出了一种自适应频率感知网络（AFANet）用于弱监督少量语义分割（WFSS）。具体来说，我们首先提出了跨粒度频率感知模块（CFM），它将RGB图像分解为高频和低频分布，并通过重新对齐进一步优化语义结构信息。与大多数现有WFSS方法不同，这些方法以离线学习方式使用多模态语言视觉模型的文本信息（例如CLIP），我们进一步提出了CLIP引导的空间适配器模块（CSM），该模块通过在线学习对文本信息进行空间域自适应转换，从而为CFM提供丰富的跨模态语义信息。在Pascal-5i和COCO-20i数据集上的大量实验表明，AFANet已达到最新性能水平。代码可在<a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jarch-ma/AFANet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17601v2">PDF</a> Accepted by TMM 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于自适应频率感知网络（AFANet）的弱监督少样本语义分割（WFSS）方法。该方法利用图像级标注，通过跨粒度频率感知模块（CFM）解耦图像的高频和低频分布，并通过CLIP引导的空间适配器模块（CSM）进行在线学习，实现空间域自适应转换，提高了跨模态语义信息。在Pascal-5i和COCO-20i数据集上的实验表明，AFANet达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本学习旨在通过从少量样本中学习到的先验知识来识别新概念。</li>
<li>针对视觉密集型任务如少样本语义分割，像素级标注是耗时且昂贵的。</li>
<li>本文使用更具挑战性的图像级标注，并提出自适应频率感知网络（AFANet）进行弱监督少样本语义分割（WFSS）。</li>
<li>AFANet包括跨粒度频率感知模块（CFM），将图像解耦为高频和低频分布，并通过对齐语义结构信息来进一步优化。</li>
<li>与大多数使用多模态语言视觉模型的离线学习方式不同，本文提出了CLIP引导的空间适配器模块（CSM），通过在线学习对文本信息进行空间域自适应转换。</li>
<li>实验结果表明，AFANet在Pascal-5i和COCO-20i数据集上达到了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8e3c275123d7cc725bbdac793e2b1c4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92b12f4ef3edec5343b33d63af5f2e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3c89546934a25924e4a0910914e013.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-912009cb34665b008f15f63397f9a37e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the model’s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>现有的少样本医学图像分割（FSMIS）模型无法解决医学成像中的一个实际问题：由不同成像技术引起的域偏移，这限制了其在当前FSMIS任务中的应用。为了克服这一局限性，我们专注于跨域少样本医学图像分割（CD-FSMIS）任务，旨在开发一种通用模型，该模型能够在新的目标域中有限标记数据的情况下，适应更广泛的医学图像分割场景。我们受到不同领域频率域相似性特征的启发，提出了一种频率感知匹配网络（FAMNet），它包括两个关键组件：频率感知匹配（FAM）模块和多光谱融合（MSF）模块。FAM模块解决了元学习阶段的两个问题：1）由于器官和病变的不同外观引起的域内方差导致的固有支持查询偏见；以及2）由于不同的医学成像技术引起的域间方差。此外，我们设计了一个MSF模块，以整合FAM模块解耦的不同频率特征，并进一步减轻域间方差对模型分割性能的影响。结合这两个模块，我们的FAMNet在三个跨域数据集上的表现超越了现有的FSMIS模型和跨域少样本语义分割模型，在CD-FSMIS任务中达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v4">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>本文关注跨域少样本医疗图像分割（CD-FSMIS）任务，旨在开发一个通用模型，能够在有限的新目标域标记数据的情况下，适应更广泛的医疗图像分割场景。为此，提出频率感知匹配网络（FAMNet），包括频率感知匹配（FAM）和多光谱融合（MSF）两个关键组件。该网络解决了元学习阶段的域内和域间方差问题，并在三个跨域数据集上超越了现有的FSMIS模型和跨域少样本语义分割模型，在CD-FSMIS任务中取得了最新性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有少样本医疗图像分割（FSMIS）模型面临因不同成像技术导致的域偏移问题，限制了其在现实任务中的应用。</li>
<li>提出跨域少样本医疗图像分割（CD-FSMIS）任务，旨在开发一个能适应更广泛医疗图像分割场景的通用模型，在有限的新目标域标记数据的情况下。</li>
<li>提出频率感知匹配网络（FAMNet），包含频率感知匹配（FAM）和多光谱融合（MSF）两个关键组件，以解决元学习阶段的域内和域间方差问题。</li>
<li>FAM模块解决了因不同器官和病变的外观造成的域内方差以及因不同医疗成像技术造成的域间方差问题。</li>
<li>MSF模块用于整合由FAM模块解耦的不同频率特征，进一步减轻域间方差对模型分割性能的影响。</li>
<li>FAMNet在三个跨域数据集上超越了现有的FSMIS模型和跨域少样本语义分割模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa98f3e63dd1dfa34a0fe8c49c6c6931.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0796f64fbec939deef49b781663f0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def190b9c5343be34695a04abbf24490.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Context-aware-Inductive-Knowledge-Graph-Completion-with-Latent-Type-Constraints-and-Subgraph-Reasoning"><a href="#Context-aware-Inductive-Knowledge-Graph-Completion-with-Latent-Type-Constraints-and-Subgraph-Reasoning" class="headerlink" title="Context-aware Inductive Knowledge Graph Completion with Latent Type   Constraints and Subgraph Reasoning"></a>Context-aware Inductive Knowledge Graph Completion with Latent Type   Constraints and Subgraph Reasoning</h2><p><strong>Authors:Muzhi Li, Cehao Yang, Chengjin Xu, Zixing Song, Xuhui Jiang, Jian Guo, Ho-fung Leung, Irwin King</strong></p>
<p>Inductive knowledge graph completion (KGC) aims to predict missing triples with unseen entities. Recent works focus on modeling reasoning paths between the head and tail entity as direct supporting evidence. However, these methods depend heavily on the existence and quality of reasoning paths, which limits their general applicability in different scenarios. In addition, we observe that latent type constraints and neighboring facts inherent in KGs are also vital in inferring missing triples. To effectively utilize all useful information in KGs, we introduce CATS, a novel context-aware inductive KGC solution. With sufficient guidance from proper prompts and supervised fine-tuning, CATS activates the strong semantic understanding and reasoning capabilities of large language models to assess the existence of query triples, which consist of two modules. First, the type-aware reasoning module evaluates whether the candidate entity matches the latent entity type as required by the query relation. Then, the subgraph reasoning module selects relevant reasoning paths and neighboring facts, and evaluates their correlation to the query triple. Experiment results on three widely used datasets demonstrate that CATS significantly outperforms state-of-the-art methods in 16 out of 18 transductive, inductive, and few-shot settings with an average absolute MRR improvement of 7.2%. </p>
<blockquote>
<p>归纳式知识图谱补全（KGC）旨在预测缺失的三元组与未见过的实体。近期的研究工作集中在将头实体和尾实体之间的推理路径建模为直接支持证据。然而，这些方法严重依赖于推理路径的存在和品质，这限制了它们在不同场景中的通用性。此外，我们观察到知识图谱中隐含的类型约束和邻近事实在推断缺失三元组中也是至关重要的。为了有效地利用知识图谱中的所有有用信息，我们引入了CATS，这是一种新型上下文感知归纳KGC解决方案。在适当的提示和经过监督精细调整的充足指导下，CATS激活了大型语言模型的强大语义理解和推理能力来评估查询三元组的存在与否，其由两个模块组成。首先，类型感知推理模块评估候选实体是否符合查询关系所需的潜在实体类型。然后，子图推理模块选择相关的推理路径和邻近事实，并评估它们与查询三元组的关联度。在三个广泛使用的数据集上的实验结果表明，CATS在18种转换、归纳和小样本设置中显著优于最先进的方法，平均绝对MRR值提高了7.2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16803v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于归纳知识图谱补全（KGC）的目标是对缺失的三元组进行预测，尤其是针对未见过的实体。当前方法主要聚焦于对头实体和尾实体之间的推理路径进行建模，作为直接的支持证据。然而，这些方法严重依赖于推理路径的存在和质量，限制了其在不同场景中的通用性。此外，知识图谱中隐性的类型约束和邻近事实对于推断缺失三元组也是至关重要的。为了有效利用知识图谱中的所有有用信息，我们提出了CATS这一新型上下文感知归纳KGC解决方案。通过适当的提示和精细监督微调，CATS能够激发大型语言模型的强大语义理解和推理能力，以评估查询三元组的存在性。该方案包含两个模块：类型感知推理模块评估候选实体是否符合查询关系所需的潜在实体类型；子图推理模块选择相关的推理路径和邻近事实，并评估其与查询三元组的关联度。在三个广泛使用的数据集上的实验结果表明，CATS在转导性、归纳性和少样本设置的16种场景下显著优于最先进的方法，平均绝对MRR值提高了7.2%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>归纳知识图谱补全旨在预测缺失的三元组，尤其是涉及未见实体的情形。</li>
<li>当前方法主要依赖推理路径进行建模，限制了其在不同场景中的通用性。</li>
<li>隐性类型约束和邻近事实在知识图谱中同样重要，对于推断缺失三元组至关重要。</li>
<li>CATS是一种新型上下文感知归纳KGC解决方案，旨在有效利用知识图谱中的所有信息。</li>
<li>CATS包含两个模块：类型感知推理模块和子图推理模块。</li>
<li>CATS通过适当的提示和精细监督微调，利用大型语言模型的语义理解和推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-945d84a6fc8a21118bc24438c8c5ce1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7dc9fdbe0bba78e1d9fd0c66a58f746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d3efa9d673e71717571bd345e914873.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9eea04177ab1b66fed7452ee8fc1b9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca287de3fc61a9c15db73dfe1c40683e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e077bfaa1a5970deb41fa4194d98bd67.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Federated-Learning-with-MMD-based-Early-Stopping-for-Adaptive-GNSS-Interference-Classification"><a href="#Federated-Learning-with-MMD-based-Early-Stopping-for-Adaptive-GNSS-Interference-Classification" class="headerlink" title="Federated Learning with MMD-based Early Stopping for Adaptive GNSS   Interference Classification"></a>Federated Learning with MMD-based Early Stopping for Adaptive GNSS   Interference Classification</h2><p><strong>Authors:Nishant S. Gaikwad, Lucas Heublein, Nisha L. Raichur, Tobias Feigl, Christopher Mutschler, Felix Ott</strong></p>
<p>Federated learning (FL) enables multiple devices to collaboratively train a global model while maintaining data on local servers. Each device trains the model on its local server and shares only the model updates (i.e., gradient weights) during the aggregation step. A significant challenge in FL is managing the feature distribution of novel and unbalanced data across devices. In this paper, we propose an FL approach using few-shot learning and aggregation of the model weights on a global server. We introduce a dynamic early stopping method to balance out-of-distribution classes based on representation learning, specifically utilizing the maximum mean discrepancy of feature embeddings between local and global models. An exemplary application of FL is to orchestrate machine learning models along highways for interference classification based on snapshots from global navigation satellite system (GNSS) receivers. Extensive experiments on four GNSS datasets from two real-world highways and controlled environments demonstrate that our FL method surpasses state-of-the-art techniques in adapting to both novel interference classes and multipath scenarios. </p>
<blockquote>
<p>联邦学习（FL）允许多个设备协同训练全局模型，同时保持数据存储在本地服务器上。每个设备都在其本地服务器上训练模型，仅在聚合步骤中共享模型更新（即梯度权重）。联邦学习面临的一个主要挑战是管理跨设备的新颖且不平衡数据的特征分布。在本文中，我们提出了一种使用少量样本学习和全局服务器上的模型权重聚合的联邦学习方法。我们引入了一种动态早期停止方法来平衡超出分布类别的表示学习，特别是通过利用本地和全局模型之间的特征嵌入的最大平均差异值来实现这一点。联邦学习的一个典型应用是在高速公路上协同组织机器学习模型，基于全球导航卫星系统（GNSS）接收器的快照进行干扰分类。在两条实际高速公路和控制环境中的四个GNSS数据集上的大量实验表明，我们的联邦学习方法在适应新颖干扰类别和多路径场景方面超越了最先进的技术手段。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15681v2">PDF</a> Git repository:   <a target="_blank" rel="noopener" href="https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/federated_learning">https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/federated_learning</a></p>
<p><strong>Summary</strong></p>
<p>联邦学习（FL）允许多个设备在本地服务器上协作训练全局模型的同时保持本地数据的安全。模型在本地服务器上训练，仅在聚合阶段共享模型更新（即梯度权重）。在FL中，管理新型和不平衡数据在设备间的特征分布是一大挑战。本文提出了一种基于联邦学习和少样本学习的模型权重聚合方法，引入了一种动态早期停止方法以平衡异常类特征分布。具体采用代表学习，基于局部和全局模型特征嵌入的最大均值差异来区分。此外，以全球导航卫星系统（GNSS）接收机的高速公路干扰分类为例展示了联邦学习的应用。实验证明，该联邦学习方法在适应新型干扰类和多径场景方面优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>联邦学习允许设备在本地训练模型并共享模型更新，以保护本地数据的安全。</li>
<li>管理新型和不平衡数据在设备间的特征分布是联邦学习的一个关键挑战。</li>
<li>提出了一种基于联邦学习和少样本学习的模型权重聚合方法。</li>
<li>动态早期停止方法被用来平衡异常类特征分布。该方法通过基于代表学习的最大均值差异来区分局部和全局模型的特征嵌入。</li>
<li>全球导航卫星系统（GNSS）接收机的干扰分类是联邦学习的一个典型应用示例。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-810090252909222be0b982630b65ac54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28bc092471cc9f294d534630e40005d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94cca7ebe7e473b704344d837d53be2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-448ab72adb3883080ade398f11e42a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4a5e361166d5466db13c6e20eac2bbe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi-scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve state-of-the-art results on benchmark datasets such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>少样本语义分割旨在解决仅使用少量标注样本对查询图像中的对象进行分割的挑战。然而，许多之前的最先进的方法要么不得不放弃复杂的局部语义特征，要么面临高计算复杂度的问题。为了解决这些挑战，我们提出了一种基于transformer架构的少样本语义分割新框架。我们的方法引入了空间变换解码器和上下文掩码生成模块，以提高支持图像和查询图像之间的关系理解。此外，我们引入了多尺度解码器，以分层的方式融入不同分辨率的特征来优化分割掩码。同时，我们的方法融合了中间编码器阶段的全局特征，以提高上下文理解，同时保持轻量级结构以降低复杂度。性能与效率之间的这种平衡使我们的方法在PASCAL-5i和COCO-20i等基准数据集上实现了最先进的1-shot和5-shot结果。值得注意的是，我们的模型仅有150万个参数，展示了具有竞争力的性能，同时克服了现有方法的局限性。可通过<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet%E4%BA%86%E8%A7%A3%E8%AF%A6%E6%83%85%E3%80%82">https://github.com/amirrezafateh/MSDNet了解详情。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Transformer架构的Few-shot语义分割框架，通过引入空间变换解码器、上下文掩膜生成模块和多尺度解码器，提高了对支持图像和查询图像之间关系的理解，实现了精细的语义分割，在PASCAL-5i和COCO-20i等基准数据集上取得了最先进的成果。该方法在保持高效的同时，实现了出色的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Few-shot语义分割面临挑战，需要仅使用少量标注样本对查询图像进行对象分割。</li>
<li>提出的基于Transformer架构的Few-shot语义分割框架，通过引入空间变换解码器、上下文掩膜生成模块改善关系理解。</li>
<li>多尺度解码器用于通过分层方式融入不同分辨率的特征，以优化分割掩膜。</li>
<li>集成中间编码器阶段的全球特征，以提高上下文理解，同时保持轻量级结构以降低复杂性。</li>
<li>该方法在基准数据集上实现最先进的成果，如PASCAL-5i和COCO-20i，在1-shot和5-shot设置下均表现优异。</li>
<li>与现有方法相比，该模型仅1.5百万参数，在保持竞争力的同时，克服了现有方法的局限性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7dfbf8a50110d45135537d58989963cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86f9eb0a4c352a4a2a0765cb5bfc2f60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4fa677efdc0fbf4dddf9a40b66fae11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55d5166f5ce99d76a8cae11e3ae5aff8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TAVP-Task-Adaptive-Visual-Prompt-for-Cross-domain-Few-shot-Segmentation"><a href="#TAVP-Task-Adaptive-Visual-Prompt-for-Cross-domain-Few-shot-Segmentation" class="headerlink" title="TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation"></a>TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation</h2><p><strong>Authors:Jiaqi Yang, Yaning Zhang, Jingxi Hu, Xiangjian He, Linlin Shen, Guoping Qiu</strong></p>
<p>While large visual models (LVM) demonstrated significant potential in image understanding, due to the application of large-scale pre-training, the Segment Anything Model (SAM) has also achieved great success in the field of image segmentation, supporting flexible interactive cues and strong learning capabilities. However, SAM’s performance often falls short in cross-domain and few-shot applications. Previous work has performed poorly in transferring prior knowledge from base models to new applications. To tackle this issue, we propose a task-adaptive auto-visual prompt framework, a new paradigm for Cross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature Fusion (MFF) was used for integrated feature extraction as prior knowledge. Besides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module to enable class-domain agnostic feature extraction and generate high-quality, learnable visual prompts. This significant advancement uses a unique generative approach to prompts alongside a comprehensive model structure and specialized prototype computation. While ensuring that the prior knowledge of SAM is not discarded, the new branch disentangles category and domain information through prototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across four cross-domain datasets demonstrate that our model outperforms the state-of-the-art CD-FSS approach, achieving an average accuracy improvement of 1.3% in the 1-shot setting and 11.76% in the 5-shot setting. </p>
<blockquote>
<p>虽然大型视觉模型（LVM）在图像理解方面表现出了巨大的潜力，并得益于大规模预训练的应用，但在图像分割领域，Segment Anything Model（SAM）也取得了巨大的成功，它支持灵活的交互提示和强大的学习能力。然而，SAM在跨域和少样本应用中的表现往往不尽如人意。以前的工作在将先验知识从基础模型转移到新应用上的效果很差。为了解决这个问题，我们提出了一种任务自适应自动视觉提示框架，这是一种用于跨域少样本分割（CD-FSS）的新范式。首先，我们使用多级特征融合（MFF）进行集成特征提取作为先验知识。此外，我们结合了类域任务自适应自动提示（CDTAP）模块，以实现类域无关的特征提取，并生成高质量、可学习的视觉提示。这项重大进展采用了一种独特的生成提示方法，并配备了全面的模型结构和专门的原型计算。在保留SAM的先验知识的同时，新分支通过原型解纠缠类别和域信息，引导其适应CD-FSS。在四个跨域数据集上的综合实验表明，我们的模型优于最先进的CD-FSS方法，在1次拍摄的情况下平均准确率提高1.3%，在5次拍摄的情况下平均准确率提高11.76%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05393v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型视觉模型（LVM）在图像理解方面展现出巨大潜力，而Segment Anything Model（SAM）在图像分割领域也取得了成功，具有灵活交互提示和强大学习能力。但在跨域和少镜头应用中，SAM表现欠佳。为解决这个问题，我们提出了任务自适应自动视觉提示框架，这是一种新的跨域少镜头分割（CD-FSS）范式。我们使用多级别特征融合（MFF）进行特征提取，并结合类域任务自适应自动提示（CDTAP）模块，生成高质量、可学习的视觉提示。实验表明，我们的模型在四个跨域数据集上优于最先进CD-FSS方法，在单镜头设置下平均精度提高1.3%，在五镜头设置下提高11.76%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉模型（LVM）在图像理解方面表现出显著潜力。</li>
<li>Segment Anything Model（SAM）在图像分割领域取得成功，但其在跨域和少镜头应用中表现不足。</li>
<li>提出的任务自适应自动视觉提示框架是一种新的跨域少镜头分割（CD-FSS）方法。</li>
<li>多级别特征融合（MFF）用于集成特征提取作为先验知识。</li>
<li>类域任务自适应自动提示（CDTAP）模块可生成高质量、可学习的视觉提示。</li>
<li>模型在四个跨域数据集上的表现优于现有CD-FSS方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4541d57fc7cef23263f894fdb4ab19c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10b5094632539bd64ce887a7ddf2dfae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1a58eb1d51be19d717063965e48a6d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c92f6aeb75f67bae2eec0ead5ca1128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b07a91a16c6948fedbdf391cc9cd89fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EHRCon-Dataset-for-Checking-Consistency-between-Unstructured-Notes-and-Structured-Tables-in-Electronic-Health-Records"><a href="#EHRCon-Dataset-for-Checking-Consistency-between-Unstructured-Notes-and-Structured-Tables-in-Electronic-Health-Records" class="headerlink" title="EHRCon: Dataset for Checking Consistency between Unstructured Notes and   Structured Tables in Electronic Health Records"></a>EHRCon: Dataset for Checking Consistency between Unstructured Notes and   Structured Tables in Electronic Health Records</h2><p><strong>Authors:Yeonsu Kwon, Jiho Kim, Gyubok Lee, Seongsu Bae, Daeun Kyung, Wonchul Cha, Tom Pollard, Alistair Johnson, Edward Choi</strong></p>
<p>Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 4,101 entities across 105 clinical notes checked against database entries for consistency. EHRCon has two versions, one using the original MIMIC-III schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/dustn1259/EHRCon">https://github.com/dustn1259/EHRCon</a>. </p>
<blockquote>
<p>电子健康记录（EHRs）对于存储全面的患者医疗记录至关重要，它结合了结构化数据（例如药物信息）和详细的临床笔记（例如医生笔记）。这些要素对于直接的数据检索至关重要，并为患者护理提供了深入、具体的见解。然而，由于电子健康记录系统的不直观设计和人为错误，它们常常会出现差异，给患者的安全带来严重风险。为了解决这个问题，我们开发了EHRCon，这是一个专门设计的新数据集和任务，旨在确保电子健康记录中结构化表格和未结构化笔记之间的数据一致性。EHRCon是与医疗保健专业人员合作，利用MIMIC-III电子健康记录数据集制作的，其中包括对数据库条目的一致性检查的105个临床笔记中的4,101个实体的手动注释。EHRCon有两个版本，一个使用原始的MIMIC-III模式，另一个使用OMOP CDM模式，以增加其适用性和通用性。此外，利用大型语言模型的能力，我们推出了CheckEHR，这是一个验证临床笔记和数据库表格之间一致性的新框架。CheckEHR采用一个八阶段的流程，并在小样和零样本环境中都显示出有希望的结果。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/dustn1259/EHRCon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dustn1259/EHRCon找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16341v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>电子健康记录（EHRs）融合了结构化数据和非结构化临床笔记，是患者医疗记录的关键存储工具。然而，由于EHR系统设计的非直观性和人为错误，数据一致性存在风险。为解决这一问题，我们开发了EHRCon数据集和任务，确保EHR中结构化表格与未结构化笔记之间的数据一致性。EHRCon与医疗保健专业人士合作创建，包括在数据库条目中手动注释的4,101个实体的一致性检查。此外，我们利用大型语言模型的能力，推出了CheckEHR框架，用于验证临床笔记和数据库表格之间的一致性。CheckEHR展示出在少量甚至无样本的情况下具有应用前景。其代码已公开。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>电子健康记录（EHRs）结合了结构化数据和非结构化临床笔记，为患者医疗记录提供了全面存储方案。</li>
<li>EHRs因系统设计的非直观性和人为错误常常出现数据不一致的问题，给患者安全带来风险。</li>
<li>开发EHRCon数据集和任务是为了确保EHR中数据的一致性。该数据集由医疗保健专业人士合作创建，并包括在数据库条目中手动注释的实体一致性检查。</li>
<li>EHRCon有两个版本，分别使用原始的MIMIC-III模式和OMOP CDM模式，以提高其适用性和通用性。</li>
<li>CheckEHR框架被用于验证临床笔记和数据库表格之间的一致性，它通过利用大型语言模型的能力实现了这一目标。</li>
<li>CheckEHR在少量样本甚至无样本的情况下表现出良好的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16341">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dafe4f9617c6741023c4ed99613be7cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971937f7c85511d62c62ea0a01eea71b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-323158506eca6384d54f1930487b5e30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a286af03a871bc25b5962f6a81fa0f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f532996c41e8376424156660f3dfaacc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>本体匹配（OM）通过使不同本体之间实现语义互操作性，并通过对齐相关实体解决其概念上的异质性。目前，OM系统主要有两种流行的设计范式：传统的基于知识的专家系统和较新的基于机器学习的预测系统。虽然大型语言模型（LLM）和LLM代理已经彻底改变了数据工程，并在许多领域得到了创造性的应用，但它们在OM中的潜力尚未得到充分探索。本研究介绍了一种基于代理的LLM设计范式的OM系统。考虑到利用LLM代理进行OM面临的若干特定挑战，我们提出了一个通用框架，即Agent-OM（用于本体匹配的代理），该框架包含两个用于检索和匹配的Siamese代理以及一组OM工具。我们的框架在一个概念验证系统中实现。对三个本体对齐评估倡议（OAEI）赛道上的最新OM系统的评估表明，我们的系统在简单OM任务上的结果非常接近长期以来的最佳性能，并在复杂和少样本OM任务上可以显著提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v6">PDF</a> 19 pages, 12 figures, 3 tables</p>
<p><strong>总结</strong><br>     本研究介绍了一种新型基于LLM代理的OM系统设计范式。针对利用LLM代理进行OM的特定挑战，提出了一个通用的Agent-OM框架，包含两个用于检索和匹配的Siamese代理和一组OM工具。在证明概念的系统上实现该框架，并通过与先进的OM系统的评价对比，结果表明该系统在简单OM任务上表现接近最佳水平，并在复杂和少样本OM任务上显著提高性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本研究提出了一种新的基于LLM代理的OM系统设计范式。</li>
<li>介绍了针对利用LLM代理进行OM的特定挑战而设计的通用Agent-OM框架。</li>
<li>Agent-OM框架包含两个用于检索和匹配的Siamese代理和一组OM工具。</li>
<li>该框架在证明概念的系统上进行了实现。</li>
<li>与先进的OM系统的评价对比显示，该系统在简单OM任务上表现接近最佳水平。</li>
<li>在复杂和少样本OM任务上，该系统显著提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e94f310dc617e93193eaa255421342af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3be1f670b4b3e5e54e8dc998918fda9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cde61049b12372f9a36b0b53054ed8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e27c225a871ec7ed4833fed181d2f8e7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Knowledge-Graph-Construction-and-Reasoning-Recent-Capabilities-and-Future-Opportunities"><a href="#LLMs-for-Knowledge-Graph-Construction-and-Reasoning-Recent-Capabilities-and-Future-Opportunities" class="headerlink" title="LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities   and Future Opportunities"></a>LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities   and Future Opportunities</h2><p><strong>Authors:Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang</strong></p>
<p>This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs’ performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs. The code and datasets are in <a target="_blank" rel="noopener" href="https://github.com/zjunlp/AutoKG">https://github.com/zjunlp/AutoKG</a>. </p>
<blockquote>
<p>本文全面定量和定性地评估了大型语言模型（LLM）在知识图谱（KG）构建和推理中的应用。我们在八个不同的数据集上进行了实验，重点研究四个代表性任务，包括实体和关系抽取、事件抽取、链接预测和问答，从而全面探索LLM在构建和推理领域的性能。实证研究结果表明，以GPT-4为代表的大型语言模型更适合作为推理助手，而非少数情况下的信息提取器。具体而言，GPT-4在知识图谱构建相关任务上表现良好，在推理任务上更是表现出卓越的性能，在某些情况下甚至超过了微调模型。此外，我们的调查还扩展到大型语言模型在信息提取方面的潜在泛化能力，从而提出了虚拟知识提取任务并开发了相应的VINE数据集。基于这些实证发现，我们进一步提出了AutoKG，这是一种基于多代理的方法，利用大型语言模型和外部资源来进行知识图谱的构建和推理。我们预期这项研究能为未来知识图谱领域的研究提供宝贵的见解。代码和数据集位于<a target="_blank" rel="noopener" href="https://github.com/zjunlp">https://github.com/zjunlp</a> 可以在这里找到代码和数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.13168v4">PDF</a> World Wide Web Journal</p>
<p><strong>Summary</strong></p>
<p>本文全面评估了大型语言模型（LLMs）在知识图谱（KG）构建和推理方面的表现。实验涉及八个不同数据集，涵盖实体和关系抽取、事件抽取、链接预测和问答等四个代表性任务，深入探讨了LLMs在构建和推理领域的性能。研究发现，GPT-4等LLMs更适合作为推理助手而非少样本信息提取器。在KG构建相关任务中表现良好，在推理任务中更是表现出超越微调模型的实力。此外，研究还扩展了LLMs在信息提取方面的潜在泛化能力，提出了虚拟知识提取任务及相应的VINE数据集。基于这些发现，本文进一步提出了AutoKG，一种基于多代理的方法，利用LLMs和外部资源用于KG构建和推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文对大型语言模型在知识图谱构建和推理方面进行了全面评估。</li>
<li>实验涉及多个数据集和代表性任务，包括实体和关系抽取、事件抽取、链接预测和问答。</li>
<li>GPT-4等LLMs更适合作为推理助手，而非少样本信息提取器。</li>
<li>在知识图谱构建任务中，GPT-4表现良好，尤其在推理任务中表现卓越，有时超越微调模型。</li>
<li>研究扩展了LLMs在信息提取方面的潜在泛化能力。</li>
<li>论文提出了虚拟知识提取任务及相应的VINE数据集。</li>
<li>基于实证发现，论文进一步提出了AutoKG，一种基于多代理的方法，利用LLMs和外部资源进行KG构建和推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.13168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76d0f5184449764f1a13ec16feb72f11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88165b4ff15dda86312ad14f379c880f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ac2e5b25845df57f9f7ae6eacab784d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90557d01fd4d2e8de91b2951f475dc79.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a8c203cf03eeb977522b9b30ca524e1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-01-02  ReFlow6D Refraction-Guided Transparent Object 6D Pose Estimation via   Intermediate Representation Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d420af9d7c6e4b553c221ca8efb92b25.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-02  Distributed Mixture-of-Agents for Edge Inference with Large Language   Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
