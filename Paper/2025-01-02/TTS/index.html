<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-01-02  Stable-TTS Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6181bb6b6954dfacf861d6a04e3e5350.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-02-更新"><a href="#2025-01-02-更新" class="headerlink" title="2025-01-02 更新"></a>2025-01-02 更新</h1><h2 id="Stable-TTS-Stable-Speaker-Adaptive-Text-to-Speech-Synthesis-via-Prosody-Prompting"><a href="#Stable-TTS-Stable-Speaker-Adaptive-Text-to-Speech-Synthesis-via-Prosody-Prompting" class="headerlink" title="Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting"></a>Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting</h2><p><strong>Authors:Wooseok Han, Minki Kang, Changhun Kim, Eunho Yang</strong></p>
<p>Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable attention due to its broad range of applications, such as personalized voice assistant services. While several approaches have been proposed, they often exhibit high sensitivity to either the quantity or the quality of target speech samples. To address these limitations, we introduce Stable-TTS, a novel speaker-adaptive TTS framework that leverages a small subset of a high-quality pre-training dataset, referred to as prior samples. Specifically, Stable-TTS achieves prosody consistency by leveraging the high-quality prosody of prior samples, while effectively capturing the timbre of the target speaker. Additionally, it employs a prior-preservation loss during fine-tuning to maintain the synthesis ability for prior samples to prevent overfitting on target samples. Extensive experiments demonstrate the effectiveness of Stable-TTS even under limited amounts of and noisy target speech samples. </p>
<blockquote>
<p>语音自适应文本转语音（TTS）合成因其广泛的应用领域，如个性化语音助手服务，而备受关注。尽管已经提出了多种方法，但它们对目标语音样本的数量或质量往往表现出较高的敏感性。为了解决这些局限性，我们引入了Stable-TTS，这是一种新型的语音自适应TTS框架，它利用高质量预训练数据集的一个小子集，称为先验样本。具体来说，Stable-TTS通过利用先验样本的高质量韵律来实现韵律一致性，同时有效地捕捉目标说话者的音色。此外，它在微调过程中采用先验保持损失，以保持对先验样本的合成能力，防止对目标样本的过拟合。大量实验表明，即使在有限的和嘈杂的目标语音样本下，Stable-TTS也是有效的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20155v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的面向演讲者的自适应文本转语音（TTS）合成框架——Stable-TTS。它通过利用高质量预训练数据集中的一小部分先验样本，实现了对目标演讲者语调的一致性和音质的捕捉。此外，Stable-TTS在微调过程中采用先验保留损失，以保持对先验样本的合成能力，防止对目标样本的过拟合。即使在目标语音样本有限且噪声较大的情况下，Stable-TTS也表现出良好的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Stable-TTS是一种面向演讲者的自适应TTS合成框架，旨在解决现有方法对于目标语音样本数量或质量的敏感性。</li>
<li>该框架利用预训练数据集中的先验样本，实现语调一致性和音质的捕捉。</li>
<li>通过在微调过程中采用先验保留损失，Stable-TTS能够保持对先验样本的合成能力。</li>
<li>Stable-TTS在目标语音样本有限和噪声较大的情况下也表现出良好的性能。</li>
<li>该框架能够实现个性化的语音助手服务等多种广泛应用。</li>
<li>Stable-TTS对于提高TTS合成的稳定性和性能具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7e8b0a0a93431b4cae192ffc8b239570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a537d6e5113dfd1c2ab103449257161b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e5b0c83131ba27f8cc6fb3f9fbf0074.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f0c8c1c25d5b7a06ea5e6d1e705abb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee9d6718b4a17eb3a7967c1768468065.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-415365a7479de602797bfaeb9871cea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86274cbea79770c1acd9d0dfc1666e04.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation"><a href="#CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation" class="headerlink" title="CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation"></a>CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation</h2><p><strong>Authors:Ji-Hoon Kim, Hong-Sun Yang, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Joon Son Chung</strong></p>
<p>The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin. </p>
<blockquote>
<p>本文的目标是在保持相同说话人身份的同时生成多种语言的自然语音，这一任务被称为跨语言语音合成。跨语言语音合成的一个关键挑战是语言与说话人的纠缠问题，导致跨语言系统的质量落后于单语言系统。针对这一问题，我们提出了CrossSpeech++，它能够有效地解开语言和说话人信息，显著提高跨语言语音合成的质量。为此，我们将复杂的语音生成管道分解为两个简单的组件：语言相关生成器和说话人相关生成器。语言相关生成器产生不受特定说话人属性影响的语言变化。说话人相关生成器则模拟能够体现说话人身份的声学变化。通过在不同的模块中处理每种类型的信息，我们的方法可以有效地解开语言和说话人的表示。我们通过使用各种度量标准进行了大量实验，证明了CrossSpeech++在跨语言语音合成方面取得了显著改进，大大优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20048v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本提出了CrossSpeech++方案，旨在解决跨语言语音合成中的语言与说话人纠缠问题，从而提高跨语言语音合成的质量。该方案将复杂的语音生成管道简化为两个组件：语言相关生成器和说话人相关生成器。前者产生不受特定说话人属性影响的语言变化，后者模拟表征说话人身份的声学变化。通过单独处理每种类型的信息，该方法可以有效地解开语言和说话人的表示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CrossSpeech++旨在解决跨语言语音合成中的语言与说话人纠缠问题。</li>
<li>该方案通过分离语言相关和说话人相关信息来提高跨语言语音合成的质量。</li>
<li>CrossSpeech++将复杂的语音生成管道简化为两个组件：语言相关生成器和说话人相关生成器。</li>
<li>语言相关生成器产生的语言变化不受特定说话人属性的影响。</li>
<li>说话人相关生成器模拟表征说话人身份的声学变化。</li>
<li>通过单独处理每种类型的信息，CrossSpeech++能够更有效地处理语言和说话人的表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a144310e8743d76fe946abe22982a1c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b27c17333f4be5fcd573abf6e8c687e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71a3e4b4f613f066ba8c895460a3ea06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-301cbe8ec42efceaaf9a0db2aa09d03b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f9d189a4c973d41d917039da036b142.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VoiceDiT-Dual-Condition-Diffusion-Transformer-for-Environment-Aware-Speech-Synthesis"><a href="#VoiceDiT-Dual-Condition-Diffusion-Transformer-for-Environment-Aware-Speech-Synthesis" class="headerlink" title="VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware   Speech Synthesis"></a>VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware   Speech Synthesis</h2><p><strong>Authors:Jaemin Jung, Junseok Ahn, Chaeyoung Jung, Tan Dat Nguyen, Youngjoon Jang, Joon Son Chung</strong></p>
<p>We present VoiceDiT, a multi-modal generative model for producing environment-aware speech and audio from text and visual prompts. While aligning speech with text is crucial for intelligible speech, achieving this alignment in noisy conditions remains a significant and underexplored challenge in the field. To address this, we present a novel audio generation pipeline named VoiceDiT. This pipeline includes three key components: (1) the creation of a large-scale synthetic speech dataset for pre-training and a refined real-world speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to efficiently preserve aligned speech information while accurately reflecting environmental conditions, and (3) a diffusion-based Image-to-Audio Translator that allows the model to bridge the gap between audio and image, facilitating the generation of environmental sound that aligns with the multi-modal prompts. Extensive experimental results demonstrate that VoiceDiT outperforms previous models on real-world datasets, showcasing significant improvements in both audio quality and modality integration. </p>
<blockquote>
<p>我们提出了VoiceDiT，这是一种多模态生成模型，能够根据文本和视觉提示生成对环境感知的语音和音频。虽然将语音与文本对齐对于可理解的语音至关重要，但在嘈杂的条件下实现这种对齐仍然是该领域一个重大且未被充分研究的挑战。为了解决这一问题，我们提出了一种名为VoiceDiT的新型音频生成管道。该管道包括三个关键组件：（1）创建用于预训练的大规模合成语音数据集和用于精细调整的真实世界语音数据集；（2）Dual-DiT模型，该模型旨在高效保留对齐的语音信息，同时准确反映环境条件；（3）基于扩散的Image-to-Audio Translator，使模型能够弥合音频和图像之间的差距，促进与环境声音的生成，这与多模态提示相吻合。大量的实验结果表明，VoiceDiT在真实世界数据集上的表现优于以前的模型，在音频质量和模态集成方面都显示出显着改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19259v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>文本介绍了一项名为VoiceDiT的多模态生成模型，该模型可以从文本和视觉提示中生成环境感知语音和音频。为解决噪声条件下语音与文本对齐的难题，该模型采用新型音频生成流程，包括大规模合成语音数据集用于预训练和优化现实世界语音数据集进行微调、设计的双DiT模型能够高效保留对齐语音信息并准确反映环境条件，以及基于扩散的图像到音频翻译器，可缩小音频与图像之间的差距，便于生成与环境提示相符的音频。实验结果表明，VoiceDiT在现实世界数据集上的表现优于先前模型，在音频质量和模态集成方面取得了显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VoiceDiT是一个多模态生成模型，可以从文本和视觉提示生成环境感知的语音和音频。</li>
<li>噪声条件下语音与文本对齐是语音合成领域的重要挑战。</li>
<li>VoiceDiT采用新型音频生成流程，包括预训练和微调阶段使用的大规模数据集。</li>
<li>双DiT模型能够高效保留对齐语音信息并反映环境条件。</li>
<li>扩散图像到音频翻译器有助于生成与环境提示相符的音频。</li>
<li>VoiceDiT在现实世界数据集上的表现优于先前模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19259">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4c03d5da319abf2cd9dc6cd3ec4eee76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6264cf236ca280f52339d7684df81d13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02cdf3af59dbf9fa385acd356726a4d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddee717a40502907bdceef316cbeeb7f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="“I’ve-Heard-of-You-”-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities"><a href="#“I’ve-Heard-of-You-”-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities" class="headerlink" title="“I’ve Heard of You!”: Generate Spoken Named Entity Recognition Data for   Unseen Entities"></a>“I’ve Heard of You!”: Generate Spoken Named Entity Recognition Data for   Unseen Entities</h2><p><strong>Authors:Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su</strong></p>
<p>Spoken named entity recognition (NER) aims to identify named entities from speech, playing an important role in speech processing. New named entities appear every day, however, annotating their Spoken NER data is costly. In this paper, we demonstrate that existing Spoken NER systems perform poorly when dealing with previously unseen named entities. To tackle this challenge, we propose a method for generating Spoken NER data based on a named entity dictionary (NED) to reduce costs. Specifically, we first use a large language model (LLM) to generate sentences from the sampled named entities and then use a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce a noise metric to filter out noisy data. To evaluate our approach, we release a novel Spoken NER benchmark along with a corresponding NED containing 8,853 entities. Experiment results show that our method achieves state-of-the-art (SOTA) performance in the in-domain, zero-shot domain adaptation, and fully zero-shot settings. Our data will be available at <a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU">https://github.com/DeepLearnXMU/HeardU</a>. </p>
<blockquote>
<p>语音命名实体识别（NER）旨在从语音中识别命名实体，在语音识别处理中扮演着重要角色。每天都有新的命名实体出现，然而，对语音NER数据进行标注的成本很高。在本文中，我们展示了现有的语音NER系统在处理之前未见过的命名实体时表现不佳。为了应对这一挑战，我们提出了一种基于命名实体词典（NED）生成语音NER数据的方法以降低成本。具体来说，我们首先使用大型语言模型（LLM）从采样的命名实体生成句子，然后使用文本到语音（TTS）系统生成语音。此外，我们还引入了一个噪声度量来过滤掉噪声数据。为了评估我们的方法，我们发布了一个新的语音NER基准测试，以及一个包含8853个实体的相应NED。实验结果表明，我们的方法在域内、零镜头域适应和完全零镜头设置中都达到了最先进的性能。我们的数据将在<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepLearnXMU/HeardU上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19102v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>语音命名实体识别（Spoken NER）旨在从语音中识别命名实体，在语音识别处理中占据重要地位。现有系统在面对未见过的命名实体时表现不佳，标注语音NER数据成本高昂。本研究提出一种基于命名实体词典（NED）生成语音NER数据的方法降低成本。利用大型语言模型（LLM）从采样命名实体生成句子，再通过文本到语音（TTS）系统生成语音。此外，引入噪声度量以过滤噪声数据。研究发布新的语音NER基准与包含8,853个实体的对应NED。实验结果证明该方法在领域内、跨领域适应以及全零样本设置下均达到最新水平。相关数据将公开于<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E3%80%82">https://github.com/DeepLearnXMU/HeardU。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音命名实体识别（Spoken NER）在语音识别处理中很重要，面临标注成本高昂和面对未见实体识别性能下降的挑战。</li>
<li>提出一种基于命名实体词典（NED）生成语音NER数据的方法，降低成本并提高性能。</li>
<li>利用大型语言模型（LLM）从采样命名实体生成句子，再通过文本到语音（TTS）系统转化。</li>
<li>引入噪声度量机制以过滤生成的噪声数据。</li>
<li>发布新的语音NER基准与对应NED，包含大量实体数据。</li>
<li>实验结果显示该方法在多种设置下达到最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b86e890486171362a149be8408a2e3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5aa85d1b8ed0df6adba19264fc7d5a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33eef7680a3f22388849094bd1a0991b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-179f247ff97294ecfdc487eec3257cc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd1ba9e961a6bbb71566fea9c46a06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515c695c189641e0236cc39dcb4c07fc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Advancing-NAM-to-Speech-Conversion-with-Novel-Methods-and-the-MultiNAM-Dataset"><a href="#Advancing-NAM-to-Speech-Conversion-with-Novel-Methods-and-the-MultiNAM-Dataset" class="headerlink" title="Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM   Dataset"></a>Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM   Dataset</h2><p><strong>Authors:Neil Shah, Shirish Karande, Vineet Gandhi</strong></p>
<p>Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning to simulate ground-truth speech from paired whispers. However, the simulated speech often lacks intelligibility and fails to generalize well across different speakers. To address this issue, we focus on learning phoneme-level alignments from paired whispers and text and employ a Text-to-Speech (TTS) system to simulate the ground-truth. To reduce dependence on whispers, we learn phoneme alignments directly from NAMs, though the quality is constrained by the available training data. To further mitigate reliance on NAM&#x2F;whisper data for ground-truth simulation, we propose incorporating the lip modality to infer speech and introduce a novel diffusion-based method that leverages recent advancements in lip-to-speech technology. Additionally, we release the MultiNAM dataset with over $7.96$ hours of paired NAM, whisper, video, and text data from two speakers and benchmark all methods on this dataset. Speech samples and the dataset are available at \url{<a target="_blank" rel="noopener" href="https://diff-nam.github.io/DiffNAM/%7D">https://diff-nam.github.io/DiffNAM/}</a> </p>
<blockquote>
<p>当前的非声低语（NAM）到语音技术依赖于语音克隆，通过模拟配对低语的原始语音来实现。然而，模拟的语音往往缺乏清晰度，在不同发音者之间的泛化效果较差。为了解决这个问题，我们专注于从配对的低语和文本中学习音素级别的对齐，并采用文本到语音（TTS）系统来模拟原始语音。为了减少对低语的依赖，我们直接从NAM学习音素对齐，尽管质量受到可用训练数据的限制。为了进一步减少对NAM&#x2F;低语数据在模拟原始语音方面的依赖，我们提出利用唇型模式进行语音推断，并引入一种新型的基于扩散的方法，该方法利用最新的唇型到语音技术。此外，我们发布了MultiNAM数据集，其中包含来自两个发音者的超过7.96小时的配对NAM、低语、视频和文本数据，并在该数据集上对所有方法进行了基准测试。语音样本和数据集可通过<a target="_blank" rel="noopener" href="https://diff-nam.github.io/DiffNAM/%E8%8E%B7%E5%8F%96%E3%80%82">https://diff-nam.github.io/DiffNAM/获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18839v1">PDF</a> Accepted at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对现有非语音嘟囔（NAM）到语音转换技术的问题，提出了一种新的方法。通过从配对嘟囔和文本中学习音素级别的对齐，并使用文本到语音（TTS）系统模拟真实语音。为了减少对嘟囔的依赖，直接从NAM中学习音素对齐，但质量受限于可用训练数据。为了进一步提高对真实语音模拟的可靠性，文章结合了唇部特征并引入了一种基于扩散的方法，利用最新的唇部到语音技术。此外，文章还发布了包含超过7.96小时配对NAM、嘟囔、视频和文本数据的MultiNAM数据集，并在该数据集上对所有方法进行了基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前非语音嘟囔（NAM）到语音转换技术主要依赖于声音克隆来模拟真实语音。</li>
<li>现有的模拟语音常常缺乏清晰度和在不同发言人之间的泛化能力。</li>
<li>新的方法专注于从配对嘟囔和文本中学习音素级别的对齐，并使用TTS系统模拟真实语音以提高清晰度。</li>
<li>直接从NAM中学习音素对齐是一种减少对嘟囔依赖的方法，但受限于训练数据的可用性。</li>
<li>为了更可靠地模拟真实语音，结合了唇部特征并引入了一种基于扩散的方法。</li>
<li>发布了一个名为MultiNAM的数据集，包含NAM、嘟囔、视频和文本数据，为基准测试提供了资源。</li>
<li>基准测试显示各种方法在MultiNAM数据集上的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18839">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b474c1019184b0d8fd921adbd27f6682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e86e8019b5ee6177011bd29524a7f563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e12cf4b934d2c9fb8fc3144e57e776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09c9fa270829f7ec68144943ab865760.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MRI2Speech-Speech-Synthesis-from-Articulatory-Movements-Recorded-by-Real-time-MRI"><a href="#MRI2Speech-Speech-Synthesis-from-Articulatory-Movements-Recorded-by-Real-time-MRI" class="headerlink" title="MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by   Real-time MRI"></a>MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by   Real-time MRI</h2><p><strong>Authors:Neil Shah, Ayan Kashyap, Shirish Karande, Vineet Gandhi</strong></p>
<p>Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily on noisy ground-truth speech. Applying loss directly over ground truth mel-spectrograms entangles speech content with MRI noise, resulting in poor intelligibility. We introduce a novel approach that adapts the multi-modal self-supervised AV-HuBERT model for text prediction from rtMRI and incorporates a new flow-based duration predictor for speaker-specific alignment. The predicted text and durations are then used by a speech decoder to synthesize aligned speech in any novel voice. We conduct thorough experiments on two datasets and demonstrate our method’s generalization ability to unseen speakers. We assess our framework’s performance by masking parts of the rtMRI video to evaluate the impact of different articulators on text prediction. Our method achieves a $15.18%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus, marking a huge improvement over the current state-of-the-art. Speech samples are available at \url{<a target="_blank" rel="noopener" href="https://mri2speech.github.io/MRI2Speech/%7D">https://mri2speech.github.io/MRI2Speech/}</a> </p>
<blockquote>
<p>之前基于实时磁共振成像（rtMRI）的语音合成模型很大程度上依赖于嘈杂的真实语音。直接在真实mel频谱图上应用损失会使语音内容与磁共振成像噪声混淆，导致语音清晰度降低。我们引入了一种新方法，该方法自适应多模态自监督AV-HuBERT模型，用于从rtMRI预测文本，并融入新的基于流技术的持续时间预测器，以实现针对特定说话人的对齐。预测的文本和持续时间随后被语音解码器用于合成任何新颖声音的语音。我们在两个数据集上进行了全面的实验，证明了我们的方法对于未见过的说话人的泛化能力。我们通过遮挡rtMRI视频的部分内容来评估我们的框架性能，以评估不同发音器对文本预测的影响。我们的方法在USC-TIMIT MRI语料库上实现了15.18%的单词错误率（WER），相较于当前最新技术，这是一个巨大的改进。语音样本可在[<a target="_blank" rel="noopener" href="https://mri2speech.github.io/MRI2Speech/]%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://mri2speech.github.io/MRI2Speech/]上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18836v1">PDF</a> Accepted at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>基于实时磁共振成像（rtMRI）的语音合成模型过去依赖于噪声较大的真实语音。直接在真实mel频谱图上应用损失会导致语音内容与磁共振成像噪声混淆，降低语音清晰度。本研究引入了一种新型多模态自监督AV-HuBERT模型，用于从rtMRI预测文本，并融入新的基于流的时长预测器进行特定于说话者的对齐。预测的文本和时长随后被语音解码器用于合成任何新颖语音。在两项数据集上的实验证明，该方法能够推广到未见过的说话者。本研究通过遮挡rtMRI视频的部分内容来评估文本预测的影响，该方法在USC-TIMIT MRI语料库上实现了15.18%的词错误率（WER），较当前先进技术有很大改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实时MRI语音合成依赖噪声真实语音的问题。</li>
<li>直接在真实mel频谱图上应用损失会导致语音内容和MRI噪声混淆，影响语音清晰度。</li>
<li>引入新型多模态自监督AV-HuBERT模型，用于从rtMRI预测文本。</li>
<li>结合基于流的时长预测器进行特定说话者的对齐。</li>
<li>使用预测的文本和时长通过语音解码器合成语音。</li>
<li>方法可推广到未见过的说话者。</li>
<li>通过遮挡rtMRI视频部分内容进行文本预测评估，在USC-TIMIT MRI语料库上实现较低词错误率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a44a44fd35cbf881b24db9761aa1a3c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f75ea5600d5ccc115028efe9d087b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6181bb6b6954dfacf861d6a04e3e5350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1ee73887499300cdd8347ee7622e8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e463ebdb3a637323babe5b23d74b8c0b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Intra-and-Inter-modal-Context-Interaction-Modeling-for-Conversational-Speech-Synthesis"><a href="#Intra-and-Inter-modal-Context-Interaction-Modeling-for-Conversational-Speech-Synthesis" class="headerlink" title="Intra- and Inter-modal Context Interaction Modeling for Conversational   Speech Synthesis"></a>Intra- and Inter-modal Context Interaction Modeling for Conversational   Speech Synthesis</h2><p><strong>Authors:Zhenqi Jia, Rui Liu</strong></p>
<p>Conversational Speech Synthesis (CSS) aims to effectively take the multimodal dialogue history (MDH) to generate speech with appropriate conversational prosody for target utterance. The key challenge of CSS is to model the interaction between the MDH and the target utterance. Note that text and speech modalities in MDH have their own unique influences, and they complement each other to produce a comprehensive impact on the target utterance. Previous works did not explicitly model such intra-modal and inter-modal interactions. To address this issue, we propose a new intra-modal and inter-modal context interaction scheme-based CSS system, termed III-CSS. Specifically, in the training phase, we combine the MDH with the text and speech modalities in the target utterance to obtain four modal combinations, including Historical Text-Next Text, Historical Speech-Next Speech, Historical Text-Next Speech, and Historical Speech-Next Text. Then, we design two contrastive learning-based intra-modal and two inter-modal interaction modules to deeply learn the intra-modal and inter-modal context interaction. In the inference phase, we take MDH and adopt trained interaction modules to fully infer the speech prosody of the target utterance’s text content. Subjective and objective experiments on the DailyTalk dataset show that III-CSS outperforms the advanced baselines in terms of prosody expressiveness. Code and speech samples are available at <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/I3CSS">https://github.com/AI-S2-Lab/I3CSS</a>. </p>
<blockquote>
<p>对话式语音合成（CSS）旨在有效利用多模式对话历史（MDH）来生成具有适当对话韵律的目标语句。CSS的关键挑战在于对MDH与目标语句之间交互的建模。需要注意的是，MDH中的文本和语音模式各自具有独特的影响，它们相互补充，对目标语句产生全面的影响。之前的工作并没有显式地建模这种内部模式和跨模式之间的交互。为了解决这一问题，我们提出了一种基于新模式的内部模式和跨模式交互方案的CSS系统，称为III-CSS。具体来说，在训练阶段，我们将MDH与目标语句中的文本和语音模式相结合，获得四种模态组合，包括历史文本-下一个文本、历史语音-下一个语音、历史文本-下一个语音，以及历史语音-下一个文本。然后，我们设计了两个基于对比学习的内部模式和两个跨模式交互模块，以深入学习内部和跨模式的上下文交互。在推理阶段，我们采用MDH和训练好的交互模块来完全推断目标语句文本内容的语音韵律。在DailyTalk数据集上的主观和客观实验表明，III-CSS在表达韵律方面超过了先进的基线方法。代码和语音样本可在<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/I3CSS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/I3CSS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18733v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>基于会话历史的多模态信息生成具有适当会话韵律的目标语句的会话语音合成（CSS）的主要挑战在于建模多模态对话历史（MDH）与目标语句之间的交互。针对这一问题，提出了基于对比学习的多模态交互方案的新型CSS系统——III-CSS。通过结合MDH与目标语句中的文本和语音模态，设计两种基于对比学习的模态内和模态间交互模块，以深度学习模态内和模态间的上下文交互。在推理阶段，采用MDH和训练好的交互模块来完全推断目标语句文本内容的语音韵律。实验表明，III-CSS在语音韵律表达方面优于先进基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>会话语音合成（CSS）旨在利用多模态对话历史（MDH）生成具有适当会话韵律的目标语句。</li>
<li>CSS的主要挑战在于建模MDH与目标语句之间的交互。</li>
<li>提出了基于对比学习的新型CSS系统——III-CSS，包含模态内和模态间的交互模块。</li>
<li>在训练阶段，结合MDH和目标语句的四种模态组合，包括历史文本-下一条文本、历史语音-下一条语音等。</li>
<li>III-CSS通过深度学习模态内和模态间的上下文交互，提高语音韵律表达的准确性。</li>
<li>在推理阶段，采用MDH和训练好的交互模块来推断目标语句的语音韵律。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-979143359b4ad5896bf0d8a1751d7970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7261ee1f9986db45b99e5e6eee2c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a59a55380a464a5e34c2251b2622abfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f95b5b27fd7517c9047fa17252fac3cc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CosyVoice-2-Scalable-Streaming-Speech-Synthesis-with-Large-Language-Models"><a href="#CosyVoice-2-Scalable-Streaming-Speech-Synthesis-with-Large-Language-Models" class="headerlink" title="CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language   Models"></a>CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language   Models</h2><p><strong>Authors:Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, Jingren Zhou</strong></p>
<p>In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2">https://funaudiollm.github.io/cosyvoice2</a>. </p>
<blockquote>
<p>在之前的工作中，我们介绍了CosyVoice，这是一个基于监督离散语音标记的多语种语音合成模型。通过采用两种流行的生成模型——语言模型和流匹配，进行渐进式语义解码，CosyVoice在语境中学习语音时表现出了高度的语调自然性、内容一致性和说话人相似性。最近，多模态大型语言模型（LLMs）取得了重大进展，其中语音合成的响应延迟和实时因子在交互体验中发挥了关键作用。因此，在本报告中，我们提出了一种改进的流式语音合成模型CosyVoice 2，它包含了全面系统的优化。具体来说，我们引入了有限标量量化法以提高语音标记的代码本利用率。对于文本-语音语言模型，我们简化了模型架构，允许直接使用预训练的大型语言模型作为骨干。此外，我们开发了一种块感知因果流匹配模型，以支持各种合成场景，能在单个模型内实现流式和非流式合成。通过在大规模多语种数据集上进行训练，CosyVoice 2达到了与人类相当的自然度、极短的响应延迟、以及几乎无损的合成质量。欢迎听众在<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2%E4%B8%8A%E8%AF%95%E5%90%AC%E6%BC%94%E7%A4%BA%E3%80%82">https://funaudiollm.github.io/cosyvoice2上试听演示。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10117v3">PDF</a> Tech report, work in progress</p>
<p><strong>摘要</strong><br>基于先前的工作，我们推出了CosyVoice 2，一个优化的流式语音合成模型。该模型采用有限标量量化改进语音令牌的码本利用率，优化文本语音的语言模型架构以直接使用预训练的大型语言模型作为骨干网。此外，我们开发了块感知因果流匹配模型，支持各种合成场景，能够在单个模型内实现流式和非流式合成。通过大规模多语种数据集的训练，CosyVoice 2达到了与人类自然度相当的水平，响应延迟最小化，流式合成质量几乎无损失。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>CosyVoice 2是CosyVoice的改进版本，是一个优化的流式语音合成模型。</li>
<li>采用有限标量量化改进语音令牌的码本利用率，以提高语音合成的质量和效率。</li>
<li>优化文本语音的语言模型架构，允许直接使用预训练的大型语言模型作为骨干网。</li>
<li>开发块感知因果流匹配模型，支持各种合成场景。</li>
<li>模型能够支持流式和非流式合成两种模式，扩大了其应用场景范围。</li>
<li>通过大规模多语种数据集的训练，CosyVoice 2达到了高水平的自然度，与人类表现相当。</li>
<li>模型具有低响应延迟和几乎无损失的合成质量。可通过访问网站体验模型的演示效果：<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2">https://funaudiollm.github.io/cosyvoice2</a> 。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10117">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f4d0cb499918d3d20046bc7c618b7a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013cedb83ae1d2a08609293706433d33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9641a2a57cf7e0560a9e11c7fd045d62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6e7db4f0253c8902f2ba241b051a88.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping"><a href="#Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping" class="headerlink" title="Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping"></a>Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping</h2><p><strong>Authors:Minki Kang, Wooseok Han, Eunho Yang</strong></p>
<p>Generating speech from a face image is crucial for developing virtual humans capable of interacting using their unique voices, without relying on pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech conditioned on a face image rather than reference speech. We hypothesize that learning entire prosodic features from a face image poses a significant challenge. To address this, our TTS model incorporates both face and prosody encoders. The prosody encoder is specifically designed to model speech style characteristics that are not fully captured by the face image, allowing the face encoder to focus on extracting speaker-specific features such as timbre. Experimental results demonstrate that Face-StyleSpeech effectively generates more natural speech from a face image than baselines, even for unseen faces. Samples are available on our demo page. </p>
<blockquote>
<p>从人脸图像生成语音对于开发能够使用其独特声音进行交互的虚拟人类至关重要，而无需依赖预先录制的人类语音。在本文中，我们提出了Face-StyleSpeech，这是一种零样本文本到语音（TTS）合成模型，它根据人脸图像而不是参考语音生成自然语音。我们假设从人脸图像中学习所有的韵律特征是一个巨大的挑战。为解决这一问题，我们的TTS模型结合了人脸和韵律编码器。韵律编码器专门用于建模语音风格特征，这些特征无法完全由人脸图像捕获，使得人脸编码器能够专注于提取说话人特定的特征，如音质。实验结果表明，Face-StyleSpeech能有效地从人脸图像生成比基线更自然的语音，即使对于未见过的面孔也是如此。样本可在我们的演示页面上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05844v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种零样本文本到语音（TTS）合成模型——Face-StyleSpeech，该模型能够根据人脸图像生成自然语音，而无需参考预录的语音。通过融入人脸和语调编码器，模型解决了从人脸图像学习全部韵律特征带来的挑战，使得生成语音更加自然。实验结果表明，Face-StyleSpeech在生成人脸图像对应的语音方面比基线方法更有效，即使对于未见过的面孔也能生成自然的语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Face-StyleSpeech是一种零样本TTS合成模型，能根据人脸图像生成自然语音。</li>
<li>模型采用人脸和语调编码器融合，解决从人脸图像学习韵律特征的挑战。</li>
<li>人脸编码器专注于提取发音者特定特征，如音色。</li>
<li>模型在生成自然语音方面比基线方法更有效，尤其对于未见过的面孔。</li>
<li>模型生成的语音样本可在演示页面查看。</li>
<li>该技术对于开发能够使用独特声音进行交互的虚拟人具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.05844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-83c43903c2a0bd6db5298f5d1fb98ba9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e7217e939336e06d64fe9d2e07ada4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-094ccae6af75cd56e466c323aea2fc39.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-f19af98ea50b7e78fd66906435a089f0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-01-02  Dialogue Director Bridging the Gap in Dialogue Visualization for   Multimodal Storytelling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e9fc1ffc5a418cb6c4022963806495ef.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-02  Unified dimensionality reduction techniques in chronic liver disease   detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
