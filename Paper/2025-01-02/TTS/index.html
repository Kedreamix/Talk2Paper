<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Stable-TTS Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6181bb6b6954dfacf861d6a04e3e5350.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Stable-TTS-Stable-Speaker-Adaptive-Text-to-Speech-Synthesis-via-Prosody-Prompting"><a href="#Stable-TTS-Stable-Speaker-Adaptive-Text-to-Speech-Synthesis-via-Prosody-Prompting" class="headerlink" title="Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting"></a>Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting</h2><p><strong>Authors:Wooseok Han, Minki Kang, Changhun Kim, Eunho Yang</strong></p>
<p>Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable attention due to its broad range of applications, such as personalized voice assistant services. While several approaches have been proposed, they often exhibit high sensitivity to either the quantity or the quality of target speech samples. To address these limitations, we introduce Stable-TTS, a novel speaker-adaptive TTS framework that leverages a small subset of a high-quality pre-training dataset, referred to as prior samples. Specifically, Stable-TTS achieves prosody consistency by leveraging the high-quality prosody of prior samples, while effectively capturing the timbre of the target speaker. Additionally, it employs a prior-preservation loss during fine-tuning to maintain the synthesis ability for prior samples to prevent overfitting on target samples. Extensive experiments demonstrate the effectiveness of Stable-TTS even under limited amounts of and noisy target speech samples. </p>
<blockquote>
<p>è¯­éŸ³è‡ªé€‚åº”æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆå› å…¶å¹¿æ³›çš„åº”ç”¨é¢†åŸŸï¼Œå¦‚ä¸ªæ€§åŒ–è¯­éŸ³åŠ©æ‰‹æœåŠ¡ï¼Œè€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡å·²ç»æå‡ºäº†å¤šç§æ–¹æ³•ï¼Œä½†å®ƒä»¬å¯¹ç›®æ ‡è¯­éŸ³æ ·æœ¬çš„æ•°é‡æˆ–è´¨é‡å¾€å¾€è¡¨ç°å‡ºè¾ƒé«˜çš„æ•æ„Ÿæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Stable-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¯­éŸ³è‡ªé€‚åº”TTSæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®é›†çš„ä¸€ä¸ªå°å­é›†ï¼Œç§°ä¸ºå…ˆéªŒæ ·æœ¬ã€‚å…·ä½“æ¥è¯´ï¼ŒStable-TTSé€šè¿‡åˆ©ç”¨å…ˆéªŒæ ·æœ¬çš„é«˜è´¨é‡éŸµå¾‹æ¥å®ç°éŸµå¾‹ä¸€è‡´æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°æ•æ‰ç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å…ˆéªŒä¿æŒæŸå¤±ï¼Œä»¥ä¿æŒå¯¹å…ˆéªŒæ ·æœ¬çš„åˆæˆèƒ½åŠ›ï¼Œé˜²æ­¢å¯¹ç›®æ ‡æ ·æœ¬çš„è¿‡æ‹Ÿåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æœ‰é™çš„å’Œå˜ˆæ‚çš„ç›®æ ‡è¯­éŸ³æ ·æœ¬ä¸‹ï¼ŒStable-TTSä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20155v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é¢å‘æ¼”è®²è€…çš„è‡ªé€‚åº”æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¡†æ¶â€”â€”Stable-TTSã€‚å®ƒé€šè¿‡åˆ©ç”¨é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®é›†ä¸­çš„ä¸€å°éƒ¨åˆ†å…ˆéªŒæ ·æœ¬ï¼Œå®ç°äº†å¯¹ç›®æ ‡æ¼”è®²è€…è¯­è°ƒçš„ä¸€è‡´æ€§å’ŒéŸ³è´¨çš„æ•æ‰ã€‚æ­¤å¤–ï¼ŒStable-TTSåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å…ˆéªŒä¿ç•™æŸå¤±ï¼Œä»¥ä¿æŒå¯¹å…ˆéªŒæ ·æœ¬çš„åˆæˆèƒ½åŠ›ï¼Œé˜²æ­¢å¯¹ç›®æ ‡æ ·æœ¬çš„è¿‡æ‹Ÿåˆã€‚å³ä½¿åœ¨ç›®æ ‡è¯­éŸ³æ ·æœ¬æœ‰é™ä¸”å™ªå£°è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼ŒStable-TTSä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Stable-TTSæ˜¯ä¸€ç§é¢å‘æ¼”è®²è€…çš„è‡ªé€‚åº”TTSåˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¯¹äºç›®æ ‡è¯­éŸ³æ ·æœ¬æ•°é‡æˆ–è´¨é‡çš„æ•æ„Ÿæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®é›†ä¸­çš„å…ˆéªŒæ ·æœ¬ï¼Œå®ç°è¯­è°ƒä¸€è‡´æ€§å’ŒéŸ³è´¨çš„æ•æ‰ã€‚</li>
<li>é€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å…ˆéªŒä¿ç•™æŸå¤±ï¼ŒStable-TTSèƒ½å¤Ÿä¿æŒå¯¹å…ˆéªŒæ ·æœ¬çš„åˆæˆèƒ½åŠ›ã€‚</li>
<li>Stable-TTSåœ¨ç›®æ ‡è¯­éŸ³æ ·æœ¬æœ‰é™å’Œå™ªå£°è¾ƒå¤§çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°ä¸ªæ€§åŒ–çš„è¯­éŸ³åŠ©æ‰‹æœåŠ¡ç­‰å¤šç§å¹¿æ³›åº”ç”¨ã€‚</li>
<li>Stable-TTSå¯¹äºæé«˜TTSåˆæˆçš„ç¨³å®šæ€§å’Œæ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7e8b0a0a93431b4cae192ffc8b239570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a537d6e5113dfd1c2ab103449257161b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e5b0c83131ba27f8cc6fb3f9fbf0074.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f0c8c1c25d5b7a06ea5e6d1e705abb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee9d6718b4a17eb3a7967c1768468065.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-415365a7479de602797bfaeb9871cea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86274cbea79770c1acd9d0dfc1666e04.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation"><a href="#CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation" class="headerlink" title="CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation"></a>CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation</h2><p><strong>Authors:Ji-Hoon Kim, Hong-Sun Yang, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Joon Son Chung</strong></p>
<p>The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯åœ¨ä¿æŒç›¸åŒè¯´è¯äººèº«ä»½çš„åŒæ—¶ç”Ÿæˆå¤šç§è¯­è¨€çš„è‡ªç„¶è¯­éŸ³ï¼Œè¿™ä¸€ä»»åŠ¡è¢«ç§°ä¸ºè·¨è¯­è¨€è¯­éŸ³åˆæˆã€‚è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯è¯­è¨€ä¸è¯´è¯äººçš„çº ç¼ é—®é¢˜ï¼Œå¯¼è‡´è·¨è¯­è¨€ç³»ç»Ÿçš„è´¨é‡è½åäºå•è¯­è¨€ç³»ç»Ÿã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrossSpeech++ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å¼€è¯­è¨€å’Œè¯´è¯äººä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å¤æ‚çš„è¯­éŸ³ç”Ÿæˆç®¡é“åˆ†è§£ä¸ºä¸¤ä¸ªç®€å•çš„ç»„ä»¶ï¼šè¯­è¨€ç›¸å…³ç”Ÿæˆå™¨å’Œè¯´è¯äººç›¸å…³ç”Ÿæˆå™¨ã€‚è¯­è¨€ç›¸å…³ç”Ÿæˆå™¨äº§ç”Ÿä¸å—ç‰¹å®šè¯´è¯äººå±æ€§å½±å“çš„è¯­è¨€å˜åŒ–ã€‚è¯´è¯äººç›¸å…³ç”Ÿæˆå™¨åˆ™æ¨¡æ‹Ÿèƒ½å¤Ÿä½“ç°è¯´è¯äººèº«ä»½çš„å£°å­¦å˜åŒ–ã€‚é€šè¿‡åœ¨ä¸åŒçš„æ¨¡å—ä¸­å¤„ç†æ¯ç§ç±»å‹çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å¼€è¯­è¨€å’Œè¯´è¯äººçš„è¡¨ç¤ºã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å„ç§åº¦é‡æ ‡å‡†è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†CrossSpeech++åœ¨è·¨è¯­è¨€è¯­éŸ³åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20048v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†CrossSpeech++æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­çš„è¯­è¨€ä¸è¯´è¯äººçº ç¼ é—®é¢˜ï¼Œä»è€Œæé«˜è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚è¯¥æ–¹æ¡ˆå°†å¤æ‚çš„è¯­éŸ³ç”Ÿæˆç®¡é“ç®€åŒ–ä¸ºä¸¤ä¸ªç»„ä»¶ï¼šè¯­è¨€ç›¸å…³ç”Ÿæˆå™¨å’Œè¯´è¯äººç›¸å…³ç”Ÿæˆå™¨ã€‚å‰è€…äº§ç”Ÿä¸å—ç‰¹å®šè¯´è¯äººå±æ€§å½±å“çš„è¯­è¨€å˜åŒ–ï¼Œåè€…æ¨¡æ‹Ÿè¡¨å¾è¯´è¯äººèº«ä»½çš„å£°å­¦å˜åŒ–ã€‚é€šè¿‡å•ç‹¬å¤„ç†æ¯ç§ç±»å‹çš„ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å¼€è¯­è¨€å’Œè¯´è¯äººçš„è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CrossSpeech++æ—¨åœ¨è§£å†³è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­çš„è¯­è¨€ä¸è¯´è¯äººçº ç¼ é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ¡ˆé€šè¿‡åˆ†ç¦»è¯­è¨€ç›¸å…³å’Œè¯´è¯äººç›¸å…³ä¿¡æ¯æ¥æé«˜è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚</li>
<li>CrossSpeech++å°†å¤æ‚çš„è¯­éŸ³ç”Ÿæˆç®¡é“ç®€åŒ–ä¸ºä¸¤ä¸ªç»„ä»¶ï¼šè¯­è¨€ç›¸å…³ç”Ÿæˆå™¨å’Œè¯´è¯äººç›¸å…³ç”Ÿæˆå™¨ã€‚</li>
<li>è¯­è¨€ç›¸å…³ç”Ÿæˆå™¨äº§ç”Ÿçš„è¯­è¨€å˜åŒ–ä¸å—ç‰¹å®šè¯´è¯äººå±æ€§çš„å½±å“ã€‚</li>
<li>è¯´è¯äººç›¸å…³ç”Ÿæˆå™¨æ¨¡æ‹Ÿè¡¨å¾è¯´è¯äººèº«ä»½çš„å£°å­¦å˜åŒ–ã€‚</li>
<li>é€šè¿‡å•ç‹¬å¤„ç†æ¯ç§ç±»å‹çš„ä¿¡æ¯ï¼ŒCrossSpeech++èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†è¯­è¨€å’Œè¯´è¯äººçš„è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a144310e8743d76fe946abe22982a1c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b27c17333f4be5fcd573abf6e8c687e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71a3e4b4f613f066ba8c895460a3ea06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-301cbe8ec42efceaaf9a0db2aa09d03b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f9d189a4c973d41d917039da036b142.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VoiceDiT-Dual-Condition-Diffusion-Transformer-for-Environment-Aware-Speech-Synthesis"><a href="#VoiceDiT-Dual-Condition-Diffusion-Transformer-for-Environment-Aware-Speech-Synthesis" class="headerlink" title="VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware   Speech Synthesis"></a>VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware   Speech Synthesis</h2><p><strong>Authors:Jaemin Jung, Junseok Ahn, Chaeyoung Jung, Tan Dat Nguyen, Youngjoon Jang, Joon Son Chung</strong></p>
<p>We present VoiceDiT, a multi-modal generative model for producing environment-aware speech and audio from text and visual prompts. While aligning speech with text is crucial for intelligible speech, achieving this alignment in noisy conditions remains a significant and underexplored challenge in the field. To address this, we present a novel audio generation pipeline named VoiceDiT. This pipeline includes three key components: (1) the creation of a large-scale synthetic speech dataset for pre-training and a refined real-world speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to efficiently preserve aligned speech information while accurately reflecting environmental conditions, and (3) a diffusion-based Image-to-Audio Translator that allows the model to bridge the gap between audio and image, facilitating the generation of environmental sound that aligns with the multi-modal prompts. Extensive experimental results demonstrate that VoiceDiT outperforms previous models on real-world datasets, showcasing significant improvements in both audio quality and modality integration. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†VoiceDiTï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬å’Œè§†è§‰æç¤ºç”Ÿæˆå¯¹ç¯å¢ƒæ„ŸçŸ¥çš„è¯­éŸ³å’ŒéŸ³é¢‘ã€‚è™½ç„¶å°†è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½å¯¹äºå¯ç†è§£çš„è¯­éŸ³è‡³å…³é‡è¦ï¼Œä½†åœ¨å˜ˆæ‚çš„æ¡ä»¶ä¸‹å®ç°è¿™ç§å¯¹é½ä»ç„¶æ˜¯è¯¥é¢†åŸŸä¸€ä¸ªé‡å¤§ä¸”æœªè¢«å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºVoiceDiTçš„æ–°å‹éŸ³é¢‘ç”Ÿæˆç®¡é“ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åˆ›å»ºç”¨äºé¢„è®­ç»ƒçš„å¤§è§„æ¨¡åˆæˆè¯­éŸ³æ•°æ®é›†å’Œç”¨äºç²¾ç»†è°ƒæ•´çš„çœŸå®ä¸–ç•Œè¯­éŸ³æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰Dual-DiTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨é«˜æ•ˆä¿ç•™å¯¹é½çš„è¯­éŸ³ä¿¡æ¯ï¼ŒåŒæ—¶å‡†ç¡®åæ˜ ç¯å¢ƒæ¡ä»¶ï¼›ï¼ˆ3ï¼‰åŸºäºæ‰©æ•£çš„Image-to-Audio Translatorï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¼¥åˆéŸ³é¢‘å’Œå›¾åƒä¹‹é—´çš„å·®è·ï¼Œä¿ƒè¿›ä¸ç¯å¢ƒå£°éŸ³çš„ç”Ÿæˆï¼Œè¿™ä¸å¤šæ¨¡æ€æç¤ºç›¸å»åˆã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVoiceDiTåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä»¥å‰çš„æ¨¡å‹ï¼Œåœ¨éŸ³é¢‘è´¨é‡å’Œæ¨¡æ€é›†æˆæ–¹é¢éƒ½æ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19259v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€é¡¹åä¸ºVoiceDiTçš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»æ–‡æœ¬å’Œè§†è§‰æç¤ºä¸­ç”Ÿæˆç¯å¢ƒæ„ŸçŸ¥è¯­éŸ³å’ŒéŸ³é¢‘ã€‚ä¸ºè§£å†³å™ªå£°æ¡ä»¶ä¸‹è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½çš„éš¾é¢˜ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ–°å‹éŸ³é¢‘ç”Ÿæˆæµç¨‹ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡åˆæˆè¯­éŸ³æ•°æ®é›†ç”¨äºé¢„è®­ç»ƒå’Œä¼˜åŒ–ç°å®ä¸–ç•Œè¯­éŸ³æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€è®¾è®¡çš„åŒDiTæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆä¿ç•™å¯¹é½è¯­éŸ³ä¿¡æ¯å¹¶å‡†ç¡®åæ˜ ç¯å¢ƒæ¡ä»¶ï¼Œä»¥åŠåŸºäºæ‰©æ•£çš„å›¾åƒåˆ°éŸ³é¢‘ç¿»è¯‘å™¨ï¼Œå¯ç¼©å°éŸ³é¢‘ä¸å›¾åƒä¹‹é—´çš„å·®è·ï¼Œä¾¿äºç”Ÿæˆä¸ç¯å¢ƒæç¤ºç›¸ç¬¦çš„éŸ³é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVoiceDiTåœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰æ¨¡å‹ï¼Œåœ¨éŸ³é¢‘è´¨é‡å’Œæ¨¡æ€é›†æˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VoiceDiTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä»æ–‡æœ¬å’Œè§†è§‰æç¤ºç”Ÿæˆç¯å¢ƒæ„ŸçŸ¥çš„è¯­éŸ³å’ŒéŸ³é¢‘ã€‚</li>
<li>å™ªå£°æ¡ä»¶ä¸‹è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½æ˜¯è¯­éŸ³åˆæˆé¢†åŸŸçš„é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>VoiceDiTé‡‡ç”¨æ–°å‹éŸ³é¢‘ç”Ÿæˆæµç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µä½¿ç”¨çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>åŒDiTæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆä¿ç•™å¯¹é½è¯­éŸ³ä¿¡æ¯å¹¶åæ˜ ç¯å¢ƒæ¡ä»¶ã€‚</li>
<li>æ‰©æ•£å›¾åƒåˆ°éŸ³é¢‘ç¿»è¯‘å™¨æœ‰åŠ©äºç”Ÿæˆä¸ç¯å¢ƒæç¤ºç›¸ç¬¦çš„éŸ³é¢‘ã€‚</li>
<li>VoiceDiTåœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c03d5da319abf2cd9dc6cd3ec4eee76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6264cf236ca280f52339d7684df81d13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02cdf3af59dbf9fa385acd356726a4d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddee717a40502907bdceef316cbeeb7f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="â€œIâ€™ve-Heard-of-You-â€-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities"><a href="#â€œIâ€™ve-Heard-of-You-â€-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities" class="headerlink" title="â€œIâ€™ve Heard of You!â€: Generate Spoken Named Entity Recognition Data for   Unseen Entities"></a>â€œIâ€™ve Heard of You!â€: Generate Spoken Named Entity Recognition Data for   Unseen Entities</h2><p><strong>Authors:Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su</strong></p>
<p>Spoken named entity recognition (NER) aims to identify named entities from speech, playing an important role in speech processing. New named entities appear every day, however, annotating their Spoken NER data is costly. In this paper, we demonstrate that existing Spoken NER systems perform poorly when dealing with previously unseen named entities. To tackle this challenge, we propose a method for generating Spoken NER data based on a named entity dictionary (NED) to reduce costs. Specifically, we first use a large language model (LLM) to generate sentences from the sampled named entities and then use a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce a noise metric to filter out noisy data. To evaluate our approach, we release a novel Spoken NER benchmark along with a corresponding NED containing 8,853 entities. Experiment results show that our method achieves state-of-the-art (SOTA) performance in the in-domain, zero-shot domain adaptation, and fully zero-shot settings. Our data will be available at <a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU">https://github.com/DeepLearnXMU/HeardU</a>. </p>
<blockquote>
<p>è¯­éŸ³å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ—¨åœ¨ä»è¯­éŸ³ä¸­è¯†åˆ«å‘½åå®ä½“ï¼Œåœ¨è¯­éŸ³è¯†åˆ«å¤„ç†ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚æ¯å¤©éƒ½æœ‰æ–°çš„å‘½åå®ä½“å‡ºç°ï¼Œç„¶è€Œï¼Œå¯¹è¯­éŸ³NERæ•°æ®è¿›è¡Œæ ‡æ³¨çš„æˆæœ¬å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„è¯­éŸ³NERç³»ç»Ÿåœ¨å¤„ç†ä¹‹å‰æœªè§è¿‡çš„å‘½åå®ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‘½åå®ä½“è¯å…¸ï¼ˆNEDï¼‰ç”Ÿæˆè¯­éŸ³NERæ•°æ®çš„æ–¹æ³•ä»¥é™ä½æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é‡‡æ ·çš„å‘½åå®ä½“ç”Ÿæˆå¥å­ï¼Œç„¶åä½¿ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆè¯­éŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå™ªå£°åº¦é‡æ¥è¿‡æ»¤æ‰å™ªå£°æ•°æ®ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„è¯­éŸ³NERåŸºå‡†æµ‹è¯•ï¼Œä»¥åŠä¸€ä¸ªåŒ…å«8853ä¸ªå®ä½“çš„ç›¸åº”NEDã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸŸå†…ã€é›¶é•œå¤´åŸŸé€‚åº”å’Œå®Œå…¨é›¶é•œå¤´è®¾ç½®ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepLearnXMU/HeardUä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19102v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³å‘½åå®ä½“è¯†åˆ«ï¼ˆSpoken NERï¼‰æ—¨åœ¨ä»è¯­éŸ³ä¸­è¯†åˆ«å‘½åå®ä½“ï¼Œåœ¨è¯­éŸ³è¯†åˆ«å¤„ç†ä¸­å æ®é‡è¦åœ°ä½ã€‚ç°æœ‰ç³»ç»Ÿåœ¨é¢å¯¹æœªè§è¿‡çš„å‘½åå®ä½“æ—¶è¡¨ç°ä¸ä½³ï¼Œæ ‡æ³¨è¯­éŸ³NERæ•°æ®æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå‘½åå®ä½“è¯å…¸ï¼ˆNEDï¼‰ç”Ÿæˆè¯­éŸ³NERæ•°æ®çš„æ–¹æ³•é™ä½æˆæœ¬ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é‡‡æ ·å‘½åå®ä½“ç”Ÿæˆå¥å­ï¼Œå†é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆè¯­éŸ³ã€‚æ­¤å¤–ï¼Œå¼•å…¥å™ªå£°åº¦é‡ä»¥è¿‡æ»¤å™ªå£°æ•°æ®ã€‚ç ”ç©¶å‘å¸ƒæ–°çš„è¯­éŸ³NERåŸºå‡†ä¸åŒ…å«8,853ä¸ªå®ä½“çš„å¯¹åº”NEDã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…ã€è·¨é¢†åŸŸé€‚åº”ä»¥åŠå…¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚ç›¸å…³æ•°æ®å°†å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E3%80%82">https://github.com/DeepLearnXMU/HeardUã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å‘½åå®ä½“è¯†åˆ«ï¼ˆSpoken NERï¼‰åœ¨è¯­éŸ³è¯†åˆ«å¤„ç†ä¸­å¾ˆé‡è¦ï¼Œé¢ä¸´æ ‡æ³¨æˆæœ¬é«˜æ˜‚å’Œé¢å¯¹æœªè§å®ä½“è¯†åˆ«æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå‘½åå®ä½“è¯å…¸ï¼ˆNEDï¼‰ç”Ÿæˆè¯­éŸ³NERæ•°æ®çš„æ–¹æ³•ï¼Œé™ä½æˆæœ¬å¹¶æé«˜æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é‡‡æ ·å‘½åå®ä½“ç”Ÿæˆå¥å­ï¼Œå†é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè½¬åŒ–ã€‚</li>
<li>å¼•å…¥å™ªå£°åº¦é‡æœºåˆ¶ä»¥è¿‡æ»¤ç”Ÿæˆçš„å™ªå£°æ•°æ®ã€‚</li>
<li>å‘å¸ƒæ–°çš„è¯­éŸ³NERåŸºå‡†ä¸å¯¹åº”NEDï¼ŒåŒ…å«å¤§é‡å®ä½“æ•°æ®ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šç§è®¾ç½®ä¸‹è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b86e890486171362a149be8408a2e3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5aa85d1b8ed0df6adba19264fc7d5a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33eef7680a3f22388849094bd1a0991b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-179f247ff97294ecfdc487eec3257cc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd1ba9e961a6bbb71566fea9c46a06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515c695c189641e0236cc39dcb4c07fc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Advancing-NAM-to-Speech-Conversion-with-Novel-Methods-and-the-MultiNAM-Dataset"><a href="#Advancing-NAM-to-Speech-Conversion-with-Novel-Methods-and-the-MultiNAM-Dataset" class="headerlink" title="Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM   Dataset"></a>Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM   Dataset</h2><p><strong>Authors:Neil Shah, Shirish Karande, Vineet Gandhi</strong></p>
<p>Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning to simulate ground-truth speech from paired whispers. However, the simulated speech often lacks intelligibility and fails to generalize well across different speakers. To address this issue, we focus on learning phoneme-level alignments from paired whispers and text and employ a Text-to-Speech (TTS) system to simulate the ground-truth. To reduce dependence on whispers, we learn phoneme alignments directly from NAMs, though the quality is constrained by the available training data. To further mitigate reliance on NAM&#x2F;whisper data for ground-truth simulation, we propose incorporating the lip modality to infer speech and introduce a novel diffusion-based method that leverages recent advancements in lip-to-speech technology. Additionally, we release the MultiNAM dataset with over $7.96$ hours of paired NAM, whisper, video, and text data from two speakers and benchmark all methods on this dataset. Speech samples and the dataset are available at \url{<a target="_blank" rel="noopener" href="https://diff-nam.github.io/DiffNAM/%7D">https://diff-nam.github.io/DiffNAM/}</a> </p>
<blockquote>
<p>å½“å‰çš„éå£°ä½è¯­ï¼ˆNAMï¼‰åˆ°è¯­éŸ³æŠ€æœ¯ä¾èµ–äºè¯­éŸ³å…‹éš†ï¼Œé€šè¿‡æ¨¡æ‹Ÿé…å¯¹ä½è¯­çš„åŸå§‹è¯­éŸ³æ¥å®ç°ã€‚ç„¶è€Œï¼Œæ¨¡æ‹Ÿçš„è¯­éŸ³å¾€å¾€ç¼ºä¹æ¸…æ™°åº¦ï¼Œåœ¨ä¸åŒå‘éŸ³è€…ä¹‹é—´çš„æ³›åŒ–æ•ˆæœè¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä»é…å¯¹çš„ä½è¯­å’Œæ–‡æœ¬ä¸­å­¦ä¹ éŸ³ç´ çº§åˆ«çš„å¯¹é½ï¼Œå¹¶é‡‡ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿæ¥æ¨¡æ‹ŸåŸå§‹è¯­éŸ³ã€‚ä¸ºäº†å‡å°‘å¯¹ä½è¯­çš„ä¾èµ–ï¼Œæˆ‘ä»¬ç›´æ¥ä»NAMå­¦ä¹ éŸ³ç´ å¯¹é½ï¼Œå°½ç®¡è´¨é‡å—åˆ°å¯ç”¨è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘å¯¹NAM&#x2F;ä½è¯­æ•°æ®åœ¨æ¨¡æ‹ŸåŸå§‹è¯­éŸ³æ–¹é¢çš„ä¾èµ–ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å”‡å‹æ¨¡å¼è¿›è¡Œè¯­éŸ³æ¨æ–­ï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æœ€æ–°çš„å”‡å‹åˆ°è¯­éŸ³æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒäº†MultiNAMæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªä¸¤ä¸ªå‘éŸ³è€…çš„è¶…è¿‡7.96å°æ—¶çš„é…å¯¹NAMã€ä½è¯­ã€è§†é¢‘å’Œæ–‡æœ¬æ•°æ®ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹æ‰€æœ‰æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯­éŸ³æ ·æœ¬å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://diff-nam.github.io/DiffNAM/%E8%8E%B7%E5%8F%96%E3%80%82">https://diff-nam.github.io/DiffNAM/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18839v1">PDF</a> Accepted at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ç°æœ‰éè¯­éŸ³å˜Ÿå›”ï¼ˆNAMï¼‰åˆ°è¯­éŸ³è½¬æ¢æŠ€æœ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚é€šè¿‡ä»é…å¯¹å˜Ÿå›”å’Œæ–‡æœ¬ä¸­å­¦ä¹ éŸ³ç´ çº§åˆ«çš„å¯¹é½ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿæ¨¡æ‹ŸçœŸå®è¯­éŸ³ã€‚ä¸ºäº†å‡å°‘å¯¹å˜Ÿå›”çš„ä¾èµ–ï¼Œç›´æ¥ä»NAMä¸­å­¦ä¹ éŸ³ç´ å¯¹é½ï¼Œä½†è´¨é‡å—é™äºå¯ç”¨è®­ç»ƒæ•°æ®ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯¹çœŸå®è¯­éŸ³æ¨¡æ‹Ÿçš„å¯é æ€§ï¼Œæ–‡ç« ç»“åˆäº†å”‡éƒ¨ç‰¹å¾å¹¶å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œåˆ©ç”¨æœ€æ–°çš„å”‡éƒ¨åˆ°è¯­éŸ³æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å‘å¸ƒäº†åŒ…å«è¶…è¿‡7.96å°æ—¶é…å¯¹NAMã€å˜Ÿå›”ã€è§†é¢‘å’Œæ–‡æœ¬æ•°æ®çš„MultiNAMæ•°æ®é›†ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹æ‰€æœ‰æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰éè¯­éŸ³å˜Ÿå›”ï¼ˆNAMï¼‰åˆ°è¯­éŸ³è½¬æ¢æŠ€æœ¯ä¸»è¦ä¾èµ–äºå£°éŸ³å…‹éš†æ¥æ¨¡æ‹ŸçœŸå®è¯­éŸ³ã€‚</li>
<li>ç°æœ‰çš„æ¨¡æ‹Ÿè¯­éŸ³å¸¸å¸¸ç¼ºä¹æ¸…æ™°åº¦å’Œåœ¨ä¸åŒå‘è¨€äººä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ–°çš„æ–¹æ³•ä¸“æ³¨äºä»é…å¯¹å˜Ÿå›”å’Œæ–‡æœ¬ä¸­å­¦ä¹ éŸ³ç´ çº§åˆ«çš„å¯¹é½ï¼Œå¹¶ä½¿ç”¨TTSç³»ç»Ÿæ¨¡æ‹ŸçœŸå®è¯­éŸ³ä»¥æé«˜æ¸…æ™°åº¦ã€‚</li>
<li>ç›´æ¥ä»NAMä¸­å­¦ä¹ éŸ³ç´ å¯¹é½æ˜¯ä¸€ç§å‡å°‘å¯¹å˜Ÿå›”ä¾èµ–çš„æ–¹æ³•ï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§ã€‚</li>
<li>ä¸ºäº†æ›´å¯é åœ°æ¨¡æ‹ŸçœŸå®è¯­éŸ³ï¼Œç»“åˆäº†å”‡éƒ¨ç‰¹å¾å¹¶å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚</li>
<li>å‘å¸ƒäº†ä¸€ä¸ªåä¸ºMultiNAMçš„æ•°æ®é›†ï¼ŒåŒ…å«NAMã€å˜Ÿå›”ã€è§†é¢‘å’Œæ–‡æœ¬æ•°æ®ï¼Œä¸ºåŸºå‡†æµ‹è¯•æä¾›äº†èµ„æºã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ˜¾ç¤ºå„ç§æ–¹æ³•åœ¨MultiNAMæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b474c1019184b0d8fd921adbd27f6682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e86e8019b5ee6177011bd29524a7f563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e12cf4b934d2c9fb8fc3144e57e776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09c9fa270829f7ec68144943ab865760.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MRI2Speech-Speech-Synthesis-from-Articulatory-Movements-Recorded-by-Real-time-MRI"><a href="#MRI2Speech-Speech-Synthesis-from-Articulatory-Movements-Recorded-by-Real-time-MRI" class="headerlink" title="MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by   Real-time MRI"></a>MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by   Real-time MRI</h2><p><strong>Authors:Neil Shah, Ayan Kashyap, Shirish Karande, Vineet Gandhi</strong></p>
<p>Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily on noisy ground-truth speech. Applying loss directly over ground truth mel-spectrograms entangles speech content with MRI noise, resulting in poor intelligibility. We introduce a novel approach that adapts the multi-modal self-supervised AV-HuBERT model for text prediction from rtMRI and incorporates a new flow-based duration predictor for speaker-specific alignment. The predicted text and durations are then used by a speech decoder to synthesize aligned speech in any novel voice. We conduct thorough experiments on two datasets and demonstrate our methodâ€™s generalization ability to unseen speakers. We assess our frameworkâ€™s performance by masking parts of the rtMRI video to evaluate the impact of different articulators on text prediction. Our method achieves a $15.18%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus, marking a huge improvement over the current state-of-the-art. Speech samples are available at \url{<a target="_blank" rel="noopener" href="https://mri2speech.github.io/MRI2Speech/%7D">https://mri2speech.github.io/MRI2Speech/}</a> </p>
<blockquote>
<p>ä¹‹å‰åŸºäºå®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰çš„è¯­éŸ³åˆæˆæ¨¡å‹å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå˜ˆæ‚çš„çœŸå®è¯­éŸ³ã€‚ç›´æ¥åœ¨çœŸå®melé¢‘è°±å›¾ä¸Šåº”ç”¨æŸå¤±ä¼šä½¿è¯­éŸ³å†…å®¹ä¸ç£å…±æŒ¯æˆåƒå™ªå£°æ··æ·†ï¼Œå¯¼è‡´è¯­éŸ³æ¸…æ™°åº¦é™ä½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è‡ªé€‚åº”å¤šæ¨¡æ€è‡ªç›‘ç£AV-HuBERTæ¨¡å‹ï¼Œç”¨äºä»rtMRIé¢„æµ‹æ–‡æœ¬ï¼Œå¹¶èå…¥æ–°çš„åŸºäºæµæŠ€æœ¯çš„æŒç»­æ—¶é—´é¢„æµ‹å™¨ï¼Œä»¥å®ç°é’ˆå¯¹ç‰¹å®šè¯´è¯äººçš„å¯¹é½ã€‚é¢„æµ‹çš„æ–‡æœ¬å’ŒæŒç»­æ—¶é—´éšåè¢«è¯­éŸ³è§£ç å™¨ç”¨äºåˆæˆä»»ä½•æ–°é¢–å£°éŸ³çš„è¯­éŸ³ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºæœªè§è¿‡çš„è¯´è¯äººçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡é®æŒ¡rtMRIè§†é¢‘çš„éƒ¨åˆ†å†…å®¹æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¡†æ¶æ€§èƒ½ï¼Œä»¥è¯„ä¼°ä¸åŒå‘éŸ³å™¨å¯¹æ–‡æœ¬é¢„æµ‹çš„å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨USC-TIMIT MRIè¯­æ–™åº“ä¸Šå®ç°äº†15.18%çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œç›¸è¾ƒäºå½“å‰æœ€æ–°æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ”¹è¿›ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨[<a target="_blank" rel="noopener" href="https://mri2speech.github.io/MRI2Speech/]%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://mri2speech.github.io/MRI2Speech/]ä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18836v1">PDF</a> Accepted at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰çš„è¯­éŸ³åˆæˆæ¨¡å‹è¿‡å»ä¾èµ–äºå™ªå£°è¾ƒå¤§çš„çœŸå®è¯­éŸ³ã€‚ç›´æ¥åœ¨çœŸå®melé¢‘è°±å›¾ä¸Šåº”ç”¨æŸå¤±ä¼šå¯¼è‡´è¯­éŸ³å†…å®¹ä¸ç£å…±æŒ¯æˆåƒå™ªå£°æ··æ·†ï¼Œé™ä½è¯­éŸ³æ¸…æ™°åº¦ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€è‡ªç›‘ç£AV-HuBERTæ¨¡å‹ï¼Œç”¨äºä»rtMRIé¢„æµ‹æ–‡æœ¬ï¼Œå¹¶èå…¥æ–°çš„åŸºäºæµçš„æ—¶é•¿é¢„æµ‹å™¨è¿›è¡Œç‰¹å®šäºè¯´è¯è€…çš„å¯¹é½ã€‚é¢„æµ‹çš„æ–‡æœ¬å’Œæ—¶é•¿éšåè¢«è¯­éŸ³è§£ç å™¨ç”¨äºåˆæˆä»»ä½•æ–°é¢–è¯­éŸ³ã€‚åœ¨ä¸¤é¡¹æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„è¯´è¯è€…ã€‚æœ¬ç ”ç©¶é€šè¿‡é®æŒ¡rtMRIè§†é¢‘çš„éƒ¨åˆ†å†…å®¹æ¥è¯„ä¼°æ–‡æœ¬é¢„æµ‹çš„å½±å“ï¼Œè¯¥æ–¹æ³•åœ¨USC-TIMIT MRIè¯­æ–™åº“ä¸Šå®ç°äº†15.18%çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œè¾ƒå½“å‰å…ˆè¿›æŠ€æœ¯æœ‰å¾ˆå¤§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶MRIè¯­éŸ³åˆæˆä¾èµ–å™ªå£°çœŸå®è¯­éŸ³çš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥åœ¨çœŸå®melé¢‘è°±å›¾ä¸Šåº”ç”¨æŸå¤±ä¼šå¯¼è‡´è¯­éŸ³å†…å®¹å’ŒMRIå™ªå£°æ··æ·†ï¼Œå½±å“è¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>å¼•å…¥æ–°å‹å¤šæ¨¡æ€è‡ªç›‘ç£AV-HuBERTæ¨¡å‹ï¼Œç”¨äºä»rtMRIé¢„æµ‹æ–‡æœ¬ã€‚</li>
<li>ç»“åˆåŸºäºæµçš„æ—¶é•¿é¢„æµ‹å™¨è¿›è¡Œç‰¹å®šè¯´è¯è€…çš„å¯¹é½ã€‚</li>
<li>ä½¿ç”¨é¢„æµ‹çš„æ–‡æœ¬å’Œæ—¶é•¿é€šè¿‡è¯­éŸ³è§£ç å™¨åˆæˆè¯­éŸ³ã€‚</li>
<li>æ–¹æ³•å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯´è¯è€…ã€‚</li>
<li>é€šè¿‡é®æŒ¡rtMRIè§†é¢‘éƒ¨åˆ†å†…å®¹è¿›è¡Œæ–‡æœ¬é¢„æµ‹è¯„ä¼°ï¼Œåœ¨USC-TIMIT MRIè¯­æ–™åº“ä¸Šå®ç°è¾ƒä½è¯é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a44a44fd35cbf881b24db9761aa1a3c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f75ea5600d5ccc115028efe9d087b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6181bb6b6954dfacf861d6a04e3e5350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1ee73887499300cdd8347ee7622e8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e463ebdb3a637323babe5b23d74b8c0b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Intra-and-Inter-modal-Context-Interaction-Modeling-for-Conversational-Speech-Synthesis"><a href="#Intra-and-Inter-modal-Context-Interaction-Modeling-for-Conversational-Speech-Synthesis" class="headerlink" title="Intra- and Inter-modal Context Interaction Modeling for Conversational   Speech Synthesis"></a>Intra- and Inter-modal Context Interaction Modeling for Conversational   Speech Synthesis</h2><p><strong>Authors:Zhenqi Jia, Rui Liu</strong></p>
<p>Conversational Speech Synthesis (CSS) aims to effectively take the multimodal dialogue history (MDH) to generate speech with appropriate conversational prosody for target utterance. The key challenge of CSS is to model the interaction between the MDH and the target utterance. Note that text and speech modalities in MDH have their own unique influences, and they complement each other to produce a comprehensive impact on the target utterance. Previous works did not explicitly model such intra-modal and inter-modal interactions. To address this issue, we propose a new intra-modal and inter-modal context interaction scheme-based CSS system, termed III-CSS. Specifically, in the training phase, we combine the MDH with the text and speech modalities in the target utterance to obtain four modal combinations, including Historical Text-Next Text, Historical Speech-Next Speech, Historical Text-Next Speech, and Historical Speech-Next Text. Then, we design two contrastive learning-based intra-modal and two inter-modal interaction modules to deeply learn the intra-modal and inter-modal context interaction. In the inference phase, we take MDH and adopt trained interaction modules to fully infer the speech prosody of the target utteranceâ€™s text content. Subjective and objective experiments on the DailyTalk dataset show that III-CSS outperforms the advanced baselines in terms of prosody expressiveness. Code and speech samples are available at <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/I3CSS">https://github.com/AI-S2-Lab/I3CSS</a>. </p>
<blockquote>
<p>å¯¹è¯å¼è¯­éŸ³åˆæˆï¼ˆCSSï¼‰æ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡å¼å¯¹è¯å†å²ï¼ˆMDHï¼‰æ¥ç”Ÿæˆå…·æœ‰é€‚å½“å¯¹è¯éŸµå¾‹çš„ç›®æ ‡è¯­å¥ã€‚CSSçš„å…³é”®æŒ‘æˆ˜åœ¨äºå¯¹MDHä¸ç›®æ ‡è¯­å¥ä¹‹é—´äº¤äº’çš„å»ºæ¨¡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒMDHä¸­çš„æ–‡æœ¬å’Œè¯­éŸ³æ¨¡å¼å„è‡ªå…·æœ‰ç‹¬ç‰¹çš„å½±å“ï¼Œå®ƒä»¬ç›¸äº’è¡¥å……ï¼Œå¯¹ç›®æ ‡è¯­å¥äº§ç”Ÿå…¨é¢çš„å½±å“ã€‚ä¹‹å‰çš„å·¥ä½œå¹¶æ²¡æœ‰æ˜¾å¼åœ°å»ºæ¨¡è¿™ç§å†…éƒ¨æ¨¡å¼å’Œè·¨æ¨¡å¼ä¹‹é—´çš„äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ–°æ¨¡å¼çš„å†…éƒ¨æ¨¡å¼å’Œè·¨æ¨¡å¼äº¤äº’æ–¹æ¡ˆçš„CSSç³»ç»Ÿï¼Œç§°ä¸ºIII-CSSã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å°†MDHä¸ç›®æ ‡è¯­å¥ä¸­çš„æ–‡æœ¬å’Œè¯­éŸ³æ¨¡å¼ç›¸ç»“åˆï¼Œè·å¾—å››ç§æ¨¡æ€ç»„åˆï¼ŒåŒ…æ‹¬å†å²æ–‡æœ¬-ä¸‹ä¸€ä¸ªæ–‡æœ¬ã€å†å²è¯­éŸ³-ä¸‹ä¸€ä¸ªè¯­éŸ³ã€å†å²æ–‡æœ¬-ä¸‹ä¸€ä¸ªè¯­éŸ³ï¼Œä»¥åŠå†å²è¯­éŸ³-ä¸‹ä¸€ä¸ªæ–‡æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªåŸºäºå¯¹æ¯”å­¦ä¹ çš„å†…éƒ¨æ¨¡å¼å’Œä¸¤ä¸ªè·¨æ¨¡å¼äº¤äº’æ¨¡å—ï¼Œä»¥æ·±å…¥å­¦ä¹ å†…éƒ¨å’Œè·¨æ¨¡å¼çš„ä¸Šä¸‹æ–‡äº¤äº’ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨MDHå’Œè®­ç»ƒå¥½çš„äº¤äº’æ¨¡å—æ¥å®Œå…¨æ¨æ–­ç›®æ ‡è¯­å¥æ–‡æœ¬å†…å®¹çš„è¯­éŸ³éŸµå¾‹ã€‚åœ¨DailyTalkæ•°æ®é›†ä¸Šçš„ä¸»è§‚å’Œå®¢è§‚å®éªŒè¡¨æ˜ï¼ŒIII-CSSåœ¨è¡¨è¾¾éŸµå¾‹æ–¹é¢è¶…è¿‡äº†å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚ä»£ç å’Œè¯­éŸ³æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/I3CSS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/I3CSSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18733v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¼šè¯å†å²çš„å¤šæ¨¡æ€ä¿¡æ¯ç”Ÿæˆå…·æœ‰é€‚å½“ä¼šè¯éŸµå¾‹çš„ç›®æ ‡è¯­å¥çš„ä¼šè¯è¯­éŸ³åˆæˆï¼ˆCSSï¼‰çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå»ºæ¨¡å¤šæ¨¡æ€å¯¹è¯å†å²ï¼ˆMDHï¼‰ä¸ç›®æ ‡è¯­å¥ä¹‹é—´çš„äº¤äº’ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€äº¤äº’æ–¹æ¡ˆçš„æ–°å‹CSSç³»ç»Ÿâ€”â€”III-CSSã€‚é€šè¿‡ç»“åˆMDHä¸ç›®æ ‡è¯­å¥ä¸­çš„æ–‡æœ¬å’Œè¯­éŸ³æ¨¡æ€ï¼Œè®¾è®¡ä¸¤ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’æ¨¡å—ï¼Œä»¥æ·±åº¦å­¦ä¹ æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„ä¸Šä¸‹æ–‡äº¤äº’ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé‡‡ç”¨MDHå’Œè®­ç»ƒå¥½çš„äº¤äº’æ¨¡å—æ¥å®Œå…¨æ¨æ–­ç›®æ ‡è¯­å¥æ–‡æœ¬å†…å®¹çš„è¯­éŸ³éŸµå¾‹ã€‚å®éªŒè¡¨æ˜ï¼ŒIII-CSSåœ¨è¯­éŸ³éŸµå¾‹è¡¨è¾¾æ–¹é¢ä¼˜äºå…ˆè¿›åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼šè¯è¯­éŸ³åˆæˆï¼ˆCSSï¼‰æ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¯¹è¯å†å²ï¼ˆMDHï¼‰ç”Ÿæˆå…·æœ‰é€‚å½“ä¼šè¯éŸµå¾‹çš„ç›®æ ‡è¯­å¥ã€‚</li>
<li>CSSçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå»ºæ¨¡MDHä¸ç›®æ ‡è¯­å¥ä¹‹é—´çš„äº¤äº’ã€‚</li>
<li>æå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–°å‹CSSç³»ç»Ÿâ€”â€”III-CSSï¼ŒåŒ…å«æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’æ¨¡å—ã€‚</li>
<li>åœ¨è®­ç»ƒé˜¶æ®µï¼Œç»“åˆMDHå’Œç›®æ ‡è¯­å¥çš„å››ç§æ¨¡æ€ç»„åˆï¼ŒåŒ…æ‹¬å†å²æ–‡æœ¬-ä¸‹ä¸€æ¡æ–‡æœ¬ã€å†å²è¯­éŸ³-ä¸‹ä¸€æ¡è¯­éŸ³ç­‰ã€‚</li>
<li>III-CSSé€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„ä¸Šä¸‹æ–‡äº¤äº’ï¼Œæé«˜è¯­éŸ³éŸµå¾‹è¡¨è¾¾çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼Œé‡‡ç”¨MDHå’Œè®­ç»ƒå¥½çš„äº¤äº’æ¨¡å—æ¥æ¨æ–­ç›®æ ‡è¯­å¥çš„è¯­éŸ³éŸµå¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-979143359b4ad5896bf0d8a1751d7970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7261ee1f9986db45b99e5e6eee2c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a59a55380a464a5e34c2251b2622abfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f95b5b27fd7517c9047fa17252fac3cc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CosyVoice-2-Scalable-Streaming-Speech-Synthesis-with-Large-Language-Models"><a href="#CosyVoice-2-Scalable-Streaming-Speech-Synthesis-with-Large-Language-Models" class="headerlink" title="CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language   Models"></a>CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language   Models</h2><p><strong>Authors:Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, Jingren Zhou</strong></p>
<p>In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2">https://funaudiollm.github.io/cosyvoice2</a>. </p>
<blockquote>
<p>åœ¨ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CosyVoiceï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç›‘ç£ç¦»æ•£è¯­éŸ³æ ‡è®°çš„å¤šè¯­ç§è¯­éŸ³åˆæˆæ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨ä¸¤ç§æµè¡Œçš„ç”Ÿæˆæ¨¡å‹â€”â€”è¯­è¨€æ¨¡å‹å’ŒæµåŒ¹é…ï¼Œè¿›è¡Œæ¸è¿›å¼è¯­ä¹‰è§£ç ï¼ŒCosyVoiceåœ¨è¯­å¢ƒä¸­å­¦ä¹ è¯­éŸ³æ—¶è¡¨ç°å‡ºäº†é«˜åº¦çš„è¯­è°ƒè‡ªç„¶æ€§ã€å†…å®¹ä¸€è‡´æ€§å’Œè¯´è¯äººç›¸ä¼¼æ€§ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå…¶ä¸­è¯­éŸ³åˆæˆçš„å“åº”å»¶è¿Ÿå’Œå®æ—¶å› å­åœ¨äº¤äº’ä½“éªŒä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚å› æ­¤ï¼Œåœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æµå¼è¯­éŸ³åˆæˆæ¨¡å‹CosyVoice 2ï¼Œå®ƒåŒ…å«äº†å…¨é¢ç³»ç»Ÿçš„ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœ‰é™æ ‡é‡é‡åŒ–æ³•ä»¥æé«˜è¯­éŸ³æ ‡è®°çš„ä»£ç æœ¬åˆ©ç”¨ç‡ã€‚å¯¹äºæ–‡æœ¬-è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ç®€åŒ–äº†æ¨¡å‹æ¶æ„ï¼Œå…è®¸ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å—æ„ŸçŸ¥å› æœæµåŒ¹é…æ¨¡å‹ï¼Œä»¥æ”¯æŒå„ç§åˆæˆåœºæ™¯ï¼Œèƒ½åœ¨å•ä¸ªæ¨¡å‹å†…å®ç°æµå¼å’Œéæµå¼åˆæˆã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡å¤šè¯­ç§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒCosyVoice 2è¾¾åˆ°äº†ä¸äººç±»ç›¸å½“çš„è‡ªç„¶åº¦ã€æçŸ­çš„å“åº”å»¶è¿Ÿã€ä»¥åŠå‡ ä¹æ— æŸçš„åˆæˆè´¨é‡ã€‚æ¬¢è¿å¬ä¼—åœ¨<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2%E4%B8%8A%E8%AF%95%E5%90%AC%E6%BC%94%E7%A4%BA%E3%80%82">https://funaudiollm.github.io/cosyvoice2ä¸Šè¯•å¬æ¼”ç¤ºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10117v3">PDF</a> Tech report, work in progress</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå…ˆå‰çš„å·¥ä½œï¼Œæˆ‘ä»¬æ¨å‡ºäº†CosyVoice 2ï¼Œä¸€ä¸ªä¼˜åŒ–çš„æµå¼è¯­éŸ³åˆæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æœ‰é™æ ‡é‡é‡åŒ–æ”¹è¿›è¯­éŸ³ä»¤ç‰Œçš„ç æœ¬åˆ©ç”¨ç‡ï¼Œä¼˜åŒ–æ–‡æœ¬è¯­éŸ³çš„è¯­è¨€æ¨¡å‹æ¶æ„ä»¥ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†å—æ„ŸçŸ¥å› æœæµåŒ¹é…æ¨¡å‹ï¼Œæ”¯æŒå„ç§åˆæˆåœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹å†…å®ç°æµå¼å’Œéæµå¼åˆæˆã€‚é€šè¿‡å¤§è§„æ¨¡å¤šè¯­ç§æ•°æ®é›†çš„è®­ç»ƒï¼ŒCosyVoice 2è¾¾åˆ°äº†ä¸äººç±»è‡ªç„¶åº¦ç›¸å½“çš„æ°´å¹³ï¼Œå“åº”å»¶è¿Ÿæœ€å°åŒ–ï¼Œæµå¼åˆæˆè´¨é‡å‡ ä¹æ— æŸå¤±ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CosyVoice 2æ˜¯CosyVoiceçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ˜¯ä¸€ä¸ªä¼˜åŒ–çš„æµå¼è¯­éŸ³åˆæˆæ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨æœ‰é™æ ‡é‡é‡åŒ–æ”¹è¿›è¯­éŸ³ä»¤ç‰Œçš„ç æœ¬åˆ©ç”¨ç‡ï¼Œä»¥æé«˜è¯­éŸ³åˆæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>ä¼˜åŒ–æ–‡æœ¬è¯­éŸ³çš„è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œå…è®¸ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ã€‚</li>
<li>å¼€å‘å—æ„ŸçŸ¥å› æœæµåŒ¹é…æ¨¡å‹ï¼Œæ”¯æŒå„ç§åˆæˆåœºæ™¯ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ”¯æŒæµå¼å’Œéæµå¼åˆæˆä¸¤ç§æ¨¡å¼ï¼Œæ‰©å¤§äº†å…¶åº”ç”¨åœºæ™¯èŒƒå›´ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡å¤šè¯­ç§æ•°æ®é›†çš„è®­ç»ƒï¼ŒCosyVoice 2è¾¾åˆ°äº†é«˜æ°´å¹³çš„è‡ªç„¶åº¦ï¼Œä¸äººç±»è¡¨ç°ç›¸å½“ã€‚</li>
<li>æ¨¡å‹å…·æœ‰ä½å“åº”å»¶è¿Ÿå’Œå‡ ä¹æ— æŸå¤±çš„åˆæˆè´¨é‡ã€‚å¯é€šè¿‡è®¿é—®ç½‘ç«™ä½“éªŒæ¨¡å‹çš„æ¼”ç¤ºæ•ˆæœï¼š<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2">https://funaudiollm.github.io/cosyvoice2</a> ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f4d0cb499918d3d20046bc7c618b7a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013cedb83ae1d2a08609293706433d33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9641a2a57cf7e0560a9e11c7fd045d62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6e7db4f0253c8902f2ba241b051a88.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping"><a href="#Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping" class="headerlink" title="Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping"></a>Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping</h2><p><strong>Authors:Minki Kang, Wooseok Han, Eunho Yang</strong></p>
<p>Generating speech from a face image is crucial for developing virtual humans capable of interacting using their unique voices, without relying on pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech conditioned on a face image rather than reference speech. We hypothesize that learning entire prosodic features from a face image poses a significant challenge. To address this, our TTS model incorporates both face and prosody encoders. The prosody encoder is specifically designed to model speech style characteristics that are not fully captured by the face image, allowing the face encoder to focus on extracting speaker-specific features such as timbre. Experimental results demonstrate that Face-StyleSpeech effectively generates more natural speech from a face image than baselines, even for unseen faces. Samples are available on our demo page. </p>
<blockquote>
<p>ä»äººè„¸å›¾åƒç”Ÿæˆè¯­éŸ³å¯¹äºå¼€å‘èƒ½å¤Ÿä½¿ç”¨å…¶ç‹¬ç‰¹å£°éŸ³è¿›è¡Œäº¤äº’çš„è™šæ‹Ÿäººç±»è‡³å…³é‡è¦ï¼Œè€Œæ— éœ€ä¾èµ–é¢„å…ˆå½•åˆ¶çš„äººç±»è¯­éŸ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Face-StyleSpeechï¼Œè¿™æ˜¯ä¸€ç§é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹ï¼Œå®ƒæ ¹æ®äººè„¸å›¾åƒè€Œä¸æ˜¯å‚è€ƒè¯­éŸ³ç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚æˆ‘ä»¬å‡è®¾ä»äººè„¸å›¾åƒä¸­å­¦ä¹ æ‰€æœ‰çš„éŸµå¾‹ç‰¹å¾æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„TTSæ¨¡å‹ç»“åˆäº†äººè„¸å’ŒéŸµå¾‹ç¼–ç å™¨ã€‚éŸµå¾‹ç¼–ç å™¨ä¸“é—¨ç”¨äºå»ºæ¨¡è¯­éŸ³é£æ ¼ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ— æ³•å®Œå…¨ç”±äººè„¸å›¾åƒæ•è·ï¼Œä½¿å¾—äººè„¸ç¼–ç å™¨èƒ½å¤Ÿä¸“æ³¨äºæå–è¯´è¯äººç‰¹å®šçš„ç‰¹å¾ï¼Œå¦‚éŸ³è´¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFace-StyleSpeechèƒ½æœ‰æ•ˆåœ°ä»äººè„¸å›¾åƒç”Ÿæˆæ¯”åŸºçº¿æ›´è‡ªç„¶çš„è¯­éŸ³ï¼Œå³ä½¿å¯¹äºæœªè§è¿‡çš„é¢å­”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05844v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹â€”â€”Face-StyleSpeechï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®äººè„¸å›¾åƒç”Ÿæˆè‡ªç„¶è¯­éŸ³ï¼Œè€Œæ— éœ€å‚è€ƒé¢„å½•çš„è¯­éŸ³ã€‚é€šè¿‡èå…¥äººè„¸å’Œè¯­è°ƒç¼–ç å™¨ï¼Œæ¨¡å‹è§£å†³äº†ä»äººè„¸å›¾åƒå­¦ä¹ å…¨éƒ¨éŸµå¾‹ç‰¹å¾å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä½¿å¾—ç”Ÿæˆè¯­éŸ³æ›´åŠ è‡ªç„¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFace-StyleSpeechåœ¨ç”Ÿæˆäººè„¸å›¾åƒå¯¹åº”çš„è¯­éŸ³æ–¹é¢æ¯”åŸºçº¿æ–¹æ³•æ›´æœ‰æ•ˆï¼Œå³ä½¿å¯¹äºæœªè§è¿‡çš„é¢å­”ä¹Ÿèƒ½ç”Ÿæˆè‡ªç„¶çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Face-StyleSpeechæ˜¯ä¸€ç§é›¶æ ·æœ¬TTSåˆæˆæ¨¡å‹ï¼Œèƒ½æ ¹æ®äººè„¸å›¾åƒç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨äººè„¸å’Œè¯­è°ƒç¼–ç å™¨èåˆï¼Œè§£å†³ä»äººè„¸å›¾åƒå­¦ä¹ éŸµå¾‹ç‰¹å¾çš„æŒ‘æˆ˜ã€‚</li>
<li>äººè„¸ç¼–ç å™¨ä¸“æ³¨äºæå–å‘éŸ³è€…ç‰¹å®šç‰¹å¾ï¼Œå¦‚éŸ³è‰²ã€‚</li>
<li>æ¨¡å‹åœ¨ç”Ÿæˆè‡ªç„¶è¯­éŸ³æ–¹é¢æ¯”åŸºçº¿æ–¹æ³•æ›´æœ‰æ•ˆï¼Œå°¤å…¶å¯¹äºæœªè§è¿‡çš„é¢å­”ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆçš„è¯­éŸ³æ ·æœ¬å¯åœ¨æ¼”ç¤ºé¡µé¢æŸ¥çœ‹ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯¹äºå¼€å‘èƒ½å¤Ÿä½¿ç”¨ç‹¬ç‰¹å£°éŸ³è¿›è¡Œäº¤äº’çš„è™šæ‹Ÿäººå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.05844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83c43903c2a0bd6db5298f5d1fb98ba9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e7217e939336e06d64fe9d2e07ada4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-094ccae6af75cd56e466c323aea2fc39.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-f19af98ea50b7e78fd66906435a089f0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Dialogue Director Bridging the Gap in Dialogue Visualization for   Multimodal Storytelling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e9fc1ffc5a418cb6c4022963806495ef.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Unified dimensionality reduction techniques in chronic liver disease   detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
