<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Enhancing Multimodal Emotion Recognition through Multi-Granularity   Cross-Modal Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7a607f50797c02cdeadb60ff81187b3f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Enhancing-Multimodal-Emotion-Recognition-through-Multi-Granularity-Cross-Modal-Alignment"><a href="#Enhancing-Multimodal-Emotion-Recognition-through-Multi-Granularity-Cross-Modal-Alignment" class="headerlink" title="Enhancing Multimodal Emotion Recognition through Multi-Granularity   Cross-Modal Alignment"></a>Enhancing Multimodal Emotion Recognition through Multi-Granularity   Cross-Modal Alignment</h2><p><strong>Authors:Xuechen Wang, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiaming Zhou, Yong Qin</strong></p>
<p>Multimodal emotion recognition (MER), leveraging speech and text, has emerged as a pivotal domain within human-computer interaction, demanding sophisticated methods for effective multimodal integration. The challenge of aligning features across these modalities is significant, with most existing approaches adopting a singular alignment strategy. Such a narrow focus not only limits model performance but also fails to address the complexity and ambiguity inherent in emotional expressions. In response, this paper introduces a Multi-Granularity Cross-Modal Alignment (MGCMA) framework, distinguished by its comprehensive approach encompassing distribution-based, instance-based, and token-based alignment modules. This framework enables a multi-level perception of emotional information across modalities. Our experiments on IEMOCAP demonstrate that our proposed method outperforms current state-of-the-art techniques. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ï¼Œåˆ©ç”¨è¯­éŸ³å’Œæ–‡æœ¬ï¼Œå·²æˆä¸ºäººæœºäº¤äº’ä¸­çš„å…³é”®é¢†åŸŸï¼Œéœ€è¦æœ‰æ•ˆçš„æ–¹æ³•è¿›è¡Œå¤šæ¨¡æ€é›†æˆçš„é«˜çº§æ–¹æ³•ã€‚ä¸åŒæ¨¡æ€çš„ç‰¹å¾å¯¹é½æŒ‘æˆ˜å·¨å¤§ï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½é‡‡ç”¨äº†å•ä¸€çš„å¯¹é½ç­–ç•¥ã€‚è¿™ç§ç‹­çª„çš„ç„¦ç‚¹ä¸ä»…é™åˆ¶äº†æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸”æœªèƒ½è§£å†³æƒ…æ„Ÿè¡¨è¾¾ä¸­å›ºæœ‰çš„å¤æ‚æ€§å’Œæ¨¡ç³Šæ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå¤šç²’åº¦è·¨æ¨¡æ€å¯¹é½ï¼ˆMGCMAï¼‰çš„æ¡†æ¶ï¼Œå…¶æ˜¾è‘—ç‰¹ç‚¹åœ¨äºåŒ…æ‹¬åŸºäºåˆ†å¸ƒã€åŸºäºå®ä¾‹å’ŒåŸºäºæ ‡è®°çš„å¯¹é½æ¨¡å—çš„ç»¼åˆæ–¹æ³•ã€‚è¯¥æ¡†æ¶å®ç°äº†è·¨æ¨¡æ€çš„æƒ…æ„Ÿä¿¡æ¯å¤šçº§æ„ŸçŸ¥ã€‚æˆ‘ä»¬åœ¨IEMOCAPä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºå½“å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20821v1">PDF</a> ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech   and Signal Processing (ICASSP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰é¢†åŸŸçš„é‡è¦æ€§å’ŒæŒ‘æˆ˜ï¼Œè¯¥é¢†åŸŸåˆ©ç”¨è¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæ˜¯äººä¸è®¡ç®—æœºäº¤äº’ä¸­çš„ä¸€ä¸ªå…³é”®é¢†åŸŸã€‚ä¸ºäº†æœ‰æ•ˆè§£å†³å¤šæ¨¡æ€èåˆçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šç²’åº¦è·¨æ¨¡æ€å¯¹é½ï¼ˆMGCMAï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬åˆ†å¸ƒå¯¹é½ã€å®ä¾‹å¯¹é½å’Œä»¤ç‰Œå¯¹é½ä¸‰ä¸ªæ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå±‚æ¬¡ä¸Šæ„ŸçŸ¥è·¨æ¨¡æ€çš„æƒ…æ„Ÿä¿¡æ¯ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰å·²æˆä¸ºäººä¸è®¡ç®—æœºäº¤äº’ä¸­çš„å…³é”®é¢†åŸŸï¼Œéœ€è¦å…ˆè¿›çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨å•ä¸€çš„æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œå­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å¤„ç†æƒ…æ„Ÿè¡¨è¾¾çš„å¤æ‚æ€§å’Œæ¨¡ç³Šæ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å¤šç²’åº¦è·¨æ¨¡æ€å¯¹é½ï¼ˆMGCMAï¼‰æ¡†æ¶ï¼ŒåŒ…å«åˆ†å¸ƒå¯¹é½ã€å®ä¾‹å¯¹é½å’Œä»¤ç‰Œå¯¹é½ä¸‰ä¸ªæ¨¡å—ï¼Œä»¥å…¨é¢åº”å¯¹æƒ…æ„Ÿä¿¡æ¯çš„å¤šå±‚æ¬¡æ„ŸçŸ¥éœ€æ±‚ã€‚</li>
<li>MGCMAæ¡†æ¶é€šè¿‡ç»¼åˆä¸åŒç²’åº¦çš„ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½å’Œå¯¹æƒ…æ„Ÿè¡¨è¾¾çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMGCMAæ¡†æ¶çš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰æœ›ä¸ºæœªæ¥çš„æƒ…æ„Ÿè¯†åˆ«å’Œäººæœºäº¤äº’æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-925933c81992edcf86e21260ffb67b40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832fcd86bad994c1e8572432a52f780b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5e603cfce5deebc035c39002ca9beaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cd7a94d7a8791b44504b5751e637bfd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Metadata-Enhanced-Speech-Emotion-Recognition-Augmented-Residual-Integration-and-Co-Attention-in-Two-Stage-Fine-Tuning"><a href="#Metadata-Enhanced-Speech-Emotion-Recognition-Augmented-Residual-Integration-and-Co-Attention-in-Two-Stage-Fine-Tuning" class="headerlink" title="Metadata-Enhanced Speech Emotion Recognition: Augmented Residual   Integration and Co-Attention in Two-Stage Fine-Tuning"></a>Metadata-Enhanced Speech Emotion Recognition: Augmented Residual   Integration and Co-Attention in Two-Stage Fine-Tuning</h2><p><strong>Authors:Zixiang Wan, Ziyue Qiu, Yiyang Liu, Wei-Qiang Zhang</strong></p>
<p>Speech Emotion Recognition (SER) involves analyzing vocal expressions to determine the emotional state of speakers, where the comprehensive and thorough utilization of audio information is paramount. Therefore, we propose a novel approach on self-supervised learning (SSL) models that employs all available auxiliary information â€“ specifically metadata â€“ to enhance performance. Through a two-stage fine-tuning method in multi-task learning, we introduce the Augmented Residual Integration (ARI) module, which enhances transformer layers in encoder of SSL models. The module efficiently preserves acoustic features across all different levels, thereby significantly improving the performance of metadata-related auxiliary tasks that require various levels of features. Moreover, the Co-attention module is incorporated due to its complementary nature with ARI, enabling the model to effectively utilize multidimensional information and contextual relationships from metadata-related auxiliary tasks. Under pre-trained base models and speaker-independent setup, our approach consistently surpasses state-of-the-art (SOTA) models on multiple SSL encoders for the IEMOCAP dataset. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ¶‰åŠåˆ†æè¯­éŸ³è¡¨è¾¾ä»¥ç¡®å®šè¯´è¯äººçš„æƒ…ç»ªçŠ¶æ€ï¼Œå…¶ä¸­å…¨é¢å½»åº•åœ°åˆ©ç”¨éŸ³é¢‘ä¿¡æ¯è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„è¾…åŠ©ä¿¡æ¯â€”â€”ç‰¹åˆ«æ˜¯å…ƒæ•°æ®â€”â€”æ¥æé«˜æ€§èƒ½ã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¢å¼ºæ®‹å·®é›†æˆï¼ˆARIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¢å¼ºäº†SSLæ¨¡å‹çš„ç¼–ç å™¨ä¸­çš„å˜å‹å™¨å±‚ã€‚è¯¥æ¨¡å—æœ‰æ•ˆåœ°ä¿ç•™äº†æ‰€æœ‰ä¸åŒçº§åˆ«çš„å£°å­¦ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ä¸å…ƒæ•°æ®ç›¸å…³çš„è¾…åŠ©ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦ä¸åŒçº§åˆ«çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç”±äºARIçš„äº’è¡¥æ€§è´¨ï¼Œè¿˜ç»“åˆäº†ååŒæ³¨æ„æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ¥è‡ªå…ƒæ•°æ®ç›¸å…³è¾…åŠ©ä»»åŠ¡çš„å¤šç»´ä¿¡æ¯å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚åœ¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å’Œç‹¬ç«‹äºè¯´è¯äººçš„è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªSSLç¼–ç å™¨ä¸Šçš„è¡¨ç°å§‹ç»ˆè¶…è¿‡äº†IEMOCAPæ•°æ®é›†çš„æœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20707v1">PDF</a> accepted by ICASSP2025. \c{opyright}2025 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, in any current or future media, including reprinting&#x2F;republishing this   material for advertising or promotional purposes, creating new collective   works, for resale or redistribution to servers or lists, or reuse of any   copyrighted component</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„è¾…åŠ©ä¿¡æ¯â€”â€”ç‰¹åˆ«æ˜¯å…ƒæ•°æ®â€”â€”æ¥æé«˜æ€§èƒ½ã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œå¼•å…¥äº†å¢å¼ºæ®‹å·®é›†æˆï¼ˆARIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¢å¼ºäº†SSLæ¨¡å‹çš„ç¼–ç å™¨ä¸­çš„å˜å‹å™¨å±‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ååŒæ³¨æ„æ¨¡å—ï¼Œä¸ARIæ¨¡å—äº’è¡¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ¥è‡ªå…ƒæ•°æ®ç›¸å…³è¾…åŠ©ä»»åŠ¡çš„å¤šç»´ä¿¡æ¯å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚åœ¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å’Œç‹¬ç«‹äºè¯´è¯äººçš„è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„è¡¨ç°æŒç»­è¶…è¶Šäº†æœ€æ–°çš„SSLç¼–ç å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é€šè¿‡åˆ†æè¯­éŸ³è¡¨è¾¾æ¥ç¡®å®šè¯´è¯äººçš„æƒ…ç»ªçŠ¶æ€ï¼Œå…¶ä¸­éŸ³é¢‘ä¿¡æ¯çš„å…¨é¢å’Œå½»åº•åˆ©ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨åŒ…æ‹¬å…ƒæ•°æ®åœ¨å†…çš„æ‰€æœ‰å¯ç”¨è¾…åŠ©ä¿¡æ¯æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†å¢å¼ºæ®‹å·®é›†æˆï¼ˆARIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¢å¼ºäº†SSLæ¨¡å‹çš„ç¼–ç å™¨ä¸­çš„å˜å‹å™¨å±‚ï¼Œèƒ½å¤Ÿä¿ç•™ä¸åŒå±‚æ¬¡çš„å£°å­¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•å’Œå¤šä»»åŠ¡å­¦ä¹ ï¼ŒARIæ¨¡å—æ˜¾è‘—æé«˜å…ƒæ•°æ®ç›¸å…³è¾…åŠ©ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ååŒæ³¨æ„æ¨¡å—ï¼Œä¸ARIæ¨¡å—äº’è¡¥ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å…ƒæ•°æ®ç›¸å…³è¾…åŠ©ä»»åŠ¡çš„å¤šç»´ä¿¡æ¯å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚</li>
<li>åœ¨ç‹¬ç«‹äºè¯´è¯äººçš„è®¾ç½®å’Œé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†æœ€æ–°çš„SSLç¼–ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc9abc02175e692a828fc54637ec15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b6c9d3b77df56f988da1e4665d86f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3113693ac03c41e3e659ba6be2ca843a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f002700029ef876934903f59cdce9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b2249e4384c228829b720b28e759509.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de938097f8491e09e30e3ef78364d34.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Utilizing-Multimodal-Data-for-Edge-Case-Robust-Call-sign-Recognition-and-Understanding"><a href="#Utilizing-Multimodal-Data-for-Edge-Case-Robust-Call-sign-Recognition-and-Understanding" class="headerlink" title="Utilizing Multimodal Data for Edge Case Robust Call-sign Recognition and   Understanding"></a>Utilizing Multimodal Data for Edge Case Robust Call-sign Recognition and   Understanding</h2><p><strong>Authors:Alexander Blatt, Dietrich Klakow</strong></p>
<p>Operational machine-learning based assistant systems must be robust in a wide range of scenarios. This hold especially true for the air-traffic control (ATC) domain. The robustness of an architecture is particularly evident in edge cases, such as high word error rate (WER) transcripts resulting from noisy ATC recordings or partial transcripts due to clipped recordings. To increase the edge-case robustness of call-sign recognition and understanding (CRU), a core tasks in ATC speech processing, we propose the multimodal call-sign-command recovery model (CCR). The CCR architecture leads to an increase in the edge case performance of up to 15%. We demonstrate this on our second proposed architecture, CallSBERT. A CRU model that has less parameters, can be fine-tuned noticeably faster and is more robust during fine-tuning than the state of the art for CRU. Furthermore, we demonstrate that optimizing for edge cases leads to a significantly higher accuracy across a wide operational range. </p>
<blockquote>
<p>åŸºäºæ“ä½œæœºå™¨å­¦ä¹ çš„è¾…åŠ©ç³»ç»Ÿå¿…é¡»åœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚è¿™åœ¨èˆªç©ºäº¤é€šç®¡åˆ¶ï¼ˆATCï¼‰é¢†åŸŸå°¤ä¸ºå‡†ç¡®ã€‚ä¸€ä¸ªæ¶æ„çš„ç¨³å¥æ€§åœ¨æç«¯æƒ…å†µä¸‹å°¤ä¸ºæ˜æ˜¾ï¼Œä¾‹å¦‚ç”±äºå˜ˆæ‚çš„ATCå½•éŸ³å¯¼è‡´çš„é«˜å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è½¬å½•æˆ–ç”±äºå‰ªè¾‘å½•éŸ³å¯¼è‡´çš„éƒ¨åˆ†è½¬å½•ã€‚ä¸ºäº†æé«˜èˆªç©ºäº¤é€šç®¡åˆ¶è¯­éŸ³è¯†åˆ«å’Œç†è§£çš„è¾¹ç¼˜æƒ…å†µç¨³å¥æ€§ï¼ˆCRUæ˜¯èˆªç©ºäº¤é€šç®¡åˆ¶è¯­éŸ³è¯†åˆ«çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡å¼å‘¼å«æ ‡å¿—å‘½ä»¤æ¢å¤æ¨¡å‹ï¼ˆCCRï¼‰ã€‚CCRæ¶æ„æé«˜äº†è¾¹ç¼˜æƒ…å†µæ€§èƒ½ï¼Œæœ€é«˜å¯è¾¾15%ã€‚æˆ‘ä»¬åœ¨ç¬¬äºŒä¸ªæå‡ºçš„æ¶æ„CallSBERTä¸Šå±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚CRUæ¨¡å‹å‚æ•°æ›´å°‘ï¼Œå¾®è°ƒé€Ÿåº¦æ˜æ˜¾æ›´å¿«ï¼Œå¹¶ä¸”åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ¯”ç°æœ‰çš„CRUæŠ€æœ¯æ›´åŠ ç¨³å¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†é’ˆå¯¹æç«¯æƒ…å†µè¿›è¡Œä¼˜åŒ–å¯ä»¥åœ¨å¹¿æ³›çš„è¿è¡ŒèŒƒå›´å†…æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨èˆªç©ºäº¤é€šç®¡åˆ¶ï¼ˆATCï¼‰é¢†åŸŸä¸­ï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„åŠ©ç†ç³»ç»Ÿéœ€è¦åœ¨å„ç§åœºæ™¯ä¸‹å…·å¤‡ç¨³å¥æ€§ã€‚ä¸ºæé«˜å‘¼å«ç¬¦å·è¯†åˆ«ä¸ç†è§£ï¼ˆCRUï¼‰çš„è¾¹ç¼˜æƒ…å†µç¨³å¥æ€§ï¼Œæå‡ºå¤šæ¨¡æ€å‘¼å«ç¬¦å·å‘½ä»¤æ¢å¤æ¨¡å‹ï¼ˆCCRï¼‰ã€‚CCRæ¶æ„æé«˜äº†è¾¹ç¼˜æ¡ˆä¾‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°15%ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å…·æœ‰æ›´å°‘å‚æ•°ã€æ›´å¿«å¾®è°ƒé€Ÿåº¦å’Œæ›´ç²¾ç»†çš„ç¨³å¥æ€§çš„CallSBERTæ¨¡å‹ã€‚ä¼˜åŒ–è¾¹ç¼˜æƒ…å†µæ˜¾è‘—æé«˜å®½æ“ä½œèŒƒå›´å†…çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åŠ©ç†ç³»ç»Ÿåœ¨èˆªç©ºäº¤é€šç®¡åˆ¶é¢†åŸŸéœ€å…·å¤‡ç¨³å¥æ€§ä»¥åº”å¯¹å„ç§åœºæ™¯ã€‚</li>
<li>å‘¼å«ç¬¦å·è¯†åˆ«ä¸ç†è§£ï¼ˆCRUï¼‰çš„è¾¹ç¼˜æƒ…å†µç¨³å¥æ€§å¯¹ATCè¯­éŸ³è¯†åˆ«è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºçš„å¤šæ¨¡æ€å‘¼å«ç¬¦å·å‘½ä»¤æ¢å¤æ¨¡å‹ï¼ˆCCRï¼‰æé«˜äº†è¾¹ç¼˜æ¡ˆä¾‹çš„æ€§èƒ½ã€‚</li>
<li>CCRæ¶æ„åœ¨æå‡è¾¹ç¼˜æ¡ˆä¾‹æ€§èƒ½æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œæ€§èƒ½æå‡è¾¾15%ã€‚</li>
<li>CallSBERTæ¨¡å‹å…·æœ‰æ›´å°‘å‚æ•°ã€æ›´å¿«çš„å¾®è°ƒé€Ÿåº¦å’Œæ›´é«˜çš„ç¨³å¥æ€§ã€‚</li>
<li>ä¼˜åŒ–è¾¹ç¼˜æƒ…å†µæ˜¾è‘—æé«˜åœ¨å„ç§æ“ä½œåœºæ™¯ä¸‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b76e371d3eb570011f617ff18932f20c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9df7e533aade730fe756e0a6a17814d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f040360051264a7654ea56fa9d501b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e736e9bb8c2ccd95a172d525fc426e19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44548341ab4429ab8171de8637922b39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6553faabe68ad8fc08bb1a0e383432d0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Distance-Based-Single-Channel-Target-Speech-Extraction"><a href="#Distance-Based-Single-Channel-Target-Speech-Extraction" class="headerlink" title="Distance Based Single-Channel Target Speech Extraction"></a>Distance Based Single-Channel Target Speech Extraction</h2><p><strong>Authors:Runwu Shi, Benjamin Yen, Kazuhiro Nakadai</strong></p>
<p>This paper aims to achieve single-channel target speech extraction (TSE) in enclosures by solely utilizing distance information. This is the first work that utilizes only distance cues without using speaker physiological information for single-channel TSE. Inspired by recent single-channel Distance-based separation and extraction methods, we introduce a novel model that efficiently fuses distance information with time-frequency (TF) bins for TSE. Experimental results in both single-room and multi-room scenarios demonstrate the feasibility and effectiveness of our approach. This method can also be employed to estimate the distances of different speakers in mixed speech. Online demos are available at <a target="_blank" rel="noopener" href="https://runwushi.github.io/distance-demo-page">https://runwushi.github.io/distance-demo-page</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡ä»…åˆ©ç”¨è·ç¦»ä¿¡æ¯å®ç°å°é—­ç¯å¢ƒä¸­çš„å•é€šé“ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ã€‚è¿™æ˜¯ç¬¬ä¸€é¡¹ä»…ä½¿ç”¨è·ç¦»çº¿ç´¢è€Œæ— éœ€è¯´è¯äººç”Ÿç†ä¿¡æ¯è¿›è¡Œå•é€šé“TSEçš„å·¥ä½œã€‚å—æœ€è¿‘çš„åŸºäºå•é€šé“è·ç¦»çš„åˆ†ç¦»å’Œæå–æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å°†è·ç¦»ä¿¡æ¯ä¸æ—¶é—´é¢‘ç‡ï¼ˆTFï¼‰ç»“åˆè¿›è¡ŒTSEã€‚åœ¨å•å®¤å’Œå¤šå®¤åœºæ™¯ä¸‹çš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä¼°è®¡æ··åˆè¯­éŸ³ä¸­ä¸åŒè¯´è¯äººçš„è·ç¦»ã€‚åœ¨çº¿æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://runwoshi.github.io/distance-demo-page%E3%80%82">https://runwoshi.github.io/distance-demo-pageã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20144v1">PDF</a> 5 pages, 3 figures, accepted by ICASSP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬è®ºæ–‡æ—¨åœ¨å®ç°å•é€šé“ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ï¼Œä»…åˆ©ç”¨è·ç¦»ä¿¡æ¯åœ¨å°é—­ç¯å¢ƒä¸­è¿›è¡Œã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•åœ¨ä¸ä½¿ç”¨è¯´è¯äººç”Ÿç†ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è·ç¦»çº¿ç´¢è¿›è¡Œå•é€šé“TSEã€‚å—è¿‘æœŸå•é€šé“åŸºäºè·ç¦»çš„åˆ†ç¦»å’Œæå–æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°å°†è·ç¦»ä¿¡æ¯ä¸æ—¶é—´é¢‘ç‡ï¼ˆTFï¼‰ä»“è¿›è¡Œèåˆä»¥å®ç°TSEã€‚åœ¨å•å®¤å’Œå¤šå®¤åœºæ™¯ä¸‹çš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤æ–¹æ³•ä¹Ÿå¯ç”¨äºä¼°ç®—æ··åˆè¯­éŸ³ä¸­ä¸åŒè¯´è¯äººçš„è·ç¦»ã€‚åœ¨çº¿æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://runwoshi.github.io/distance-demo-page%E3%80%82">https://runwoshi.github.io/distance-demo-pageã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬è®ºæ–‡å®ç°äº†ä»…åˆ©ç”¨è·ç¦»ä¿¡æ¯è¿›è¡Œå•é€šé“ç›®æ ‡è¯­éŸ³æå–çš„æ–¹æ³•ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„é™åˆ¶ã€‚</li>
<li>é¦–æ¬¡åœ¨ä¸ä½¿ç”¨è¯´è¯äººç”Ÿç†ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è·ç¦»çº¿ç´¢è¿›è¡Œå•é€šé“TSEã€‚</li>
<li>æå‡ºäº†ä¸€ç§èåˆè·ç¦»ä¿¡æ¯ä¸æ—¶é—´é¢‘ç‡ä»“çš„æ–°å‹æ¨¡å‹ï¼Œä»¥æé«˜TSEæ•ˆç‡ã€‚</li>
<li>åœ¨å•å®¤å’Œå¤šå®¤åœºæ™¯ä¸‹è¿›è¡Œçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åº”ç”¨äºä¼°ç®—æ··åˆè¯­éŸ³ä¸­ä¸åŒè¯´è¯äººçš„è·ç¦»ã€‚</li>
<li>è®ºæ–‡æä¾›åœ¨çº¿æ¼”ç¤ºï¼Œä¾¿äºè¿›ä¸€æ­¥äº†è§£å’ŒéªŒè¯æ‰€æå‡ºçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d505b166d46977d4f188eca8049b8011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f66383f256f97ab3885e74da347f35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cf7ea8b71d4794f15f732a4422b5a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4701137f5a14d9d35cea6c59bb209fae.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation"><a href="#CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation" class="headerlink" title="CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation"></a>CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation</h2><p><strong>Authors:Ji-Hoon Kim, Hong-Sun Yang, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Joon Son Chung</strong></p>
<p>The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯åœ¨å¤šç§è¯­è¨€ç”Ÿæˆè‡ªç„¶è¯­éŸ³çš„åŒæ—¶ä¿æŒç›¸åŒçš„è¯´è¯äººèº«ä»½ï¼Œè¿™ä¸€ä»»åŠ¡è¢«ç§°ä¸ºè·¨è¯­è¨€è¯­éŸ³åˆæˆã€‚è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯è¯­è¨€ä¸è¯´è¯äººçš„çº ç¼ é—®é¢˜ï¼Œè¿™å¯¼è‡´è·¨è¯­è¨€ç³»ç»Ÿçš„è´¨é‡è½åäºå•è¯­è¨€ç³»ç»Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CrossSpeech++ï¼Œå®ƒæœ‰æ•ˆåœ°è§£å¼€äº†è¯­è¨€å’Œè¯´è¯äººçš„ä¿¡æ¯ï¼Œå¹¶å¤§å¤§æé«˜äº†è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å¤æ‚çš„è¯­éŸ³ç”Ÿæˆç®¡é“åˆ†è§£ä¸ºä¸¤ä¸ªç®€å•çš„ç»„ä»¶ï¼šè¯­è¨€ç›¸å…³ç”Ÿæˆå™¨å’Œè¯´è¯äººç›¸å…³ç”Ÿæˆå™¨ã€‚è¯­è¨€ç›¸å…³ç”Ÿæˆå™¨äº§ç”Ÿä¸å—ç‰¹å®šè¯´è¯äººå±æ€§å½±å“çš„è¯­è¨€å˜åŒ–ã€‚è¯´è¯äººç›¸å…³ç”Ÿæˆå™¨å¯¹è¡¨å¾è¯´è¯äººèº«ä»½çš„å£°å­¦å˜åŒ–è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡åœ¨ä¸åŒçš„æ¨¡å—ä¸­å¤„ç†æ¯ç§ç±»å‹çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å¼€è¯­è¨€å’Œè¯´è¯äººçš„è¡¨ç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨å„ç§åº¦é‡æŒ‡æ ‡è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜CrossSpeech++åœ¨è·¨è¯­è¨€è¯­éŸ³åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20048v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­çš„è¯­è¨€ä¸è¯´è¯è€…çº ç¼ é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºCrossSpeech++çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆåˆ†ç¦»è¯­è¨€å’Œè¯´è¯è€…ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚é€šè¿‡æŠŠå¤æ‚çš„è¯­éŸ³ç”Ÿæˆç®¡é“åˆ†è§£æˆä¸¤ä¸ªç®€å•ç»„ä»¶ï¼šè¯­è¨€ç›¸å…³ç”Ÿæˆå™¨å’Œè¯´è¯è€…ç›¸å…³ç”Ÿæˆå™¨ï¼Œåˆ†åˆ«å¤„ç†è¯­è¨€å’Œè¯´è¯è€…ä¿¡æ¯ï¼Œä»è€Œå®ç°è¯­è¨€ä¸è¯´è¯è€…çš„æœ‰æ•ˆåˆ†ç¦»ã€‚å®éªŒè¯æ˜ï¼ŒCrossSpeech++åœ¨è·¨è¯­è¨€è¯­éŸ³åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ—¨åœ¨å®ç°è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­çš„è¯­è¨€ä¸è¯´è¯è€…ä¿¡æ¯çš„æœ‰æ•ˆåˆ†ç¦»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºCrossSpeech++çš„æ–°æ–¹æ³•ï¼Œå°†å¤æ‚çš„è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªç®€å•ç»„ä»¶æ¥å¤„ç†è¯­è¨€å’Œè¯´è¯è€…ä¿¡æ¯ã€‚</li>
<li>è¯­è¨€ç›¸å…³ç”Ÿæˆå™¨å¯ä»¥äº§ç”Ÿä¸å—ç‰¹å®šè¯´è¯äººå±æ€§å½±å“çš„è¯­è¨€å˜åŒ–ã€‚</li>
<li>è¯´è¯è€…ç›¸å…³ç”Ÿæˆå™¨å¯ä»¥æ¨¡æ‹Ÿè¯´è¯äººçš„å£°å­¦ç‰¹å¾å˜åŒ–ã€‚</li>
<li>é€šè¿‡åˆ†ç¦»è¯­è¨€å’Œè¯´è¯è€…ä¿¡æ¯ï¼ŒCrossSpeech++æé«˜äº†è·¨è¯­è¨€è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCrossSpeech++åœ¨è·¨è¯­è¨€è¯­éŸ³åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œå¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a144310e8743d76fe946abe22982a1c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b27c17333f4be5fcd573abf6e8c687e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71a3e4b4f613f066ba8c895460a3ea06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-301cbe8ec42efceaaf9a0db2aa09d03b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f9d189a4c973d41d917039da036b142.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniAvatar-Taming-Lifelike-Audio-Driven-Talking-Head-Generation-with-Comprehensive-Motion-and-Lighting-Control"><a href="#UniAvatar-Taming-Lifelike-Audio-Driven-Talking-Head-Generation-with-Comprehensive-Motion-and-Lighting-Control" class="headerlink" title="UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with   Comprehensive Motion and Lighting Control"></a>UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with   Comprehensive Motion and Lighting Control</h2><p><strong>Authors:Wenzhang Sun, Xiang Li, Donglin Di, Zhuding Liang, Qiyuan Zhang, Hao Li, Wei Chen, Jianxun Cui</strong></p>
<p>Recently, animating portrait images using audio input is a popular task. Creating lifelike talking head videos requires flexible and natural movements, including facial and head dynamics, camera motion, realistic light and shadow effects. Existing methods struggle to offer comprehensive, multifaceted control over these aspects. In this work, we introduce UniAvatar, a designed method that provides extensive control over a wide range of motion and illumination conditions. Specifically, we use the FLAME model to render all motion information onto a single image, maintaining the integrity of 3D motion details while enabling fine-grained, pixel-level control. Beyond motion, this approach also allows for comprehensive global illumination control. We design independent modules to manage both 3D motion and illumination, permitting separate and combined control. Extensive experiments demonstrate that our method outperforms others in both broad-range motion control and lighting control. Additionally, to enhance the diversity of motion and environmental contexts in current datasets, we collect and plan to publicly release two datasets, DH-FaceDrasMvVid-100 and DH-FaceReliVid-200, which capture significant head movements during speech and various lighting scenarios. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä½¿ç”¨éŸ³é¢‘è¾“å…¥æ¥ç”ŸæˆåŠ¨ç”»è‚–åƒå›¾åƒæ˜¯ä¸€é¡¹çƒ­é—¨ä»»åŠ¡ã€‚åˆ›å»ºé€¼çœŸçš„è°ˆè¯å¤´éƒ¨è§†é¢‘éœ€è¦çµæ´»å’Œè‡ªç„¶çš„åŠ¨ä½œï¼ŒåŒ…æ‹¬é¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€ã€ç›¸æœºè¿åŠ¨ã€é€¼çœŸçš„å…‰å½±æ•ˆæœã€‚ç°æœ‰æ–¹æ³•å¾ˆéš¾åœ¨è¿™äº›æ–¹é¢æä¾›å…¨é¢ã€å¤šå…ƒçš„æ§åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniAvataræ–¹æ³•ï¼Œè¯¥æ–¹æ³•æä¾›äº†å¯¹å„ç§è¿åŠ¨å’Œç…§æ˜æ¡ä»¶çš„å¹¿æ³›æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨FLAMEæ¨¡å‹å°†æ‰€æœ‰è¿åŠ¨ä¿¡æ¯æ¸²æŸ“åˆ°å•ä¸ªå›¾åƒä¸Šï¼Œä¿æŒ3Dè¿åŠ¨ç»†èŠ‚çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶å®ç°ç²¾ç»†çš„åƒç´ çº§æ§åˆ¶ã€‚é™¤äº†è¿åŠ¨ä¹‹å¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜å…è®¸è¿›è¡Œå…¨é¢çš„å…¨å±€ç…§æ˜æ§åˆ¶ã€‚æˆ‘ä»¬è®¾è®¡ç‹¬ç«‹çš„æ¨¡å—æ¥ç®¡ç†3Dè¿åŠ¨å’Œç…§æ˜ï¼Œä»¥å®ç°å•ç‹¬å’Œç»„åˆæ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„è¿åŠ¨æ§åˆ¶å’Œç…§æ˜æ§åˆ¶æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å½“å‰æ•°æ®é›†ä¸­è¿åŠ¨å’Œç¯å¢ƒä¸Šä¸‹æ–‡çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æ”¶é›†å¹¶è®¡åˆ’å…¬å¼€å‘å¸ƒä¸¤ä¸ªæ•°æ®é›†DH-FaceDrasMvVid-100å’ŒDH-FaceReliVid-200ï¼Œå®ƒä»¬æ•æ‰äº†æ¼”è®²è¿‡ç¨‹ä¸­çš„é‡è¦å¤´éƒ¨è¿åŠ¨ä»¥åŠå„ç§ç…§æ˜åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19860v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸï¼Œä½¿ç”¨éŸ³é¢‘è¾“å…¥åˆ¶ä½œåŠ¨æ€è‚–åƒå›¾åƒæ˜¯ä¸€é¡¹çƒ­é—¨ä»»åŠ¡ã€‚åˆ›å»ºé€¼çœŸçš„è°ˆè¯å¤´è§†é¢‘éœ€è¦çµæ´»è‡ªç„¶çš„åŠ¨ä½œï¼ŒåŒ…æ‹¬é¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€ã€ç›¸æœºè¿åŠ¨ã€çœŸå®çš„å…‰å½±æ•ˆæœã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å…¨é¢æ§åˆ¶è¿™äº›æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UniAvataræ–¹æ³•ï¼Œæä¾›å¹¿æ³›çš„åŠ¨ä½œå’Œç…§æ˜æ¡ä»¶ä¸‹çš„å…¨é¢æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨FLAMEæ¨¡å‹å°†æ‰€æœ‰åŠ¨ä½œä¿¡æ¯æ¸²æŸ“åˆ°ä¸€å¼ å›¾åƒä¸Šï¼Œä¿æŒ3DåŠ¨ä½œç»†èŠ‚çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶å®ç°ç²¾ç»†çš„åƒç´ çº§æ§åˆ¶ã€‚é™¤äº†åŠ¨ä½œæ§åˆ¶å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å…è®¸å…¨é¢çš„å…¨å±€ç…§æ˜æ§åˆ¶ã€‚æˆ‘ä»¬è®¾è®¡ç‹¬ç«‹çš„æ¨¡å—æ¥ç®¡ç†3DåŠ¨ä½œå’Œç…§æ˜ï¼Œå…è®¸å•ç‹¬å’Œç»„åˆæ§åˆ¶ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„åŠ¨ä½œæ§åˆ¶å’Œç…§æ˜æ§åˆ¶æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºå½“å‰æ•°æ®é›†çš„åŠ¨ä½œå’Œç¯å¢ƒä¸Šä¸‹æ–‡å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æ”¶é›†å’Œè®¡åˆ’å…¬å¼€ä¸¤ä¸ªæ•°æ®é›†DH-FaceDrasMvVid-100å’ŒDH-FaceReliVid-200ï¼Œå®ƒä»¬æ•æ‰äº†æ¼”è®²è¿‡ç¨‹ä¸­çš„é‡å¤§å¤´éƒ¨åŠ¨ä½œå’Œå„ç§ç…§æ˜åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä½¿ç”¨éŸ³é¢‘è¾“å…¥åˆ›å»ºåŠ¨æ€è‚–åƒå›¾åƒæ˜¯çƒ­é—¨ä»»åŠ¡ï¼Œéœ€å®ç°çµæ´»è‡ªç„¶çš„åŠ¨ä½œå’Œå…‰å½±æ•ˆæœã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åŠ¨ä½œå’Œç…§æ˜æ§åˆ¶æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>UniAvataræ–¹æ³•æä¾›å¹¿æ³›çš„åŠ¨ä½œå’Œç…§æ˜æ¡ä»¶ä¸‹çš„å…¨é¢æ§åˆ¶ã€‚</li>
<li>ä½¿ç”¨FLAMEæ¨¡å‹å®ç°3DåŠ¨ä½œç»†èŠ‚çš„å®Œæ•´æ€§å’Œç²¾ç»†çš„åƒç´ çº§æ§åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸ç‹¬ç«‹çš„åŠ¨ä½œå’Œç…§æ˜æ§åˆ¶æ¨¡å—ã€‚</li>
<li>å®éªŒè¯æ˜UniAvataræ–¹æ³•åœ¨åŠ¨ä½œå’Œç…§æ˜æ§åˆ¶æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce33f782831d00d41d6af0a841793817.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c7b5f4400ea613c7ec09009b5313fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb230f76e9ab08e2df9ed9f1277e8ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bf6cd5153f33d83b4b5ac3ab45b6ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82df05efdeddf0ac4de28269cfc77d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb380341b10903c1c07db2aa096186aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Whisperâ€™s-Accuracy-and-Speed-for-Indian-Languages-through-Prompt-Tuning-and-Tokenization"><a href="#Enhancing-Whisperâ€™s-Accuracy-and-Speed-for-Indian-Languages-through-Prompt-Tuning-and-Tokenization" class="headerlink" title="Enhancing Whisperâ€™s Accuracy and Speed for Indian Languages through   Prompt-Tuning and Tokenization"></a>Enhancing Whisperâ€™s Accuracy and Speed for Indian Languages through   Prompt-Tuning and Tokenization</h2><p><strong>Authors:Kumud Tripathi, Raj Gothi, Pankaj Wasnik</strong></p>
<p>Automatic speech recognition has recently seen a significant advancement with large foundational models such as Whisper. However, these models often struggle to perform well in low-resource languages, such as Indian languages. This paper explores two novel approaches to enhance Whisperâ€™s multilingual speech recognition performance in Indian languages. First, we propose prompt-tuning with language family information, which enhances Whisperâ€™s accuracy in linguistically similar languages. Second, we introduce a novel tokenizer that reduces the number of generated tokens, thereby accelerating Whisperâ€™s inference speed. Our extensive experiments demonstrate that the tokenizer significantly reduces inference time, while prompt-tuning enhances accuracy across various Whisper model sizes, including Small, Medium, and Large. Together, these techniques achieve a balance between optimal WER and inference speed. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯è¿‘æœŸå‡­å€Ÿå¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆå¦‚whisperï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€åœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚å°åº¦è¯­è¨€ï¼‰ä¸­çš„è¡¨ç°ä¸å°½å¦‚äººæ„ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸¤ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜whisperåœ¨å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ä¸­çš„å°åº¦è¯­è¨€æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¸¦æœ‰è¯­è¨€å®¶æ—ä¿¡æ¯çš„æç¤ºè°ƒæ•´ï¼ˆprompt-tuningï¼‰æ–¹æ³•ï¼Œä»¥æé«˜åœ¨ç›¸ä¼¼è¯­è¨€ä¸­çš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åˆ†è¯å™¨ï¼Œå¯ä»¥å‡å°‘ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€ŒåŠ å¿«whisperçš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåˆ†è¯å™¨å¯ä»¥å¤§å¤§å‡å°‘æ¨ç†æ—¶é—´ï¼Œè€Œæç¤ºè°ƒæ•´åˆ™æé«˜äº†å„ç§whisperæ¨¡å‹å¤§å°çš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬å°å‹ã€ä¸­å‹å’Œå¤§å‹ã€‚è¿™äº›æŠ€æœ¯å…±åŒå®ç°äº†æœ€ä¼˜çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œæ¨ç†é€Ÿåº¦çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19785v1">PDF</a> Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¢å¼ºWhisperåœ¨å¤šè¯­ç§è¯­éŸ³è¯†åˆ«æ€§èƒ½æ–¹é¢çš„ä¸¤å¤§ç­–ç•¥ã€‚é¦–å…ˆæå‡ºåˆ©ç”¨è¯­è¨€å®¶æ—ä¿¡æ¯è¿›è¡Œæç¤ºè°ƒæ•´ï¼Œæé«˜åœ¨ç›¸ä¼¼è¯­è¨€ä¸Šçš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰ŒåŒ–å™¨ï¼Œå‡å°‘äº†ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€ŒåŠ å¿«äº†Whisperçš„æ¨ç†é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œä»¤ç‰ŒåŒ–å™¨æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œæç¤ºè°ƒæ•´åˆ™æé«˜äº†ä¸åŒè§„æ¨¡çš„Whisperæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç»“åˆä½¿ç”¨è¿™ä¸¤ç§æŠ€æœ¯ï¼Œå®ç°äº†æœ€ä½³å­—è¯é”™è¯¯ç‡å’Œæ¨ç†é€Ÿåº¦çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¢å¼ºWhisperåœ¨å¤šè¯­ç§è¯­éŸ³è¯†åˆ«æ–¹é¢çš„ä¸¤å¤§ç­–ç•¥ã€‚</li>
<li>åˆ©ç”¨è¯­è¨€å®¶æ—ä¿¡æ¯è¿›è¡Œæç¤ºè°ƒæ•´ä»¥æé«˜ç±»ä¼¼è¯­è¨€è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰ŒåŒ–å™¨æ¥åŠ å¿«Whisperçš„æ¨ç†é€Ÿåº¦å¹¶å‡å°‘ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»¤ç‰ŒåŒ–å™¨æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚</li>
<li>æç¤ºè°ƒæ•´æé«˜äº†ä¸åŒè§„æ¨¡çš„Whisperæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆä½¿ç”¨è¿™ä¸¤ç§æŠ€æœ¯å®ç°äº†å­—è¯é”™è¯¯ç‡å’Œæ¨ç†é€Ÿåº¦çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42fab726694b093c31720db9cbd45471.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b65d5699af93873f1eda9afa14d5ba6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-052a267d290654316e5ee0e9acddf704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e9bac2067c4a9b9888253d06f4c766e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3a2f83ea463629080a327744b321cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5c1875a901c452c197147dcd95ec2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7e539cda2cea3567158089c6c9c695.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-a-Single-ASR-Model-That-Generalizes-to-Disordered-Speech"><a href="#Towards-a-Single-ASR-Model-That-Generalizes-to-Disordered-Speech" class="headerlink" title="Towards a Single ASR Model That Generalizes to Disordered Speech"></a>Towards a Single ASR Model That Generalizes to Disordered Speech</h2><p><strong>Authors:Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan</strong></p>
<p>This study investigates the impact of integrating a dataset of disordered speech recordings ($\sim$1,000 hours) into the fine-tuning of a near state-of-the-art ASR baseline system. Contrary to what one might expect, despite the data being less than 1% of the training data of the ASR system, we find a considerable improvement in disordered speech recognition accuracy. Specifically, we observe a 33% improvement on prompted speech, and a 26% improvement on a newly gathered spontaneous, conversational dataset of disordered speech. Importantly, there is no significant performance decline on standard speech recognition benchmarks. Further, we observe that the proposed tuning strategy helps close the gap between the baseline system and personalized models by 64% highlighting the significant progress as well as the room for improvement. Given the substantial benefits of our findings, this experiment suggests that from a fairness perspective, incorporating a small fraction of high quality disordered speech data in a training recipe is an easy step that could be done to make speech technology more accessible for users with speech disabilities. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å°†åŒ…å«çº¦1000å°æ—¶æ— åºè¯­éŸ³è®°å½•çš„è¯­éŸ³æ•°æ®é›†æ•´åˆåˆ°æ¥è¿‘æœ€æ–°æ°´å¹³çš„ASRåŸºçº¿ç³»ç»Ÿçš„å¾®è°ƒä¸­æ‰€äº§ç”Ÿçš„å½±å“ã€‚ä¸äººä»¬å¯èƒ½ä¼šé¢„æœŸçš„ä¸åŒï¼Œå°½ç®¡è¿™äº›æ•°æ®ä¸åˆ°ASRç³»ç»Ÿè®­ç»ƒæ•°æ®çš„1%ï¼Œä½†æˆ‘ä»¬å‘ç°æ— åºè¯­éŸ³è¯†åˆ«çš„å‡†ç¡®åº¦æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æç¤ºæ€§è¯­éŸ³ä¸Šè§‚å¯Ÿåˆ°æå‡äº†33%ï¼Œå¹¶åœ¨æ–°æ”¶é›†çš„æ— åºè¯­éŸ³çš„è‡ªå‘è¨€è®ºæ•°æ®é›†ä¸­æå‡äº†26%ã€‚é‡è¦çš„æ˜¯ï¼Œåœ¨æ ‡å‡†è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½å¹¶æ²¡æœ‰å‡ºç°æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œæ‰€æå‡ºçš„è°ƒæ•´ç­–ç•¥æœ‰åŠ©äºå°†åŸºçº¿ç³»ç»Ÿä¸ä¸ªæ€§åŒ–æ¨¡å‹ä¹‹é—´çš„å·®è·ç¼©å°64%ï¼Œè¿™æ—¢çªå‡ºäº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä¹Ÿè¡¨æ˜äº†ä»æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚é‰´äºæˆ‘ä»¬çš„å‘ç°æ‰€å¸¦æ¥çš„å·¨å¤§ç›Šå¤„ï¼Œæœ¬å®éªŒè¡¨æ˜ï¼Œä»å…¬å¹³æ€§çš„è§’åº¦æ¥çœ‹ï¼Œåœ¨è®­ç»ƒé…æ–¹ä¸­åŠ å…¥ä¸€å°éƒ¨åˆ†é«˜è´¨é‡çš„æ— åºè¯­éŸ³æ•°æ®æ˜¯ä¸€ä¸ªç®€å•çš„æ­¥éª¤ï¼Œå¯ä»¥ä½¿è¯­éŸ³æŠ€æœ¯æ›´æ˜“äºä¾›æœ‰è¨€è¯­éšœç¢çš„ç”¨æˆ·ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19315v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å°†åŒ…å«çº¦1000å°æ—¶çš„æ— åºè¯­éŸ³æ•°æ®é›†èå…¥å…ˆè¿›çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¾®è°ƒç¯èŠ‚çš„å½±å“ã€‚å°½ç®¡è¿™äº›æ•°æ®ä¸åˆ°è®­ç»ƒæ•°æ®çš„ç™¾åˆ†ä¹‹ä¸€ï¼Œä½†å‘ç°æ— åºè¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œå…¶ä¸­æç¤ºæ€§è¯­éŸ³æé«˜äº†33%ï¼Œæ–°æ”¶é›†çš„éšæ„å¯¹è¯æ•°æ®é›†æé«˜äº†26%ï¼Œä¸”åœ¨æ ‡å‡†è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ²¡æœ‰æ˜æ˜¾ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥æœ‰åŠ©äºç¼©å°åŸºå‡†ç³»ç»Ÿä¸ä¸ªæ€§åŒ–æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œæé«˜äº†æ˜¾è‘—è¿›æ­¥çš„ç©ºé—´ã€‚å› æ­¤ï¼Œä»å…¬å¹³æ€§çš„è§’åº¦è€ƒè™‘ï¼Œåœ¨è®­ç»ƒä¸­åŠ å…¥ä¸€å°éƒ¨åˆ†é«˜è´¨é‡çš„æ— åºè¯­éŸ³æ•°æ®ï¼Œæœ‰åŠ©äºè®©è¯­éŸ³æŠ€æœ¯å¯¹æœ‰è¯­éŸ³éšœç¢çš„ç”¨æˆ·æ›´åŠ å‹å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥åŒ…å«çº¦1000å°æ—¶æ— åºè¯­éŸ³çš„æ•°æ®é›†è¿›è¡ŒASRç³»ç»Ÿå¾®è°ƒã€‚</li>
<li>å³ä½¿æ•°æ®åªå ä¸€å°éƒ¨åˆ†ï¼Œæ— åºè¯­éŸ³è¯†åˆ«çš„å‡†ç¡®ç‡ä¹Ÿæœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æç¤ºæ€§è¯­éŸ³å’Œéšæ„å¯¹è¯æ•°æ®é›†çš„è¯†åˆ«å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†33%å’Œ26%ã€‚</li>
<li>åœ¨æ ‡å‡†è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­æœªè§æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è¯¥ç­–ç•¥æœ‰åŠ©äºç¼©å°åŸºå‡†ç³»ç»Ÿä¸ä¸ªæ€§åŒ–æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</li>
<li>åŠ å…¥é«˜è´¨é‡çš„æ— åºè¯­éŸ³æ•°æ®æœ‰åŠ©äºæé«˜è¯­éŸ³æŠ€æœ¯çš„å…¬å¹³æ€§å’Œå¯åŠæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d8058bb87b1f361e8854aba26cb303c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e7db2dba422d3185d6be440356f4c87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ade4499580f53a5f03a42c2241805a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff841c6fc951fed0c156bd47f1019af4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c8cfd88dc016ba1bcd2a72b66281e76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-232a505d073965473d0fae2bbb637d83.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Causal-Speech-Enhancement-with-Predicting-Semantics-based-on-Quantized-Self-supervised-Learning-Features"><a href="#Causal-Speech-Enhancement-with-Predicting-Semantics-based-on-Quantized-Self-supervised-Learning-Features" class="headerlink" title="Causal Speech Enhancement with Predicting Semantics based on Quantized   Self-supervised Learning Features"></a>Causal Speech Enhancement with Predicting Semantics based on Quantized   Self-supervised Learning Features</h2><p><strong>Authors:Emiru Tsunoo, Yuki Saito, Wataru Nakata, Hiroshi Saruwatari</strong></p>
<p>Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰å¯¹äºåœ¨çº¿è¯­éŸ³é€šä¿¡è‡³å…³é‡è¦ã€‚å› æœSEæ¨¡å‹åœ¨é¢„æµ‹æœªæ¥ä¿¡æ¯æ—¶åªä½¿ç”¨å…ˆå‰çš„ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚è¯­éŸ³å»¶ç»­ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºè¿›è¡Œå› æœSEã€‚è¯­éŸ³ä¿¡æ¯é€šå¸¸é€šè¿‡é‡åŒ–è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾æ¥è¡¨ç¤ºã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡å°†SSLç‰¹å¾ä¸å› æœæ€§èå…¥SEæ¨¡å‹ã€‚å› æœSSLç‰¹å¾é€šè¿‡ç‰¹å¾çº§çº¿æ€§è°ƒåˆ¶ä¸é¢‘è°±ç‰¹å¾ç¼–ç ç›¸ç»“åˆï¼Œä»¥ä¼°è®¡ç”¨äºå¢å¼ºå¸¦å™ªè¾“å…¥è¯­éŸ³çš„æ©æ¨¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å‘é‡é‡åŒ–æ¥é‡åŒ–å› æœSSLç‰¹å¾ï¼Œå°†å…¶è¡¨ç¤ºä¸ºè¡¨ç¤ºè¯­éŸ³ç‰¹å¾çš„è¯­ä¹‰ä»¤ç‰Œã€‚è¯¥æ¨¡å‹ä¸ä»…ç¼–ç SSLç‰¹å¾ï¼Œè€Œä¸”è¿˜åœ¨å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ä¸­é¢„æµ‹æœªæ¥çš„è¯­ä¹‰ä»¤ç‰Œã€‚ä½¿ç”¨VoiceBank + DEMANDæ•°æ®é›†è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨PESQä¸Šè¾¾åˆ°äº†2.88ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰é¢„æµ‹MTLä¸­ï¼Œæˆ‘ä»¬è¯å®è¯­ä¹‰é¢„æµ‹åœ¨å› æœSEä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19248v1">PDF</a> Accepted for ICASSP 2025, 5 pages</p>
<p><strong>Summary</strong></p>
<p>å®æ—¶è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰å¯¹åœ¨çº¿è¯­éŸ³é€šä¿¡è‡³å…³é‡è¦ã€‚å› æœSEæ¨¡å‹ä»…ä½¿ç”¨è¿‡å»çš„ä¿¡æ¯è¿›è¡Œé¢„æµ‹ï¼Œè€Œåˆ©ç”¨è¯­éŸ³å»¶ç»­ç­‰æœªæ¥ä¿¡æ¯å¯èƒ½æœ‰åŠ©äºè¿›è¡Œå› æœSEã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„å› æœç‰¹æ€§ä¸è¯­éŸ³å¢å¼ºæ¨¡å‹ç»“åˆã€‚åˆ©ç”¨ç‰¹å¾çº§çº¿æ€§è°ƒåˆ¶å°†å› æœSSLç‰¹å¾ä¸é¢‘è°±å›¾ç‰¹å¾ç¼–ç å¹¶åˆå¹¶ï¼Œä»¥ä¼°è®¡ç”¨äºå¢å¼ºå™ªå£°è¾“å…¥è¯­éŸ³çš„æ©è†œã€‚åŒæ—¶ï¼Œä½¿ç”¨å‘é‡é‡åŒ–å¯¹å› æœSSLç‰¹å¾è¿›è¡Œé‡åŒ–ï¼Œä»¥è¯­ä¹‰ä»¤ç‰Œçš„å½¢å¼è¡¨ç¤ºè¯­éŸ³ç‰¹å¾ã€‚æ¨¡å‹ä¸ä»…ç¼–ç SSLç‰¹å¾ï¼Œè¿˜åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­é¢„æµ‹æœªæ¥çš„è¯­ä¹‰ä»¤ç‰Œã€‚ä½¿ç”¨VoiceBank + DEMANDæ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨PESQä¸Šè¾¾åˆ°2.88ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è¯­ä¹‰é¢„æµ‹çš„å¤šä»»åŠ¡å­¦ä¹ ï¼Œæˆ‘ä»¬è¯å®äº†è¯­ä¹‰é¢„æµ‹åœ¨å› æœSEä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰åœ¨åœ¨çº¿è¯­éŸ³é€šä¿¡ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>å› æœSEæ¨¡å‹é€šè¿‡ä½¿ç”¨æœªæ¥çš„è¯­éŸ³ä¿¡æ¯ï¼ˆå¦‚è¯­éŸ³å»¶ç»­ï¼‰æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶ç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„å› æœç‰¹æ€§ä¸è¯­éŸ³å¢å¼ºæ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç‰¹å¾çº§çº¿æ€§è°ƒåˆ¶ï¼Œå°†å› æœSSLç‰¹å¾ä¸é¢‘è°±å›¾ç‰¹å¾ç»“åˆï¼Œä»¥ä¼°è®¡è¯­éŸ³æ©è†œã€‚</li>
<li>å‘é‡é‡åŒ–ç”¨äºé‡åŒ–å› æœSSLç‰¹å¾ï¼Œè¡¨ç¤ºä¸ºè¯­ä¹‰ä»¤ç‰Œã€‚</li>
<li>æ¨¡å‹åœ¨ç¼–ç SSLç‰¹å¾çš„åŒæ—¶ï¼Œè¿˜åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­é¢„æµ‹æœªæ¥çš„è¯­ä¹‰ä»¤ç‰Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd796e1622d3abd9e3056859fd643f5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c247892360654b629b6366357509359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be856b00cc323086059dcc7c97b618b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bba354b389206ec3a0c7f0570999651.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6b663cfad71c97a84d36efb97bb505.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="â€œIâ€™ve-Heard-of-You-â€-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities"><a href="#â€œIâ€™ve-Heard-of-You-â€-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities" class="headerlink" title="â€œIâ€™ve Heard of You!â€: Generate Spoken Named Entity Recognition Data for   Unseen Entities"></a>â€œIâ€™ve Heard of You!â€: Generate Spoken Named Entity Recognition Data for   Unseen Entities</h2><p><strong>Authors:Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su</strong></p>
<p>Spoken named entity recognition (NER) aims to identify named entities from speech, playing an important role in speech processing. New named entities appear every day, however, annotating their Spoken NER data is costly. In this paper, we demonstrate that existing Spoken NER systems perform poorly when dealing with previously unseen named entities. To tackle this challenge, we propose a method for generating Spoken NER data based on a named entity dictionary (NED) to reduce costs. Specifically, we first use a large language model (LLM) to generate sentences from the sampled named entities and then use a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce a noise metric to filter out noisy data. To evaluate our approach, we release a novel Spoken NER benchmark along with a corresponding NED containing 8,853 entities. Experiment results show that our method achieves state-of-the-art (SOTA) performance in the in-domain, zero-shot domain adaptation, and fully zero-shot settings. Our data will be available at <a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU">https://github.com/DeepLearnXMU/HeardU</a>. </p>
<blockquote>
<p>è¯­éŸ³å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ—¨åœ¨ä»è¯­éŸ³ä¸­è¯†åˆ«å‘½åå®ä½“ï¼Œåœ¨è¯­éŸ³å¤„ç†ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æ¯å¤©éƒ½æœ‰æ–°çš„å‘½åå®ä½“å‡ºç°ï¼Œç„¶è€Œï¼Œå¯¹å®ƒä»¬çš„è¯­éŸ³NERæ•°æ®è¿›è¡Œæ ‡æ³¨çš„æˆæœ¬å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ç°æœ‰çš„è¯­éŸ³NERç³»ç»Ÿåœ¨å¤„ç†ä¹‹å‰æœªè§è¿‡çš„å‘½åå®ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‘½åå®ä½“è¯å…¸ï¼ˆNEDï¼‰ç”Ÿæˆè¯­éŸ³NERæ•°æ®çš„æ–¹æ³•ï¼Œä»¥é™ä½æ ‡æ³¨æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é‡‡æ ·çš„å‘½åå®ä½“ç”Ÿæˆå¥å­ï¼Œç„¶åä½¿ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆè¯­éŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå™ªå£°åº¦é‡æ¥è¿‡æ»¤æ‰å™ªå£°æ•°æ®ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„è¯­éŸ³NERåŸºå‡†æµ‹è¯•ä»¥åŠä¸€ä¸ªåŒ…å«8853ä¸ªå®ä½“çš„ç›¸åº”NEDã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸŸå†…ã€é›¶æ ·æœ¬åŸŸé€‚åº”å’Œå®Œå…¨é›¶æ ·æœ¬è®¾ç½®ä¸­éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„æ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepLearnXMU/HeardUä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19102v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ—¨åœ¨ä»è¯­éŸ³ä¸­è¯†åˆ«å‘½åå®ä½“ï¼Œåœ¨è¯­éŸ³è¯†åˆ«å¤„ç†ä¸­å æ®é‡è¦åœ°ä½ã€‚ç°æœ‰Spoken NERç³»ç»Ÿå¯¹äºæœªè§è¿‡çš„å‘½åå®ä½“è¡¨ç°ä¸ä½³ï¼Œæ ‡æ³¨Spoken NERæ•°æ®æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‘½åå®ä½“è¯å…¸ï¼ˆNEDï¼‰ç”ŸæˆSpoken NERæ•°æ®çš„æ–¹æ³•é™ä½æˆæœ¬ã€‚é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é‡‡æ ·å‘½åå®ä½“ç”Ÿæˆå¥å­ï¼Œå†ä½¿ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆè¯­éŸ³ã€‚æ­¤å¤–ï¼Œå¼•å…¥å™ªå£°åº¦é‡ä»¥è¿‡æ»¤å™ªå£°æ•°æ®ã€‚æœ¬æ–‡å‘å¸ƒæ–°å‹Spoken NERåŸºå‡†å’ŒåŒ…å«8,853ä¸ªå®ä½“çš„å¯¹åº”NEDã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…ã€é›¶æ ·æœ¬åŸŸé€‚åº”å’Œå®Œå…¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç›¸å…³æ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepLearnXMU/HeardUä¸Šæä¾›ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spoken NERæ—¨åœ¨ä»è¯­éŸ³ä¸­è¯†åˆ«å‘½åå®ä½“ï¼Œå¯¹äºæ–°å‡ºç°çš„å‘½åå®ä½“ï¼Œç°æœ‰ç³»ç»Ÿè¡¨ç°ä¸ä½³ã€‚</li>
<li>æ ‡æ³¨Spoken NERæ•°æ®çš„æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦å¯»æ‰¾é™ä½æˆæœ¬çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‘½åå®ä½“è¯å…¸ï¼ˆNEDï¼‰ç”ŸæˆSpoken NERæ•°æ®çš„æ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚</li>
<li>å¼•å…¥å™ªå£°åº¦é‡ä»¥è¿‡æ»¤ç”Ÿæˆçš„å™ªå£°æ•°æ®ã€‚</li>
<li>å‘å¸ƒäº†æ–°å‹çš„Spoken NERåŸºå‡†å’Œå¯¹åº”çš„åŒ…å«å¤§é‡å®ä½“çš„NEDã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªè®¾ç½®ä¸‹è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b86e890486171362a149be8408a2e3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5aa85d1b8ed0df6adba19264fc7d5a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33eef7680a3f22388849094bd1a0991b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-179f247ff97294ecfdc487eec3257cc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd1ba9e961a6bbb71566fea9c46a06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515c695c189641e0236cc39dcb4c07fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BSDB-Net-Band-Split-Dual-Branch-Network-with-Selective-State-Spaces-Mechanism-for-Monaural-Speech-Enhancement"><a href="#BSDB-Net-Band-Split-Dual-Branch-Network-with-Selective-State-Spaces-Mechanism-for-Monaural-Speech-Enhancement" class="headerlink" title="BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces   Mechanism for Monaural Speech Enhancement"></a>BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces   Mechanism for Monaural Speech Enhancement</h2><p><strong>Authors:Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv</strong></p>
<p>Although the complex spectrum-based speech enhancement(SE) methods have achieved significant performance, coupling amplitude and phase can lead to a compensation effect, where amplitude information is sacrificed to compensate for the phase that is harmful to SE. In addition, to further improve the performance of SE, many modules are stacked onto SE, resulting in increased model complexity that limits the application of SE. To address these problems, we proposed a dual-path network based on compressed frequency using Mamba. First, we extract amplitude and phase information through parallel dual branches. This approach leverages structured complex spectra to implicitly capture phase information and solves the compensation effect by decoupling amplitude and phase, and the network incorporates an interaction module to suppress unnecessary parts and recover missing components from the other branch. Second, to reduce network complexity, the network introduces a band-split strategy to compress the frequency dimension. To further reduce complexity while maintaining good performance, we designed a Mamba-based module that models the time and frequency dimensions under linear complexity. Finally, compared to baselines, our model achieves an average 8.3 times reduction in computational complexity while maintaining superior performance. Furthermore, it achieves a 25 times reduction in complexity compared to transformer-based models. </p>
<blockquote>
<p>è™½ç„¶åŸºäºå¤æ‚é¢‘è°±çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å¹…åº¦å’Œç›¸ä½ä¹‹é—´çš„è€¦åˆä¼šå¯¼è‡´è¡¥å¿æ•ˆåº”ï¼Œå³ç‰ºç‰²å¹…åº¦ä¿¡æ¯æ¥è¡¥å¿å¯¹è¯­éŸ³å¢å¼ºæœ‰å®³çš„ç›¸ä½ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜è¯­éŸ³å¢å¼ºçš„æ€§èƒ½ï¼Œè®¸å¤šæ¨¡å—è¢«å †å åœ¨è¯­éŸ³å¢å¼ºä¸Šï¼Œå¯¼è‡´æ¨¡å‹å¤æ‚åº¦å¢åŠ ï¼Œé™åˆ¶äº†è¯­éŸ³å¢å¼ºçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‹ç¼©é¢‘ç‡çš„åŒè·¯å¾„ç½‘ç»œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¹¶è¡ŒåŒåˆ†æ”¯æå–å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨ç»“æ„åŒ–å¤æ‚å…‰è°±æ¥éšå«åœ°æ•è·ç›¸ä½ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è§£è€¦å¹…åº¦å’Œç›¸ä½æ¥è§£å†³è¡¥å¿æ•ˆåº”ã€‚ç½‘ç»œè¿˜åŒ…å«ä¸€ä¸ªäº¤äº’æ¨¡å—ï¼Œç”¨äºæŠ‘åˆ¶ä¸å¿…è¦çš„éƒ¨åˆ†å¹¶ä»å¦ä¸€ä¸ªåˆ†æ”¯æ¢å¤ä¸¢å¤±çš„ç»„ä»¶ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é™ä½ç½‘ç»œå¤æ‚åº¦ï¼Œç½‘ç»œå¼•å…¥äº†ä¸€ç§åˆ†é¢‘ç­–ç•¥æ¥å‹ç¼©é¢‘ç‡ç»´åº¦ã€‚ä¸ºäº†ä¿æŒæ€§èƒ½çš„åŒæ—¶è¿›ä¸€æ­¥é™ä½å¤æ‚åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºMambaçš„æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨çº¿æ€§å¤æ‚åº¦ä¸‹å¯¹æ—¶é—´å’Œé¢‘ç‡ç»´åº¦è¿›è¡Œå»ºæ¨¡ã€‚æœ€åï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå®ç°äº†å¹³å‡8.3å€çš„é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸åŸºäºå˜å‹å™¨çš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒå®ç°äº†é«˜è¾¾25å€çš„å¤æ‚åº¦é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19099v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>å¤æ‚è°±åŸºè¯­éŸ³å¢å¼ºæ–¹æ³•è™½å–å¾—æ˜¾è‘—æ€§èƒ½ï¼Œä½†å¹…åº¦ä¸ç›¸ä½è€¦åˆä¼šäº§ç”Ÿè¡¥å¿æ•ˆåº”ï¼Œç‰ºç‰²å¹…åº¦ä¿¡æ¯æ¥è¡¥å¿å¯¹è¯­éŸ³å¢å¼ºæœ‰å®³çš„ç›¸ä½ã€‚ä¸ºæ”¹å–„è¯­éŸ³å¢å¼ºæ€§èƒ½ï¼Œåœ¨è¯­éŸ³å¢å¼ºä¸Šå åŠ äº†è®¸å¤šæ¨¡å—ï¼Œå¯¼è‡´æ¨¡å‹å¤æ‚åº¦å¢åŠ ã€‚æˆ‘ä»¬æå‡ºåŸºäºå‹ç¼©é¢‘ç‡çš„åŒè·¯å¾„ç½‘ç»œï¼Œé€šè¿‡å¹¶è¡ŒåŒåˆ†æ”¯æå–å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ï¼Œåˆ©ç”¨ç»“æ„å¤æ‚è°±éšå¼æ•è·ç›¸ä½ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è§£è€¦å¹…åº¦å’Œç›¸ä½æ¥è§£å†³è¡¥å¿æ•ˆåº”ã€‚ç½‘ç»œå¼•å…¥äº¤äº’æ¨¡å—ä»¥æŠ‘åˆ¶ä¸å¿…è¦çš„éƒ¨åˆ†å¹¶ä»å¦ä¸€åˆ†æ”¯æ¢å¤ç¼ºå¤±çš„ç»„ä»¶ã€‚ä¸ºé™ä½ç½‘ç»œå¤æ‚åº¦ï¼Œå¼•å…¥é¢‘å¸¦åˆ†å‰²ç­–ç•¥æ¥å‹ç¼©é¢‘ç‡ç»´åº¦ã€‚ä¸ºè¿›ä¸€æ­¥é™ä½å¤æ‚åº¦å¹¶ä¿æŒè‰¯å¥½æ€§èƒ½ï¼Œè®¾è®¡åŸºäºMambaçš„æ¨¡å—ï¼Œåœ¨çº¿æ€§å¤æ‚åº¦ä¸‹å¯¹æ—¶é—´å’Œé¢‘ç‡ç»´åº¦è¿›è¡Œå»ºæ¨¡ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå¹³å‡é™ä½äº†8.3å€ï¼ŒåŒæ—¶ä¿æŒä¼˜è¶Šæ€§èƒ½ï¼Œä¸åŸºäºå˜å‹å™¨çš„æ¨¡å‹ç›¸æ¯”ï¼Œå¤æ‚åº¦é™ä½äº†25å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹…åº¦ä¸ç›¸ä½è€¦åˆå¯èƒ½å¯¼è‡´è¡¥å¿æ•ˆåº”ï¼Œç‰ºç‰²å¹…åº¦ä¿¡æ¯æ¥è¡¥å¿ç›¸ä½ã€‚</li>
<li>ä¸ºæé«˜è¯­éŸ³å¢å¼ºæ€§èƒ½ï¼Œè®¸å¤šæ¨¡å—è¢«å åŠ åœ¨SEä¸Šï¼Œå¯¼è‡´æ¨¡å‹å¤æ‚åº¦å¢åŠ ã€‚</li>
<li>æå‡ºåŸºäºå‹ç¼©é¢‘ç‡çš„åŒè·¯å¾„ç½‘ç»œæ¥è§£å†³å¹…åº¦å’Œç›¸ä½è€¦åˆé—®é¢˜ã€‚</li>
<li>ç½‘ç»œåˆ©ç”¨ç»“æ„å¤æ‚è°±éšå¼æ•è·ç›¸ä½ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è§£è€¦å¹…åº¦å’Œç›¸ä½æ¥è§£å†³è¡¥å¿æ•ˆåº”ã€‚</li>
<li>ç½‘ç»œå¼•å…¥äº¤äº’æ¨¡å—ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>ä¸ºé™ä½ç½‘ç»œå¤æ‚åº¦ï¼Œé‡‡ç”¨é¢‘å¸¦åˆ†å‰²ç­–ç•¥å’ŒåŸºäºMambaçš„æ¨¡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60f903e42ce7100485730769a8cb1f33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3aca892af3e125201bb2e393e2d22a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79799ea3362b2b10d70bbe3207a1732f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5559c4ec8ddf095d7cad444973e7fb3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fea64496a9df7d9af93cd846bada43c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e5dfc344d4009a6d436210f28f6f5f1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Attacking-Voice-Anonymization-Systems-with-Augmented-Feature-and-Speaker-Identity-Difference"><a href="#Attacking-Voice-Anonymization-Systems-with-Augmented-Feature-and-Speaker-Identity-Difference" class="headerlink" title="Attacking Voice Anonymization Systems with Augmented Feature and Speaker   Identity Difference"></a>Attacking Voice Anonymization Systems with Augmented Feature and Speaker   Identity Difference</h2><p><strong>Authors:Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan</strong></p>
<p>This study focuses on the First VoicePrivacy Attacker Challenge within the ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker verification systems capable of determining whether two anonymized speech signals are from the same speaker. However, differences between feature distributions of original and anonymized speech complicate this task. To address this challenge, we propose an attacker system that combines Data Augmentation enhanced feature representation and Speaker Identity Difference enhanced classifier to improve verification performance, termed DA-SID. Specifically, data augmentation strategies (i.e., data fusion and SpecAugment) are utilized to mitigate feature distribution gaps, while probabilistic linear discriminant analysis (PLDA) is employed to further enhance speaker identity difference. Our system significantly outperforms the baseline, demonstrating exceptional effectiveness and robustness against various voice anonymization systems, ultimately securing a top-5 ranking in the challenge. </p>
<blockquote>
<p>æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨ICASSP 2025ä¿¡å·å¤„ç†å¤§èµ›ä¸­çš„é¦–ä¸ªè¯­éŸ³éšç§æ”»å‡»æŒ‘æˆ˜ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿåˆ¤æ–­ä¸¤ä¸ªåŒ¿åè¯­éŸ³ä¿¡å·æ˜¯å¦æ¥è‡ªåŒä¸€è¯´è¯äººçš„è¯´è¯äººéªŒè¯ç³»ç»Ÿã€‚ç„¶è€Œï¼ŒåŸå§‹è¯­éŸ³å’ŒåŒ¿åè¯­éŸ³ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ä½¿è¿™ä¸€ä»»åŠ¡å¤æ‚åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”»å‡»è€…ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†æ•°æ®å¢å¼ºå¢å¼ºç‰¹å¾è¡¨ç¤ºå’Œè¯´è¯äººèº«ä»½å·®å¼‚å¢å¼ºåˆ†ç±»å™¨æ¥æé«˜éªŒè¯æ€§èƒ½ï¼Œè¢«ç§°ä¸ºDA-SIDã€‚å…·ä½“æ¥è¯´ï¼Œæ•°æ®å¢å¼ºç­–ç•¥ï¼ˆå³æ•°æ®èåˆå’ŒSpecAugmentï¼‰è¢«ç”¨æ¥ç¼“è§£ç‰¹å¾åˆ†å¸ƒå·®è·ï¼Œè€Œæ¦‚ç‡çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆPLDAï¼‰è¢«ç”¨æ¥è¿›ä¸€æ­¥æé«˜è¯´è¯äººçš„èº«ä»½å·®å¼‚è¯†åˆ«ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿ç³»ç»Ÿï¼Œå±•ç°å‡ºåœ¨å„ç§è¯­éŸ³åŒ¿ååŒ–ç³»ç»Ÿä¸­çš„å“è¶Šæœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œæœ€ç»ˆåœ¨è¯¥æŒ‘æˆ˜ä¸­å–å¾—äº†å‰äº”çš„æ’åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19068v1">PDF</a> 2 pages, submitted to ICASSP 2025 GC-7: The First VoicePrivacy   Attacker Challenge (by invitation)</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å…³æ³¨ICASSP 2025ä¿¡å·å¤„ç†å¤§èµ›ä¸­çš„First VoicePrivacyæ”»å‡»è€…æŒ‘æˆ˜ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿåˆ¤æ–­ä¸¤ä¸ªåŒ¿åè¯­éŸ³ä¿¡å·æ˜¯å¦æ¥è‡ªåŒä¸€å‘è¨€äººçš„è¯´è¯äººéªŒè¯ç³»ç»Ÿã€‚ä¸ºè§£å†³åŸå§‹è¯­éŸ³ä¸åŒ¿ååŒ–è¯­éŸ³ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§ç»“åˆæ•°æ®å¢å¼ºå¢å¼ºç‰¹å¾è¡¨ç¤ºå’Œè¯´è¯äººèº«ä»½å·®å¼‚å¢å¼ºåˆ†ç±»å™¨çš„æ”»å‡»è€…ç³»ç»Ÿï¼Œç§°ä¸ºDA-SIDã€‚é€šè¿‡ä½¿ç”¨æ•°æ®èåˆå’ŒSpecAugmentç­‰æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç¼©å°ç‰¹å¾åˆ†å¸ƒå·®è·ï¼ŒåŒæ—¶é‡‡ç”¨æ¦‚ç‡çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆPLDAï¼‰è¿›ä¸€æ­¥å¼ºåŒ–è¯´è¯äººèº«ä»½å·®å¼‚ã€‚è¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¯¹å„ç§è¯­éŸ³åŒ¿ååŒ–ç³»ç»Ÿè¡¨ç°å‡ºæé«˜çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œæœ€ç»ˆåœ¨æŒ‘æˆ˜ä¸­ä½åˆ—å‰äº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨ICASSP 2025çš„VoicePrivacyæ”»å‡»è€…æŒ‘æˆ˜ï¼Œä¸»è¦ç›®æ ‡æ˜¯å¼€å‘é«˜æ•ˆçš„è¯´è¯äººéªŒè¯ç³»ç»Ÿã€‚</li>
<li>åŒ¿ååŒ–è¯­éŸ³å’ŒåŸå§‹è¯­éŸ³çš„ç‰¹å¾åˆ†å¸ƒå­˜åœ¨å·®å¼‚ï¼Œç»™éªŒè¯å¸¦æ¥å›°éš¾ã€‚</li>
<li>æå‡ºç»“åˆæ•°æ®å¢å¼ºå’Œè¯´è¯äººèº«ä»½å·®å¼‚å¢å¼ºçš„ç³»ç»Ÿï¼ˆDA-SIDï¼‰ä»¥æ”¹å–„éªŒè¯æ€§èƒ½ã€‚</li>
<li>æ•°æ®èåˆå’ŒSpecAugmentç­‰æ•°æ®å¢å¼ºç­–ç•¥ç”¨äºç¼©å°ç‰¹å¾åˆ†å¸ƒå·®è·ã€‚</li>
<li>æ¦‚ç‡çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆPLDAï¼‰ç”¨äºå¢å¼ºè¯´è¯äººèº«ä»½å·®å¼‚çš„è¯†åˆ«ã€‚</li>
<li>ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¯¹å„ç§è¯­éŸ³åŒ¿ååŒ–ç³»ç»Ÿå…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-113bd748c918ca055fa8d85977c5a15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-679d03fe7e5a383906de9acd98013692.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Audiovisual-Speech-Recognition-through-Bifocal-Preference-Optimization"><a href="#Enhancing-Audiovisual-Speech-Recognition-through-Bifocal-Preference-Optimization" class="headerlink" title="Enhancing Audiovisual Speech Recognition through Bifocal Preference   Optimization"></a>Enhancing Audiovisual Speech Recognition through Bifocal Preference   Optimization</h2><p><strong>Authors:Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe</strong></p>
<p>Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech recognition accuracy by leveraging visual signals. It is particularly challenging in unconstrained real-world scenarios across various domains due to noisy acoustic environments, spontaneous speech, and the uncertain use of visual information. Most previous works fine-tune audio-only ASR models on audiovisual datasets, optimizing them for conventional ASR objectives. However, they often neglect visual features and common errors in unconstrained video scenarios. In this paper, we propose using a preference optimization strategy to improve speech recognition accuracy for real-world videos. First, we create preference data via simulating common errors that occurred in AV-ASR from two focals: manipulating the audio or vision input and rewriting the output transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization method to improve AV-ASR models by leveraging both input-side and output-side preference. Extensive experiments demonstrate that our approach significantly improves speech recognition accuracy across various domains, outperforming previous state-of-the-art models on real-world video speech recognition. </p>
<blockquote>
<p>è§†å¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆAV-ASRï¼‰æ—¨åœ¨åˆ©ç”¨è§†è§‰ä¿¡å·æé«˜è¯­éŸ³è¯†åˆ«ç²¾åº¦ã€‚åœ¨è·¨å¤šä¸ªé¢†åŸŸçš„æ— çº¦æŸç°å®åœºæ™¯ä¸­ï¼Œç”±äºå™ªå£°ç¯å¢ƒã€è‡ªå‘è¯­éŸ³å’Œè§†è§‰ä¿¡æ¯çš„ä¸ç¡®å®šæ€§ï¼Œå®ƒé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¹‹å‰çš„å¤§å¤šæ•°å·¥ä½œéƒ½å¯¹ä»…ä½¿ç”¨éŸ³é¢‘çš„ASRæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ä¼ ç»Ÿçš„ASRç›®æ ‡ã€‚ç„¶è€Œï¼Œä»–ä»¬å¾€å¾€å¿½è§†äº†è§†è§‰ç‰¹å¾å’Œåœ¨ä¸å—çº¦æŸçš„è§†é¢‘åœºæ™¯ä¸­çš„å¸¸è§é”™è¯¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åå¥½ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜ç°å®ä¸–ç•Œè§†é¢‘çš„è¯­éŸ³è¯†åˆ«ç²¾åº¦ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æ¨¡æ‹ŸAV-ASRä¸­å‘ç”Ÿçš„å¸¸è§é”™è¯¯æ¥åˆ›å»ºåå¥½æ•°æ®ï¼Œè¿™äº›é”™è¯¯ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªæ–¹é¢ï¼šæ“çºµéŸ³é¢‘æˆ–è§†è§‰è¾“å…¥å’Œé‡å†™è¾“å‡ºå­—å¹•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒå‘åå¥½ä¼˜åŒ–æ–¹æ³•BPO-AVASRï¼Œé€šè¿‡åˆ©ç”¨è¾“å…¥å’Œè¾“å‡ºåå¥½æ¥æé«˜AV-ASRæ¨¡å‹çš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è·¨å¤šä¸ªé¢†åŸŸçš„è¯­éŸ³è¯†åˆ«ç²¾åº¦ï¼Œåœ¨ç°å®ä¸–ç•Œè§†é¢‘è¯­éŸ³è¯†åˆ«æ–¹é¢ä¼˜äºæœ€æ–°å…ˆè¿›æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19005v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä½¿ç”¨åå¥½ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡æ¨¡æ‹ŸAV-ASRä¸­çš„å¸¸è§é”™è¯¯å¹¶åˆ›å»ºåå¥½æ•°æ®ï¼Œæ”¹è¿›çœŸå®ä¸–ç•Œè§†é¢‘çš„è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§ã€‚é€šè¿‡æ“çºµéŸ³é¢‘æˆ–è§†è§‰è¾“å…¥ä»¥åŠé‡å†™è¾“å‡ºå­—å¹•ï¼Œæå‡ºäº†åŸºäºè¾“å…¥å’Œè¾“å‡ºåå¥½çš„åŒç„¦ç‚¹åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ˆBPO-AVASRï¼‰ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸçš„è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§ä¸Šæ˜¾è‘—æé«˜ï¼Œä¼˜äºç°æœ‰æœ€ä½³æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Audiovisual Automatic Speech Recognition (AV-ASR)åˆ©ç”¨è§†è§‰ä¿¡å·æé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ä¸åŒçš„é¢†åŸŸå’Œä¸å—çº¦æŸçš„ç°å®åœºæ™¯ä¸­ï¼ŒAV-ASRé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å™ªå£°ç¯å¢ƒã€è‡ªå‘è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>å¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œæ˜¯å¯¹ä»…ä½¿ç”¨éŸ³é¢‘çš„ASRæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¼˜åŒ–ä¼ ç»Ÿçš„ASRç›®æ ‡ï¼Œä½†å¿½ç•¥äº†è§†è§‰ç‰¹å¾å’Œç°å®è§†é¢‘åœºæ™¯ä¸­çš„å¸¸è§é”™è¯¯ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡æ¨¡æ‹ŸAV-ASRä¸­çš„å¸¸è§é”™è¯¯åˆ›å»ºåå¥½æ•°æ®ï¼ŒåŒ…æ‹¬æ“çºµéŸ³é¢‘æˆ–è§†è§‰è¾“å…¥å’Œé‡å†™è¾“å‡ºå­—å¹•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”BPO-AVASRï¼ˆåŒç„¦ç‚¹åå¥½ä¼˜åŒ–ï¼‰ï¼Œåˆ©ç”¨è¾“å…¥å’Œè¾“å‡ºä¸¤ä¾§çš„åå¥½æ¥æé«˜AV-ASRæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„è§†é¢‘è¯­éŸ³è¯†åˆ«ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e88ba69a61a4eddc442a30e862a1d51b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-893047d2f6bbcb8a3638234c5453b43a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6724f15b5050249781be970b8270eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07530beeaac4a476887a8a8f04b2523d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attention-Enhanced-Short-Time-Wiener-Solution-for-Acoustic-Echo-Cancellation"><a href="#Attention-Enhanced-Short-Time-Wiener-Solution-for-Acoustic-Echo-Cancellation" class="headerlink" title="Attention-Enhanced Short-Time Wiener Solution for Acoustic Echo   Cancellation"></a>Attention-Enhanced Short-Time Wiener Solution for Acoustic Echo   Cancellation</h2><p><strong>Authors:Fei Zhao, Xueliang Zhang</strong></p>
<p>Acoustic Echo Cancellation (AEC) is an essential speech signal processing technology that removes echoes from microphone inputs to facilitate natural-sounding full-duplex communication. Currently, deep learning-based AEC methods primarily focus on refining model architectures, frequently neglecting the incorporation of knowledge from traditional filter theory. This paper presents an innovative approach to AEC by introducing an attention-enhanced short-time Wiener solution. Our method strategically harnesses attention mechanisms to mitigate the impact of double-talk interference, thereby optimizing the efficiency of knowledge utilization. The derivation of the short-term Wiener solution, which adapts classical Wiener solutions to finite input causality, integrates established insights from filter theory into this method. The experimental outcomes corroborate the effectiveness of our proposed approach, surpassing other baseline models in performance and generalization. The official code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhaoF-i/ASTWS-AEC">https://github.com/ZhaoF-i/ASTWS-AEC</a> </p>
<blockquote>
<p>å£°å­¦å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰æ˜¯ä¸€é¡¹é‡è¦çš„è¯­éŸ³ä¿¡å·å¤„ç†æŠ€æœ¯å’Œæ¶ˆé™¤éº¦å…‹é£è¾“å…¥ä¸­çš„å›å£°ï¼Œä»¥ä¿ƒè¿›è‡ªç„¶å‘å£°çš„å…¨åŒå·¥é€šä¿¡ã€‚ç›®å‰ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„AECæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›æ¨¡å‹æ¶æ„ä¸Šï¼Œå¾€å¾€å¿½ç•¥äº†ä¼ ç»Ÿæ»¤æ³¢ç†è®ºçŸ¥è¯†çš„èåˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„AECæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¢å¼ºæ³¨æ„åŠ›çŸ­æ—¶ç»´çº³è§£æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç­–ç•¥æ€§åœ°åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å‡è½»åŒå‘é€šè¯å¹²æ‰°çš„å½±å“ï¼Œä»è€Œä¼˜åŒ–çŸ¥è¯†åˆ©ç”¨æ•ˆç‡ã€‚çŸ­æ—¶ç»´çº³è§£çš„æ¨å¯¼å°†ç»å…¸çš„ç»´çº³è§£é€‚åº”äºæœ‰é™è¾“å…¥å› æœæ€§ï¼Œå°†æ»¤æ³¢ç†è®ºçš„æ—¢å®šè§è§£èå…¥æ­¤æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§è¶…è¶Šäº†å…¶ä»–åŸºçº¿æ¨¡å‹åœ¨æ€§èƒ½å’Œæ³›åŒ–æ–¹é¢çš„è¡¨ç°ã€‚å®˜æ–¹ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhaoF-i/ASTWS-AEC">https://github.com/ZhaoF-i/ASTWS-AEC</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18851v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›å¢å¼ºçš„çŸ­æ—¶Wienerè§£å†³æ–¹æ¡ˆçš„å£°å­¦å›å£°æ¶ˆé™¤æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ç»“åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆå‡è½»åŒè®²å¹²æ‰°çš„å½±å“ï¼Œä¼˜åŒ–çŸ¥è¯†åˆ©ç”¨æ•ˆç‡ã€‚å…¶å°†ä¼ ç»Ÿçš„Wienerè§£å†³æ–¹æ¡ˆè¿›è¡Œæ”¹è¿›ï¼Œä»¥é€‚åº”æœ‰é™çš„è¾“å…¥å› æœæ€§ï¼Œå°†æ»¤æ³¢å™¨ç†è®ºçš„ç°æœ‰è§è§£èå…¥å…¶ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶Šäº†å…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å£°å­¦å›å£°æ¶ˆé™¤æ–°æŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯ç»“åˆæ·±åº¦å­¦ä¹ åŠä¼ ç»Ÿæ»¤æ³¢å™¨ç†è®ºï¼Œé€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–çŸ¥è¯†åˆ©ç”¨æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶äººå‘˜æ”¹è¿›äº†ç»å…¸çš„Wienerè§£å†³æ–¹æ¡ˆï¼Œä»¥é€‚åº”æœ‰é™çš„è¾“å…¥å› æœæ€§ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æŠ€æœ¯åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
<li>æœ¬æŠ€æœ¯æœ‰åŠ©äºå®ç°æ›´è‡ªç„¶çš„åŒå‘é€šä¿¡ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚</li>
<li>ç ”ç©¶äººå‘˜å·²åœ¨GitHubä¸Šå…¬å¼€äº†ç›¸å…³ä»£ç ï¼Œä¾¿äºä»–äººå‚è€ƒå’Œè¿›ä¸€æ­¥ç ”å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-949f6da314b79fcce63eb504f2b0b15d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c80e52ff14c0029a0248aa13b90790ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe21003b25219e73c52475889cf765a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6fe497930df57e452c54a4b64764c2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Structured-Speaker-Deficiency-Adaptation-of-Foundation-Models-for-Dysarthric-and-Elderly-Speech-Recognition"><a href="#Structured-Speaker-Deficiency-Adaptation-of-Foundation-Models-for-Dysarthric-and-Elderly-Speech-Recognition" class="headerlink" title="Structured Speaker-Deficiency Adaptation of Foundation Models for   Dysarthric and Elderly Speech Recognition"></a>Structured Speaker-Deficiency Adaptation of Foundation Models for   Dysarthric and Elderly Speech Recognition</h2><p><strong>Authors:Shujie Hu, Xurong Xie, Mengzhe Geng, Jiajun Deng, Zengrui Jin, Tianzi Wang, Mingyu Cui, Guinan Li, Zhaoqing Li, Helen Meng, Xunying Liu</strong></p>
<p>Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and diverse dysarthric and elderly speech leads to data bias and poor generalization to unseen speakers. This paper proposes novel structured speaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data. Speaker and speech deficiency invariant SFMs were constructed in their supervised adaptive fine-tuning stage to reduce undue bias to training data speakers, and serves as a more neutral and robust starting point for test time unsupervised adaptation. Speech variability attributed to speaker identity and speech impairment severity, or aging induced neurocognitive decline, are modelled using separate adapters that can be combined together to model any seen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest structured speaker-deficiency adaptation of HuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs using either: a) no adapters; b) global adapters shared among all speakers; or c) single attribute adapters modelling speaker or deficiency labels alone by statistically significant WER reductions up to 3.01% and 1.50% absolute (10.86% and 6.94% relative) on the two tasks respectively. The lowest published WER of 19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained on the UASpeech test set of 16 dysarthric speakers. </p>
<blockquote>
<p>é’ˆå¯¹è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰çš„æ•°æ®å¯†é›†å‹ç²¾ç»†è°ƒæ•´ï¼Œåœ¨é¢å¯¹ç¨€ç¼ºä¸”å¤šæ ·çš„è¨€è¯­éšœç¢è€…å’Œè€å¹´è¯­éŸ³æ—¶ï¼Œä¼šå¯¼è‡´æ•°æ®åå·®ï¼Œå¹¶ä¸”å¯¹æ–°æœªè§è¯´è¯è€…çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ¬æ–‡é’ˆå¯¹æ­¤ç±»æ•°æ®ï¼Œæå‡ºäº†æ–°å‹çš„ç»“æ„åŒ–è¯´è¯è€…ç¼ºé™·é€‚åº”æ–¹æ³•ï¼Œç”¨äºSSLé¢„è®­ç»ƒçš„SFMsã€‚åœ¨ç›‘ç£é€‚åº”æ€§ç²¾ç»†è°ƒæ•´é˜¶æ®µï¼Œæ„å»ºäº†è¯´è¯è€…å’Œè¯­éŸ³ç¼ºé™·ä¸å˜çš„SFMsï¼Œä»¥å‡å°‘å¯¹è®­ç»ƒæ•°æ®è¯´è¯äººçš„ä¸å¿…è¦åè§ï¼Œå¹¶ä½œä¸ºæµ‹è¯•æ—¶é—´æ— ç›‘ç£é€‚åº”çš„æ›´ä¸­æ€§å’Œç¨³å¥çš„èµ·ç‚¹ã€‚ä¸è¯´è¯è€…èº«ä»½ã€è¯­éŸ³éšœç¢ä¸¥é‡ç¨‹åº¦æˆ–è¡°è€å¼•èµ·çš„ç¥ç»è®¤çŸ¥ä¸‹é™ç›¸å…³çš„è¯­éŸ³å˜åŒ–ï¼Œé€šè¿‡ä½¿ç”¨å•ç‹¬çš„é€‚é…å™¨è¿›è¡Œå»ºæ¨¡ï¼Œè¿™äº›é€‚é…å™¨å¯ä»¥ç»„åˆåœ¨ä¸€èµ·ï¼Œå¯¹ä»»ä½•å·²è§æˆ–æœªè§çš„è¯´è¯è€…è¿›è¡Œå»ºæ¨¡ã€‚åœ¨UASpeechè¨€è¯­éšœç¢å’ŒDementiaBank Pittè€å¹´è¯­éŸ³è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹HuBERTå’ŒWav2vec2-conformeræ¨¡å‹è¿›è¡Œç»“æ„åŒ–è¯´è¯è€…ç¼ºé™·é€‚åº”ï¼Œå§‹ç»ˆä¼˜äºä½¿ç”¨ä»¥ä¸‹æ–¹æ³•çš„åŸºçº¿SFMsï¼šaï¼‰æ— é€‚é…å™¨ï¼›bï¼‰æ‰€æœ‰è¯´è¯äººå…±äº«çš„å…¨å±€é€‚é…å™¨ï¼›cï¼‰ä»…å¯¹è¯´è¯äººæˆ–ç¼ºé™·æ ‡ç­¾è¿›è¡Œå»ºæ¨¡çš„å•å±æ€§é€‚é…å™¨ã€‚é€šè¿‡æ˜¾è‘—çš„WERé™ä½ï¼Œç»å¯¹é™ä½äº†3.01%å’Œ1.50%ï¼ˆç›¸å¯¹é™ä½äº†10.86%å’Œ6.94%ï¼‰ï¼Œåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚åœ¨UASpeechæµ‹è¯•é›†ä¸Šçš„æœ€ä½WERä¸º19.45%ï¼ˆåœ¨æä½æ¸…æ™°åº¦ä¸Šè¾¾åˆ°49.34%ï¼Œåœ¨æœªè§è¯è¯­ä¸Šè¾¾åˆ°33.17%ï¼‰ï¼Œè¯¥æµ‹è¯•é›†åŒ…å«16åè¨€è¯­éšœç¢è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18832v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ•°æ®å¯†é›†å‹çš„è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰å¯¹ç¨€ç¼ºå’Œå¤šæ ·åŒ–çš„è¨€è¯­éšœç¢è€…å’Œè€å¹´è¯­éŸ³è¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´æ•°æ®åè§ï¼Œå¹¶ä¸”éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯´è¯è€…ã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹SSLé¢„è®­ç»ƒSFMçš„æ–°å‹ç»“æ„åŒ–è¯´è¯è€…ç¼ºé™·é€‚åº”æ–¹æ³•ã€‚åœ¨ç›‘ç£è‡ªé€‚åº”å¾®è°ƒé˜¶æ®µï¼Œæ„å»ºäº†è¯´è¯è€…å’Œè¯­éŸ³ç¼ºé™·ä¸å˜çš„SFMï¼Œä»¥å‡å°‘å¯¹è®­ç»ƒæ•°æ®è¯´è¯äººçš„ä¸å¿…è¦åè§ï¼Œå¹¶ä½œä¸ºæµ‹è¯•æ—¶é—´æ— ç›‘ç£é€‚åº”çš„æ›´ä¸­æ€§å’Œç¨³å¥çš„èµ·ç‚¹ã€‚è¯­éŸ³å˜åŒ–å½’å› äºè¯´è¯äººèº«ä»½ã€è¯­éŸ³éšœç¢ä¸¥é‡ç¨‹åº¦æˆ–ç”±è¡°è€å¼•èµ·çš„ç¥ç»è®¤çŸ¥ä¸‹é™ï¼Œé€šè¿‡ä½¿ç”¨å•ç‹¬çš„é€‚é…å™¨è¿›è¡Œå»ºæ¨¡ï¼Œè¿™äº›é€‚é…å™¨å¯ä»¥ç»„åˆåœ¨ä¸€èµ·ï¼Œå¯¹ä»»ä½•å·²è§æˆ–æœªè§çš„è¯´è¯è€…è¿›è¡Œå»ºæ¨¡ã€‚åœ¨UASpeechè¨€è¯­éšœç¢è€…å’ŒDementiaBank Pittè€å¹´è¯­éŸ³è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHuBERTå’ŒWav2vec2-conformeræ¨¡å‹çš„ç»“æ„åŒ–è¯´è¯è€…ç¼ºé™·é€‚åº”å§‹ç»ˆä¼˜äºåŸºçº¿SFMä½¿ç”¨ï¼šaï¼‰æ— é€‚é…å™¨ï¼›bï¼‰æ‰€æœ‰è¯´è¯äººå…±äº«çš„å…¨å±€é€‚é…å™¨ï¼›cï¼‰å•ç‹¬å»ºæ¨¡è¯´è¯äººæˆ–ç¼ºé™·æ ‡ç­¾çš„å•å±æ€§é€‚é…å™¨ã€‚é€šè¿‡æ˜¾è‘—çš„WERé™ä½ï¼Œç»å¯¹é™ä½å€¼æœ€é«˜è¾¾åˆ°3.01ï¼…å’Œ1.50ï¼…ï¼ˆç›¸å¯¹é™ä½åˆ†åˆ«ä¸º10.86ï¼…å’Œ6.94ï¼…ï¼‰ã€‚åœ¨UASpeechæµ‹è¯•é›†ä¸Šçš„æœ€ä½å·²å‘å¸ƒWERä¸º19.45ï¼…ï¼Œæ¶µç›–äº†16ä½è¨€è¯­éšœç¢è€…çš„æµ‹è¯•é›†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®å¯†é›†å‹çš„è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰å¯¹ç‰¹å®šæ•°æ®ï¼ˆå¦‚ç¨€ç¼ºå’Œå¤šæ ·åŒ–çš„è¨€è¯­éšœç¢è€…å’Œè€å¹´è¯­éŸ³ï¼‰è¿›è¡Œå¾®è°ƒæ—¶ï¼Œæ˜“äº§ç”Ÿæ•°æ®åè§ï¼Œå½±å“æœªè§è¯´è¯è€…çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºæ–°å‹ç»“æ„åŒ–æ¼”è®²è€…ç¼ºé™·é€‚åº”æ–¹æ³•ï¼Œä»¥ç¼“è§£è®­ç»ƒæ•°æ®è¯´è¯äººçš„åè§é—®é¢˜ï¼Œå¹¶æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡åœ¨ç›‘ç£è‡ªé€‚åº”å¾®è°ƒé˜¶æ®µæ„å»ºè¯´è¯è€…å’Œè¯­éŸ³ç¼ºé™·ä¸å˜çš„SFMï¼Œä¸ºæµ‹è¯•æ—¶çš„æ— ç›‘ç£é€‚åº”æä¾›äº†æ›´ä¸­æ€§å’Œç¨³å›ºçš„èµ·ç‚¹ã€‚</li>
<li>å»ºæ¨¡èƒ½å¤Ÿå¤„ç†ä¸è¯´è¯äººèº«ä»½ã€è¯­éŸ³éšœç¢çš„ä¸¥é‡ç¨‹åº¦ä»¥åŠè¡°è€å¼•èµ·çš„ç¥ç»è®¤çŸ¥ä¸‹é™ç›¸å…³çš„è¯­éŸ³å˜åŒ–ã€‚</li>
<li>é€šè¿‡ç»„åˆå•ç‹¬çš„é€‚é…å™¨ï¼Œæ¨¡å‹å¯ä»¥é€‚åº”ä»»ä½•å·²è§æˆ–æœªè§çš„è¯´è¯è€…ã€‚</li>
<li>åœ¨UASpeechå’ŒDementiaBank Pittè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»“æ„åŒ–æ¼”è®²è€…ç¼ºé™·é€‚åº”æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿SFMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-13e567b74a8ad468d0cf3bf6844b073a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c63cea5c1f4ba43a2764cfa52be963f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2e8236d70eb02e4aaa3981e15c860ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfa131c5b07998582a88733c64002b5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0aed6e37afcc31f172f73437c6a1e550.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction"><a href="#Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction" class="headerlink" title="Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction"></a>Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction</h2><p><strong>Authors:Yuan Zhao, Rui Liu, Gaoxiang Cong</strong></p>
<p>Automatic Video Dubbing (AVD) generates speech aligned with lip motion and facial emotion from scripts. Recent research focuses on modeling multimodal context to enhance prosody expressiveness but overlooks two key issues: 1) Multiscale prosody expression attributes in the context influence the current sentenceâ€™s prosody. 2) Prosody cues in context interact with the current sentence, impacting the final prosody expressiveness. To tackle these challenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction scheme for AVD. This scheme includes two shared M2CI encoders to model the multiscale multimodal context and facilitate its deep interaction with the current sentence. By extracting global and local features for each modality in the context, utilizing attention-based mechanisms for aggregation and interaction, and employing an interaction-based graph attention network for fusion, the proposed approach enhances the prosody expressiveness of synthesized speech for the current sentence. Experiments on the Chem dataset show our model outperforms baselines in dubbing expressiveness. The code and demos are available at \textcolor[rgb]{0.93,0.0,0.47}{<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%7D">https://github.com/AI-S2-Lab/M2CI-Dubber}</a>. </p>
<blockquote>
<p>è‡ªåŠ¨è§†é¢‘é…éŸ³ï¼ˆAVDï¼‰æ ¹æ®è„šæœ¬ç”Ÿæˆä¸å”‡éƒ¨åŠ¨ä½œå’Œé¢éƒ¨æƒ…ç»ªç›¸åŒ¹é…çš„è¯­éŸ³ã€‚æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨å»ºç«‹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä»¥æé«˜éŸµå¾‹è¡¨è¾¾æ€§ï¼Œä½†å¿½ç•¥äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1ï¼‰ä¸Šä¸‹æ–‡ä¸­çš„å¤šå°ºåº¦éŸµå¾‹è¡¨è¾¾å±æ€§ä¼šå½±å“å½“å‰å¥å­çš„éŸµå¾‹ã€‚2ï¼‰ä¸Šä¸‹æ–‡ä¸­çš„éŸµå¾‹çº¿ç´¢ä¸å½“å‰å¥å­ç›¸äº’ä½œç”¨ï¼Œå½±å“æœ€ç»ˆçš„éŸµå¾‹è¡¨è¾¾æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2CI-Dubberï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºAVDçš„å¤šå°ºåº¦å¤šæ¨¡æ€ä¸Šä¸‹æ–‡äº¤äº’æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåŒ…æ‹¬ä¸¤ä¸ªå…±äº«çš„M2CIç¼–ç å™¨ï¼Œç”¨äºå»ºç«‹å¤šå°ºåº¦å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¿ƒè¿›å…¶ä¸å½“å‰å¥å­çš„æ·±åº¦äº¤äº’ã€‚é€šè¿‡æå–ä¸Šä¸‹æ–‡ä¸­æ¯ç§æ¨¡æ€çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶è¿›è¡Œèšåˆå’Œäº¤äº’ï¼Œå¹¶é‡‡ç”¨åŸºäºäº¤äº’çš„å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œèåˆï¼Œæ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†å½“å‰å¥å­åˆæˆè¯­éŸ³çš„éŸµå¾‹è¡¨è¾¾æ€§ã€‚åœ¨Chemæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é…éŸ³è¡¨è¾¾æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/M2CI-Dubberä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18748v1">PDF</a> Accepted by ICSSP 2025</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è§†é¢‘é…éŸ³ï¼ˆAVDï¼‰æŠ€æœ¯èƒ½å¤Ÿæ ¹æ®è„šæœ¬ç”Ÿæˆä¸å”‡éƒ¨åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ç›¸åŒ¹é…çš„è¯­éŸ³ã€‚ç„¶è€Œï¼Œå½“å‰ç ”ç©¶åœ¨æ¨¡æ‹Ÿå¤šæ¨¡æ€è¯­å¢ƒä»¥æå‡è¯­è°ƒè¡¨è¾¾æ–¹é¢å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯å¤šå°ºåº¦è¯­è°ƒè¡¨è¾¾å±æ€§åœ¨è¯­å¢ƒä¸­çš„å½±å“ï¼›äºŒæ˜¯è¯­å¢ƒä¸­çš„è¯­è°ƒçº¿ç´¢ä¸å½“å‰å¥å­çš„äº’åŠ¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2CI-Dubberæ–¹æ¡ˆï¼Œè¿™æ˜¯ä¸€ç§å¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒäº¤äº’çš„AVDæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåŒ…å«ä¸¤ä¸ªå…±äº«çš„M2CIç¼–ç å™¨ï¼Œç”¨äºæ¨¡æ‹Ÿå¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒï¼Œå¹¶ä¸å½“å‰å¥å­è¿›è¡Œæ·±åº¦äº¤äº’ã€‚é€šè¿‡æå–è¯­å¢ƒä¸­æ¯ç§æ¨¡æ€çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶è¿›è¡Œèšåˆå’Œäº’åŠ¨ï¼Œå¹¶é‡‡ç”¨åŸºäºäº¤äº’çš„å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œèåˆï¼Œæå‡äº†åˆæˆè¯­éŸ³çš„è¯­è°ƒè¡¨è¾¾åŠ›ã€‚åœ¨Chemæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é…éŸ³è¡¨ç°åŠ›æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVDæŠ€æœ¯èƒ½å¤Ÿç”Ÿæˆä¸å”‡éƒ¨åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…åŒ¹é…çš„è¯­éŸ³ã€‚</li>
<li>å½“å‰ç ”ç©¶åœ¨æ¨¡æ‹Ÿå¤šæ¨¡æ€è¯­å¢ƒä»¥æå‡è¯­è°ƒè¡¨è¾¾æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>M2CI-Dubberæ–¹æ¡ˆæ—¨åœ¨è§£å†³å¤šå°ºåº¦è¯­è°ƒè¡¨è¾¾å’Œè¯­å¢ƒä¸­çš„è¯­è°ƒçº¿ç´¢ä¸å½“å‰å¥å­çš„äº’åŠ¨é—®é¢˜ã€‚</li>
<li>M2CI-DubberåŒ…å«ä¸¤ä¸ªå…±äº«çš„M2CIç¼–ç å™¨ï¼Œç”¨äºæ¨¡æ‹Ÿå¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒã€‚</li>
<li>é€šè¿‡æå–å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€åˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶ä»¥åŠå›¾æ³¨æ„åŠ›ç½‘ç»œï¼ŒM2CI-Dubberæå‡äº†åˆæˆè¯­éŸ³çš„è¯­è°ƒè¡¨è¾¾åŠ›ã€‚</li>
<li>åœ¨Chemæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM2CI-Dubberæ¨¡å‹åœ¨é…éŸ³è¡¨ç°åŠ›æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2f8891ba94765568bb0584102e832fb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31038594f70b86676915cf3312a1d616.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb24b0ed7585afdbba1513c1215118e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa61ec535aabd0fdb085abd902e9ebf.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario"><a href="#Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario" class="headerlink" title="Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario"></a>Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario</h2><p><strong>Authors:Wen Wen, Qiang Zhou, Yu Xi, Haoyu Li, Ziqi Gong, Kai Yu</strong></p>
<p>In multi-speaker scenarios, leveraging spatial features is essential for enhancing target speech. While with limited microphone arrays, developing a compact multi-channel speech enhancement system remains challenging, especially in extremely low signal-to-noise ratio (SNR) conditions. To tackle this issue, we propose a triple-steering spatial selection method, a flexible framework that uses three steering vectors to guide enhancement and determine the enhancement range. Specifically, we introduce a causal-directed U-Net (CDUNet) model, which takes raw multi-channel speech and the desired enhancement width as inputs. This enables dynamic adjustment of steering vectors based on the target direction and fine-tuning of the enhancement region according to the angular separation between the target and interference signals. Our model with only a dual microphone array, excels in both speech quality and downstream task performance. It operates in real-time with minimal parameters, making it ideal for low-latency, on-device streaming applications. </p>
<blockquote>
<p>åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œåˆ©ç”¨ç©ºé—´ç‰¹å¾å¢å¼ºç›®æ ‡è¯­éŸ³è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨æœ‰é™çš„éº¦å…‹é£é˜µåˆ—ä¸‹ï¼Œå¼€å‘ç´§å‡‘çš„å¤šé€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡å·ä¸å™ªå£°æ¯”ï¼ˆSNRï¼‰æä½çš„æ¡ä»¶ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰å‘å¼•å¯¼çš„ç©ºé—´é€‰æ‹©æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»æ¡†æ¶ï¼Œä½¿ç”¨ä¸‰ä¸ªå¼•å¯¼å‘é‡æ¥æŒ‡å¯¼å¢å¼ºå¹¶ç¡®å®šå¢å¼ºèŒƒå›´ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå› æœå¯¼å‘çš„U-Netï¼ˆCDUNetï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥åŸå§‹å¤šé€šé“è¯­éŸ³å’Œæ‰€éœ€çš„å¢å¼ºå®½åº¦ä¸ºè¾“å…¥ã€‚è¿™å¯ä»¥æ ¹æ®ç›®æ ‡æ–¹å‘å’Œç›®æ ‡ä¿¡å·ä¸å¹²æ‰°ä¿¡å·ä¹‹é—´çš„è§’åº¦é—´éš”åŠ¨æ€è°ƒæ•´å¼•å¯¼å‘é‡å¹¶å¾®è°ƒå¢å¼ºåŒºåŸŸã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»…ä½¿ç”¨åŒéº¦å…‹é£é˜µåˆ—ï¼Œåœ¨è¯­éŸ³è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚å®ƒä»¥å®æ—¶æ–¹å¼è¿è¡Œï¼Œå‚æ•°æœ€å°‘ï¼Œéå¸¸é€‚åˆä½å»¶è¿Ÿçš„åœ¨çº¿æµå¼åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18141v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong>ï¼šåœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œåˆ©ç”¨ç©ºé—´ç‰¹å¾å¢å¼ºç›®æ ‡è¯­éŸ³è‡³å…³é‡è¦ã€‚é’ˆå¯¹å…·æœ‰æœ‰é™çš„éº¦å…‹é£é˜µåˆ—å’Œæä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é‡‡ç”¨ä¸‰ä¸ªå¯¼å¼•å‘é‡è¿›è¡Œå¢å¼ºå’Œå¼•å¯¼çš„çµæ´»æ¡†æ¶ï¼Œå¹¶å¼•å…¥å› æœå¯¼å‘U-Netæ¨¡å‹ï¼Œå®ç°åŠ¨æ€è°ƒæ•´å¯¼å¼•å‘é‡å’Œç²¾ç»†è°ƒæ•´å¢å¼ºåŒºåŸŸã€‚è¯¥æ¨¡å‹ä»…ä½¿ç”¨åŒéº¦å…‹é£é˜µåˆ—ï¼Œåœ¨è¯­éŸ³è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºä½å»¶è¿Ÿçš„åœ¨çº¿æµå¼åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œç©ºé—´ç‰¹å¾å¯¹äºå¢å¼ºç›®æ ‡è¯­éŸ³è‡³å…³é‡è¦ã€‚</li>
<li>é¢å¯¹æœ‰é™çš„éº¦å…‹é£é˜µåˆ—å’Œæç«¯ä½ä¿¡å™ªæ¯”æ¡ä»¶ï¼Œè®¾è®¡çµæ´»æ¡†æ¶ä»¥è¿›è¡Œå¤šé€šé“è¯­éŸ³å¢å¼ºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é‡‡ç”¨ä¸‰ä¸ªå¯¼å¼•å‘é‡çš„æ–¹æ³•æ¥ç¡®å®šå¢å¼ºèŒƒå›´ã€‚</li>
<li>å¼•å…¥äº†å› æœå¯¼å‘U-Netæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç›®æ ‡æ–¹å‘å’Œè§’åº¦åˆ†ç¦»åŠ¨æ€è°ƒæ•´å¯¼å¼•å‘é‡å’Œå¢å¼ºåŒºåŸŸã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨åŒéº¦å…‹é£é˜µåˆ—çš„æƒ…å†µä¸‹ï¼Œåœ¨è¯­éŸ³è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹å¯å®ç°å®æ—¶æ“ä½œï¼Œå‚æ•°å°‘ï¼Œé€‚åˆä½å»¶è¿Ÿçš„åœ¨çº¿æµå¼åº”ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³å¢å¼ºé—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23976b356b116997ee29214ac866dfe4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86e4be31bad6d3e8241d5204a21f28fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46340f8c64ec6158dce1f979716c2895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52c0f6feadd984d4947f6c33c8111bf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4039e0ead841fd19d578014b9991a087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2fe450e8623772df435eca211f85e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687ccc37a1d7be71ca26e2a21500e6c8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions"><a href="#DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions" class="headerlink" title="DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions"></a>DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions</h2><p><strong>Authors:Shu-Tong Niu, Jun Du, Ruo-Yu Wang, Gao-Bin Yang, Tian Gao, Jia Pan, Yu Hu</strong></p>
<p>We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end automatic speech recognition (ASR), combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art single-channel speech recognition performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å•é€šé“æ·±åº¦çº§è”èåˆåˆ†æ²»ä¸åˆ†ç¦»ï¼ˆDCF-DSï¼‰æ¡†æ¶ï¼Œç”¨äºåç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç¥ç»ç½‘ç»œåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨è”åˆè®­ç»ƒæ¡†æ¶ä¸­æŒ‰é¡ºåºæ•´åˆNSDå’ŒSSæ¨¡å—ï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆ†æ²»æ¨¡å—ä¸­çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œã€‚ç„¶åï¼Œä¸ºäº†è¡¥å……DCF-DSè®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çª—å£çº§åˆ«çš„è§£ç æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå…è®¸DCF-DSæ¡†æ¶è§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šï¼ˆSDCIï¼‰é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢ä½¿ç”¨åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒNSDç³»ç»Ÿæ¥æä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨DCF-DSæ¡†æ¶ä¸­èå…¥äº†ä¸€ä¸ªå¯é€‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿™å¸¦æ¥äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°èšç±»DCF-DSè¾“å‡ºç»“æœæ¥æå‡åˆ†æ²»ç»“æœï¼Œæé«˜ASRçš„å‡†ç¡®æ€§ã€‚é€šè¿‡é‡‡ç”¨DCF-DSæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜çš„ç°å®ä¸­å•é€šé“èµ›é“ä¸Šå–å¾—äº†ç¬¬ä¸€åã€‚æˆ‘ä»¬è¿˜å¯¹å¼€æ”¾çš„LibriCSSæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†æœ€æ–°çš„å•é€šé“è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06667v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å•é€šé“æ·±åº¦çº§è”èåˆåˆ†æ²»ä¸åˆ†ç¦»ï¼ˆDCF-DSï¼‰æ¡†æ¶ï¼Œç”¨äºåç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚é¦–å…ˆï¼Œåœ¨ä¸€ä¸ªè”åˆè®­ç»ƒæ¡†æ¶å†…æŒ‰é¡ºåºæ•´åˆNSDå’ŒSSæ¨¡å—ï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆ†æ²»æ¨¡å—æä¾›çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œã€‚å…¶æ¬¡ï¼Œä¸ºäº†è¡¥å……DCF-DSè®­ç»ƒï¼Œå¼•å…¥äº†ä¸€ç§çª—å£çº§è§£ç æ–¹æ¡ˆï¼Œä»¥è§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šï¼ˆSDCIï¼‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†ä½¿ç”¨åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„NSDç³»ç»Ÿæ¥æä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚åŒæ—¶ï¼Œåœ¨DCF-DSæ¡†æ¶ä¸­èå…¥äº†ä¸€ä¸ªå¯é€‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚æœ€åï¼Œé€šè¿‡å¯¹DCF-DSè¾“å‡ºçš„é‡æ–°èšç±»ï¼Œä¼˜åŒ–äº†åˆ†æ²»ç»“æœï¼Œæé«˜äº†ASRçš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜çš„ç°å®å•é€šé“èµ›é“ä¸­è£è·ç¬¬ä¸€åï¼Œå¹¶åœ¨å¼€æ”¾çš„LibriCSSæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„å•é€šé“è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å•é€šé“Deep Cascade Fusion of Diarization and Separationï¼ˆDCF-DSï¼‰æ¡†æ¶ï¼Œç»“åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒNSDå’ŒSSæ¨¡å—ï¼Œä½¿è¯­éŸ³åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿåˆ©ç”¨è¯´è¯äººæ—¶é—´è¾¹ç•Œä¿¡æ¯ã€‚</li>
<li>å¼•å…¥çª—å£çº§è§£ç æ–¹æ¡ˆï¼Œè§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>ä½¿ç”¨çœŸå®æ•°æ®é›†è®­ç»ƒçš„NSDç³»ç»Ÿæä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚</li>
<li>å¯é€‰èå…¥å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>é€šè¿‡é‡æ–°èšç±»DCF-DSè¾“å‡ºï¼Œä¼˜åŒ–åˆ†æ²»ç»“æœï¼Œæé«˜ASRå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.06667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1561da96892171e73ece2cfd77ae8d85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a607f50797c02cdeadb60ff81187b3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17eacfcd52ab774b92fd352740262db9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28209e53ddd34d65028a5b1c616c21aa.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Mamba-for-Streaming-ASR-Combined-with-Unimodal-Aggregation"><a href="#Mamba-for-Streaming-ASR-Combined-with-Unimodal-Aggregation" class="headerlink" title="Mamba for Streaming ASR Combined with Unimodal Aggregation"></a>Mamba for Streaming ASR Combined with Unimodal Aggregation</h2><p><strong>Authors:Ying Fang, Xiaofei Li</strong></p>
<p>This paper works on streaming automatic speech recognition (ASR). Mamba, a recently proposed state space model, has demonstrated the ability to match or surpass Transformers in various tasks while benefiting from a linear complexity advantage. We explore the efficiency of Mamba encoder for streaming ASR and propose an associated lookahead mechanism for leveraging controllable future information. Additionally, a streaming-style unimodal aggregation (UMA) method is implemented, which automatically detects token activity and streamingly triggers token output, and meanwhile aggregates feature frames for better learning token representation. Based on UMA, an early termination (ET) method is proposed to further reduce recognition latency. Experiments conducted on two Mandarin Chinese datasets demonstrate that the proposed model achieves competitive ASR performance in terms of both recognition accuracy and latency. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶æµå¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚Mambaæ˜¯ä¸€ç§æœ€è¿‘æå‡ºçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä¸Transformerç›¸åŒ¹é…æˆ–æ›´é«˜çš„æ€§èƒ½ï¼ŒåŒæ—¶å¾—ç›Šäºå…¶çº¿æ€§å¤æ‚åº¦çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬æ¢ç´¢äº†Mambaç¼–ç å™¨åœ¨æµå¼ASRä¸­çš„æ•ˆç‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç›¸å…³çš„å‰ç»æœºåˆ¶ï¼Œä»¥åˆ©ç”¨å¯æ§çš„æœªæ¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†ä¸€ç§æµå¼å•æ¨¡æ€èšåˆï¼ˆUMAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨æ£€æµ‹æ ‡è®°æ´»åŠ¨å¹¶æµå¼è§¦å‘æ ‡è®°è¾“å‡ºï¼ŒåŒæ—¶èšåˆç‰¹å¾å¸§ä»¥æ›´å¥½åœ°å­¦ä¹ æ ‡è®°è¡¨ç¤ºã€‚åŸºäºUMAï¼Œæå‡ºäº†ä¸€ç§æ—©æœŸç»ˆæ­¢ï¼ˆETï¼‰æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥é™ä½è¯†åˆ«å»¶è¿Ÿã€‚åœ¨ä¸¤ç§ä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯†åˆ«å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢éƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ASRæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00070v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºæµåª’ä½“çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚Mambaä½œä¸ºä¸€ç§æ–°è¿‘æå‡ºçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºä¸Transformerç›¸åŒ¹é…æˆ–æ›´ä½³çš„æ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„ä¼˜åŠ¿ã€‚æœ¬æ–‡æ¢ç´¢äº†Mambaç¼–ç å™¨åœ¨æµå¼ASRä¸­çš„æ•ˆç‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç›¸å…³çš„å‰ç»æœºåˆ¶ï¼Œä»¥åˆ©ç”¨å¯æ§çš„æœªæ¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ç°äº†ä¸€ç§æµå¼å•æ¨¡æ€èšåˆï¼ˆUMAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨æ£€æµ‹ä»¤ç‰Œæ´»åŠ¨å¹¶æµå¼è§¦å‘ä»¤ç‰Œè¾“å‡ºï¼ŒåŒæ—¶èšåˆç‰¹å¾å¸§ä»¥æ›´å¥½åœ°å­¦ä¹ ä»¤ç‰Œè¡¨ç¤ºã€‚åŸºäºUMAï¼Œè¿›ä¸€æ­¥æå‡ºäº†æ—©æœŸç»ˆæ­¢ï¼ˆETï¼‰æ–¹æ³•æ¥å‡å°‘è¯†åˆ«å»¶è¿Ÿã€‚åœ¨ä¸¤ä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯†åˆ«å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢å‡è¡¨ç°å‡ºç«äº‰åŠ›çš„ASRæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Mambaæ¨¡å‹åœ¨æµå¼ASRä¸­å±•ç°å‡ºé«˜æ•ˆæ€§èƒ½ï¼Œå¯åŒ¹é…æˆ–è¶…è¶ŠTransformeræ¨¡å‹ï¼Œå¹¶å…·å¤‡çº¿æ€§å¤æ‚åº¦ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†åˆ©ç”¨Mambaç¼–ç å™¨çš„æµå¼ASRå‰ç»æœºåˆ¶ï¼Œä»¥åˆ©ç”¨å¯æ§çš„æœªæ¥ä¿¡æ¯ã€‚</li>
<li>å®ç°äº†æµå¼å•æ¨¡æ€èšåˆï¼ˆUMAï¼‰æ–¹æ³•ï¼Œèƒ½è‡ªåŠ¨æ£€æµ‹ä»¤ç‰Œæ´»åŠ¨å¹¶è§¦å‘æµå¼ä»¤ç‰Œè¾“å‡ºï¼ŒåŒæ—¶èšåˆç‰¹å¾å¸§ä»¥ä¼˜åŒ–å­¦ä¹ ã€‚</li>
<li>åŸºäºUMAæ–¹æ³•ï¼Œæå‡ºäº†æ—©æœŸç»ˆæ­¢ï¼ˆETï¼‰ç­–ç•¥æ¥è¿›ä¸€æ­¥é™ä½è¯†åˆ«å»¶è¿Ÿã€‚</li>
<li>åœ¨ä¸¤ä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨ASRæ€§èƒ½å’Œè¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„ç«äº‰åŠ›ã€‚</li>
<li>Mambaæ¨¡å‹åœ¨ASRä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆç‡å’Œä½å»¶è¿Ÿçš„æµå¼å¤„ç†åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88de93fb2c3556b0051ceccac09bd460.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d08199b60b416658557070fbcad3c60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea353adc4c5919e31d59e2c1d766350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f46fb661c0f69e98ac382a39cb86b760.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="A-Modular-based-Strategy-for-Mitigating-Gradient-Conflicts-in-Simultaneous-Speech-Translation"><a href="#A-Modular-based-Strategy-for-Mitigating-Gradient-Conflicts-in-Simultaneous-Speech-Translation" class="headerlink" title="A Modular-based Strategy for Mitigating Gradient Conflicts in   Simultaneous Speech Translation"></a>A Modular-based Strategy for Mitigating Gradient Conflicts in   Simultaneous Speech Translation</h2><p><strong>Authors:Xiaoqian Liu, Yangfan Du, Jianjin Wang, Yuan Ge, Chen Xu, Tong Xiao, Guocheng Chen, Jingbo Zhu</strong></p>
<p>Simultaneous Speech Translation (SimulST) involves generating target language text while continuously processing streaming speech input, presenting significant real-time challenges. Multi-task learning is often employed to enhance SimulST performance but introduces optimization conflicts between primary and auxiliary tasks, potentially compromising overall efficiency. The existing model-level conflict resolution methods are not well-suited for this task which exacerbates inefficiencies and leads to high GPU memory consumption. To address these challenges, we propose a Modular Gradient Conflict Mitigation (MGCM) strategy that detects conflicts at a finer-grained modular level and resolves them utilizing gradient projection. Experimental results demonstrate that MGCM significantly improves SimulST performance, particularly under medium and high latency conditions, achieving a 0.68 BLEU score gain in offline tasks. Additionally, MGCM reduces GPU memory consumption by over 95% compared to other conflict mitigation methods, establishing it as a robust solution for SimulST tasks. </p>
<blockquote>
<p>åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰æ¶‰åŠåœ¨æŒç»­å¤„ç†æµå¼è¯­éŸ³è¾“å…¥çš„åŒæ—¶ç”Ÿæˆç›®æ ‡è¯­è¨€æ–‡æœ¬ï¼Œè¿™å¸¦æ¥äº†æ˜¾è‘—çš„çœŸå®æ—¶é—´æŒ‘æˆ˜ã€‚å¤šä»»åŠ¡å­¦ä¹ é€šå¸¸è¢«ç”¨æ¥æé«˜SimulSTçš„æ€§èƒ½ï¼Œä½†ä¼šåœ¨ä¸»è¦ä»»åŠ¡å’Œè¾…åŠ©ä»»åŠ¡ä¹‹é—´å¼•å…¥ä¼˜åŒ–å†²çªï¼Œå¯èƒ½æŸå®³æ•´ä½“æ•ˆç‡ã€‚ç°æœ‰çš„æ¨¡å‹çº§å†²çªè§£å†³æ–¹æ³•å¹¶ä¸é€‚åˆè¿™é¡¹ä»»åŠ¡ï¼Œè¿™åŠ å‰§äº†æ•ˆç‡é—®é¢˜å¹¶å¯¼è‡´GPUå†…å­˜æ¶ˆè€—è¿‡é«˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å—æ¢¯åº¦å†²çªç¼“è§£ï¼ˆMGCMï¼‰ç­–ç•¥ï¼Œå®ƒåœ¨æ›´ç²¾ç»†çš„æ¨¡å—çº§åˆ«æ£€æµ‹å¹¶è§£å†³å†²çªï¼Œåˆ©ç”¨æ¢¯åº¦æŠ•å½±æ¥å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMGCMèƒ½æ˜¾è‘—æé«˜SimulSTçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ç­‰å’Œé«˜çš„å»¶è¿Ÿæ¡ä»¶ä¸‹ï¼Œç¦»çº¿ä»»åŠ¡ä¸­BLEUå¾—åˆ†æé«˜äº†0.68ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–å†²çªç¼“è§£æ–¹æ³•ç›¸æ¯”ï¼ŒMGCMå°†GPUå†…å­˜æ¶ˆè€—é™ä½äº†è¶…è¿‡95%ï¼Œä½¿å…¶æˆä¸ºSimulSTä»»åŠ¡çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15911v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å®æ—¶å¤„ç†æµå¼è¯­éŸ³è¾“å…¥æ—¶ç”Ÿæˆç›®æ ‡è¯­è¨€æ–‡æœ¬çš„é—®é¢˜ã€‚è™½ç„¶å¤šä»»åŠ¡å­¦ä¹ è¢«ç”¨æ¥æé«˜SimulSTæ€§èƒ½ï¼Œä½†å®ƒå¼•å…¥äº†ä¸»è¦ä»»åŠ¡å’Œè¾…åŠ©ä»»åŠ¡ä¹‹é—´çš„ä¼˜åŒ–å†²çªï¼Œå¯èƒ½æŸå®³æ•´ä½“æ•ˆç‡ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–æ¢¯åº¦å†²çªç¼“è§£ï¼ˆMGCMï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ›´ç²¾ç»†çš„æ¨¡å—åŒ–å±‚é¢æ£€æµ‹å¹¶è§£å†³å†²çªï¼Œåˆ©ç”¨æ¢¯åº¦æŠ•å½±æ¥å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMGCMèƒ½æ˜¾è‘—æé«˜SimulSTæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ç­‰å’Œè¾ƒé«˜çš„å»¶è¿Ÿæ¡ä»¶ä¸‹ï¼Œåœ¨ç¦»çº¿ä»»åŠ¡ä¸­å®ç°äº†0.68çš„BLEUå¾—åˆ†å¢ç›Šã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–å†²çªç¼“è§£æ–¹æ³•ç›¸æ¯”ï¼ŒMGCMé™ä½äº†è¶…è¿‡95%çš„GPUå†…å­˜æ¶ˆè€—ï¼Œæˆä¸ºSimulSTä»»åŠ¡çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰é¢ä¸´å®æ—¶å¤„ç†æµå¼è¯­éŸ³è¾“å…¥çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ åœ¨SimulSTä¸­å¼•å…¥ä¼˜åŒ–å†²çªï¼Œå¯èƒ½å½±å“æ•ˆç‡ã€‚</li>
<li>æ¨¡å—åŒ–æ¢¯åº¦å†²çªç¼“è§£ï¼ˆMGCMï¼‰ç­–ç•¥èƒ½åœ¨æ›´ç²¾ç»†çš„æ¨¡å—åŒ–å±‚é¢æ£€æµ‹å¹¶è§£å†³å†²çªã€‚</li>
<li>MGCMé€šè¿‡æ¢¯åº¦æŠ•å½±å®ç°ï¼Œå¯æé«˜SimulSTæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMGCMåœ¨ç¦»çº¿ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„BLEUå¾—åˆ†å¢ç›Šã€‚</li>
<li>MGCMé™ä½äº†GPUå†…å­˜æ¶ˆè€—ï¼Œè¶…è¿‡95%ï¼Œæ¯”å…¶ä»–å†²çªç¼“è§£æ–¹æ³•æ›´æœ‰æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b343a1576b486d4de58c19650776685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b4fc0af5683f4c2df49c5fab39af256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-168c05f51f0caa66211e84b4e1d634a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eda20af0c87a7e65ed6f40f8a571cde7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bb372937362ebe61d84b01951b971cab.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  KunServe Elastic and Efficient Large Language Model Serving with   Parameter-centric Memory Management
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-caf3ad39b704755169a9ae027e45fd58.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Neighbor Does Matter Density-Aware Contrastive Learning for Medical   Semi-supervised Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
