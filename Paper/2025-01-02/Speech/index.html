<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-02  Enhancing Multimodal Emotion Recognition through Multi-Granularity   Cross-Modal Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7a607f50797c02cdeadb60ff81187b3f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    72 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-02-更新"><a href="#2025-01-02-更新" class="headerlink" title="2025-01-02 更新"></a>2025-01-02 更新</h1><h2 id="Enhancing-Multimodal-Emotion-Recognition-through-Multi-Granularity-Cross-Modal-Alignment"><a href="#Enhancing-Multimodal-Emotion-Recognition-through-Multi-Granularity-Cross-Modal-Alignment" class="headerlink" title="Enhancing Multimodal Emotion Recognition through Multi-Granularity   Cross-Modal Alignment"></a>Enhancing Multimodal Emotion Recognition through Multi-Granularity   Cross-Modal Alignment</h2><p><strong>Authors:Xuechen Wang, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiaming Zhou, Yong Qin</strong></p>
<p>Multimodal emotion recognition (MER), leveraging speech and text, has emerged as a pivotal domain within human-computer interaction, demanding sophisticated methods for effective multimodal integration. The challenge of aligning features across these modalities is significant, with most existing approaches adopting a singular alignment strategy. Such a narrow focus not only limits model performance but also fails to address the complexity and ambiguity inherent in emotional expressions. In response, this paper introduces a Multi-Granularity Cross-Modal Alignment (MGCMA) framework, distinguished by its comprehensive approach encompassing distribution-based, instance-based, and token-based alignment modules. This framework enables a multi-level perception of emotional information across modalities. Our experiments on IEMOCAP demonstrate that our proposed method outperforms current state-of-the-art techniques. </p>
<blockquote>
<p>多模态情感识别（MER），利用语音和文本，已成为人机交互中的关键领域，需要有效的方法进行多模态集成的高级方法。不同模态的特征对齐挑战巨大，现有的大多数方法都采用了单一的对齐策略。这种狭窄的焦点不仅限制了模型性能，而且未能解决情感表达中固有的复杂性和模糊性。针对这一问题，本文提出了基于多粒度跨模态对齐（MGCMA）的框架，其显著特点在于包括基于分布、基于实例和基于标记的对齐模块的综合方法。该框架实现了跨模态的情感信息多级感知。我们在IEMOCAP上的实验表明，我们提出的方法优于当前的最先进技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20821v1">PDF</a> ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech   and Signal Processing (ICASSP)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态情感识别（MER）领域的重要性和挑战，该领域利用语音和文本信息，是人与计算机交互中的一个关键领域。为了有效解决多模态融合的问题，本文提出了一种名为多粒度跨模态对齐（MGCMA）的框架，该框架包括分布对齐、实例对齐和令牌对齐三个模块，能够在多个层次上感知跨模态的情感信息。在IEMOCAP数据集上的实验表明，该方法优于当前最先进的技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态情感识别（MER）已成为人与计算机交互中的关键领域，需要先进的多模态融合方法。</li>
<li>现有方法大多采用单一的模态对齐策略，存在局限性，难以处理情感表达的复杂性和模糊性。</li>
<li>本文提出了多粒度跨模态对齐（MGCMA）框架，包含分布对齐、实例对齐和令牌对齐三个模块，以全面应对情感信息的多层次感知需求。</li>
<li>MGCMA框架通过综合不同粒度的信息，提高了模型性能和对情感表达的适应性。</li>
<li>在IEMOCAP数据集上的实验表明，MGCMA框架的方法优于当前最先进的技术。</li>
<li>此方法有望为未来的情感识别和人机交互提供新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20821">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-925933c81992edcf86e21260ffb67b40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832fcd86bad994c1e8572432a52f780b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5e603cfce5deebc035c39002ca9beaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cd7a94d7a8791b44504b5751e637bfd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Metadata-Enhanced-Speech-Emotion-Recognition-Augmented-Residual-Integration-and-Co-Attention-in-Two-Stage-Fine-Tuning"><a href="#Metadata-Enhanced-Speech-Emotion-Recognition-Augmented-Residual-Integration-and-Co-Attention-in-Two-Stage-Fine-Tuning" class="headerlink" title="Metadata-Enhanced Speech Emotion Recognition: Augmented Residual   Integration and Co-Attention in Two-Stage Fine-Tuning"></a>Metadata-Enhanced Speech Emotion Recognition: Augmented Residual   Integration and Co-Attention in Two-Stage Fine-Tuning</h2><p><strong>Authors:Zixiang Wan, Ziyue Qiu, Yiyang Liu, Wei-Qiang Zhang</strong></p>
<p>Speech Emotion Recognition (SER) involves analyzing vocal expressions to determine the emotional state of speakers, where the comprehensive and thorough utilization of audio information is paramount. Therefore, we propose a novel approach on self-supervised learning (SSL) models that employs all available auxiliary information – specifically metadata – to enhance performance. Through a two-stage fine-tuning method in multi-task learning, we introduce the Augmented Residual Integration (ARI) module, which enhances transformer layers in encoder of SSL models. The module efficiently preserves acoustic features across all different levels, thereby significantly improving the performance of metadata-related auxiliary tasks that require various levels of features. Moreover, the Co-attention module is incorporated due to its complementary nature with ARI, enabling the model to effectively utilize multidimensional information and contextual relationships from metadata-related auxiliary tasks. Under pre-trained base models and speaker-independent setup, our approach consistently surpasses state-of-the-art (SOTA) models on multiple SSL encoders for the IEMOCAP dataset. </p>
<blockquote>
<p>语音情感识别（SER）涉及分析语音表达以确定说话人的情绪状态，其中全面彻底地利用音频信息至关重要。因此，我们提出了一种基于自监督学习（SSL）模型的新方法，该方法利用所有可用的辅助信息——特别是元数据——来提高性能。通过多任务学习中的两阶段微调方法，我们引入了增强残差集成（ARI）模块，该模块增强了SSL模型的编码器中的变压器层。该模块有效地保留了所有不同级别的声学特征，从而显著提高了与元数据相关的辅助任务的性能，这些任务需要不同级别的特征。此外，由于ARI的互补性质，还结合了协同注意模块，使模型能够有效利用来自元数据相关辅助任务的多维信息和上下文关系。在预训练基础模型和独立于说话人的设置下，我们的方法在多个SSL编码器上的表现始终超过了IEMOCAP数据集的最新模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20707v1">PDF</a> accepted by ICASSP2025. \c{opyright}2025 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, in any current or future media, including reprinting&#x2F;republishing this   material for advertising or promotional purposes, creating new collective   works, for resale or redistribution to servers or lists, or reuse of any   copyrighted component</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于自监督学习（SSL）模型的语音情感识别（SER）新方法。该方法利用所有可用的辅助信息——特别是元数据——来提高性能。通过多任务学习中的两阶段微调方法，引入了增强残差集成（ARI）模块，该模块增强了SSL模型的编码器中的变压器层。此外，还引入了协同注意模块，与ARI模块互补，使模型能够有效地利用来自元数据相关辅助任务的多维信息和上下文关系。在预训练基础模型和独立于说话人的设置下，该方法在IEMOCAP数据集上的表现持续超越了最新的SSL编码器。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）通过分析语音表达来确定说话人的情绪状态，其中音频信息的全面和彻底利用至关重要。</li>
<li>提出了一种基于自监督学习（SSL）模型的新方法，利用包括元数据在内的所有可用辅助信息来提高性能。</li>
<li>引入了增强残差集成（ARI）模块，该模块增强了SSL模型的编码器中的变压器层，能够保留不同层次的声学特征。</li>
<li>通过两阶段微调方法和多任务学习，ARI模块显著提高元数据相关辅助任务的性能。</li>
<li>引入了协同注意模块，与ARI模块互补，能更有效地利用元数据相关辅助任务的多维信息和上下文关系。</li>
<li>在独立于说话人的设置和预训练基础模型下，该方法在IEMOCAP数据集上的表现超越了最新的SSL编码器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7dc9abc02175e692a828fc54637ec15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b6c9d3b77df56f988da1e4665d86f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3113693ac03c41e3e659ba6be2ca843a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f002700029ef876934903f59cdce9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b2249e4384c228829b720b28e759509.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de938097f8491e09e30e3ef78364d34.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Utilizing-Multimodal-Data-for-Edge-Case-Robust-Call-sign-Recognition-and-Understanding"><a href="#Utilizing-Multimodal-Data-for-Edge-Case-Robust-Call-sign-Recognition-and-Understanding" class="headerlink" title="Utilizing Multimodal Data for Edge Case Robust Call-sign Recognition and   Understanding"></a>Utilizing Multimodal Data for Edge Case Robust Call-sign Recognition and   Understanding</h2><p><strong>Authors:Alexander Blatt, Dietrich Klakow</strong></p>
<p>Operational machine-learning based assistant systems must be robust in a wide range of scenarios. This hold especially true for the air-traffic control (ATC) domain. The robustness of an architecture is particularly evident in edge cases, such as high word error rate (WER) transcripts resulting from noisy ATC recordings or partial transcripts due to clipped recordings. To increase the edge-case robustness of call-sign recognition and understanding (CRU), a core tasks in ATC speech processing, we propose the multimodal call-sign-command recovery model (CCR). The CCR architecture leads to an increase in the edge case performance of up to 15%. We demonstrate this on our second proposed architecture, CallSBERT. A CRU model that has less parameters, can be fine-tuned noticeably faster and is more robust during fine-tuning than the state of the art for CRU. Furthermore, we demonstrate that optimizing for edge cases leads to a significantly higher accuracy across a wide operational range. </p>
<blockquote>
<p>基于操作机器学习的辅助系统必须在各种场景中表现出强大的稳健性。这在航空交通管制（ATC）领域尤为准确。一个架构的稳健性在极端情况下尤为明显，例如由于嘈杂的ATC录音导致的高单词错误率（WER）转录或由于剪辑录音导致的部分转录。为了提高航空交通管制语音识别和理解的边缘情况稳健性（CRU是航空交通管制语音识别的核心任务之一），我们提出了多模式呼叫标志命令恢复模型（CCR）。CCR架构提高了边缘情况性能，最高可达15%。我们在第二个提出的架构CallSBERT上展示了这一点。CRU模型参数更少，微调速度明显更快，并且在微调过程中比现有的CRU技术更加稳健。此外，我们证明了针对极端情况进行优化可以在广泛的运行范围内显著提高准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在航空交通管制（ATC）领域中，基于机器学习的助理系统需要在各种场景下具备稳健性。为提高呼叫符号识别与理解（CRU）的边缘情况稳健性，提出多模态呼叫符号命令恢复模型（CCR）。CCR架构提高了边缘案例的性能，达到15%。同时，我们展示了具有更少参数、更快微调速度和更精细的稳健性的CallSBERT模型。优化边缘情况显著提高宽操作范围内的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习助理系统在航空交通管制领域需具备稳健性以应对各种场景。</li>
<li>呼叫符号识别与理解（CRU）的边缘情况稳健性对ATC语音识别至关重要。</li>
<li>提出的多模态呼叫符号命令恢复模型（CCR）提高了边缘案例的性能。</li>
<li>CCR架构在提升边缘案例性能方面效果显著，性能提升达15%。</li>
<li>CallSBERT模型具有更少参数、更快的微调速度和更高的稳健性。</li>
<li>优化边缘情况显著提高在各种操作场景下的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b76e371d3eb570011f617ff18932f20c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9df7e533aade730fe756e0a6a17814d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f040360051264a7654ea56fa9d501b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e736e9bb8c2ccd95a172d525fc426e19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44548341ab4429ab8171de8637922b39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6553faabe68ad8fc08bb1a0e383432d0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Distance-Based-Single-Channel-Target-Speech-Extraction"><a href="#Distance-Based-Single-Channel-Target-Speech-Extraction" class="headerlink" title="Distance Based Single-Channel Target Speech Extraction"></a>Distance Based Single-Channel Target Speech Extraction</h2><p><strong>Authors:Runwu Shi, Benjamin Yen, Kazuhiro Nakadai</strong></p>
<p>This paper aims to achieve single-channel target speech extraction (TSE) in enclosures by solely utilizing distance information. This is the first work that utilizes only distance cues without using speaker physiological information for single-channel TSE. Inspired by recent single-channel Distance-based separation and extraction methods, we introduce a novel model that efficiently fuses distance information with time-frequency (TF) bins for TSE. Experimental results in both single-room and multi-room scenarios demonstrate the feasibility and effectiveness of our approach. This method can also be employed to estimate the distances of different speakers in mixed speech. Online demos are available at <a target="_blank" rel="noopener" href="https://runwushi.github.io/distance-demo-page">https://runwushi.github.io/distance-demo-page</a>. </p>
<blockquote>
<p>本文旨在通过仅利用距离信息实现封闭环境中的单通道目标语音提取（TSE）。这是第一项仅使用距离线索而无需说话人生理信息进行单通道TSE的工作。受最近的基于单通道距离的分离和提取方法的启发，我们引入了一种新型模型，该模型可以有效地将距离信息与时间频率（TF）结合进行TSE。在单室和多室场景下的实验结果证明了我们的方法的可行性和有效性。此方法还可以用于估计混合语音中不同说话人的距离。在线演示请访问：<a target="_blank" rel="noopener" href="https://runwoshi.github.io/distance-demo-page%E3%80%82">https://runwoshi.github.io/distance-demo-page。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20144v1">PDF</a> 5 pages, 3 figures, accepted by ICASSP 2025</p>
<p><strong>总结</strong></p>
<p>本论文旨在实现单通道目标语音提取（TSE），仅利用距离信息在封闭环境中进行。这是首次尝试在不使用说话人生理信息的情况下，利用距离线索进行单通道TSE。受近期单通道基于距离的分离和提取方法的启发，我们引入了一种新型模型，该模型能够高效地将距离信息与时间频率（TF）仓进行融合以实现TSE。在单室和多室场景下的实验结果证明了我们的方法的可行性和有效性。此方法也可用于估算混合语音中不同说话人的距离。在线演示请访问：<a target="_blank" rel="noopener" href="https://runwoshi.github.io/distance-demo-page%E3%80%82">https://runwoshi.github.io/distance-demo-page。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>本论文实现了仅利用距离信息进行单通道目标语音提取的方法，突破了传统方法的限制。</li>
<li>首次在不使用说话人生理信息的情况下，使用距离线索进行单通道TSE。</li>
<li>提出了一种融合距离信息与时间频率仓的新型模型，以提高TSE效率。</li>
<li>在单室和多室场景下进行的实验证明了该方法的可行性和有效性。</li>
<li>该方法可应用于估算混合语音中不同说话人的距离。</li>
<li>论文提供在线演示，便于进一步了解和验证所提出的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d505b166d46977d4f188eca8049b8011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f66383f256f97ab3885e74da347f35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cf7ea8b71d4794f15f732a4422b5a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4701137f5a14d9d35cea6c59bb209fae.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation"><a href="#CrossSpeech-Cross-lingual-Speech-Synthesis-with-Decoupled-Language-and-Speaker-Generation" class="headerlink" title="CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation"></a>CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language   and Speaker Generation</h2><p><strong>Authors:Ji-Hoon Kim, Hong-Sun Yang, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Joon Son Chung</strong></p>
<p>The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin. </p>
<blockquote>
<p>本文的目标是在多种语言生成自然语音的同时保持相同的说话人身份，这一任务被称为跨语言语音合成。跨语言语音合成的一个关键挑战是语言与说话人的纠缠问题，这导致跨语言系统的质量落后于单语言系统。在本文中，我们提出了CrossSpeech++，它有效地解开了语言和说话人的信息，并大大提高了跨语言语音合成的质量。为此，我们将复杂的语音生成管道分解为两个简单的组件：语言相关生成器和说话人相关生成器。语言相关生成器产生不受特定说话人属性影响的语言变化。说话人相关生成器对表征说话人身份的声学变化进行建模。通过在不同的模块中处理每种类型的信息，我们的方法可以有效地解开语言和说话人的表示。我们使用各种度量指标进行了广泛的实验，证明CrossSpeech++在跨语言语音合成方面取得了显著的改进，大大优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20048v1">PDF</a> </p>
<p><strong>Summary</strong><br>该论文旨在解决跨语言语音合成中的语言与说话者纠缠问题，提出一种名为CrossSpeech++的新方法，该方法可有效分离语言和说话者信息，显著提高跨语言语音合成的质量。通过把复杂的语音生成管道分解成两个简单组件：语言相关生成器和说话者相关生成器，分别处理语言和说话者信息，从而实现语言与说话者的有效分离。实验证明，CrossSpeech++在跨语言语音合成方面取得了显著改进，大幅超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文旨在实现跨语言语音合成中的语言与说话者信息的有效分离。</li>
<li>提出了一种名为CrossSpeech++的新方法，将复杂的语音生成过程分解为两个简单组件来处理语言和说话者信息。</li>
<li>语言相关生成器可以产生不受特定说话人属性影响的语言变化。</li>
<li>说话者相关生成器可以模拟说话人的声学特征变化。</li>
<li>通过分离语言和说话者信息，CrossSpeech++提高了跨语言语音合成的质量。</li>
<li>实验证明，CrossSpeech++在跨语言语音合成方面取得了显著成效，大幅超越了现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a144310e8743d76fe946abe22982a1c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b27c17333f4be5fcd573abf6e8c687e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71a3e4b4f613f066ba8c895460a3ea06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-301cbe8ec42efceaaf9a0db2aa09d03b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f9d189a4c973d41d917039da036b142.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniAvatar-Taming-Lifelike-Audio-Driven-Talking-Head-Generation-with-Comprehensive-Motion-and-Lighting-Control"><a href="#UniAvatar-Taming-Lifelike-Audio-Driven-Talking-Head-Generation-with-Comprehensive-Motion-and-Lighting-Control" class="headerlink" title="UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with   Comprehensive Motion and Lighting Control"></a>UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with   Comprehensive Motion and Lighting Control</h2><p><strong>Authors:Wenzhang Sun, Xiang Li, Donglin Di, Zhuding Liang, Qiyuan Zhang, Hao Li, Wei Chen, Jianxun Cui</strong></p>
<p>Recently, animating portrait images using audio input is a popular task. Creating lifelike talking head videos requires flexible and natural movements, including facial and head dynamics, camera motion, realistic light and shadow effects. Existing methods struggle to offer comprehensive, multifaceted control over these aspects. In this work, we introduce UniAvatar, a designed method that provides extensive control over a wide range of motion and illumination conditions. Specifically, we use the FLAME model to render all motion information onto a single image, maintaining the integrity of 3D motion details while enabling fine-grained, pixel-level control. Beyond motion, this approach also allows for comprehensive global illumination control. We design independent modules to manage both 3D motion and illumination, permitting separate and combined control. Extensive experiments demonstrate that our method outperforms others in both broad-range motion control and lighting control. Additionally, to enhance the diversity of motion and environmental contexts in current datasets, we collect and plan to publicly release two datasets, DH-FaceDrasMvVid-100 and DH-FaceReliVid-200, which capture significant head movements during speech and various lighting scenarios. </p>
<blockquote>
<p>最近，使用音频输入来生成动画肖像图像是一项热门任务。创建逼真的谈话头部视频需要灵活和自然的动作，包括面部和头部动态、相机运动、逼真的光影效果。现有方法很难在这些方面提供全面、多元的控制。在这项工作中，我们引入了UniAvatar方法，该方法提供了对各种运动和照明条件的广泛控制。具体来说，我们使用FLAME模型将所有运动信息渲染到单个图像上，保持3D运动细节的完整性，同时实现精细的像素级控制。除了运动之外，这种方法还允许进行全面的全局照明控制。我们设计独立的模块来管理3D运动和照明，以实现单独和组合控制。大量实验表明，我们的方法在广泛的运动控制和照明控制方面都优于其他方法。此外，为了提高当前数据集中运动和环境上下文的多样性，我们收集并计划公开发布两个数据集DH-FaceDrasMvVid-100和DH-FaceReliVid-200，它们捕捉了演讲过程中的重要头部运动以及各种照明场景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19860v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>近期，使用音频输入制作动态肖像图像是一项热门任务。创建逼真的谈话头视频需要灵活自然的动作，包括面部和头部动态、相机运动、真实的光影效果。现有方法难以全面控制这些方面。在这项工作中，我们推出了UniAvatar方法，提供广泛的动作和照明条件下的全面控制。具体来说，我们使用FLAME模型将所有动作信息渲染到一张图像上，保持3D动作细节的完整性，同时实现精细的像素级控制。除了动作控制外，该方法还允许全面的全局照明控制。我们设计独立的模块来管理3D动作和照明，允许单独和组合控制。大量实验证明，我们的方法在广泛的动作控制和照明控制方面都优于其他方法。此外，为了增强当前数据集的动作和环境上下文多样性，我们收集和计划公开两个数据集DH-FaceDrasMvVid-100和DH-FaceReliVid-200，它们捕捉了演讲过程中的重大头部动作和各种照明场景。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>使用音频输入创建动态肖像图像是热门任务，需实现灵活自然的动作和光影效果。</li>
<li>现有方法在动作和照明控制方面存在挑战。</li>
<li>UniAvatar方法提供广泛的动作和照明条件下的全面控制。</li>
<li>使用FLAME模型实现3D动作细节的完整性和精细的像素级控制。</li>
<li>该方法允许独立的动作和照明控制模块。</li>
<li>实验证明UniAvatar方法在动作和照明控制方面优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce33f782831d00d41d6af0a841793817.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c7b5f4400ea613c7ec09009b5313fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb230f76e9ab08e2df9ed9f1277e8ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bf6cd5153f33d83b4b5ac3ab45b6ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82df05efdeddf0ac4de28269cfc77d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb380341b10903c1c07db2aa096186aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Whisper’s-Accuracy-and-Speed-for-Indian-Languages-through-Prompt-Tuning-and-Tokenization"><a href="#Enhancing-Whisper’s-Accuracy-and-Speed-for-Indian-Languages-through-Prompt-Tuning-and-Tokenization" class="headerlink" title="Enhancing Whisper’s Accuracy and Speed for Indian Languages through   Prompt-Tuning and Tokenization"></a>Enhancing Whisper’s Accuracy and Speed for Indian Languages through   Prompt-Tuning and Tokenization</h2><p><strong>Authors:Kumud Tripathi, Raj Gothi, Pankaj Wasnik</strong></p>
<p>Automatic speech recognition has recently seen a significant advancement with large foundational models such as Whisper. However, these models often struggle to perform well in low-resource languages, such as Indian languages. This paper explores two novel approaches to enhance Whisper’s multilingual speech recognition performance in Indian languages. First, we propose prompt-tuning with language family information, which enhances Whisper’s accuracy in linguistically similar languages. Second, we introduce a novel tokenizer that reduces the number of generated tokens, thereby accelerating Whisper’s inference speed. Our extensive experiments demonstrate that the tokenizer significantly reduces inference time, while prompt-tuning enhances accuracy across various Whisper model sizes, including Small, Medium, and Large. Together, these techniques achieve a balance between optimal WER and inference speed. </p>
<blockquote>
<p>自动语音识别技术近期凭借大型基础模型（如whisper）取得了显著进展。然而，这些模型往往在低资源语言（如印度语言）中的表现不尽如人意。本文探索了两种新方法，旨在提高whisper在多语言语音识别中的印度语言性能。首先，我们提出使用带有语言家族信息的提示调整（prompt-tuning）方法，以提高在相似语言中的准确性。其次，我们引入了一种新型的分词器，可以减少生成的令牌数量，从而加快whisper的推理速度。我们的广泛实验表明，分词器可以大大减少推理时间，而提示调整则提高了各种whisper模型大小的准确性，包括小型、中型和大型。这些技术共同实现了最优的词错误率（WER）和推理速度的平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19785v1">PDF</a> Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>本文探讨了增强Whisper在多语种语音识别性能方面的两大策略。首先提出利用语言家族信息进行提示调整，提高在相似语言上的准确性。其次，引入了一种新颖的令牌化器，减少了生成的令牌数量，从而加快了Whisper的推理速度。实验表明，令牌化器显著减少了推理时间，提示调整则提高了不同规模的Whisper模型的准确性。结合使用这两种技术，实现了最佳字词错误率和推理速度的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了增强Whisper在多语种语音识别方面的两大策略。</li>
<li>利用语言家族信息进行提示调整以提高类似语言识别的准确性。</li>
<li>提出了一种新颖的令牌化器来加快Whisper的推理速度并减少生成的令牌数量。</li>
<li>实验结果显示，令牌化器显著减少了推理时间。</li>
<li>提示调整提高了不同规模的Whisper模型的性能。</li>
<li>结合使用这两种技术实现了字词错误率和推理速度的平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-42fab726694b093c31720db9cbd45471.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b65d5699af93873f1eda9afa14d5ba6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-052a267d290654316e5ee0e9acddf704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e9bac2067c4a9b9888253d06f4c766e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3a2f83ea463629080a327744b321cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5c1875a901c452c197147dcd95ec2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7e539cda2cea3567158089c6c9c695.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-a-Single-ASR-Model-That-Generalizes-to-Disordered-Speech"><a href="#Towards-a-Single-ASR-Model-That-Generalizes-to-Disordered-Speech" class="headerlink" title="Towards a Single ASR Model That Generalizes to Disordered Speech"></a>Towards a Single ASR Model That Generalizes to Disordered Speech</h2><p><strong>Authors:Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan</strong></p>
<p>This study investigates the impact of integrating a dataset of disordered speech recordings ($\sim$1,000 hours) into the fine-tuning of a near state-of-the-art ASR baseline system. Contrary to what one might expect, despite the data being less than 1% of the training data of the ASR system, we find a considerable improvement in disordered speech recognition accuracy. Specifically, we observe a 33% improvement on prompted speech, and a 26% improvement on a newly gathered spontaneous, conversational dataset of disordered speech. Importantly, there is no significant performance decline on standard speech recognition benchmarks. Further, we observe that the proposed tuning strategy helps close the gap between the baseline system and personalized models by 64% highlighting the significant progress as well as the room for improvement. Given the substantial benefits of our findings, this experiment suggests that from a fairness perspective, incorporating a small fraction of high quality disordered speech data in a training recipe is an easy step that could be done to make speech technology more accessible for users with speech disabilities. </p>
<blockquote>
<p>本研究探讨了将包含约1000小时无序语音记录的语音数据集整合到接近最新水平的ASR基线系统的微调中所产生的影响。与人们可能会预期的不同，尽管这些数据不到ASR系统训练数据的1%，但我们发现无序语音识别的准确度有了显著的提高。具体来说，我们在提示性语音上观察到提升了33%，并在新收集的无序语音的自发言论数据集中提升了26%。重要的是，在标准语音识别基准测试中，性能并没有出现显著下降。此外，我们还发现，所提出的调整策略有助于将基线系统与个性化模型之间的差距缩小64%，这既突出了显著的进步，也表明了仍有改进的空间。鉴于我们的发现所带来的巨大益处，本实验表明，从公平性的角度来看，在训练配方中加入一小部分高质量的无序语音数据是一个简单的步骤，可以使语音技术更易于供有言语障碍的用户使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19315v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本研究探讨了将包含约1000小时的无序语音数据集融入先进的语音识别系统微调环节的影响。尽管这些数据不到训练数据的百分之一，但发现无序语音识别准确率显著提高，其中提示性语音提高了33%，新收集的随意对话数据集提高了26%，且在标准语音识别基准测试中性能没有明显下降。此外，该策略有助于缩小基准系统与个性化模型之间的差距，提高了显著进步的空间。因此，从公平性的角度考虑，在训练中加入一小部分高质量的无序语音数据，有助于让语音技术对有语音障碍的用户更加友好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入包含约1000小时无序语音的数据集进行ASR系统微调。</li>
<li>即使数据只占一小部分，无序语音识别的准确率也有显著提高。</li>
<li>提示性语音和随意对话数据集的识别准确率分别提高了33%和26%。</li>
<li>在标准语音识别基准测试中未见性能下降。</li>
<li>该策略有助于缩小基准系统与个性化模型之间的差距。</li>
<li>加入高质量的无序语音数据有助于提高语音技术的公平性和可及性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d8058bb87b1f361e8854aba26cb303c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e7db2dba422d3185d6be440356f4c87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ade4499580f53a5f03a42c2241805a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff841c6fc951fed0c156bd47f1019af4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c8cfd88dc016ba1bcd2a72b66281e76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-232a505d073965473d0fae2bbb637d83.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Causal-Speech-Enhancement-with-Predicting-Semantics-based-on-Quantized-Self-supervised-Learning-Features"><a href="#Causal-Speech-Enhancement-with-Predicting-Semantics-based-on-Quantized-Self-supervised-Learning-Features" class="headerlink" title="Causal Speech Enhancement with Predicting Semantics based on Quantized   Self-supervised Learning Features"></a>Causal Speech Enhancement with Predicting Semantics based on Quantized   Self-supervised Learning Features</h2><p><strong>Authors:Emiru Tsunoo, Yuki Saito, Wataru Nakata, Hiroshi Saruwatari</strong></p>
<p>Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE. </p>
<blockquote>
<p>实时语音增强（SE）对于在线语音通信至关重要。因果SE模型在预测未来信息时只使用先前的上下文，例如语音延续，这可能有助于进行因果SE。语音信息通常通过量化自监督学习（SSL）模型的潜在特征来表示。这项工作首次将SSL特征与因果性融入SE模型。因果SSL特征通过特征级线性调制与频谱特征编码相结合，以估计用于增强带噪输入语音的掩模。同时，我们使用向量量化来量化因果SSL特征，将其表示为表示语音特征的语义令牌。该模型不仅编码SSL特征，而且还在多任务学习（MTL）中预测未来的语义令牌。使用VoiceBank + DEMAND数据集进行的实验结果表明，我们提出的方法在PESQ上达到了2.88，特别是在语义预测MTL中，我们证实语义预测在因果SE中发挥了重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19248v1">PDF</a> Accepted for ICASSP 2025, 5 pages</p>
<p><strong>Summary</strong></p>
<p>实时语音增强（SE）对在线语音通信至关重要。因果SE模型仅使用过去的信息进行预测，而利用语音延续等未来信息可能有助于进行因果SE。本研究首次将自监督学习（SSL）模型的因果特性与语音增强模型结合。利用特征级线性调制将因果SSL特征与频谱图特征编码并合并，以估计用于增强噪声输入语音的掩膜。同时，使用向量量化对因果SSL特征进行量化，以语义令牌的形式表示语音特征。模型不仅编码SSL特征，还在多任务学习中预测未来的语义令牌。使用VoiceBank + DEMAND数据集的实验结果表明，我们提出的方法在PESQ上达到2.88，特别是通过语义预测的多任务学习，我们证实了语义预测在因果SE中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实时语音增强（SE）在在线语音通信中至关重要。</li>
<li>因果SE模型通过使用未来的语音信息（如语音延续）来提高性能。</li>
<li>本研究结合了自监督学习（SSL）模型的因果特性与语音增强模型。</li>
<li>通过特征级线性调制，将因果SSL特征与频谱图特征结合，以估计语音掩膜。</li>
<li>向量量化用于量化因果SSL特征，表示为语义令牌。</li>
<li>模型在编码SSL特征的同时，还在多任务学习中预测未来的语义令牌。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19248">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cd796e1622d3abd9e3056859fd643f5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c247892360654b629b6366357509359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be856b00cc323086059dcc7c97b618b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bba354b389206ec3a0c7f0570999651.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6b663cfad71c97a84d36efb97bb505.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="“I’ve-Heard-of-You-”-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities"><a href="#“I’ve-Heard-of-You-”-Generate-Spoken-Named-Entity-Recognition-Data-for-Unseen-Entities" class="headerlink" title="“I’ve Heard of You!”: Generate Spoken Named Entity Recognition Data for   Unseen Entities"></a>“I’ve Heard of You!”: Generate Spoken Named Entity Recognition Data for   Unseen Entities</h2><p><strong>Authors:Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su</strong></p>
<p>Spoken named entity recognition (NER) aims to identify named entities from speech, playing an important role in speech processing. New named entities appear every day, however, annotating their Spoken NER data is costly. In this paper, we demonstrate that existing Spoken NER systems perform poorly when dealing with previously unseen named entities. To tackle this challenge, we propose a method for generating Spoken NER data based on a named entity dictionary (NED) to reduce costs. Specifically, we first use a large language model (LLM) to generate sentences from the sampled named entities and then use a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce a noise metric to filter out noisy data. To evaluate our approach, we release a novel Spoken NER benchmark along with a corresponding NED containing 8,853 entities. Experiment results show that our method achieves state-of-the-art (SOTA) performance in the in-domain, zero-shot domain adaptation, and fully zero-shot settings. Our data will be available at <a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU">https://github.com/DeepLearnXMU/HeardU</a>. </p>
<blockquote>
<p>语音命名实体识别（NER）旨在从语音中识别命名实体，在语音处理中发挥着重要作用。每天都有新的命名实体出现，然而，对它们的语音NER数据进行标注的成本很高。在本文中，我们证明了现有的语音NER系统在处理之前未见过的命名实体时表现不佳。为了应对这一挑战，我们提出了一种基于命名实体词典（NED）生成语音NER数据的方法，以降低标注成本。具体来说，我们首先使用大型语言模型（LLM）从采样的命名实体生成句子，然后使用文本到语音（TTS）系统生成语音。此外，我们还引入了一个噪声度量来过滤掉噪声数据。为了评估我们的方法，我们发布了一个新的语音NER基准测试以及一个包含8853个实体的相应NED。实验结果表明，我们的方法在域内、零样本域适应和完全零样本设置中都达到了最新技术水平。我们的数据将在<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepLearnXMU/HeardU上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19102v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>语音命名实体识别（NER）旨在从语音中识别命名实体，在语音识别处理中占据重要地位。现有Spoken NER系统对于未见过的命名实体表现不佳，标注Spoken NER数据成本高昂。为解决此问题，本文提出一种基于命名实体词典（NED）生成Spoken NER数据的方法降低成本。首先使用大型语言模型（LLM）从采样命名实体生成句子，再使用文本到语音（TTS）系统生成语音。此外，引入噪声度量以过滤噪声数据。本文发布新型Spoken NER基准和包含8,853个实体的对应NED。实验结果证明该方法在领域内、零样本域适应和完全零样本设置下均达到最新技术水平。相关数据将在<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/HeardU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepLearnXMU/HeardU上提供。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spoken NER旨在从语音中识别命名实体，对于新出现的命名实体，现有系统表现不佳。</li>
<li>标注Spoken NER数据的成本较高，需要寻找降低成本的方法。</li>
<li>本文提出一种基于命名实体词典（NED）生成Spoken NER数据的方法，使用大型语言模型（LLM）和文本到语音（TTS）系统。</li>
<li>引入噪声度量以过滤生成的噪声数据。</li>
<li>发布了新型的Spoken NER基准和对应的包含大量实体的NED。</li>
<li>实验结果表明该方法在多个设置下达到最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b86e890486171362a149be8408a2e3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5aa85d1b8ed0df6adba19264fc7d5a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33eef7680a3f22388849094bd1a0991b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-179f247ff97294ecfdc487eec3257cc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd1ba9e961a6bbb71566fea9c46a06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515c695c189641e0236cc39dcb4c07fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BSDB-Net-Band-Split-Dual-Branch-Network-with-Selective-State-Spaces-Mechanism-for-Monaural-Speech-Enhancement"><a href="#BSDB-Net-Band-Split-Dual-Branch-Network-with-Selective-State-Spaces-Mechanism-for-Monaural-Speech-Enhancement" class="headerlink" title="BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces   Mechanism for Monaural Speech Enhancement"></a>BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces   Mechanism for Monaural Speech Enhancement</h2><p><strong>Authors:Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv</strong></p>
<p>Although the complex spectrum-based speech enhancement(SE) methods have achieved significant performance, coupling amplitude and phase can lead to a compensation effect, where amplitude information is sacrificed to compensate for the phase that is harmful to SE. In addition, to further improve the performance of SE, many modules are stacked onto SE, resulting in increased model complexity that limits the application of SE. To address these problems, we proposed a dual-path network based on compressed frequency using Mamba. First, we extract amplitude and phase information through parallel dual branches. This approach leverages structured complex spectra to implicitly capture phase information and solves the compensation effect by decoupling amplitude and phase, and the network incorporates an interaction module to suppress unnecessary parts and recover missing components from the other branch. Second, to reduce network complexity, the network introduces a band-split strategy to compress the frequency dimension. To further reduce complexity while maintaining good performance, we designed a Mamba-based module that models the time and frequency dimensions under linear complexity. Finally, compared to baselines, our model achieves an average 8.3 times reduction in computational complexity while maintaining superior performance. Furthermore, it achieves a 25 times reduction in complexity compared to transformer-based models. </p>
<blockquote>
<p>虽然基于复杂频谱的语音增强（SE）方法已经取得了显著的性能，但幅度和相位之间的耦合会导致补偿效应，即牺牲幅度信息来补偿对语音增强有害的相位。此外，为了进一步提高语音增强的性能，许多模块被堆叠在语音增强上，导致模型复杂度增加，限制了语音增强的应用。为了解决这些问题，我们提出了一种基于压缩频率的双路径网络。首先，我们通过并行双分支提取幅度和相位信息。这种方法利用结构化复杂光谱来隐含地捕获相位信息，并通过解耦幅度和相位来解决补偿效应。网络还包含一个交互模块，用于抑制不必要的部分并从另一个分支恢复丢失的组件。其次，为了降低网络复杂度，网络引入了一种分频策略来压缩频率维度。为了保持性能的同时进一步降低复杂度，我们设计了一个基于Mamba的模块，该模块在线性复杂度下对时间和频率维度进行建模。最后，与基线相比，我们的模型在计算复杂度上实现了平均8.3倍的降低，同时保持了优越的性能。此外，与基于变压器的模型相比，它实现了高达25倍的复杂度降低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19099v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>复杂谱基语音增强方法虽取得显著性能，但幅度与相位耦合会产生补偿效应，牺牲幅度信息来补偿对语音增强有害的相位。为改善语音增强性能，在语音增强上叠加了许多模块，导致模型复杂度增加。我们提出基于压缩频率的双路径网络，通过并行双分支提取幅度和相位信息，利用结构复杂谱隐式捕获相位信息，并通过解耦幅度和相位来解决补偿效应。网络引入交互模块以抑制不必要的部分并从另一分支恢复缺失的组件。为降低网络复杂度，引入频带分割策略来压缩频率维度。为进一步降低复杂度并保持良好性能，设计基于Mamba的模块，在线性复杂度下对时间和频率维度进行建模。与基线相比，我们的模型在计算复杂度上平均降低了8.3倍，同时保持优越性能，与基于变压器的模型相比，复杂度降低了25倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>幅度与相位耦合可能导致补偿效应，牺牲幅度信息来补偿相位。</li>
<li>为提高语音增强性能，许多模块被叠加在SE上，导致模型复杂度增加。</li>
<li>提出基于压缩频率的双路径网络来解决幅度和相位耦合问题。</li>
<li>网络利用结构复杂谱隐式捕获相位信息，并通过解耦幅度和相位来解决补偿效应。</li>
<li>网络引入交互模块以优化性能。</li>
<li>为降低网络复杂度，采用频带分割策略和基于Mamba的模块。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19099">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-60f903e42ce7100485730769a8cb1f33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3aca892af3e125201bb2e393e2d22a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79799ea3362b2b10d70bbe3207a1732f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5559c4ec8ddf095d7cad444973e7fb3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fea64496a9df7d9af93cd846bada43c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e5dfc344d4009a6d436210f28f6f5f1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Attacking-Voice-Anonymization-Systems-with-Augmented-Feature-and-Speaker-Identity-Difference"><a href="#Attacking-Voice-Anonymization-Systems-with-Augmented-Feature-and-Speaker-Identity-Difference" class="headerlink" title="Attacking Voice Anonymization Systems with Augmented Feature and Speaker   Identity Difference"></a>Attacking Voice Anonymization Systems with Augmented Feature and Speaker   Identity Difference</h2><p><strong>Authors:Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan</strong></p>
<p>This study focuses on the First VoicePrivacy Attacker Challenge within the ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker verification systems capable of determining whether two anonymized speech signals are from the same speaker. However, differences between feature distributions of original and anonymized speech complicate this task. To address this challenge, we propose an attacker system that combines Data Augmentation enhanced feature representation and Speaker Identity Difference enhanced classifier to improve verification performance, termed DA-SID. Specifically, data augmentation strategies (i.e., data fusion and SpecAugment) are utilized to mitigate feature distribution gaps, while probabilistic linear discriminant analysis (PLDA) is employed to further enhance speaker identity difference. Our system significantly outperforms the baseline, demonstrating exceptional effectiveness and robustness against various voice anonymization systems, ultimately securing a top-5 ranking in the challenge. </p>
<blockquote>
<p>本研究重点关注ICASSP 2025信号处理大赛中的首个语音隐私攻击挑战，旨在开发能够判断两个匿名语音信号是否来自同一说话人的说话人验证系统。然而，原始语音和匿名语音特征分布之间的差异使这一任务复杂化。为了应对这一挑战，我们提出了一种攻击者系统，该系统结合了数据增强增强特征表示和说话人身份差异增强分类器来提高验证性能，被称为DA-SID。具体来说，数据增强策略（即数据融合和SpecAugment）被用来缓解特征分布差距，而概率线性判别分析（PLDA）被用来进一步提高说话人的身份差异识别。我们的系统在性能上显著超越了基线系统，展现出在各种语音匿名化系统中的卓越有效性和稳健性，最终在该挑战中取得了前五的排名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19068v1">PDF</a> 2 pages, submitted to ICASSP 2025 GC-7: The First VoicePrivacy   Attacker Challenge (by invitation)</p>
<p><strong>Summary</strong></p>
<p>本研究关注ICASSP 2025信号处理大赛中的First VoicePrivacy攻击者挑战，旨在开发能够判断两个匿名语音信号是否来自同一发言人的说话人验证系统。为解决原始语音与匿名化语音特征分布差异带来的挑战，研究团队提出一种结合数据增强增强特征表示和说话人身份差异增强分类器的攻击者系统，称为DA-SID。通过使用数据融合和SpecAugment等数据增强策略，缩小特征分布差距，同时采用概率线性判别分析（PLDA）进一步强化说话人身份差异。该系统显著优于基线，对各种语音匿名化系统表现出极高的有效性和鲁棒性，最终在挑战中位列前五。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注ICASSP 2025的VoicePrivacy攻击者挑战，主要目标是开发高效的说话人验证系统。</li>
<li>匿名化语音和原始语音的特征分布存在差异，给验证带来困难。</li>
<li>提出结合数据增强和说话人身份差异增强的系统（DA-SID）以改善验证性能。</li>
<li>数据融合和SpecAugment等数据增强策略用于缩小特征分布差距。</li>
<li>概率线性判别分析（PLDA）用于增强说话人身份差异的识别。</li>
<li>系统显著优于基线，对各种语音匿名化系统具有鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-113bd748c918ca055fa8d85977c5a15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-679d03fe7e5a383906de9acd98013692.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Audiovisual-Speech-Recognition-through-Bifocal-Preference-Optimization"><a href="#Enhancing-Audiovisual-Speech-Recognition-through-Bifocal-Preference-Optimization" class="headerlink" title="Enhancing Audiovisual Speech Recognition through Bifocal Preference   Optimization"></a>Enhancing Audiovisual Speech Recognition through Bifocal Preference   Optimization</h2><p><strong>Authors:Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe</strong></p>
<p>Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech recognition accuracy by leveraging visual signals. It is particularly challenging in unconstrained real-world scenarios across various domains due to noisy acoustic environments, spontaneous speech, and the uncertain use of visual information. Most previous works fine-tune audio-only ASR models on audiovisual datasets, optimizing them for conventional ASR objectives. However, they often neglect visual features and common errors in unconstrained video scenarios. In this paper, we propose using a preference optimization strategy to improve speech recognition accuracy for real-world videos. First, we create preference data via simulating common errors that occurred in AV-ASR from two focals: manipulating the audio or vision input and rewriting the output transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization method to improve AV-ASR models by leveraging both input-side and output-side preference. Extensive experiments demonstrate that our approach significantly improves speech recognition accuracy across various domains, outperforming previous state-of-the-art models on real-world video speech recognition. </p>
<blockquote>
<p>视听自动语音识别（AV-ASR）旨在利用视觉信号提高语音识别精度。在跨多个领域的无约束现实场景中，由于噪声环境、自发语音和视觉信息的不确定性，它面临着巨大的挑战。之前的大多数工作都对仅使用音频的ASR模型进行微调，使其适应传统的ASR目标。然而，他们往往忽视了视觉特征和在不受约束的视频场景中的常见错误。在本文中，我们提出了一种偏好优化策略，以提高现实世界视频的语音识别精度。首先，我们通过模拟AV-ASR中发生的常见错误来创建偏好数据，这些错误主要来自于两个方面：操纵音频或视觉输入和重写输出字幕。其次，我们提出了一种双向偏好优化方法BPO-AVASR，通过利用输入和输出偏好来提高AV-ASR模型的性能。大量实验表明，我们的方法显著提高了跨多个领域的语音识别精度，在现实世界视频语音识别方面优于最新先进模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19005v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     本文提出使用偏好优化策略，通过模拟AV-ASR中的常见错误并创建偏好数据，改进真实世界视频的语音识别准确性。通过操纵音频或视觉输入以及重写输出字幕，提出了基于输入和输出偏好的双焦点偏好优化方法（BPO-AVASR）。实验证明，该方法在不同领域的语音识别准确性上显著提高，优于现有最佳模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Audiovisual Automatic Speech Recognition (AV-ASR)利用视觉信号提高语音识别准确性。</li>
<li>在不同的领域和不受约束的现实场景中，AV-ASR面临诸多挑战，如噪声环境、自发语言和视觉信息的不确定性。</li>
<li>大多数先前的工作是对仅使用音频的ASR模型进行微调，并优化传统的ASR目标，但忽略了视觉特征和现实视频场景中的常见错误。</li>
<li>本文通过模拟AV-ASR中的常见错误创建偏好数据，包括操纵音频或视觉输入和重写输出字幕。</li>
<li>提出了一种新的方法——BPO-AVASR（双焦点偏好优化），利用输入和输出两侧的偏好来提高AV-ASR模型的性能。</li>
<li>实验表明，该方法在真实世界的视频语音识别中显著提高了准确性，优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e88ba69a61a4eddc442a30e862a1d51b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-893047d2f6bbcb8a3638234c5453b43a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6724f15b5050249781be970b8270eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07530beeaac4a476887a8a8f04b2523d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attention-Enhanced-Short-Time-Wiener-Solution-for-Acoustic-Echo-Cancellation"><a href="#Attention-Enhanced-Short-Time-Wiener-Solution-for-Acoustic-Echo-Cancellation" class="headerlink" title="Attention-Enhanced Short-Time Wiener Solution for Acoustic Echo   Cancellation"></a>Attention-Enhanced Short-Time Wiener Solution for Acoustic Echo   Cancellation</h2><p><strong>Authors:Fei Zhao, Xueliang Zhang</strong></p>
<p>Acoustic Echo Cancellation (AEC) is an essential speech signal processing technology that removes echoes from microphone inputs to facilitate natural-sounding full-duplex communication. Currently, deep learning-based AEC methods primarily focus on refining model architectures, frequently neglecting the incorporation of knowledge from traditional filter theory. This paper presents an innovative approach to AEC by introducing an attention-enhanced short-time Wiener solution. Our method strategically harnesses attention mechanisms to mitigate the impact of double-talk interference, thereby optimizing the efficiency of knowledge utilization. The derivation of the short-term Wiener solution, which adapts classical Wiener solutions to finite input causality, integrates established insights from filter theory into this method. The experimental outcomes corroborate the effectiveness of our proposed approach, surpassing other baseline models in performance and generalization. The official code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhaoF-i/ASTWS-AEC">https://github.com/ZhaoF-i/ASTWS-AEC</a> </p>
<blockquote>
<p>声学回声消除（AEC）是一项重要的语音信号处理技术和消除麦克风输入中的回声，以促进自然发声的全双工通信。目前，基于深度学习的AEC方法主要集中在改进模型架构上，往往忽略了传统滤波理论知识的融合。本文提出了一种创新的AEC方法，通过引入增强注意力短时维纳解来解决这一问题。我们的方法策略性地利用注意力机制来减轻双向通话干扰的影响，从而优化知识利用效率。短时维纳解的推导将经典的维纳解适应于有限输入因果性，将滤波理论的既定见解融入此方法。实验结果表明，我们提出的方法的有效性超越了其他基线模型在性能和泛化方面的表现。官方代码可在<a target="_blank" rel="noopener" href="https://github.com/ZhaoF-i/ASTWS-AEC">https://github.com/ZhaoF-i/ASTWS-AEC</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18851v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>本文介绍了一种基于注意力增强的短时Wiener解决方案的声学回声消除技术。该技术结合注意力机制，有效减轻双讲干扰的影响，优化知识利用效率。其将传统的Wiener解决方案进行改进，以适应有限的输入因果性，将滤波器理论的现有见解融入其中。实验结果表明，该方法在性能和泛化能力上超越了其他基线模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本研究介绍了一种基于注意力机制的声学回声消除新技术。</li>
<li>该技术结合深度学习及传统滤波器理论，通过引入注意力机制优化知识利用效率。</li>
<li>研究人员改进了经典的Wiener解决方案，以适应有限的输入因果性。</li>
<li>实验结果证明，该技术在性能上超越了其他基线模型。</li>
<li>本技术有助于实现更自然的双向通信，提升用户体验。</li>
<li>研究人员已在GitHub上公开了相关代码，便于他人参考和进一步研发。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18851">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-949f6da314b79fcce63eb504f2b0b15d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c80e52ff14c0029a0248aa13b90790ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe21003b25219e73c52475889cf765a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6fe497930df57e452c54a4b64764c2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Structured-Speaker-Deficiency-Adaptation-of-Foundation-Models-for-Dysarthric-and-Elderly-Speech-Recognition"><a href="#Structured-Speaker-Deficiency-Adaptation-of-Foundation-Models-for-Dysarthric-and-Elderly-Speech-Recognition" class="headerlink" title="Structured Speaker-Deficiency Adaptation of Foundation Models for   Dysarthric and Elderly Speech Recognition"></a>Structured Speaker-Deficiency Adaptation of Foundation Models for   Dysarthric and Elderly Speech Recognition</h2><p><strong>Authors:Shujie Hu, Xurong Xie, Mengzhe Geng, Jiajun Deng, Zengrui Jin, Tianzi Wang, Mingyu Cui, Guinan Li, Zhaoqing Li, Helen Meng, Xunying Liu</strong></p>
<p>Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and diverse dysarthric and elderly speech leads to data bias and poor generalization to unseen speakers. This paper proposes novel structured speaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data. Speaker and speech deficiency invariant SFMs were constructed in their supervised adaptive fine-tuning stage to reduce undue bias to training data speakers, and serves as a more neutral and robust starting point for test time unsupervised adaptation. Speech variability attributed to speaker identity and speech impairment severity, or aging induced neurocognitive decline, are modelled using separate adapters that can be combined together to model any seen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest structured speaker-deficiency adaptation of HuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs using either: a) no adapters; b) global adapters shared among all speakers; or c) single attribute adapters modelling speaker or deficiency labels alone by statistically significant WER reductions up to 3.01% and 1.50% absolute (10.86% and 6.94% relative) on the two tasks respectively. The lowest published WER of 19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained on the UASpeech test set of 16 dysarthric speakers. </p>
<blockquote>
<p>针对语音基础模型（SFMs）的数据密集型精细调整，在面对稀缺且多样的言语障碍者和老年语音时，会导致数据偏差，并且对新未见说话者的泛化能力较差。本文针对此类数据，提出了新型的结构化说话者缺陷适应方法，用于SSL预训练的SFMs。在监督适应性精细调整阶段，构建了说话者和语音缺陷不变的SFMs，以减少对训练数据说话人的不必要偏见，并作为测试时间无监督适应的更中性和稳健的起点。与说话者身份、语音障碍严重程度或衰老引起的神经认知下降相关的语音变化，通过使用单独的适配器进行建模，这些适配器可以组合在一起，对任何已见或未见的说话者进行建模。在UASpeech言语障碍和DementiaBank Pitt老年语音语料库上的实验表明，对HuBERT和Wav2vec2-conformer模型进行结构化说话者缺陷适应，始终优于使用以下方法的基线SFMs：a）无适配器；b）所有说话人共享的全局适配器；c）仅对说话人或缺陷标签进行建模的单属性适配器。通过显著的WER降低，绝对降低了3.01%和1.50%（相对降低了10.86%和6.94%），在两项任务上分别实现了最佳性能。在UASpeech测试集上的最低WER为19.45%（在极低清晰度上达到49.34%，在未见词语上达到33.17%），该测试集包含16名言语障碍者。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18832v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>数据密集型的语音基础模型（SFMs）对稀缺和多样化的言语障碍者和老年语音进行微调会导致数据偏见，并且难以推广到未见过的说话者。本文提出了针对SSL预训练SFM的新型结构化说话者缺陷适应方法。在监督自适应微调阶段，构建了说话者和语音缺陷不变的SFM，以减少对训练数据说话人的不必要偏见，并作为测试时间无监督适应的更中性和稳健的起点。语音变化归因于说话人身份、语音障碍严重程度或由衰老引起的神经认知下降，通过使用单独的适配器进行建模，这些适配器可以组合在一起，对任何已见或未见的说话者进行建模。在UASpeech言语障碍者和DementiaBank Pitt老年语音语料库上的实验表明，HuBERT和Wav2vec2-conformer模型的结构化说话者缺陷适应始终优于基线SFM使用：a）无适配器；b）所有说话人共享的全局适配器；c）单独建模说话人或缺陷标签的单属性适配器。通过显著的WER降低，绝对降低值最高达到3.01％和1.50％（相对降低分别为10.86％和6.94％）。在UASpeech测试集上的最低已发布WER为19.45％，涵盖了16位言语障碍者的测试集。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>数据密集型的语音基础模型（SFMs）对特定数据（如稀缺和多样化的言语障碍者和老年语音）进行微调时，易产生数据偏见，影响未见说话者的泛化能力。</li>
<li>提出新型结构化演讲者缺陷适应方法，以缓解训练数据说话人的偏见问题，并提高模型的稳健性。</li>
<li>通过在监督自适应微调阶段构建说话者和语音缺陷不变的SFM，为测试时的无监督适应提供了更中性和稳固的起点。</li>
<li>建模能够处理与说话人身份、语音障碍的严重程度以及衰老引起的神经认知下降相关的语音变化。</li>
<li>通过组合单独的适配器，模型可以适应任何已见或未见的说话者。</li>
<li>在UASpeech和DementiaBank Pitt语料库上的实验表明，结构化演讲者缺陷适应方法显著优于基线SFM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-13e567b74a8ad468d0cf3bf6844b073a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c63cea5c1f4ba43a2764cfa52be963f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2e8236d70eb02e4aaa3981e15c860ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfa131c5b07998582a88733c64002b5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0aed6e37afcc31f172f73437c6a1e550.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction"><a href="#Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction" class="headerlink" title="Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction"></a>Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction</h2><p><strong>Authors:Yuan Zhao, Rui Liu, Gaoxiang Cong</strong></p>
<p>Automatic Video Dubbing (AVD) generates speech aligned with lip motion and facial emotion from scripts. Recent research focuses on modeling multimodal context to enhance prosody expressiveness but overlooks two key issues: 1) Multiscale prosody expression attributes in the context influence the current sentence’s prosody. 2) Prosody cues in context interact with the current sentence, impacting the final prosody expressiveness. To tackle these challenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction scheme for AVD. This scheme includes two shared M2CI encoders to model the multiscale multimodal context and facilitate its deep interaction with the current sentence. By extracting global and local features for each modality in the context, utilizing attention-based mechanisms for aggregation and interaction, and employing an interaction-based graph attention network for fusion, the proposed approach enhances the prosody expressiveness of synthesized speech for the current sentence. Experiments on the Chem dataset show our model outperforms baselines in dubbing expressiveness. The code and demos are available at \textcolor[rgb]{0.93,0.0,0.47}{<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%7D">https://github.com/AI-S2-Lab/M2CI-Dubber}</a>. </p>
<blockquote>
<p>自动视频配音（AVD）根据脚本生成与唇部动作和面部情绪相匹配的语音。最近的研究集中在建立多模态上下文以提高韵律表达性，但忽略了两个关键问题：1）上下文中的多尺度韵律表达属性会影响当前句子的韵律。2）上下文中的韵律线索与当前句子相互作用，影响最终的韵律表达性。为了解决这些挑战，我们提出了M2CI-Dubber，这是一种用于AVD的多尺度多模态上下文交互方案。该方案包括两个共享的M2CI编码器，用于建立多尺度多模态上下文，并促进其与当前句子的深度交互。通过提取上下文中每种模态的全局和局部特征，利用基于注意力的机制进行聚合和交互，并采用基于交互的图注意力网络进行融合，所提出的方法提高了当前句子合成语音的韵律表达性。在Chem数据集上的实验表明，我们的模型在配音表达性方面优于基准模型。代码和演示可在<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/M2CI-Dubber上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18748v1">PDF</a> Accepted by ICSSP 2025</p>
<p><strong>Summary</strong><br>自动视频配音（AVD）技术能够根据脚本生成与唇部动作和面部表情相匹配的语音。然而，当前研究在模拟多模态语境以提升语调表达方面存在两个关键问题：一是多尺度语调表达属性在语境中的影响；二是语境中的语调线索与当前句子的互动。为了应对这些挑战，我们提出了M2CI-Dubber方案，这是一种多尺度多模态语境交互的AVD方案。该方案包含两个共享的M2CI编码器，用于模拟多尺度多模态语境，并与当前句子进行深度交互。通过提取语境中每种模态的全局和局部特征，利用基于注意力的机制进行聚合和互动，并采用基于交互的图注意力网络进行融合，提升了合成语音的语调表达力。在Chem数据集上的实验表明，我们的模型在配音表现力方面优于基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVD技术能够生成与唇部动作和面部表情匹配的语音。</li>
<li>当前研究在模拟多模态语境以提升语调表达方面存在挑战。</li>
<li>M2CI-Dubber方案旨在解决多尺度语调表达和语境中的语调线索与当前句子的互动问题。</li>
<li>M2CI-Dubber包含两个共享的M2CI编码器，用于模拟多尺度多模态语境。</li>
<li>通过提取全局和局部特征、利用基于注意力的机制以及图注意力网络，M2CI-Dubber提升了合成语音的语调表达力。</li>
<li>在Chem数据集上的实验表明，M2CI-Dubber模型在配音表现力方面优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18748">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2f8891ba94765568bb0584102e832fb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31038594f70b86676915cf3312a1d616.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb24b0ed7585afdbba1513c1215118e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa61ec535aabd0fdb085abd902e9ebf.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario"><a href="#Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario" class="headerlink" title="Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario"></a>Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario</h2><p><strong>Authors:Wen Wen, Qiang Zhou, Yu Xi, Haoyu Li, Ziqi Gong, Kai Yu</strong></p>
<p>In multi-speaker scenarios, leveraging spatial features is essential for enhancing target speech. While with limited microphone arrays, developing a compact multi-channel speech enhancement system remains challenging, especially in extremely low signal-to-noise ratio (SNR) conditions. To tackle this issue, we propose a triple-steering spatial selection method, a flexible framework that uses three steering vectors to guide enhancement and determine the enhancement range. Specifically, we introduce a causal-directed U-Net (CDUNet) model, which takes raw multi-channel speech and the desired enhancement width as inputs. This enables dynamic adjustment of steering vectors based on the target direction and fine-tuning of the enhancement region according to the angular separation between the target and interference signals. Our model with only a dual microphone array, excels in both speech quality and downstream task performance. It operates in real-time with minimal parameters, making it ideal for low-latency, on-device streaming applications. </p>
<blockquote>
<p>在多说话人场景中，利用空间特征增强目标语音至关重要。然而，在有限的麦克风阵列下，开发紧凑的多通道语音增强系统仍然具有挑战性，特别是在信号与噪声比（SNR）极低的条件下。为了解决这个问题，我们提出了一种三向引导的空间选择方法，这是一个灵活框架，使用三个引导向量来指导增强并确定增强范围。具体来说，我们引入了一个因果导向的U-Net（CDUNet）模型，该模型以原始多通道语音和所需的增强宽度为输入。这可以根据目标方向和目标信号与干扰信号之间的角度间隔动态调整引导向量并微调增强区域。我们的模型仅使用双麦克风阵列，在语音质量和下游任务性能方面都表现出色。它以实时方式运行，参数最少，非常适合低延迟的在线流式应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18141v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong>：在多说话人场景中，利用空间特征增强目标语音至关重要。针对具有有限的麦克风阵列和极低信噪比条件下的挑战，提出了一种采用三个导引向量进行增强和引导的灵活框架，并引入因果导向U-Net模型，实现动态调整导引向量和精细调整增强区域。该模型仅使用双麦克风阵列，在语音质量和下游任务性能上表现出色，适用于低延迟的在线流式应用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>在多说话人场景中，空间特征对于增强目标语音至关重要。</li>
<li>面对有限的麦克风阵列和极端低信噪比条件，设计灵活框架以进行多通道语音增强是一个挑战。</li>
<li>提出了一种采用三个导引向量的方法来确定增强范围。</li>
<li>引入了因果导向U-Net模型，能够根据目标方向和角度分离动态调整导引向量和增强区域。</li>
<li>该模型在仅使用双麦克风阵列的情况下，在语音质量和下游任务性能上表现出色。</li>
<li>模型可实现实时操作，参数少，适合低延迟的在线流式应用。</li>
<li>该方法为解决具有挑战性的语音增强问题提供了新的思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-23976b356b116997ee29214ac866dfe4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86e4be31bad6d3e8241d5204a21f28fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46340f8c64ec6158dce1f979716c2895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52c0f6feadd984d4947f6c33c8111bf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4039e0ead841fd19d578014b9991a087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2fe450e8623772df435eca211f85e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687ccc37a1d7be71ca26e2a21500e6c8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions"><a href="#DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions" class="headerlink" title="DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions"></a>DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions</h2><p><strong>Authors:Shu-Tong Niu, Jun Du, Ruo-Yu Wang, Gao-Bin Yang, Tian Gao, Jia Pan, Yu Hu</strong></p>
<p>We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end automatic speech recognition (ASR), combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art single-channel speech recognition performance. </p>
<blockquote>
<p>我们提出了一种单通道深度级联融合分治与分离（DCF-DS）框架，用于后端自动语音识别（ASR），该框架结合了神经网络分治（NSD）和语音分离（SS）。首先，我们在联合训练框架中按顺序整合NSD和SS模块，使分离模块能够有效地利用分治模块中的说话人时间边界。然后，为了补充DCF-DS训练，我们引入了一种窗口级别的解码方案，该方案允许DCF-DS框架解决稀疏数据收敛不稳定（SDCI）问题。我们还探索使用在真实数据集上训练NSD系统来提供更准确的说话人边界。此外，我们在DCF-DS框架中融入了一个可选的多输入多输出语音增强模块（MIMO-SE），这带来了进一步的性能提升。最后，我们通过重新聚类DCF-DS输出结果来提升分治结果，提高ASR的准确性。通过采用DCF-DS方法，我们在CHiME-8 NOTSOFAR-1挑战的现实中单通道赛道上取得了第一名。我们还对开放的LibriCSS数据集进行了评估，实现了最新的单通道语音识别性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06667v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种单通道深度级联融合分治与分离（DCF-DS）框架，用于后端自动语音识别（ASR）。该框架结合了神经网络说话人分治（NSD）和语音分离（SS）。首先，在一个联合训练框架内按顺序整合NSD和SS模块，使分离模块能够有效地利用分治模块提供的说话人时间边界。其次，为了补充DCF-DS训练，引入了一种窗口级解码方案，以解决稀疏数据收敛不稳定（SDCI）问题。此外，还探索了使用在真实数据集上训练的NSD系统来提供更准确的说话人边界。同时，在DCF-DS框架中融入了一个可选的多输入多输出语音增强模块（MIMO-SE），进一步提升了性能。最后，通过对DCF-DS输出的重新聚类，优化了分治结果，提高了ASR的准确性。该框架在CHiME-8 NOTSOFAR-1挑战的现实单通道赛道中荣获第一名，并在开放的LibriCSS数据集上取得了最新的单通道语音识别性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了单通道Deep Cascade Fusion of Diarization and Separation（DCF-DS）框架，结合了神经网络说话人分治（NSD）和语音分离（SS）。</li>
<li>通过联合训练NSD和SS模块，使语音分离模块能够利用说话人时间边界信息。</li>
<li>引入窗口级解码方案，解决稀疏数据收敛不稳定问题。</li>
<li>使用真实数据集训练的NSD系统提供更准确的说话人边界。</li>
<li>可选融入多输入多输出语音增强模块（MIMO-SE），进一步提升性能。</li>
<li>通过重新聚类DCF-DS输出，优化分治结果，提高ASR准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.06667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1561da96892171e73ece2cfd77ae8d85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a607f50797c02cdeadb60ff81187b3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17eacfcd52ab774b92fd352740262db9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28209e53ddd34d65028a5b1c616c21aa.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Mamba-for-Streaming-ASR-Combined-with-Unimodal-Aggregation"><a href="#Mamba-for-Streaming-ASR-Combined-with-Unimodal-Aggregation" class="headerlink" title="Mamba for Streaming ASR Combined with Unimodal Aggregation"></a>Mamba for Streaming ASR Combined with Unimodal Aggregation</h2><p><strong>Authors:Ying Fang, Xiaofei Li</strong></p>
<p>This paper works on streaming automatic speech recognition (ASR). Mamba, a recently proposed state space model, has demonstrated the ability to match or surpass Transformers in various tasks while benefiting from a linear complexity advantage. We explore the efficiency of Mamba encoder for streaming ASR and propose an associated lookahead mechanism for leveraging controllable future information. Additionally, a streaming-style unimodal aggregation (UMA) method is implemented, which automatically detects token activity and streamingly triggers token output, and meanwhile aggregates feature frames for better learning token representation. Based on UMA, an early termination (ET) method is proposed to further reduce recognition latency. Experiments conducted on two Mandarin Chinese datasets demonstrate that the proposed model achieves competitive ASR performance in terms of both recognition accuracy and latency. </p>
<blockquote>
<p>本文研究流式自动语音识别（ASR）。Mamba是一种最近提出的状态空间模型，在各种任务中展现出了与Transformer相匹配或更高的性能，同时得益于其线性复杂度的优势。我们探索了Mamba编码器在流式ASR中的效率，并提出了一种相关的前瞻机制，以利用可控的未来信息。此外，还实现了一种流式单模态聚合（UMA）方法，该方法可自动检测标记活动并流式触发标记输出，同时聚合特征帧以更好地学习标记表示。基于UMA，提出了一种早期终止（ET）方法，以进一步降低识别延迟。在两种中文数据集上的实验表明，该模型在识别准确性和延迟方面都取得了具有竞争力的ASR性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00070v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文研究了基于流媒体的自动语音识别（ASR）。Mamba作为一种新近提出的状态空间模型，在各种任务中展现出与Transformer相匹配或更佳的性能，并且具有线性复杂度的优势。本文探索了Mamba编码器在流式ASR中的效率，并提出了一种相关的前瞻机制，以利用可控的未来信息。此外，实现了一种流式单模态聚合（UMA）方法，该方法可自动检测令牌活动并流式触发令牌输出，同时聚合特征帧以更好地学习令牌表示。基于UMA，进一步提出了早期终止（ET）方法来减少识别延迟。在两个中文数据集上的实验表明，该模型在识别准确性和延迟方面均表现出竞争力的ASR性能。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Mamba模型在流式ASR中展现出高效性能，可匹配或超越Transformer模型，并具备线性复杂度优势。</li>
<li>提出了利用Mamba编码器的流式ASR前瞻机制，以利用可控的未来信息。</li>
<li>实现了流式单模态聚合（UMA）方法，能自动检测令牌活动并触发流式令牌输出，同时聚合特征帧以优化学习。</li>
<li>基于UMA方法，提出了早期终止（ET）策略来进一步降低识别延迟。</li>
<li>在两个中文数据集上的实验验证了该模型在ASR性能和识别准确性方面的竞争力。</li>
<li>Mamba模型在ASR任务中具有广泛的应用前景，特别是在需要高效率和低延迟的流式处理场景中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-88de93fb2c3556b0051ceccac09bd460.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d08199b60b416658557070fbcad3c60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea353adc4c5919e31d59e2c1d766350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f46fb661c0f69e98ac382a39cb86b760.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="A-Modular-based-Strategy-for-Mitigating-Gradient-Conflicts-in-Simultaneous-Speech-Translation"><a href="#A-Modular-based-Strategy-for-Mitigating-Gradient-Conflicts-in-Simultaneous-Speech-Translation" class="headerlink" title="A Modular-based Strategy for Mitigating Gradient Conflicts in   Simultaneous Speech Translation"></a>A Modular-based Strategy for Mitigating Gradient Conflicts in   Simultaneous Speech Translation</h2><p><strong>Authors:Xiaoqian Liu, Yangfan Du, Jianjin Wang, Yuan Ge, Chen Xu, Tong Xiao, Guocheng Chen, Jingbo Zhu</strong></p>
<p>Simultaneous Speech Translation (SimulST) involves generating target language text while continuously processing streaming speech input, presenting significant real-time challenges. Multi-task learning is often employed to enhance SimulST performance but introduces optimization conflicts between primary and auxiliary tasks, potentially compromising overall efficiency. The existing model-level conflict resolution methods are not well-suited for this task which exacerbates inefficiencies and leads to high GPU memory consumption. To address these challenges, we propose a Modular Gradient Conflict Mitigation (MGCM) strategy that detects conflicts at a finer-grained modular level and resolves them utilizing gradient projection. Experimental results demonstrate that MGCM significantly improves SimulST performance, particularly under medium and high latency conditions, achieving a 0.68 BLEU score gain in offline tasks. Additionally, MGCM reduces GPU memory consumption by over 95% compared to other conflict mitigation methods, establishing it as a robust solution for SimulST tasks. </p>
<blockquote>
<p>同步语音识别翻译（SimulST）涉及在持续处理流式语音输入的同时生成目标语言文本，这带来了显著的真实时间挑战。多任务学习通常被用来提高SimulST的性能，但会在主要任务和辅助任务之间引入优化冲突，可能损害整体效率。现有的模型级冲突解决方法并不适合这项任务，这加剧了效率问题并导致GPU内存消耗过高。为了应对这些挑战，我们提出了一种模块梯度冲突缓解（MGCM）策略，它在更精细的模块级别检测并解决冲突，利用梯度投影来实现。实验结果表明，MGCM能显著提高SimulST的性能，特别是在中等和高的延迟条件下，离线任务中BLEU得分提高了0.68。此外，与其他冲突缓解方法相比，MGCM将GPU内存消耗降低了超过95%，使其成为SimulST任务的稳健解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15911v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了同步语音识别翻译（SimulST）面临的挑战，包括实时处理流式语音输入时生成目标语言文本的问题。虽然多任务学习被用来提高SimulST性能，但它引入了主要任务和辅助任务之间的优化冲突，可能损害整体效率。针对这些挑战，本文提出了一种模块化梯度冲突缓解（MGCM）策略，该策略在更精细的模块化层面检测并解决冲突，利用梯度投影来实现。实验结果表明，MGCM能显著提高SimulST性能，特别是在中等和较高的延迟条件下，在离线任务中实现了0.68的BLEU得分增益。此外，与其他冲突缓解方法相比，MGCM降低了超过95%的GPU内存消耗，成为SimulST任务的稳健解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>同步语音识别翻译（SimulST）面临实时处理流式语音输入的挑战。</li>
<li>多任务学习在SimulST中引入优化冲突，可能影响效率。</li>
<li>模块化梯度冲突缓解（MGCM）策略能在更精细的模块化层面检测并解决冲突。</li>
<li>MGCM通过梯度投影实现，可提高SimulST性能。</li>
<li>实验结果显示，MGCM在离线任务中实现了显著的BLEU得分增益。</li>
<li>MGCM降低了GPU内存消耗，超过95%，比其他冲突缓解方法更有效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4b343a1576b486d4de58c19650776685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b4fc0af5683f4c2df49c5fab39af256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-168c05f51f0caa66211e84b4e1d634a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eda20af0c87a7e65ed6f40f8a571cde7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bb372937362ebe61d84b01951b971cab.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-01-02  KunServe Elastic and Efficient Large Language Model Serving with   Parameter-centric Memory Management
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-caf3ad39b704755169a9ae027e45fd58.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-02  Neighbor Does Matter Density-Aware Contrastive Learning for Medical   Semi-supervised Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
