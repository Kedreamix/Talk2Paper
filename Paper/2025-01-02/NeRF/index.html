<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-02  Bringing Objects to Life 4D generation from 3D objects">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e0d61a3ec03e4f212d16afb4fead0a57.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-02-更新"><a href="#2025-01-02-更新" class="headerlink" title="2025-01-02 更新"></a>2025-01-02 更新</h1><h2 id="Bringing-Objects-to-Life-4D-generation-from-3D-objects"><a href="#Bringing-Objects-to-Life-4D-generation-from-3D-objects" class="headerlink" title="Bringing Objects to Life: 4D generation from 3D objects"></a>Bringing Objects to Life: 4D generation from 3D objects</h2><p><strong>Authors:Ohad Rahamim, Ori Malca, Dvir Samuel, Gal Chechik</strong></p>
<p>Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry of generated content. In this work, we introduce a method for animating user-provided 3D objects by conditioning on textual prompts to guide 4D generation, enabling custom animations while maintaining the identity of the original object. We first convert a 3D mesh into a &#96;&#96;static” 4D Neural Radiance Field (NeRF) that preserves the visual attributes of the input object. Then, we animate the object using an Image-to-Video diffusion model driven by text. To improve motion realism, we introduce an incremental viewpoint selection protocol for sampling perspectives to promote lifelike movement and a masked Score Distillation Sampling (SDS) loss, which leverages attention maps to focus optimization on relevant regions. We evaluate our model in terms of temporal coherence, prompt adherence, and visual fidelity and find that our method outperforms baselines that are based on other approaches, achieving up to threefold improvements in identity preservation measured using LPIPS scores, and effectively balancing visual quality with dynamic content. </p>
<blockquote>
<p>最近生成模型领域的进展现在已能够实现使用文本提示创建4D内容（动态3D物体）。4D生成在虚拟世界、媒体和游戏等领域具有巨大潜力，但现有方法对于生成内容的外貌和几何结构的控制有限。在这项工作中，我们介绍了一种通过文本提示进行条件动画处理用户提供的3D物体的方法，以指导4D生成，实现在保持原始物体身份的同时进行自定义动画。我们首先将3D网格转换为“静态”的4D神经辐射场（NeRF），以保留输入物体的视觉属性。然后，我们使用文本驱动的图像到视频的扩散模型对物体进行动画处理。为了提高运动逼真度，我们引入了增量视角选择协议用于采样视角，以促进更逼真的运动，以及遮罩分数蒸馏采样（SDS）损失，该损失利用注意力图将优化重点放在相关区域。我们从时间连贯性、提示遵循程度和视觉保真度三个方面评估了我们的模型，发现我们的方法在其他方法的基础上表现更好，在通过LPIPS分数测量的身份保留方面实现了高达三倍的提升，并在视觉质量与动态内容之间实现了有效平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种通过文本提示驱动用户提供的3D对象动画的方法，实现了4D内容的生成。该方法先将3D网格转换为静态的4D神经辐射场（NeRF），保留输入对象的视觉属性，然后使用图像到视频的扩散模型进行动画化。为提高运动真实感，引入了增量视角选择协议和带注意力图的掩膜得分蒸馏采样（SDS）损失。实验表明，该方法在保持身份连续性、遵循提示和视觉保真度方面优于其他方法，实现了三倍的身份保留改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该技术能够创建通过文本提示控制的4D内容（动态3D对象）。</li>
<li>将3D网格转换为静态的4D神经辐射场（NeRF），保留对象的视觉属性。</li>
<li>使用图像到视频的扩散模型对对象进行动画化。</li>
<li>引入增量视角选择协议，促进更逼真的运动。</li>
<li>使用掩膜得分蒸馏采样（SDS）损失提高运动真实感，利用注意力图优化相关区域。</li>
<li>在保持身份连续性、遵循提示和视觉保真度方面优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20422">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-438771f144c153b43a01cfe97fab3ae4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e9598f4e678da3d65c4be1085d22a3c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-Radiance-Fields-from-a-Single-Snapshot-Compressive-Image"><a href="#Learning-Radiance-Fields-from-a-Single-Snapshot-Compressive-Image" class="headerlink" title="Learning Radiance Fields from a Single Snapshot Compressive Image"></a>Learning Radiance Fields from a Single Snapshot Compressive Image</h2><p><strong>Authors:Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu</strong></p>
<p>In this paper, we explore the potential of Snapshot Compressive Imaging (SCI) technique for recovering the underlying 3D scene structure from a single temporal compressed image. SCI is a cost-effective method that enables the recording of high-dimensional data, such as hyperspectral or temporal information, into a single image using low-cost 2D imaging sensors. To achieve this, a series of specially designed 2D masks are usually employed, reducing storage and transmission requirements and offering potential privacy protection. Inspired by this, we take one step further to recover the encoded 3D scene information leveraging powerful 3D scene representation capabilities of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we formulate the physical imaging process of SCI as part of the training of NeRF, allowing us to exploit its impressive performance in capturing complex scene structures. In addition, we further integrate the popular 3D Gaussian Splatting (3DGS) framework and propose SCISplat to improve 3D scene reconstruction quality and training&#x2F;rendering speed by explicitly optimizing point clouds into 3D Gaussian representations. To assess the effectiveness of our method, we conduct extensive evaluations using both synthetic data and real data captured by our SCI system. Experimental results demonstrate that our proposed approach surpasses the state-of-the-art methods in terms of image reconstruction and novel view synthesis. Moreover, our method also exhibits the ability to render high frame-rate multi-view consistent images in real time by leveraging SCI and the rendering capabilities of 3DGS. Codes will be available at: <a target="_blank" rel="noopener" href="https://github.com/WU-">https://github.com/WU-</a> CVGL&#x2F;SCISplat. </p>
<blockquote>
<p>本文探讨了快照压缩成像（SCI）技术在从单个时间压缩图像中恢复潜在的三维场景结构方面的潜力。SCI是一种经济高效的方法，它能够将高维数据（如光谱或时间信息）使用低成本的二维成像传感器记录为单个图像。为实现这一点，通常采用一系列专门设计的二维掩膜，以降低存储和传输要求并提供潜在的隐私保护。在此基础上，我们更进一步利用神经辐射场（NeRF）的强大三维场景表示能力，恢复编码的三维场景信息。具体来说，我们提出了SCINeRF，其中我们将SCI的物理成像过程作为NeRF训练的一部分，使我们能够利用其捕捉复杂场景结构的令人印象深刻的表现。此外，我们进一步集成了流行的三维高斯喷绘（3DGS）框架，并提出了SCISplat，通过显式优化点云到三维高斯表示来提高三维场景重建质量以及训练和渲染速度。为了评估我们方法的有效性，我们使用合成数据和由我们的SCI系统捕获的真实数据进行了广泛评估。实验结果表明，我们提出的方法在图像重建和新颖视图合成方面超过了最先进的方法。而且，我们的方法还展示了利用SCI和3DGS的渲染能力实时呈现高帧率多视角一致图像的能力。代码将在<a target="_blank" rel="noopener" href="https://github.com/WU-CVGL/SCISplat%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/WU-CVGL/SCISplat上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19483v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文探讨了基于快照压缩成像（SCI）技术恢复潜在三维场景结构的方法，从单张时间压缩图像中恢复三维场景信息。SCI是一种经济高效的方法，能够使用低成本的二维成像传感器记录高维数据，如超光谱或时间信息。通过设计一系列二维掩膜实现编码过程，这种方法减少了存储和传输需求，并提供潜在隐私保护。本研究在此基础上进一步采用神经网络辐射场（NeRF）强大的三维场景表示能力恢复编码的三维场景信息。具体来说，我们提出了SCINeRF方法，将SCI的物理成像过程纳入NeRF的训练过程中，利用其在捕捉复杂场景结构方面的出色性能。此外，我们整合了流行的三维高斯喷涂（3DGS）框架，提出了SCISplat方法，通过优化点云到三维高斯表示形式提高三维场景重建质量并加速训练和渲染速度。通过合成数据和由我们的SCI系统捕获的真实数据进行了广泛评估，实验结果表明我们的方法超越最先进的图像重建和新型视图合成方法。此外，我们的方法还展现了借助SCI和3DGS的渲染能力实时渲染高帧率多视角一致图像的能力。相关代码将发布在：<a target="_blank" rel="noopener" href="https://github.com/WU-CVGL/SCISplat%E3%80%82">https://github.com/WU-CVGL/SCISplat。</a></p>
<p><strong>要点摘要</strong></p>
<ol>
<li>研究利用快照压缩成像（SCI）技术从单张时间压缩图像中恢复三维场景结构。</li>
<li>SCI方法利用低成本的二维成像传感器记录高维数据。</li>
<li>提出SCINeRF方法，结合NeRF的三维场景表示能力恢复编码的三维场景信息。</li>
<li>结合三维高斯喷涂（3DGS）框架的SCISplat方法提高了三维场景重建质量和训练&#x2F;渲染速度。</li>
<li>实验结果表明，所提出的方法在图像重建和新型视图合成方面超越现有技术。</li>
<li>方法能够实时渲染高帧率多视角一致图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-078e653f005f60dd8eddb8f7e540ad8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aa43ff602cb8e162d882386d84cb03d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-975272d5740b4d803f8a988529d6283f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BeSplat-–-Gaussian-Splatting-from-a-Single-Blurry-Image-and-Event-Stream"><a href="#BeSplat-–-Gaussian-Splatting-from-a-Single-Blurry-Image-and-Event-Stream" class="headerlink" title="BeSplat – Gaussian Splatting from a Single Blurry Image and Event   Stream"></a>BeSplat – Gaussian Splatting from a Single Blurry Image and Event   Stream</h2><p><strong>Authors:Gopi Raju Matta, Reddypalli Trisha, Kaushik Mitra</strong></p>
<p>Novel view synthesis has been greatly enhanced by the development of radiance field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively addressed key challenges, such as long training times and slow rendering speeds, typically associated with Neural Radiance Fields (NeRF), while maintaining high-quality reconstructions. In this work (BeSplat), we demonstrate the recovery of sharp radiance field (Gaussian splats) from a single motion-blurred image and its corresponding event stream. Our method jointly learns the scene representation via Gaussian Splatting and recovers the camera motion through Bezier SE(3) formulation effectively, minimizing discrepancies between synthesized and real-world measurements of both blurry image and corresponding event stream. We evaluate our approach on both synthetic and real datasets, showcasing its ability to render view-consistent, sharp images from the learned radiance field and the estimated camera trajectory. To the best of our knowledge, ours is the first work to address this highly challenging ill-posed problem in a Gaussian Splatting framework with the effective incorporation of temporal information captured using the event stream. </p>
<blockquote>
<p>随着辐射场方法的发展，新型视图合成技术得到了极大的增强。3D高斯喷涂技术（3DGS）的引入有效地解决了与神经辐射场（NeRF）通常相关的主要挑战，如训练时间长和渲染速度慢，同时保持了高质量的重建。在这项工作（BeSplat）中，我们展示了从单个运动模糊图像及其相应的事件流中恢复锐利的辐射场（高斯喷涂）的能力。我们的方法通过高斯喷涂联合学习场景表示，并通过贝塞尔SE(3)公式有效恢复相机运动，从而最小化合成和真实世界测量之间的模糊图像和相应事件流的差异。我们在合成数据集和真实数据集上评估了我们的方法，展示了从学习的辐射场和估计的相机轨迹渲染出一致、清晰图像的能力。据我们所知，我们的工作是在高斯喷涂框架下解决这一极具挑战性的不适定问题的首批工作之一，有效地结合了使用事件流捕获的时间信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19370v1">PDF</a> Accepted for publication at EVGEN2025, WACV-25 Workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何通过引入3D高斯Splatting（3DGS）技术，解决神经辐射场（NeRF）长期存在的训练时间长和渲染速度慢的问题，同时保持了高质量的重构。该研究展示了从单张运动模糊图像及其对应的事件流中恢复出锐化的辐射场（高斯Splats）的能力。该方法通过高斯Splatting学习场景表示，并通过Bezier SE(3)公式有效地恢复相机运动，最小化合成图像和真实世界测量之间的模糊图像和对应事件流的差异。在合成和真实数据集上的评估证明了该方法能够从学习的辐射场和估计的相机轨迹中渲染出连贯、清晰的图像。这是首次在Gaussian Splatting框架下解决这一具有挑战性的不适定问题，并有效利用事件流捕获的暂时信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了3D高斯Splatting技术以解决NeRF存在的训练时间长和渲染速度慢的问题。</li>
<li>成功从单张运动模糊图像及其对应的事件流中恢复出锐化的辐射场。</li>
<li>通过高斯Splatting学习场景表示，并有效地恢复相机运动。</li>
<li>利用Bezier SE(3)公式最小化合成图像和真实测量之间的差异。</li>
<li>在合成和真实数据集上的评估证明了该方法的有效性。</li>
<li>该方法能够渲染出连贯、清晰的图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dab0d13209460a513c971d77a5e93e0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6935f2f426b133e610bbdd5ce6245dfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21b9e3e3453ea7e8aa8e5bc0f84b2ee4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de024f1fe0d234244b550d2c09cffce8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generating-Editable-Head-Avatars-with-3D-Gaussian-GANs"><a href="#Generating-Editable-Head-Avatars-with-3D-Gaussian-GANs" class="headerlink" title="Generating Editable Head Avatars with 3D Gaussian GANs"></a>Generating Editable Head Avatars with 3D Gaussian GANs</h2><p><strong>Authors:Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang</strong></p>
<p>Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/liguohao96/EGG3D">https://github.com/liguohao96/EGG3D</a>. </p>
<blockquote>
<p>生成可动画化和可编辑的3D头像对于计算机视觉和图形学中的各种应用至关重要。传统的基于隐式场的3D感知生成对抗网络（GANs），常常使用如神经辐射场（NeRF）等技术，实现了逼真的、视角一致的3D头像合成。然而，这些方法在变形灵活性和可编辑性方面存在局限性，阻碍了生动且易于修改的3D头像的创建。我们提出了一种结合显式三维表示法——三维高斯喷射（3DGS）的新方法，以提高三维头像的可编辑性和动画控制力。这种方法使得光照控制更加容易，可编辑性也更强。我们的方法的核心是可编辑高斯头（EG-Head）模型，它将三维可变形模型（3DMM）与纹理映射相结合，允许精确的表情控制和灵活的纹理编辑，以实现准确的动画同时保留身份特征。为了捕捉头发等非面部复杂几何结构，我们使用一组辅助的3DGS和三平面特征。大量实验证明，我们的方法实现了高质量的具有先进可控性的三维感知合成。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/liguohao96/EGG3D%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/liguohao96/EGG3D获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了生成可动画和可编辑的3D头像的重要性及其在计算机视觉和图形学中的多种应用。传统的使用NeRF等隐式场的3D GANs虽然可以实现逼真的3D头像合成，但在变形灵活性和可编辑性方面存在局限。本文提出了一种新方法，通过引入3D高斯涂抹（3DGS）作为明确的3D表示，提高了3D头像的可编辑性和动画控制。该方法使用可编辑的高斯头像（EG-Head）模型，结合3D形态模型和纹理映射，实现精确的表情控制和灵活的纹理编辑，同时保持身份识别。为了捕捉非面部几何形状（如头发），使用了辅助的3DGS和三平面特征。实验证明，该方法实现了高质量的3D感知合成，并具有较高的可控性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成可动画和可编辑的3D头像对于计算机视觉和图形学应用至关重要。</li>
<li>传统使用NeRF的3D GANs在变形灵活性和可编辑性方面存在局限。</li>
<li>引入3D高斯涂抹（3DGS）作为明确的3D表示，提高了3D头像的可编辑性和动画控制。</li>
<li>可编辑的高斯头像（EG-Head）模型结合了3D形态模型和纹理映射，实现精确的表情控制和灵活的纹理编辑。</li>
<li>为了捕捉非面部几何形状（如头发），使用了辅助的3DGS和三平面特征。</li>
<li>该方法实现了高质量的3D感知合成，并具有高度的可控性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3892a306159a58dca2515cad8e802c6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-519b6cc1bd7cf2c02053876e12ac88ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad7b562387e86a3f6562767bcf1c692.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MVS-GS-High-Quality-3D-Gaussian-Splatting-Mapping-via-Online-Multi-View-Stereo"><a href="#MVS-GS-High-Quality-3D-Gaussian-Splatting-Mapping-via-Online-Multi-View-Stereo" class="headerlink" title="MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View   Stereo"></a>MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View   Stereo</h2><p><strong>Authors:Byeonggwon Lee, Junkyu Park, Khang Truong Giang, Sungho Jo, Soohwan Song</strong></p>
<p>This study addresses the challenge of online 3D model generation for neural rendering using an RGB image stream. Previous research has tackled this issue by incorporating Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS) as scene representations within dense SLAM methods. However, most studies focus primarily on estimating coarse 3D scenes rather than achieving detailed reconstructions. Moreover, depth estimation based solely on images is often ambiguous, resulting in low-quality 3D models that lead to inaccurate renderings. To overcome these limitations, we propose a novel framework for high-quality 3DGS modeling that leverages an online multi-view stereo (MVS) approach. Our method estimates MVS depth using sequential frames from a local time window and applies comprehensive depth refinement techniques to filter out outliers, enabling accurate initialization of Gaussians in 3DGS. Furthermore, we introduce a parallelized backend module that optimizes the 3DGS model efficiently, ensuring timely updates with each new keyframe. Experimental results demonstrate that our method outperforms state-of-the-art dense SLAM methods, particularly excelling in challenging outdoor environments. </p>
<blockquote>
<p>本研究旨在应对使用RGB图像流进行神经渲染的在线3D模型生成挑战。之前的研究已经通过结合神经辐射场（NeRF）或3D高斯喷绘（3DGS）作为密集SLAM方法内的场景表示来解决这个问题。然而，大多数研究主要集中在估计粗糙的3D场景，而不是实现详细的重建。此外，仅基于图像的深度估计往往具有模糊性，导致质量较低的3D模型，从而导致渲染不准确。为了克服这些局限性，我们提出了一种基于在线多视图立体（MVS）方法的高质量的3DGS建模新框架。我们的方法使用来自局部时间窗口的连续帧来估计MVS深度，并采用全面的深度细化技术来过滤异常值，从而实现3DGS中高斯值的准确初始化。此外，我们还引入了一个并行化的后端模块，该模块可以有效地优化3DGS模型，确保每个新关键帧都能及时得到更新。实验结果表明，我们的方法在挑战性户外环境中表现优异，超越了最先进的密集SLAM方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19130v1">PDF</a> 7 pages, 6 figures, submitted to IEEE ICRA 2025</p>
<p><strong>Summary</strong><br>本文提出了一种基于在线多视角立体（MVS）方法的高质量三维高斯拼贴（3DGS）建模框架，用于解决在线三维模型生成中的神经渲染挑战。通过利用序列帧的局部时间窗口估计MVS深度，并应用全面的深度优化技术过滤异常值，实现了高精度的三维场景重建和渲染。此外，还引入了并行后端模块，优化3DGS模型的效率，确保及时更新每个新关键帧。实验结果表明，该方法优于现有密集SLAM方法，特别是在具有挑战性的室外环境中表现更优秀。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>研究背景：在线三维模型生成对于神经渲染是一个挑战，尤其是基于RGB图像流进行估计。先前的研究主要聚焦于通过引入NeRF或3DGS技术来建立场景表示，但这些研究更多地关注粗略的三维场景估计而非详细重建。</p>
</li>
<li><p>局限性的解决：针对现有研究的不足，本研究提出了一种新颖的基于在线多视角立体（MVS）方法的框架进行高质量建模。该框架通过利用局部时间窗口内的序列帧估计MVS深度，解决了深度估计的模糊性问题。</p>
</li>
<li><p>深度优化技术：通过全面的深度优化技术过滤异常值，提高了深度估计的准确性，从而实现了高精度的三维场景重建和渲染。</p>
</li>
<li><p>并行后端模块：引入并行后端模块以优化3DGS模型的效率，确保每个新关键帧都能得到及时更新。</p>
</li>
<li><p>实验结果：实验结果表明，该研究提出的方法在性能上超越了现有的密集SLAM方法。特别是在挑战性的室外环境中，其表现尤为出色。这种方法的优势在于能够生成更准确、更精细的三维模型。</p>
</li>
<li><p>方法创新点：该研究的主要创新点在于结合了在线多视角立体方法和深度优化技术，实现了高质量的三维场景重建和渲染效果。同时，引入的并行后端模块进一步提高了效率。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19130">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e3972536eeef7a870fd4b6f2e82c4eff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e26773a7c0b29e8469003d8fd2c8b0d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2488254f795e6af4e6e6e3a7288a8b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bc1c8b72af53bd6298553df981f5fac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a64f64cd37fbee41aeaab3cc2f4d545a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8636ce2cc6fb43899937bfaa1789ea4a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Humans-as-a-Calibration-Pattern-Dynamic-3D-Scene-Reconstruction-from-Unsynchronized-and-Uncalibrated-Videos"><a href="#Humans-as-a-Calibration-Pattern-Dynamic-3D-Scene-Reconstruction-from-Unsynchronized-and-Uncalibrated-Videos" class="headerlink" title="Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from   Unsynchronized and Uncalibrated Videos"></a>Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from   Unsynchronized and Uncalibrated Videos</h2><p><strong>Authors:Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, Young Min Kim</strong></p>
<p>Recent works on dynamic neural field reconstruction assume input from synchronized multi-view videos with known poses. These input constraints are often unmet in real-world setups, making the approach impractical. We demonstrate that unsynchronized videos with unknown poses can generate dynamic neural fields if the videos capture human motion. Humans are one of the most common dynamic subjects whose poses can be estimated using state-of-the-art methods. While noisy, the estimated human shape and pose parameters provide a decent initialization for the highly non-convex and under-constrained problem of training a consistent dynamic neural representation. Given the sequences of pose and shape of humans, we estimate the time offsets between videos, followed by camera pose estimations by analyzing 3D joint locations. Then, we train dynamic NeRF employing multiresolution rids while simultaneously refining both time offsets and camera poses. The setup still involves optimizing many parameters, therefore, we introduce a robust progressive learning strategy to stabilize the process. Experiments show that our approach achieves accurate spatiotemporal calibration and high-quality scene reconstruction in challenging conditions. </p>
<blockquote>
<p>关于动态神经网络重建的最新工作假设输入来自同步的多视角视频，并且已知姿态。然而在实际环境中，这些输入约束通常无法得到满足，使得这种方法不切实际。我们证明，对于捕捉人类动作的视频，即使视频不同步且姿态未知，也能生成动态神经网络。人类是最常见的动态主题之一，其姿态可以使用最新方法进行估计。虽然存在噪声，但估计出的人类形状和姿态参数对于训练一致的动态神经网络这一高度非凸和欠约束的问题提供了一个不错的初始值。给定人类姿势和形状的序列，我们估计视频之间的时间偏移，然后通过分析3D关节位置进行相机姿态估计。然后，我们采用多分辨率网格训练动态NeRF，同时调整时间偏移和相机姿态。该设置仍然需要优化许多参数，因此，我们引入了一种稳健的渐进学习策略来稳定这一过程。实验表明，我们的方法实现了准确的时空标定和高质量的场景重建，在具有挑战的环境中也能取得良好的效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19089v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文解决动态神经场重建在真实世界场景中的实用性问题。通过对人体姿态的估计和时空校准的优化，实现动态神经场重建。使用多分辨率网格和渐进学习方法，提高了场景重建的质量和稳定性。即使面对不同步视频和未知姿态的挑战，本文方法仍然可以生成高质量的场景重建结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究工作解决动态神经场重建在真实场景中的局限性，尤其是视频同步和姿态未知的问题。</li>
<li>提出使用人体姿态估计来解决训练动态神经表示中的高度非凸和欠约束问题。</li>
<li>通过对人体姿态和形状的序列进行时间偏移估计和摄像机姿态估计，实现了对摄像机角度的调整和优化。</li>
<li>利用多分辨率网格技术训练动态NeRF模型，提高模型性能。</li>
<li>提出一种稳健的渐进学习策略来稳定训练过程。</li>
<li>实验表明，即使在具有挑战性的条件下，该方法的时空校准准确性较高且场景重建质量较好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fca17d77eea34ee6a279440b357d94bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-258a5fd0cfe85172780255e5bcadd5e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861a6a221059ed3122f6776120082d83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3c161c3e3ef89e5c6697a800176ef66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0d61a3ec03e4f212d16afb4fead0a57.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Developing-Cryptocurrency-Trading-Strategy-Based-on-Autoencoder-CNN-GANs-Algorithms"><a href="#Developing-Cryptocurrency-Trading-Strategy-Based-on-Autoencoder-CNN-GANs-Algorithms" class="headerlink" title="Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs   Algorithms"></a>Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs   Algorithms</h2><p><strong>Authors:Zhuohuan Hu, Richard Yu, Zizhou Zhang, Haoran Zheng, Qianying Liu, Yining Zhou</strong></p>
<p>This paper leverages machine learning algorithms to forecast and analyze financial time series. The process begins with a denoising autoencoder to filter out random noise fluctuations from the main contract price data. Then, one-dimensional convolution reduces the dimensionality of the filtered data and extracts key information. The filtered and dimensionality-reduced price data is fed into a GANs network, and its output serve as input of a fully connected network. Through cross-validation, a model is trained to capture features that precede large price fluctuations. The model predicts the likelihood and direction of significant price changes in real-time price sequences, placing trades at moments of high prediction accuracy. Empirical results demonstrate that using autoencoders and convolution to filter and denoise financial data, combined with GANs, achieves a certain level of predictive performance, validating the capabilities of machine learning algorithms to discover underlying patterns in financial sequences. Keywords - CNN;GANs; Cryptocurrency; Prediction. </p>
<blockquote>
<p>本文利用机器学习算法对金融时间序列进行预测和分析。流程始于使用降噪自编码器对主合约价格数据进行随机噪声波动的过滤。然后，一维卷积对过滤后的数据进行降维并提取关键信息。过滤和降维后的价格数据被输入到GANs网络中，其输出作为全连接网络的输入。通过交叉验证，训练模型以捕获先于大幅价格波动的特征。该模型预测实时价格序列中重大价格变动的可能性和方向，在高预测准确性的时刻进行交易。实证结果表明，结合使用自编码器和卷积对金融数据进行过滤和去噪，结合GANs，达到了一定的预测性能，验证了机器学习算法在发现金融序列中潜在模式方面的能力。关键词——卷积神经网络、生成对抗网络、加密货币、预测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18202v2">PDF</a> The paper was accepted by 2024 4th International Conference on   Artificial Intelligence, Robotics, and Communication(ICAIRC 2024)</p>
<p><strong>摘要</strong><br>    本文利用机器学习算法对金融时间序列进行预测与分析。通过降噪自编码器过滤主合同价格数据中的随机噪声波动，一维卷积降低数据维度并提取关键信息。经过过滤和降维的价格数据输入生成对抗网络，其输出作为全连接网络的输入。通过交叉验证，训练模型捕捉大价格波动前的特征。该模型预测实时价格序列中重大价格变化的可能性和方向，在高预测准确率时刻进行交易。实证结果表明，结合自编码器、卷积降噪和生成对抗网络，机器学习算法在发现金融序列中的潜在模式方面具有一定的预测性能。</p>
<p><strong>要点</strong></p>
<ol>
<li>使用降噪自编码器过滤金融时间序列数据中的随机噪声。</li>
<li>一维卷积用于降低数据维度并提取关键信息。</li>
<li>结合生成对抗网络（GANs）和全连接网络进行模型训练。</li>
<li>通过交叉验证，模型能够捕捉大价格波动前的特征。</li>
<li>模型可预测实时价格序列中重大价格变化的可能性和方向。</li>
<li>实证结果表明，结合自编码器和卷积的金融数据预处理，以及GANs的使用，机器学习算法具有良好的预测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-712324057607daf0b286f3c929152760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-718132e6f5bbf9d8ece72adb831f3ee7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-775bd2dbb4b1d784c9ce616ede537d86.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LiHi-GS-LiDAR-Supervised-Gaussian-Splatting-for-Highway-Driving-Scene-Reconstruction"><a href="#LiHi-GS-LiDAR-Supervised-Gaussian-Splatting-for-Highway-Driving-Scene-Reconstruction" class="headerlink" title="LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene   Reconstruction"></a>LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene   Reconstruction</h2><p><strong>Authors:Pou-Chun Kung, Xianling Zhang, Katherine A. Skinner, Nikita Jaipuria</strong></p>
<p>Photorealistic 3D scene reconstruction plays an important role in autonomous driving, enabling the generation of novel data from existing datasets to simulate safety-critical scenarios and expand training data without additional acquisition costs. Gaussian Splatting (GS) facilitates real-time, photorealistic rendering with an explicit 3D Gaussian representation of the scene, providing faster processing and more intuitive scene editing than the implicit Neural Radiance Fields (NeRFs). While extensive GS research has yielded promising advancements in autonomous driving applications, they overlook two critical aspects: First, existing methods mainly focus on low-speed and feature-rich urban scenes and ignore the fact that highway scenarios play a significant role in autonomous driving. Second, while LiDARs are commonplace in autonomous driving platforms, existing methods learn primarily from images and use LiDAR only for initial estimates or without precise sensor modeling, thus missing out on leveraging the rich depth information LiDAR offers and limiting the ability to synthesize LiDAR data. In this paper, we propose a novel GS method for dynamic scene synthesis and editing with improved scene reconstruction through LiDAR supervision and support for LiDAR rendering. Unlike prior works that are tested mostly on urban datasets, to the best of our knowledge, we are the first to focus on the more challenging and highly relevant highway scenes for autonomous driving, with sparse sensor views and monotone backgrounds. Visit our project page at: <a target="_blank" rel="noopener" href="https://umautobots.github.io/lihi_gs">https://umautobots.github.io/lihi_gs</a> </p>
<blockquote>
<p>真实感三维场景重建在自动驾驶中扮演着重要角色。它可以从现有数据集中生成新的数据，模拟关键安全场景，并在无需额外采集成本的情况下扩展训练数据。高斯拼贴（GS）技术通过场景的显式三维高斯表示，实现了实时真实感渲染，相较于隐式神经辐射场（NeRF）提供了更快的处理和更直观的场景编辑。尽管关于高斯拼贴的研究已经在自动驾驶应用方面取得了有前景的进展，但它们忽略了两个关键方面：首先，现有方法主要集中在低速且特征丰富的城市场景上，忽略了高速公路场景在自动驾驶中的重要地位。其次，虽然激光雷达在自动驾驶平台中很普遍，但现有方法主要依赖图像进行学习，仅将激光雷达用于初步估计或不精确的传感器建模，从而未能充分利用激光雷达提供的丰富深度信息，并限制了合成激光雷达数据的能力。在本文中，我们提出了一种新的高斯拼贴方法，用于动态场景合成和编辑。通过激光雷达监督和支持激光雷达渲染，改进了场景重建。据我们所知，与大多数仅在城市数据集上测试的先前工作不同，我们首次关注更具挑战性和高度相关的自动驾驶高速公路场景，具有稀疏的传感器视图和单调的背景。请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://umautobots.github.io/lihi_gs">https://umautobots.github.io/lihi_gs</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15447v2">PDF</a> </p>
<p><strong>Summary</strong><br>在自动驾驶中，基于LiDAR的三维场景重建具有重要的研究价值。本文通过引入高斯渲染技术，结合LiDAR的丰富深度信息，提出了一种新型的动态场景合成与编辑方法。此方法不仅提高了场景重建的精度，还支持LiDAR渲染，适用于高速公路场景。此研究对自动驾驶的数据扩充及模拟安全关键场景具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>光场渲染技术在自动驾驶中具有重要的应用价值，尤其是生成新的数据集以模拟安全关键场景和扩大训练数据方面。</li>
<li>高斯渲染（GS）方法提供了一种实时、逼真的渲染方式，通过明确的3D高斯场景表示，实现更快的处理和更直观的场景编辑。</li>
<li>目前GS方法主要关注低速度、特征丰富的城市场景，但在高速公路场景的自动驾驶应用中仍显不足。</li>
<li>LiDAR在自动驾驶平台中普及，但现有方法主要依赖图像学习，未充分利用LiDAR提供的丰富深度信息。</li>
<li>本文提出了一种新型的GS方法，通过LiDAR监督和支持LiDAR渲染，改善了场景重建效果。</li>
<li>与大多数仅在城市数据集上测试的方法不同，本文首次专注于更具挑战性和高度相关的高速公路场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15447">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84726a72b31a702cc0e7e50f20e6b797.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b914278b4ab4040b4fff83b0f97ae386.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c03668d4232ef3060541db9668ec81c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc807d9bc5dfd097b8376314e642a7ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45b042aa74d3b92a52cf1a4ba431f5f6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Topo-Field-Topometric-mapping-with-Brain-inspired-Hierarchical-Layout-Object-Position-Fields"><a href="#Topo-Field-Topometric-mapping-with-Brain-inspired-Hierarchical-Layout-Object-Position-Fields" class="headerlink" title="Topo-Field: Topometric mapping with Brain-inspired Hierarchical   Layout-Object-Position Fields"></a>Topo-Field: Topometric mapping with Brain-inspired Hierarchical   Layout-Object-Position Fields</h2><p><strong>Authors:Jiawei Hou, Wenhao Guan, Longfei Liang, Jianfeng Feng, Xiangyang Xue, Taiping Zeng</strong></p>
<p>Mobile robots require comprehensive scene understanding to operate effectively in diverse environments, enriched with contextual information such as layouts, objects, and their relationships. Although advances like neural radiation fields (NeRFs) offer high-fidelity 3D reconstructions, they are computationally intensive and often lack efficient representations of traversable spaces essential for planning and navigation. In contrast, topological maps are computationally efficient but lack the semantic richness necessary for a more complete understanding of the environment. Inspired by a population code in the postrhinal cortex (POR) strongly tuned to spatial layouts over scene content rapidly forming a high-level cognitive map, this work introduces Topo-Field, a framework that integrates Layout-Object-Position (LOP) associations into a neural field and constructs a topometric map from this learned representation. LOP associations are modeled by explicitly encoding object and layout information, while a Large Foundation Model (LFM) technique allows for efficient training without extensive annotations. The topometric map is then constructed by querying the learned neural representation, offering both semantic richness and computational efficiency. Empirical evaluations in multi-room environments demonstrate the effectiveness of Topo-Field in tasks such as position attribute inference, query localization, and topometric planning, successfully bridging the gap between high-fidelity scene understanding and efficient robotic navigation. </p>
<blockquote>
<p>移动机器人需要全面的场景理解，才能在各种环境中有效运行，这些环境富含布局、物体及其关系的上下文信息。尽管神经辐射场（NeRF）等先进技术提供了高保真度的3D重建，但它们计算量大，通常缺乏对可通行空间的有效表示，这对于规划和导航至关重要。相比之下，拓扑地图计算效率高，但缺乏必要的语义丰富性，无法更全面地理解环境。这项工作受到后梨状皮层（POR）中人群编码的启发，该编码对场景内容中的空间布局进行快速高级认知地图构建，介绍了一种名为Topo-Field的框架，该框架将布局-对象-位置（LOP）关联集成到神经场中，并从这种学习表示中构建拓扑地图。LOP关联通过显式编码对象和布局信息来建模，而大型基础模型（LFM）技术则可实现高效训练，无需广泛注释。然后，通过查询学习到的神经表示来构建拓扑地图，提供丰富的语义和计算效率。在多房间环境中的经验评估表明，Topo-Field在位置属性推断、查询定位和拓扑规划等任务中的有效性，成功弥合了高保真场景理解和高效机器人导航之间的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05985v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Topo-Field的框架，融合布局-物体-位置（LOP）关联到神经网络中，构建拓扑地图。利用种群编码和后颞叶皮层（POR）的空间布局敏感性，以及大型基础模型（LFM）技术，实现高效学习与训练。Topo-Field能在多房间环境中进行有效评估，成功应用于位置属性推断、查询定位和拓扑规划任务，实现了高保真场景理解与高效机器人导航之间的桥梁。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Topo-Field框架结合了布局、物体和位置（LOP）的关联，构建了一个神经场模型用于机器人场景理解。</li>
<li>借鉴后颞叶皮层（POR）的种群编码思想，快速形成高级认知地图。</li>
<li>采用大型基础模型（LFM）技术，实现无需大量标注的高效训练。</li>
<li>Topo-Field能构建拓扑地图，兼具语义丰富性和计算效率。</li>
<li>在多房间环境中进行实证评估，表现优异。</li>
<li>成功应用于位置属性推断、查询定位和拓扑规划任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-716b83cf82910d21feb0e3387a0ee090.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f98270ffa533f295a774a030506d9eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7cd400622784450b987d48614f138df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95683416866809e158ee5579eeebff5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53d0570834843767bf1a3e1a677169b5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NeRF-DetS-Enhanced-Adaptive-Spatial-wise-Sampling-and-View-wise-Fusion-Strategies-for-NeRF-based-Indoor-Multi-view-3D-Object-Detection"><a href="#NeRF-DetS-Enhanced-Adaptive-Spatial-wise-Sampling-and-View-wise-Fusion-Strategies-for-NeRF-based-Indoor-Multi-view-3D-Object-Detection" class="headerlink" title="NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion   Strategies for NeRF-based Indoor Multi-view 3D Object Detection"></a>NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion   Strategies for NeRF-based Indoor Multi-view 3D Object Detection</h2><p><strong>Authors:Chi Huang, Xinyang Li, Yansong Qu, Changli Wu, Xiaofan Li, Shengchuan Zhang, Liujuan Cao</strong></p>
<p>In indoor scenes, the diverse distribution of object locations and scales makes the visual 3D perception task a big challenge.   Previous works (e.g, NeRF-Det) have demonstrated that implicit representation has the capacity to benefit the visual 3D perception task in indoor scenes with high amount of overlap between input images.   However, previous works cannot fully utilize the advancement of implicit representation because of fixed sampling and simple multi-view feature fusion.   In this paper, inspired by sparse fashion method (e.g, DETR3D), we propose a simple yet effective method, NeRF-DetS, to address above issues. NeRF-DetS includes two modules: Progressive Adaptive Sampling Strategy (PASS) and Depth-Guided Simplified Multi-Head Attention Fusion (DS-MHA).   Specifically,   (1)PASS can automatically sample features of each layer within a dense 3D detector, using offsets predicted by the previous layer.   (2)DS-MHA can not only efficiently fuse multi-view features with strong occlusion awareness but also reduce computational cost.   Extensive experiments on ScanNetV2 dataset demonstrate our NeRF-DetS outperforms NeRF-Det, by achieving +5.02% and +5.92% improvement in mAP under IoU25 and IoU50, respectively. Also, NeRF-DetS shows consistent improvements on ARKITScenes. </p>
<blockquote>
<p>在室内场景中，物体位置和尺度的多样分布使得视觉3D感知任务面临巨大挑战。先前的工作（例如NeRF-Det）已经证明，隐式表示有益于室内场景的视觉3D感知任务，尤其在输入图像之间存在大量重叠时。然而，由于固定的采样和简单的多视图特征融合，先前的工作不能完全利用隐式表示的进步。本文受稀疏方式方法的启发（例如DETR3D），提出了一种简单而有效的方法NeRF-DetS来解决上述问题。NeRF-DetS包括两个模块：渐进式自适应采样策略（PASS）和深度引导简化多头注意力融合（DS-MHA）。具体来说，（1）PASS可以自动在密集3D检测器内的每一层采样特征，使用前一层的预测偏移。（2）DS-MHA不仅可以有效地融合多视图特征，具有强烈的遮挡意识，而且可以降低计算成本。在ScanNetV2数据集上的大量实验表明，我们的NeRF-DetS优于NeRF-Det，在IoU25和IoU50的mAP上分别提高了+5.02%和+5.92%。此外，NeRF-DetS在ARKITScenes上也表现出了一致性的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13921v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>室内场景中，物体位置和尺度的多样性使得视觉3D感知任务极具挑战。NeRF-DetS是一种简单有效的方法，解决了之前工作中固定采样和简单多视角特征融合的问题。NeRF-DetS包括两个模块：渐进自适应采样策略（PASS）和深度引导简化多头注意力融合（DS-MHA）。PASS可自动在密集3D检测器中每层采样特征，使用前一层的预测偏移。DS-MHA不仅能有效地融合具有强遮挡意识的多个视角特征，还能降低计算成本。在ScanNetV2数据集上的实验表明，NeRF-DetS优于NeRF-Det，在IoU25和IoU50下mAP分别提高了+5.02%和+5.92%。同时，NeRF-DetS在ARKITScenes上也表现出一致的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>室内场景中，物体位置和尺度的多样性使得视觉3D感知具有挑战性。</li>
<li>之前的作品如NeRF-Det虽展示了隐式表示在视觉3D感知任务中的潜力，但无法充分利用其优势。</li>
<li>NeRF-DetS通过引入两个模块：PASS和DS-MHA，解决了固定采样和简单多视角特征融合的问题。</li>
<li>PASS能自动在密集3D检测器的每一层进行特征采样。</li>
<li>DS-MHA能高效融合多视角特征，增强遮挡意识并降低计算成本。</li>
<li>在ScanNetV2数据集上的实验显示，NeRF-DetS相比NeRF-Det有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13921">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-306731d7f1d69eb434e06e08799097f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6cfddae21ae946b554002b6fbfddba5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81289d67066949786f358bca9834b9d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-584e618350edd2bbd3f4bc1b85781e60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ee6efda62aa34115ce79937185712e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c606b27e21365bf72d1c862888c3d944.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-02  Prometheus 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b51444c746b853c2990f78df6232bb9c.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-01-02  Prometheus 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19211.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
