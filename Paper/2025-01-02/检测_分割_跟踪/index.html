<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-01-02  SM3Det A Unified Model for Multi-Modal Remote Sensing Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    56 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-02-更新"><a href="#2025-01-02-更新" class="headerlink" title="2025-01-02 更新"></a>2025-01-02 更新</h1><h2 id="SM3Det-A-Unified-Model-for-Multi-Modal-Remote-Sensing-Object-Detection"><a href="#SM3Det-A-Unified-Model-for-Multi-Modal-Remote-Sensing-Object-Detection" class="headerlink" title="SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection"></a>SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection</h2><p><strong>Authors:Yuxuan Li, Xiang Li, Yunheng Li, Yicheng Zhang, Yimian Dai, Qibin Hou, Ming-Ming Cheng, Jian Yang</strong></p>
<p>With the rapid advancement of remote sensing technology, high-resolution multi-modal imagery is now more widely accessible. Conventional Object detection models are trained on a single dataset, often restricted to a specific imaging modality and annotation format. However, such an approach overlooks the valuable shared knowledge across multi-modalities and limits the model’s applicability in more versatile scenarios. This paper introduces a new task called Multi-Modal Datasets and Multi-Task Object Detection (M2Det) for remote sensing, designed to accurately detect horizontal or oriented objects from any sensor modality. This task poses challenges due to 1) the trade-offs involved in managing multi-modal modelling and 2) the complexities of multi-task optimization. To address these, we establish a benchmark dataset and propose a unified model, SM3Det (Single Model for Multi-Modal datasets and Multi-Task object Detection). SM3Det leverages a grid-level sparse MoE backbone to enable joint knowledge learning while preserving distinct feature representations for different modalities. Furthermore, it integrates a consistency and synchronization optimization strategy using dynamic learning rate adjustment, allowing it to effectively handle varying levels of learning difficulty across modalities and tasks. Extensive experiments demonstrate SM3Det’s effectiveness and generalizability, consistently outperforming specialized models on individual datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">https://github.com/zcablii/SM3Det</a>. </p>
<blockquote>
<p>随着遥感技术的快速发展，高分辨率多模态图像现在更加普及。传统的目标检测模型是在单一数据集上进行训练的，通常局限于特定的成像方式和注释格式。然而，这种方法忽略了多模态之间宝贵的共享知识，并限制了模型在更通用场景中的应用。本文介绍了一个针对遥感的新任务，称为多模态数据集和多任务目标检测（M2Det）。此任务旨在准确检测来自任何传感器模态的水平或定向目标。此任务面临着两个挑战：1）涉及多模态建模的权衡问题；2 2）多任务优化的复杂性。为了应对这些挑战，我们建立了一个基准数据集并提出了一种统一模型SM3Det（多模态数据集和多任务目标检测的单模型）。SM3Det利用网格级稀疏MoE骨干网实现联合知识学习，同时保留不同模态的不同特征表示。此外，它采用动态学习率调整的一致性同步优化策略，能够有效处理不同模态和任务之间不同等级的学习难度。大量实验证明了SM3Det的有效性和通用性，在单个数据集上的表现均优于专业模型。相关代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">https://github.com/zcablii/SM3Det</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20665v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着遥感技术的快速发展，高分辨率多模态图像现在更加易于获取。常规目标检测模型通常仅在单一数据集上进行训练，受限于特定的成像方式和注释格式。然而，这种方法忽略了多模态之间的共享知识，并限制了模型在更通用场景中的应用。本文介绍了遥感中的多模态数据集多任务目标检测（M2Det）新任务，旨在从任何传感器模态准确检测水平或定向目标。该任务面临管理多模态建模和多任务优化的权衡问题。为应对这些挑战，我们建立了一个基准数据集并提出了一种统一模型SM3Det。SM3Det利用网格级稀疏MoE主干实现联合知识学习，同时保留不同模态的独特特征表示。此外，它采用一致性同步优化策略及动态学习率调整，有效处理不同模态和任务的学习难度差异。实验证明SM3Det的有效性和通用性，在个别数据集上表现优于专业模型。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">链接</a>找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感技术快速发展，高分辨率多模态图像广泛可获取。</li>
<li>常规目标检测模型通常在单一数据集上训练，限制其在多模态和更通用场景中的应用。</li>
<li>引入新的任务——多模态数据集多任务目标检测（M2Det），旨在从任何传感器模态准确检测目标。</li>
<li>M2Det面临管理多模态建模和多任务优化的挑战。</li>
<li>提出SM3Det模型，利用网格级稀疏MoE主干实现联合知识学习，同时保留不同模态的独特特征表示。</li>
<li>SM3Det采用一致性同步优化策略及动态学习率调整，以处理不同模态和任务的学习难度差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dabf692458edb22d114cba6e6c0a6714.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fb90ef035e0f6d3047bfa57a2b1db30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c8676918f0c1ace9673fbb9506a9c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a936ee18d0195a01bc008a5242370e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a50e8092c61b403e38354666349b94f5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="YOLO-UniOW-Efficient-Universal-Open-World-Object-Detection"><a href="#YOLO-UniOW-Efficient-Universal-Open-World-Object-Detection" class="headerlink" title="YOLO-UniOW: Efficient Universal Open-World Object Detection"></a>YOLO-UniOW: Efficient Universal Open-World Object Detection</h2><p><strong>Authors:Lihao Liu, Juexiao Feng, Hui Chen, Ao Wang, Lin Song, Jungong Han, Guiguang Ding</strong></p>
<p>Traditional object detection models are constrained by the limitations of closed-set datasets, detecting only categories encountered during training. While multimodal models have extended category recognition by aligning text and image modalities, they introduce significant inference overhead due to cross-modality fusion and still remain restricted by predefined vocabulary, leaving them ineffective at handling unknown objects in open-world scenarios. In this work, we introduce Universal Open-World Object Detection (Uni-OWD), a new paradigm that unifies open-vocabulary and open-world object detection tasks. To address the challenges of this setting, we propose YOLO-UniOW, a novel model that advances the boundaries of efficiency, versatility, and performance. YOLO-UniOW incorporates Adaptive Decision Learning to replace computationally expensive cross-modality fusion with lightweight alignment in the CLIP latent space, achieving efficient detection without compromising generalization. Additionally, we design a Wildcard Learning strategy that detects out-of-distribution objects as “unknown” while enabling dynamic vocabulary expansion without the need for incremental learning. This design empowers YOLO-UniOW to seamlessly adapt to new categories in open-world environments. Extensive experiments validate the superiority of YOLO-UniOW, achieving achieving 34.6 AP and 30.0 APr on LVIS with an inference speed of 69.6 FPS. The model also sets benchmarks on M-OWODB, S-OWODB, and nuScenes datasets, showcasing its unmatched performance in open-world object detection. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/THU-MIG/YOLO-UniOW">https://github.com/THU-MIG/YOLO-UniOW</a>. </p>
<blockquote>
<p>传统目标检测模型受限于封闭数据集，只能检测训练时遇到的目标类别。虽然多模态模型通过文本和图像模态的对齐扩展了类别识别能力，但由于跨模态融合而引入了显著的推理开销，并且仍然受到预设词汇表的限制，无法处理开放世界场景中的未知对象。在这项工作中，我们引入了统一开放世界目标检测（Uni-OWD）这一新范式，将开放词汇表和开放世界目标检测任务统一起来。为了应对这一设置中的挑战，我们提出了YOLO-UniOW这一新型模型，在效率、通用性和性能上推进了边界。YOLO-UniOW结合了自适应决策学习，用轻量级的对齐替换计算昂贵的跨模态融合，在CLIP潜在空间中实现高效检测而不损害泛化能力。此外，我们还设计了Wildcard学习策略，能够检测分布外的对象并将其标记为“未知”，同时实现动态词汇扩展而无需增量学习。这一设计使YOLO-UniOW能够轻松适应开放世界环境中的新类别。大量实验验证了YOLO-UniOW的优越性，在LVIS上实现了34.6的AP和30.0的APr，推理速度为每秒69.6帧。该模型还在M-OWODB、S-OWODB和nuScenes数据集上设定了基准测试，展示了其在开放世界目标检测中的卓越性能。相关代码和模型可通过<a target="_blank" rel="noopener" href="https://github.com/THU-MIG/YOLO-UniOW%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THU-MIG/YOLO-UniOW获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20645v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>传统目标检测模型受限于封闭数据集，只能检测训练时遇到的目标类别。多模态模型通过文本和图像模态的对齐扩展了类别识别能力，但引入了由于跨模态融合造成的推理开销，并且仍然受限于预设词汇表，无法处理开放世界场景中的未知对象。本文提出统一开放词汇和开放世界目标检测任务的新范式——通用开放世界目标检测（Uni-OWD）。针对此设置，我们提出了YOLO-UniOW模型，该模型在效率、通用性和性能方面取得了突破。YOLO-UniOW采用自适应决策学习，以轻量级对齐替换计算昂贵的跨模态融合，在CLIP潜在空间中实现高效检测而不影响泛化。此外，我们设计了Wildcard学习策略，能够检测异常对象并将其标记为“未知”，同时实现无需增量学习的动态词汇扩展。这使得YOLO-UniOW能够轻松适应开放环境中的新类别。经过广泛实验验证，YOLO-UniOW在LVIS数据集上取得了34.6的AP和30.0的APr，推理速度为69.6 FPS。该模型还在M-OWODB、S-OWODB和nuScenes数据集上设定了基准，展示了其在开放世界目标检测中的卓越性能。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>传统目标检测模型受限于封闭数据集，难以应对开放世界中的未知对象。</li>
<li>YOLO-UniOW模型通过结合自适应决策学习和Wildcard学习策略，实现了开放词汇和开放世界目标检测的统一。</li>
<li>YOLO-UniOW采用轻量级对齐方式，在CLIP潜在空间中实现高效检测。</li>
<li>Wildcard学习策略能检测异常对象并标记为“未知”，同时实现动态词汇扩展。</li>
<li>YOLO-UniOW在多个数据集上实现了卓越性能，包括LVIS、M-OWODB、S-OWODB和nuScenes。</li>
<li>YOLO-UniOW模型具有高效的推理速度，达到69.6 FPS。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20645">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-50a73c4303f17df361e9de1537973b6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2ea5548c68a0b9c7c37733adc5fcbe1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ddd7fe86b772fdac7dd17756a55d4d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b9a23b21d39012b8149e281f6c8837.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation Agent for Weakly Supervised Semantic Segmentation"></a>Image Augmentation Agent for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets. </p>
<blockquote>
<p>弱监督语义分割（WSSS）仅使用图像级标签取得了显著的进步。然而，大多数现有的WSSS方法主要关注设计新的网络结构和损失函数来生成更精确的密集标签，而忽视了固定数据集所带来的限制，这些限制可能会限制性能的提升。我们认为，提供更多可训练图像的多样性可以为WSSS提供更丰富的信息，并帮助模型理解更全面的语义模式。因此，在本文中，我们介绍了一种新方法，称为图像增强代理（IAA），它表明从数据生成的角度增强WSSS是可能的。IAA主要设计了一个增强代理，它利用大型语言模型（LLM）和扩散模型来自动为WSSS生成额外的图像。在实践中，为了解决LLM在提示生成中的不稳定性问题，我们开发了一种提示自我优化机制。它允许LLM重新评估生成的提示的合理性，以产生更连贯的提示。此外，我们在扩散生成过程中插入了一个在线过滤器，以动态确保生成图像的质量和平衡。实验结果表明，我们的方法在PASCAL VOC 2012和MS COCO 2014数据集上显著超越了最先进的WSSS方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20439v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本文提出一种名为Image Augmentation Agent（IAA）的方法，从数据生成的角度提升弱监督语义分割（WSSS）性能。通过利用大型语言模型（LLMs）和扩散模型自动生成额外图像，增强WSSS的训练图像多样性。提出一种提示自我完善机制，确保生成的提示更加合理和连贯。在PASCAL VOC 2012和MS COCO 2014数据集上的实验结果表明，该方法显著超越了现有WSSS方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IAA方法从数据生成角度提升弱监督语义分割性能。</li>
<li>利用大型语言模型（LLMs）和扩散模型自动生成额外图像，增加训练图像多样性。</li>
<li>提出一种提示自我完善机制，解决语言模型提示生成的不稳定问题。</li>
<li>在在线过滤中确保生成图像的质量和平衡。</li>
<li>方法在PASCAL VOC 2012和MS COCO 2014数据集上表现优异。</li>
<li>突破现有WSSS方法的局限，不仅仅关注网络结构和损失函数设计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af2a3ebd09a1bf1b97a1d039283df260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d420af9d7c6e4b553c221ca8efb92b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50df88fe53c498b6f64d28ef4ad06123.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7d6bc9d163130f0f670b8ebe70adb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4acc489d52ac34b0db3e21364468b2a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP"><a href="#Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP" class="headerlink" title="Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP"></a>Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP</h2><p><strong>Authors:Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge</strong></p>
<p>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）在弱监督语义分割（WSSS）研究中的应用展现了强大的跨模态语义理解功能。现有方法试图优化输入文本提示，以改进图像和文本的对齐方式，通过微调文本原型来促进语义匹配。然而，鉴于文本和视觉空间之间的模态差距，这些方法所采用的文本原型并未有效地与像素级视觉特征建立紧密对应关系。在我们的理论分析中，固有的模态差距会导致文本和区域特征的错位，并且仅仅通过最小化CLIP中的对比损失不足以缩小这一差距。为了减轻模态差距的影响，我们提出了视觉原型学习（VPL）框架，通过引入更具代表性的视觉原型。该框架的核心是在视觉空间的帮助下，学习特定类别的视觉原型，以捕获高质量的定位图。此外，我们提出了区域语义对比模块，该模块将区域嵌入与相应的原型进行对比，从而实现更全面和稳健的特征学习。实验结果表明，我们提出的框架在两个基准数据集上达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19650v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究应用Contrastive Language-Image Pre-training（CLIP）于Weakly Supervised Semantic Segmentation（WSSS）中，实现跨模态语义理解。现有方法通过微调文本提示优化图像与文本的匹配度，但模态间的差距导致文本原型未能与像素级视觉特征建立紧密对应关系。本研究提出Vision Prototype Learning（VPL）框架，引入更具代表性的视觉原型，以学习特定类别的视觉特征。同时，本研究设计了一种区域语义对比模块，该模块对比区域嵌入与对应原型的特征，实现更全面和鲁棒的特征学习。实验结果表明，该框架在两项基准测试中达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP在WSSS研究中的应用展示了强大的跨模态语义理解功能。</li>
<li>现有方法试图通过微调文本提示来优化图像和文本的匹配度。</li>
<li>模态差距导致文本原型未能有效地与像素级视觉特征相对应。</li>
<li>引入VPL框架以学习更具代表性的视觉原型。</li>
<li>VPL框架借助文本原型在视觉空间内学习特定类别的视觉特征。</li>
<li>设计了区域语义对比模块，实现更全面和鲁棒的特征学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-90ee0f753b63bdc6501a80c978c40689.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b4383b6e1ce4886b79ce30a149d538c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6d4d4b8aed96474ae553e45f0a7d71.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Open-Vocabulary-Remote-Sensing-Image-Semantic-Segmentation"><a href="#Towards-Open-Vocabulary-Remote-Sensing-Image-Semantic-Segmentation" class="headerlink" title="Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation"></a>Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation</h2><p><strong>Authors:Chengyang Ye, Yunzhi Zhuge, Pingping Zhang</strong></p>
<p>Recently, deep learning based methods have revolutionized remote sensing image segmentation. However, these methods usually rely on a pre-defined semantic class set, thus needing additional image annotation and model training when adapting to new classes. More importantly, they are unable to segment arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary semantic classes in remote sensing images. To address the lack of OVRSISS datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images covering 40 diverse semantic classes. In addition, we propose a novel framework named GSNet that integrates domain priors from special remote sensing models and versatile capabilities of general vision-language models. Technically, GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE first captures comprehensive features from both special models and general models in dual streams. Then, with the guidance of variable vocabularies, QGFF integrates specialist and generalist features, enabling them to complement each other. Finally, RIPD is proposed to aggregate multi-source features for more accurate mask predictions. Experiments show that our method outperforms other methods by a large margin, and our proposed LandDiscover50K improves the performance of OVRSISS methods. The proposed dataset and method will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/yecy749/GSNet">https://github.com/yecy749/GSNet</a>. </p>
<blockquote>
<p>最近，基于深度学习的方法在遥感图像分割领域掀起了革命性的变革。然而，这些方法通常依赖于预先定义的语义类别集，因此在适应新类别时需要额外的图像标注和模型训练。更重要的是，它们无法对任意语义类别进行分割。在这项工作中，我们引入了开放词汇遥感图像语义分割（OVRSISS），旨在实现对遥感图像中任意语义类别的分割。为了解决OVRSISS数据集缺乏的问题，我们开发了LandDiscover50K，这是一个包含51,846张图像的综合数据集，涵盖了40个多样化的语义类别。此外，我们提出了一种新的框架GSNet，它融合了特殊遥感模型的领域先验知识和通用视觉语言模型的通用能力。技术上，GSNet包括双流图像编码器（DSIE）、查询引导特征融合（QGFF）和残差信息保留解码器（RIPD）。首先，DSIE从特殊模型和一般模型的双流中捕获全面的特征。然后，在可变词汇表的指导下，QGFF融合了专业特征和通用特征，使它们能够相互补充。最后，RIPD被提出来聚合多源特征，以进行更准确的掩膜预测。实验表明，我们的方法在其他方法的基础上取得了很大的优势，我们提出的LandDiscover50K也提高了OVRSISS方法的性能。该数据集和方法将在<a target="_blank" rel="noopener" href="https://github.com/yecy749/GSNet%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/yecy749/GSNet上公开发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19492v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong>：近期深度学习在遥感图像分割领域引发革命。然而，现有方法受限于预定义语义类别集，难以适应新类别。本文提出开放词汇遥感图像语义分割（OVRSISS），旨在实现对遥感图像中的任意语义类别进行分割。为应对OVRSISS数据集缺乏的问题，本文开发了LandDiscover50K数据集。此外，提出了一种名为GSNet的新框架，融合了特殊遥感模型的领域先验知识和通用视觉语言模型的通用能力。实验表明，该方法大幅优于其他方法，LandDiscover50K数据集提高了OVRSISS方法的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度学习在遥感图像分割中取得显著进展。</li>
<li>现有方法受限于预定义语义类别，需额外图像标注和模型训练以适应新类别。</li>
<li>提出开放词汇遥感图像语义分割（OVRSISS）以分割任意语义类别。</li>
<li>开发LandDiscover50K数据集，包含51,846张图像，覆盖40个多样语义类别。</li>
<li>提出GSNet框架，融合特殊遥感模型和通用视觉语言模型的优点。</li>
<li>GSNet包括双流图像编码器（DSIE）、查询引导特征融合（QGFF）和残差信息保留解码器（RIPD）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-611fb43ebba71a36ad42688e7c6a5822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7431be40eede724e0182b1a0b7a219b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff1a61fcc4ab227c5c72f28ac6a1f3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfc655f72b48818862a7c183d2d50477.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Coin-to-Data-The-Impact-of-Object-Detection-on-Digital-Numismatics"><a href="#From-Coin-to-Data-The-Impact-of-Object-Detection-on-Digital-Numismatics" class="headerlink" title="From Coin to Data: The Impact of Object Detection on Digital Numismatics"></a>From Coin to Data: The Impact of Object Detection on Digital Numismatics</h2><p><strong>Authors:Rafael Cabral, Maria De Iorio, Andrew Harris</strong></p>
<p>In this work we investigate the application of advanced object detection techniques to digital numismatics, focussing on the analysis of historical coins. Leveraging models such as Contrastive Language-Image Pre-training (CLIP), we develop a flexible framework for identifying and classifying specific coin features using both image and textual descriptions. By examining two distinct datasets, modern Russian coins featuring intricate “Saint George and the Dragon” designs and degraded 1st millennium AD Southeast Asian coins bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection algorithms in search and classification tasks. Our results demonstrate the superior performance of larger CLIP models in detecting complex imagery, while traditional methods excel in identifying simple geometric patterns. Additionally, we propose a statistical calibration mechanism to enhance the reliability of similarity scores in low-quality datasets. This work highlights the transformative potential of integrating state-of-the-art object detection into digital numismatics, enabling more scalable, precise, and efficient analysis of historical artifacts. These advancements pave the way for new methodologies in cultural heritage research, artefact provenance studies, and the detection of forgeries. </p>
<blockquote>
<p>在这项工作中，我们研究了先进的目标检测技术在数字钱币学中的应用，重点分析了历史硬币。我们利用诸如对比语言图像预训练（CLIP）的模型，开发了一个灵活的框架，该框架可以使用图像和文本描述来识别和分类特定的硬币特征。通过检查两个独特的数据集——具有复杂“圣乔治和龙”设计的现代俄罗斯硬币和退化的一千年前的东南亚带有印度教佛教符号的硬币，我们评估了不同检测算法在搜索和分类任务中的有效性。我们的结果表明，在检测复杂图像方面，较大的CLIP模型表现出卓越的性能，而传统方法在识别简单几何模式方面表现出色。此外，我们提出了一种统计校准机制，以提高低质量数据集中相似性评分的可靠性。这项工作强调了将最新目标检测技术集成到数字钱币学中所带来的变革性潜力，能够实现更大规模、更精确、更高效的历史文物分析。这些进步为文化遗产研究、文物原产地研究和伪造检测等领域开辟了新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了先进的目标检测技术在数字钱币学中的应用，重点分析了对历史硬币的分析。研究团队利用对比语言图像预训练（CLIP）模型，开发了一个灵活框架，能够通过图像和文本描述来识别和分类硬币特征。通过对带有复杂“圣乔治与龙”设计的现代俄罗斯硬币和带有印度教佛教符号的1世纪东南亚硬币两个数据集的研究，评估了不同检测算法在搜索和分类任务中的有效性。结果显示大型CLIP模型在检测复杂图像方面表现优异，而传统方法在识别简单几何图案方面更出色。此外，研究还提出了一种统计校准机制，以提高低质量数据集中相似度评分的可靠性。该研究突显了将最新目标检测技术融入数字钱币学的潜力，为更规模化、精确和高效的历史文物分析打开了新途径，并为文化遗产研究、文物溯源研究和伪造检测提供了新的方法论。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究将先进的目标检测技术应用于数字钱币学，专注于历史硬币的分析。</li>
<li>利用CLIP模型开发了一个灵活框架，可通过图像和文本描述进行硬币特征识别和分类。</li>
<li>通过两个不同数据集的研究，评估了不同检测算法在硬币识别和分类任务中的效能。</li>
<li>大型CLIP模型在检测复杂图像方面表现优异，而传统方法更擅长识别简单几何图案。</li>
<li>提出了一种统计校准机制，以提高低质量数据集中相似度评分的可靠性。</li>
<li>研究突显了将最新目标检测技术融入数字钱币学的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-30714de7ccb62e036fec704f34d33bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9e9e53bee649a9a079e3c7d76162da9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HV-BEV-Decoupling-Horizontal-and-Vertical-Feature-Sampling-for-Multi-View-3D-Object-Detection"><a href="#HV-BEV-Decoupling-Horizontal-and-Vertical-Feature-Sampling-for-Multi-View-3D-Object-Detection" class="headerlink" title="HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for   Multi-View 3D Object Detection"></a>HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for   Multi-View 3D Object Detection</h2><p><strong>Authors:Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang</strong></p>
<p>The application of vision-based multi-view environmental perception system has been increasingly recognized in autonomous driving technology, especially the BEV-based models. Current state-of-the-art solutions primarily encode image features from each camera view into the BEV space through explicit or implicit depth prediction. However, these methods often focus on improving the accuracy of projecting 2D features into corresponding depth regions, while overlooking the highly structured information of real-world objects and the varying height distributions of objects across different scenes. In this work, we propose HV-BEV, a novel approach that decouples feature sampling in the BEV grid queries paradigm into horizontal feature aggregation and vertical adaptive height-aware reference point sampling, aiming to improve both the aggregation of objects’ complete information and generalization to diverse road environments. Specifically, we construct a learnable graph structure in the horizontal plane aligned with the ground for 3D reference points, reinforcing the association of the same instance across different BEV grids, especially when the instance spans multiple image views around the vehicle. Additionally, instead of relying on uniform sampling within a fixed height range, we introduce a height-aware module that incorporates historical information, enabling the reference points to adaptively focus on the varying heights at which objects appear in different scenes. Extensive experiments validate the effectiveness of our proposed method, demonstrating its superior performance over the baseline across the nuScenes dataset. Moreover, our best-performing model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing set. </p>
<blockquote>
<p>基于视觉的多视图环境感知系统在自动驾驶技术中的应用，特别是基于鸟瞰视图（BEV）的模型，已经得到了越来越多的认可。当前先进解决方案主要通过显式或隐式的深度预测，将每个相机视图的图像特征编码到鸟瞰视图空间中。然而，这些方法往往侧重于提高将二维特征投影到相应深度区域的准确性，而忽略了现实世界物体的高度结构化信息和不同场景中物体高度分布的多样性。</p>
</blockquote>
<p>针对这一问题，我们提出了HV-BEV这一新方法，它将鸟瞰视图网格查询范式中的特征采样解耦为水平特征聚合和垂直自适应高度感知参考点采样，旨在提高物体完整信息的聚合能力以及对不同道路环境的泛化能力。具体而言，我们在与地面对齐的水平面上构建了可学习的图结构，用于三维参考点，加强了不同鸟瞰视图网格中同一实例的关联，特别是当实例跨越车辆周围多个视图时。此外，我们引入了高度感知模块，该模块利用历史信息，而不是在固定高度范围内进行均匀采样，使参考点能够自适应地关注不同场景中物体出现的高度变化。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18884v2">PDF</a> 12 pages, 7 figures, submitted to T-ITS</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在自动驾驶技术中，基于视觉的多视角环境感知系统的应用，特别是基于BEV（鸟瞰视图）的模型。针对现有方法忽略真实世界物体的结构化信息和场景中的物体高度分布的问题，提出了一种新的方法HV-BEV。该方法将BEV网格查询范式中的特征采样解耦为水平特征聚合和垂直自适应高度感知参考点采样，旨在改进物体的完整信息聚合和多样化道路环境的泛化能力。通过构建与地面对齐的可学习图结构，强化不同BEV网格中同一实例的关联。同时，引入高度感知模块，结合历史信息，使参考点能自适应关注不同场景中物体的不同高度。实验验证了该方法的有效性，在nuScenes数据集上优于基线方法，最佳模型在nuScenes测试集上达到了50.5%的mAP和59.8%的NDS。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶技术中，基于视觉的多视角环境感知系统的重要性日益凸显，特别是BEV模型。</li>
<li>当前方法主要通过对图像特征进行编码并映射到BEV空间，但忽视了物体的结构化信息和高度分布。</li>
<li>HV-BEV方法旨在改进物体的完整信息聚合和泛化能力，通过水平特征聚合和垂直自适应高度感知参考点采样解耦特征采样。</li>
<li>HV-BEV构建了一个与地面对齐的可学习图结构，强化不同BEV网格中同一实例的关联。</li>
<li>引入高度感知模块，结合历史信息，使参考点能自适应关注不同场景中物体的不同高度。</li>
<li>实验验证了HV-BEV方法的有效性，在nuScenes数据集上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eeab49952df98bba3f2eb0ed04bb9edf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-030efa7d39eec8c6225d78a1ccdc908b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-071d8aa36213597c2ca97127db03c494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f6735f528f8f7b121761262ad46158.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation"><a href="#AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation" class="headerlink" title="AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation"></a>AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation</h2><p><strong>Authors:Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li</strong></p>
<p>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet">https://github.com/jarch-ma/AFANet</a>. </p>
<blockquote>
<p>少量学习旨在通过从少量样本中学习到的先验知识来识别新概念。然而，对于视觉密集型任务（如少量语义分割）而言，像素级注释既耗时又成本高昂。因此，本文利用更具挑战性的图像级注释，并提出自适应频率感知网络（AFANet）进行弱监督下的少量语义分割（WFSS）。具体来说，我们首先提出了跨粒度频率感知模块（CFM），它将RGB图像分解成高频和低频分布，并通过重新对齐进一步优化语义结构信息。与大多数现有的WFSS方法不同，这些方法采用离线学习的多模态语言视觉模型的文本信息（例如CLIP），我们还提出了CLIP引导的空间适配器模块（CSM），该模块通过在线学习对文本信息进行空间域自适应转换，从而为CFM提供丰富的跨模态语义信息。在Pascal-5i和COCO-20i数据集上的大量实验表明，AFANet已经达到了最先进的性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jarch-ma/AFANet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17601v2">PDF</a> Accepted by TMM 2024</p>
<p><strong>Summary</strong></p>
<p>该论文探讨了小样本学习在弱监督下的语义分割应用。针对图像级别的标注挑战，提出了自适应频率感知网络（AFANet）。通过交叉粒度频率感知模块（CFM）处理图像，并结合CLIP指导的空间适配器模块（CSM），实现了在线学习的跨模态语义信息，优化分割性能并达到了先进水平。相关代码已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文关注小样本学习在弱监督下的语义分割问题，旨在解决像素级标注耗时耗资的问题。</li>
<li>引入自适应频率感知网络（AFANet），采用图像级标注进行训练。</li>
<li>提出交叉粒度频率感知模块（CFM），能够分离图像的高频和低频分布，并优化语义结构信息。</li>
<li>与大多数使用离线学习方式的弱监督语义分割方法不同，该论文采用在线学习的方式处理文本信息。</li>
<li>引入CLIP指导的空间适配器模块（CSM），对文本信息进行空间域自适应转换，提供丰富的跨模态语义信息。</li>
<li>在Pascal-5i和COCO-20i数据集上进行了广泛实验，验证了AFANet的有效性，并达到了领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e3c275123d7cc725bbdac793e2b1c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92b12f4ef3edec5343b33d63af5f2e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3c89546934a25924e4a0910914e013.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-912009cb34665b008f15f63397f9a37e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation"><a href="#MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation"></a>MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation</h2><p><strong>Authors:Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe">https://github.com/zwyang6/MoRe</a>. </p>
<blockquote>
<p>使用图像级标签的弱监督语义分割（WSSS）通常使用类激活图（CAM）来实现密集预测。最近，视觉转换器（ViT）提供了一种从类补丁注意力生成定位图的替代方案。然而，由于对这类注意力的建模约束不足，我们观察到定位注意力图（LAM）经常面临伪影问题，即语义相关性极小的补丁区域会被类令牌错误激活。在这项工作中，我们提出MoRe来解决这个问题，并进一步研究LAM的潜力。我们的研究结果表明，对类补丁注意力施加额外的正则化是必要的。为此，我们首先将注意力视为一种新型的有向图，并提出图类别表示模块，以隐含地正则化类补丁实体之间的交互。它确保类令牌能够动态地凝聚相关的补丁信息，并在图级别抑制不相关的伪影。其次，受分类权重CAM能够保持对象定位平滑性的观察启发，我们设计了定位信息正则化模块，以显式地正则化类补丁注意力。它直接从CAM挖掘令牌关系，并以可学习的方式监督类令牌和补丁令牌之间的一致性。在PASCAL VOC和MS COCO上进行了大量实验，验证了MoRe有效地解决了伪影问题，并实现了最先进的性能，超越了最近的单阶段甚至多阶段方法。代码可用在<a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe%E3%80%82">https://github.com/zwyang6/MoRe。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11076v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于图像级别标签的弱监督语义分割（WSSS）问题。文章指出，使用Vision Transformer（ViT）生成的Localization Attention Maps（LAM）常常受到伪影问题的影响。为解决这一问题，本文提出了MoRe方法，通过额外的正则化对类补丁注意力进行约束。MoRe包括两个模块：Graph Category Representation模块和Localization-informed Regularization模块，前者通过构建类补丁实体的有向图来隐式地约束类补丁之间的交互，后者则从分类权重的CAM中提取token关系，以显式地监督类补丁注意力的一致性。实验证明，MoRe有效地解决了伪影问题，并在PASCAL VOC和MS COCO数据集上实现了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSS中使用Class Activation Maps (CAM) 实现密集预测。</li>
<li>Vision Transformer (ViT) 可生成Localization Attention Maps (LAM)。</li>
<li>LAM存在伪影问题，即不相关的补丁区域会被类标记错误激活。</li>
<li>MoRe方法通过额外的正则化解决LAM的伪影问题。</li>
<li>MoRe包括Graph Category Representation模块和Localization-informed Regularization模块。</li>
<li>MoRe在PASCAL VOC和MS COCO数据集上实现了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11076">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-872f56d51aa5d225fb4d9dbe916addf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-558c55d8af6b022179480697698069b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3e4750dfaf5c1d3fd23143f2d50fec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699cfbb8aa515db4cbb28229318109c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ERUP-YOLO-Enhancing-Object-Detection-Robustness-for-Adverse-Weather-Condition-by-Unified-Image-Adaptive-Processing"><a href="#ERUP-YOLO-Enhancing-Object-Detection-Robustness-for-Adverse-Weather-Condition-by-Unified-Image-Adaptive-Processing" class="headerlink" title="ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather   Condition by Unified Image-Adaptive Processing"></a>ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather   Condition by Unified Image-Adaptive Processing</h2><p><strong>Authors:Yuka Ogino, Yuho Shoji, Takahiro Toizumi, Atsushi Ito</strong></p>
<p>We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light. Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections. Our framework introduces two differentiable filters: a B&#39;ezier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. These filters unify the functions of classical image processing filters and improve performance of object detection. We also propose a domain-agnostic data augmentation strategy using the BPW filter. Our method does not require data-specific customization of the filter combinations, parameter ranges, and data augmentation. We evaluate our proposed approach, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO, by applying it to the YOLOv3 detector. Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions, including fog and low-light conditions. </p>
<blockquote>
<p>我们提出了一种针对恶劣天气条件（如雾和低光）的自适应图像目标检测方法。我们的框架采用可微分的预处理滤波器，执行适用于后期目标检测的图像增强。我们的框架引入两种可微分滤波器：基于贝塞尔曲线的像素级（BPW）滤波器和基于核的局部（KBL）滤波器。这些滤波器结合了传统图像处理滤波器的功能，提高了目标检测的性能。我们还提出了一种基于BPW滤波器的领域无关数据增强策略。我们的方法不需要针对特定数据集定制滤波器组合、参数范围和数据增强。我们将所提方法称为统一图像处理增强稳健性（ERUP）-YOLO，并将其应用于YOLOv3检测器。在恶劣天气数据集上的实验表明，我们所提出的滤波器与常规方法的表达能力相匹配，甚至更高，并且我们的ERUP-YOLO在包括雾和低光条件在内的各种恶劣天气条件下均取得了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02799v4">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong>：</p>
<p>针对恶劣天气条件（如雾和低光环境），我们提出了一种自适应图像的目标检测方法。该方法采用可微分预处理滤波器进行图像增强，为后期目标检测提供有力支持。研究中引入了两种可微分滤波器：基于Bézier曲线的像素级（BPW）滤波器和基于核的局部（KBL）滤波器。它们融合了传统图像处理滤波器的功能，提高了目标检测性能。同时，我们提出了一种基于BPW滤波器的通用数据增强策略。该方法无需针对特定数据定制滤波器组合、参数范围和数据增强方式。将该方法应用于YOLOv3检测器，实验表明，所提滤波器在恶劣天气数据集中的表现与常规方法相当或更优，ERUP-YOLO在雾和低光条件下的性能卓越。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了一种自适应图像的目标检测方法，适用于恶劣天气条件。</li>
<li>引入两种可微分预处理滤波器：BPW滤波器和KBL滤波器，融合传统图像处理功能。</li>
<li>采用BPW滤波器设计了一种通用数据增强策略，提高了模型在恶劣天气下的鲁棒性。</li>
<li>方法无需针对特定数据集定制滤波器组合、参数范围和数据增强方式。</li>
<li>实验表明，所提滤波器在恶劣天气数据集中的表现良好。</li>
<li>ERUP-YOLO在雾和低光条件下的性能卓越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4a7d5773f70439ce69b4de83b5e0161f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00731155fa4a62fd16e2a99ba37d853c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32177d17d29f08f59e63c30d1de02fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8868e47103cd1a3fc362b7f7ebeed214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69025a156fdbaef8d1a026fdb1df05f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="WeatherDG-LLM-assisted-Diffusion-Model-for-Procedural-Weather-Generation-in-Domain-Generalized-Semantic-Segmentation"><a href="#WeatherDG-LLM-assisted-Diffusion-Model-for-Procedural-Weather-Generation-in-Domain-Generalized-Semantic-Segmentation" class="headerlink" title="WeatherDG: LLM-assisted Diffusion Model for Procedural Weather   Generation in Domain-Generalized Semantic Segmentation"></a>WeatherDG: LLM-assisted Diffusion Model for Procedural Weather   Generation in Domain-Generalized Semantic Segmentation</h2><p><strong>Authors:Chenghao Qian, Yuhu Guo, Yuhong Mo, Wenjing Li</strong></p>
<p>In this work, we propose a novel approach, namely WeatherDG, that can generate realistic, weather-diverse, and driving-screen images based on the cooperation of two foundation models, i.e, Stable Diffusion (SD) and Large Language Model (LLM). Specifically, we first fine-tune the SD with source data, aligning the content and layout of generated samples with real-world driving scenarios. Then, we propose a procedural prompt generation method based on LLM, which can enrich scenario descriptions and help SD automatically generate more diverse, detailed images. In addition, we introduce a balanced generation strategy, which encourages the SD to generate high-quality objects of tailed classes under various weather conditions, such as riders and motorcycles. This segmentation-model-agnostic method can improve the generalization ability of existing models by additionally adapting them with the generated synthetic data. Experiments on three challenging datasets show that our method can significantly improve the segmentation performance of different state-of-the-art models on target domains. Notably, in the setting of ‘’Cityscapes to ACDC’’, our method improves the baseline HRDA by 13.9% in mIoU. </p>
<blockquote>
<p>在这项工作中，我们提出了一种新的方法，名为WeatherDG，它可以通过两个基础模型的合作，即Stable Diffusion（SD）和大型语言模型（LLM），生成现实、天气多样、驾驶屏图像。具体来说，我们首先使用源数据对SD进行微调，使生成样本的内容和布局与真实世界的驾驶场景对齐。然后，我们提出了一种基于LLM的程序化提示生成方法，可以丰富场景描述，帮助SD自动生成更多样化、更详细的图像。此外，我们引入了一种平衡生成策略，鼓励SD在各种天气条件下为长尾类生成高质量对象，如骑行者和摩托车。这种与分割模型无关的方法可以通过使用生成的合成数据额外适应现有模型，从而提高其泛化能力。在三个具有挑战性的数据集上的实验表明，我们的方法可以显著提高目标域上不同最新模型的分割性能。值得注意的是，在“城市景观到ACDC”的设置中，我们的方法将基线HRDA的mIoU提高了13.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12075v2">PDF</a> </p>
<p><strong>Summary</strong><br>本文提出了一种基于Stable Diffusion（SD）和Large Language Model（LLM）协同工作的新型图像生成方法WeatherDG。该方法能够生成逼真的、天气多变的驾驶场景图像。通过微调SD模型以匹配真实驾驶场景的内容和布局，并结合LLM生成丰富的场景描述，实现自动生成更多样化、更详细的图像。此外，引入了一种平衡生成策略，鼓励SD在多种天气条件下生成高质量的目标长尾类图像，如骑行者和摩托车。该方法对现有的模型具有良好的通用性，通过适应生成的合成数据，可在三个具有挑战性的数据集上的实验证明，能显著提高不同最先进模型的分割性能。特别是在“Cityscapes到ACDC”的设置中，该方法将基线HRDA提高了13.9%的mIoU。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了名为WeatherDG的新型图像生成方法，基于Stable Diffusion和Large Language Model。</li>
<li>通过微调SD模型，使生成的图像内容与真实驾驶场景布局相匹配。</li>
<li>利用LLM生成丰富的场景描述，使SD能够自动生成更多样化、详细的图像。</li>
<li>引入平衡生成策略，鼓励生成高质量的长尾类目标图像，如骑行者和摩托车。</li>
<li>方法具有良好的通用性，适用于多种模型。</li>
<li>实验表明，该方法在三个数据集上显著提高模型的分割性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12075">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ae1562108c1dde5b0be4cfcf06740ad2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6195b50573918a510c360235b6a960c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2dffa50a206bee5356c45af83b3c12e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca2f3f86ebb0a2d25a9078f27a3aebc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1c44c04809dda3b0df429ae01b56036.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0b7c55374f449d30412c6de02008e5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-089d6b8c1b18bf56a320853fed728a87.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi-scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve state-of-the-art results on benchmark datasets such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>少数语义分割（Few-shot Semantic Segmentation）旨在解决仅使用少量标注样本对查询图像进行对象分割的挑战。然而，许多之前的最先进方法要么需要丢弃复杂的局部语义特征，要么面临高计算复杂度的问题。为了应对这些挑战，我们提出了一种基于Transformer架构的少数语义分割新框架。我们的方法引入了空间变换解码器和上下文掩膜生成模块，以提高支持图像和查询图像之间的关系理解。此外，我们引入了多尺度解码器，以分层方式融入不同分辨率的特征来优化分割掩膜。同时，我们的方法融合了中间编码器阶段的全局特征以提高上下文理解，同时保持轻量级结构以降低复杂度。性能和效率之间的这种平衡使我们的方法在PASCAL-5i和COCO-20i等基准数据集上实现了最先进的成果，无论是在一次性拍摄（1-shot）还是五次拍摄（5-shot）的设置中都是如此。值得注意的是，我们的模型仅有150万参数，在克服现有方法局限性的同时表现出了竞争力。详情请访问：<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Transformer架构，提出新的少样本语义分割框架，通过空间变换解码器、上下文掩膜生成模块和多尺度解码器，改善支持图像和查询图像之间的关系理解，实现精细的分割掩膜。在PASCAL-5^i和COCO-20^i等基准数据集上实现了一阶和五阶情况下的业界领先结果，展现出高效且具备竞争力的性能。更多详情访问：<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet%E3%80%82">https://github.com/amirrezafateh/MSDNet。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>针对少样本语义分割的挑战，提出新的基于Transformer架构的框架。</li>
<li>引入空间变换解码器和上下文掩膜生成模块，改善支持图像和查询图像的关系理解。</li>
<li>通过多尺度解码器结合不同分辨率的特征，实现分割掩膜的精细化。</li>
<li>整合中间编码器阶段的全球特征，提升上下文理解。</li>
<li>保持轻量级结构以降低复杂性，在基准数据集上实现业界领先结果。</li>
<li>仅用150万参数便展现出竞争力的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7dfbf8a50110d45135537d58989963cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86f9eb0a4c352a4a2a0765cb5bfc2f60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4fa677efdc0fbf4dddf9a40b66fae11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55d5166f5ce99d76a8cae11e3ae5aff8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="NeRF-DetS-Enhanced-Adaptive-Spatial-wise-Sampling-and-View-wise-Fusion-Strategies-for-NeRF-based-Indoor-Multi-view-3D-Object-Detection"><a href="#NeRF-DetS-Enhanced-Adaptive-Spatial-wise-Sampling-and-View-wise-Fusion-Strategies-for-NeRF-based-Indoor-Multi-view-3D-Object-Detection" class="headerlink" title="NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion   Strategies for NeRF-based Indoor Multi-view 3D Object Detection"></a>NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion   Strategies for NeRF-based Indoor Multi-view 3D Object Detection</h2><p><strong>Authors:Chi Huang, Xinyang Li, Yansong Qu, Changli Wu, Xiaofan Li, Shengchuan Zhang, Liujuan Cao</strong></p>
<p>In indoor scenes, the diverse distribution of object locations and scales makes the visual 3D perception task a big challenge.   Previous works (e.g, NeRF-Det) have demonstrated that implicit representation has the capacity to benefit the visual 3D perception task in indoor scenes with high amount of overlap between input images.   However, previous works cannot fully utilize the advancement of implicit representation because of fixed sampling and simple multi-view feature fusion.   In this paper, inspired by sparse fashion method (e.g, DETR3D), we propose a simple yet effective method, NeRF-DetS, to address above issues. NeRF-DetS includes two modules: Progressive Adaptive Sampling Strategy (PASS) and Depth-Guided Simplified Multi-Head Attention Fusion (DS-MHA).   Specifically,   (1)PASS can automatically sample features of each layer within a dense 3D detector, using offsets predicted by the previous layer.   (2)DS-MHA can not only efficiently fuse multi-view features with strong occlusion awareness but also reduce computational cost.   Extensive experiments on ScanNetV2 dataset demonstrate our NeRF-DetS outperforms NeRF-Det, by achieving +5.02% and +5.92% improvement in mAP under IoU25 and IoU50, respectively. Also, NeRF-DetS shows consistent improvements on ARKITScenes. </p>
<blockquote>
<p>在室内场景中，物体位置和尺度的多样分布使得视觉3D感知任务面临巨大挑战。先前的工作（例如NeRF-Det）已经证明，隐式表示有助于室内场景的视觉3D感知任务，尤其是在输入图像之间存在大量重叠的情况下。然而，由于固定的采样和简单的多视图特征融合，先前的工作无法充分利用隐式表示的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13921v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了室内场景中视觉3D感知的挑战，包括对象位置和尺度的多样性。受稀疏方法（如DETR3D）的启发，提出了一种简单有效的方法NeRF-DetS，包括渐进自适应采样策略（PASS）和深度引导简化多头注意力融合（DS-MHA）。实验表明，NeRF-DetS在ScanNetV2数据集上的性能优于NeRF-Det，在IoU25和IoU50下的mAP分别提高了+5.02%和+5.92%，并且在ARKITScenes上也表现出一致的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>室内场景中，对象位置和尺度的多样性使得视觉3D感知任务充满挑战。</li>
<li>之前的作品（如NeRF-Det）已经证明了隐式表示对室内场景视觉3D感知任务的潜力。</li>
<li>NeRF-DetS方法通过结合渐进自适应采样策略和深度引导简化多头注意力融合来解决先前方法的问题。</li>
<li>PASS能够自动在密集3D检测器中的每一层采样特征。</li>
<li>DS-MHA能够高效地融合多视图特征，具有强大的遮挡意识，并降低计算成本。</li>
<li>在ScanNetV2数据集上的实验表明，NeRF-DetS的性能优于NeRF-Det，并且在ARKITScenes上也有一致的提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13921">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-306731d7f1d69eb434e06e08799097f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cfddae21ae946b554002b6fbfddba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81289d67066949786f358bca9834b9d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-584e618350edd2bbd3f4bc1b85781e60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ee6efda62aa34115ce79937185712e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Language-Guided-Instance-Aware-Domain-Adaptive-Panoptic-Segmentation"><a href="#Language-Guided-Instance-Aware-Domain-Adaptive-Panoptic-Segmentation" class="headerlink" title="Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation"></a>Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation</h2><p><strong>Authors:Elham Amin Mansour, Ozan Unal, Suman Saha, Benjamin Bejar, Luc Van Gool</strong></p>
<p>The increasing relevance of panoptic segmentation is tied to the advancements in autonomous driving and AR&#x2F;VR applications. However, the deployment of such models has been limited due to the expensive nature of dense data annotation, giving rise to unsupervised domain adaptation (UDA). A key challenge in panoptic UDA is reducing the domain gap between a labeled source and an unlabeled target domain while harmonizing the subtasks of semantic and instance segmentation to limit catastrophic interference. While considerable progress has been achieved, existing approaches mainly focus on the adaptation of semantic segmentation. In this work, we focus on incorporating instance-level adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix significantly enhances the panoptic quality by improving instance segmentation performance. Specifically, we propose inserting high-confidence predicted instances from the target domain onto source images, retaining the exhaustiveness of the resulting pseudo-labels while reducing the injected confirmation bias. Nevertheless, such an enhancement comes at the cost of degraded semantic performance, attributed to catastrophic forgetting. To mitigate this issue, we regularize our semantic branch by employing CLIP-based domain alignment (CDA), exploiting the domain-robustness of natural language prompts. Finally, we present an end-to-end model incorporating these two mechanisms called LIDAPS, achieving state-of-the-art results on all popular panoptic UDA benchmarks. </p>
<blockquote>
<p>全景分割的日益重要性是与自动驾驶和AR&#x2F;VR应用的进步相联系的。然而，由于密集数据标注的高昂成本，此类模型的部署受到限制，催生了无监督域自适应（UDA）的出现。全景UDA的关键挑战在于缩小有标签源域和无标签目标域之间的域差距，同时协调语义分割和实例分割的子任务，以避免灾难性干扰。虽然已取得了相当大的进展，但现有方法主要集中在语义分割的适应上。在这项工作中，我们专注于通过一种新型实例感知跨域混合策略IMix，融入实例级适应。IMix通过提高实例分割性能，显著提高了全景质量。具体来说，我们提出了一种将目标域的高置信度预测实例插入源图像的方法，保留了所得伪标签的详尽性，同时减少了注入的确认偏见。然而，这种增强是以语义性能下降为代价的，这是由于灾难性遗忘造成的。为了缓解这个问题，我们通过采用基于CLIP的域对齐（CDA）来规范我们的语义分支，利用自然语言提示的域稳健性。最后，我们提出了一种端到端的模型，该模型融合了这两种机制，称为LIDAPS，在所有人气旺盛的全景UDA基准测试上均达到最新水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.03799v2">PDF</a> Accepted at the 2025 IEEE&#x2F;CVF Winter Conference on Applications of   Computer Vision (WACV)</p>
<p><strong>Summary</strong>：随着自动驾驶和AR&#x2F;VR应用的不断发展，全景分割的重要性日益凸显。然而，由于密集数据标注的高成本，全景分割模型的部署受到限制，这推动了无监督域自适应（UDA）的研究。本文关注无监督域自适应中的全景分割问题，提出了一种结合实例级自适应的新方法IMix，通过插入目标域的高置信度预测实例到源图像中，提高了全景分割的质量。然而，这可能导致语义性能下降，出现灾难性遗忘问题。为解决这一问题，本文采用基于CLIP的域对齐（CDA）方法，利用自然语言提示的域稳健性进行语义分支正则化。最终，本文提出了一种结合这两种机制的端到端模型LIDAPS，在主流全景UDA基准测试上实现最佳结果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>全景分割在自动驾驶和AR&#x2F;VR应用中越来越重要，但密集数据标注的高成本限制了其模型部署。</li>
<li>无监督域自适应（UDA）是解决全景分割中跨域问题的一种有效方法。</li>
<li>IMix方法通过实例级自适应提高全景分割质量，通过将目标域的高置信度预测实例插入源图像中实现。</li>
<li>IMix方法可能导致语义性能下降和灾难性遗忘问题。</li>
<li>为解决语义性能下降问题，采用基于CLIP的域对齐（CDA）方法进行语义分支正则化。</li>
<li>LIDAPS模型结合IMix和CDA机制，实现了在主流全景UDA基准测试上的最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.03799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b8eecd2ace0d93f212a0117545242e16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40dcc694e54acb6012e594336a89a4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad641fe06dbb25a6e31d5b8b5de3ede.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47d517cb53d9dfe49bb1dd90de881938.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-21b174cacbba0e2e635a191464151bfa.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-01-02  Distilled Transformers with Locally Enhanced Global Representations for   Face Forgery Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-01-02  Multi-Modality Driven LoRA for Adverse Condition Depth Estimation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">10242.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
