<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  SM3Det A Unified Model for Multi-Modal Remote Sensing Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="SM3Det-A-Unified-Model-for-Multi-Modal-Remote-Sensing-Object-Detection"><a href="#SM3Det-A-Unified-Model-for-Multi-Modal-Remote-Sensing-Object-Detection" class="headerlink" title="SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection"></a>SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection</h2><p><strong>Authors:Yuxuan Li, Xiang Li, Yunheng Li, Yicheng Zhang, Yimian Dai, Qibin Hou, Ming-Ming Cheng, Jian Yang</strong></p>
<p>With the rapid advancement of remote sensing technology, high-resolution multi-modal imagery is now more widely accessible. Conventional Object detection models are trained on a single dataset, often restricted to a specific imaging modality and annotation format. However, such an approach overlooks the valuable shared knowledge across multi-modalities and limits the modelâ€™s applicability in more versatile scenarios. This paper introduces a new task called Multi-Modal Datasets and Multi-Task Object Detection (M2Det) for remote sensing, designed to accurately detect horizontal or oriented objects from any sensor modality. This task poses challenges due to 1) the trade-offs involved in managing multi-modal modelling and 2) the complexities of multi-task optimization. To address these, we establish a benchmark dataset and propose a unified model, SM3Det (Single Model for Multi-Modal datasets and Multi-Task object Detection). SM3Det leverages a grid-level sparse MoE backbone to enable joint knowledge learning while preserving distinct feature representations for different modalities. Furthermore, it integrates a consistency and synchronization optimization strategy using dynamic learning rate adjustment, allowing it to effectively handle varying levels of learning difficulty across modalities and tasks. Extensive experiments demonstrate SM3Detâ€™s effectiveness and generalizability, consistently outperforming specialized models on individual datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">https://github.com/zcablii/SM3Det</a>. </p>
<blockquote>
<p>éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å›¾åƒç°åœ¨æ›´åŠ æ™®åŠã€‚ä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹æ˜¯åœ¨å•ä¸€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œé€šå¸¸å±€é™äºç‰¹å®šçš„æˆåƒæ–¹å¼å’Œæ³¨é‡Šæ ¼å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†å¤šæ¨¡æ€ä¹‹é—´å®è´µçš„å…±äº«çŸ¥è¯†ï¼Œå¹¶é™åˆ¶äº†æ¨¡å‹åœ¨æ›´é€šç”¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿçš„æ–°ä»»åŠ¡ï¼Œç§°ä¸ºå¤šæ¨¡æ€æ•°æ®é›†å’Œå¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹ï¼ˆM2Detï¼‰ã€‚æ­¤ä»»åŠ¡æ—¨åœ¨å‡†ç¡®æ£€æµ‹æ¥è‡ªä»»ä½•ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ°´å¹³æˆ–å®šå‘ç›®æ ‡ã€‚æ­¤ä»»åŠ¡é¢ä¸´ç€ä¸¤ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰æ¶‰åŠå¤šæ¨¡æ€å»ºæ¨¡çš„æƒè¡¡é—®é¢˜ï¼›2 2ï¼‰å¤šä»»åŠ¡ä¼˜åŒ–çš„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹SM3Detï¼ˆå¤šæ¨¡æ€æ•°æ®é›†å’Œå¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹çš„å•æ¨¡å‹ï¼‰ã€‚SM3Detåˆ©ç”¨ç½‘æ ¼çº§ç¨€ç–MoEéª¨å¹²ç½‘å®ç°è”åˆçŸ¥è¯†å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™ä¸åŒæ¨¡æ€çš„ä¸åŒç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´çš„ä¸€è‡´æ€§åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒæ¨¡æ€å’Œä»»åŠ¡ä¹‹é—´ä¸åŒç­‰çº§çš„å­¦ä¹ éš¾åº¦ã€‚å¤§é‡å®éªŒè¯æ˜äº†SM3Detçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œåœ¨å•ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¸“ä¸šæ¨¡å‹ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">https://github.com/zcablii/SM3Det</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20665v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å›¾åƒç°åœ¨æ›´åŠ æ˜“äºè·å–ã€‚å¸¸è§„ç›®æ ‡æ£€æµ‹æ¨¡å‹é€šå¸¸ä»…åœ¨å•ä¸€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå—é™äºç‰¹å®šçš„æˆåƒæ–¹å¼å’Œæ³¨é‡Šæ ¼å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†å¤šæ¨¡æ€ä¹‹é—´çš„å…±äº«çŸ¥è¯†ï¼Œå¹¶é™åˆ¶äº†æ¨¡å‹åœ¨æ›´é€šç”¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†é¥æ„Ÿä¸­çš„å¤šæ¨¡æ€æ•°æ®é›†å¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹ï¼ˆM2Detï¼‰æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨ä»ä»»ä½•ä¼ æ„Ÿå™¨æ¨¡æ€å‡†ç¡®æ£€æµ‹æ°´å¹³æˆ–å®šå‘ç›®æ ‡ã€‚è¯¥ä»»åŠ¡é¢ä¸´ç®¡ç†å¤šæ¨¡æ€å»ºæ¨¡å’Œå¤šä»»åŠ¡ä¼˜åŒ–çš„æƒè¡¡é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹SM3Detã€‚SM3Detåˆ©ç”¨ç½‘æ ¼çº§ç¨€ç–MoEä¸»å¹²å®ç°è”åˆçŸ¥è¯†å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™ä¸åŒæ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨ä¸€è‡´æ€§åŒæ­¥ä¼˜åŒ–ç­–ç•¥åŠåŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´ï¼Œæœ‰æ•ˆå¤„ç†ä¸åŒæ¨¡æ€å’Œä»»åŠ¡çš„å­¦ä¹ éš¾åº¦å·®å¼‚ã€‚å®éªŒè¯æ˜SM3Detçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œåœ¨ä¸ªåˆ«æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¸“ä¸šæ¨¡å‹ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„ŸæŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å›¾åƒå¹¿æ³›å¯è·å–ã€‚</li>
<li>å¸¸è§„ç›®æ ‡æ£€æµ‹æ¨¡å‹é€šå¸¸åœ¨å•ä¸€æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œé™åˆ¶å…¶åœ¨å¤šæ¨¡æ€å’Œæ›´é€šç”¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li>
<li>å¼•å…¥æ–°çš„ä»»åŠ¡â€”â€”å¤šæ¨¡æ€æ•°æ®é›†å¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹ï¼ˆM2Detï¼‰ï¼Œæ—¨åœ¨ä»ä»»ä½•ä¼ æ„Ÿå™¨æ¨¡æ€å‡†ç¡®æ£€æµ‹ç›®æ ‡ã€‚</li>
<li>M2Deté¢ä¸´ç®¡ç†å¤šæ¨¡æ€å»ºæ¨¡å’Œå¤šä»»åŠ¡ä¼˜åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºSM3Detæ¨¡å‹ï¼Œåˆ©ç”¨ç½‘æ ¼çº§ç¨€ç–MoEä¸»å¹²å®ç°è”åˆçŸ¥è¯†å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™ä¸åŒæ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>SM3Deté‡‡ç”¨ä¸€è‡´æ€§åŒæ­¥ä¼˜åŒ–ç­–ç•¥åŠåŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´ï¼Œä»¥å¤„ç†ä¸åŒæ¨¡æ€å’Œä»»åŠ¡çš„å­¦ä¹ éš¾åº¦å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dabf692458edb22d114cba6e6c0a6714.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fb90ef035e0f6d3047bfa57a2b1db30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c8676918f0c1ace9673fbb9506a9c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a936ee18d0195a01bc008a5242370e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a50e8092c61b403e38354666349b94f5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="YOLO-UniOW-Efficient-Universal-Open-World-Object-Detection"><a href="#YOLO-UniOW-Efficient-Universal-Open-World-Object-Detection" class="headerlink" title="YOLO-UniOW: Efficient Universal Open-World Object Detection"></a>YOLO-UniOW: Efficient Universal Open-World Object Detection</h2><p><strong>Authors:Lihao Liu, Juexiao Feng, Hui Chen, Ao Wang, Lin Song, Jungong Han, Guiguang Ding</strong></p>
<p>Traditional object detection models are constrained by the limitations of closed-set datasets, detecting only categories encountered during training. While multimodal models have extended category recognition by aligning text and image modalities, they introduce significant inference overhead due to cross-modality fusion and still remain restricted by predefined vocabulary, leaving them ineffective at handling unknown objects in open-world scenarios. In this work, we introduce Universal Open-World Object Detection (Uni-OWD), a new paradigm that unifies open-vocabulary and open-world object detection tasks. To address the challenges of this setting, we propose YOLO-UniOW, a novel model that advances the boundaries of efficiency, versatility, and performance. YOLO-UniOW incorporates Adaptive Decision Learning to replace computationally expensive cross-modality fusion with lightweight alignment in the CLIP latent space, achieving efficient detection without compromising generalization. Additionally, we design a Wildcard Learning strategy that detects out-of-distribution objects as â€œunknownâ€ while enabling dynamic vocabulary expansion without the need for incremental learning. This design empowers YOLO-UniOW to seamlessly adapt to new categories in open-world environments. Extensive experiments validate the superiority of YOLO-UniOW, achieving achieving 34.6 AP and 30.0 APr on LVIS with an inference speed of 69.6 FPS. The model also sets benchmarks on M-OWODB, S-OWODB, and nuScenes datasets, showcasing its unmatched performance in open-world object detection. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/THU-MIG/YOLO-UniOW">https://github.com/THU-MIG/YOLO-UniOW</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹å—é™äºå°é—­æ•°æ®é›†ï¼Œåªèƒ½æ£€æµ‹è®­ç»ƒæ—¶é‡åˆ°çš„ç›®æ ‡ç±»åˆ«ã€‚è™½ç„¶å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„å¯¹é½æ‰©å±•äº†ç±»åˆ«è¯†åˆ«èƒ½åŠ›ï¼Œä½†ç”±äºè·¨æ¨¡æ€èåˆè€Œå¼•å…¥äº†æ˜¾è‘—çš„æ¨ç†å¼€é”€ï¼Œå¹¶ä¸”ä»ç„¶å—åˆ°é¢„è®¾è¯æ±‡è¡¨çš„é™åˆ¶ï¼Œæ— æ³•å¤„ç†å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„æœªçŸ¥å¯¹è±¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»Ÿä¸€å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ï¼ˆUni-OWDï¼‰è¿™ä¸€æ–°èŒƒå¼ï¼Œå°†å¼€æ”¾è¯æ±‡è¡¨å’Œå¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ç»Ÿä¸€èµ·æ¥ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€è®¾ç½®ä¸­çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†YOLO-UniOWè¿™ä¸€æ–°å‹æ¨¡å‹ï¼Œåœ¨æ•ˆç‡ã€é€šç”¨æ€§å’Œæ€§èƒ½ä¸Šæ¨è¿›äº†è¾¹ç•Œã€‚YOLO-UniOWç»“åˆäº†è‡ªé€‚åº”å†³ç­–å­¦ä¹ ï¼Œç”¨è½»é‡çº§çš„å¯¹é½æ›¿æ¢è®¡ç®—æ˜‚è´µçš„è·¨æ¨¡æ€èåˆï¼Œåœ¨CLIPæ½œåœ¨ç©ºé—´ä¸­å®ç°é«˜æ•ˆæ£€æµ‹è€Œä¸æŸå®³æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†Wildcardå­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤Ÿæ£€æµ‹åˆ†å¸ƒå¤–çš„å¯¹è±¡å¹¶å°†å…¶æ ‡è®°ä¸ºâ€œæœªçŸ¥â€ï¼ŒåŒæ—¶å®ç°åŠ¨æ€è¯æ±‡æ‰©å±•è€Œæ— éœ€å¢é‡å­¦ä¹ ã€‚è¿™ä¸€è®¾è®¡ä½¿YOLO-UniOWèƒ½å¤Ÿè½»æ¾é€‚åº”å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„æ–°ç±»åˆ«ã€‚å¤§é‡å®éªŒéªŒè¯äº†YOLO-UniOWçš„ä¼˜è¶Šæ€§ï¼Œåœ¨LVISä¸Šå®ç°äº†34.6çš„APå’Œ30.0çš„APrï¼Œæ¨ç†é€Ÿåº¦ä¸ºæ¯ç§’69.6å¸§ã€‚è¯¥æ¨¡å‹è¿˜åœ¨M-OWODBã€S-OWODBå’ŒnuScenesæ•°æ®é›†ä¸Šè®¾å®šäº†åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ä¸­çš„å“è¶Šæ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/THU-MIG/YOLO-UniOW%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THU-MIG/YOLO-UniOWè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20645v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹å—é™äºå°é—­æ•°æ®é›†ï¼Œåªèƒ½æ£€æµ‹è®­ç»ƒæ—¶é‡åˆ°çš„ç›®æ ‡ç±»åˆ«ã€‚å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„å¯¹é½æ‰©å±•äº†ç±»åˆ«è¯†åˆ«èƒ½åŠ›ï¼Œä½†å¼•å…¥äº†ç”±äºè·¨æ¨¡æ€èåˆé€ æˆçš„æ¨ç†å¼€é”€ï¼Œå¹¶ä¸”ä»ç„¶å—é™äºé¢„è®¾è¯æ±‡è¡¨ï¼Œæ— æ³•å¤„ç†å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„æœªçŸ¥å¯¹è±¡ã€‚æœ¬æ–‡æå‡ºç»Ÿä¸€å¼€æ”¾è¯æ±‡å’Œå¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„æ–°èŒƒå¼â€”â€”é€šç”¨å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ï¼ˆUni-OWDï¼‰ã€‚é’ˆå¯¹æ­¤è®¾ç½®ï¼Œæˆ‘ä»¬æå‡ºäº†YOLO-UniOWæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ•ˆç‡ã€é€šç”¨æ€§å’Œæ€§èƒ½æ–¹é¢å–å¾—äº†çªç ´ã€‚YOLO-UniOWé‡‡ç”¨è‡ªé€‚åº”å†³ç­–å­¦ä¹ ï¼Œä»¥è½»é‡çº§å¯¹é½æ›¿æ¢è®¡ç®—æ˜‚è´µçš„è·¨æ¨¡æ€èåˆï¼Œåœ¨CLIPæ½œåœ¨ç©ºé—´ä¸­å®ç°é«˜æ•ˆæ£€æµ‹è€Œä¸å½±å“æ³›åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†Wildcardå­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤Ÿæ£€æµ‹å¼‚å¸¸å¯¹è±¡å¹¶å°†å…¶æ ‡è®°ä¸ºâ€œæœªçŸ¥â€ï¼ŒåŒæ—¶å®ç°æ— éœ€å¢é‡å­¦ä¹ çš„åŠ¨æ€è¯æ±‡æ‰©å±•ã€‚è¿™ä½¿å¾—YOLO-UniOWèƒ½å¤Ÿè½»æ¾é€‚åº”å¼€æ”¾ç¯å¢ƒä¸­çš„æ–°ç±»åˆ«ã€‚ç»è¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼ŒYOLO-UniOWåœ¨LVISæ•°æ®é›†ä¸Šå–å¾—äº†34.6çš„APå’Œ30.0çš„APrï¼Œæ¨ç†é€Ÿåº¦ä¸º69.6 FPSã€‚è¯¥æ¨¡å‹è¿˜åœ¨M-OWODBã€S-OWODBå’ŒnuScenesæ•°æ®é›†ä¸Šè®¾å®šäº†åŸºå‡†ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ä¸­çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹å—é™äºå°é—­æ•°æ®é›†ï¼Œéš¾ä»¥åº”å¯¹å¼€æ”¾ä¸–ç•Œä¸­çš„æœªçŸ¥å¯¹è±¡ã€‚</li>
<li>YOLO-UniOWæ¨¡å‹é€šè¿‡ç»“åˆè‡ªé€‚åº”å†³ç­–å­¦ä¹ å’ŒWildcardå­¦ä¹ ç­–ç•¥ï¼Œå®ç°äº†å¼€æ”¾è¯æ±‡å’Œå¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹çš„ç»Ÿä¸€ã€‚</li>
<li>YOLO-UniOWé‡‡ç”¨è½»é‡çº§å¯¹é½æ–¹å¼ï¼Œåœ¨CLIPæ½œåœ¨ç©ºé—´ä¸­å®ç°é«˜æ•ˆæ£€æµ‹ã€‚</li>
<li>Wildcardå­¦ä¹ ç­–ç•¥èƒ½æ£€æµ‹å¼‚å¸¸å¯¹è±¡å¹¶æ ‡è®°ä¸ºâ€œæœªçŸ¥â€ï¼ŒåŒæ—¶å®ç°åŠ¨æ€è¯æ±‡æ‰©å±•ã€‚</li>
<li>YOLO-UniOWåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬LVISã€M-OWODBã€S-OWODBå’ŒnuScenesã€‚</li>
<li>YOLO-UniOWæ¨¡å‹å…·æœ‰é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°69.6 FPSã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-50a73c4303f17df361e9de1537973b6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2ea5548c68a0b9c7c37733adc5fcbe1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ddd7fe86b772fdac7dd17756a55d4d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b9a23b21d39012b8149e281f6c8837.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation Agent for Weakly Supervised Semantic Segmentation"></a>Image Augmentation Agent for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„WSSSæ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°æ¥ç”Ÿæˆæ›´ç²¾ç¡®çš„å¯†é›†æ ‡ç­¾ï¼Œè€Œå¿½è§†äº†å›ºå®šæ•°æ®é›†æ‰€å¸¦æ¥çš„é™åˆ¶ï¼Œè¿™äº›é™åˆ¶å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½çš„æå‡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæä¾›æ›´å¤šå¯è®­ç»ƒå›¾åƒçš„å¤šæ ·æ€§å¯ä»¥ä¸ºWSSSæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶å¸®åŠ©æ¨¡å‹ç†è§£æ›´å…¨é¢çš„è¯­ä¹‰æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºå›¾åƒå¢å¼ºä»£ç†ï¼ˆIAAï¼‰ï¼Œå®ƒè¡¨æ˜ä»æ•°æ®ç”Ÿæˆçš„è§’åº¦å¢å¼ºWSSSæ˜¯å¯èƒ½çš„ã€‚IAAä¸»è¦è®¾è®¡äº†ä¸€ä¸ªå¢å¼ºä»£ç†ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹æ¥è‡ªåŠ¨ä¸ºWSSSç”Ÿæˆé¢å¤–çš„å›¾åƒã€‚åœ¨å®è·µä¸­ï¼Œä¸ºäº†è§£å†³LLMåœ¨æç¤ºç”Ÿæˆä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æç¤ºè‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶ã€‚å®ƒå…è®¸LLMé‡æ–°è¯„ä¼°ç”Ÿæˆçš„æç¤ºçš„åˆç†æ€§ï¼Œä»¥äº§ç”Ÿæ›´è¿è´¯çš„æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥äº†ä¸€ä¸ªåœ¨çº¿è¿‡æ»¤å™¨ï¼Œä»¥åŠ¨æ€ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„WSSSæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20439v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºImage Augmentation Agentï¼ˆIAAï¼‰çš„æ–¹æ³•ï¼Œä»æ•°æ®ç”Ÿæˆçš„è§’åº¦æå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢å¤–å›¾åƒï¼Œå¢å¼ºWSSSçš„è®­ç»ƒå›¾åƒå¤šæ ·æ€§ã€‚æå‡ºä¸€ç§æç¤ºè‡ªæˆ‘å®Œå–„æœºåˆ¶ï¼Œç¡®ä¿ç”Ÿæˆçš„æç¤ºæ›´åŠ åˆç†å’Œè¿è´¯ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰WSSSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IAAæ–¹æ³•ä»æ•°æ®ç”Ÿæˆè§’åº¦æå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢å¤–å›¾åƒï¼Œå¢åŠ è®­ç»ƒå›¾åƒå¤šæ ·æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æç¤ºè‡ªæˆ‘å®Œå–„æœºåˆ¶ï¼Œè§£å†³è¯­è¨€æ¨¡å‹æç¤ºç”Ÿæˆçš„ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>åœ¨åœ¨çº¿è¿‡æ»¤ä¸­ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡ã€‚</li>
<li>æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>çªç ´ç°æœ‰WSSSæ–¹æ³•çš„å±€é™ï¼Œä¸ä»…ä»…å…³æ³¨ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af2a3ebd09a1bf1b97a1d039283df260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d420af9d7c6e4b553c221ca8efb92b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50df88fe53c498b6f64d28ef4ad06123.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7d6bc9d163130f0f670b8ebe70adb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4acc489d52ac34b0db3e21364468b2a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP"><a href="#Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP" class="headerlink" title="Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP"></a>Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP</h2><p><strong>Authors:Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge</strong></p>
<p>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ç ”ç©¶ä¸­çš„åº”ç”¨å±•ç°äº†å¼ºå¤§çš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£åŠŸèƒ½ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾ä¼˜åŒ–è¾“å…¥æ–‡æœ¬æç¤ºï¼Œä»¥æ”¹è¿›å›¾åƒå’Œæ–‡æœ¬çš„å¯¹é½æ–¹å¼ï¼Œé€šè¿‡å¾®è°ƒæ–‡æœ¬åŸå‹æ¥ä¿ƒè¿›è¯­ä¹‰åŒ¹é…ã€‚ç„¶è€Œï¼Œé‰´äºæ–‡æœ¬å’Œè§†è§‰ç©ºé—´ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œè¿™äº›æ–¹æ³•æ‰€é‡‡ç”¨çš„æ–‡æœ¬åŸå‹å¹¶æœªæœ‰æ•ˆåœ°ä¸åƒç´ çº§è§†è§‰ç‰¹å¾å»ºç«‹ç´§å¯†å¯¹åº”å…³ç³»ã€‚åœ¨æˆ‘ä»¬çš„ç†è®ºåˆ†æä¸­ï¼Œå›ºæœ‰çš„æ¨¡æ€å·®è·ä¼šå¯¼è‡´æ–‡æœ¬å’ŒåŒºåŸŸç‰¹å¾çš„é”™ä½ï¼Œå¹¶ä¸”ä»…ä»…é€šè¿‡æœ€å°åŒ–CLIPä¸­çš„å¯¹æ¯”æŸå¤±ä¸è¶³ä»¥ç¼©å°è¿™ä¸€å·®è·ã€‚ä¸ºäº†å‡è½»æ¨¡æ€å·®è·çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åŸå‹å­¦ä¹ ï¼ˆVPLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ›´å…·ä»£è¡¨æ€§çš„è§†è§‰åŸå‹ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åœ¨è§†è§‰ç©ºé—´çš„å¸®åŠ©ä¸‹ï¼Œå­¦ä¹ ç‰¹å®šç±»åˆ«çš„è§†è§‰åŸå‹ï¼Œä»¥æ•è·é«˜è´¨é‡çš„å®šä½å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŒºåŸŸè¯­ä¹‰å¯¹æ¯”æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åŒºåŸŸåµŒå…¥ä¸ç›¸åº”çš„åŸå‹è¿›è¡Œå¯¹æ¯”ï¼Œä»è€Œå®ç°æ›´å…¨é¢å’Œç¨³å¥çš„ç‰¹å¾å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19650v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åº”ç”¨Contrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰äºWeakly Supervised Semantic Segmentationï¼ˆWSSSï¼‰ä¸­ï¼Œå®ç°è·¨æ¨¡æ€è¯­ä¹‰ç†è§£ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¾®è°ƒæ–‡æœ¬æç¤ºä¼˜åŒ–å›¾åƒä¸æ–‡æœ¬çš„åŒ¹é…åº¦ï¼Œä½†æ¨¡æ€é—´çš„å·®è·å¯¼è‡´æ–‡æœ¬åŸå‹æœªèƒ½ä¸åƒç´ çº§è§†è§‰ç‰¹å¾å»ºç«‹ç´§å¯†å¯¹åº”å…³ç³»ã€‚æœ¬ç ”ç©¶æå‡ºVision Prototype Learningï¼ˆVPLï¼‰æ¡†æ¶ï¼Œå¼•å…¥æ›´å…·ä»£è¡¨æ€§çš„è§†è§‰åŸå‹ï¼Œä»¥å­¦ä¹ ç‰¹å®šç±»åˆ«çš„è§†è§‰ç‰¹å¾ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§åŒºåŸŸè¯­ä¹‰å¯¹æ¯”æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯¹æ¯”åŒºåŸŸåµŒå…¥ä¸å¯¹åº”åŸå‹çš„ç‰¹å¾ï¼Œå®ç°æ›´å…¨é¢å’Œé²æ£’çš„ç‰¹å¾å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤é¡¹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨WSSSç ”ç©¶ä¸­çš„åº”ç”¨å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£åŠŸèƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å¾®è°ƒæ–‡æœ¬æç¤ºæ¥ä¼˜åŒ–å›¾åƒå’Œæ–‡æœ¬çš„åŒ¹é…åº¦ã€‚</li>
<li>æ¨¡æ€å·®è·å¯¼è‡´æ–‡æœ¬åŸå‹æœªèƒ½æœ‰æ•ˆåœ°ä¸åƒç´ çº§è§†è§‰ç‰¹å¾ç›¸å¯¹åº”ã€‚</li>
<li>å¼•å…¥VPLæ¡†æ¶ä»¥å­¦ä¹ æ›´å…·ä»£è¡¨æ€§çš„è§†è§‰åŸå‹ã€‚</li>
<li>VPLæ¡†æ¶å€ŸåŠ©æ–‡æœ¬åŸå‹åœ¨è§†è§‰ç©ºé—´å†…å­¦ä¹ ç‰¹å®šç±»åˆ«çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>è®¾è®¡äº†åŒºåŸŸè¯­ä¹‰å¯¹æ¯”æ¨¡å—ï¼Œå®ç°æ›´å…¨é¢å’Œé²æ£’çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90ee0f753b63bdc6501a80c978c40689.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b4383b6e1ce4886b79ce30a149d538c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6d4d4b8aed96474ae553e45f0a7d71.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Open-Vocabulary-Remote-Sensing-Image-Semantic-Segmentation"><a href="#Towards-Open-Vocabulary-Remote-Sensing-Image-Semantic-Segmentation" class="headerlink" title="Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation"></a>Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation</h2><p><strong>Authors:Chengyang Ye, Yunzhi Zhuge, Pingping Zhang</strong></p>
<p>Recently, deep learning based methods have revolutionized remote sensing image segmentation. However, these methods usually rely on a pre-defined semantic class set, thus needing additional image annotation and model training when adapting to new classes. More importantly, they are unable to segment arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary semantic classes in remote sensing images. To address the lack of OVRSISS datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images covering 40 diverse semantic classes. In addition, we propose a novel framework named GSNet that integrates domain priors from special remote sensing models and versatile capabilities of general vision-language models. Technically, GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE first captures comprehensive features from both special models and general models in dual streams. Then, with the guidance of variable vocabularies, QGFF integrates specialist and generalist features, enabling them to complement each other. Finally, RIPD is proposed to aggregate multi-source features for more accurate mask predictions. Experiments show that our method outperforms other methods by a large margin, and our proposed LandDiscover50K improves the performance of OVRSISS methods. The proposed dataset and method will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/yecy749/GSNet">https://github.com/yecy749/GSNet</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²é¢†åŸŸæ€èµ·äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„è¯­ä¹‰ç±»åˆ«é›†ï¼Œå› æ­¤åœ¨é€‚åº”æ–°ç±»åˆ«æ—¶éœ€è¦é¢å¤–çš„å›¾åƒæ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä»¬æ— æ³•å¯¹ä»»æ„è¯­ä¹‰ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆOVRSISSï¼‰ï¼Œæ—¨åœ¨å®ç°å¯¹é¥æ„Ÿå›¾åƒä¸­ä»»æ„è¯­ä¹‰ç±»åˆ«çš„åˆ†å‰²ã€‚ä¸ºäº†è§£å†³OVRSISSæ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†LandDiscover50Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«51,846å¼ å›¾åƒçš„ç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–äº†40ä¸ªå¤šæ ·åŒ–çš„è¯­ä¹‰ç±»åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶GSNetï¼Œå®ƒèåˆäº†ç‰¹æ®Šé¥æ„Ÿæ¨¡å‹çš„é¢†åŸŸå…ˆéªŒçŸ¥è¯†å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚æŠ€æœ¯ä¸Šï¼ŒGSNetåŒ…æ‹¬åŒæµå›¾åƒç¼–ç å™¨ï¼ˆDSIEï¼‰ã€æŸ¥è¯¢å¼•å¯¼ç‰¹å¾èåˆï¼ˆQGFFï¼‰å’Œæ®‹å·®ä¿¡æ¯ä¿ç•™è§£ç å™¨ï¼ˆRIPDï¼‰ã€‚é¦–å…ˆï¼ŒDSIEä»ç‰¹æ®Šæ¨¡å‹å’Œä¸€èˆ¬æ¨¡å‹çš„åŒæµä¸­æ•è·å…¨é¢çš„ç‰¹å¾ã€‚ç„¶åï¼Œåœ¨å¯å˜è¯æ±‡è¡¨çš„æŒ‡å¯¼ä¸‹ï¼ŒQGFFèåˆäº†ä¸“ä¸šç‰¹å¾å’Œé€šç”¨ç‰¹å¾ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç›¸äº’è¡¥å……ã€‚æœ€åï¼ŒRIPDè¢«æå‡ºæ¥èšåˆå¤šæºç‰¹å¾ï¼Œä»¥è¿›è¡Œæ›´å‡†ç¡®çš„æ©è†œé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–æ–¹æ³•çš„åŸºç¡€ä¸Šå–å¾—äº†å¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºçš„LandDiscover50Kä¹Ÿæé«˜äº†OVRSISSæ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†å’Œæ–¹æ³•å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yecy749/GSNet%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/yecy749/GSNetä¸Šå…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19492v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸæ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²é¢†åŸŸå¼•å‘é©å‘½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—é™äºé¢„å®šä¹‰è¯­ä¹‰ç±»åˆ«é›†ï¼Œéš¾ä»¥é€‚åº”æ–°ç±»åˆ«ã€‚æœ¬æ–‡æå‡ºå¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆOVRSISSï¼‰ï¼Œæ—¨åœ¨å®ç°å¯¹é¥æ„Ÿå›¾åƒä¸­çš„ä»»æ„è¯­ä¹‰ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚ä¸ºåº”å¯¹OVRSISSæ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼€å‘äº†LandDiscover50Kæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGSNetçš„æ–°æ¡†æ¶ï¼Œèåˆäº†ç‰¹æ®Šé¥æ„Ÿæ¨¡å‹çš„é¢†åŸŸå…ˆéªŒçŸ¥è¯†å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¤§å¹…ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒLandDiscover50Kæ•°æ®é›†æé«˜äº†OVRSISSæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºé¢„å®šä¹‰è¯­ä¹‰ç±»åˆ«ï¼Œéœ€é¢å¤–å›¾åƒæ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒä»¥é€‚åº”æ–°ç±»åˆ«ã€‚</li>
<li>æå‡ºå¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆOVRSISSï¼‰ä»¥åˆ†å‰²ä»»æ„è¯­ä¹‰ç±»åˆ«ã€‚</li>
<li>å¼€å‘LandDiscover50Kæ•°æ®é›†ï¼ŒåŒ…å«51,846å¼ å›¾åƒï¼Œè¦†ç›–40ä¸ªå¤šæ ·è¯­ä¹‰ç±»åˆ«ã€‚</li>
<li>æå‡ºGSNetæ¡†æ¶ï¼Œèåˆç‰¹æ®Šé¥æ„Ÿæ¨¡å‹å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>GSNetåŒ…æ‹¬åŒæµå›¾åƒç¼–ç å™¨ï¼ˆDSIEï¼‰ã€æŸ¥è¯¢å¼•å¯¼ç‰¹å¾èåˆï¼ˆQGFFï¼‰å’Œæ®‹å·®ä¿¡æ¯ä¿ç•™è§£ç å™¨ï¼ˆRIPDï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-611fb43ebba71a36ad42688e7c6a5822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7431be40eede724e0182b1a0b7a219b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff1a61fcc4ab227c5c72f28ac6a1f3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfc655f72b48818862a7c183d2d50477.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Coin-to-Data-The-Impact-of-Object-Detection-on-Digital-Numismatics"><a href="#From-Coin-to-Data-The-Impact-of-Object-Detection-on-Digital-Numismatics" class="headerlink" title="From Coin to Data: The Impact of Object Detection on Digital Numismatics"></a>From Coin to Data: The Impact of Object Detection on Digital Numismatics</h2><p><strong>Authors:Rafael Cabral, Maria De Iorio, Andrew Harris</strong></p>
<p>In this work we investigate the application of advanced object detection techniques to digital numismatics, focussing on the analysis of historical coins. Leveraging models such as Contrastive Language-Image Pre-training (CLIP), we develop a flexible framework for identifying and classifying specific coin features using both image and textual descriptions. By examining two distinct datasets, modern Russian coins featuring intricate â€œSaint George and the Dragonâ€ designs and degraded 1st millennium AD Southeast Asian coins bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection algorithms in search and classification tasks. Our results demonstrate the superior performance of larger CLIP models in detecting complex imagery, while traditional methods excel in identifying simple geometric patterns. Additionally, we propose a statistical calibration mechanism to enhance the reliability of similarity scores in low-quality datasets. This work highlights the transformative potential of integrating state-of-the-art object detection into digital numismatics, enabling more scalable, precise, and efficient analysis of historical artifacts. These advancements pave the way for new methodologies in cultural heritage research, artefact provenance studies, and the detection of forgeries. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯åœ¨æ•°å­—é’±å¸å­¦ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹åˆ†æäº†å†å²ç¡¬å¸ã€‚æˆ‘ä»¬åˆ©ç”¨è¯¸å¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬æè¿°æ¥è¯†åˆ«å’Œåˆ†ç±»ç‰¹å®šçš„ç¡¬å¸ç‰¹å¾ã€‚é€šè¿‡æ£€æŸ¥ä¸¤ä¸ªç‹¬ç‰¹çš„æ•°æ®é›†â€”â€”å…·æœ‰å¤æ‚â€œåœ£ä¹”æ²»å’Œé¾™â€è®¾è®¡çš„ç°ä»£ä¿„ç½—æ–¯ç¡¬å¸å’Œé€€åŒ–çš„ä¸€åƒå¹´å‰çš„ä¸œå—äºšå¸¦æœ‰å°åº¦æ•™ä½›æ•™ç¬¦å·çš„ç¡¬å¸ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒæ£€æµ‹ç®—æ³•åœ¨æœç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æ£€æµ‹å¤æ‚å›¾åƒæ–¹é¢ï¼Œè¾ƒå¤§çš„CLIPæ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•åœ¨è¯†åˆ«ç®€å•å‡ ä½•æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿè®¡æ ¡å‡†æœºåˆ¶ï¼Œä»¥æé«˜ä½è´¨é‡æ•°æ®é›†ä¸­ç›¸ä¼¼æ€§è¯„åˆ†çš„å¯é æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å°†æœ€æ–°ç›®æ ‡æ£€æµ‹æŠ€æœ¯é›†æˆåˆ°æ•°å­—é’±å¸å­¦ä¸­æ‰€å¸¦æ¥çš„å˜é©æ€§æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°æ›´å¤§è§„æ¨¡ã€æ›´ç²¾ç¡®ã€æ›´é«˜æ•ˆçš„å†å²æ–‡ç‰©åˆ†æã€‚è¿™äº›è¿›æ­¥ä¸ºæ–‡åŒ–é—äº§ç ”ç©¶ã€æ–‡ç‰©åŸäº§åœ°ç ”ç©¶å’Œä¼ªé€ æ£€æµ‹ç­‰é¢†åŸŸå¼€è¾Ÿäº†æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯åœ¨æ•°å­—é’±å¸å­¦ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹åˆ†æäº†å¯¹å†å²ç¡¬å¸çš„åˆ†æã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ä¸ªçµæ´»æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å›¾åƒå’Œæ–‡æœ¬æè¿°æ¥è¯†åˆ«å’Œåˆ†ç±»ç¡¬å¸ç‰¹å¾ã€‚é€šè¿‡å¯¹å¸¦æœ‰å¤æ‚â€œåœ£ä¹”æ²»ä¸é¾™â€è®¾è®¡çš„ç°ä»£ä¿„ç½—æ–¯ç¡¬å¸å’Œå¸¦æœ‰å°åº¦æ•™ä½›æ•™ç¬¦å·çš„1ä¸–çºªä¸œå—äºšç¡¬å¸ä¸¤ä¸ªæ•°æ®é›†çš„ç ”ç©¶ï¼Œè¯„ä¼°äº†ä¸åŒæ£€æµ‹ç®—æ³•åœ¨æœç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºå¤§å‹CLIPæ¨¡å‹åœ¨æ£€æµ‹å¤æ‚å›¾åƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•åœ¨è¯†åˆ«ç®€å•å‡ ä½•å›¾æ¡ˆæ–¹é¢æ›´å‡ºè‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç»Ÿè®¡æ ¡å‡†æœºåˆ¶ï¼Œä»¥æé«˜ä½è´¨é‡æ•°æ®é›†ä¸­ç›¸ä¼¼åº¦è¯„åˆ†çš„å¯é æ€§ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†å°†æœ€æ–°ç›®æ ‡æ£€æµ‹æŠ€æœ¯èå…¥æ•°å­—é’±å¸å­¦çš„æ½œåŠ›ï¼Œä¸ºæ›´è§„æ¨¡åŒ–ã€ç²¾ç¡®å’Œé«˜æ•ˆçš„å†å²æ–‡ç‰©åˆ†ææ‰“å¼€äº†æ–°é€”å¾„ï¼Œå¹¶ä¸ºæ–‡åŒ–é—äº§ç ”ç©¶ã€æ–‡ç‰©æº¯æºç ”ç©¶å’Œä¼ªé€ æ£€æµ‹æä¾›äº†æ–°çš„æ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å°†å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯åº”ç”¨äºæ•°å­—é’±å¸å­¦ï¼Œä¸“æ³¨äºå†å²ç¡¬å¸çš„åˆ†æã€‚</li>
<li>åˆ©ç”¨CLIPæ¨¡å‹å¼€å‘äº†ä¸€ä¸ªçµæ´»æ¡†æ¶ï¼Œå¯é€šè¿‡å›¾åƒå’Œæ–‡æœ¬æè¿°è¿›è¡Œç¡¬å¸ç‰¹å¾è¯†åˆ«å’Œåˆ†ç±»ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªä¸åŒæ•°æ®é›†çš„ç ”ç©¶ï¼Œè¯„ä¼°äº†ä¸åŒæ£€æµ‹ç®—æ³•åœ¨ç¡¬å¸è¯†åˆ«å’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„æ•ˆèƒ½ã€‚</li>
<li>å¤§å‹CLIPæ¨¡å‹åœ¨æ£€æµ‹å¤æ‚å›¾åƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•æ›´æ“…é•¿è¯†åˆ«ç®€å•å‡ ä½•å›¾æ¡ˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿè®¡æ ¡å‡†æœºåˆ¶ï¼Œä»¥æé«˜ä½è´¨é‡æ•°æ®é›†ä¸­ç›¸ä¼¼åº¦è¯„åˆ†çš„å¯é æ€§ã€‚</li>
<li>ç ”ç©¶çªæ˜¾äº†å°†æœ€æ–°ç›®æ ‡æ£€æµ‹æŠ€æœ¯èå…¥æ•°å­—é’±å¸å­¦çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30714de7ccb62e036fec704f34d33bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9e9e53bee649a9a079e3c7d76162da9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HV-BEV-Decoupling-Horizontal-and-Vertical-Feature-Sampling-for-Multi-View-3D-Object-Detection"><a href="#HV-BEV-Decoupling-Horizontal-and-Vertical-Feature-Sampling-for-Multi-View-3D-Object-Detection" class="headerlink" title="HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for   Multi-View 3D Object Detection"></a>HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for   Multi-View 3D Object Detection</h2><p><strong>Authors:Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang</strong></p>
<p>The application of vision-based multi-view environmental perception system has been increasingly recognized in autonomous driving technology, especially the BEV-based models. Current state-of-the-art solutions primarily encode image features from each camera view into the BEV space through explicit or implicit depth prediction. However, these methods often focus on improving the accuracy of projecting 2D features into corresponding depth regions, while overlooking the highly structured information of real-world objects and the varying height distributions of objects across different scenes. In this work, we propose HV-BEV, a novel approach that decouples feature sampling in the BEV grid queries paradigm into horizontal feature aggregation and vertical adaptive height-aware reference point sampling, aiming to improve both the aggregation of objectsâ€™ complete information and generalization to diverse road environments. Specifically, we construct a learnable graph structure in the horizontal plane aligned with the ground for 3D reference points, reinforcing the association of the same instance across different BEV grids, especially when the instance spans multiple image views around the vehicle. Additionally, instead of relying on uniform sampling within a fixed height range, we introduce a height-aware module that incorporates historical information, enabling the reference points to adaptively focus on the varying heights at which objects appear in different scenes. Extensive experiments validate the effectiveness of our proposed method, demonstrating its superior performance over the baseline across the nuScenes dataset. Moreover, our best-performing model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing set. </p>
<blockquote>
<p>åŸºäºè§†è§‰çš„å¤šè§†å›¾ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åŸºäºé¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰çš„æ¨¡å‹ï¼Œå·²ç»å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„è®¤å¯ã€‚å½“å‰å…ˆè¿›è§£å†³æ–¹æ¡ˆä¸»è¦é€šè¿‡æ˜¾å¼æˆ–éšå¼çš„æ·±åº¦é¢„æµ‹ï¼Œå°†æ¯ä¸ªç›¸æœºè§†å›¾çš„å›¾åƒç‰¹å¾ç¼–ç åˆ°é¸Ÿç°è§†å›¾ç©ºé—´ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¾§é‡äºæé«˜å°†äºŒç»´ç‰¹å¾æŠ•å½±åˆ°ç›¸åº”æ·±åº¦åŒºåŸŸçš„å‡†ç¡®æ€§ï¼Œè€Œå¿½ç•¥äº†ç°å®ä¸–ç•Œç‰©ä½“çš„é«˜åº¦ç»“æ„åŒ–ä¿¡æ¯å’Œä¸åŒåœºæ™¯ä¸­ç‰©ä½“é«˜åº¦åˆ†å¸ƒçš„å¤šæ ·æ€§ã€‚</p>
</blockquote>
<p>é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HV-BEVè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒå°†é¸Ÿç°è§†å›¾ç½‘æ ¼æŸ¥è¯¢èŒƒå¼ä¸­çš„ç‰¹å¾é‡‡æ ·è§£è€¦ä¸ºæ°´å¹³ç‰¹å¾èšåˆå’Œå‚ç›´è‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥å‚è€ƒç‚¹é‡‡æ ·ï¼Œæ—¨åœ¨æé«˜ç‰©ä½“å®Œæ•´ä¿¡æ¯çš„èšåˆèƒ½åŠ›ä»¥åŠå¯¹ä¸åŒé“è·¯ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨ä¸åœ°é¢å¯¹é½çš„æ°´å¹³é¢ä¸Šæ„å»ºäº†å¯å­¦ä¹ çš„å›¾ç»“æ„ï¼Œç”¨äºä¸‰ç»´å‚è€ƒç‚¹ï¼ŒåŠ å¼ºäº†ä¸åŒé¸Ÿç°è§†å›¾ç½‘æ ¼ä¸­åŒä¸€å®ä¾‹çš„å…³è”ï¼Œç‰¹åˆ«æ˜¯å½“å®ä¾‹è·¨è¶Šè½¦è¾†å‘¨å›´å¤šä¸ªè§†å›¾æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜åº¦æ„ŸçŸ¥æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åœ¨å›ºå®šé«˜åº¦èŒƒå›´å†…è¿›è¡Œå‡åŒ€é‡‡æ ·ï¼Œä½¿å‚è€ƒç‚¹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨ä¸åŒåœºæ™¯ä¸­ç‰©ä½“å‡ºç°çš„é«˜åº¦å˜åŒ–ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18884v2">PDF</a> 12 pages, 7 figures, submitted to T-ITS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­ï¼ŒåŸºäºè§†è§‰çš„å¤šè§†è§’ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åŸºäºBEVï¼ˆé¸Ÿç°è§†å›¾ï¼‰çš„æ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½ç•¥çœŸå®ä¸–ç•Œç‰©ä½“çš„ç»“æ„åŒ–ä¿¡æ¯å’Œåœºæ™¯ä¸­çš„ç‰©ä½“é«˜åº¦åˆ†å¸ƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•HV-BEVã€‚è¯¥æ–¹æ³•å°†BEVç½‘æ ¼æŸ¥è¯¢èŒƒå¼ä¸­çš„ç‰¹å¾é‡‡æ ·è§£è€¦ä¸ºæ°´å¹³ç‰¹å¾èšåˆå’Œå‚ç›´è‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥å‚è€ƒç‚¹é‡‡æ ·ï¼Œæ—¨åœ¨æ”¹è¿›ç‰©ä½“çš„å®Œæ•´ä¿¡æ¯èšåˆå’Œå¤šæ ·åŒ–é“è·¯ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸åœ°é¢å¯¹é½çš„å¯å­¦ä¹ å›¾ç»“æ„ï¼Œå¼ºåŒ–ä¸åŒBEVç½‘æ ¼ä¸­åŒä¸€å®ä¾‹çš„å…³è”ã€‚åŒæ—¶ï¼Œå¼•å…¥é«˜åº¦æ„ŸçŸ¥æ¨¡å—ï¼Œç»“åˆå†å²ä¿¡æ¯ï¼Œä½¿å‚è€ƒç‚¹èƒ½è‡ªé€‚åº”å…³æ³¨ä¸åŒåœºæ™¯ä¸­ç‰©ä½“çš„ä¸åŒé«˜åº¦ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨nuScenesæ•°æ®é›†ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæœ€ä½³æ¨¡å‹åœ¨nuScenesæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†50.5%çš„mAPå’Œ59.8%çš„NDSã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­ï¼ŒåŸºäºè§†è§‰çš„å¤šè§†è§’ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿçš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ï¼Œç‰¹åˆ«æ˜¯BEVæ¨¡å‹ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡å¯¹å›¾åƒç‰¹å¾è¿›è¡Œç¼–ç å¹¶æ˜ å°„åˆ°BEVç©ºé—´ï¼Œä½†å¿½è§†äº†ç‰©ä½“çš„ç»“æ„åŒ–ä¿¡æ¯å’Œé«˜åº¦åˆ†å¸ƒã€‚</li>
<li>HV-BEVæ–¹æ³•æ—¨åœ¨æ”¹è¿›ç‰©ä½“çš„å®Œæ•´ä¿¡æ¯èšåˆå’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æ°´å¹³ç‰¹å¾èšåˆå’Œå‚ç›´è‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥å‚è€ƒç‚¹é‡‡æ ·è§£è€¦ç‰¹å¾é‡‡æ ·ã€‚</li>
<li>HV-BEVæ„å»ºäº†ä¸€ä¸ªä¸åœ°é¢å¯¹é½çš„å¯å­¦ä¹ å›¾ç»“æ„ï¼Œå¼ºåŒ–ä¸åŒBEVç½‘æ ¼ä¸­åŒä¸€å®ä¾‹çš„å…³è”ã€‚</li>
<li>å¼•å…¥é«˜åº¦æ„ŸçŸ¥æ¨¡å—ï¼Œç»“åˆå†å²ä¿¡æ¯ï¼Œä½¿å‚è€ƒç‚¹èƒ½è‡ªé€‚åº”å…³æ³¨ä¸åŒåœºæ™¯ä¸­ç‰©ä½“çš„ä¸åŒé«˜åº¦ã€‚</li>
<li>å®éªŒéªŒè¯äº†HV-BEVæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨nuScenesæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eeab49952df98bba3f2eb0ed04bb9edf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-030efa7d39eec8c6225d78a1ccdc908b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-071d8aa36213597c2ca97127db03c494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f6735f528f8f7b121761262ad46158.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation"><a href="#AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation" class="headerlink" title="AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation"></a>AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation</h2><p><strong>Authors:Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li</strong></p>
<p>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet">https://github.com/jarch-ma/AFANet</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ æ—¨åœ¨é€šè¿‡ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥è¯†åˆ«æ–°æ¦‚å¿µã€‚ç„¶è€Œï¼Œå¯¹äºè§†è§‰å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼‰è€Œè¨€ï¼Œåƒç´ çº§æ³¨é‡Šæ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œæœ¬æ–‡åˆ©ç”¨æ›´å…·æŒ‘æˆ˜æ€§çš„å›¾åƒçº§æ³¨é‡Šï¼Œå¹¶æå‡ºè‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰è¿›è¡Œå¼±ç›‘ç£ä¸‹çš„å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼ˆWFSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†è·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œå®ƒå°†RGBå›¾åƒåˆ†è§£æˆé«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶é€šè¿‡é‡æ–°å¯¹é½è¿›ä¸€æ­¥ä¼˜åŒ–è¯­ä¹‰ç»“æ„ä¿¡æ¯ã€‚ä¸å¤§å¤šæ•°ç°æœ‰çš„WFSSæ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•é‡‡ç”¨ç¦»çº¿å­¦ä¹ çš„å¤šæ¨¡æ€è¯­è¨€è§†è§‰æ¨¡å‹çš„æ–‡æœ¬ä¿¡æ¯ï¼ˆä¾‹å¦‚CLIPï¼‰ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†CLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡åœ¨çº¿å­¦ä¹ å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ï¼Œä»è€Œä¸ºCFMæä¾›ä¸°å¯Œçš„è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAFANetå·²ç»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jarch-ma/AFANetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17601v2">PDF</a> Accepted by TMM 2024</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æ¢è®¨äº†å°æ ·æœ¬å­¦ä¹ åœ¨å¼±ç›‘ç£ä¸‹çš„è¯­ä¹‰åˆ†å‰²åº”ç”¨ã€‚é’ˆå¯¹å›¾åƒçº§åˆ«çš„æ ‡æ³¨æŒ‘æˆ˜ï¼Œæå‡ºäº†è‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰ã€‚é€šè¿‡äº¤å‰ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰å¤„ç†å›¾åƒï¼Œå¹¶ç»“åˆCLIPæŒ‡å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œå®ç°äº†åœ¨çº¿å­¦ä¹ çš„è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ï¼Œä¼˜åŒ–åˆ†å‰²æ€§èƒ½å¹¶è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡å…³æ³¨å°æ ·æœ¬å­¦ä¹ åœ¨å¼±ç›‘ç£ä¸‹çš„è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³åƒç´ çº§æ ‡æ³¨è€—æ—¶è€—èµ„çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰ï¼Œé‡‡ç”¨å›¾åƒçº§æ ‡æ³¨è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æå‡ºäº¤å‰ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œèƒ½å¤Ÿåˆ†ç¦»å›¾åƒçš„é«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶ä¼˜åŒ–è¯­ä¹‰ç»“æ„ä¿¡æ¯ã€‚</li>
<li>ä¸å¤§å¤šæ•°ä½¿ç”¨ç¦»çº¿å­¦ä¹ æ–¹å¼çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¸åŒï¼Œè¯¥è®ºæ–‡é‡‡ç”¨åœ¨çº¿å­¦ä¹ çš„æ–¹å¼å¤„ç†æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥CLIPæŒ‡å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œå¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ï¼Œæä¾›ä¸°å¯Œçš„è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>åœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†AFANetçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e3c275123d7cc725bbdac793e2b1c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92b12f4ef3edec5343b33d63af5f2e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3c89546934a25924e4a0910914e013.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-912009cb34665b008f15f63397f9a37e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation"><a href="#MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation"></a>MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation</h2><p><strong>Authors:Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe">https://github.com/zwyang6/MoRe</a>. </p>
<blockquote>
<p>ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é€šå¸¸ä½¿ç”¨ç±»æ¿€æ´»å›¾ï¼ˆCAMï¼‰æ¥å®ç°å¯†é›†é¢„æµ‹ã€‚æœ€è¿‘ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æä¾›äº†ä¸€ç§ä»ç±»è¡¥ä¸æ³¨æ„åŠ›ç”Ÿæˆå®šä½å›¾çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå¯¹è¿™ç±»æ³¨æ„åŠ›çš„å»ºæ¨¡çº¦æŸä¸è¶³ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰ç»å¸¸é¢ä¸´ä¼ªå½±é—®é¢˜ï¼Œå³è¯­ä¹‰ç›¸å…³æ€§æå°çš„è¡¥ä¸åŒºåŸŸä¼šè¢«ç±»ä»¤ç‰Œé”™è¯¯æ¿€æ´»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºMoReæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶LAMçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹ç±»è¡¥ä¸æ³¨æ„åŠ›æ–½åŠ é¢å¤–çš„æ­£åˆ™åŒ–æ˜¯å¿…è¦çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ³¨æ„åŠ›è§†ä¸ºä¸€ç§æ–°å‹çš„æœ‰å‘å›¾ï¼Œå¹¶æå‡ºå›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—ï¼Œä»¥éšå«åœ°æ­£åˆ™åŒ–ç±»è¡¥ä¸å®ä½“ä¹‹é—´çš„äº¤äº’ã€‚å®ƒç¡®ä¿ç±»ä»¤ç‰Œèƒ½å¤ŸåŠ¨æ€åœ°å‡èšç›¸å…³çš„è¡¥ä¸ä¿¡æ¯ï¼Œå¹¶åœ¨å›¾çº§åˆ«æŠ‘åˆ¶ä¸ç›¸å…³çš„ä¼ªå½±ã€‚å…¶æ¬¡ï¼Œå—åˆ†ç±»æƒé‡CAMèƒ½å¤Ÿä¿æŒå¯¹è±¡å®šä½å¹³æ»‘æ€§çš„è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†å®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—ï¼Œä»¥æ˜¾å¼åœ°æ­£åˆ™åŒ–ç±»è¡¥ä¸æ³¨æ„åŠ›ã€‚å®ƒç›´æ¥ä»CAMæŒ–æ˜ä»¤ç‰Œå…³ç³»ï¼Œå¹¶ä»¥å¯å­¦ä¹ çš„æ–¹å¼ç›‘ç£ç±»ä»¤ç‰Œå’Œè¡¥ä¸ä»¤ç‰Œä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨PASCAL VOCå’ŒMS COCOä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†MoReæœ‰æ•ˆåœ°è§£å†³äº†ä¼ªå½±é—®é¢˜ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€è¿‘çš„å•é˜¶æ®µç”šè‡³å¤šé˜¶æ®µæ–¹æ³•ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe%E3%80%82">https://github.com/zwyang6/MoReã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11076v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå›¾åƒçº§åˆ«æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä½¿ç”¨Vision Transformerï¼ˆViTï¼‰ç”Ÿæˆçš„Localization Attention Mapsï¼ˆLAMï¼‰å¸¸å¸¸å—åˆ°ä¼ªå½±é—®é¢˜çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MoReæ–¹æ³•ï¼Œé€šè¿‡é¢å¤–çš„æ­£åˆ™åŒ–å¯¹ç±»è¡¥ä¸æ³¨æ„åŠ›è¿›è¡Œçº¦æŸã€‚MoReåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šGraph Category Representationæ¨¡å—å’ŒLocalization-informed Regularizationæ¨¡å—ï¼Œå‰è€…é€šè¿‡æ„å»ºç±»è¡¥ä¸å®ä½“çš„æœ‰å‘å›¾æ¥éšå¼åœ°çº¦æŸç±»è¡¥ä¸ä¹‹é—´çš„äº¤äº’ï¼Œåè€…åˆ™ä»åˆ†ç±»æƒé‡çš„CAMä¸­æå–tokenå…³ç³»ï¼Œä»¥æ˜¾å¼åœ°ç›‘ç£ç±»è¡¥ä¸æ³¨æ„åŠ›çš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒMoReæœ‰æ•ˆåœ°è§£å†³äº†ä¼ªå½±é—®é¢˜ï¼Œå¹¶åœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSSä¸­ä½¿ç”¨Class Activation Maps (CAM) å®ç°å¯†é›†é¢„æµ‹ã€‚</li>
<li>Vision Transformer (ViT) å¯ç”ŸæˆLocalization Attention Maps (LAM)ã€‚</li>
<li>LAMå­˜åœ¨ä¼ªå½±é—®é¢˜ï¼Œå³ä¸ç›¸å…³çš„è¡¥ä¸åŒºåŸŸä¼šè¢«ç±»æ ‡è®°é”™è¯¯æ¿€æ´»ã€‚</li>
<li>MoReæ–¹æ³•é€šè¿‡é¢å¤–çš„æ­£åˆ™åŒ–è§£å†³LAMçš„ä¼ªå½±é—®é¢˜ã€‚</li>
<li>MoReåŒ…æ‹¬Graph Category Representationæ¨¡å—å’ŒLocalization-informed Regularizationæ¨¡å—ã€‚</li>
<li>MoReåœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-872f56d51aa5d225fb4d9dbe916addf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-558c55d8af6b022179480697698069b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3e4750dfaf5c1d3fd23143f2d50fec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699cfbb8aa515db4cbb28229318109c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ERUP-YOLO-Enhancing-Object-Detection-Robustness-for-Adverse-Weather-Condition-by-Unified-Image-Adaptive-Processing"><a href="#ERUP-YOLO-Enhancing-Object-Detection-Robustness-for-Adverse-Weather-Condition-by-Unified-Image-Adaptive-Processing" class="headerlink" title="ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather   Condition by Unified Image-Adaptive Processing"></a>ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather   Condition by Unified Image-Adaptive Processing</h2><p><strong>Authors:Yuka Ogino, Yuho Shoji, Takahiro Toizumi, Atsushi Ito</strong></p>
<p>We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light. Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections. Our framework introduces two differentiable filters: a B&#39;ezier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. These filters unify the functions of classical image processing filters and improve performance of object detection. We also propose a domain-agnostic data augmentation strategy using the BPW filter. Our method does not require data-specific customization of the filter combinations, parameter ranges, and data augmentation. We evaluate our proposed approach, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO, by applying it to the YOLOv3 detector. Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions, including fog and low-light conditions. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ¶åŠ£å¤©æ°”æ¡ä»¶ï¼ˆå¦‚é›¾å’Œä½å…‰ï¼‰çš„è‡ªé€‚åº”å›¾åƒç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨å¯å¾®åˆ†çš„é¢„å¤„ç†æ»¤æ³¢å™¨ï¼Œæ‰§è¡Œé€‚ç”¨äºåæœŸç›®æ ‡æ£€æµ‹çš„å›¾åƒå¢å¼ºã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥ä¸¤ç§å¯å¾®åˆ†æ»¤æ³¢å™¨ï¼šåŸºäºè´å¡å°”æ›²çº¿çš„åƒç´ çº§ï¼ˆBPWï¼‰æ»¤æ³¢å™¨å’ŒåŸºäºæ ¸çš„å±€éƒ¨ï¼ˆKBLï¼‰æ»¤æ³¢å™¨ã€‚è¿™äº›æ»¤æ³¢å™¨ç»“åˆäº†ä¼ ç»Ÿå›¾åƒå¤„ç†æ»¤æ³¢å™¨çš„åŠŸèƒ½ï¼Œæé«˜äº†ç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºBPWæ»¤æ³¢å™¨çš„é¢†åŸŸæ— å…³æ•°æ®å¢å¼ºç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ•°æ®é›†å®šåˆ¶æ»¤æ³¢å™¨ç»„åˆã€å‚æ•°èŒƒå›´å’Œæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬å°†æ‰€ææ–¹æ³•ç§°ä¸ºç»Ÿä¸€å›¾åƒå¤„ç†å¢å¼ºç¨³å¥æ€§ï¼ˆERUPï¼‰-YOLOï¼Œå¹¶å°†å…¶åº”ç”¨äºYOLOv3æ£€æµ‹å™¨ã€‚åœ¨æ¶åŠ£å¤©æ°”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æ‰€æå‡ºçš„æ»¤æ³¢å™¨ä¸å¸¸è§„æ–¹æ³•çš„è¡¨è¾¾èƒ½åŠ›ç›¸åŒ¹é…ï¼Œç”šè‡³æ›´é«˜ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„ERUP-YOLOåœ¨åŒ…æ‹¬é›¾å’Œä½å…‰æ¡ä»¶åœ¨å†…çš„å„ç§æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹å‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02799v4">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹æ¶åŠ£å¤©æ°”æ¡ä»¶ï¼ˆå¦‚é›¾å’Œä½å…‰ç¯å¢ƒï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å›¾åƒçš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯å¾®åˆ†é¢„å¤„ç†æ»¤æ³¢å™¨è¿›è¡Œå›¾åƒå¢å¼ºï¼Œä¸ºåæœŸç›®æ ‡æ£€æµ‹æä¾›æœ‰åŠ›æ”¯æŒã€‚ç ”ç©¶ä¸­å¼•å…¥äº†ä¸¤ç§å¯å¾®åˆ†æ»¤æ³¢å™¨ï¼šåŸºäºBÃ©zieræ›²çº¿çš„åƒç´ çº§ï¼ˆBPWï¼‰æ»¤æ³¢å™¨å’ŒåŸºäºæ ¸çš„å±€éƒ¨ï¼ˆKBLï¼‰æ»¤æ³¢å™¨ã€‚å®ƒä»¬èåˆäº†ä¼ ç»Ÿå›¾åƒå¤„ç†æ»¤æ³¢å™¨çš„åŠŸèƒ½ï¼Œæé«˜äº†ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºBPWæ»¤æ³¢å™¨çš„é€šç”¨æ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®å®šåˆ¶æ»¤æ³¢å™¨ç»„åˆã€å‚æ•°èŒƒå›´å’Œæ•°æ®å¢å¼ºæ–¹å¼ã€‚å°†è¯¥æ–¹æ³•åº”ç”¨äºYOLOv3æ£€æµ‹å™¨ï¼Œå®éªŒè¡¨æ˜ï¼Œæ‰€ææ»¤æ³¢å™¨åœ¨æ¶åŠ£å¤©æ°”æ•°æ®é›†ä¸­çš„è¡¨ç°ä¸å¸¸è§„æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒERUP-YOLOåœ¨é›¾å’Œä½å…‰æ¡ä»¶ä¸‹çš„æ€§èƒ½å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å›¾åƒçš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œé€‚ç”¨äºæ¶åŠ£å¤©æ°”æ¡ä»¶ã€‚</li>
<li>å¼•å…¥ä¸¤ç§å¯å¾®åˆ†é¢„å¤„ç†æ»¤æ³¢å™¨ï¼šBPWæ»¤æ³¢å™¨å’ŒKBLæ»¤æ³¢å™¨ï¼Œèåˆä¼ ç»Ÿå›¾åƒå¤„ç†åŠŸèƒ½ã€‚</li>
<li>é‡‡ç”¨BPWæ»¤æ³¢å™¨è®¾è®¡äº†ä¸€ç§é€šç”¨æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹åœ¨æ¶åŠ£å¤©æ°”ä¸‹çš„é²æ£’æ€§ã€‚</li>
<li>æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†å®šåˆ¶æ»¤æ³¢å™¨ç»„åˆã€å‚æ•°èŒƒå›´å’Œæ•°æ®å¢å¼ºæ–¹å¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ‰€ææ»¤æ³¢å™¨åœ¨æ¶åŠ£å¤©æ°”æ•°æ®é›†ä¸­çš„è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ERUP-YOLOåœ¨é›¾å’Œä½å…‰æ¡ä»¶ä¸‹çš„æ€§èƒ½å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a7d5773f70439ce69b4de83b5e0161f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00731155fa4a62fd16e2a99ba37d853c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32177d17d29f08f59e63c30d1de02fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8868e47103cd1a3fc362b7f7ebeed214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69025a156fdbaef8d1a026fdb1df05f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="WeatherDG-LLM-assisted-Diffusion-Model-for-Procedural-Weather-Generation-in-Domain-Generalized-Semantic-Segmentation"><a href="#WeatherDG-LLM-assisted-Diffusion-Model-for-Procedural-Weather-Generation-in-Domain-Generalized-Semantic-Segmentation" class="headerlink" title="WeatherDG: LLM-assisted Diffusion Model for Procedural Weather   Generation in Domain-Generalized Semantic Segmentation"></a>WeatherDG: LLM-assisted Diffusion Model for Procedural Weather   Generation in Domain-Generalized Semantic Segmentation</h2><p><strong>Authors:Chenghao Qian, Yuhu Guo, Yuhong Mo, Wenjing Li</strong></p>
<p>In this work, we propose a novel approach, namely WeatherDG, that can generate realistic, weather-diverse, and driving-screen images based on the cooperation of two foundation models, i.e, Stable Diffusion (SD) and Large Language Model (LLM). Specifically, we first fine-tune the SD with source data, aligning the content and layout of generated samples with real-world driving scenarios. Then, we propose a procedural prompt generation method based on LLM, which can enrich scenario descriptions and help SD automatically generate more diverse, detailed images. In addition, we introduce a balanced generation strategy, which encourages the SD to generate high-quality objects of tailed classes under various weather conditions, such as riders and motorcycles. This segmentation-model-agnostic method can improve the generalization ability of existing models by additionally adapting them with the generated synthetic data. Experiments on three challenging datasets show that our method can significantly improve the segmentation performance of different state-of-the-art models on target domains. Notably, in the setting of â€˜â€™Cityscapes to ACDCâ€™â€™, our method improves the baseline HRDA by 13.9% in mIoU. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåä¸ºWeatherDGï¼Œå®ƒå¯ä»¥é€šè¿‡ä¸¤ä¸ªåŸºç¡€æ¨¡å‹çš„åˆä½œï¼Œå³Stable Diffusionï¼ˆSDï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”Ÿæˆç°å®ã€å¤©æ°”å¤šæ ·ã€é©¾é©¶å±å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æºæ•°æ®å¯¹SDè¿›è¡Œå¾®è°ƒï¼Œä½¿ç”Ÿæˆæ ·æœ¬çš„å†…å®¹å’Œå¸ƒå±€ä¸çœŸå®ä¸–ç•Œçš„é©¾é©¶åœºæ™¯å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„ç¨‹åºåŒ–æç¤ºç”Ÿæˆæ–¹æ³•ï¼Œå¯ä»¥ä¸°å¯Œåœºæ™¯æè¿°ï¼Œå¸®åŠ©SDè‡ªåŠ¨ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´è¯¦ç»†çš„å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¹³è¡¡ç”Ÿæˆç­–ç•¥ï¼Œé¼“åŠ±SDåœ¨å„ç§å¤©æ°”æ¡ä»¶ä¸‹ä¸ºé•¿å°¾ç±»ç”Ÿæˆé«˜è´¨é‡å¯¹è±¡ï¼Œå¦‚éª‘è¡Œè€…å’Œæ‘©æ‰˜è½¦ã€‚è¿™ç§ä¸åˆ†å‰²æ¨¡å‹æ— å…³çš„æ–¹æ³•å¯ä»¥é€šè¿‡ä½¿ç”¨ç”Ÿæˆçš„åˆæˆæ•°æ®é¢å¤–é€‚åº”ç°æœ‰æ¨¡å‹ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ç›®æ ‡åŸŸä¸Šä¸åŒæœ€æ–°æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨â€œåŸå¸‚æ™¯è§‚åˆ°ACDCâ€çš„è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†åŸºçº¿HRDAçš„mIoUæé«˜äº†13.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12075v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºStable Diffusionï¼ˆSDï¼‰å’ŒLarge Language Modelï¼ˆLLMï¼‰ååŒå·¥ä½œçš„æ–°å‹å›¾åƒç”Ÿæˆæ–¹æ³•WeatherDGã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„ã€å¤©æ°”å¤šå˜çš„é©¾é©¶åœºæ™¯å›¾åƒã€‚é€šè¿‡å¾®è°ƒSDæ¨¡å‹ä»¥åŒ¹é…çœŸå®é©¾é©¶åœºæ™¯çš„å†…å®¹å’Œå¸ƒå±€ï¼Œå¹¶ç»“åˆLLMç”Ÿæˆä¸°å¯Œçš„åœºæ™¯æè¿°ï¼Œå®ç°è‡ªåŠ¨ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´è¯¦ç»†çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§å¹³è¡¡ç”Ÿæˆç­–ç•¥ï¼Œé¼“åŠ±SDåœ¨å¤šç§å¤©æ°”æ¡ä»¶ä¸‹ç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡é•¿å°¾ç±»å›¾åƒï¼Œå¦‚éª‘è¡Œè€…å’Œæ‘©æ‰˜è½¦ã€‚è¯¥æ–¹æ³•å¯¹ç°æœ‰çš„æ¨¡å‹å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œé€šè¿‡é€‚åº”ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œå¯åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œèƒ½æ˜¾è‘—æé«˜ä¸åŒæœ€å…ˆè¿›æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨â€œCityscapesåˆ°ACDCâ€çš„è®¾ç½®ä¸­ï¼Œè¯¥æ–¹æ³•å°†åŸºçº¿HRDAæé«˜äº†13.9%çš„mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åä¸ºWeatherDGçš„æ–°å‹å›¾åƒç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºStable Diffusionå’ŒLarge Language Modelã€‚</li>
<li>é€šè¿‡å¾®è°ƒSDæ¨¡å‹ï¼Œä½¿ç”Ÿæˆçš„å›¾åƒå†…å®¹ä¸çœŸå®é©¾é©¶åœºæ™¯å¸ƒå±€ç›¸åŒ¹é…ã€‚</li>
<li>åˆ©ç”¨LLMç”Ÿæˆä¸°å¯Œçš„åœºæ™¯æè¿°ï¼Œä½¿SDèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€è¯¦ç»†çš„å›¾åƒã€‚</li>
<li>å¼•å…¥å¹³è¡¡ç”Ÿæˆç­–ç•¥ï¼Œé¼“åŠ±ç”Ÿæˆé«˜è´¨é‡çš„é•¿å°¾ç±»ç›®æ ‡å›¾åƒï¼Œå¦‚éª‘è¡Œè€…å’Œæ‘©æ‰˜è½¦ã€‚</li>
<li>æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œé€‚ç”¨äºå¤šç§æ¨¡å‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ae1562108c1dde5b0be4cfcf06740ad2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6195b50573918a510c360235b6a960c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2dffa50a206bee5356c45af83b3c12e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca2f3f86ebb0a2d25a9078f27a3aebc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1c44c04809dda3b0df429ae01b56036.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0b7c55374f449d30412c6de02008e5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-089d6b8c1b18bf56a320853fed728a87.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi-scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve state-of-the-art results on benchmark datasets such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>å°‘æ•°è¯­ä¹‰åˆ†å‰²ï¼ˆFew-shot Semantic Segmentationï¼‰æ—¨åœ¨è§£å†³ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œå¯¹è±¡åˆ†å‰²çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè®¸å¤šä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•è¦ä¹ˆéœ€è¦ä¸¢å¼ƒå¤æ‚çš„å±€éƒ¨è¯­ä¹‰ç‰¹å¾ï¼Œè¦ä¹ˆé¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„å°‘æ•°è¯­ä¹‰åˆ†å‰²æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—ï¼Œä»¥æé«˜æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥åˆ†å±‚æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©è†œã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èåˆäº†ä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨å±€ç‰¹å¾ä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚åº¦ã€‚æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„è¿™ç§å¹³è¡¡ä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL-5iå’ŒCOCO-20iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ— è®ºæ˜¯åœ¨ä¸€æ¬¡æ€§æ‹æ‘„ï¼ˆ1-shotï¼‰è¿˜æ˜¯äº”æ¬¡æ‹æ‘„ï¼ˆ5-shotï¼‰çš„è®¾ç½®ä¸­éƒ½æ˜¯å¦‚æ­¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…æœ‰150ä¸‡å‚æ•°ï¼Œåœ¨å…‹æœç°æœ‰æ–¹æ³•å±€é™æ€§çš„åŒæ—¶è¡¨ç°å‡ºäº†ç«äº‰åŠ›ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformeræ¶æ„ï¼Œæå‡ºæ–°çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´å˜æ¢è§£ç å™¨ã€ä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—å’Œå¤šå°ºåº¦è§£ç å™¨ï¼Œæ”¹å–„æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ï¼Œå®ç°ç²¾ç»†çš„åˆ†å‰²æ©è†œã€‚åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ä¸€é˜¶å’Œäº”é˜¶æƒ…å†µä¸‹çš„ä¸šç•Œé¢†å…ˆç»“æœï¼Œå±•ç°å‡ºé«˜æ•ˆä¸”å…·å¤‡ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ›´å¤šè¯¦æƒ…è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet%E3%80%82">https://github.com/amirrezafateh/MSDNetã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é’ˆå¯¹å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæå‡ºæ–°çš„åŸºäºTransformeræ¶æ„çš„æ¡†æ¶ã€‚</li>
<li>å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—ï¼Œæ”¹å–„æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒçš„å…³ç³»ç†è§£ã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦è§£ç å™¨ç»“åˆä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾ï¼Œå®ç°åˆ†å‰²æ©è†œçš„ç²¾ç»†åŒ–ã€‚</li>
<li>æ•´åˆä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨çƒç‰¹å¾ï¼Œæå‡ä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚æ€§ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°ä¸šç•Œé¢†å…ˆç»“æœã€‚</li>
<li>ä»…ç”¨150ä¸‡å‚æ•°ä¾¿å±•ç°å‡ºç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7dfbf8a50110d45135537d58989963cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86f9eb0a4c352a4a2a0765cb5bfc2f60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4fa677efdc0fbf4dddf9a40b66fae11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55d5166f5ce99d76a8cae11e3ae5aff8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="NeRF-DetS-Enhanced-Adaptive-Spatial-wise-Sampling-and-View-wise-Fusion-Strategies-for-NeRF-based-Indoor-Multi-view-3D-Object-Detection"><a href="#NeRF-DetS-Enhanced-Adaptive-Spatial-wise-Sampling-and-View-wise-Fusion-Strategies-for-NeRF-based-Indoor-Multi-view-3D-Object-Detection" class="headerlink" title="NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion   Strategies for NeRF-based Indoor Multi-view 3D Object Detection"></a>NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion   Strategies for NeRF-based Indoor Multi-view 3D Object Detection</h2><p><strong>Authors:Chi Huang, Xinyang Li, Yansong Qu, Changli Wu, Xiaofan Li, Shengchuan Zhang, Liujuan Cao</strong></p>
<p>In indoor scenes, the diverse distribution of object locations and scales makes the visual 3D perception task a big challenge.   Previous works (e.g, NeRF-Det) have demonstrated that implicit representation has the capacity to benefit the visual 3D perception task in indoor scenes with high amount of overlap between input images.   However, previous works cannot fully utilize the advancement of implicit representation because of fixed sampling and simple multi-view feature fusion.   In this paper, inspired by sparse fashion method (e.g, DETR3D), we propose a simple yet effective method, NeRF-DetS, to address above issues. NeRF-DetS includes two modules: Progressive Adaptive Sampling Strategy (PASS) and Depth-Guided Simplified Multi-Head Attention Fusion (DS-MHA).   Specifically,   (1)PASS can automatically sample features of each layer within a dense 3D detector, using offsets predicted by the previous layer.   (2)DS-MHA can not only efficiently fuse multi-view features with strong occlusion awareness but also reduce computational cost.   Extensive experiments on ScanNetV2 dataset demonstrate our NeRF-DetS outperforms NeRF-Det, by achieving +5.02% and +5.92% improvement in mAP under IoU25 and IoU50, respectively. Also, NeRF-DetS shows consistent improvements on ARKITScenes. </p>
<blockquote>
<p>åœ¨å®¤å†…åœºæ™¯ä¸­ï¼Œç‰©ä½“ä½ç½®å’Œå°ºåº¦çš„å¤šæ ·åˆ†å¸ƒä½¿å¾—è§†è§‰3Dæ„ŸçŸ¥ä»»åŠ¡é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚å…ˆå‰çš„å·¥ä½œï¼ˆä¾‹å¦‚NeRF-Detï¼‰å·²ç»è¯æ˜ï¼Œéšå¼è¡¨ç¤ºæœ‰åŠ©äºå®¤å†…åœºæ™¯çš„è§†è§‰3Dæ„ŸçŸ¥ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥å›¾åƒä¹‹é—´å­˜åœ¨å¤§é‡é‡å çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç”±äºå›ºå®šçš„é‡‡æ ·å’Œç®€å•çš„å¤šè§†å›¾ç‰¹å¾èåˆï¼Œå…ˆå‰çš„å·¥ä½œæ— æ³•å……åˆ†åˆ©ç”¨éšå¼è¡¨ç¤ºçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13921v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å®¤å†…åœºæ™¯ä¸­è§†è§‰3Dæ„ŸçŸ¥çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹è±¡ä½ç½®å’Œå°ºåº¦çš„å¤šæ ·æ€§ã€‚å—ç¨€ç–æ–¹æ³•ï¼ˆå¦‚DETR3Dï¼‰çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•NeRF-DetSï¼ŒåŒ…æ‹¬æ¸è¿›è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼ˆPASSï¼‰å’Œæ·±åº¦å¼•å¯¼ç®€åŒ–å¤šå¤´æ³¨æ„åŠ›èåˆï¼ˆDS-MHAï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒNeRF-DetSåœ¨ScanNetV2æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºNeRF-Detï¼Œåœ¨IoU25å’ŒIoU50ä¸‹çš„mAPåˆ†åˆ«æé«˜äº†+5.02%å’Œ+5.92%ï¼Œå¹¶ä¸”åœ¨ARKITScenesä¸Šä¹Ÿè¡¨ç°å‡ºä¸€è‡´çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®¤å†…åœºæ™¯ä¸­ï¼Œå¯¹è±¡ä½ç½®å’Œå°ºåº¦çš„å¤šæ ·æ€§ä½¿å¾—è§†è§‰3Dæ„ŸçŸ¥ä»»åŠ¡å……æ»¡æŒ‘æˆ˜ã€‚</li>
<li>ä¹‹å‰çš„ä½œå“ï¼ˆå¦‚NeRF-Detï¼‰å·²ç»è¯æ˜äº†éšå¼è¡¨ç¤ºå¯¹å®¤å†…åœºæ™¯è§†è§‰3Dæ„ŸçŸ¥ä»»åŠ¡çš„æ½œåŠ›ã€‚</li>
<li>NeRF-DetSæ–¹æ³•é€šè¿‡ç»“åˆæ¸è¿›è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥å’Œæ·±åº¦å¼•å¯¼ç®€åŒ–å¤šå¤´æ³¨æ„åŠ›èåˆæ¥è§£å†³å…ˆå‰æ–¹æ³•çš„é—®é¢˜ã€‚</li>
<li>PASSèƒ½å¤Ÿè‡ªåŠ¨åœ¨å¯†é›†3Dæ£€æµ‹å™¨ä¸­çš„æ¯ä¸€å±‚é‡‡æ ·ç‰¹å¾ã€‚</li>
<li>DS-MHAèƒ½å¤Ÿé«˜æ•ˆåœ°èåˆå¤šè§†å›¾ç‰¹å¾ï¼Œå…·æœ‰å¼ºå¤§çš„é®æŒ¡æ„è¯†ï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>åœ¨ScanNetV2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNeRF-DetSçš„æ€§èƒ½ä¼˜äºNeRF-Detï¼Œå¹¶ä¸”åœ¨ARKITScenesä¸Šä¹Ÿæœ‰ä¸€è‡´çš„æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-306731d7f1d69eb434e06e08799097f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cfddae21ae946b554002b6fbfddba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81289d67066949786f358bca9834b9d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-584e618350edd2bbd3f4bc1b85781e60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ee6efda62aa34115ce79937185712e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Language-Guided-Instance-Aware-Domain-Adaptive-Panoptic-Segmentation"><a href="#Language-Guided-Instance-Aware-Domain-Adaptive-Panoptic-Segmentation" class="headerlink" title="Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation"></a>Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation</h2><p><strong>Authors:Elham Amin Mansour, Ozan Unal, Suman Saha, Benjamin Bejar, Luc Van Gool</strong></p>
<p>The increasing relevance of panoptic segmentation is tied to the advancements in autonomous driving and AR&#x2F;VR applications. However, the deployment of such models has been limited due to the expensive nature of dense data annotation, giving rise to unsupervised domain adaptation (UDA). A key challenge in panoptic UDA is reducing the domain gap between a labeled source and an unlabeled target domain while harmonizing the subtasks of semantic and instance segmentation to limit catastrophic interference. While considerable progress has been achieved, existing approaches mainly focus on the adaptation of semantic segmentation. In this work, we focus on incorporating instance-level adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix significantly enhances the panoptic quality by improving instance segmentation performance. Specifically, we propose inserting high-confidence predicted instances from the target domain onto source images, retaining the exhaustiveness of the resulting pseudo-labels while reducing the injected confirmation bias. Nevertheless, such an enhancement comes at the cost of degraded semantic performance, attributed to catastrophic forgetting. To mitigate this issue, we regularize our semantic branch by employing CLIP-based domain alignment (CDA), exploiting the domain-robustness of natural language prompts. Finally, we present an end-to-end model incorporating these two mechanisms called LIDAPS, achieving state-of-the-art results on all popular panoptic UDA benchmarks. </p>
<blockquote>
<p>å…¨æ™¯åˆ†å‰²çš„æ—¥ç›Šé‡è¦æ€§æ˜¯ä¸è‡ªåŠ¨é©¾é©¶å’ŒAR&#x2F;VRåº”ç”¨çš„è¿›æ­¥ç›¸è”ç³»çš„ã€‚ç„¶è€Œï¼Œç”±äºå¯†é›†æ•°æ®æ ‡æ³¨çš„é«˜æ˜‚æˆæœ¬ï¼Œæ­¤ç±»æ¨¡å‹çš„éƒ¨ç½²å—åˆ°é™åˆ¶ï¼Œå‚¬ç”Ÿäº†æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰çš„å‡ºç°ã€‚å…¨æ™¯UDAçš„å…³é”®æŒ‘æˆ˜åœ¨äºç¼©å°æœ‰æ ‡ç­¾æºåŸŸå’Œæ— æ ‡ç­¾ç›®æ ‡åŸŸä¹‹é—´çš„åŸŸå·®è·ï¼ŒåŒæ—¶åè°ƒè¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²çš„å­ä»»åŠ¡ï¼Œä»¥é¿å…ç¾éš¾æ€§å¹²æ‰°ã€‚è™½ç„¶å·²å–å¾—äº†ç›¸å½“å¤§çš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¯­ä¹‰åˆ†å‰²çš„é€‚åº”ä¸Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºé€šè¿‡ä¸€ç§æ–°å‹å®ä¾‹æ„ŸçŸ¥è·¨åŸŸæ··åˆç­–ç•¥IMixï¼Œèå…¥å®ä¾‹çº§é€‚åº”ã€‚IMixé€šè¿‡æé«˜å®ä¾‹åˆ†å‰²æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†å…¨æ™¯è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ç›®æ ‡åŸŸçš„é«˜ç½®ä¿¡åº¦é¢„æµ‹å®ä¾‹æ’å…¥æºå›¾åƒçš„æ–¹æ³•ï¼Œä¿ç•™äº†æ‰€å¾—ä¼ªæ ‡ç­¾çš„è¯¦å°½æ€§ï¼ŒåŒæ—¶å‡å°‘äº†æ³¨å…¥çš„ç¡®è®¤åè§ã€‚ç„¶è€Œï¼Œè¿™ç§å¢å¼ºæ˜¯ä»¥è¯­ä¹‰æ€§èƒ½ä¸‹é™ä¸ºä»£ä»·çš„ï¼Œè¿™æ˜¯ç”±äºç¾éš¾æ€§é—å¿˜é€ æˆçš„ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨åŸºäºCLIPçš„åŸŸå¯¹é½ï¼ˆCDAï¼‰æ¥è§„èŒƒæˆ‘ä»¬çš„è¯­ä¹‰åˆ†æ”¯ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„åŸŸç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èåˆäº†è¿™ä¸¤ç§æœºåˆ¶ï¼Œç§°ä¸ºLIDAPSï¼Œåœ¨æ‰€æœ‰äººæ°”æ—ºç››çš„å…¨æ™¯UDAåŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.03799v2">PDF</a> Accepted at the 2025 IEEE&#x2F;CVF Winter Conference on Applications of   Computer Vision (WACV)</p>
<p><strong>Summary</strong>ï¼šéšç€è‡ªåŠ¨é©¾é©¶å’ŒAR&#x2F;VRåº”ç”¨çš„ä¸æ–­å‘å±•ï¼Œå…¨æ™¯åˆ†å‰²çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç”±äºå¯†é›†æ•°æ®æ ‡æ³¨çš„é«˜æˆæœ¬ï¼Œå…¨æ™¯åˆ†å‰²æ¨¡å‹çš„éƒ¨ç½²å—åˆ°é™åˆ¶ï¼Œè¿™æ¨åŠ¨äº†æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰çš„ç ”ç©¶ã€‚æœ¬æ–‡å…³æ³¨æ— ç›‘ç£åŸŸè‡ªé€‚åº”ä¸­çš„å…¨æ™¯åˆ†å‰²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå®ä¾‹çº§è‡ªé€‚åº”çš„æ–°æ–¹æ³•IMixï¼Œé€šè¿‡æ’å…¥ç›®æ ‡åŸŸçš„é«˜ç½®ä¿¡åº¦é¢„æµ‹å®ä¾‹åˆ°æºå›¾åƒä¸­ï¼Œæé«˜äº†å…¨æ™¯åˆ†å‰²çš„è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™å¯èƒ½å¯¼è‡´è¯­ä¹‰æ€§èƒ½ä¸‹é™ï¼Œå‡ºç°ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨åŸºäºCLIPçš„åŸŸå¯¹é½ï¼ˆCDAï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„åŸŸç¨³å¥æ€§è¿›è¡Œè¯­ä¹‰åˆ†æ”¯æ­£åˆ™åŒ–ã€‚æœ€ç»ˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè¿™ä¸¤ç§æœºåˆ¶çš„ç«¯åˆ°ç«¯æ¨¡å‹LIDAPSï¼Œåœ¨ä¸»æµå…¨æ™¯UDAåŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…¨æ™¯åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶å’ŒAR&#x2F;VRåº”ç”¨ä¸­è¶Šæ¥è¶Šé‡è¦ï¼Œä½†å¯†é›†æ•°æ®æ ‡æ³¨çš„é«˜æˆæœ¬é™åˆ¶äº†å…¶æ¨¡å‹éƒ¨ç½²ã€‚</li>
<li>æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ˜¯è§£å†³å…¨æ™¯åˆ†å‰²ä¸­è·¨åŸŸé—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>IMixæ–¹æ³•é€šè¿‡å®ä¾‹çº§è‡ªé€‚åº”æé«˜å…¨æ™¯åˆ†å‰²è´¨é‡ï¼Œé€šè¿‡å°†ç›®æ ‡åŸŸçš„é«˜ç½®ä¿¡åº¦é¢„æµ‹å®ä¾‹æ’å…¥æºå›¾åƒä¸­å®ç°ã€‚</li>
<li>IMixæ–¹æ³•å¯èƒ½å¯¼è‡´è¯­ä¹‰æ€§èƒ½ä¸‹é™å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³è¯­ä¹‰æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œé‡‡ç”¨åŸºäºCLIPçš„åŸŸå¯¹é½ï¼ˆCDAï¼‰æ–¹æ³•è¿›è¡Œè¯­ä¹‰åˆ†æ”¯æ­£åˆ™åŒ–ã€‚</li>
<li>LIDAPSæ¨¡å‹ç»“åˆIMixå’ŒCDAæœºåˆ¶ï¼Œå®ç°äº†åœ¨ä¸»æµå…¨æ™¯UDAåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.03799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8eecd2ace0d93f212a0117545242e16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40dcc694e54acb6012e594336a89a4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad641fe06dbb25a6e31d5b8b5de3ede.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47d517cb53d9dfe49bb1dd90de881938.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-21b174cacbba0e2e635a191464151bfa.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Distilled Transformers with Locally Enhanced Global Representations for   Face Forgery Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Multi-Modality Driven LoRA for Adverse Condition Depth Estimation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">10242.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
