<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Multi-Modality Driven LoRA for Adverse Condition Depth Estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Multi-Modality-Driven-LoRA-for-Adverse-Condition-Depth-Estimation"><a href="#Multi-Modality-Driven-LoRA-for-Adverse-Condition-Depth-Estimation" class="headerlink" title="Multi-Modality Driven LoRA for Adverse Condition Depth Estimation"></a>Multi-Modality Driven LoRA for Adverse Condition Depth Estimation</h2><p><strong>Authors:Guanglei Yang, Rui Tian, Yongqiang Zhang, Zhun Zhong, Yongqiang Li, Wangmeng Zuo</strong></p>
<p>The autonomous driving community is increasingly focused on addressing corner case problems, particularly those related to ensuring driving safety under adverse conditions (e.g., nighttime, fog, rain). To this end, the task of Adverse Condition Depth Estimation (ACDE) has gained significant attention. Previous approaches in ACDE have primarily relied on generative models, which necessitate additional target images to convert the sunny condition into adverse weather, or learnable parameters for feature augmentation to adapt domain gaps, resulting in increased model complexity and tuning efforts. Furthermore, unlike CLIP-based methods where textual and visual features have been pre-aligned, depth estimation models lack sufficient alignment between multimodal features, hindering coherent understanding under adverse conditions. To address these limitations, we propose Multi-Modality Driven LoRA (MMD-LoRA), which leverages low-rank adaptation matrices for efficient fine-tuning from source-domain to target-domain. It consists of two core components: Prompt Driven Domain Alignment (PDDA) and Visual-Text Consistent Contrastive Learning(VTCCL). During PDDA, the image encoder with MMD-LoRA generates target-domain visual representations, supervised by alignment loss that the source-target difference between language and image should be equal. Meanwhile, VTCCL bridges the gap between textual features from CLIP and visual features from diffusion model, pushing apart different weather representations (vision and text) and bringing together similar ones. Through extensive experiments, the proposed method achieves state-of-the-art performance on the nuScenes and Oxford RobotCar datasets, underscoring robustness and efficiency in adapting to varied adverse environments. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶é¢†åŸŸè¶Šæ¥è¶Šå…³æ³¨æç«¯æƒ…å†µé—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯ä¸æ¶åŠ£æ¡ä»¶ä¸‹çš„é©¾é©¶å®‰å…¨ç›¸å…³çš„é—®é¢˜ï¼ˆä¾‹å¦‚å¤œé—´ã€é›¾éœ¾ã€é›¨å¤©ï¼‰ã€‚ä¸ºæ­¤ï¼Œæ¶åŠ£æ¡ä»¶æ·±åº¦ä¼°è®¡ï¼ˆACDEï¼‰ä»»åŠ¡å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ACDEçš„å…ˆå‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹éœ€è¦å°†æ™´æœ—æ¡ä»¶ä¸‹çš„å›¾åƒè½¬æ¢ä¸ºæ¶åŠ£å¤©æ°”ï¼Œæˆ–å­¦ä¹ å‚æ•°ä»¥å¢å¼ºç‰¹å¾ä»¥é€‚åº”åŸŸå·®å¼‚ï¼Œè¿™å¢åŠ äº†æ¨¡å‹å¤æ‚æ€§å’Œè°ƒæ•´å·¥ä½œã€‚æ­¤å¤–ï¼Œä¸åŸºäºCLIPçš„æ–¹æ³•ä¸åŒï¼Œå…¶ä¸­æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å·²ç»é¢„å…ˆå¯¹é½ï¼Œæ·±åº¦ä¼°è®¡æ¨¡å‹çš„å¤šæ¨¡æ€ç‰¹å¾ç¼ºä¹è¶³å¤Ÿçš„å¯¹é½ï¼Œé˜»ç¢äº†æ¶åŠ£æ¡ä»¶ä¸‹çš„è¿è´¯ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€é©±åŠ¨LoRAï¼ˆMMD-LoRAï¼‰ï¼Œå®ƒåˆ©ç”¨ä½ç§©é€‚åº”çŸ©é˜µä»æºåŸŸåˆ°ç›®æ ‡åŸŸè¿›è¡Œé«˜æ•ˆçš„å¾®è°ƒã€‚å®ƒåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæç¤ºé©±åŠ¨åŸŸå¯¹é½ï¼ˆPDDAï¼‰å’Œè§†è§‰æ–‡æœ¬ä¸€è‡´å¯¹æ¯”å­¦ä¹ ï¼ˆVTCCLï¼‰ã€‚åœ¨PDDAæœŸé—´ï¼Œå¸¦æœ‰MMD-LoRAçš„å›¾åƒç¼–ç å™¨ç”Ÿæˆç›®æ ‡åŸŸçš„è§†è§‰è¡¨ç¤ºï¼Œç”±å¯¹é½æŸå¤±ç›‘ç£ï¼Œè¯¥æŸå¤±ä½¿è¯­è¨€å’Œå›¾åƒä¹‹é—´çš„æºç›®æ ‡å·®å¼‚åº”ç›¸ç­‰ã€‚åŒæ—¶ï¼ŒVTCCLå¼¥åˆäº†æ¥è‡ªCLIPçš„æ–‡æœ¬ç‰¹å¾å’Œæ¥è‡ªæ‰©æ•£æ¨¡å‹çš„è§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œå°†ä¸åŒçš„å¤©æ°”è¡¨ç¤ºï¼ˆè§†è§‰å’Œæ–‡æœ¬ï¼‰åˆ†å¼€å¹¶å°†ç›¸ä¼¼çš„è¡¨ç¤ºèšé›†åœ¨ä¸€èµ·ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨nuSceneså’ŒOxford RobotCaræ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†åœ¨é€‚åº”å„ç§æ¶åŠ£ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20162v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­çš„æç«¯æƒ…å†µé—®é¢˜å¤„ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„é©¾é©¶å®‰å…¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸ºAdverse Condition Depth Estimationï¼ˆACDEï¼‰çš„ä»»åŠ¡ã€‚ä¹‹å‰çš„ACDEæ–¹æ³•ä¸»è¦ä¾èµ–ç”Ÿæˆæ¨¡å‹ï¼Œéœ€è¦å¤§é‡ç›®æ ‡å›¾åƒæ¥è½¬æ¢å¤©æ°”æ¡ä»¶æˆ–ä½¿ç”¨å¯å­¦ä¹ å‚æ•°è¿›è¡Œç‰¹å¾å¢å¼ºæ¥é€‚åº”ä¸åŒé¢†åŸŸå·®å¼‚ï¼Œå¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§å’Œè°ƒä¼˜å·¥ä½œã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†Multi-Modality Driven LoRAï¼ˆMMD-LoRAï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä½ç§©é€‚åº”çŸ©é˜µå®ç°é«˜æ•ˆå¾®è°ƒä»æºåŸŸåˆ°ç›®æ ‡åŸŸçš„è¿‡ç¨‹ã€‚å®ƒåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šPrompt Driven Domain Alignmentï¼ˆPDDAï¼‰å’ŒVisual-Text Consistent Contrastive Learningï¼ˆVTCCLï¼‰ã€‚PDDAä½¿å›¾åƒç¼–ç å™¨ç”Ÿæˆç›®æ ‡åŸŸè§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡å¯¹é½æŸå¤±å®ç°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„è¯­è¨€ä¸å›¾åƒå·®å¼‚å¹³è¡¡ã€‚è€ŒVTCCLåˆ™ç¼©å°äº†CLIPæ–‡æœ¬ç‰¹å¾å’Œæ‰©æ•£æ¨¡å‹è§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œå°†ä¸åŒçš„å¤©æ°”è¡¨ç¤ºï¼ˆè§†è§‰å’Œæ–‡æœ¬ï¼‰åŒºåˆ†å¼€å¹¶æ‹‰è¿‘ç›¸ä¼¼çš„è¡¨ç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨nuSceneså’ŒOxford RobotCaræ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå±•ç°äº†åœ¨ä¸åŒæ¶åŠ£ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…³æ³¨æç«¯æƒ…å†µé—®é¢˜å¤„ç†ï¼Œå°¤å…¶æ˜¯æ¶åŠ£æ¡ä»¶ä¸‹çš„é©¾é©¶å®‰å…¨ã€‚</li>
<li>Adverse Condition Depth Estimation (ACDE)ä»»åŠ¡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ä»¥å¾€çš„ACDEæ–¹æ³•ä¸»è¦ä¾èµ–ç”Ÿæˆæ¨¡å‹ï¼Œå­˜åœ¨æ¨¡å‹å¤æ‚åº¦é«˜å’Œè°ƒä¼˜å·¥ä½œé‡å¤§çš„é—®é¢˜ã€‚</li>
<li>Multi-Modality Driven LoRA (MMD-LoRA)æ–¹æ³•é€šè¿‡ä½ç§©é€‚åº”çŸ©é˜µå®ç°é«˜æ•ˆå¾®è°ƒæºåŸŸåˆ°ç›®æ ‡åŸŸçš„è¿‡ç¨‹ã€‚</li>
<li>PDDAç»„ä»¶é€šè¿‡å›¾åƒç¼–ç å™¨ç”Ÿæˆç›®æ ‡åŸŸè§†è§‰è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¯¹é½æŸå¤±å®ç°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„è¯­è¨€ä¸å›¾åƒå¹³è¡¡ã€‚</li>
<li>VTCCLç»„ä»¶ç¼©å°äº†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œæé«˜äº†ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d6ba7e737ca1fd7784d8845ded42829.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-110a999a5ed86e6cfa01e5eb58c42228.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41e9dfb01cefe108ebdd6fcd1acf3820.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78889cb0db855720c42c5d2c166f4375.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15a4f6cdc42ed92a1bd3ccf2ec1032c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP"><a href="#Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP" class="headerlink" title="Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP"></a>Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP</h2><p><strong>Authors:Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge</strong></p>
<p>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets. </p>
<blockquote>
<p>åœ¨å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ç ”ç©¶ä¸­ï¼Œåº”ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å…·æœ‰å¼ºå¤§çš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å¾®è°ƒæ–‡æœ¬æç¤ºæ¥ä¼˜åŒ–å›¾åƒå’Œæ–‡æœ¬çš„å¯¹é½æ–¹å¼ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´æ–‡æœ¬åŸå‹æ¥ä¿ƒè¿›è¯­ä¹‰åŒ¹é…ã€‚ç„¶è€Œï¼Œé‰´äºæ–‡æœ¬å’Œè§†è§‰ç©ºé—´ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œè¿™äº›æ–¹æ³•æ‰€é‡‡ç”¨çš„æ–‡æœ¬åŸå‹å¹¶æœªæœ‰æ•ˆåœ°ä¸åƒç´ çº§è§†è§‰ç‰¹å¾å»ºç«‹ç´§å¯†å¯¹åº”å…³ç³»ã€‚åœ¨æˆ‘ä»¬çš„ç†è®ºåˆ†æä¸­ï¼ŒæŒ‡å‡ºå›ºæœ‰çš„æ¨¡æ€å·®è·ä¼šå¯¼è‡´æ–‡æœ¬å’ŒåŒºåŸŸç‰¹å¾çš„é”™ä½ï¼Œå¹¶ä¸”ä»…ä»…é€šè¿‡å‡å°CLIPä¸­çš„å¯¹æ¯”æŸå¤±ä¸è¶³ä»¥å……åˆ†ç¼©å°è¿™ä¸€å·®è·ã€‚ä¸ºäº†ç¼“è§£æ¨¡æ€å·®è·çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åŸå‹å­¦ä¹ ï¼ˆVPLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ›´å…·ä»£è¡¨æ€§çš„è§†è§‰åŸå‹ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åœ¨è§†è§‰ç©ºé—´çš„å¸®åŠ©ä¸‹ï¼Œå­¦ä¹ ç‰¹å®šç±»åˆ«çš„è§†è§‰åŸå‹ï¼Œä»¥æ•è·é«˜è´¨é‡çš„å®šä½å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŒºåŸŸè¯­ä¹‰å¯¹æ¯”æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åŒºåŸŸåµŒå…¥ä¸ç›¸åº”çš„åŸå‹è¿›è¡Œå¯¹æ¯”ï¼Œä»è€Œå®ç°æ›´å…¨é¢å’Œç¨³å¥çš„ç‰¹å¾å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19650v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†Contrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰åœ¨Weakly Supervised Semantic Segmentationï¼ˆWSSSï¼‰ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºå½“å‰æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥æ”¹è¿›å›¾åƒå’Œæ–‡æœ¬çš„åŒ¹é…åº¦ï¼Œä½†æ–‡æœ¬åŸå‹å¹¶æœªæœ‰æ•ˆåœ°ä¸åƒç´ çº§è§†è§‰ç‰¹å¾å»ºç«‹ç´§å¯†å¯¹åº”å…³ç³»ã€‚æœ¬æ–‡åˆ†æäº†æ–‡æœ¬å’Œè§†è§‰ç©ºé—´ä¹‹é—´çš„å›ºæœ‰æ¨¡æ€å·®è·å¯¼è‡´çš„æ–‡æœ¬å’ŒåŒºåŸŸç‰¹å¾ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶æå‡ºé€šè¿‡å¼•å…¥æ›´å…·ä»£è¡¨æ€§çš„è§†è§‰åŸå‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†Vision Prototype Learningï¼ˆVPLï¼‰æ¡†æ¶å’ŒåŒºåŸŸè¯­ä¹‰å¯¹æ¯”æ¨¡å—ï¼Œå®ç°äº†é«˜è´¨é‡å®šä½å›¾çš„æ•è·å’Œæ›´å…¨é¢ã€ç¨³å¥çš„ç‰¹å¾å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨WSSSç ”ç©¶ä¸­çš„åº”ç”¨å±•ç°äº†å…¶å¼ºå¤§çš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šè¿‡å¾®è°ƒæ–‡æœ¬åŸå‹æ¥ä¼˜åŒ–å›¾åƒå’Œæ–‡æœ¬çš„åŒ¹é…åº¦ï¼Œä½†ä»å­˜åœ¨æ¨¡æ€å·®è·é—®é¢˜ã€‚</li>
<li>æ¨¡æ€å·®è·å¯¼è‡´æ–‡æœ¬å’ŒåŒºåŸŸç‰¹å¾çš„ä¸åŒ¹é…ï¼Œä»…é€šè¿‡å‡å°CLIPä¸­çš„å¯¹æ¯”æŸå¤±æ— æ³•å……åˆ†å‡å°‘è¿™ç§å·®è·ã€‚</li>
<li>æå‡ºäº†Vision Prototype Learningï¼ˆVPLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ›´å…·ä»£è¡¨æ€§çš„è§†è§‰åŸå‹æ¥è§£å†³æ¨¡æ€å·®è·é—®é¢˜ã€‚</li>
<li>VPLæ¡†æ¶çš„æ ¸å¿ƒæ˜¯åœ¨è§†è§‰ç©ºé—´çš„å¸®åŠ©ä¸‹å­¦ä¹ ç‰¹å®šç±»åˆ«çš„è§†è§‰åŸå‹ï¼Œä»¥æ•è·é«˜è´¨é‡å®šä½å›¾ã€‚</li>
<li>æå‡ºäº†åŒºåŸŸè¯­ä¹‰å¯¹æ¯”æ¨¡å—ï¼Œé€šè¿‡åŒºåŸŸåµŒå…¥ä¸ç›¸åº”åŸå‹çš„å¯¹æ¯”ï¼Œå®ç°æ›´å…¨é¢å’Œç¨³å¥çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90ee0f753b63bdc6501a80c978c40689.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b4383b6e1ce4886b79ce30a149d538c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6d4d4b8aed96474ae553e45f0a7d71.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis"><a href="#VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis" class="headerlink" title="VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis"></a>VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis</h2><p><strong>Authors:Shicheng Yin, Kaixuan Yin, Weixing Chen, Enbo Huang, Yang Liu</strong></p>
<p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two dominant models for image analysis. While CNNs excel at extracting multi-scale features and ViTs effectively capture global dependencies, both suffer from high computational costs, particularly when processing high-resolution images. Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have attracted attention due to their efficiency. However, their performance in image classification tasks remains limited. To address these challenges, this paper introduces VisionGRU, a novel RNN-based architecture designed for efficient image classification. VisionGRU leverages a simplified Gated Recurrent Unit (minGRU) to process large-scale image features with linear complexity. It divides images into smaller patches and progressively reduces the sequence length while increasing the channel depth, thus facilitating multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional scanning captures both local and global contexts, improving long-range dependency modeling, particularly for tasks like semantic segmentation. Experimental results on the ImageNet and ADE20K datasets demonstrate that VisionGRU outperforms ViTs, significantly reducing memory usage and computational costs, especially for high-resolution images. These findings underscore the potential of RNN-based approaches for developing efficient and scalable computer vision solutions. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ˜¯å›¾åƒåˆ†æçš„ä¸¤ç§ä¸»å¯¼æ¨¡å‹ã€‚è™½ç„¶CNNæ“…é•¿æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œè€ŒViTèƒ½æœ‰æ•ˆåœ°æ•æ‰å…¨å±€ä¾èµ–æ€§ï¼Œä½†ä¸¤è€…éƒ½å­˜åœ¨ç€è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚æœ€è¿‘ï¼Œç”±äºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„æ•ˆç‡ï¼Œå®ƒä»¬å—åˆ°äº†äººä»¬çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆå›¾åƒåˆ†ç±»çš„æ–°å‹RNNæ¶æ„â€”â€”VisionGRUã€‚VisionGRUåˆ©ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ã€‚å®ƒå°†å›¾åƒåˆ†æˆè¾ƒå°çš„æ–‘å—ï¼Œé€æ­¥å‡å°‘åºåˆ—é•¿åº¦ï¼ŒåŒæ—¶å¢åŠ é€šé“æ·±åº¦ï¼Œä»è€Œä¾¿äºå¤šå°ºåº¦ç‰¹å¾æå–ã€‚å…·æœ‰åŒå‘æ‰«æçš„åˆ†å±‚2DGRUæ¨¡å—å¯ä»¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›äº†é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ã€‚åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionGRUä¼˜äºViTï¼Œå¤§å¤§é™ä½äº†å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŸºäºRNNçš„æ–¹æ³•åœ¨å¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„è®¡ç®—æœºè§†è§‰è§£å†³æ–¹æ¡ˆæ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18178v2">PDF</a> Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºRNNçš„æ–°å‹å›¾åƒåˆ†ç±»æ¶æ„VisionGRUï¼Œåˆ©ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ã€‚VisionGRUé€šè¿‡åˆ†å‰²å›¾åƒæˆå°å—å¹¶æ¸è¿›å‡å°‘åºåˆ—é•¿åº¦åŒæ—¶å¢åŠ é€šé“æ·±åº¦ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾æå–ã€‚å…¶å±‚æ¬¡åŒ–çš„2DGRUæ¨¡å—ç»“åˆåŒå‘æ‰«æï¼Œæ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›äº†é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionGRUä¼˜äºViTsï¼Œå¤§å¹…é™ä½äº†å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ï¼Œå°¤å…¶é€‚ç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionGRUæ˜¯ä¸€ç§æ–°å‹çš„RNNæ¶æ„ï¼Œç”¨äºå›¾åƒåˆ†ç±»ï¼Œç»“åˆäº†CNNå’ŒViTçš„ä¼˜ç‚¹ã€‚</li>
<li>VisionGRUä½¿ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚</li>
<li>VisionGRUé€šè¿‡åˆ†å‰²å›¾åƒæˆå°å—å®ç°å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œæ¸è¿›å‡å°‘åºåˆ—é•¿åº¦åŒæ—¶å¢åŠ é€šé“æ·±åº¦ã€‚</li>
<li>å±‚æ¬¡åŒ–çš„2DGRUæ¨¡å—ç»“åˆåŒå‘æ‰«æï¼Œèƒ½æœ‰æ•ˆæ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ã€‚</li>
<li>VisionGRUåœ¨è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºViTsã€‚</li>
<li>VisionGRUé™ä½äº†å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ï¼Œå°¤å…¶é€‚ç”¨äºå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a85fe118fa527a73edc6f2d8766f4732.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd437ed5c29515688f16cc8c0903e437.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73664fb72c773b1bf490d6a20e930639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fecfa3a65b425b6a4fd26d38b4a8f5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantics-Prompting-Data-Free-Quantization-for-Low-Bit-Vision-Transformers"><a href="#Semantics-Prompting-Data-Free-Quantization-for-Low-Bit-Vision-Transformers" class="headerlink" title="Semantics Prompting Data-Free Quantization for Low-Bit Vision   Transformers"></a>Semantics Prompting Data-Free Quantization for Low-Bit Vision   Transformers</h2><p><strong>Authors:Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Shen Li, Yong Li, Fei Chao, Zhanpeng Zeng, Rongrong Ji</strong></p>
<p>Data-free quantization (DFQ), which facilitates model quantization without real data to address increasing concerns about data security, has garnered significant attention within the model compression community. Recently, the unique architecture of vision transformers (ViTs) has driven the development of specialized DFQ techniques. However, we observe that the synthetic images from existing methods suffer from the deficient semantics issue compared to real images, thereby compromising performance. Motivated by this, we propose SPDFQ, a Semantics Prompting Data-Free Quantization method for ViTs. First, SPDFQ incorporates Attention Priors Alignment (APA), which uses randomly generated attention priors to enhance the semantics of synthetic images. Second, SPDFQ introduces Multi-Semantic Reinforcement (MSR), which utilizes localized patch optimization to prompt efficient parameterization and diverse semantics in synthetic images. Finally, SPDFQ employs Softlabel Learning (SL), where soft learning targets are adapted to encourage more complex semantics and accommodate images augmented by MSR. Experimental results demonstrate that SPDFQ significantly outperforms existing methods. For instance, SPDFQ achieves a 15.52% increase in top-1 accuracy on ImageNet for W4A4 ViT-B </p>
<blockquote>
<p>æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰æŠ€æœ¯å› å…¶èƒ½å¤Ÿåœ¨æ— éœ€çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œç¼“è§£äº†æ•°æ®å®‰å…¨æ–¹é¢çš„æ‹…å¿§ï¼Œåœ¨æ¨¡å‹å‹ç¼©é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æœ€è¿‘ï¼Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„ç‹¬ç‰¹æ¶æ„æ¨åŠ¨äº†ä¸“é—¨çš„DFQæŠ€æœ¯çš„å‘å±•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒä¸çœŸå®å›¾åƒç›¸æ¯”å­˜åœ¨è¯­ä¹‰ç¼ºé™·é—®é¢˜ï¼Œä»è€Œå½±å“äº†æ€§èƒ½ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹ViTçš„è¯­ä¹‰æç¤ºæ— æ•°æ®é‡åŒ–æ–¹æ³•SPDFQã€‚é¦–å…ˆï¼ŒSPDFQç»“åˆäº†æ³¨æ„åŠ›å…ˆéªŒå¯¹é½ï¼ˆAPAï¼‰ï¼Œä½¿ç”¨éšæœºç”Ÿæˆçš„æ³¨æ„åŠ›å…ˆéªŒæ¥å¢å¼ºåˆæˆå›¾åƒçš„è¯­ä¹‰ã€‚å…¶æ¬¡ï¼ŒSPDFQå¼•å…¥äº†å¤šè¯­ä¹‰å¢å¼ºï¼ˆMSRï¼‰ï¼Œåˆ©ç”¨å±€éƒ¨æ–‘å—ä¼˜åŒ–æ¥æç¤ºåˆæˆå›¾åƒä¸­çš„é«˜æ•ˆå‚æ•°åŒ–å’Œå¤šæ ·è¯­ä¹‰ã€‚æœ€åï¼ŒSPDFQé‡‡ç”¨è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œå…¶ä¸­è½¯å­¦ä¹ ç›®æ ‡æ˜¯é€‚åº”å’Œé¼“åŠ±æ›´å¤æ‚çš„è¯­ä¹‰ï¼Œå¹¶é€‚åº”ç”±MSRå¢å¼ºçš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPDFQæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetä¸Šï¼ŒSPDFQåœ¨W4A4 ViT-Bçš„top-1å‡†ç¡®ç‡ä¸Šæé«˜äº†15.52%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16553v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°æ®æ— å…³çš„é‡åŒ–æ–¹æ³•ï¼ˆDFQï¼‰åœ¨æ¨¡å‹å‹ç¼©é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œè§£å†³äº†å¯¹æ•°æ®å®‰å…¨æ€§çš„æ‹…å¿§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„è¯­ä¹‰æç¤ºæ•°æ®æ— å…³é‡åŒ–æ–¹æ³•ï¼ˆSPDFQï¼‰ã€‚SPDFQåŒ…æ‹¬æ³¨æ„åŠ›å…ˆéªŒå¯¹é½ï¼ˆAPAï¼‰ã€å¤šè¯­ä¹‰å¢å¼ºï¼ˆMSRï¼‰å’Œè½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œä»¥æé«˜åˆæˆå›¾åƒçš„è¯­ä¹‰æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPDFQæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ImageNetä¸Šçš„top-1å‡†ç¡®ç‡æé«˜äº†15.52%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ— å…³çš„é‡åŒ–ï¼ˆDFQï¼‰æ–¹æ³•æ— éœ€çœŸå®æ•°æ®å³å¯å®ç°æ¨¡å‹é‡åŒ–ï¼Œè§£å†³æ•°æ®å®‰å…¨æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰DFQæ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒå­˜åœ¨è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„SPDFQæ–¹æ³•æ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…æ‹¬APAã€MSRå’ŒSLä¸‰å¤§ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>APAåˆ©ç”¨éšæœºç”Ÿæˆçš„æ³¨æ„åŠ›å…ˆéªŒå¢å¼ºåˆæˆå›¾åƒçš„è¯­ä¹‰ã€‚</li>
<li>MSRé€šè¿‡å±€éƒ¨è¡¥ä¸ä¼˜åŒ–å®ç°é«˜æ•ˆå‚æ•°åŒ–å’Œåˆæˆå›¾åƒä¸­ä¸°å¯Œçš„è¯­ä¹‰æç¤ºã€‚</li>
<li>SLé‡‡ç”¨è½¯å­¦ä¹ ç›®æ ‡æ¥é¼“åŠ±æ›´å¤æ‚çš„è¯­ä¹‰å¹¶é€‚åº”ç”±MSRå¢å¼ºçš„å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fba7b8c0df5e84af44c46d3c4f65b7c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bdcf7f10af23fb123b745fa1a1a2e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6482bf5fa3b12700286ab1d6759e61ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11803b47a55e718bd7ecfb4b194c8033.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning"><a href="#FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning" class="headerlink" title="FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning"></a>FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning</h2><p><strong>Authors:Gaojian Wang, Feng Lin, Tong Wu, Zhenguang Liu, Zhongjie Ba, Kui Ren</strong></p>
<p>This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region consistency and challenging inter-region coherency. Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global correspondence via tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision foundation model for downstream face security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šåœ¨å¤§é‡æ— æ ‡ç­¾çš„çœŸå®äººè„¸æƒ…å†µä¸‹ï¼Œå¦‚ä½•å­¦ä¹ ä¸€ç§ç¨³å¥ä¸”å¯è¿ç§»çš„äººè„¸è¡¨ç¤ºï¼Œä»¥æé«˜å„ç§äººè„¸å®‰å…¨ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ï¼Ÿæˆ‘ä»¬é¦–æ¬¡å°è¯•å¹¶æå‡ºä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºï¼ŒFSFMï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹åˆ¤åˆ«ï¼ˆIDï¼‰ä¹‹é—´çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†MIMçš„å„ç§é¢éƒ¨æ©è†œç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„CRFR-Pæ©è†œï¼Œå®ƒæ˜ç¡®åœ°è¿«ä½¿æ¨¡å‹æ•æ‰åŒºåŸŸå†…æœ‰æ„ä¹‰çš„ä¸€è‡´æ€§ä»¥åŠåŒºåŸŸé—´çš„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªä¸MIMè‡ªç„¶ç»“åˆçš„IDç½‘ç»œï¼Œé€šè¿‡å®šåˆ¶çš„è‡ªè’¸é¦å»ºç«‹åŸºæœ¬çš„å±€éƒ¨åˆ°å…¨å±€å¯¹åº”å…³ç³»ã€‚è¿™ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼Œä½¿ç¼–ç çœŸå®äººè„¸çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰æˆä¸ºå¯èƒ½ã€‚é¢„è®­ç»ƒåï¼Œä¸€ä¸ªæ™®é€šçš„ViTä½œä¸ºé€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç”¨äºä¸‹æ¸¸äººè„¸å®‰å…¨ä»»åŠ¡ï¼šè·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªã€æœªè§æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ã€‚åœ¨10ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿ç§»æ•ˆæœä¼˜äºç›‘ç£é¢„è®­ç»ƒã€è§†è§‰å’Œé¢éƒ¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œç”šè‡³è¶…è¶Šäº†ä»»åŠ¡ä¸“ä¸šåŒ–çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12032v2">PDF</a> 21 pages, 11 figures, project page: <a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/">https://fsfm-3c.github.io</a></p>
<p><strong>Summary</strong><br>è¯¥å·¥ä½œé€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒå­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºï¼Œé‡‡ç”¨æ©æ¨¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹åˆ¤åˆ«ï¼ˆIDï¼‰çš„ç»“åˆã€‚æå‡ºå¤šç§é¢éƒ¨æ©æ¨¡ç­–ç•¥ï¼Œå…¶ä¸­CRFR-Pæ©æ¨¡èƒ½ä¿ƒä½¿æ¨¡å‹æ•æ‰åŒºåŸŸå†…ä¸€è‡´æ€§åŠåŒºåŸŸé—´è¿è´¯æ€§ã€‚åŒæ—¶è®¾è®¡IDç½‘ç»œï¼Œä¸MIMç»“åˆå»ºç«‹å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ã€‚é€šè¿‡ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼ˆ3Cï¼‰å®ç°æœ¬åœ°ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰çš„ç¼–ç ã€‚é¢„è®­ç»ƒåï¼Œå¯ä½œä¸ºä¸‹æ¸¸äººè„¸å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¦‚è·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªå’Œæœªè§æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ã€‚åœ¨10ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„è¿ç§»æ•ˆæœä¼˜äºç›‘ç£é¢„è®­ç»ƒå’Œå…¶ä»–é¢éƒ¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†ä»»åŠ¡ä¸“ä¸šåŒ–çš„SOTAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥å·¥ä½œæå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶æ¥å­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚</li>
<li>ç»“åˆäº†æ©æ¨¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹åˆ¤åˆ«ï¼ˆIDï¼‰ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é¢éƒ¨æ©æ¨¡ç­–ç•¥â€”â€”CRFR-Pï¼Œå¼ºè°ƒåŒºåŸŸå†…å’ŒåŒºåŸŸé—´çš„è¿è´¯æ€§ã€‚</li>
<li>è®¾è®¡äº†ä¸MIMç»“åˆçš„IDç½‘ç»œï¼Œå»ºç«‹å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>é€šè¿‡ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡å®ç°æœ¬åœ°ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰çš„ç¼–ç ï¼Œæé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œè¿ç§»èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šç§äººè„¸å®‰å…¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬è·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€é¢éƒ¨é˜²ä¼ªç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f18d01f38cf3f280b2c52aac9f1c746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6fe8794954004c0d53cb83f71153bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed3ef88cb703ce660e6cf558c18a841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbdd69074a50d3e26d3aeab3f98d8cfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45dd4ef520c7cc87cd57e415e4c9b7f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation"><a href="#MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation"></a>MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation</h2><p><strong>Authors:Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe">https://github.com/zwyang6/MoRe</a>. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é€šå¸¸ä½¿ç”¨å›¾åƒçº§åˆ«çš„æ ‡ç­¾å’Œç±»åˆ«æ¿€æ´»å›¾ï¼ˆCAMï¼‰æ¥å®ç°å¯†é›†é¢„æµ‹ã€‚æœ€è¿‘ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æä¾›äº†ä¸€ç§ç”Ÿæˆå®šä½å›¾ï¼ˆLocalization Attention Mapsï¼ŒLAMï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ç±»åˆ«è¡¥ä¸æ³¨æ„åŠ›ç”Ÿæˆå®šä½å›¾ã€‚ç„¶è€Œï¼Œç”±äºå¯¹è¿™ç±»æ³¨æ„åŠ›çš„å»ºæ¨¡çº¦æŸä¸è¶³ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰ç»å¸¸é¢ä¸´ä¼ªå½±é—®é¢˜ï¼Œå³è¯­ä¹‰ç›¸å…³æ€§æå°çš„è¡¥ä¸åŒºåŸŸä¼šè¢«ç±»åˆ«æ ‡è®°é”™è¯¯æ¿€æ´»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºMoReæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶LAMçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹ç±»åˆ«è¡¥ä¸æ³¨æ„åŠ›æ–½åŠ é¢å¤–çš„æ­£åˆ™åŒ–æ˜¯å¿…è¦çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ³¨æ„åŠ›è§†ä¸ºä¸€ç§æ–°å‹çš„æœ‰å‘å›¾ï¼Œå¹¶æå‡ºå›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—æ¥éšå«åœ°æ­£åˆ™åŒ–ç±»åˆ«è¡¥ä¸å®ä½“ä¹‹é—´çš„äº¤äº’ã€‚å®ƒç¡®ä¿ç±»åˆ«æ ‡è®°åŠ¨æ€åœ°å‡èšç›¸å…³è¡¥ä¸ä¿¡æ¯ï¼Œå¹¶åœ¨å›¾å½¢çº§åˆ«æŠ‘åˆ¶ä¸ç›¸å…³çš„ä¼ªå½±ã€‚å…¶æ¬¡ï¼Œå—åˆ†ç±»æƒé‡CAMèƒ½ç»´æŒå¯¹è±¡å®šä½å¹³æ»‘çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†å®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—æ¥æ˜¾å¼åœ°æ­£åˆ™åŒ–ç±»åˆ«è¡¥ä¸æ³¨æ„åŠ›ã€‚å®ƒä»CAMä¸­æå–æ ‡è®°å…³ç³»ï¼Œå¹¶ä»¥å¯å­¦ä¹ çš„æ–¹å¼ç›‘ç£ç±»åˆ«æ ‡è®°å’Œè¡¥ä¸æ ‡è®°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨PASCAL VOCå’ŒMS COCOä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†MoReæœ‰æ•ˆåœ°è§£å†³äº†ä¼ªå½±é—®é¢˜ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¶…è¶Šäº†æœ€æ–°çš„å•é˜¶æ®µç”šè‡³å¤šé˜¶æ®µæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zwyang6/MoReä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11076v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰çš„é—®é¢˜ã€‚è™½ç„¶Vision Transformerï¼ˆViTï¼‰å¯ä»¥é€šè¿‡ç±»è¡¥ä¸æ³¨æ„åŠ›ç”Ÿæˆå®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰ï¼Œä½†ç”±äºç¼ºä¹å¯¹è¿™ç±»æ³¨æ„åŠ›çš„å……åˆ†çº¦æŸï¼ŒLAMå®¹æ˜“å‡ºç°ä¼ªæ¿€æ´»ç°è±¡ï¼Œå³æ— å…³è¡¥ä¸åŒºåŸŸè¢«ç±»æ ‡è®°é”™è¯¯æ¿€æ´»ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MoReæ–¹æ³•ï¼Œå¹¶æ¢ç´¢äº†LAMçš„æ½œåŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹ç±»è¡¥ä¸æ³¨æ„åŠ›æ–½åŠ é¢å¤–çš„æ­£åˆ™åŒ–æ˜¯å¿…è¦çš„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é¦–å…ˆå…³æ³¨æ³¨æ„åŠ›ä½œä¸ºä¸€ç§æ–°å‹æœ‰å‘å›¾ï¼Œå¹¶æå‡ºå›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—æ¥éšå«åœ°è§„èŒƒç±»è¡¥ä¸å®ä½“é—´çš„äº¤äº’ã€‚åŒæ—¶ï¼Œå—åˆ†ç±»æƒé‡CAMä¿æŒå¯¹è±¡å®šä½å¹³æ»‘çš„å¯å‘ï¼Œæœ¬æ–‡è®¾è®¡äº†å®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—æ¥æ˜¾å¼åœ°è§„èŒƒç±»è¡¥ä¸æ³¨æ„åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒMoReæœ‰æ•ˆåœ°è§£å†³äº†ä¼ªæ¿€æ´»é—®é¢˜ï¼Œå®ç°äº†PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šçš„æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†è¿‘æœŸå•é˜¶æ®µç”šè‡³å¤šé˜¶æ®µæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) å¯é€šè¿‡ç±»è¡¥ä¸æ³¨æ„åŠ›ç”Ÿæˆå®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰ã€‚</li>
<li>LAMå­˜åœ¨ä¼ªæ¿€æ´»é—®é¢˜ï¼Œå³æ— å…³è¡¥ä¸åŒºåŸŸè¢«ç±»æ ‡è®°é”™è¯¯æ¿€æ´»ã€‚</li>
<li>å¯¹ç±»è¡¥ä¸æ³¨æ„åŠ›æ–½åŠ é¢å¤–çš„æ­£åˆ™åŒ–æ˜¯è§£å†³LAMä¼ªæ¿€æ´»é—®é¢˜çš„å…³é”®ã€‚</li>
<li>æå‡ºäº†å›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—ï¼Œéšå«åœ°è§„èŒƒç±»è¡¥ä¸å®ä½“é—´çš„äº¤äº’ã€‚</li>
<li>è®¾è®¡äº†å®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—ï¼Œæ˜¾å¼åœ°è§„èŒƒç±»è¡¥ä¸æ³¨æ„åŠ›ï¼Œåˆ©ç”¨åˆ†ç±»æƒé‡CAMçš„å¹³æ»‘å®šä½ç‰¹æ€§ã€‚</li>
<li>MoReæ–¹æ³•åœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-872f56d51aa5d225fb4d9dbe916addf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-558c55d8af6b022179480697698069b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3e4750dfaf5c1d3fd23143f2d50fec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699cfbb8aa515db4cbb28229318109c5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HiRED-Attention-Guided-Token-Dropping-for-Efficient-Inference-of-High-Resolution-Vision-Language-Models"><a href="#HiRED-Attention-Guided-Token-Dropping-for-Efficient-Inference-of-High-Resolution-Vision-Language-Models" class="headerlink" title="HiRED: Attention-Guided Token Dropping for Efficient Inference of   High-Resolution Vision-Language Models"></a>HiRED: Attention-Guided Token Dropping for Efficient Inference of   High-Resolution Vision-Language Models</h2><p><strong>Authors:Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, Bo Ji</strong></p>
<p>High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to the need to encode multiple partitions of a high-resolution image input. Processing such a large number of visual tokens through multiple transformer networks poses significant computational challenges, particularly for resource-constrained commodity GPUs. To address this challenge, we propose High-Resolution Early Dropping (HiRED), a plug-and-play token-dropping method designed to operate within a fixed token budget. HiRED leverages the attention of CLS token in the vision transformer (ViT) to assess the visual content of the image partitions and allocate an optimal token budget for each partition accordingly. The most informative visual tokens from each partition within the allocated budget are then selected and passed to the subsequent Large Language Model (LLM). We showed that HiRED achieves superior accuracy and performance, compared to existing token-dropping methods. Empirically, HiRED-20% (i.e., a 20% token budget) on LLaVA-Next-7B achieves a 4.7x increase in token generation throughput, reduces response latency by 78%, and saves 14% of GPU memory for single inference on an NVIDIA TESLA P40 (24 GB). For larger batch sizes (e.g., 4), HiRED-20% prevents out-of-memory errors by cutting memory usage by 30%, while preserving throughput and latency benefits.   Code - <a target="_blank" rel="noopener" href="https://github.com/hasanar1f/HiRED">https://github.com/hasanar1f/HiRED</a> </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡ä¿ç•™è¯¦ç»†çš„å›¾åƒä¿¡æ¯æ¥æé«˜å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒè¾“å…¥çš„å¤šä¸ªåˆ†åŒºè¿›è¡Œç¼–ç ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¼šäº§ç”Ÿè¿‡å¤šçš„è§†è§‰ä»¤ç‰Œã€‚é€šè¿‡å¤šä¸ªtransformerç½‘ç»œå¤„ç†å¦‚æ­¤å¤§é‡çš„è§†è§‰ä»¤ç‰Œï¼Œå¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºèµ„æºå—é™çš„å•†å“GPUã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜åˆ†è¾¨ç‡æ—©æœŸä¸¢å¼ƒï¼ˆHiREDï¼‰ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„ä»¤ç‰Œä¸¢å¼ƒæ–¹æ³•ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªå›ºå®šçš„ä»¤ç‰Œé¢„ç®—å†…è¿è¡Œã€‚HiREDåˆ©ç”¨è§†è§‰transformerï¼ˆViTï¼‰ä¸­CLSä»¤ç‰Œçš„æ³¨æ„åŠ›æ¥è¯„ä¼°å›¾åƒåˆ†åŒºçš„è§†è§‰å†…å®¹ï¼Œå¹¶ç›¸åº”åœ°åˆ†é…æ¯ä¸ªåˆ†åŒºçš„æœ€ä½³ä»¤ç‰Œé¢„ç®—ã€‚ç„¶åï¼Œä»æ¯ä¸ªåˆ†åŒºä¸­é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§‰ä»¤ç‰Œï¼Œå¹¶å°†å…¶ä¼ é€’ç»™éšåçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸ç°æœ‰çš„ä»¤ç‰Œä¸¢å¼ƒæ–¹æ³•ç›¸æ¯”ï¼ŒHiREDåœ¨å‡†ç¡®æ€§å’Œæ€§èƒ½ä¸Šå‡è¾¾åˆ°äº†ä¼˜è¶Šæ°´å¹³ã€‚åœ¨å®è·µä¸­ï¼ŒHiRED-20%ï¼ˆå³20%çš„ä»¤ç‰Œé¢„ç®—ï¼‰åœ¨LLaVA-Next-7Bä¸Šå®ç°äº†ä»¤ç‰Œç”Ÿæˆååé‡4.7å€çš„æå‡ï¼Œå“åº”å»¶è¿Ÿå‡å°‘äº†78%ï¼Œå¹¶åœ¨NVIDIA TESLA P40ï¼ˆ24GBï¼‰ä¸Šå•æ¬¡æ¨ç†èŠ‚çœäº†14%çš„GPUå†…å­˜ã€‚å¯¹äºè¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°ï¼ˆä¾‹å¦‚4ï¼‰ï¼ŒHiRED-20%é€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨30%æ¥é˜²æ­¢å†…å­˜æº¢å‡ºé”™è¯¯ï¼ŒåŒæ—¶ä¿æŒååé‡å’Œå»¶è¿Ÿä¼˜åŠ¿ã€‚ä»£ç -<a target="_blank" rel="noopener" href="https://github.com/hasanar1f/HiRED">https://github.com/hasanar1f/HiRED</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10945v3">PDF</a> Accepted in AAAI 2025</p>
<p><strong>Summary</strong><br>     é«˜è§£æåº¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œæ—¨åœ¨ä¿ç•™å›¾åƒè¯¦ç»†ä¿¡æ¯ä»¥æé«˜å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦ç¼–ç é«˜è§£æåº¦å›¾åƒçš„å¤šä¸ªåˆ†åŒºï¼Œè¿™äº›æ¨¡å‹ä¼šäº§ç”Ÿè¿‡å¤šçš„è§†è§‰ä»¤ç‰Œã€‚åœ¨èµ„æºå—é™çš„å•†å“GPUä¸Šå¤„ç†å¤§é‡è§†è§‰ä»¤ç‰Œä¼šå¸¦æ¥é‡å¤§è®¡ç®—æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºHigh-Resolution Early Droppingï¼ˆHiREDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å›ºå®šä»¤ç‰Œé¢„ç®—å†…è¿è¡Œçš„å³æ’å³ç”¨ä»¤ç‰Œä¸¢å¼ƒæ–¹æ³•ã€‚HiREDåˆ©ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸­çš„CLSä»¤ç‰Œçš„æ³¨æ„åŠ›æ¥è¯„ä¼°å›¾åƒåˆ†åŒºçš„è§†è§‰å†…å®¹ï¼Œå¹¶ä¸ºæ¯ä¸ªåˆ†åŒºåˆ†é…æœ€ä½³ä»¤ç‰Œé¢„ç®—ã€‚éšåï¼Œä»æ¯ä¸ªåˆ†åŒºä¸­é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§‰ä»¤ç‰Œï¼Œå¹¶å°†å…¶ä¼ é€’ç»™éšåçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç›¸æ¯”ç°æœ‰çš„ä»¤ç‰Œä¸¢å¼ƒæ–¹æ³•ï¼ŒHiREDåœ¨å‡†ç¡®æ€§å’Œæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨LLaVA-Next-7Bä¸Šï¼ŒHiRED-20%ï¼ˆå³20%ä»¤ç‰Œé¢„ç®—ï¼‰ä½¿ä»¤ç‰Œç”Ÿæˆååé‡å¢åŠ äº†4.7å€ï¼Œå“åº”å»¶è¿Ÿå‡å°‘äº†78%ï¼Œå¹¶èŠ‚çœäº†å•ä¸ªæ¨ç†çš„GPUå†…å­˜è¾¾14%ã€‚å¯¹äºè¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°ï¼ˆä¾‹å¦‚4ï¼‰ï¼ŒHiRED-20%èƒ½å¤Ÿåœ¨ä¿æŒååé‡å’Œå»¶è¿Ÿä¼˜åŠ¿çš„åŒæ—¶ï¼Œé€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨30%æ¥é˜²æ­¢å†…å­˜æº¢å‡ºé”™è¯¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è§£æåº¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶ï¼Œéœ€è¦å¤„ç†å¤§é‡è§†è§‰ä»¤ç‰Œå¸¦æ¥çš„è®¡ç®—æŒ‘æˆ˜é—®é¢˜äºŸå¾…è§£å†³ã€‚  </li>
<li>HiREDæ˜¯ä¸€ç§æ—¨åœ¨å›ºå®šä»¤ç‰Œé¢„ç®—å†…è¿è¡Œçš„ä»¤ç‰Œä¸¢å¼ƒæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚  </li>
<li>HiREDåˆ©ç”¨è§†è§‰å˜å‹å™¨ä¸­çš„CLSä»¤ç‰Œçš„æ³¨æ„åŠ›æœºåˆ¶æ¥è¯„ä¼°å›¾åƒåˆ†åŒºçš„è§†è§‰å†…å®¹ã€‚  </li>
<li>HiREDä¸ºæ¯ä¸ªå›¾åƒåˆ†åŒºåˆ†é…æœ€ä¼˜ä»¤ç‰Œé¢„ç®—å¹¶é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§‰ä»¤ç‰Œä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ã€‚  </li>
<li>HiREDç›¸æ¯”ç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚  </li>
<li>åœ¨ç‰¹å®šå®éªŒä¸­ï¼ŒHiREDæ˜¾è‘—æé«˜äº†ä»¤ç‰Œç”Ÿæˆååé‡ã€é™ä½äº†å“åº”å»¶è¿Ÿå¹¶èŠ‚çœäº†GPUå†…å­˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea7c00edf444bcd4b8100bd0cada307a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06e1c5a2a0d068472992fe96174eb011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa9195ed3823fe037c9716ed6019985c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a952846f79593958fc0b439ec7708734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59a25b2a84552a4c4387b4e0e32526c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5a7b46ceb99720167d425f2364311e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="M-Tuning-Prompt-Tuning-with-Mitigated-Label-Bias-in-Open-Set-Scenarios"><a href="#M-Tuning-Prompt-Tuning-with-Mitigated-Label-Bias-in-Open-Set-Scenarios" class="headerlink" title="M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios"></a>M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios</h2><p><strong>Authors:Ning Liao, Xiaopeng Zhang, Min Cao, Junchi Yan</strong></p>
<p>In realistic open-set scenarios where labels of a part of testing data are totally unknown, when vision-language (VL) prompt learning methods encounter inputs related to unknown classes (i.e., not seen during training), they always predict them as one of the training classes. The exhibited label bias causes difficulty in open set recognition (OSR), in which an image should be correctly predicted as one of the known classes or the unknown one. To achieve this goal, we propose a vision-language prompt tuning method with mitigated label bias (M-Tuning). It introduces open words from the WordNet to extend the range of words forming the prompt texts from only closed-set label words to more, and thus prompts are tuned in a simulated open-set scenario. Besides, inspired by the observation that classifying directly on large datasets causes a much higher false positive rate than on small datasets, we propose a Combinatorial Tuning and Testing (CTT) strategy for improving performance. CTT decomposes M-Tuning on large datasets as multiple independent group-wise tuning on fewer classes, then makes accurate and comprehensive predictions by selecting the optimal sub-prompt. Finally, given the lack of VL-based OSR baselines in the literature, especially for prompt methods, we contribute new baselines for fair comparisons. Our method achieves the best performance on datasets with various scales, and extensive ablation studies also validate its effectiveness. </p>
<blockquote>
<p>åœ¨çœŸå®çš„å¼€æ”¾é›†åœºæ™¯ä¸­ï¼Œéƒ¨åˆ†æµ‹è¯•æ•°æ®çš„æ ‡ç­¾æ˜¯æœªçŸ¥çš„ï¼Œå½“è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æç¤ºå­¦ä¹ æ–¹æ³•é‡åˆ°ä¸æœªçŸ¥ç±»åˆ«ç›¸å…³çš„è¾“å…¥æ—¶ï¼ˆå³åœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„ï¼‰ï¼Œå®ƒä»¬é€šå¸¸ä¼šé¢„æµ‹ä¸ºè®­ç»ƒç±»åˆ«ä¹‹ä¸€ã€‚è¿™ç§è¡¨ç°å‡ºæ¥çš„æ ‡ç­¾åè§ç»™å¼€æ”¾é›†è¯†åˆ«ï¼ˆOSRï¼‰å¸¦æ¥äº†å›°éš¾ï¼Œåœ¨OSRä¸­ï¼Œå›¾åƒåº”è¯¥è¢«æ­£ç¡®é¢„æµ‹ä¸ºå·²çŸ¥ç±»åˆ«ä¹‹ä¸€æˆ–æœªçŸ¥ç±»åˆ«ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰å‡è½»æ ‡ç­¾åè§çš„è§†è§‰è¯­è¨€æç¤ºè°ƒæ•´æ–¹æ³•ï¼ˆM-Tuningï¼‰ã€‚å®ƒå¼•å…¥äº†WordNetä¸­çš„å¼€æ”¾è¯æ±‡ï¼Œæ‰©å¤§äº†æ„æˆæç¤ºæ–‡æœ¬çš„è¯çš„èŒƒå›´ï¼Œä»ä»…å°é—­é›†æ ‡ç­¾è¯åˆ°æ›´å¤šï¼Œä»è€Œå¯¹æç¤ºè¿›è¡Œäº†æ¨¡æ‹Ÿçš„å¼€æ”¾é›†åœºæ™¯çš„è°ƒæ•´ã€‚æ­¤å¤–ï¼Œå—ç›´æ¥åœ¨å¤§æ•°æ®é›†ä¸Šè¿›è¡Œåˆ†ç±»æ¯”åœ¨å°æ•°æ®é›†ä¸Šå¯¼è‡´æ›´é«˜çš„è¯¯æŠ¥ç‡çš„è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»„åˆè°ƒæ•´å’Œæµ‹è¯•ï¼ˆCTTï¼‰ç­–ç•¥ä»¥æé«˜æ€§èƒ½ã€‚CTTå°†å¤§å‹æ•°æ®é›†ä¸Šçš„M-Tuningåˆ†è§£ä¸ºåœ¨è¾ƒå°‘ç±»åˆ«ä¸Šçš„å¤šä¸ªç‹¬ç«‹ç»„çº§è°ƒæ•´ï¼Œç„¶åé€šè¿‡é€‰æ‹©æœ€ä½³å­æç¤ºæ¥è¿›è¡Œå‡†ç¡®å’Œå…¨é¢çš„é¢„æµ‹ã€‚æœ€åï¼Œé‰´äºæ–‡çŒ®ä¸­ç¼ºä¹åŸºäºè§†è§‰è¯­è¨€çš„OSRåŸºå‡†çº¿ï¼Œå°¤å…¶æ˜¯æç¤ºæ–¹æ³•ï¼Œæˆ‘ä»¬ä¸ºå…¬å¹³æ¯”è¾ƒæä¾›äº†æ–°çš„åŸºå‡†çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è§„æ¨¡çš„æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¿æ³›çš„æ¶ˆèç ”ç©¶ä¹ŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.05122v3">PDF</a> Accepted by IEEE TCSVT</p>
<p><strong>Summary</strong>ï¼šåœ¨çœŸå®å¼€æ”¾åœºæ™¯ä¸­å¯¹æœªçŸ¥æ ‡ç­¾æ•°æ®çš„è¯†åˆ«å­˜åœ¨æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§å…·æœ‰å‡è½»æ ‡ç­¾åè§çš„è§†è§‰è¯­è¨€æç¤ºè°ƒæ•´æ–¹æ³•ï¼ˆM-Tuningï¼‰ã€‚é€šè¿‡å¼•å…¥WordNetä¸­çš„å¼€æ”¾è¯æ±‡æ¥æ‰©å±•æç¤ºæ–‡æœ¬çš„è¯æ±‡èŒƒå›´ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿçš„å¼€æ”¾åœºæ™¯ä¸­è°ƒæ•´æç¤ºã€‚æ­¤å¤–ï¼Œå—å°æ•°æ®é›†ä¸Šç›´æ¥åˆ†ç±»çš„å‡é˜³æ€§ç‡è¾ƒä½å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»„åˆè°ƒæ•´æµ‹è¯•ï¼ˆCTTï¼‰ç­–ç•¥æ¥æé«˜æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºå…¬å¹³æ¯”è¾ƒæä¾›äº†åŸºäºè§†è§‰è¯­è¨€ï¼ˆVLï¼‰çš„OSRåŸºå‡†çº¿ï¼Œå¹¶åœ¨ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åœ¨ç°å®å¼€æ”¾åœºæ™¯è¯†åˆ«ä¸­ï¼Œå­˜åœ¨å°†æœªçŸ¥æ ‡ç­¾æ•°æ®é¢„æµ‹ä¸ºè®­ç»ƒç±»åˆ«çš„é—®é¢˜ï¼Œè¿™å¼•å‘äº†æ ‡ç­¾åè§çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰å‡è½»æ ‡ç­¾åè§çš„è§†è§‰è¯­è¨€æç¤ºè°ƒæ•´æ–¹æ³•ï¼ˆM-Tuningï¼‰ï¼Œé€šè¿‡å¼•å…¥WordNetä¸­çš„å¼€æ”¾è¯æ±‡æ¥æ‰©å±•æç¤ºæ–‡æœ¬çš„è¯æ±‡èŒƒå›´ã€‚</li>
<li>ç»„åˆè°ƒæ•´æµ‹è¯•ï¼ˆCTTï¼‰ç­–ç•¥è¢«æå‡ºä»¥æé«˜æ€§èƒ½ï¼Œé€šè¿‡å°†å¤§å‹æ•°æ®é›†çš„åˆ†ç±»åˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„å°ç»„è°ƒæ•´ï¼Œç„¶ååœ¨æ›´å°‘ç±»åˆ«ä¸Šè¿›è¡Œå‡†ç¡®å’Œå…¨é¢çš„é¢„æµ‹ã€‚</li>
<li>ç¼ºå°‘åŸºäºè§†è§‰è¯­è¨€ï¼ˆVLï¼‰çš„OSRåŸºå‡†çº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æç¤ºæ–¹æ³•æ–¹é¢ï¼Œå› æ­¤æœ¬æ–‡æä¾›äº†å…¬å¹³æ¯”è¾ƒçš„åŸºå‡†çº¿ã€‚</li>
<li>æœ¬æ–‡çš„æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†M-Tuningå’ŒCTTç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¼€æ”¾è¯æ±‡å’Œç»„åˆè°ƒæ•´æµ‹è¯•ç­–ç•¥ï¼Œæœ¬æ–‡ä¸ºè§£å†³å¼€æ”¾é›†è¯†åˆ«é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.05122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbc2a5e27ca4aff14ced2527b20f9eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-103fed9ca41922cffd0ed6c3f1fd7c43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-627ea26d442c16b5af8d7576631cb251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b785a7968c61880723a7a920507d0aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200d32a2f8b7e49827059bf4b362926a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  SM3Det A Unified Model for Multi-Modal Remote Sensing Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05ab81e184e0eaaadea6962fe3808181.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  ReTaKe Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
