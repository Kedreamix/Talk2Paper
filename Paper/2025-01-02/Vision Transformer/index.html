<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-01-02  Multi-Modality Driven LoRA for Adverse Condition Depth Estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-02-更新"><a href="#2025-01-02-更新" class="headerlink" title="2025-01-02 更新"></a>2025-01-02 更新</h1><h2 id="Multi-Modality-Driven-LoRA-for-Adverse-Condition-Depth-Estimation"><a href="#Multi-Modality-Driven-LoRA-for-Adverse-Condition-Depth-Estimation" class="headerlink" title="Multi-Modality Driven LoRA for Adverse Condition Depth Estimation"></a>Multi-Modality Driven LoRA for Adverse Condition Depth Estimation</h2><p><strong>Authors:Guanglei Yang, Rui Tian, Yongqiang Zhang, Zhun Zhong, Yongqiang Li, Wangmeng Zuo</strong></p>
<p>The autonomous driving community is increasingly focused on addressing corner case problems, particularly those related to ensuring driving safety under adverse conditions (e.g., nighttime, fog, rain). To this end, the task of Adverse Condition Depth Estimation (ACDE) has gained significant attention. Previous approaches in ACDE have primarily relied on generative models, which necessitate additional target images to convert the sunny condition into adverse weather, or learnable parameters for feature augmentation to adapt domain gaps, resulting in increased model complexity and tuning efforts. Furthermore, unlike CLIP-based methods where textual and visual features have been pre-aligned, depth estimation models lack sufficient alignment between multimodal features, hindering coherent understanding under adverse conditions. To address these limitations, we propose Multi-Modality Driven LoRA (MMD-LoRA), which leverages low-rank adaptation matrices for efficient fine-tuning from source-domain to target-domain. It consists of two core components: Prompt Driven Domain Alignment (PDDA) and Visual-Text Consistent Contrastive Learning(VTCCL). During PDDA, the image encoder with MMD-LoRA generates target-domain visual representations, supervised by alignment loss that the source-target difference between language and image should be equal. Meanwhile, VTCCL bridges the gap between textual features from CLIP and visual features from diffusion model, pushing apart different weather representations (vision and text) and bringing together similar ones. Through extensive experiments, the proposed method achieves state-of-the-art performance on the nuScenes and Oxford RobotCar datasets, underscoring robustness and efficiency in adapting to varied adverse environments. </p>
<blockquote>
<p>自动驾驶领域越来越关注极端情况问题的解决方案，特别是与恶劣条件下的驾驶安全相关的问题（例如夜间、雾霾、雨天）。为此，恶劣条件深度估计（ACDE）任务引起了广泛关注。ACDE的先前方法主要依赖于生成模型，这些模型需要将晴朗条件下的图像转换为恶劣天气，或学习参数以增强特征以适应域差异，这增加了模型复杂性和调整工作。此外，与基于CLIP的方法不同，其中文本和视觉特征已经预先对齐，深度估计模型的多模态特征缺乏足够的对齐，阻碍了恶劣条件下的连贯理解。为了解决这些限制，我们提出了多模态驱动LoRA（MMD-LoRA），它利用低秩适应矩阵从源域到目标域进行高效的微调。它包括两个核心组件：提示驱动域对齐（PDDA）和视觉文本一致对比学习（VTCCL）。在PDDA期间，带有MMD-LoRA的图像编码器生成目标域的视觉表示，由对齐损失监督，该损失使语言和图像之间的源目标差异应相等。同时，VTCCL弥合了来自CLIP的文本特征和来自扩散模型的视觉特征之间的差距，将不同的天气表示（视觉和文本）分开并将相似的表示聚集在一起。通过广泛的实验，所提出的方法在nuScenes和Oxford RobotCar数据集上实现了卓越的性能，突显了在适应各种恶劣环境中的稳健性和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20162v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注自动驾驶领域中的极端情况问题处理，特别是在恶劣条件下的驾驶安全。为解决这一问题，研究者提出了名为Adverse Condition Depth Estimation（ACDE）的任务。之前的ACDE方法主要依赖生成模型，需要大量目标图像来转换天气条件或使用可学习参数进行特征增强来适应不同领域差异，增加了模型的复杂性和调优工作。为此，研究者提出了Multi-Modality Driven LoRA（MMD-LoRA）方法，通过低秩适应矩阵实现高效微调从源域到目标域的过程。它包括两个核心组件：Prompt Driven Domain Alignment（PDDA）和Visual-Text Consistent Contrastive Learning（VTCCL）。PDDA使图像编码器生成目标域视觉表示，通过对齐损失实现源域和目标域之间的语言与图像差异平衡。而VTCCL则缩小了CLIP文本特征和扩散模型视觉特征之间的差距，将不同的天气表示（视觉和文本）区分开并拉近相似的表示。实验证明，该方法在nuScenes和Oxford RobotCar数据集上取得了最佳性能，展现了在不同恶劣环境下的稳健性和适应性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶领域关注极端情况问题处理，尤其是恶劣条件下的驾驶安全。</li>
<li>Adverse Condition Depth Estimation (ACDE)任务旨在解决这一问题。</li>
<li>以往的ACDE方法主要依赖生成模型，存在模型复杂度高和调优工作量大的问题。</li>
<li>Multi-Modality Driven LoRA (MMD-LoRA)方法通过低秩适应矩阵实现高效微调源域到目标域的过程。</li>
<li>PDDA组件通过图像编码器生成目标域视觉表示，并通过对齐损失实现源域和目标域之间的语言与图像平衡。</li>
<li>VTCCL组件缩小了文本和视觉特征之间的差距，提高了不同天气条件下模型的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20162">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3d6ba7e737ca1fd7784d8845ded42829.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-110a999a5ed86e6cfa01e5eb58c42228.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41e9dfb01cefe108ebdd6fcd1acf3820.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78889cb0db855720c42c5d2c166f4375.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15a4f6cdc42ed92a1bd3ccf2ec1032c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP"><a href="#Toward-Modality-Gap-Vision-Prototype-Learning-for-Weakly-supervised-Semantic-Segmentation-with-CLIP" class="headerlink" title="Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP"></a>Toward Modality Gap: Vision Prototype Learning for Weakly-supervised   Semantic Segmentation with CLIP</h2><p><strong>Authors:Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge</strong></p>
<p>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets. </p>
<blockquote>
<p>在弱监督语义分割（WSSS）研究中，应用对比语言图像预训练（CLIP）具有强大的跨模态语义理解能力。现有方法试图通过微调文本提示来优化图像和文本的对齐方式，通过精细调整文本原型来促进语义匹配。然而，鉴于文本和视觉空间之间的模态差距，这些方法所采用的文本原型并未有效地与像素级视觉特征建立紧密对应关系。在我们的理论分析中，指出固有的模态差距会导致文本和区域特征的错位，并且仅仅通过减小CLIP中的对比损失不足以充分缩小这一差距。为了缓解模态差距的影响，我们提出了视觉原型学习（VPL）框架，通过引入更具代表性的视觉原型。该框架的核心是在视觉空间的帮助下，学习特定类别的视觉原型，以捕获高质量的定位图。此外，我们提出了区域语义对比模块，该模块将区域嵌入与相应的原型进行对比，从而实现更全面和稳健的特征学习。实验结果表明，我们提出的框架在两个基准数据集上达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19650v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探讨了Contrastive Language-Image Pre-training（CLIP）在Weakly Supervised Semantic Segmentation（WSSS）研究中的应用。文章指出当前方法通过优化文本提示来改进图像和文本的匹配度，但文本原型并未有效地与像素级视觉特征建立紧密对应关系。本文分析了文本和视觉空间之间的固有模态差距导致的文本和区域特征不匹配问题，并提出通过引入更具代表性的视觉原型来解决这一问题。同时，本文提出了Vision Prototype Learning（VPL）框架和区域语义对比模块，实现了高质量定位图的捕获和更全面、稳健的特征学习。实验结果表明，该框架在两个基准数据集上取得了最新性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP在WSSS研究中的应用展现了其强大的跨模态语义理解能力。</li>
<li>当前方法通过微调文本原型来优化图像和文本的匹配度，但仍存在模态差距问题。</li>
<li>模态差距导致文本和区域特征的不匹配，仅通过减小CLIP中的对比损失无法充分减少这种差距。</li>
<li>提出了Vision Prototype Learning（VPL）框架，通过引入更具代表性的视觉原型来解决模态差距问题。</li>
<li>VPL框架的核心是在视觉空间的帮助下学习特定类别的视觉原型，以捕获高质量定位图。</li>
<li>提出了区域语义对比模块，通过区域嵌入与相应原型的对比，实现更全面和稳健的特征学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-90ee0f753b63bdc6501a80c978c40689.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b4383b6e1ce4886b79ce30a149d538c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6d4d4b8aed96474ae553e45f0a7d71.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis"><a href="#VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis" class="headerlink" title="VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis"></a>VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis</h2><p><strong>Authors:Shicheng Yin, Kaixuan Yin, Weixing Chen, Enbo Huang, Yang Liu</strong></p>
<p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two dominant models for image analysis. While CNNs excel at extracting multi-scale features and ViTs effectively capture global dependencies, both suffer from high computational costs, particularly when processing high-resolution images. Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have attracted attention due to their efficiency. However, their performance in image classification tasks remains limited. To address these challenges, this paper introduces VisionGRU, a novel RNN-based architecture designed for efficient image classification. VisionGRU leverages a simplified Gated Recurrent Unit (minGRU) to process large-scale image features with linear complexity. It divides images into smaller patches and progressively reduces the sequence length while increasing the channel depth, thus facilitating multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional scanning captures both local and global contexts, improving long-range dependency modeling, particularly for tasks like semantic segmentation. Experimental results on the ImageNet and ADE20K datasets demonstrate that VisionGRU outperforms ViTs, significantly reducing memory usage and computational costs, especially for high-resolution images. These findings underscore the potential of RNN-based approaches for developing efficient and scalable computer vision solutions. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>. </p>
<blockquote>
<p>卷积神经网络（CNN）和视觉转换器（ViT）是图像分析的两种主导模型。虽然CNN擅长提取多尺度特征，而ViT能有效地捕捉全局依赖性，但两者都存在着较高的计算成本，特别是在处理高分辨率图像时。最近，由于状态空间模型（SSM）和循环神经网络（RNN）的效率，它们受到了人们的关注。然而，它们在图像分类任务中的表现仍然有限。为了解决这些挑战，本文提出了一种用于高效图像分类的新型RNN架构——VisionGRU。VisionGRU利用简化的门控循环单元（minGRU）以线性复杂度处理大规模图像特征。它将图像分成较小的斑块，逐步减少序列长度，同时增加通道深度，从而便于多尺度特征提取。具有双向扫描的分层2DGRU模块可以捕获局部和全局上下文，改进了长距离依赖建模，特别是对于语义分割等任务。在ImageNet和ADE20K数据集上的实验结果表明，VisionGRU优于ViT，大大降低了内存使用和计算成本，特别是在处理高分辨率图像时。这些发现强调了基于RNN的方法在开发高效且可扩展的计算机视觉解决方案方面的潜力。代码将在<a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18178v2">PDF</a> Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a></p>
<p><strong>Summary</strong><br>     本文提出一种基于RNN的新型图像分类架构VisionGRU，利用简化的门控循环单元（minGRU）以线性复杂度处理大规模图像特征。VisionGRU通过分割图像成小块并渐进减少序列长度同时增加通道深度，实现多尺度特征提取。其层次化的2DGRU模块结合双向扫描，捕捉局部和全局上下文，改进了长距离依赖建模，特别是在语义分割等任务上表现优秀。在ImageNet和ADE20K数据集上的实验结果表明，VisionGRU优于ViTs，大幅降低了内存使用和计算成本，尤其适用于高分辨率图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionGRU是一种新型的RNN架构，用于图像分类，结合了CNN和ViT的优点。</li>
<li>VisionGRU使用简化的门控循环单元（minGRU）处理大规模图像特征，具有线性复杂度。</li>
<li>VisionGRU通过分割图像成小块实现多尺度特征提取，渐进减少序列长度同时增加通道深度。</li>
<li>层次化的2DGRU模块结合双向扫描，能有效捕捉局部和全局上下文，改进长距离依赖建模。</li>
<li>VisionGRU在语义分割等任务上表现优秀，实验结果表明其性能优于ViTs。</li>
<li>VisionGRU降低了内存使用和计算成本，尤其适用于处理高分辨率图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a85fe118fa527a73edc6f2d8766f4732.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd437ed5c29515688f16cc8c0903e437.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73664fb72c773b1bf490d6a20e930639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fecfa3a65b425b6a4fd26d38b4a8f5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantics-Prompting-Data-Free-Quantization-for-Low-Bit-Vision-Transformers"><a href="#Semantics-Prompting-Data-Free-Quantization-for-Low-Bit-Vision-Transformers" class="headerlink" title="Semantics Prompting Data-Free Quantization for Low-Bit Vision   Transformers"></a>Semantics Prompting Data-Free Quantization for Low-Bit Vision   Transformers</h2><p><strong>Authors:Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Shen Li, Yong Li, Fei Chao, Zhanpeng Zeng, Rongrong Ji</strong></p>
<p>Data-free quantization (DFQ), which facilitates model quantization without real data to address increasing concerns about data security, has garnered significant attention within the model compression community. Recently, the unique architecture of vision transformers (ViTs) has driven the development of specialized DFQ techniques. However, we observe that the synthetic images from existing methods suffer from the deficient semantics issue compared to real images, thereby compromising performance. Motivated by this, we propose SPDFQ, a Semantics Prompting Data-Free Quantization method for ViTs. First, SPDFQ incorporates Attention Priors Alignment (APA), which uses randomly generated attention priors to enhance the semantics of synthetic images. Second, SPDFQ introduces Multi-Semantic Reinforcement (MSR), which utilizes localized patch optimization to prompt efficient parameterization and diverse semantics in synthetic images. Finally, SPDFQ employs Softlabel Learning (SL), where soft learning targets are adapted to encourage more complex semantics and accommodate images augmented by MSR. Experimental results demonstrate that SPDFQ significantly outperforms existing methods. For instance, SPDFQ achieves a 15.52% increase in top-1 accuracy on ImageNet for W4A4 ViT-B </p>
<blockquote>
<p>无数据量化（DFQ）技术因其能够在无需真实数据的情况下实现模型量化，缓解了数据安全方面的担忧，在模型压缩领域引起了广泛关注。最近，视觉变压器（ViT）的独特架构推动了专门的DFQ技术的发展。然而，我们观察到现有方法生成的合成图像与真实图像相比存在语义缺陷问题，从而影响了性能。受此启发，我们提出了针对ViT的语义提示无数据量化方法SPDFQ。首先，SPDFQ结合了注意力先验对齐（APA），使用随机生成的注意力先验来增强合成图像的语义。其次，SPDFQ引入了多语义增强（MSR），利用局部斑块优化来提示合成图像中的高效参数化和多样语义。最后，SPDFQ采用软标签学习（SL），其中软学习目标是适应和鼓励更复杂的语义，并适应由MSR增强的图像。实验结果表明，SPDFQ显著优于现有方法。例如，在ImageNet上，SPDFQ在W4A4 ViT-B的top-1准确率上提高了15.52%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16553v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>数据无关的量化方法（DFQ）在模型压缩领域受到广泛关注，解决了对数据安全性的担忧。针对现有方法生成的合成图像语义不足的问题，本文提出了一种针对视觉变压器（ViTs）的语义提示数据无关量化方法（SPDFQ）。SPDFQ包括注意力先验对齐（APA）、多语义增强（MSR）和软标签学习（SL），以提高合成图像的语义性能。实验结果表明，SPDFQ显著优于现有方法，在ImageNet上的top-1准确率提高了15.52%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据无关的量化（DFQ）方法无需真实数据即可实现模型量化，解决数据安全性问题。</li>
<li>现有DFQ方法生成的合成图像存在语义不足的问题。</li>
<li>针对视觉变压器（ViTs）的SPDFQ方法旨在解决上述问题，包括APA、MSR和SL三大组成部分。</li>
<li>APA利用随机生成的注意力先验增强合成图像的语义。</li>
<li>MSR通过局部补丁优化实现高效参数化和合成图像中丰富的语义提示。</li>
<li>SL采用软学习目标来鼓励更复杂的语义并适应由MSR增强的图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fba7b8c0df5e84af44c46d3c4f65b7c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bdcf7f10af23fb123b745fa1a1a2e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6482bf5fa3b12700286ab1d6759e61ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11803b47a55e718bd7ecfb4b194c8033.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning"><a href="#FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning" class="headerlink" title="FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning"></a>FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning</h2><p><strong>Authors:Gaojian Wang, Feng Lin, Tong Wu, Zhenguang Liu, Zhongjie Ba, Kui Ren</strong></p>
<p>This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region consistency and challenging inter-region coherency. Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global correspondence via tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision foundation model for downstream face security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods. </p>
<blockquote>
<p>本文提出了一个问题：在大量无标签的真实人脸情况下，如何学习一种稳健且可迁移的人脸表示，以提高各种人脸安全任务的泛化性能？我们首次尝试并提出一种自监督预训练框架，用于学习真实人脸图像的基本表示，FSFM，该框架利用掩膜图像建模（MIM）和实例判别（ID）之间的协同作用。我们探索了MIM的各种面部掩膜策略，并提出了一种简单而强大的CRFR-P掩膜，它明确地迫使模型捕捉区域内有意义的一致性以及区域间的连贯性。此外，我们还设计了一个与MIM自然结合的ID网络，通过定制的自蒸馏建立基本的局部到全局对应关系。这三个学习目标，即3C，使编码真实人脸的局部特征和全局语义成为可能。预训练后，一个普通的ViT作为通用视觉基础模型，用于下游人脸安全任务：跨数据集深度伪造检测、跨域面部防伪、未见扩散面部伪造检测。在10个公开数据集上的大量实验表明，我们的模型迁移效果优于监督预训练、视觉和面部自监督学习技术，甚至超越了任务专业化的最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12032v2">PDF</a> 21 pages, 11 figures, project page: <a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/">https://fsfm-3c.github.io</a></p>
<p><strong>Summary</strong><br>该工作通过自监督预训练学习真实人脸图像的基本表示，采用掩模图像建模（MIM）和实例判别（ID）的结合。提出多种面部掩模策略，其中CRFR-P掩模能促使模型捕捉区域内一致性及区域间连贯性。同时设计ID网络，与MIM结合建立局部到全局的对应关系。通过三个学习目标（3C）实现本地特征和全局语义的编码。预训练后，可作为下游人脸安全任务的通用视觉基础模型，如跨数据集深度伪造检测、跨域面部防伪和未见扩散面部伪造检测。在10个公开数据集上的实验表明，该模型的迁移效果优于监督预训练和其他面部自监督学习方法，甚至超越了任务专业化的SOTA方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该工作提出了一个自监督预训练框架来学习真实人脸图像的基本表示。</li>
<li>结合了掩模图像建模（MIM）和实例判别（ID）以增强模型的表示能力。</li>
<li>引入了一种新的面部掩模策略——CRFR-P，强调区域内和区域间的连贯性。</li>
<li>设计了与MIM结合的ID网络，建立局部到全局的对应关系。</li>
<li>通过三个学习目标实现本地特征和全局语义的编码，提高模型的通用性和迁移能力。</li>
<li>实验证明，该预训练模型在多种人脸安全任务上表现出优越性能，包括跨数据集深度伪造检测、面部防伪等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7f18d01f38cf3f280b2c52aac9f1c746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6fe8794954004c0d53cb83f71153bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed3ef88cb703ce660e6cf558c18a841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbdd69074a50d3e26d3aeab3f98d8cfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45dd4ef520c7cc87cd57e415e4c9b7f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation"><a href="#MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation"></a>MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation</h2><p><strong>Authors:Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe">https://github.com/zwyang6/MoRe</a>. </p>
<blockquote>
<p>弱监督语义分割（WSSS）通常使用图像级别的标签和类别激活图（CAM）来实现密集预测。最近，视觉转换器（ViT）提供了一种生成定位图（Localization Attention Maps，LAM）的替代方案，该方案通过类别补丁注意力生成定位图。然而，由于对这类注意力的建模约束不足，我们观察到定位注意力图（LAM）经常面临伪影问题，即语义相关性极小的补丁区域会被类别标记错误激活。在这项工作中，我们提出MoRe来解决这个问题，并进一步研究LAM的潜力。我们的研究结果表明，对类别补丁注意力施加额外的正则化是必要的。为此，我们首先将注意力视为一种新型的有向图，并提出图类别表示模块来隐含地正则化类别补丁实体之间的交互。它确保类别标记动态地凝聚相关补丁信息，并在图形级别抑制不相关的伪影。其次，受分类权重CAM能维持对象定位平滑的启发，我们设计了定位信息正则化模块来显式地正则化类别补丁注意力。它从CAM中提取标记关系，并以可学习的方式监督类别标记和补丁标记之间的一致性。在PASCAL VOC和MS COCO上进行了大量实验，验证了MoRe有效地解决了伪影问题，达到了最先进的性能水平，超越了最新的单阶段甚至多阶段方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zwyang6/MoRe上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11076v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了使用图像级标签进行弱监督语义分割（WSSS）的问题。虽然Vision Transformer（ViT）可以通过类补丁注意力生成定位注意力图（LAM），但由于缺乏对这类注意力的充分约束，LAM容易出现伪激活现象，即无关补丁区域被类标记错误激活。为解决这一问题，本文提出了MoRe方法，并探索了LAM的潜力。研究发现，对类补丁注意力施加额外的正则化是必要的。为此，本文首先关注注意力作为一种新型有向图，并提出图类别表示模块来隐含地规范类补丁实体间的交互。同时，受分类权重CAM保持对象定位平滑的启发，本文设计了定位信息正则化模块来显式地规范类补丁注意力。实验证明，MoRe有效地解决了伪激活问题，实现了PASCAL VOC和MS COCO数据集上的最新性能，超越了近期单阶段甚至多阶段方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) 可通过类补丁注意力生成定位注意力图（LAM）。</li>
<li>LAM存在伪激活问题，即无关补丁区域被类标记错误激活。</li>
<li>对类补丁注意力施加额外的正则化是解决LAM伪激活问题的关键。</li>
<li>提出了图类别表示模块，隐含地规范类补丁实体间的交互。</li>
<li>设计了定位信息正则化模块，显式地规范类补丁注意力，利用分类权重CAM的平滑定位特性。</li>
<li>MoRe方法在PASCAL VOC和MS COCO数据集上实现了最新性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11076">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-872f56d51aa5d225fb4d9dbe916addf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-558c55d8af6b022179480697698069b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3e4750dfaf5c1d3fd23143f2d50fec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699cfbb8aa515db4cbb28229318109c5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HiRED-Attention-Guided-Token-Dropping-for-Efficient-Inference-of-High-Resolution-Vision-Language-Models"><a href="#HiRED-Attention-Guided-Token-Dropping-for-Efficient-Inference-of-High-Resolution-Vision-Language-Models" class="headerlink" title="HiRED: Attention-Guided Token Dropping for Efficient Inference of   High-Resolution Vision-Language Models"></a>HiRED: Attention-Guided Token Dropping for Efficient Inference of   High-Resolution Vision-Language Models</h2><p><strong>Authors:Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, Bo Ji</strong></p>
<p>High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to the need to encode multiple partitions of a high-resolution image input. Processing such a large number of visual tokens through multiple transformer networks poses significant computational challenges, particularly for resource-constrained commodity GPUs. To address this challenge, we propose High-Resolution Early Dropping (HiRED), a plug-and-play token-dropping method designed to operate within a fixed token budget. HiRED leverages the attention of CLS token in the vision transformer (ViT) to assess the visual content of the image partitions and allocate an optimal token budget for each partition accordingly. The most informative visual tokens from each partition within the allocated budget are then selected and passed to the subsequent Large Language Model (LLM). We showed that HiRED achieves superior accuracy and performance, compared to existing token-dropping methods. Empirically, HiRED-20% (i.e., a 20% token budget) on LLaVA-Next-7B achieves a 4.7x increase in token generation throughput, reduces response latency by 78%, and saves 14% of GPU memory for single inference on an NVIDIA TESLA P40 (24 GB). For larger batch sizes (e.g., 4), HiRED-20% prevents out-of-memory errors by cutting memory usage by 30%, while preserving throughput and latency benefits.   Code - <a target="_blank" rel="noopener" href="https://github.com/hasanar1f/HiRED">https://github.com/hasanar1f/HiRED</a> </p>
<blockquote>
<p>高分辨率视觉语言模型（VLMs）在多模态任务中广泛应用，通过保留详细的图像信息来提高准确性。然而，由于需要对高分辨率图像输入的多个分区进行编码，这些模型通常会产生过多的视觉令牌。通过多个transformer网络处理如此大量的视觉令牌，带来了巨大的计算挑战，特别是对于资源受限的商品GPU。为了应对这一挑战，我们提出了高分辨率早期丢弃（HiRED）策略，这是一种即插即用的令牌丢弃方法，旨在在一个固定的令牌预算内运行。HiRED利用视觉transformer（ViT）中CLS令牌的注意力来评估图像分区的视觉内容，并相应地分配每个分区的最佳令牌预算。然后，从每个分区中选择最具信息量的视觉令牌，并将其传递给随后的大型语言模型（LLM）。我们证明，与现有的令牌丢弃方法相比，HiRED在准确性和性能上均达到了优越水平。在实践中，HiRED-20%（即20%的令牌预算）在LLaVA-Next-7B上实现了令牌生成吞吐量4.7倍的提升，响应延迟减少了78%，并在NVIDIA TESLA P40（24GB）上单次推理节省了14%的GPU内存。对于较大的批次大小（例如4），HiRED-20%通过减少内存使用30%来防止内存溢出错误，同时保持吞吐量和延迟优势。代码-<a target="_blank" rel="noopener" href="https://github.com/hasanar1f/HiRED">https://github.com/hasanar1f/HiRED</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10945v3">PDF</a> Accepted in AAAI 2025</p>
<p><strong>Summary</strong><br>     高解析度视觉语言模型（VLMs）在多模态任务中广泛应用，旨在保留图像详细信息以提高准确性。然而，由于需要编码高解析度图像的多个分区，这些模型会产生过多的视觉令牌。在资源受限的商品GPU上处理大量视觉令牌会带来重大计算挑战。为解决此问题，我们提出High-Resolution Early Dropping（HiRED）方法，这是一种旨在固定令牌预算内运行的即插即用令牌丢弃方法。HiRED利用视觉变压器（ViT）中的CLS令牌的注意力来评估图像分区的视觉内容，并为每个分区分配最佳令牌预算。随后，从每个分区中选择最具信息量的视觉令牌，并将其传递给随后的大型语言模型（LLM）。相比现有的令牌丢弃方法，HiRED在准确性和性能方面表现出卓越性能。具体而言，在LLaVA-Next-7B上，HiRED-20%（即20%令牌预算）使令牌生成吞吐量增加了4.7倍，响应延迟减少了78%，并节省了单个推理的GPU内存达14%。对于较大的批次大小（例如4），HiRED-20%能够在保持吞吐量和延迟优势的同时，通过减少内存使用30%来防止内存溢出错误。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高解析度视觉语言模型在处理多模态任务时，需要处理大量视觉令牌带来的计算挑战问题亟待解决。  </li>
<li>HiRED是一种旨在固定令牌预算内运行的令牌丢弃方法，可以有效应对这一挑战。  </li>
<li>HiRED利用视觉变压器中的CLS令牌的注意力机制来评估图像分区的视觉内容。  </li>
<li>HiRED为每个图像分区分配最优令牌预算并选择最具信息量的视觉令牌传递给大型语言模型。  </li>
<li>HiRED相比现有方法在准确性、性能和内存使用方面表现出卓越性能。  </li>
<li>在特定实验中，HiRED显著提高了令牌生成吞吐量、降低了响应延迟并节省了GPU内存使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10945">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ea7c00edf444bcd4b8100bd0cada307a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06e1c5a2a0d068472992fe96174eb011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa9195ed3823fe037c9716ed6019985c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a952846f79593958fc0b439ec7708734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59a25b2a84552a4c4387b4e0e32526c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5a7b46ceb99720167d425f2364311e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="M-Tuning-Prompt-Tuning-with-Mitigated-Label-Bias-in-Open-Set-Scenarios"><a href="#M-Tuning-Prompt-Tuning-with-Mitigated-Label-Bias-in-Open-Set-Scenarios" class="headerlink" title="M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios"></a>M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios</h2><p><strong>Authors:Ning Liao, Xiaopeng Zhang, Min Cao, Junchi Yan</strong></p>
<p>In realistic open-set scenarios where labels of a part of testing data are totally unknown, when vision-language (VL) prompt learning methods encounter inputs related to unknown classes (i.e., not seen during training), they always predict them as one of the training classes. The exhibited label bias causes difficulty in open set recognition (OSR), in which an image should be correctly predicted as one of the known classes or the unknown one. To achieve this goal, we propose a vision-language prompt tuning method with mitigated label bias (M-Tuning). It introduces open words from the WordNet to extend the range of words forming the prompt texts from only closed-set label words to more, and thus prompts are tuned in a simulated open-set scenario. Besides, inspired by the observation that classifying directly on large datasets causes a much higher false positive rate than on small datasets, we propose a Combinatorial Tuning and Testing (CTT) strategy for improving performance. CTT decomposes M-Tuning on large datasets as multiple independent group-wise tuning on fewer classes, then makes accurate and comprehensive predictions by selecting the optimal sub-prompt. Finally, given the lack of VL-based OSR baselines in the literature, especially for prompt methods, we contribute new baselines for fair comparisons. Our method achieves the best performance on datasets with various scales, and extensive ablation studies also validate its effectiveness. </p>
<blockquote>
<p>在真实的开放集场景中，部分测试数据的标签是未知的，当视觉语言（VL）提示学习方法遇到与未知类别相关的输入时（即在训练期间未见过的），它们通常会预测为训练类别之一。这种表现出来的标签偏见给开放集识别（OSR）带来了困难，在OSR中，图像应该被正确预测为已知类别之一或未知类别。为了实现这一目标，我们提出了一种带有减轻标签偏见的视觉语言提示调整方法（M-Tuning）。它引入了WordNet中的开放词汇，扩大了构成提示文本的词的范围，从仅封闭集标签词到更多，从而对提示进行了模拟的开放集场景的调整。此外，受直接在大数据集上进行分类比在小数据集上导致更高的误报率的观察启发，我们提出了一种组合调整和测试（CTT）策略以提高性能。CTT将大型数据集上的M-Tuning分解为在较少类别上的多个独立组级调整，然后通过选择最佳子提示来进行准确和全面的预测。最后，鉴于文献中缺乏基于视觉语言的OSR基准线，尤其是提示方法，我们为公平比较提供了新的基准线。我们的方法在各种规模的数据集上取得了最佳性能，广泛的消融研究也验证了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.05122v3">PDF</a> Accepted by IEEE TCSVT</p>
<p><strong>Summary</strong>：在真实开放场景中对未知标签数据的识别存在挑战，本文提出一种具有减轻标签偏见的视觉语言提示调整方法（M-Tuning）。通过引入WordNet中的开放词汇来扩展提示文本的词汇范围，并在模拟的开放场景中调整提示。此外，受小数据集上直接分类的假阳性率较低启发，我们提出了一种组合调整测试（CTT）策略来提高性能。最后，我们为公平比较提供了基于视觉语言（VL）的OSR基准线，并在不同规模的数据集上实现最佳性能。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>在现实开放场景识别中，存在将未知标签数据预测为训练类别的问题，这引发了标签偏见的问题。</li>
<li>本文提出了一种具有减轻标签偏见的视觉语言提示调整方法（M-Tuning），通过引入WordNet中的开放词汇来扩展提示文本的词汇范围。</li>
<li>组合调整测试（CTT）策略被提出以提高性能，通过将大型数据集的分类分解为多个独立的小组调整，然后在更少类别上进行准确和全面的预测。</li>
<li>缺少基于视觉语言（VL）的OSR基准线，特别是在提示方法方面，因此本文提供了公平比较的基准线。</li>
<li>本文的方法在不同规模的数据集上都取得了最佳性能。</li>
<li>广泛的消融研究验证了M-Tuning和CTT策略的有效性。</li>
<li>通过引入开放词汇和组合调整测试策略，本文为解决开放集识别问题提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.05122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dbc2a5e27ca4aff14ced2527b20f9eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-103fed9ca41922cffd0ed6c3f1fd7c43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-627ea26d442c16b5af8d7576631cb251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b785a7968c61880723a7a920507d0aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200d32a2f8b7e49827059bf4b362926a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d388042ffc8a5b39a08044cfba01e86c.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-01-02  SM3Det A Unified Model for Multi-Modal Remote Sensing Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05ab81e184e0eaaadea6962fe3808181.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-01-02  ReTaKe Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
