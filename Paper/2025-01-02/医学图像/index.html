<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Unified dimensionality reduction techniques in chronic liver disease   detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e9fc1ffc5a418cb6c4022963806495ef.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-02-æ›´æ–°"><a href="#2025-01-02-æ›´æ–°" class="headerlink" title="2025-01-02 æ›´æ–°"></a>2025-01-02 æ›´æ–°</h1><h2 id="Unified-dimensionality-reduction-techniques-in-chronic-liver-disease-detection"><a href="#Unified-dimensionality-reduction-techniques-in-chronic-liver-disease-detection" class="headerlink" title="Unified dimensionality reduction techniques in chronic liver disease   detection"></a>Unified dimensionality reduction techniques in chronic liver disease   detection</h2><p><strong>Authors:Anand Karna, Naina Khan, Rahul Rauniyar, Prashant Giridhar Shambharkar</strong></p>
<p>Globally, chronic liver disease continues to be a major health concern that requires precise predictive models for prompt detection and treatment. Using the Indian Liver Patient Dataset (ILPD) from the University of California at Irvineâ€™s UCI Machine Learning Repository, a number of machine learning algorithms are investigated in this study. The main focus of our research is this dataset, which includes the medical records of 583 patients, 416 of whom have been diagnosed with liver disease and 167 of whom have not. There are several aspects to this work, including feature extraction and dimensionality reduction methods like Linear Discriminant Analysis (LDA), Factor Analysis (FA), t-distributed Stochastic Neighbour Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP). The purpose of the study is to investigate how well these approaches work for converting high-dimensional datasets and improving prediction accuracy. To assess the prediction ability of the improved models, a number of classification methods were used, such as Multi-layer Perceptron, Random Forest, K-nearest neighbours, and Logistic Regression. Remarkably, the improved models performed admirably, with Random Forest having the highest accuracy of 98.31% in 10-fold cross-validation and 95.79% in train-test split evaluation. Findings offer important new perspectives on the choice and use of customized feature extraction and dimensionality reduction methods, which improve predictive models for patients with chronic liver disease. </p>
<blockquote>
<p>åœ¨å…¨çƒèŒƒå›´ï¼Œæ…¢æ€§è‚ç—…ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„å¥åº·é—®é¢˜ï¼Œéœ€è¦ç²¾ç¡®çš„é¢„æµ‹æ¨¡å‹æ¥è¿›è¡ŒåŠæ—¶çš„æ£€æµ‹å’ŒåŒ»æ²»ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ¥è‡ªåŠ å·å¤§å­¦æ¬§æ–‡åˆ†æ ¡UCIæœºå™¨å­¦ä¹ å­˜å‚¨åº“çš„å°åº¦è‚ç—…æ•°æ®é›†ï¼ˆILPDï¼‰ï¼Œå¹¶æ¢ç´¢äº†å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šï¼Œå…¶ä¸­åŒ…æ‹¬583åæ‚£è€…çš„åŒ»ç–—è®°å½•ï¼Œå…¶ä¸­416äººè¢«è¯Šæ–­ä¸ºè‚ç—…ï¼Œè€Œ167äººæœªè¢«è¯Šæ–­å‡ºè‚ç—…ã€‚è¿™é¡¹å·¥ä½œåŒ…æ‹¬ç‰¹å¾æå–å’Œé™ç»´æ–¹æ³•ï¼Œå¦‚çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ã€å› å­åˆ†æï¼ˆFAï¼‰ã€tåˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ï¼ˆt-SNEï¼‰å’Œç»Ÿä¸€æµå½¢é€¼è¿‘ä¸æŠ•å½±ï¼ˆUMAPï¼‰ã€‚ç ”ç©¶çš„ç›®çš„æ˜¯è°ƒæŸ¥è¿™äº›æ–¹æ³•åœ¨é«˜ç»´æ•°æ®é›†è½¬æ¢å’Œé¢„æµ‹ç²¾åº¦æ”¹è¿›æ–¹é¢çš„è¡¨ç°å¦‚ä½•ã€‚ä¸ºäº†è¯„ä¼°æ”¹è¿›æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œé‡‡ç”¨äº†å¤šå±‚æ„ŸçŸ¥å™¨ã€éšæœºæ£®æ—ã€Kè¿‘é‚»å’Œé€»è¾‘å›å½’ç­‰åˆ†ç±»æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ”¹è¿›åçš„æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå…¶ä¸­éšæœºæ£®æ—åœ¨10å€äº¤å‰éªŒè¯ä¸­çš„å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°98.31%ï¼Œåœ¨è®­ç»ƒæµ‹è¯•åˆ†å‰²è¯„ä¼°ä¸­çš„å‡†ç¡®ç‡ä¸º95.79%ã€‚ç ”ç©¶ç»“æœæä¾›äº†å…³äºé€‰æ‹©å’Œä½¿ç”¨é’ˆå¯¹æ…¢æ€§è‚ç—…æ‚£è€…å®šåˆ¶çš„ç‰¹å¾æå–å’Œé™ç»´æ–¹æ³•çš„é‡è¦æ–°è§†è§’ï¼Œæœ‰åŠ©äºæ”¹è¿›é¢„æµ‹æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21156v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨å°åº¦è‚è„ç—…äººæ•°æ®é›†ï¼ˆILPDï¼‰ï¼Œè°ƒæŸ¥äº†å¤šç§æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ…¢æ€§è‚ç—…é¢„æµ‹æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶åŒ…æ‹¬ç‰¹å¾æå–å’Œé™ç»´æ–¹æ³•ï¼Œä»¥åŠåˆ†ç±»æ–¹æ³•ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨ã€éšæœºæ£®æ—ã€Kè¿‘é‚»å’Œé€»è¾‘å›å½’ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ”¹è¿›åçš„æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­éšæœºæ£®æ—åœ¨10å€äº¤å‰éªŒè¯å’Œè®­ç»ƒ-æµ‹è¯•é›†åˆ†å‰²è¯„ä¼°ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«é«˜è¾¾98.31%å’Œ95.79%ã€‚è¿™ä¸ºæ…¢æ€§è‚ç—…é¢„æµ‹æ¨¡å‹çš„é€‰æ‹©å’Œä½¿ç”¨æä¾›äº†é‡è¦çš„æ–°è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ…¢æ€§è‚ç—…æ˜¯å…¨çƒæ€§çš„ä¸»è¦å¥åº·é—®é¢˜ï¼Œéœ€è¦ç²¾ç¡®çš„é¢„æµ‹æ¨¡å‹è¿›è¡ŒåŠæ—¶æ£€æµ‹å’Œæ²»ç–—çš„æ”¯æŒã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†å°åº¦è‚è„ç—…äººæ•°æ®é›†ï¼ˆILPDï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†583åæ‚£è€…çš„åŒ»ç–—è®°å½•ï¼Œå…¶ä¸­416åè¢«è¯Šæ–­ä¸ºè‚ç—…ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠç‰¹å¾æå–å’Œé™ç»´æ–¹æ³•ï¼Œå¦‚çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ã€å› å­åˆ†æï¼ˆFAï¼‰ã€tåˆ†å¸ƒéšæœºé‚»åµŒå…¥ï¼ˆt-SNEï¼‰å’Œç»Ÿä¸€æµå½¢é€¼è¿‘å’ŒæŠ•å½±ï¼ˆUMAPï¼‰ã€‚</li>
<li>ä½¿ç”¨äº†å¤šç§åˆ†ç±»æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤šå±‚æ„ŸçŸ¥å™¨ã€éšæœºæ£®æ—ã€Kè¿‘é‚»å’Œé€»è¾‘å›å½’æ¥è¯„ä¼°æ”¹è¿›åçš„æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>éšæœºæ£®æ—åœ¨10å€äº¤å‰éªŒè¯å’Œè®­ç»ƒ-æµ‹è¯•é›†åˆ†å‰²è¯„ä¼°ä¸­è¡¨ç°å‡ºæœ€é«˜çš„å‡†ç¡®æ€§ï¼Œåˆ†åˆ«ä¸º98.31%å’Œ95.79%ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å…³äºç‰¹å¾æå–å’Œé™ç»´æ–¹æ³•é€‰æ‹©å’Œä½¿ç”¨çš„é‡è¦æ–°è§†è§’ï¼Œè¿™äº›æ–¹æ³•å¯æ”¹å–„æ…¢æ€§è‚ç—…çš„é¢„æµ‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3cc3623471fbd52b0f3772df46085522.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cde237fe8df6f8c4ff54ebf0fd4bb89b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b8146c1547b3a0fdb466c4c213b9a94.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-2D-and-3D-ResNet-Architectures-for-IDH-and-MGMT-Mutation-Detection-in-Glioma-Patients"><a href="#Comparative-Analysis-of-2D-and-3D-ResNet-Architectures-for-IDH-and-MGMT-Mutation-Detection-in-Glioma-Patients" class="headerlink" title="Comparative Analysis of 2D and 3D ResNet Architectures for IDH and MGMT   Mutation Detection in Glioma Patients"></a>Comparative Analysis of 2D and 3D ResNet Architectures for IDH and MGMT   Mutation Detection in Glioma Patients</h2><p><strong>Authors:Danial Elyassirad, Benyamin Gheiji, Mahsa Vatanparast, Amir Mahmoud Ahmadzadeh, Neda Kamandi, Amirmohammad Soleimanian, Sara Salehi, Shahriar Faghani</strong></p>
<p>Gliomas are the most common cause of mortality among primary brain tumors. Molecular markers, including Isocitrate Dehydrogenase (IDH) and O[6]-methylguanine-DNA methyltransferase (MGMT) influence treatment responses and prognosis. Deep learning (DL) models may provide a non-invasive method for predicting the status of these molecular markers. To achieve non-invasive determination of gene mutations in glioma patients, we compare 2D and 3D ResNet models to predict IDH and MGMT status, using T1, post-contrast T1, and FLAIR MRI sequences. USCF glioma dataset was used, which contains 495 patients with known IDH and 410 patients with known MGMT status. The dataset was divided into training (60%), tuning (20%), and test (20%) subsets at the patient level. The 2D models take axial, coronal, and sagittal tumor slices as three separate models. To ensemble the 2D predictions the three different views were combined using logistic regression. Various ResNet architectures (ResNet10, 18, 34, 50, 101, 152) were trained. For the 3D approach, we incorporated the entire brain tumor volume in the ResNet10, 18, and 34 models. After optimizing each model, the models with the lowest tuning loss were selected for further evaluation on the separate test sets. The best-performing models in IDH prediction were the 2D ResNet50, achieving a test area under the receiver operating characteristic curve (AUROC) of 0.9096, and the 3D ResNet34, which reached a test AUROC of 0.8999. For MGMT status prediction, the 2D ResNet152 achieved a test AUROC of 0.6168; however, all 3D models yielded AUROCs less than 0.5. Overall, the study indicated that both 2D and 3D models showed high predictive value for IDH prediction, with slightly better performance in 2D models. </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤æ˜¯åŸå‘æ€§è„‘è‚¿ç˜¤ä¸­å¯¼è‡´æ­»äº¡æœ€å¸¸è§çš„åŸå› ã€‚åˆ†å­æ ‡è®°ç‰©ï¼ŒåŒ…æ‹¬å¼‚æŸ æª¬é…¸è„±æ°¢é…¶ï¼ˆIDHï¼‰å’ŒO[6]-ç”²åŸºé¸Ÿå˜Œå‘¤-DNAç”²åŸºè½¬ç§»é…¶ï¼ˆMGMTï¼‰å½±å“æ²»ç–—æ•ˆæœå’Œé¢„åã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹å¯èƒ½æä¾›ä¸€ç§éä¾µå…¥æ€§æ–¹æ³•æ¥é¢„æµ‹è¿™äº›åˆ†å­æ ‡è®°ç‰©çš„çŠ¶æ€ã€‚ä¸ºäº†å®ç°èƒ¶è´¨ç˜¤æ‚£è€…åŸºå› çªå˜çš„éä¾µå…¥æ€§æµ‹å®šï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ç”¨äºé¢„æµ‹IDHå’ŒMGMTçŠ¶æ€çš„2Då’Œ3D ResNetæ¨¡å‹ï¼Œä½¿ç”¨T1ã€å¢å¼ºT1å’ŒFLAIR MRIåºåˆ—ã€‚USCFèƒ¶è´¨ç˜¤æ•°æ®é›†åŒ…å«495ä¾‹å·²çŸ¥IDHçŠ¶æ€å’Œ410ä¾‹å·²çŸ¥MGMTçŠ¶æ€çš„æ‚£è€…ã€‚æ•°æ®é›†æŒ‰æ‚£è€…æ°´å¹³åˆ†ä¸ºè®­ç»ƒï¼ˆ60%ï¼‰ã€è°ƒæ•´ï¼ˆ20%ï¼‰å’Œæµ‹è¯•ï¼ˆ20%ï¼‰å­é›†ã€‚2Dæ¨¡å‹å°†è½´å‘ã€å† çŠ¶å‘å’ŒçŸ¢çŠ¶é¢çš„è‚¿ç˜¤åˆ‡ç‰‡ä½œä¸ºä¸‰ä¸ªå•ç‹¬æ¨¡å‹ã€‚ä¸ºäº†é›†æˆ2Dé¢„æµ‹ï¼Œä½¿ç”¨é€»è¾‘å›å½’å°†ä¸‰ä¸ªä¸åŒè§†å›¾ç»“åˆèµ·æ¥ã€‚å¯¹å„ç§ResNetæ¶æ„ï¼ˆResNet10ã€18ã€34ã€50ã€101ã€152ï¼‰è¿›è¡Œäº†è®­ç»ƒã€‚å¯¹äº3Dæ–¹æ³•ï¼Œæˆ‘ä»¬å°†æ•´ä¸ªè„‘è‚¿ç˜¤ä½“ç§¯çº³å…¥ResNet10ã€18å’Œ34æ¨¡å‹ä¸­ã€‚åœ¨ä¼˜åŒ–æ¯ä¸ªæ¨¡å‹åï¼Œé€‰æ‹©è°ƒæ•´æŸå¤±æœ€ä½çš„æ¨¡å‹åœ¨å•ç‹¬çš„æµ‹è¯•é›†ä¸Šè¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°ã€‚åœ¨IDHé¢„æµ‹ä¸­è¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯2D ResNet50ï¼Œæµ‹è¯•æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.9096ï¼Œ3D ResNet34çš„æµ‹è¯•AUROCä¸º0.8999ã€‚åœ¨MGMTçŠ¶æ€é¢„æµ‹æ–¹é¢ï¼Œ2D ResNet152çš„æµ‹è¯•AUROCä¸º0.6168ï¼›ç„¶è€Œï¼Œæ‰€æœ‰3Dæ¨¡å‹çš„AUROCå‡ä½äº0.5ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œ2Då’Œ3Dæ¨¡å‹åœ¨IDHé¢„æµ‹ä¸­éƒ½è¡¨ç°å‡ºè¾ƒé«˜çš„é¢„æµ‹ä»·å€¼ï¼Œ2Dæ¨¡å‹çš„æ€§èƒ½ç•¥å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21091v1">PDF</a> 11 PAGES, 2 Figures, 3 Tables</p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹éä¾µå…¥æ€§åœ°é¢„æµ‹è„‘èƒ¶è´¨ç˜¤æ‚£è€…IDHå’ŒMGMTåˆ†å­æ ‡è®°çŠ¶æ€çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹æ¯”2Då’Œ3D ResNetæ¨¡å‹ï¼Œå‘ç°2Dæ¨¡å‹åœ¨IDHé¢„æµ‹æ–¹é¢è¡¨ç°ç¨ä¼˜ï¼Œè€Œ3Dæ¨¡å‹åœ¨MGMTé¢„æµ‹æ–¹é¢æ•ˆæœæ¬ ä½³ã€‚ç ”ç©¶ä¸ºè„‘èƒ¶è´¨ç˜¤çš„ä¸ªæ€§åŒ–æ²»ç–—æä¾›äº†æ½œåœ¨çš„éä¾µå…¥æ€§é¢„æµ‹å·¥å…·ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Gliomasæ˜¯ä¸»è¦å¯¼è‡´åŸå‘æ€§è„‘è‚¿ç˜¤æ­»äº¡çš„åŸå› ã€‚</li>
<li>åˆ†å­æ ‡è®°å¦‚IDHå’ŒMGMTå½±å“æ²»ç–—ååº”å’Œé¢„åã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†ä¸€ç§éä¾µå…¥æ€§é¢„æµ‹è¿™äº›åˆ†å­æ ‡è®°çŠ¶æ€çš„æ–¹æ³•ã€‚</li>
<li>å¯¹æ¯”äº†2Då’Œ3D ResNetæ¨¡å‹ï¼Œåœ¨é¢„æµ‹IDHçŠ¶æ€æ–¹é¢éƒ½æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>2Dæ¨¡å‹åœ¨IDHé¢„æµ‹æ–¹é¢è¡¨ç°ç¨ä¼˜äº3Dæ¨¡å‹ã€‚</li>
<li>MGMTçŠ¶æ€é¢„æµ‹æ–¹é¢ï¼Œ2Dæ¨¡å‹è¡¨ç°è¾ƒå¥½ï¼Œè€Œæ‰€æœ‰3Dæ¨¡å‹çš„é¢„æµ‹æ•ˆæœå‡ä¸ç†æƒ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b87bb2fcdc54dfef3bef3ed93d5f71ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88dbfeed98c2b783d6374a2e62824cdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6aa3a0fb6e6bcfdb814cf6042f32419.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9417c4f96916b6af8628dae7276f2a02.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Varformer-Adapting-VARâ€™s-Generative-Prior-for-Image-Restoration"><a href="#Varformer-Adapting-VARâ€™s-Generative-Prior-for-Image-Restoration" class="headerlink" title="Varformer: Adapting VARâ€™s Generative Prior for Image Restoration"></a>Varformer: Adapting VARâ€™s Generative Prior for Image Restoration</h2><p><strong>Authors:Siyang Wang, Feng Zhao</strong></p>
<p>Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VARâ€™s adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ç»è¿‡åœ¨å¤§é‡é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¸…æ´å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå°†é€€åŒ–ç‰¹å¾è½¬æ¢ä¸ºæ¸…æ´å›¾åƒçš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ï¼Œç”¨äºå›¾åƒä¿®å¤ã€‚VARä½œä¸ºä¸€ç§æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡åº”ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡è‡ªå›å½’è¿‡ç¨‹é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œè¿™ä¸ä¿®å¤ç•Œå¹¿æ³›è®¤å¯çš„å¤šå°ºåº¦ä¿®å¤åŸåˆ™ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨åˆ©ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºçš„è¿‡ç¨‹ä¸­ï¼Œå°ºåº¦é¢„æµ‹ä¼šè‡ªåŠ¨è°ƒæ•´è¾“å…¥ï¼Œä¿ƒè¿›åç»­å°ºåº¦çš„è¡¨ç¤ºä¸æ¸…æ´å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚ä¸ºäº†åˆ©ç”¨VARåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†VARå†…çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºå®šä¹‰ä¸ºä¿®å¤å…ˆéªŒï¼Œä»è€Œæ¨è¿›äº†æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„VarFormeræ¡†æ¶ã€‚è¿™äº›å…ˆéªŒçŸ¥è¯†çš„æˆ˜ç•¥åº”ç”¨ä½¿æˆ‘ä»¬çš„VarFormeråœ¨æœªè§ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„VarFormeråœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¤šä»»åŠ¡å›¾åƒä¿®å¤æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21063v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ¨¡å‹ç»å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†è®­ç»ƒåï¼Œèƒ½æœ‰æ•ˆæ•æ‰æ¸…æ™°å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œåœ¨å›¾åƒä¿®å¤ä¸­å°†é€€åŒ–ç‰¹å¾è½¬åŒ–ä¸ºæ¸…æ™°ç‰¹å¾æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„å…ˆéªŒèƒ½åŠ›ã€‚æ–°å‹å›¾åƒç”ŸæˆèŒƒå¼VARï¼Œé€šè¿‡åº”ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚VARéµå¾ªå¤šå°ºåº¦ä¿®å¤åŸåˆ™ï¼Œæ¸è¿›æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ã€‚åœ¨åˆ©ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºæ—¶ï¼Œå°ºåº¦é¢„æµ‹å¯è‡ªåŠ¨è°ƒæ•´è¾“å…¥ï¼Œæœ‰åŠ©äºåç»­å°ºåº¦è¡¨ç¤ºä¸æ¸…æ™°å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚æˆ‘ä»¬å°†VARä¸­çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒï¼Œæ„å»ºäº†ç²¾å·§çš„VarFormeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚VarFormeråœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å¤šä»»åŠ¡å›¾åƒä¿®å¤æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹ç»é«˜è´¨é‡æ•°æ®é›†è®­ç»ƒåèƒ½æœ‰æ•ˆæ•æ‰å›¾åƒç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ã€‚</li>
<li>VARé‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œç”Ÿæˆè´¨é‡è¶…è¶Šæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>VARéµå¾ªå¤šå°ºåº¦ä¿®å¤åŸåˆ™ï¼Œæ¸è¿›æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>VARåœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´å°ºåº¦é¢„æµ‹ã€‚</li>
<li>VarFormeråˆ©ç”¨VARä¸­çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒã€‚</li>
<li>VarFormerå…·æœ‰è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>VarFormeråœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å¤šä»»åŠ¡å›¾åƒä¿®å¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-047e0b1ea1ffd6f557c3611c4cf56c0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5f0425205ce9beaa2989c326e55abfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3511e2679316454b668466915f771e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-379997d28d2bc021f95d178576011611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d4c3d485f3d32ba6a149661c2425bba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5372e9d2fb99038ba2b27a4174366b6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HisynSeg-Weakly-Supervised-Histopathological-Image-Segmentation-via-Image-Mixing-Synthesis-and-Consistency-Regularization"><a href="#HisynSeg-Weakly-Supervised-Histopathological-Image-Segmentation-via-Image-Mixing-Synthesis-and-Consistency-Regularization" class="headerlink" title="HisynSeg: Weakly-Supervised Histopathological Image Segmentation via   Image-Mixing Synthesis and Consistency Regularization"></a>HisynSeg: Weakly-Supervised Histopathological Image Segmentation via   Image-Mixing Synthesis and Consistency Regularization</h2><p><strong>Authors:Zijie Fang, Yifeng Wang, Peizhang Xie, Zhi Wang, Yongbing Zhang</strong></p>
<p>Tissue semantic segmentation is one of the key tasks in computational pathology. To avoid the expensive and laborious acquisition of pixel-level annotations, a wide range of studies attempt to adopt the class activation map (CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue segmentation. However, CAM-based methods are prone to suffer from under-activation and over-activation issues, leading to poor segmentation performance. To address this problem, we propose a novel weakly-supervised semantic segmentation framework for histopathological images based on image-mixing synthesis and consistency regularization, dubbed HisynSeg. Specifically, synthesized histopathological images with pixel-level masks are generated for fully-supervised model training, where two synthesis strategies are proposed based on Mosaic transformation and B&#39;ezier mask generation. Besides, an image filtering module is developed to guarantee the authenticity of the synthesized images. In order to further avoid the model overfitting to the occasional synthesis artifacts, we additionally propose a novel self-supervised consistency regularization, which enables the real images without segmentation masks to supervise the training of the segmentation model. By integrating the proposed techniques, the HisynSeg framework successfully transforms the weakly-supervised semantic segmentation problem into a fully-supervised one, greatly improving the segmentation accuracy. Experimental results on three datasets prove that the proposed method achieves a state-of-the-art performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Vison307/HisynSeg">https://github.com/Vison307/HisynSeg</a>. </p>
<blockquote>
<p>ç»„ç»‡è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—ç—…ç†å­¦ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ä¸ºäº†é¿å…æ˜‚è´µä¸”è´¹åŠ›çš„åƒç´ çº§æ³¨é‡Šé‡‡é›†ï¼Œå¤§é‡ç ”ç©¶å°è¯•é‡‡ç”¨ç±»æ¿€æ´»å›¾ï¼ˆCAMï¼‰è¿™ç§å¼±ç›‘ç£å­¦ä¹ æ–¹æ¡ˆï¼Œä»¥å®ç°åƒç´ çº§ç»„ç»‡åˆ†å‰²ã€‚ç„¶è€Œï¼ŒåŸºäºCAMçš„æ–¹æ³•å®¹æ˜“é­å—æ¬ æ¿€æ´»å’Œè¿‡åº¦æ¿€æ´»çš„é—®é¢˜ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒæ··åˆåˆæˆå’Œä¸€è‡´æ€§æ­£åˆ™åŒ–çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œç”¨äºç—…ç†å›¾åƒåˆ†æï¼Œç§°ä¸ºHisynSegã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ç”Ÿæˆå¸¦æœ‰åƒç´ çº§æ©è†œçš„åˆæˆç—…ç†å›¾åƒï¼Œç”¨äºå…¨ç›‘ç£æ¨¡å‹è®­ç»ƒï¼Œå…¶ä¸­æå‡ºäº†ä¸¤ç§åŸºäºé©¬èµ›å…‹å˜æ¢å’Œè´å¡å°”æ©è†œç”Ÿæˆçš„åˆæˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªå›¾åƒè¿‡æ»¤æ¨¡å—ï¼Œä»¥ä¿è¯åˆæˆå›¾åƒçš„çœŸå®æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥é¿å…æ¨¡å‹å¯¹å¶å°”çš„åˆæˆä¼ªå½±è¿‡åº¦é€‚åº”ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œè¿™ä½¿å¾—æ²¡æœ‰åˆ†å‰²æ©è†œçš„çœŸå®å›¾åƒèƒ½å¤Ÿç›‘ç£åˆ†å‰²æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡æ•´åˆæ‰€æå‡ºçš„æŠ€æœ¯ï¼ŒHisynSegæ¡†æ¶æˆåŠŸå°†å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²é—®é¢˜è½¬åŒ–ä¸ºå…¨ç›‘ç£é—®é¢˜ï¼Œå¤§å¤§æé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Vison307/HisynSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Vison307/HisynSegè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20924v1">PDF</a> Accepted by IEEE Transactions on Medical Imaging</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è®¡ç®—ç—…ç†å­¦ä¸­çš„ç»„ç»‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒæ··åˆåˆæˆå’Œä¸€è‡´æ€§æ­£åˆ™åŒ–çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ¡†æ¶HisynSegã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆåˆæˆç—…ç†å›¾åƒå’Œåƒç´ çº§æ©è†œè¿›è¡Œå…¨ç›‘ç£æ¨¡å‹è®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†ä¸¤ç§åˆæˆç­–ç•¥ï¼šåŸºäºé©¬èµ›å…‹å˜æ¢å’Œè´å¡å°”æ©è†œç”Ÿæˆã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸€ä¸ªå›¾åƒè¿‡æ»¤æ¨¡å—ä»¥ä¿è¯åˆæˆå›¾åƒçš„çœŸå®æ€§ã€‚ä¸ºé¿å…æ¨¡å‹å¯¹åˆæˆå·¥ä»¶çš„è¿‡åº¦æ‹Ÿåˆï¼Œå¼•å…¥äº†è‡ªæˆ‘ç›‘ç£çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œä½¿å¾—æ— éœ€åˆ†å‰²æ©è†œçš„çœŸå®å›¾åƒèƒ½å¤Ÿç›‘ç£åˆ†å‰²æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡æ•´åˆè¿™äº›æŠ€æœ¯ï¼ŒHisynSegæ¡†æ¶æˆåŠŸå°†å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²é—®é¢˜è½¬åŒ–ä¸ºå…¨ç›‘ç£é—®é¢˜ï¼Œå¤§å¤§æé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HisynSegæ¡†æ¶è§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­çš„ç»„ç»‡è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œé‡‡ç”¨å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨å›¾åƒæ··åˆåˆæˆè¿›è¡Œå…¨ç›‘ç£æ¨¡å‹è®­ç»ƒï¼ŒåŒ…æ‹¬é©¬èµ›å…‹å˜æ¢å’Œè´å¡å°”æ©è†œç”Ÿæˆçš„ä¸¤ç§åˆæˆç­–ç•¥ã€‚</li>
<li>å›¾åƒè¿‡æ»¤æ¨¡å—ç¡®ä¿åˆæˆå›¾åƒçš„çœŸå®æ€§ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘ç›‘ç£çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œé¿å…æ¨¡å‹å¯¹åˆæˆå·¥ä»¶çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>HisynSegæ¡†æ¶æˆåŠŸæé«˜åˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d161c4713d53464505748ef59bc033d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d6c7ada6508a4369702692a60a6a07a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef9a56e17f7159d327405f4110e1c3b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1897b3c4afc13ea7244bc92e907accfb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49a8f0af625496295864868d62b4086d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dual-Space-Augmented-Intrinsic-LoRA-for-Wind-Turbine-Segmentation"><a href="#Dual-Space-Augmented-Intrinsic-LoRA-for-Wind-Turbine-Segmentation" class="headerlink" title="Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation"></a>Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation</h2><p><strong>Authors:Shubh Singhal, RaÃ¼l PÃ©rez-Gonzalo, Andreas Espersen, Antonio Agudo</strong></p>
<p>Accurate segmentation of wind turbine blade (WTB) images is critical for effective assessments, as it directly influences the performance of automated damage detection systems. Despite advancements in large universal vision models, these models often underperform in domain-specific tasks like WTB segmentation. To address this, we extend Intrinsic LoRA for image segmentation, and propose a novel dual-space augmentation strategy that integrates both image-level and latent-space augmentations. The image-space augmentation is achieved through linear interpolation between image pairs, while the latent-space augmentation is accomplished by introducing a noise-based latent probabilistic model. Our approach significantly boosts segmentation accuracy, surpassing current state-of-the-art methods in WTB image segmentation. </p>
<blockquote>
<p>é£åŠ›å‘ç”µæœºå¶ç‰‡ï¼ˆWTBï¼‰å›¾åƒçš„ç²¾ç¡®åˆ†å‰²å¯¹äºæœ‰æ•ˆè¯„ä¼°è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒç›´æ¥å½±å“è‡ªåŠ¨åŒ–æŸä¼¤æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ã€‚å°½ç®¡é€šç”¨è§†è§‰æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼ˆå¦‚WTBåˆ†å‰²ï¼‰ä¸­å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ‰©å±•äº†ç”¨äºå›¾åƒåˆ†å‰²çš„å†…åœ¨LoRAæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åŒç©ºé—´å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†å›¾åƒçº§å’Œæ½œåœ¨ç©ºé—´çº§çš„å¢å¼ºã€‚å›¾åƒç©ºé—´çš„å¢å¼ºæ˜¯é€šè¿‡å›¾åƒå¯¹ä¹‹é—´çš„çº¿æ€§æ’å€¼æ¥å®ç°çš„ï¼Œè€Œæ½œåœ¨ç©ºé—´çš„å¢å¼ºåˆ™æ˜¯é€šè¿‡å¼•å…¥åŸºäºå™ªå£°çš„æ½œåœ¨æ¦‚ç‡æ¨¡å‹æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ï¼Œåœ¨WTBå›¾åƒåˆ†å‰²ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20838v1">PDF</a> Authors Shubh Singhal and Ra&quot;ul P&#39;erez-Gonzalo contributed equally   to this work. Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é£åŠ›å‘ç”µæœºå¶ç‰‡ï¼ˆWTBï¼‰å›¾åƒç²¾å‡†åˆ†å‰²çš„é‡è¦æ€§åŠå…¶å¯¹è‡ªåŠ¨åŒ–æŸä¼¤æ£€æµ‹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚é’ˆå¯¹é€šç”¨å¤§å‹è§†è§‰æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæœ¬æ–‡æ‰©å±•äº†Intrinsic LoRAå›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åŒç©ºé—´å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†å›¾åƒçº§å’Œæ½œåœ¨ç©ºé—´çº§çš„å¢å¼ºã€‚é€šè¿‡å›¾åƒå¯¹ä¹‹é—´çš„çº¿æ€§æ’å€¼å®ç°å›¾åƒç©ºé—´å¢å¼ºï¼Œè€Œé€šè¿‡å¼•å…¥åŸºäºå™ªå£°çš„æ½œåœ¨æ¦‚ç‡æ¨¡å‹å®ç°æ½œåœ¨ç©ºé—´å¢å¼ºã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ï¼Œè¶…è¶Šäº†å½“å‰é£åŠ›å‘ç”µæœºå¶ç‰‡å›¾åƒåˆ†å‰²é¢†åŸŸæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é£åŠ›å‘ç”µæœºå¶ç‰‡ï¼ˆWTBï¼‰å›¾åƒåˆ†å‰²å¯¹äºè¯„ä¼°è‡³å…³é‡è¦ï¼Œå½±å“è‡ªåŠ¨åŒ–æŸä¼¤æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>é€šç”¨å¤§å‹è§†è§‰æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚WTBåˆ†å‰²ï¼‰ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>æ‰©å±•äº†Intrinsic LoRAæ–¹æ³•ç”¨äºå›¾åƒåˆ†å‰²ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŒç©ºé—´å¢å¼ºç­–ç•¥ï¼Œç»“åˆå›¾åƒçº§å’Œæ½œåœ¨ç©ºé—´çº§çš„å¢å¼ºã€‚</li>
<li>å›¾åƒç©ºé—´å¢å¼ºé€šè¿‡å›¾åƒå¯¹ä¹‹é—´çš„çº¿æ€§æ’å€¼å®ç°ã€‚</li>
<li>æ½œåœ¨ç©ºé—´å¢å¼ºé€šè¿‡å¼•å…¥åŸºäºå™ªå£°çš„æ½œåœ¨æ¦‚ç‡æ¨¡å‹å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ce87df231bb7650637a80620c87fcfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ad29d68cc9ba9b6115cd891409db28e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36436d75c625365db880c83f91a67038.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cd7b78957dd048d51ef3a0140e0e93d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Solar-Filaments-Detection-using-Active-Contours-Without-Edges"><a href="#Solar-Filaments-Detection-using-Active-Contours-Without-Edges" class="headerlink" title="Solar Filaments Detection using Active Contours Without Edges"></a>Solar Filaments Detection using Active Contours Without Edges</h2><p><strong>Authors:Sanmoy Bandyopadhyay, Vaibhav Pant</strong></p>
<p>In this article, an active contours without edges (ACWE)-based algorithm has been proposed for the detection of solar filaments in H-alpha full-disk solar images. The overall algorithm consists of three main steps of image processing. These are image pre-processing, image segmentation, and image post-processing. Here in the work, contours are initialized on the solar image and allowed to deform based on the energy function. As soon as the contour reaches the boundary of the desired object, the energy function gets reduced, and the contour stops evolving. The proposed algorithm has been applied to few benchmark datasets and has been compared with the classical technique of object detection. The results analysis indicates that the proposed algorithm outperforms the results obtained using the existing classical algorithm of object detection. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ— è¾¹ç¼˜æ´»åŠ¨è½®å»“ï¼ˆACWEï¼‰çš„ç®—æ³•ï¼Œç”¨äºæ£€æµ‹H-alphaå…¨æ—¥é¢å¤ªé˜³å›¾åƒä¸­çš„å¤ªé˜³ä¸ã€‚è¯¥ç®—æ³•æ€»ä½“åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦çš„å›¾åƒå¤„ç†æ­¥éª¤ï¼šå›¾åƒé¢„å¤„ç†ã€å›¾åƒåˆ†å‰²å’Œå›¾åƒåå¤„ç†ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œè½®å»“åœ¨å¤ªé˜³å›¾åƒä¸Šè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶å…è®¸å…¶æ ¹æ®èƒ½é‡å‡½æ•°è¿›è¡Œå˜å½¢ã€‚ä¸€æ—¦è½®å»“è¾¾åˆ°ç›®æ ‡å¯¹è±¡çš„è¾¹ç•Œï¼Œèƒ½é‡å‡½æ•°å°±ä¼šå‡å°ï¼Œè½®å»“å°±ä¼šåœæ­¢æ¼”åŒ–ã€‚è¯¥ç®—æ³•å·²åº”ç”¨äºä¸€äº›åŸºå‡†æ•°æ®é›†ï¼Œå¹¶ä¸ç»å…¸çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœåˆ†æè¡¨æ˜ï¼Œè¯¥ç®—æ³•çš„æ€§èƒ½ä¼˜äºç°æœ‰ç»å…¸ç›®æ ‡æ£€æµ‹ç®—æ³•æ‰€è·å¾—çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20749v1">PDF</a> 6 pages, 2 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ— è¾¹ç¼˜æ´»åŠ¨è½®å»“ï¼ˆACWEï¼‰çš„ç®—æ³•ï¼Œç”¨äºåœ¨H-alphaå…¨æ—¥é¢å›¾åƒä¸­æ£€æµ‹å¤ªé˜³çº¤ç»´ã€‚ç®—æ³•ä¸»è¦åŒ…æ‹¬å›¾åƒé¢„å¤„ç†ã€å›¾åƒåˆ†å‰²å’Œå›¾åƒåå¤„ç†ä¸‰ä¸ªæ­¥éª¤ã€‚é€šè¿‡åˆå§‹åŒ–å¤ªé˜³å›¾åƒä¸Šçš„è½®å»“ï¼Œå¹¶å…è®¸å…¶æ ¹æ®èƒ½é‡å‡½æ•°å˜å½¢ï¼Œå½“è½®å»“è¾¾åˆ°ç›®æ ‡å¯¹è±¡è¾¹ç•Œæ—¶ï¼Œèƒ½é‡å‡½æ•°å‡å°‘ï¼Œè½®å»“åœæ­¢æ¼”åŒ–ã€‚è¯¥ç®—æ³•å·²åº”ç”¨äºå¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼Œå¹¶ä¸ç»å…¸çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯è¿›è¡Œå’Œå¯¹ï¼Œç»“æœè¡¨æ˜è¯¥ç®—æ³•ä¼˜äºç°æœ‰çš„ç»å…¸ç›®æ ‡æ£€æµ‹ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºæ— è¾¹ç¼˜æ´»åŠ¨è½®å»“ï¼ˆACWEï¼‰çš„ç®—æ³•ç”¨äºæ£€æµ‹å¤ªé˜³çº¤ç»´ã€‚</li>
<li>ç®—æ³•åŒ…å«å›¾åƒé¢„å¤„ç†ã€å›¾åƒåˆ†å‰²å’Œå›¾åƒåå¤„ç†ä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚</li>
<li>è½®å»“åœ¨å¤ªé˜³å›¾åƒä¸Šåˆå§‹åŒ–ï¼Œå¹¶åŸºäºèƒ½é‡å‡½æ•°è¿›è¡Œå˜å½¢ã€‚</li>
<li>å½“è½®å»“è¾¾åˆ°ç›®æ ‡å¯¹è±¡è¾¹ç•Œæ—¶ï¼Œèƒ½é‡å‡½æ•°å‡å°‘ï¼Œè½®å»“åœæ­¢æ¼”åŒ–ã€‚</li>
<li>è¯¥ç®—æ³•å·²åº”ç”¨äºå¤šä¸ªåŸºå‡†æ•°æ®é›†ã€‚</li>
<li>è¯¥ç®—æ³•ä¸ç»å…¸çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-557a1fd4fd524cda20be4e4197ecc52e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3af016840b81b8eba9aedfff61e5dd2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9fc1ffc5a418cb6c4022963806495ef.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Regulating-radiology-AI-medical-devices-that-evolve-in-their-lifecycle"><a href="#Regulating-radiology-AI-medical-devices-that-evolve-in-their-lifecycle" class="headerlink" title="Regulating radiology AI medical devices that evolve in their lifecycle"></a>Regulating radiology AI medical devices that evolve in their lifecycle</h2><p><strong>Authors:Camila GonzÃ¡lez, Moritz Fuchs, Daniel Pinto dos Santos, Philipp Matthies, Manuel Trenz, Maximilian GrÃ¼ning, Akshay Chaudhari, David B. Larson, Ahmed Othman, Moon Kim, Felix Nensa, Anirban Mukhopadhyay</strong></p>
<p>Over time, the distribution of medical image data drifts due to multiple factors, including shifts in patient demographics, acquisition devices, and disease manifestation. While human radiologists can extrapolate their knowledge to such changes, AI systems cannot. In fact, deep learning models are highly susceptible to even slight variations in image characteristics. Therefore, manufacturers must update their models with new data to ensure that they remain safe and effective. Until recently, conducting such model updates in the USA and European Union meant applying for re-approval. Given the time and monetary costs associated with these processes, updates were infrequent, and obsolete systems continued functioning for too long. During 2024, several developments in the regulatory frameworks of these regions have taken place that promise to streamline the process of rolling out model updates safely: The European Artificial Intelligence Act came into effect last August, and the Food and Drug Administration (FDA) released the final marketing submission recommendations for a Predetermined Change Control Plan (PCCP) in December. We give an overview of the requirements and objectives of recent regulatory efforts and summarize the building blocks needed for successfully deploying dynamic systems. At the center of these pieces of regulation - and as prerequisites for manufacturers to conduct model updates without re-approval - are the need to describe the data collection and re-training processes and to establish real-world quality monitoring mechanisms. </p>
<blockquote>
<p>éšç€æ—¶é—´çš„æ¨ç§»ï¼Œç”±äºæ‚£è€…äººå£ç»Ÿè®¡å­¦å˜åŒ–ã€é‡‡é›†è®¾å¤‡å’Œç–¾ç—…è¡¨ç°çš„å˜åŒ–ç­‰å¤šä¸ªå› ç´ ï¼ŒåŒ»å­¦å›¾åƒæ•°æ®çš„åˆ†å¸ƒä¼šå‘ç”Ÿå˜åŒ–ã€‚è™½ç„¶äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿå¯ä»¥æ¨æ–­å‡ºä»–ä»¬çš„çŸ¥è¯†èƒ½å¤Ÿåº”å¯¹è¿™äº›å˜åŒ–ï¼Œä½†äººå·¥æ™ºèƒ½ç³»ç»Ÿå´æ— æ³•åšåˆ°ã€‚äº‹å®ä¸Šï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ç”šè‡³å¯¹å›¾åƒç‰¹æ€§ä¸­çš„è½»å¾®å˜åŒ–ä¹Ÿé«˜åº¦æ•æ„Ÿã€‚å› æ­¤ï¼Œåˆ¶é€ å•†å¿…é¡»ä½¿ç”¨æ–°æ•°æ®æ›´æ–°ä»–ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿å…¶å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç›´åˆ°æœ€è¿‘ï¼Œåœ¨ç¾å›½å’Œæ¬§ç›Ÿè¿›è¡Œæ­¤ç±»æ¨¡å‹æ›´æ–°æ„å‘³ç€éœ€è¦é‡æ–°è·å¾—æ‰¹å‡†ã€‚ç”±äºè¿™äº›æµç¨‹æ¶‰åŠçš„æ—¶é—´å’Œé‡‘é’±æˆæœ¬ï¼Œæ›´æ–°å¹¶ä¸é¢‘ç¹ï¼Œä¸”è¿‡æ—¶çš„ç³»ç»Ÿç»§ç»­è¿è¡Œäº†å¤ªé•¿æ—¶é—´ã€‚åœ¨2024å¹´æœŸé—´ï¼Œè¿™äº›åœ°åŒºç›‘ç®¡æ¡†æ¶çš„è‹¥å¹²å‘å±•æœ‰æœ›ç®€åŒ–å®‰å…¨æ¨å‡ºæ¨¡å‹æ›´æ–°çš„æµç¨‹ï¼šæ¬§æ´²ã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹å·²äºå»å¹´å…«æœˆç”Ÿæ•ˆï¼Œç¾å›½é£Ÿå“å’Œè¯ç‰©ç®¡ç†å±€ï¼ˆFDAï¼‰åœ¨åäºŒæœˆå‘å¸ƒäº†é¢„å®šçš„å˜æ›´æ§åˆ¶è®¡åˆ’ï¼ˆPCCPï¼‰çš„æœ€ç»ˆå¸‚åœºè¥é”€æäº¤å»ºè®®ã€‚æˆ‘ä»¬æ¦‚è¿°äº†æœ€è¿‘çš„ç›‘ç®¡å·¥ä½œçš„è¦æ±‚å’Œç›®æ ‡ï¼Œå¹¶æ€»ç»“äº†æˆåŠŸéƒ¨ç½²åŠ¨æ€ç³»ç»Ÿæ‰€éœ€çš„æ„å»ºæ¨¡å—ã€‚è¿™äº›æ³•è§„çš„æ ¸å¿ƒâ€”â€”ä»¥åŠåˆ¶é€ å•†è¿›è¡Œæ¨¡å‹æ›´æ–°è€Œä¸éœ€é‡æ–°è·å¾—æ‰¹å‡†çš„å…ˆå†³æ¡ä»¶â€”â€”æ˜¯éœ€è¦æè¿°æ•°æ®æ”¶é›†å’Œå†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å»ºç«‹ç°å®ä¸–ç•Œçš„è´¨é‡ç›‘æ§æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20498v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å›¾åƒæ•°æ®çš„åˆ†å¸ƒå› æ‚£è€…äººå£ç»“æ„ã€é‡‡é›†è®¾å¤‡å’Œç–¾ç—…è¡¨ç°ç­‰å¤šä¸ªå› ç´ è€Œéšæ—¶é—´æ¼‚ç§»ã€‚äººå·¥æ™ºèƒ½ç³»ç»Ÿéš¾ä»¥åº”å¯¹è¿™ç§å˜åŒ–ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹å›¾åƒç‰¹æ€§ç”šè‡³ç»†å¾®å˜åŒ–éƒ½éå¸¸æ•æ„Ÿã€‚åˆ¶é€ å•†å¿…é¡»æ›´æ–°æ¨¡å‹ä»¥ç¡®ä¿å…¶å®‰å…¨å’Œæœ‰æ•ˆæ€§ã€‚è¿‘æœŸï¼Œç¾å›½å’Œæ¬§ç›Ÿçš„ç›‘ç®¡æ¡†æ¶å‘å±•ç®€åŒ–äº†æ¨¡å‹æ›´æ–°çš„æµç¨‹ï¼šæ¬§æ´²äººå·¥æ™ºèƒ½æ³•æ¡ˆç”Ÿæ•ˆï¼ŒFDAå‘å¸ƒé¢„å®šå˜æ›´æ§åˆ¶è®¡åˆ’çš„æœ€ç»ˆå¸‚åœºæäº¤å»ºè®®ã€‚æœ¬æ–‡æ¦‚è¿°äº†æœ€æ–°ç›‘ç®¡è¦æ±‚çš„ç›®æ ‡å’Œè¦ç‚¹ï¼Œå¹¶æ€»ç»“äº†æˆåŠŸéƒ¨ç½²åŠ¨æ€ç³»ç»Ÿæ‰€éœ€çš„æ„å»ºæ¨¡å—ã€‚æ ¸å¿ƒåœ¨äºæè¿°æ•°æ®æ”¶é›†å’Œå†è®­ç»ƒæµç¨‹ï¼Œå¹¶å»ºç«‹çœŸå®ä¸–ç•Œè´¨é‡ç›‘æµ‹æœºåˆ¶ï¼Œä½œä¸ºåˆ¶é€ å•†è¿›è¡Œæ— éœ€é‡æ–°å®¡æ‰¹çš„æ¨¡å‹æ›´æ–°çš„å…ˆå†³æ¡ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒæ•°æ®çš„åˆ†å¸ƒä¼šéšæ—¶é—´æ¼‚ç§»ï¼Œè¿™å¯¹AIç³»ç»Ÿæ¥è¯´æ˜¯ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹å›¾åƒç‰¹æ€§çš„å˜åŒ–éå¸¸æ•æ„Ÿã€‚</li>
<li>åˆ¶é€ å•†éœ€è¦æ›´æ–°åŒ»ç–—å›¾åƒæ•°æ®æ¨¡å‹ä»¥ç¡®ä¿å…¶å®‰å…¨å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>ç¾å›½å’Œæ¬§ç›Ÿçš„ç›‘ç®¡æ¡†æ¶å‘å±•ç®€åŒ–äº†æ¨¡å‹æ›´æ–°çš„æµç¨‹ã€‚</li>
<li>æ¬§æ´²äººå·¥æ™ºèƒ½æ³•æ¡ˆå’ŒFDAçš„é¢„å®šå˜æ›´æ§åˆ¶è®¡åˆ’å¯¹æ¨¡å‹æ›´æ–°æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚</li>
<li>æè¿°æ•°æ®æ”¶é›†å’Œå†è®­ç»ƒæµç¨‹æ˜¯æ¨¡å‹æ›´æ–°çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-49dbed3a3d853f1fadaaeaacc440bdda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09d246f8f50d36570a2ab90f7dc586f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6cc187764ce95bab5e590923ffdceeb5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diff4MMLiTS-Advanced-Multimodal-Liver-Tumor-Segmentation-via-Diffusion-Based-Image-Synthesis-and-Alignment"><a href="#Diff4MMLiTS-Advanced-Multimodal-Liver-Tumor-Segmentation-via-Diffusion-Based-Image-Synthesis-and-Alignment" class="headerlink" title="Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via   Diffusion-Based Image Synthesis and Alignment"></a>Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via   Diffusion-Based Image Synthesis and Alignment</h2><p><strong>Authors:Shiyun Chen, Li Lin, Pujin Cheng, ZhiCheng Jin, JianJian Chen, HaiDong Zhu, Kenneth K. Y. Wong, Xiaoying Tang</strong></p>
<p>Multimodal learning has been demonstrated to enhance performance across various clinical tasks, owing to the diverse perspectives offered by different modalities of data. However, existing multimodal segmentation methods rely on well-registered multimodal data, which is unrealistic for real-world clinical images, particularly for indistinct and diffuse regions such as liver tumors. In this paper, we introduce Diff4MMLiTS, a four-stage multimodal liver tumor segmentation pipeline: pre-registration of the target organs in multimodal CTs; dilation of the annotated modalityâ€™s mask and followed by its use in inpainting to obtain multimodal normal CTs without tumors; synthesis of strictly aligned multimodal CTs with tumors using the latent diffusion model based on multimodal CT features and randomly generated tumor masks; and finally, training the segmentation model, thus eliminating the need for strictly aligned multimodal data. Extensive experiments on public and internal datasets demonstrate the superiority of Diff4MMLiTS over other state-of-the-art multimodal segmentation methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€å­¦ä¹ å·²è¯æ˜å¯ä»¥é€šè¿‡ä¸åŒæ¨¡æ€æ•°æ®æä¾›çš„ä¸åŒè§†è§’æ¥æå‡å„ç§ä¸´åºŠä»»åŠ¡çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ä¾èµ–äºè‰¯å¥½é…å‡†çš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™å¯¹äºçœŸå®ä¸–ç•Œçš„ä¸´åºŠå›¾åƒæ¥è¯´å¹¶ä¸ç°å®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¨¡ç³Šå’Œæ•£æ¼«çš„åŒºåŸŸï¼ˆå¦‚è‚è„è‚¿ç˜¤ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Diff4MMLiTSï¼Œè¿™æ˜¯ä¸€ä¸ªå››é˜¶æ®µçš„å¤šæ¨¡æ€è‚è„è‚¿ç˜¤åˆ†å‰²æµç¨‹ï¼šåœ¨å¤šæ¨¡æ€CTä¸­å¯¹ç›®æ ‡å™¨å®˜è¿›è¡Œé¢„æ³¨å†Œï¼›æ‰©å¤§æ ‡æ³¨æ¨¡æ€çš„æ©è†œï¼Œç„¶åç”¨äºå›¾åƒä¿®å¤ï¼Œä»¥è·å¾—ä¸å«è‚¿ç˜¤çš„å¤šæ¨¡æ€æ­£å¸¸CTï¼›ä½¿ç”¨åŸºäºå¤šæ¨¡æ€CTç‰¹å¾å’Œéšæœºç”Ÿæˆçš„è‚¿ç˜¤æ©è†œè¿›è¡Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ä¸¥æ ¼å¯¹é½å¤šæ¨¡æ€CTè‚¿ç˜¤åˆæˆï¼›æœ€åï¼Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ã€‚åœ¨å…¬å…±å’Œå†…éƒ¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiff4MMLiTSä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diff4MMLiTSï¼Œè¿™æ˜¯ä¸€ç§å››é˜¶æ®µçš„å¤šæ¨¡æ€è‚è„è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›®æ ‡å™¨å®˜åœ¨å¤šç§CTä¸­çš„é¢„æ³¨å†Œã€å¯¹æ ‡æ³¨æ¨¡æ€çš„æ©è†œè¿›è¡Œè†¨èƒ€å¹¶ç”¨äºå¡«å……ä»¥è·å¾—ä¸å«è‚¿ç˜¤çš„å¤šç§CTã€åˆæˆä¸¥æ ¼å¯¹é½çš„å«è‚¿ç˜¤çš„å¤šæ¨¡æ€CTå›¾åƒä»¥åŠè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ã€‚å®éªŒè¯æ˜ï¼Œä¸å…¶ä»–æœ€å…ˆè¿›çš„æ¨¡æ€åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼ŒDiff4MMLiTSå…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ å¯ä»¥æé«˜åœ¨å„ç§ä¸´åºŠä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸åŒæ•°æ®æ¨¡å¼çš„å¤šæ ·åŒ–è§†è§’ã€‚</li>
<li>ç°æœ‰çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ä¾èµ–äºè‰¯å¥½æ³¨å†Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠå›¾åƒä¸­æ˜¯ä¸åˆ‡å®é™…çš„ã€‚</li>
<li>Diff4MMLiTSæ˜¯ä¸€ç§æ–°å‹çš„å››é˜¶æ®µå¤šæ¨¡æ€è‚è„è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢„æ³¨å†Œã€æ©è†œè†¨èƒ€ä¸å¡«å……ã€åˆæˆä¸¥æ ¼å¯¹é½çš„å«è‚¿ç˜¤CTå›¾åƒä»¥åŠè®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>Diff4MMLiTSé€šè¿‡æ¶ˆé™¤å¯¹ä¸¥æ ¼å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ï¼Œæ”¹è¿›äº†å¤šæ¨¡æ€åˆ†å‰²ã€‚</li>
<li>å…¬å…±å’Œå†…éƒ¨æ•°æ®é›†çš„å®éªŒè¯æ˜äº†Diff4MMLiTSç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>Diff4MMLiTSåœ¨å¤„ç†æ¨¡ç³Šå’Œæ‰©æ•£åŒºåŸŸï¼ˆå¦‚è‚è„è‚¿ç˜¤ï¼‰æ—¶å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cace16f62ce7fcb8ed89257406ea95c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87cc73f2ef25bbe1ad1e75395b73ba02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88849d3feb6fa4fbac26cb6d7f16bb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27f0d44d208b0a67c86b9c51315a3542.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbd40a0c7d26fb792740f6692b1abc2b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="On-dataset-transferability-in-medical-image-classification"><a href="#On-dataset-transferability-in-medical-image-classification" class="headerlink" title="On dataset transferability in medical image classification"></a>On dataset transferability in medical image classification</h2><p><strong>Authors:Dovile Juodelyte, Enzo Ferrante, Yucheng Lu, Prabhant Singh, Joaquin Vanschoren, Veronika Cheplygina</strong></p>
<p>Current transferability estimation methods designed for natural image datasets are often suboptimal in medical image classification. These methods primarily focus on estimating the suitability of pre-trained source model features for a target dataset, which can lead to unrealistic predictions, such as suggesting that the target dataset is the best source for itself. To address this, we propose a novel transferability metric that combines feature quality with gradients to evaluate both the suitability and adaptability of source model features for target tasks. We evaluate our approach in two new scenarios: source dataset transferability for medical image classification and cross-domain transferability. Our results show that our method outperforms existing transferability metrics in both settings. We also provide insight into the factors influencing transfer performance in medical image classification, as well as the dynamics of cross-domain transfer from natural to medical images. Additionally, we provide ground-truth transfer performance benchmarking results to encourage further research into transferability estimation for medical image classification. Our code and experiments are available at <a target="_blank" rel="noopener" href="https://github.com/DovileDo/transferability-in-medical-imaging">https://github.com/DovileDo/transferability-in-medical-imaging</a>. </p>
<blockquote>
<p>é’ˆå¯¹è‡ªç„¶å›¾åƒæ•°æ®é›†è®¾è®¡çš„ç°æœ‰å¯è¿ç§»æ€§ä¼°è®¡æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­é€šå¸¸è¡¨ç°ä¸ä½³ã€‚è¿™äº›æ–¹æ³•ä¸»è¦å…³æ³¨ä¼°è®¡é¢„è®­ç»ƒæºæ¨¡å‹ç‰¹å¾å¯¹ç›®æ ‡æ•°æ®é›†çš„é€‚ç”¨æ€§ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸ç°å®çš„é¢„æµ‹ï¼Œä¾‹å¦‚è®¤ä¸ºç›®æ ‡æ•°æ®é›†æœ¬èº«æ˜¯æœ€ä½³çš„æ¥æºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯è¿ç§»æ€§åº¦é‡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç‰¹å¾è´¨é‡å’Œæ¢¯åº¦ï¼Œä»¥è¯„ä¼°æºæ¨¡å‹ç‰¹å¾å¯¹ç›®æ ‡ä»»åŠ¡çš„é€‚ç”¨æ€§å’Œé€‚åº”æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§æ–°åœºæ™¯ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šåŒ»å­¦å›¾åƒåˆ†ç±»çš„æºæ•°æ®é›†å¯è¿ç§»æ€§å’Œè·¨åŸŸå¯è¿ç§»æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ç§è®¾ç½®ä¸­éƒ½ä¼˜äºç°æœ‰çš„å¯è¿ç§»æ€§åº¦é‡æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æ·±å…¥äº†è§£äº†å½±å“åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­è¿ç§»æ€§èƒ½çš„å› ç´ ï¼Œä»¥åŠä»è‡ªç„¶å›¾åƒåˆ°åŒ»å­¦å›¾åƒçš„è·¨åŸŸè¿ç§»çš„åŠ¨åŠ›å­¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†çœŸå®çš„è¿ç§»æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœï¼Œä»¥é¼“åŠ±å¯¹åŒ»å­¦å›¾åƒåˆ†ç±»çš„å¯è¿ç§»æ€§ä¼°è®¡è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„ä»£ç å’Œå®éªŒå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DovileDo/transferability-in-medical-imaging%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DovileDo/transferability-in-medical-imagingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20172v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ï¼Œç°æœ‰è¿ç§»èƒ½åŠ›è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é¢„è®­ç»ƒæºæ¨¡å‹ç‰¹å¾å¯¹ç›®æ ‡æ•°æ®é›†çš„é€‚ç”¨æ€§è¯„ä¼°ï¼Œå¯èƒ½å¯¼è‡´ä¸åˆç†é¢„æµ‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»“åˆç‰¹å¾è´¨é‡ä¸æ¢¯åº¦çš„æ–°è¿ç§»èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡ï¼Œè¯„ä¼°æºæ¨¡å‹ç‰¹å¾å¯¹ç›®æ ‡ä»»åŠ¡çš„é€‚åº”æ€§å’Œå¯è°ƒæ•´æ€§ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»çš„æºæ•°æ®é›†è¿ç§»æ€§å’Œè·¨åŸŸè¿ç§»æ€§ä¸¤ä¸ªåœºæ™¯ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†å½±å“åŒ»å­¦å›¾åƒåˆ†ç±»è¿ç§»æ€§èƒ½çš„å› ç´ ä»¥åŠä»è‡ªç„¶å›¾åƒåˆ°åŒ»å­¦å›¾åƒçš„è·¨åŸŸè¿ç§»åŠ¨æ€ã€‚æœ¬æ–‡æä¾›çœŸå®è¿ç§»æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœï¼Œä»¥æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†ç±»çš„è¿ç§»èƒ½åŠ›è¯„ä¼°ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ä¸ºè‡ªç„¶å›¾åƒæ•°æ®é›†è®¾è®¡çš„è¿ç§»èƒ½åŠ›è¯„ä¼°æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é¢„è®­ç»ƒæºæ¨¡å‹ç‰¹å¾å¯¹ç›®æ ‡æ•°æ®é›†çš„é€‚ç”¨æ€§ï¼Œå¯èƒ½å¯¼è‡´ä¸åˆç†é¢„æµ‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ç»“åˆç‰¹å¾è´¨é‡ä¸æ¢¯åº¦çš„è¿ç§»èƒ½åŠ›è¯„ä¼°æ–°æ–¹æ³•ï¼Œæ›´å…¨é¢åœ°è¯„ä»·æºæ¨¡å‹ç‰¹å¾çš„é€‚åº”æ€§å’Œå¯è°ƒæ•´æ€§ã€‚</li>
<li>éªŒè¯äº†è¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»çš„æºæ•°æ®é›†è¿ç§»æ€§å’Œè·¨åŸŸè¿ç§»æ€§ä¸¤ä¸ªåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¢è®¨äº†å½±å“åŒ»å­¦å›¾åƒåˆ†ç±»è¿ç§»æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>æä¾›äº†ä»è‡ªç„¶å›¾åƒåˆ°åŒ»å­¦å›¾åƒçš„è·¨åŸŸè¿ç§»åŠ¨æ€çš„åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4396ab50a6340405615d33aa36e6c6c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-565e5e31d91d7132d223d7fb36930236.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e5d59a3e7a6fc538ae1b3f1bd11689a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="On-the-Compositional-Generalization-of-Multimodal-LLMs-for-Medical-Imaging"><a href="#On-the-Compositional-Generalization-of-Multimodal-LLMs-for-Medical-Imaging" class="headerlink" title="On the Compositional Generalization of Multimodal LLMs for Medical   Imaging"></a>On the Compositional Generalization of Multimodal LLMs for Medical   Imaging</h2><p><strong>Authors:Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, Benyou Wang</strong></p>
<p>Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks, providing limited guidance on selecting datasets to enhance specific tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG)-the ability of models to understand novel combinations by recombining learned elements-as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG. Therefore, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and delivers consistent performance across different backbones, highlighting its versatility and broad applicability. Med-MAT is publicly available at <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/Med-MAT">https://github.com/FreedomIntelligence/Med-MAT</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶èƒ½åŠ›å¾€å¾€å› æŸäº›åŒ»å­¦é¢†åŸŸæ•°æ®ä¸è¶³è€Œå—åˆ°é™åˆ¶ï¼Œè¿™å¼ºè°ƒäº†å¯¹MLLMså¯ç”¨äºæ³›åŒ–çš„å›¾åƒç±»å‹ç†è§£çš„é‡è¦æ€§ã€‚å½“å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒä¼˜äºå•ä»»åŠ¡è®­ç»ƒï¼Œå› ä¸ºä¸åŒçš„ä»»åŠ¡å¯ä»¥ç›¸äº’å—ç›Šï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†è¿™äº›ä»»åŠ¡ä¹‹é—´çš„å†…éƒ¨å…³ç³»ï¼Œåœ¨å¦‚ä½•é€‰æ‹©æ•°æ®é›†ä»¥å¢å¼ºç‰¹å®šä»»åŠ¡æ–¹é¢æä¾›æœ‰é™çš„æŒ‡å¯¼ã€‚ä¸ºäº†åˆ†æè¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å°è¯•é‡‡ç”¨ç»„åˆæ³›åŒ–ï¼ˆCGï¼‰ä½œä¸ºæŒ‡å¯¼æ¡†æ¶ï¼Œå³æ¨¡å‹é€šè¿‡é‡æ–°ç»„åˆå·²å­¦ä¹ çš„å…ƒç´ æ¥ç†è§£æ–°å‹ç»„åˆçš„èƒ½åŠ›ã€‚ç”±äºåŒ»å­¦å›¾åƒå¯ä»¥é€šè¿‡æ¨¡æ€ã€è§£å‰–éƒ¨ä½å’Œä»»åŠ¡è¿›è¡Œç²¾ç¡®å®šä¹‰ï¼Œå› æ­¤è‡ªç„¶ä¸ºæ¢ç´¢CGæä¾›äº†ç¯å¢ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ”¶é›†äº†1â€”0ä¸ªåŒ»å­¦æ•°æ®é›†æ¥åˆ›å»ºMed-MATè¿›è¡Œç»¼åˆå®éªŒã€‚å®éªŒè¯å®ï¼ŒMLLMså¯ä»¥ä½¿ç”¨CGæ¥ç†è§£æœªè§è¿‡çš„åŒ»å­¦å›¾åƒï¼Œå¹¶ç¡®å®šCGæ˜¯å¤šä»»åŠ¡è®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„æ³›åŒ–çš„ä¸»è¦é©±åŠ¨åŠ›ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œè¿›ä¸€æ­¥ç ”ç©¶è¯æ˜CGå¯æœ‰æ•ˆæ”¯æŒæœ‰é™æ•°æ®é›†ï¼Œå¹¶åœ¨ä¸åŒä¸»å¹²ç½‘ç»œä¸Šå®ç°ä¸€è‡´æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶é€šç”¨æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚Med-MATå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/Med-MAT%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/FreedomIntelligence/Med-MATä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20070v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶èƒ½åŠ›å¾€å¾€å—é™äºç‰¹å®šåŒ»å­¦é¢†åŸŸçš„æ•°æ®ä¸è¶³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒä¼˜äºå•ä»»åŠ¡è®­ç»ƒï¼Œä½†å¸¸å¸¸å¿½è§†ä»»åŠ¡é—´çš„å†…åœ¨å…³ç³»ï¼Œå¯¹äºå¦‚ä½•é€‰æ‹©æ•°æ®é›†ä»¥å¢å¼ºç‰¹å®šä»»åŠ¡çš„æŒ‡å¯¼æœ‰é™ã€‚æœ¬ç ”ç©¶å°è¯•é‡‡ç”¨ç»„åˆæ³›åŒ–ï¼ˆCGï¼‰ä½œä¸ºæŒ‡å¯¼æ¡†æ¶ï¼Œåˆ†æè¿™ä¸€ç°è±¡ã€‚ç”±äºåŒ»å­¦å›¾åƒå¯é€šè¿‡æ¨¡æ€ã€è§£å‰–åŒºåŸŸå’Œä»»åŠ¡è¿›è¡Œç²¾ç¡®å®šä¹‰ï¼Œè‡ªç„¶ä¸ºæ¢ç´¢CGæä¾›äº†ç¯å¢ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Med-MATè¿›è¡Œç»¼åˆæ€§å®éªŒã€‚å®éªŒè¯å®ï¼ŒMLLMså¯åˆ©ç”¨CGç†è§£æœªè§åŒ»å­¦å›¾åƒï¼Œå¹¶ç¡®å®šä¸ºå¤šä»»åŠ¡è®­ç»ƒä¸­æ‰€è§æ³›åŒ–çš„ä¸»è¦é©±åŠ¨åŠ›ä¹‹ä¸€ã€‚CGå¯æ”¯æŒæ•°æ®é›†æœ‰é™çš„æƒ…å†µï¼Œå¹¶åœ¨ä¸åŒä¸»å¹²ç½‘ç»œä¸Šè¡¨ç°ä¸€è‡´ï¼Œå‡¸æ˜¾å…¶é€šç”¨æ€§å’Œå¹¿æ³›åº”ç”¨æ€§ã€‚Med-MATå·²åœ¨å…¬å¼€å¹³å°ä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸæœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æ•°æ®ä¸è¶³é™åˆ¶å…¶å‘å±•ã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒä¼˜äºå•ä»»åŠ¡è®­ç»ƒï¼Œä½†ç¼ºä¹å¯¹ä»»åŠ¡é—´å†…åœ¨å…³ç³»çš„å…³æ³¨ã€‚</li>
<li>ç»„åˆæ³›åŒ–ï¼ˆCGï¼‰ä½œä¸ºæŒ‡å¯¼æ¡†æ¶ï¼Œæœ‰åŠ©äºç†è§£MLLMså¦‚ä½•å¤„ç†æœªè§åŒ»å­¦å›¾åƒã€‚</li>
<li>åŒ»å­¦å›¾åƒå¯é€šè¿‡æ¨¡æ€ã€è§£å‰–åŒºåŸŸå’Œä»»åŠ¡è¿›è¡Œç²¾ç¡®å®šä¹‰ï¼Œé€‚åˆæ¢ç´¢CGã€‚</li>
<li>Med-MATå®éªŒè¯å®MLLMså¯åˆ©ç”¨CGè¿›è¡Œæ³›åŒ–ï¼Œå¹¶ç¡®å®šå…¶ä¸ºå¤šä»»åŠ¡è®­ç»ƒä¸­çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚</li>
<li>CGåœ¨æ•°æ®é›†æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæ”¯æŒèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-484e9bc99824e780b727bc32c3e5ea59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bf8b6d1b973037a4eb6f9a283999988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a598394953cec92841e39b3521b03d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c02c10cc13103b528775fac7775479cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a49c174153cc5ef0192bf0843d45012a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90efe30a1890f7dac8970eb189e48c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-292091a2a9ba42c61fcd9f12710673da.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Quantified-Deep-Learning-and-Regression-Analysis-Framework-for-Image-Segmentation-of-Skin-Cancer-Lesions"><a href="#Uncertainty-Quantified-Deep-Learning-and-Regression-Analysis-Framework-for-Image-Segmentation-of-Skin-Cancer-Lesions" class="headerlink" title="Uncertainty Quantified Deep Learning and Regression Analysis Framework   for Image Segmentation of Skin Cancer Lesions"></a>Uncertainty Quantified Deep Learning and Regression Analysis Framework   for Image Segmentation of Skin Cancer Lesions</h2><p><strong>Authors:Elhoucine Elfatimi, Pratik Shah</strong></p>
<p>Deep learning models (DLMs) frequently achieve accurate segmentation and classification of tumors from medical images. However, DLMs lacking feedback on their image segmentation mechanisms, such as Dice coefficients and confidence in their performance, face challenges when processing previously unseen images in real-world clinical settings. Uncertainty estimates to identify DLM predictions at the cellular or single-pixel level that require clinician review can enhance trust. However, their deployment requires significant computational resources. This study reports two DLMs, one trained from scratch and another based on transfer learning, with Monte Carlo dropout or Bayes-by-backprop uncertainty estimations to segment lesions from the publicly available The International Skin Imaging Collaboration-19 dermoscopy image database with cancerous lesions. A novel approach to compute pixel-by-pixel uncertainty estimations of DLM segmentation performance in multiple clinical regions from a single dermoscopy image with corresponding Dice scores is reported for the first time. Image-level uncertainty maps demonstrated correspondence between imperfect DLM segmentation and high uncertainty levels in specific skin tissue regions, with or without lesions. Four new linear regression models that can predict the Dice performance of DLM segmentation using constants and uncertainty measures, either individually or in combination from lesions, tissue structures, and non-tissue pixel regions critical for clinical diagnosis and prognostication in skin images (Spearmanâ€™s correlation, p &lt; 0.05), are reported for the first time for low-compute uncertainty estimation workflows. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆDLMsï¼‰é€šå¸¸èƒ½å‡†ç¡®åœ°ä»åŒ»å­¦å›¾åƒä¸­åˆ†å‰²å’Œè¯†åˆ«è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå¯¹äºç¼ºä¹å…³äºå›¾åƒåˆ†å‰²æœºåˆ¶åé¦ˆçš„DLMsï¼Œå¦‚åœ¨diceç³»æ•°å’Œè¡¨ç°ç½®ä¿¡åº¦ç­‰æ–¹é¢çš„åé¦ˆï¼Œåœ¨å¤„ç†ç°å®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­çš„å…ˆå‰æœªè§è¿‡çš„å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ç¡®å®šæ€§ä¼°è®¡æœ‰åŠ©äºè¯†åˆ«DLMé¢„æµ‹åœ¨ç»†èƒæˆ–å•ä¸ªåƒç´ çº§åˆ«çš„é—®é¢˜ï¼Œéœ€è¦è¿›è¡ŒåŒ»å¸ˆå®¡æ ¸ä»¥å¢å¼ºä¿¡ä»»ã€‚ç„¶è€Œï¼Œå…¶éƒ¨ç½²éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æœ¬ç ”ç©¶æŠ¥å‘Šäº†ä¸¤ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸€ä¸ªä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¦ä¸€ä¸ªåŸºäºè¿ç§»å­¦ä¹ ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›dropoutæˆ–è´å¶æ–¯åå‘ä¼ æ’­ä¸ç¡®å®šæ€§ä¼°è®¡æ¥ä»å›½é™…çš®è‚¤æˆåƒåä½œç»„ç»‡ï¼ˆInternational Skin Imaging Collaborationï¼‰å…¬å¼€çš„19ä¸ªçš®è‚¤é•œå›¾åƒæ•°æ®åº“ä¸­åˆ†å‰²ç—…å˜éƒ¨ä½ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æŠ¥é“äº†ä¸€ç§è®¡ç®—å•ä¸ªçš®è‚¤é•œå›¾åƒä¸­å¤šä¸ªä¸´åºŠåŒºåŸŸçš„åƒç´ çº§ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ–¹æ³•ï¼Œå¹¶ç»™å‡ºäº†ç›¸åº”çš„Diceè¯„åˆ†ã€‚å›¾åƒçº§åˆ«çš„ä¸ç¡®å®šæ€§åœ°å›¾æ˜¾ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†å‰²çš„ä¸å®Œç¾ä¸é«˜ä¸ç¡®å®šæ€§çš„ç‰¹å®šçš®è‚¤ç»„ç»‡åŒºåŸŸä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œæ— è®ºè¿™äº›åŒºåŸŸæ˜¯å¦æœ‰ç—…å˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æŠ¥é“äº†å››ä¸ªæ–°çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¸¸æ•°å’Œä¸ç¡®å®šæ€§åº¦é‡æ¥é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„Diceåˆ†å‰²æ€§èƒ½ï¼Œè¿™äº›åº¦é‡å¯å•ç‹¬æˆ–ç»„åˆä½¿ç”¨ï¼Œé’ˆå¯¹ç—…å˜ã€ç»„ç»‡ç»“æ„å’Œçš®è‚¤å›¾åƒä¸­ç”¨äºä¸´åºŠè¯Šæ–­å’Œé¢„åè¯„ä¼°çš„éç»„ç»‡åƒç´ åŒºåŸŸï¼ˆSpearmanç›¸å…³æ€§ï¼Œp &lt; 0.05ï¼‰ï¼Œç”¨äºä½è®¡ç®—ä¸ç¡®å®šæ€§ä¼°è®¡å·¥ä½œæµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20007v1">PDF</a> Presented at the 2024 IEEE International Conference on Machine   Learning and Applications (ICMLA), accepted for publication and in press by   IEEE</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä¸­è¡¨ç°å‡†ç¡®ã€‚ç„¶è€Œï¼Œç¼ºä¹åé¦ˆæœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†æœªè§è¿‡çš„å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ç¡®å®šæ€§ä¼°è®¡æœ‰åŠ©äºè¯†åˆ«ç»†èƒæˆ–å•åƒç´ çº§åˆ«çš„é¢„æµ‹ï¼Œä½†éœ€è¦ä¸´åºŠåŒ»ç”Ÿå®¡æ ¸ã€‚æœ¬ç ”ç©¶æŠ¥å‘Šäº†ä¸¤ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸€ä¸ªä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¦ä¸€ä¸ªåŸºäºè¿ç§»å­¦ä¹ ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›dropoutæˆ–è´å¶æ–¯åå‘ä¼ æ’­ä¸ç¡®å®šæ€§ä¼°è®¡æ¥åˆ†å‰²æ¥è‡ªå…¬å¼€å¯ç”¨å›½é™…çš®è‚¤æˆåƒåä½œç»„ç»‡çš„çš®è‚¤é•œå›¾åƒåº“ä¸­çš„ç—…å˜ã€‚æŠ¥å‘Šäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯åœ¨å•ä¸ªçš®è‚¤é•œå›¾åƒä¸­è®¡ç®—å¤šä¸ªä¸´åºŠåŒºåŸŸåƒç´ çº§çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶é¦–æ¬¡æŠ¥é“äº†ç›¸åº”çš„Diceå¾—åˆ†ã€‚å›¾åƒçº§åˆ«çš„ä¸ç¡®å®šæ€§æ˜ å°„æ˜¾ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†å‰²ä¸å®Œç¾ä¸ç‰¹å®šçš®è‚¤ç»„ç»‡åŒºåŸŸçš„é«˜ä¸ç¡®å®šæ€§æ°´å¹³ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œè¿™äº›åŒºåŸŸå¯èƒ½æœ‰ç—…å˜ä¹Ÿå¯èƒ½æ²¡æœ‰ã€‚æ­¤å¤–ï¼Œè¿˜é¦–æ¬¡æŠ¥å‘Šäº†å››ä¸ªæ–°çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ä½¿ç”¨å¸¸æ•°å’Œä¸ç¡®å®šæ€§åº¦é‡æ¥é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†å‰²çš„Diceæ€§èƒ½ï¼Œé€‚ç”¨äºç—…å˜ã€ç»„ç»‡ç»“æ„å’Œéç»„ç»‡åƒç´ åŒºåŸŸï¼Œå¯¹ä¸´åºŠè¯Šæ–­å’Œé¢„åé¢„æµ‹å…·æœ‰é‡è¦æ„ä¹‰ï¼ˆæ–¯çš®å°”æ›¼ç›¸å…³æ€§ï¼Œp &lt; 0.05ï¼‰ã€‚æŠ¥å‘Šçš„è¿™äº›æ–¹æ³•ä¸»è¦é’ˆå¯¹ä½è®¡ç®—ä¸ç¡®å®šæ€§ä¼°è®¡å·¥ä½œæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä¸­å…·æœ‰é«˜ç²¾åº¦ã€‚</li>
<li>ç¼ºä¹åé¦ˆæœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†æœªè§è¿‡çš„å›¾åƒæ—¶å¯èƒ½é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸ç¡®å®šæ€§ä¼°è®¡æœ‰åŠ©äºæé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹çš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ä½¿ç”¨è’™ç‰¹å¡æ´›dropoutæˆ–è´å¶æ–¯åå‘ä¼ æ’­æ–¹æ³•è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå®ç°äº†å¯¹çš®è‚¤ç—…å˜çš„å‡†ç¡®åˆ†å‰²ã€‚</li>
<li>æŠ¥å‘Šäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå¯ä»¥åœ¨å•ä¸ªçš®è‚¤é•œå›¾åƒä¸­è®¡ç®—å¤šä¸ªä¸´åºŠåŒºåŸŸçš„åƒç´ çº§ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>å›¾åƒçº§åˆ«çš„ä¸ç¡®å®šæ€§æ˜ å°„æ˜¾ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†å‰²ä¸ç‰¹å®šçš®è‚¤ç»„ç»‡åŒºåŸŸçš„é«˜ä¸ç¡®å®šæ€§æ°´å¹³ä¹‹é—´çš„å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e100961b7be30e095a72874ce840a7e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a5095b53ad79dd97cac5b593ffbb02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47adb2b706a6eb354fe44952ff9d75ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cb1f8dd823c8083ea501bb1ac8b4e1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13d0b05d57917a26c8494ed59ea1cceb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb5edd6db9c22bf56c105323e02bf203.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FashionFAE-Fine-grained-Attributes-Enhanced-Fashion-Vision-Language-Pre-training"><a href="#FashionFAE-Fine-grained-Attributes-Enhanced-Fashion-Vision-Language-Pre-training" class="headerlink" title="FashionFAE: Fine-grained Attributes Enhanced Fashion Vision-Language   Pre-training"></a>FashionFAE: Fine-grained Attributes Enhanced Fashion Vision-Language   Pre-training</h2><p><strong>Authors:Jiale Huang, Dehong Gao, Jinxia Zhang, Zechao Zhan, Yang Hu, Xin Wang</strong></p>
<p>Large-scale Vision-Language Pre-training (VLP) has demonstrated remarkable success in the general domain. However, in the fashion domain, items are distinguished by fine-grained attributes like texture and material, which are crucial for tasks such as retrieval. Existing models often fail to leverage these fine-grained attributes from both text and image modalities. To address the above issues, we propose a novel approach for the fashion domain, Fine-grained Attributes Enhanced VLP (FashionFAE), which focuses on the detailed characteristics of fashion data. An attribute-emphasized text prediction task is proposed to predict fine-grained attributes of the items. This forces the model to focus on the salient attributes from the text modality. Additionally, a novel attribute-promoted image reconstruction task is proposed, which further enhances the fine-grained ability of the model by leveraging the representative attributes from the image modality. Extensive experiments show that FashionFAE significantly outperforms State-Of-The-Art (SOTA) methods, achieving 2.9% and 5.2% improvements in retrieval on sub-test and full test sets, respectively, and a 1.6% average improvement in recognition tasks. </p>
<blockquote>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨é€šç”¨é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨æ—¶å°šé¢†åŸŸï¼Œç‰©å“æ˜¯é€šè¿‡è¯¸å¦‚è´¨åœ°å’Œææ–™ç­‰ç»†å¾®ç‰¹å¾æ¥åŒºåˆ†çš„ï¼Œè¿™å¯¹äºæ£€ç´¢ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€æ— æ³•ä»æ–‡æœ¬å’Œå›¾åƒæ¨¡å¼ä¸¤æ–¹é¢åˆ©ç”¨è¿™äº›ç»†å¾®ç‰¹å¾ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹æ—¶å°šé¢†åŸŸæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ç²¾ç»†ç‰¹å¾å±æ€§å¢å¼ºVLPï¼ˆFashionFAEï¼‰ï¼Œå®ƒä¸“æ³¨äºæ—¶å°šæ•°æ®çš„è¯¦ç»†ç‰¹å¾ã€‚æå‡ºäº†ä¸€ç§ä»¥å±æ€§ä¸ºé‡ç‚¹çš„æ–‡æœ¬é¢„æµ‹ä»»åŠ¡ï¼Œç”¨äºé¢„æµ‹ç‰©å“çš„ç»†å¾®å±æ€§ã€‚è¿™è¿«ä½¿æ¨¡å‹å…³æ³¨æ–‡æœ¬æ¨¡å¼çš„æ˜¾è‘—å±æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„å±æ€§ä¿ƒè¿›å›¾åƒé‡å»ºä»»åŠ¡ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒæ¨¡å¼çš„ä»£è¡¨æ€§å±æ€§ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„ç²¾ç»†ç‰¹å¾èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFashionFAEæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨å­æµ‹è¯•é›†å’Œå…¨é›†æµ‹è¯•é›†çš„æ£€ç´¢ä»»åŠ¡ä¸­åˆ†åˆ«æé«˜äº†2.9%å’Œ5.2%ï¼Œåœ¨è¯†åˆ«ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†1.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨æ—¶å°šé¢†åŸŸï¼Œç”±äºæ—¶å°šäº§å“ä¹‹é—´çš„ç»†å¾®å·®åˆ«å¦‚çº¹ç†å’Œæè´¨ç­‰å¯¹äºæ£€ç´¢ç­‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œç°æœ‰æ¨¡å‹å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨è¿™äº›ç»†å¾®ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ—¶å°šé¢†åŸŸçš„æ–°å‹æ–¹æ³•â€”â€”ç²¾ç»†å±æ€§å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆFashionFAEï¼‰ï¼Œé€šè¿‡å¼ºè°ƒå±æ€§é¢„æµ‹ä»»åŠ¡ï¼Œä½¿æ¨¡å‹å…³æ³¨æ–‡æœ¬æ¨¡æ€ä¸­çš„å…³é”®å±æ€§ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„å±æ€§ä¿ƒè¿›å›¾åƒé‡å»ºä»»åŠ¡ï¼Œåˆ©ç”¨å›¾åƒæ¨¡æ€ä¸­çš„ä»£è¡¨æ€§å±æ€§è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„ç²¾ç»†è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒFashionFAEåœ¨å­æµ‹è¯•é›†å’Œå…¨é›†æµ‹è¯•é›†ä¸Šçš„æ£€ç´¢ä»»åŠ¡åˆ†åˆ«æé«˜äº†2.9%å’Œ5.2%ï¼Œè¯†åˆ«ä»»åŠ¡å¹³å‡æé«˜äº†1.6%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨æ—¶å°šé¢†åŸŸçš„è¡¨ç°å­˜åœ¨æå‡ç©ºé—´ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ—¶å°šé¢†åŸŸæœªèƒ½å……åˆ†åˆ©ç”¨æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¸­çš„ç»†å¾®ç‰¹å¾ï¼ˆå¦‚çº¹ç†å’Œæè´¨ï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹æ—¶å°šé¢†åŸŸçš„æ–°å‹æ–¹æ³•â€”â€”FashionFAEï¼Œé€šè¿‡å¼ºè°ƒå±æ€§é¢„æµ‹ä»»åŠ¡å’Œå¼•å…¥å±æ€§ä¿ƒè¿›å›¾åƒé‡å»ºä»»åŠ¡æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>FashionFAEèƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ¨¡æ€ä¸­çš„å…³é”®å±æ€§ï¼Œå¹¶è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„ç²¾ç»†è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFashionFAEåœ¨æ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯å‰æ²¿ï¼Œåœ¨å­æµ‹è¯•é›†å’Œå…¨é›†æµ‹è¯•é›†ä¸Šçš„æ”¹è¿›åˆ†åˆ«ä¸º2.9%å’Œ5.2%ã€‚</li>
<li>åœ¨è¯†åˆ«ä»»åŠ¡ä¸Šï¼ŒFashionFAEä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¹³å‡æé«˜äº†1.6%ã€‚</li>
<li>FashionFAEçš„æ–¹æ³•ä¸ºæ—¶å°šé¢†åŸŸçš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4605786e68526e257ba9bbabf88d478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9603e429e48a5d2959e4b318a81750c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d41d1e76fa53a3b4fe2a7ee0e85eb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5593878e966e9c41f0b6d9828ff416f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70b86867b42e265ea1ebe265defe3442.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aeefbcb52e0e978fe859442b06afddd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56ac66586e68001e088f6a65f4ceeeca.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SegKAN-High-Resolution-Medical-Image-Segmentation-with-Long-Distance-Dependencies"><a href="#SegKAN-High-Resolution-Medical-Image-Segmentation-with-Long-Distance-Dependencies" class="headerlink" title="SegKAN: High-Resolution Medical Image Segmentation with Long-Distance   Dependencies"></a>SegKAN: High-Resolution Medical Image Segmentation with Long-Distance   Dependencies</h2><p><strong>Authors:Shengbo Tan, Rundong Xue, Shipeng Luo, Zeyu Zhang, Xinran Wang, Lei Zhang, Daji Ergu, Zhang Yi, Yang Zhao, Ying Cai</strong></p>
<p>Hepatic vessels in computed tomography scans often suffer from image fragmentation and noise interference, making it difficult to maintain vessel integrity and posing significant challenges for vessel segmentation. To address this issue, we propose an innovative model: SegKAN. First, we improve the conventional embedding module by adopting a novel convolutional network structure for image embedding, which smooths out image noise and prevents issues such as gradient explosion in subsequent stages. Next, we transform the spatial relationships between Patch blocks into temporal relationships to solve the problem of capturing positional relationships between Patch blocks in traditional Vision Transformer models. We conducted experiments on a Hepatic vessel dataset, and compared to the existing state-of-the-art model, the Dice score improved by 1.78%. These results demonstrate that the proposed new structure effectively enhances the segmentation performance of high-resolution extended objects. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/goblin327/SegKAN">https://github.com/goblin327/SegKAN</a> </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æä¸­ï¼Œè‚è„è¡€ç®¡ç»å¸¸å—åˆ°å›¾åƒç¢ç‰‡å’Œå™ªå£°å¹²æ‰°çš„å½±å“ï¼Œè¿™ä½¿å¾—ä¿æŒè¡€ç®¡çš„å®Œæ•´æ€§å˜å¾—å›°éš¾ï¼Œå¹¶ä¸ºè¡€ç®¡åˆ†å‰²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹ï¼šSegKANã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨æ–°å‹å·ç§¯ç½‘ç»œç»“æ„å¯¹å›¾åƒè¿›è¡ŒåµŒå…¥ï¼Œæ”¹è¿›äº†ä¼ ç»Ÿçš„åµŒå…¥æ¨¡å—ï¼Œè¯¥ç»“æ„èƒ½å¤Ÿå¹³æ»‘å›¾åƒå™ªå£°ï¼Œå¹¶åœ¨åç»­é˜¶æ®µé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ç­‰é—®é¢˜ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†Patchå—ä¹‹é—´çš„ç©ºé—´å…³ç³»è½¬æ¢ä¸ºæ—¶é—´å…³ç³»ï¼Œè§£å†³äº†ä¼ ç»Ÿè§†è§‰è½¬æ¢å™¨æ¨¡å‹ä¸­æ•è·Patchå—ä¹‹é—´ä½ç½®å…³ç³»çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨è‚è„è¡€ç®¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒDiceå¾—åˆ†æé«˜äº†1.7t%ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–°ç»“æ„æœ‰æ•ˆåœ°æé«˜äº†é«˜åˆ†è¾¨ç‡æ‰©å±•å¯¹è±¡çš„åˆ†å‰²æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/goblin327/SegKAN%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/goblin327/SegKANä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19990v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSegKANçš„æ–°æ¨¡å‹ï¼Œç”¨äºè§£å†³è‚è„è¡€ç®¡åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æä¸­çš„å›¾åƒç¢ç‰‡åŒ–ä¸å™ªå£°å¹²æ‰°é—®é¢˜ã€‚è¯¥æ¨¡å‹æ”¹è¿›äº†ä¼ ç»Ÿçš„åµŒå…¥æ¨¡å—ï¼Œé‡‡ç”¨æ–°å‹å·ç§¯ç½‘ç»œç»“æ„è¿›è¡Œå›¾åƒåµŒå…¥ï¼Œå¹³æ»‘å›¾åƒå™ªå£°ï¼Œå¹¶è§£å†³äº†åç»­é˜¶æ®µçš„æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å°†Patchå—ä¹‹é—´çš„ç©ºé—´å…³ç³»è½¬æ¢ä¸ºæ—¶é—´å…³ç³»ï¼Œè§£å†³äº†ä¼ ç»Ÿè§†è§‰è½¬æ¢å™¨æ¨¡å‹ä¸­æ•è·Patchå—ä¹‹é—´ä½ç½®å…³ç³»çš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒSegKANåœ¨è‚è„è¡€ç®¡æ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†æé«˜äº†1.78%ï¼Œè¡¨æ˜è¯¥æ–°ç»“æ„æœ‰æ•ˆæé«˜äº†é«˜åˆ†è¾¨ç‡æ‰©å±•å¯¹è±¡çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SegKANæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è‚è„è¡€ç®¡åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æä¸­çš„å›¾åƒç¢ç‰‡åŒ–åŠå™ªå£°é—®é¢˜ã€‚</li>
<li>æ”¹è¿›äº†ä¼ ç»Ÿçš„åµŒå…¥æ¨¡å—ï¼Œé‡‡ç”¨æ–°å‹å·ç§¯ç½‘ç»œç»“æ„è¿›è¡Œå›¾åƒåµŒå…¥ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå¹³æ»‘å›¾åƒå™ªå£°ï¼Œå¹¶é˜²æ­¢åç»­é˜¶æ®µçš„æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚</li>
<li>é€šè¿‡å°†Patchå—ä¹‹é—´çš„ç©ºé—´å…³ç³»è½¬æ¢ä¸ºæ—¶é—´å…³ç³»ï¼Œè§£å†³äº†ä¼ ç»Ÿè§†è§‰è½¬æ¢å™¨æ¨¡å‹çš„ç¼ºé™·ã€‚</li>
<li>å®éªŒç»“æœå¯¹æ¯”æ˜¾ç¤ºï¼ŒSegKANæ¨¡å‹åœ¨è‚è„è¡€ç®¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
<li>SegKANæ¨¡å‹çš„Diceå¾—åˆ†æé«˜äº†1.78%ï¼Œè¡¨æ˜å…¶æœ‰æ•ˆæé«˜äº†é«˜åˆ†è¾¨ç‡æ‰©å±•å¯¹è±¡çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6579867176b33b31383c561c517d8af3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59bebfc6cd0ecc3a3e51b947d04010be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a761f6d887fc783a98a5498eb1679b12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22e687a0a0b7bbbbfbd1d2585c12b1f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbbe7e42d3a1a8be20aad442c6fb679a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-756887ce034c159519603b001465073c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8253e8b9bf8a65ee53ccb4ff55df2d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9126be62539a85dc67029c28d3fb700.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Neighbor-Does-Matter-Density-Aware-Contrastive-Learning-for-Medical-Semi-supervised-Segmentation"><a href="#Neighbor-Does-Matter-Density-Aware-Contrastive-Learning-for-Medical-Semi-supervised-Segmentation" class="headerlink" title="Neighbor Does Matter: Density-Aware Contrastive Learning for Medical   Semi-supervised Segmentation"></a>Neighbor Does Matter: Density-Aware Contrastive Learning for Medical   Semi-supervised Segmentation</h2><p><strong>Authors:Feilong Tang, Zhongxing Xu, Ming Hu, Wenxue Li, Peng Xia, Yiheng Zhong, Hanjun Wu, Jionglong Su, Zongyuan Ge</strong></p>
<p>In medical image analysis, multi-organ semi-supervised segmentation faces challenges such as insufficient labels and low contrast in soft tissues. To address these issues, existing studies typically employ semi-supervised segmentation techniques using pseudo-labeling and consistency regularization. However, these methods mainly rely on individual data samples for training, ignoring the rich neighborhood information present in the feature space. In this work, we argue that supervisory information can be directly extracted from the geometry of the feature space. Inspired by the density-based clustering hypothesis, we propose using feature density to locate sparse regions within feature clusters. Our goal is to increase intra-class compactness by addressing sparsity issues. To achieve this, we propose a Density-Aware Contrastive Learning (DACL) strategy, pushing anchored features in sparse regions towards cluster centers approximated by high-density positive samples, resulting in more compact clusters. Specifically, our method constructs density-aware neighbor graphs using labeled and unlabeled data samples to estimate feature density and locate sparse regions. We also combine label-guided co-training with density-guided geometric regularization to form complementary supervision for unlabeled data. Experiments on the Multi-Organ Segmentation Challenge dataset demonstrate that our proposed method outperforms state-of-the-art methods, highlighting its efficacy in medical image segmentation tasks. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œå¤šå™¨å®˜åŠç›‘ç£åˆ†å‰²é¢ä¸´ç€æ ‡ç­¾ä¸è¶³å’Œè½¯ç»„ç»‡å¯¹æ¯”åº¦ä½ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç°æœ‰ç ”ç©¶é€šå¸¸é‡‡ç”¨åŸºäºä¼ªæ ‡ç­¾å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–çš„åŠç›‘ç£åˆ†å‰²æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸ªåˆ«æ•°æ®æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¿½ç•¥äº†ç‰¹å¾ç©ºé—´ä¸­ä¸°å¯Œçš„é‚»è¿‘ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºç›‘ç£ä¿¡æ¯å¯ä»¥ç›´æ¥ä»ç‰¹å¾ç©ºé—´çš„å‡ ä½•å½¢çŠ¶ä¸­æå–ã€‚å—åŸºäºå¯†åº¦çš„èšç±»å‡è®¾çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç‰¹å¾å¯†åº¦æ¥å®šä½ç‰¹å¾èšç±»ä¸­çš„ç¨€ç–åŒºåŸŸã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡è§£å†³ç¨€ç–æ€§é—®é¢˜æ¥å¢åŠ ç±»å†…ç´§å‡‘æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯†åº¦æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼ˆDACLï¼‰ç­–ç•¥ï¼Œå°†é”šç‚¹ç‰¹å¾æ¨å‘ç”±é«˜å¯†åº¦æ­£æ ·æœ¬è¿‘ä¼¼çš„èšç±»ä¸­å¿ƒï¼Œä»è€Œå½¢æˆæ›´ç´§å‡‘çš„èšç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„æ•°æ®æ ·æœ¬æ„å»ºå¯†åº¦æ„ŸçŸ¥é‚»è¿‘å›¾ï¼Œä»¥ä¼°è®¡ç‰¹å¾å¯†åº¦å¹¶å®šä½ç¨€ç–åŒºåŸŸã€‚æˆ‘ä»¬è¿˜ç»“åˆäº†æ ‡ç­¾å¼•å¯¼çš„ååŒè®­ç»ƒå’Œå¯†åº¦å¼•å¯¼çš„å‡ ä½•æ­£åˆ™åŒ–ï¼Œä¸ºæ— æ ‡ç­¾æ•°æ®å½¢æˆäº’è¡¥ç›‘ç£ã€‚åœ¨Multi-Organ Segmentation Challengeæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç‰¹å¾å¯†åº¦çš„å¯¹æ¯”å­¦ä¹ ï¼ˆDACLï¼‰ç­–ç•¥ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒå¤šå™¨å®˜åˆ†å‰²ä¸­çš„åŠç›‘ç£é—®é¢˜ã€‚é’ˆå¯¹ç¨€ç–åŒºåŸŸï¼Œé€šè¿‡åˆ©ç”¨ç‰¹å¾ç©ºé—´çš„å‡ ä½•ç»“æ„æå–ç›‘ç£ä¿¡æ¯ï¼Œæé«˜ç±»å†…ç´§å‡‘æ€§ã€‚åœ¨Multi-Organ Segmentation Challengeæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæœ‰æ•ˆåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå¤šå™¨å®˜åˆ†å‰²ä¸­çš„åŠç›‘ç£é—®é¢˜é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ ‡ç­¾ä¸è¶³å’Œè½¯ç»„ç»‡å¯¹æ¯”åº¦ä½ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šå¸¸ä½¿ç”¨ä¼ªæ ‡ç­¾å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–ç­‰åŠç›‘ç£åˆ†å‰²æŠ€æœ¯ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨ç‰¹å¾ç©ºé—´çš„å‡ ä½•ç»“æ„ç›´æ¥æå–ç›‘ç£ä¿¡æ¯ï¼Œè§£å†³ç¨€ç–åŒºåŸŸé—®é¢˜ï¼Œæé«˜ç±»å†…ç´§å‡‘æ€§ã€‚</li>
<li>æå‡ºå¯†åº¦æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼ˆDACLï¼‰ç­–ç•¥ï¼Œå°†é”šç‚¹ç‰¹å¾æ¨å‘ç”±é«˜å¯†åº¦æ­£æ ·æœ¬è¿‘ä¼¼è¡¨ç¤ºçš„ç°‡ä¸­å¿ƒï¼Œå½¢æˆæ›´ç´§å‡‘çš„ç°‡ã€‚</li>
<li>é€šè¿‡æ„å»ºå¯†åº¦æ„ŸçŸ¥çš„é‚»å±…å›¾æ¥ä¼°è®¡ç‰¹å¾å¯†åº¦å¹¶å®šä½ç¨€ç–åŒºåŸŸï¼Œä½¿ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®æ ·æœ¬ã€‚</li>
<li>ç»“åˆæ ‡ç­¾å¼•å¯¼çš„ååŒè®­ç»ƒå’Œå¯†åº¦å¼•å¯¼å‡ ä½•æ­£åˆ™åŒ–ï¼Œä¸ºæ— æ ‡ç­¾æ•°æ®å½¢æˆäº’è¡¥ç›‘ç£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a5df606be54a5d8b85ec816d6c1c8ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e10dfa075654ba01825d80fc077025d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c5354feca5eb2073810814b74a61c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01a3c67d9183fcf00617505c144fdd4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5bcc2aa28d571eb85726b5542f3854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3be3ff7b0469bf01630ff462bc7ecfa4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ProKAN-Progressive-Stacking-of-Kolmogorov-Arnold-Networks-for-Efficient-Liver-Segmentation"><a href="#ProKAN-Progressive-Stacking-of-Kolmogorov-Arnold-Networks-for-Efficient-Liver-Segmentation" class="headerlink" title="ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient   Liver Segmentation"></a>ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient   Liver Segmentation</h2><p><strong>Authors:Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy</strong></p>
<p>The growing need for accurate and efficient 3D identification of tumors, particularly in liver segmentation, has spurred considerable research into deep learning models. While many existing architectures offer strong performance, they often face challenges such as overfitting and excessive computational costs. An adjustable and flexible architecture that strikes a balance between time efficiency and model complexity remains an unmet requirement. In this paper, we introduce proKAN, a progressive stacking methodology for Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike traditional architectures, proKAN dynamically adjusts its complexity by progressively adding KAN blocks during training, based on overfitting behavior. This approach allows the network to stop growing when overfitting is detected, preventing unnecessary computational overhead while maintaining high accuracy. Additionally, proKAN utilizes KANâ€™s learnable activation functions modeled through B-splines, which provide enhanced flexibility in learning complex relationships in 3D medical data. Our proposed architecture achieves state-of-the-art performance in liver segmentation tasks, outperforming standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The dynamic nature of proKAN ensures efficient training times and high accuracy without the risk of overfitting. Furthermore, proKAN provides better interpretability by allowing insight into the decision-making process through its learnable coefficients. The experimental results demonstrate a significant improvement in accuracy, Dice score, and time efficiency, making proKAN a compelling solution for 3D medical image segmentation tasks. </p>
<blockquote>
<p>å¯¹äºå‡†ç¡®ä¸”é«˜æ•ˆçš„3Dè‚¿ç˜¤è¯†åˆ«ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚è„åˆ†å‰²ä¸­çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œå·²ç»æ¿€å‘äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ·±å…¥ç ”ç©¶ã€‚å°½ç®¡ç°æœ‰çš„è®¸å¤šæ¶æ„éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¸¸å¸¸é¢ä¸´è¿‡æ‹Ÿåˆå’Œè®¡ç®—æˆæœ¬è¿‡é«˜çš„æŒ‘æˆ˜ã€‚ä¸€ç§èƒ½åœ¨æ—¶é—´æ•ˆç‡å’Œæ¨¡å‹å¤æ‚åº¦ä¹‹é—´å–å¾—å¹³è¡¡çš„å¯è°ƒæ•´å’Œçµæ´»æ¶æ„ä»ç„¶æ˜¯ä¸€ä¸ªæœªæ»¡è¶³çš„éœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†proKANï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰çš„æ¸è¿›å †å æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„æ¶æ„ä¸åŒï¼ŒproKANæ ¹æ®è¿‡æ‹Ÿåˆè¡Œä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€åœ°æ·»åŠ KANå—æ¥è°ƒæ•´å…¶å¤æ‚æ€§ã€‚è¿™ç§æ–¹æ³•å…è®¸ç½‘ç»œåœ¨æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆæ—¶åœæ­¢å¢é•¿ï¼Œæ—¢é˜²æ­¢äº†ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ï¼Œåˆä¿æŒäº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒproKANåˆ©ç”¨é€šè¿‡Bæ ·æ¡å»ºæ¨¡çš„KANçš„å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œæä¾›äº†åœ¨3DåŒ»å­¦æ•°æ®ä¸­å­¦ä¹ å¤æ‚å…³ç³»çš„å¢å¼ºçµæ´»æ€§ã€‚æˆ‘ä»¬æå‡ºçš„æ¶æ„åœ¨è‚è„åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºæ ‡å‡†çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å’Œå›ºå®šçš„KANæ¶æ„ã€‚proKANçš„åŠ¨æ€ç‰¹æ€§ç¡®ä¿äº†é«˜æ•ˆçš„è®­ç»ƒæ—¶é—´å’Œé«˜å‡†ç¡®æ€§ï¼Œä¸”æ²¡æœ‰è¿‡æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼ŒproKANé€šè¿‡å…¶å¯å­¦ä¹ ç³»æ•°æä¾›äº†æ›´å¥½çš„å¯è§£é‡Šæ€§ï¼Œå¯ä»¥æ·±å…¥äº†è§£å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€Diceåˆ†æ•°å’Œæ—¶é—´æ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä½¿proKANæˆä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„ç†æƒ³è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19713v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è‚è„åˆ†å‰²ä»»åŠ¡ä¸­çš„ä¸‰ç»´è‚¿ç˜¤è¯†åˆ«éœ€æ±‚ï¼Œæå‡ºä¸€ç§åä¸ºproKANçš„æ–°å‹æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ã€‚è¯¥æ¶æ„é‡‡ç”¨æ¸è¿›å †å Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰çš„æ–¹æ³•ï¼Œå¯åŠ¨æ€è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ï¼Œé€šè¿‡æ£€æµ‹è¿‡æ‹Ÿåˆç°è±¡æ¥åœæ­¢ç½‘ç»œå¢é•¿ï¼Œå®ç°é«˜æ•ˆçš„æ—¶é—´åˆ©ç”¨å’Œé«˜ç²¾åº¦ã€‚åŒæ—¶ï¼ŒproKANåˆ©ç”¨é€šè¿‡Bæ ·æ¡æ›²çº¿å»ºæ¨¡çš„å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œå¢å¼ºäº†åœ¨ä¸‰ç»´åŒ»å­¦æ•°æ®ä¸­å­¦ä¹ å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒproKANåœ¨è‚è„åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€Diceè¯„åˆ†å’Œæ—¶é—´æ•ˆç‡ï¼Œæˆä¸ºè§£å†³ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æœ‰åŠ›è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>proKANæ˜¯ä¸€ç§é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ–°çš„æ¶æ„æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºè§£å†³è‚è„åˆ†å‰²ä¸­çš„ä¸‰ç»´è‚¿ç˜¤è¯†åˆ«é—®é¢˜ã€‚</li>
<li>proKANé‡‡ç”¨æ¸è¿›å †å Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰ï¼Œå¯åŠ¨æ€è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ä»¥å¹³è¡¡æ—¶é—´å’Œæ•ˆç‡ã€‚</li>
<li>è¿‡æ‹Ÿåˆæ£€æµ‹æœºåˆ¶å…è®¸ç½‘ç»œåœ¨æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆæ—¶åœæ­¢å¢é•¿ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>proKANä½¿ç”¨é€šè¿‡Bæ ·æ¡æ›²çº¿å»ºæ¨¡çš„å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œå¢å¼ºäº†åœ¨ä¸‰ç»´åŒ»å­¦æ•°æ®ä¸­çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºproKANåœ¨è‚è„åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å’Œå›ºå®šKANæ¶æ„ã€‚</li>
<li>proKANä¸ä»…æé«˜äº†å‡†ç¡®æ€§å’ŒDiceè¯„åˆ†ï¼Œè¿˜æé«˜äº†æ—¶é—´æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae897ac91d1bcbefb912f2cf41a547b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbd68da1a9fe7b2d6f54f07160e77f2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22c36ed2dc905b66db2047f9424d2022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e988f44ac62733a193a038692a6daf6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-112e608b8795f4430e6c4457c40d7ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d9a8dfe654e2f1537ae9e9889902f20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbb28d94526b5f1c4d6de9e30f24d4b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e379b9695d294282d99d72f7d5eb8dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff6195174770798d66cae971d1cfe7ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49c76f452beece9a3d46059bba70a966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83aab3810dc24ff9caf66e74407348ab.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CAD-GPT-Synthesising-CAD-Construction-Sequence-with-Spatial-Reasoning-Enhanced-Multimodal-LLMs"><a href="#CAD-GPT-Synthesising-CAD-Construction-Sequence-with-Spatial-Reasoning-Enhanced-Multimodal-LLMs" class="headerlink" title="CAD-GPT: Synthesising CAD Construction Sequence with Spatial   Reasoning-Enhanced Multimodal LLMs"></a>CAD-GPT: Synthesising CAD Construction Sequence with Spatial   Reasoning-Enhanced Multimodal LLMs</h2><p><strong>Authors:Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang</strong></p>
<p>Computer-aided design (CAD) significantly enhances the efficiency, accuracy, and innovation of design processes by enabling precise 2D and 3D modeling, extensive analysis, and optimization. Existing methods for creating CAD models rely on latent vectors or point clouds, which are difficult to obtain and costly to store. Recent advances in Multimodal Large Language Models (MLLMs) have inspired researchers to use natural language instructions and images for CAD model construction. However, these models still struggle with inferring accurate 3D spatial location and orientation, leading to inaccuracies in determining the spatial 3D starting points and extrusion directions for constructing geometries. This work introduces CAD-GPT, a CAD synthesis method with spatial reasoning-enhanced MLLM that takes either a single image or a textual description as input. To achieve precise spatial inference, our approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D spatial positions and 3D sketch plane rotation angles into a 1D linguistic feature space using a specialized spatial unfolding mechanism, while discretizing 2D sketch coordinates into an appropriate planar space to enable precise determination of spatial starting position, sketch orientation, and 2D sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT consistently outperforms existing state-of-the-art methods in CAD model synthesis, both quantitatively and qualitatively. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰é€šè¿‡å®ç°ç²¾ç¡®çš„2Då’Œ3Då»ºæ¨¡ã€æ·±å…¥çš„åˆ†æå’Œä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è®¾è®¡è¿‡ç¨‹çš„æ•ˆç‡ã€å‡†ç¡®æ€§å’Œåˆ›æ–°æ€§ã€‚ç°æœ‰çš„åˆ›å»ºCADæ¨¡å‹çš„æ–¹æ³•ä¾èµ–äºæ½œåœ¨å‘é‡æˆ–ç‚¹äº‘ï¼Œè¿™äº›éš¾ä»¥è·å–ä¸”æˆæœ¬é«˜æ˜‚ã€‚æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥æ¿€å‘äº†ç ”ç©¶äººå‘˜ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒè¿›è¡ŒCADæ¨¡å‹æ„å»ºã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ¨æ–­å‡†ç¡®çš„3Dç©ºé—´ä½ç½®å’Œæ–¹å‘æ—¶ä»å­˜åœ¨é—®é¢˜ï¼Œå¯¼è‡´åœ¨ç¡®å®šæ„å»ºå‡ ä½•ä½“çš„ç©ºé—´3Dèµ·å§‹ç‚¹å’ŒæŒ¤å‹æ–¹å‘æ—¶äº§ç”Ÿä¸ç²¾ç¡®ç»“æœã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†CAD-GPTï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰ç©ºé—´æ¨ç†å¢å¼ºå‹MLLMçš„CADåˆæˆæ–¹æ³•ï¼Œå®ƒæ¥å—å•å¼ å›¾åƒæˆ–æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚ä¸ºäº†ç²¾ç¡®çš„ç©ºé—´æ¨æ–­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§3Då»ºæ¨¡ç©ºé—´æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨ä¸“é—¨çš„å±•å¼€æœºåˆ¶å°†3Dç©ºé—´ä½ç½®å’Œ3Dè‰å›¾å¹³é¢æ—‹è½¬è§’åº¦æ˜ å°„åˆ°ä¸€ç»´è¯­è¨€ç‰¹å¾ç©ºé—´ä¸­ï¼ŒåŒæ—¶å°†äºŒç»´è‰å›¾åæ ‡ç¦»æ•£åŒ–åˆ°é€‚å½“çš„å¹³é¢ç©ºé—´ä¸­ï¼Œä»¥å®ç°ç²¾ç¡®çš„ç©ºé—´èµ·å§‹ä½ç½®ã€è‰å›¾æ–¹å‘å’ŒäºŒç»´è‰å›¾åæ ‡è½¬æ¢çš„ç¡®å®šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨æ•°é‡ä¸Šè¿˜æ˜¯åœ¨è´¨é‡ä¸Šï¼ŒCAD-GPTåœ¨CADæ¨¡å‹åˆæˆæ–¹é¢å‡æŒç»­ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19663v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAD-GPTæ–¹æ³•åˆ©ç”¨ç©ºé—´æ¨ç†å¢å¼ºå‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å•å¼ å›¾åƒæˆ–æ–‡æœ¬æè¿°è¾“å…¥ï¼Œå®ç°è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åˆæˆã€‚è¯¥æ–¹æ³•å¼•å…¥3Då»ºæ¨¡ç©ºé—´æœºåˆ¶ï¼Œå°†3Dç©ºé—´ä½ç½®å’Œ3Dè‰å›¾å¹³é¢æ—‹è½¬è§’åº¦æ˜ å°„åˆ°1Dè¯­è¨€ç‰¹å¾ç©ºé—´ï¼ŒåŒæ—¶ç¦»æ•£åŒ–2Dè‰å›¾åæ ‡ï¼Œä»¥å®ç°ç©ºé—´èµ·å§‹ä½ç½®ã€è‰å›¾æ–¹å‘å’Œ2Dè‰å›¾åæ ‡ç¿»è¯‘çš„ç²¾ç¡®ç¡®å®šã€‚å®éªŒè¡¨æ˜ï¼ŒCAD-GPTåœ¨CADæ¨¡å‹åˆæˆæ–¹é¢ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§ï¼Œéƒ½ä¸€è‡´ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-GPTåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡åˆæˆï¼Œæ¥å—å›¾åƒæˆ–æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚</li>
<li>ç°æœ‰CADæ¨¡å‹åˆ›å»ºæ–¹æ³•ä¸»è¦ä¾èµ–æ½œåœ¨å‘é‡æˆ–ç‚¹äº‘ï¼Œä½†è·å–å’Œå­˜å‚¨æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>CAD-GPTé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ¨æ–­å‡†ç¡®çš„3Dç©ºé—´ä½ç½®å’Œæ–¹å‘ï¼Œå½±å“å‡ ä½•æ„é€ çš„ç©ºé—´3Dèµ·ç‚¹å’ŒæŒ¤å‹æ–¹å‘ã€‚</li>
<li>CAD-GPTå¼•å…¥çš„3Då»ºæ¨¡ç©ºé—´æœºåˆ¶å°†3Dç©ºé—´ä½ç½®å’Œæ—‹è½¬è§’åº¦æ˜ å°„åˆ°1Dè¯­è¨€ç‰¹å¾ç©ºé—´ã€‚</li>
<li>è¯¥æ–¹æ³•ç¦»æ•£åŒ–2Dè‰å›¾åæ ‡ï¼Œä»¥å®ç°ç©ºé—´ä½ç½®çš„ç²¾ç¡®ç¡®å®šã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCAD-GPTåœ¨CADæ¨¡å‹åˆæˆæ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CAD-GPTçš„å¼•å…¥æœ‰æœ›æé«˜è®¾è®¡æ•ˆç‡ã€å‡†ç¡®æ€§å’Œåˆ›æ–°æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c96674278d8b92cdd93b2f81c9428cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c88bad49f607f685340be8d2797c1d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38ca61e75930a255f7f73a7ca4376e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008c9f8ed9a123c9e6cd6aa1ad9330a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f9fc3adaf3fdaeac04ccd1dc5cb19b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cdc0f6bc1d7ec6b5137ba0df52163ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45021ffedc14d26f86c15b141097ab3f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Structural-Similarity-in-Deep-Features-Image-Quality-Assessment-Robust-to-Geometrically-Disparate-Reference"><a href="#Structural-Similarity-in-Deep-Features-Image-Quality-Assessment-Robust-to-Geometrically-Disparate-Reference" class="headerlink" title="Structural Similarity in Deep Features: Image Quality Assessment Robust   to Geometrically Disparate Reference"></a>Structural Similarity in Deep Features: Image Quality Assessment Robust   to Geometrically Disparate Reference</h2><p><strong>Authors:Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang</strong></p>
<p>Image Quality Assessment (IQA) with references plays an important role in optimizing and evaluating computer vision tasks. Traditional methods assume that all pixels of the reference and test images are fully aligned. Such Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world problems with various geometric deformations between the two images. Although significant effort has been made to attack Geometrically-Disparate-Reference IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for example, by dedicated designs for image super-resolution and retargeting, or by assuming the geometric distortions to be small that can be countered by translation-robust filters or by explicit image registrations. Here we rethink this problem and propose a unified, non-training-based Deep Structural Similarity (DeepSSIM) approach to address the above problems in a single framework, which assesses structural similarity of deep features in a simple but efficient way and uses an attention calibration strategy to alleviate attention deviation. The proposed method, without application-specific design, achieves state-of-the-art performance on AR-IQA datasets and meanwhile shows strong robustness to various GDR-IQA test cases. Interestingly, our test also shows the effectiveness of DeepSSIM as an optimization tool for training image super-resolution, enhancement and restoration, implying an even wider generalizability. \footnote{Source code will be made public after the review is completed. </p>
<blockquote>
<p>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰åœ¨ä¼˜åŒ–å’Œè¯„ä¼°è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•å‡è®¾å‚è€ƒå›¾åƒå’Œæµ‹è¯•å›¾åƒçš„æ‰€æœ‰åƒç´ å®Œå…¨å¯¹é½ã€‚è¿™ç§å¯¹é½å‚è€ƒIQAï¼ˆAR-IQAï¼‰æ–¹æ³•æ— æ³•è§£å†³ä¸¤ä¸ªå›¾åƒä¹‹é—´å­˜åœ¨å„ç§å‡ ä½•å˜å½¢ç­‰çœŸå®ä¸–ç•Œé—®é¢˜ã€‚å°½ç®¡äººä»¬å·²ç»ä¸ºè§£å†³å‡ ä½•ç¦»æ•£å‚è€ƒIQAï¼ˆGDR-IQAï¼‰é—®é¢˜ä»˜å‡ºäº†å·¨å¤§åŠªåŠ›ï¼Œä½†å®ƒä»ç„¶æ˜¯ä»¥ä»»åŠ¡ä¾èµ–çš„æ–¹å¼è§£å†³ï¼Œä¾‹å¦‚é€šè¿‡ä¸ºå›¾åƒè¶…åˆ†è¾¨ç‡å’Œé‡å®šå‘è®¾è®¡çš„ä¸“ç”¨è®¾å¤‡ï¼Œæˆ–è€…å‡è®¾å‡ ä½•ç•¸å˜è¶³å¤Ÿå°å¯ä»¥é€šè¿‡å¹³ç§»ç¨³å¥æ»¤æ³¢å™¨æˆ–æ˜¾å¼å›¾åƒæ³¨å†Œæ¥å…‹æœã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œäº†é‡æ–°æ€è€ƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ã€éåŸºäºè®­ç»ƒæ·±åº¦ç»“æ„ç›¸ä¼¼æ€§ï¼ˆDeepSSIMï¼‰æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ƒä»¥ä¸€ç§ç®€å•é«˜æ•ˆçš„æ–¹å¼è¯„ä¼°æ·±åº¦ç‰¹å¾çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æ ¡å‡†ç­–ç•¥æ¥ç¼“è§£æ³¨æ„åŠ›åå·®ã€‚è¯¥æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šåº”ç”¨è¿›è¡Œè®¾è®¡ï¼Œåœ¨AR-IQAæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å„ç§GDR-IQAæµ‹è¯•ç”¨ä¾‹ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æµ‹è¯•è¿˜æ˜¾ç¤ºäº†DeepSSIMä½œä¸ºå›¾åƒè¶…åˆ†è¾¨ç‡ã€å¢å¼ºå’Œä¿®å¤è®­ç»ƒä¼˜åŒ–å·¥å…·çš„æœ‰æ•ˆæ€§ï¼Œè¿™è¡¨æ˜å…¶æ›´å¹¿æ³›çš„é€šç”¨æ€§ã€‚\footnote{æºä»£ç å°†åœ¨å®¡æŸ¥å®Œæˆåå…¬å¼€ã€‚}</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç»“æ„ç›¸ä¼¼æ€§ï¼ˆDeepSSIMï¼‰çš„ç»Ÿä¸€éè®­ç»ƒæ–¹æ³•æ¥è§£å†³å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä¸­çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®¾è®¡ï¼Œå³å¯åœ¨å•ä¸€æ¡†æ¶å†…è§£å†³å¯¹é½å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆAR-IQAï¼‰å’Œå‡ ä½•ç¦»æ•£å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆGDR-IQAï¼‰é—®é¢˜ã€‚å®ƒé€šè¿‡è¯„ä¼°æ·±åº¦ç‰¹å¾çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æ ¡å‡†ç­–ç•¥æ¥å‡è½»æ³¨æ„åŠ›åå·®ï¼Œå®ç°äº†å¯¹å¤šç§æƒ…å†µä¸‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒDeepSSIMè¿˜è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„ä¼˜åŒ–å·¥å…·ï¼Œå¯ç”¨äºè®­ç»ƒå›¾åƒè¶…åˆ†è¾¨ç‡ã€å¢å¼ºå’Œä¿®å¤ä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºå…¶æ›´å¹¿æ³›çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•å‡è®¾å‚è€ƒå’Œæµ‹è¯•å›¾åƒçš„æ‰€æœ‰åƒç´ å®Œå…¨å¯¹é½ï¼Œä¸é€‚ç”¨äºå…·æœ‰å‡ ä½•å˜å½¢çš„çœŸå®ä¸–ç•Œé—®é¢˜ã€‚</li>
<li>ä¸€ç§æ–°çš„éè®­ç»ƒåŸºäºæ·±åº¦ç»“æ„ç›¸ä¼¼æ€§ï¼ˆDeepSSIMï¼‰çš„æ–¹æ³•è¢«æå‡ºï¼Œç”¨äºè§£å†³å¯¹é½å’Œå‡ ä½•ç¦»æ•£å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆAR-IQAå’ŒGDR-IQAï¼‰é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¯„ä¼°æ·±åº¦ç‰¹å¾çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå®ç°ç®€å•è€Œæœ‰æ•ˆçš„å›¾åƒè´¨é‡è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨äº†æ³¨æ„åŠ›æ ¡å‡†ç­–ç•¥æ¥å‡è½»æ³¨æ„åŠ›åå·®ï¼Œå¢å¼ºäº†æ–¹æ³•çš„é²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®¾è®¡ï¼Œä¸”åœ¨AR-IQAæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>DeepSSIMå¯¹äºè§£å†³å›¾åƒè¶…åˆ†è¾¨ç‡ã€å¢å¼ºå’Œæ¢å¤ç­‰ä»»åŠ¡å…·æœ‰ä¼˜åŒ–ä½œç”¨ï¼Œæ˜¾ç¤ºå‡ºå…¶å¹¿æ³›çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d0e8dc790a6e1b67c1c78460d6611f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-436dd04353f3f99d1d77c924c939e91a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deef5d83c88c1dfb2fc08c8cb8f7c593.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-587eca8e221408b1b2a3ad8639e78139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71894a57841c7fccc064d2802557f018.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Open-Vocabulary-Remote-Sensing-Image-Semantic-Segmentation"><a href="#Towards-Open-Vocabulary-Remote-Sensing-Image-Semantic-Segmentation" class="headerlink" title="Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation"></a>Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation</h2><p><strong>Authors:Chengyang Ye, Yunzhi Zhuge, Pingping Zhang</strong></p>
<p>Recently, deep learning based methods have revolutionized remote sensing image segmentation. However, these methods usually rely on a pre-defined semantic class set, thus needing additional image annotation and model training when adapting to new classes. More importantly, they are unable to segment arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary semantic classes in remote sensing images. To address the lack of OVRSISS datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images covering 40 diverse semantic classes. In addition, we propose a novel framework named GSNet that integrates domain priors from special remote sensing models and versatile capabilities of general vision-language models. Technically, GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE first captures comprehensive features from both special models and general models in dual streams. Then, with the guidance of variable vocabularies, QGFF integrates specialist and generalist features, enabling them to complement each other. Finally, RIPD is proposed to aggregate multi-source features for more accurate mask predictions. Experiments show that our method outperforms other methods by a large margin, and our proposed LandDiscover50K improves the performance of OVRSISS methods. The proposed dataset and method will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/yecy749/GSNet">https://github.com/yecy749/GSNet</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²é¢†åŸŸæ€èµ·äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„è¯­ä¹‰ç±»åˆ«é›†ï¼Œå› æ­¤åœ¨é€‚åº”æ–°ç±»åˆ«æ—¶éœ€è¦é¢å¤–çš„å›¾åƒæ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä»¬æ— æ³•å¯¹ä»»æ„è¯­ä¹‰ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆOVRSISSï¼‰ï¼Œæ—¨åœ¨å®ç°å¯¹é¥æ„Ÿå›¾åƒä¸­ä»»æ„è¯­ä¹‰ç±»åˆ«çš„åˆ†å‰²ã€‚ä¸ºäº†è§£å†³OVRSISSæ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†LandDiscover50Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«51,846å¼ å›¾åƒçš„ç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–40ä¸ªå¤šæ ·åŒ–çš„è¯­ä¹‰ç±»åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGSNetçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†ç‰¹æ®Šé¥æ„Ÿæ¨¡å‹çš„é¢†åŸŸå…ˆéªŒçŸ¥è¯†å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚åœ¨æŠ€æœ¯ä¸Šï¼ŒGSNetç”±åŒæµå›¾åƒç¼–ç å™¨ï¼ˆDSIEï¼‰ã€æŸ¥è¯¢å¼•å¯¼ç‰¹å¾èåˆï¼ˆQGFFï¼‰å’Œæ®‹å·®ä¿¡æ¯ä¿ç•™è§£ç å™¨ï¼ˆRIPDï¼‰ç»„æˆã€‚DSIEé¦–å…ˆä»ç‰¹æ®Šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹çš„åŒæµä¸­æå–å…¨é¢ç‰¹å¾ã€‚ç„¶åï¼Œåœ¨å¯å˜è¯æ±‡è¡¨çš„æŒ‡å¯¼ä¸‹ï¼ŒQGFFæ•´åˆä¸“ä¸šå’Œæ™®é€šç‰¹å¾ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç›¸äº’è¡¥å……ã€‚æœ€åï¼ŒRIPDè¢«æå‡ºæ¥èšåˆå¤šæºç‰¹å¾ï¼Œä»¥è¿›è¡Œæ›´å‡†ç¡®çš„æ©è†œé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–æ–¹æ³•ä¸Šå…·æœ‰è¾ƒå¤§ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºçš„LandDiscover50Kæé«˜äº†OVRSISSæ–¹æ³•çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ•°æ®é›†å’Œæ–¹æ³•å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yecy749/GSNet">https://github.com/yecy749/GSNet</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19492v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²é¢†åŸŸå¼•å‘äº†ä¸€åœºé©å‘½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦é¢„è®¾çš„è¯­ä¹‰ç±»åˆ«é›†ï¼Œå½“é€‚åº”æ–°ç±»åˆ«æ—¶éœ€è¦é¢å¤–çš„å›¾åƒæ³¨é‡Šå’Œæ¨¡å‹è®­ç»ƒã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆOVRSISSï¼‰ï¼Œæ—¨åœ¨å®ç°å¯¹é¥æ„Ÿå›¾åƒä¸­çš„ä»»æ„è¯­ä¹‰ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚ä¸ºåº”å¯¹ç¼ºä¹OVRSISSæ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼€å‘äº†LandDiscover50Kæ•°æ®é›†ï¼ŒåŒ…å«51,846å¼ å›¾åƒï¼Œæ¶µç›–40ä¸ªå¤šæ ·åŒ–çš„è¯­ä¹‰ç±»åˆ«ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGSNetçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†ç‰¹æ®Šé¥æ„Ÿæ¨¡å‹çš„é¢†åŸŸå…ˆéªŒçŸ¥è¯†å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒLandDiscover50Kæ•°æ®é›†æé«˜äº†OVRSISSæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨å·²ç»å¼•å‘äº†ä¸€åœºé©å‘½ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–äºé¢„è®¾çš„è¯­ä¹‰ç±»åˆ«é›†ï¼Œéš¾ä»¥é€‚åº”æ–°ç±»åˆ«ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆOVRSISSï¼‰ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LandDiscover50Kæ•°æ®é›†è¢«å¼€å‘å‡ºæ¥ä»¥æ”¯æŒOVRSISSï¼ŒåŒ…å«å¤šæ ·åŒ–çš„è¯­ä¹‰ç±»åˆ«ã€‚</li>
<li>GSNetæ¡†æ¶è¢«æå‡ºï¼Œèåˆäº†ç‰¹æ®Šé¥æ„Ÿæ¨¡å‹å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>GSNetåŒ…æ‹¬åŒæµå›¾åƒç¼–ç å™¨ã€æŸ¥è¯¢å¼•å¯¼ç‰¹å¾èåˆå’Œæ®‹å·®ä¿¡æ¯ä¿ç•™è§£ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-611fb43ebba71a36ad42688e7c6a5822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7431be40eede724e0182b1a0b7a219b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ff1a61fcc4ab227c5c72f28ac6a1f3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfc655f72b48818862a7c183d2d50477.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-Radiance-Fields-from-a-Single-Snapshot-Compressive-Image"><a href="#Learning-Radiance-Fields-from-a-Single-Snapshot-Compressive-Image" class="headerlink" title="Learning Radiance Fields from a Single Snapshot Compressive Image"></a>Learning Radiance Fields from a Single Snapshot Compressive Image</h2><p><strong>Authors:Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu</strong></p>
<p>In this paper, we explore the potential of Snapshot Compressive Imaging (SCI) technique for recovering the underlying 3D scene structure from a single temporal compressed image. SCI is a cost-effective method that enables the recording of high-dimensional data, such as hyperspectral or temporal information, into a single image using low-cost 2D imaging sensors. To achieve this, a series of specially designed 2D masks are usually employed, reducing storage and transmission requirements and offering potential privacy protection. Inspired by this, we take one step further to recover the encoded 3D scene information leveraging powerful 3D scene representation capabilities of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we formulate the physical imaging process of SCI as part of the training of NeRF, allowing us to exploit its impressive performance in capturing complex scene structures. In addition, we further integrate the popular 3D Gaussian Splatting (3DGS) framework and propose SCISplat to improve 3D scene reconstruction quality and training&#x2F;rendering speed by explicitly optimizing point clouds into 3D Gaussian representations. To assess the effectiveness of our method, we conduct extensive evaluations using both synthetic data and real data captured by our SCI system. Experimental results demonstrate that our proposed approach surpasses the state-of-the-art methods in terms of image reconstruction and novel view synthesis. Moreover, our method also exhibits the ability to render high frame-rate multi-view consistent images in real time by leveraging SCI and the rendering capabilities of 3DGS. Codes will be available at: <a target="_blank" rel="noopener" href="https://github.com/WU-">https://github.com/WU-</a> CVGL&#x2F;SCISplat. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†Snapshot Compressive Imagingï¼ˆSCIï¼‰æŠ€æœ¯åœ¨ä»å•ä¸ªæ—¶é—´å‹ç¼©å›¾åƒä¸­æ¢å¤æ½œåœ¨çš„3Dåœºæ™¯ç»“æ„æ–¹é¢çš„æ½œåŠ›ã€‚SCIæ˜¯ä¸€ç§ç»æµé«˜æ•ˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†é«˜ç»´æ•°æ®ï¼ˆå¦‚è¶…å…‰è°±æˆ–æ—¶é—´ä¿¡æ¯ï¼‰ä½¿ç”¨ä½æˆæœ¬2Dæˆåƒä¼ æ„Ÿå™¨è®°å½•ä¸ºå•ä¸ªå›¾åƒã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œé€šå¸¸ä½¿ç”¨ä¸€ç³»åˆ—ä¸“é—¨è®¾è®¡çš„2Dæ©è†œï¼Œä»¥é™ä½å­˜å‚¨å’Œä¼ è¾“è¦æ±‚ï¼Œå¹¶æä¾›æ½œåœ¨çš„éšç§ä¿æŠ¤ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å¼ºå¤§çš„3Dåœºæ™¯è¡¨ç¤ºèƒ½åŠ›ï¼Œæ¢å¤ç¼–ç çš„3Dåœºæ™¯ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†SCINeRFï¼Œå°†SCIçš„ç‰©ç†æˆåƒè¿‡ç¨‹ä½œä¸ºNeRFè®­ç»ƒçš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œå……åˆ†åˆ©ç”¨å…¶åœ¨æ•æ‰å¤æ‚åœºæ™¯ç»“æ„æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é›†æˆäº†æµè¡Œçš„3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰æ¡†æ¶ï¼Œå¹¶æå‡ºäº†SCISplatï¼Œé€šè¿‡æ˜¾å¼ä¼˜åŒ–ç‚¹äº‘åˆ°3Dé«˜æ–¯è¡¨ç¤ºï¼Œæé«˜3Dåœºæ™¯é‡å»ºè´¨é‡ä»¥åŠè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®å’Œç”±æˆ‘ä»¬çš„SCIç³»ç»Ÿæ•è·çš„çœŸå®æ•°æ®è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å±•ç¤ºäº†åˆ©ç”¨SCIå’Œ3DGSçš„æ¸²æŸ“èƒ½åŠ›å®æ—¶å‘ˆç°é«˜å¸§ç‡å¤šè§†è§’ä¸€è‡´å›¾åƒçš„èƒ½åŠ›ã€‚ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/WU-CVGL/SCISplat%E3%80%82">https://github.com/WU-CVGL/SCISplatã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19483v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¿«ç…§å‹ç¼©æˆåƒï¼ˆSCIï¼‰æŠ€æœ¯åœ¨ä»å•ä¸ªæ—¶é—´å‹ç¼©å›¾åƒä¸­æ¢å¤æ½œåœ¨çš„ä¸‰ç»´åœºæ™¯ç»“æ„æ–¹é¢çš„æ½œåŠ›ã€‚SCIæ˜¯ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†é«˜ç»´æ•°æ®ï¼ˆå¦‚å…‰è°±æˆ–æ—¶é—´ä¿¡æ¯ï¼‰ä½¿ç”¨ä½æˆæœ¬çš„äºŒç»´æˆåƒä¼ æ„Ÿå™¨è®°å½•ä¸ºå•ä¸ªå›¾åƒã€‚ä¸ºè¾¾åˆ°æ­¤ç›®çš„ï¼Œé€šå¸¸ä½¿ç”¨ä¸€ç³»åˆ—ä¸“é—¨è®¾è®¡çš„äºŒç»´æ©è†œï¼Œä»¥é™ä½å­˜å‚¨å’Œä¼ è¾“è¦æ±‚ï¼Œå¹¶æä¾›æ½œåœ¨çš„éšç§ä¿æŠ¤ã€‚å—å¯å‘äºæ­¤ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å¼ºå¤§ä¸‰ç»´åœºæ™¯è¡¨ç¤ºèƒ½åŠ›æ¥æ¢å¤ç¼–ç çš„ä¸‰ç»´åœºæ™¯ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†SCINeRFå’ŒSCISplatæ–¹æ³•ï¼Œå°†SCIçš„ç‰©ç†æˆåƒè¿‡ç¨‹çº³å…¥NeRFçš„è®­ç»ƒä¸­ï¼Œå¹¶é€šè¿‡æ˜¾å¼ä¼˜åŒ–ç‚¹äº‘åˆ°ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºæ¥æé«˜ä¸‰ç»´åœºæ™¯é‡å»ºè´¨é‡ã€è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç¤ºäº†åˆ©ç”¨SCIå’Œ3DGSçš„æ¸²æŸ“èƒ½åŠ›å®æ—¶å‘ˆç°é«˜å¸§ç‡ã€å¤šè§†è§’ä¸€è‡´å›¾åƒçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SCIæŠ€æœ¯èƒ½å¤Ÿä»ä½æˆæœ¬2Dæˆåƒä¼ æ„Ÿå™¨è®°å½•é«˜ç»´æ•°æ®ï¼Œå¦‚å…‰è°±æˆ–æ—¶é—´ä¿¡æ¯ï¼Œä¸ºå­˜å‚¨å’Œä¼ è¾“æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä½¿ç”¨ä¸€ç³»åˆ—ä¸“é—¨è®¾è®¡çš„äºŒç»´æ©è†œå¯ä»¥å¢å¼ºéšç§ä¿æŠ¤ã€‚</li>
<li>SCINeRFæ–¹æ³•é€šè¿‡å°†SCIçš„ç‰©ç†æˆåƒè¿‡ç¨‹çº³å…¥NeRFçš„è®­ç»ƒä¸­ï¼Œæé«˜äº†å¯¹å¤æ‚åœºæ™¯ç»“æ„çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>SCISplatæ–¹æ³•é€šè¿‡æ•´åˆ3Dé«˜æ–¯Splattingæ¡†æ¶æé«˜äº†ä¸‰ç»´åœºæ™¯é‡å»ºè´¨é‡ã€è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å›¾åƒé‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>SCISplatèƒ½å¤Ÿå®ç°é«˜å¸§ç‡ã€å¤šè§†è§’ä¸€è‡´å›¾åƒçš„å®æ—¶æ¸²æŸ“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-078e653f005f60dd8eddb8f7e540ad8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aa43ff602cb8e162d882386d84cb03d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975272d5740b4d803f8a988529d6283f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6181bb6b6954dfacf861d6a04e3e5350.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Stable-TTS Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody   Prompting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-02/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d93f74034282349cb9c977e84f2d9c33.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-02  Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image   Modeling for CBCT Tooth Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
