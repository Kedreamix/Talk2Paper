<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-10-11  MeanVC Lightweight and Streaming Zero-Shot Voice Conversion via Mean   Flows">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5af576cacd857cc23f3a3b763f36df56~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126197&auth_key=1760126197-0-0-477902b496ad642e75afdc180b93f0e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-11-更新"><a href="#2025-10-11-更新" class="headerlink" title="2025-10-11 更新"></a>2025-10-11 更新</h1><h2 id="MeanVC-Lightweight-and-Streaming-Zero-Shot-Voice-Conversion-via-Mean-Flows"><a href="#MeanVC-Lightweight-and-Streaming-Zero-Shot-Voice-Conversion-via-Mean-Flows" class="headerlink" title="MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean   Flows"></a>MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean   Flows</h2><p><strong>Authors:Guobin Ma, Jixun Yao, Ziqian Ning, Yuepeng Jiang, Lingxin Xiong, Lei Xie, Pengcheng Zhu</strong></p>
<p>Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker to any unseen target speaker while preserving linguistic content. Growing application scenarios demand models with streaming inference capabilities. This has created a pressing need for models that are simultaneously fast, lightweight, and high-fidelity. However, existing streaming methods typically rely on either autoregressive (AR) or non-autoregressive (NAR) frameworks, which either require large parameter sizes to achieve strong performance or struggle to generalize to unseen speakers. In this study, we propose MeanVC, a lightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion transformer with a chunk-wise autoregressive denoising strategy, combining the strengths of both AR and NAR paradigms for efficient streaming processing. By introducing mean flows, MeanVC regresses the average velocity field during training, enabling zero-shot VC with superior speech quality and speaker similarity in a single sampling step by directly mapping from the start to the endpoint of the flow trajectory. Additionally, we incorporate diffusion adversarial post-training to mitigate over-smoothing and further enhance speech quality. Experimental results demonstrate that MeanVC significantly outperforms existing zero-shot streaming VC systems, achieving superior conversion quality with higher efficiency and significantly fewer parameters. Audio demos and code are publicly available at <a target="_blank" rel="noopener" href="https://aslp-lab.github.io/MeanVC">https://aslp-lab.github.io/MeanVC</a>. </p>
<blockquote>
<p>零样本语音转换（VC）旨在将源说话者的音质转移到任何未见过的目标说话者，同时保留语言内容。不断增长的应用场景要求模型具备流式推理能力。这产生了对同时具备快速、轻便和高保真性能的模型的迫切需求。然而，现有的流式方法通常依赖于自回归（AR）或非自回归（NAR）框架，这些框架要么需要较大的参数规模以实现强劲性能，要么在推广到未见过的说话者时面临困难。在这项研究中，我们提出了MeanVC，这是一种轻便的流式零样本VC方法。MeanVC引入了一个扩散变压器，采用分块自回归去噪策略，结合了AR和NAR范式的优点，以实现高效的流式处理。通过引入平均流，MeanVC在训练过程中回归平均速度场，使零样本VC能够在单个采样步骤中以卓越的语音质量和说话人相似性直接从流的起点映射到终点。此外，我们引入了扩散对抗后训练，以减轻过度平滑并进一步改善语音质量。实验结果表明，MeanVC显著优于现有的零样本流式VC系统，实现了高质量的转换性能，具有更高的效率和更少的参数。音频演示和代码可在<a target="_blank" rel="noopener" href="https://aslp-lab.github.io/MeanVC%E4%B8%8A%E5%85%AC%E5%BC%80%E6%9F%A5%E7%9C%8B%E3%80%82">https://aslp-lab.github.io/MeanVC上公开查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08392v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种名为MeanVC的轻量级流式零样本语音转换方法。它结合了自回归和非自回归框架的优势，采用基于扩散变压器的块级自回归去噪策略，实现高效流式处理。通过引入平均流，MeanVC在训练过程中回归平均速度场，实现单步采样中的零样本语音转换，具有出色的语音质量和说话人相似性。此外，还采用扩散对抗后训练来缓解过度平滑问题，进一步提高语音质量。实验结果表明，MeanVC在零样本流式语音转换系统中显著优于现有方法，具有更高的转换质量、效率和较少的参数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MeanVC是一种轻量级的流式零样本语音转换方法，结合了自回归和非自回归框架的优点。</li>
<li>采用基于扩散变压器的块级自回归去噪策略，实现高效流式处理。</li>
<li>通过引入平均流，MeanVC在训练过程中回归平均速度场，提高了语音转换的质量和效率。</li>
<li>MeanVC实现了单步采样中的零样本语音转换，具有出色的语音质量和说话人相似性。</li>
<li>采用扩散对抗后训练技术，缓解语音转换中的过度平滑问题，进一步提升语音质量。</li>
<li>实验结果表明，MeanVC显著优于现有零样本流式语音转换系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08392">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bb7b63cdb43cc11ffc458c855770cf0f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126066&auth_key=1760126066-0-0-4c0b866a50d59ddb83fd4aded833d368&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d865f80258014398c148c2019b637f1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126073&auth_key=1760126073-0-0-91747d04102c011ca3246b6c1216ecac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73c8643b72f52625d89444d17e6eade3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126079&auth_key=1760126079-0-0-6fd51f065419965347911d31aa53cfe3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4495f14bffa2c624bea30933ef21c8fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126086&auth_key=1760126086-0-0-855fe58c813d36755a28fdba3dad7a48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-55f4367afe0ff7482df2100a68f615e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126092&auth_key=1760126092-0-0-d3c017d5017da3c22c923119037e232b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-081959126068aa8370148a4dd7005e87~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126098&auth_key=1760126098-0-0-cf296e3248f9ccb1530170e860e70351&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CS3-Bench-Evaluating-and-Enhancing-Speech-to-Speech-LLMs-for-Mandarin-English-Code-Switching"><a href="#CS3-Bench-Evaluating-and-Enhancing-Speech-to-Speech-LLMs-for-Mandarin-English-Code-Switching" class="headerlink" title="CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for   Mandarin-English Code-Switching"></a>CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for   Mandarin-English Code-Switching</h2><p><strong>Authors:Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</strong></p>
<p>The advancement of multimodal large language models has accelerated the development of speech-to-speech interaction systems. While natural monolingual interaction has been achieved, we find existing models exhibit deficiencies in language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark (CS3-Bench), experiments on 7 mainstream models demonstrate a relative performance drop of up to 66% in knowledge-intensive question answering and varying degrees of misunderstanding in open-ended conversations. Starting from a model with severe performance deterioration, we propose both data constructions and training approaches to improve the language alignment capabilities, specifically employing Chain of Recognition (CoR) to enhance understanding and Keyword Highlighting (KH) to guide generation. Our approach improves the knowledge accuracy from 25.14% to 46.13%, with open-ended understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation errors in the secondary language. CS3-Bench is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/VocalNet/CS3-Bench">https://huggingface.co/datasets/VocalNet/CS3-Bench</a>. </p>
<blockquote>
<p>多模态大型语言模型的进步加速了语音到语音交互系统的发展。虽然自然单语交互已经实现，但我们发现现有模型在语言对齐方面存在缺陷。在我们提出的跨语言语音到语音基准测试（CS3-Bench）中，对7种主流模型的实验表明，在知识密集型问答方面，性能相对下降了高达66%，在开放式对话中存在不同程度的误解。针对性能严重退化的模型，我们提出数据构建和训练方法来提高语言对齐能力，具体采用识别链（CoR）增强理解和关键词高亮（KH）来指导生成。我们的方法将知识准确性从25.14%提高到46.13%，开放对话的理解率从64.5%提高到86.5%，并显著减少了第二语言的发音错误。CS3-Bench可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/VocalNet/CS3-Bench%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/VocalNet/CS3-Bench访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着多模态大型语言模型的进步，推动了语音到语音交互系统的发展。当前模型在自然单语交互方面取得了进展，但在语言对齐方面存在缺陷。在提出的跨语言语音到语音基准测试（CS3-Bench）中，对7种主流模型的实验表明，在知识密集型问答方面性能下降相对高达66%，并且在开放式对话中存在不同程度的误解。通过数据构建和训练方法的改进，特别是采用识别链（CoR）增强理解和关键词高亮（KH）引导生成的方法，改善了语言对齐能力。该方法提高了知识准确性从25.14%至46.13%，增强了开放式对话的理解率从64.5%至86.5%，并显著减少了次要语言的发音错误。CS3-Bench基准测试可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/VocalNet/CS3%E4%BD%BF%EFF%ACF%BC%EF%BC%BD%E8%AE%BF%E6%B5%8B%E3%80%82">https://huggingface.co/datasets/VocalNet/CS3-Bench访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型的进步推动了语音到语音交互系统的发展。</li>
<li>当前模型在自然单语交互方面存在性能提升，但在语言对齐方面存在缺陷。</li>
<li>跨语言语音到语音基准测试（CS3-Bench）揭示了主流模型在知识密集型问答方面的性能下降和开放式对话中的误解问题。</li>
<li>通过数据构建和训练方法的改进，提高了语言对齐能力。</li>
<li>采用识别链（CoR）和关键词高亮（KH）的方法改善了语言对齐，提高了知识准确性和开放式对话的理解率。</li>
<li>提出的改进方法显著减少了次要语言的发音错误。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-91c5eb097bc71ac7c95ab86524cf4482~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126105&auth_key=1760126105-0-0-42ad76c32f074bfb1e68e3baa8270968&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c332ff0a963ee3f25b3b28739d98862~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126112&auth_key=1760126112-0-0-1121c240a535dff7ce6722e56ab54b35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22791f29fdb650d4fdbeb837c4b0be3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126119&auth_key=1760126119-0-0-0c2892aab775fcc138cab585de4d1ead&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4dea7fa61914071e59fb73833c2135fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126126&auth_key=1760126126-0-0-f9aec9226f4586cdeb9013072c4f752a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1478c1b86b1adf4073ed69341dc2ee7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126132&auth_key=1760126132-0-0-e767455c696a5336f6ae45f06fb83aca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-GUI-Architecture-for-Interfacing-with-LLM-Based-Conversational-Assistants"><a href="#A-Multimodal-GUI-Architecture-for-Interfacing-with-LLM-Based-Conversational-Assistants" class="headerlink" title="A Multimodal GUI Architecture for Interfacing with LLM-Based   Conversational Assistants"></a>A Multimodal GUI Architecture for Interfacing with LLM-Based   Conversational Assistants</h2><p><strong>Authors:Hans G. W. van Dam</strong></p>
<p>Advances in large language models (LLMs) and real-time speech recognition now make it possible to issue any graphical user interface (GUI) action through natural language and receive the corresponding system response directly through the GUI. Most production applications were never designed with speech in mind. This article provides a concrete architecture that enables GUIs to interface with LLM-based speech-enabled assistants.   The architecture makes an application’s navigation graph and semantics available through the Model Context Protocol (MCP). The ViewModel, part of the MVVM (Model-View-ViewModel) pattern, exposes the application’s capabilities to the assistant by supplying both tools applicable to a currently visible view and application-global tools extracted from the GUI tree router. This architecture facilitates full voice accessibility while ensuring reliable alignment between spoken input and the visual interface, accompanied by consistent feedback across modalities. It future-proofs apps for upcoming OS super assistants that employ computer use agents (CUAs) and natively consume MCP if an application provides it.   To address concerns about privacy and data security, the practical effectiveness of locally deployable, open-weight LLMs for speech-enabled multimodal UIs is evaluated. Findings suggest that recent smaller open-weight models approach the performance of leading proprietary models in overall accuracy and require enterprise-grade hardware for fast responsiveness.   A demo implementation of the proposed architecture can be found at <a target="_blank" rel="noopener" href="https://github.com/hansvdam/langbar">https://github.com/hansvdam/langbar</a> </p>
<blockquote>
<p>随着大型语言模型（LLM）和实时语音识别技术的不断进步，现在可以通过自然语言执行任何图形用户界面（GUI）操作，并直接通过GUI接收相应的系统响应。大多数生产应用程序在设计时并未考虑语音功能。本文提供了一个具体的架构，使GUI能够与基于LLM的语音助手进行交互。该架构通过模型上下文协议（MCP）使应用程序的导航图和语义得以呈现。作为MVVM（Model-View-ViewModel）模式的一部分，ViewModel通过提供适用于当前可见视图的工具以及从GUI树路由器中提取的应用程序全局工具，向助手公开应用程序的功能。此架构实现了全面的语音访问功能，同时确保口头输入与视觉界面之间的可靠对齐，并提供跨模式的持续反馈。它为即将到来的操作系统超级助手提供了保障，这些助手使用计算机使用代理（CUA）并原生消费MCP，如果应用程序提供的话。为了应对关于隐私和数据安全的担忧，评估了用于语音启用的多模式UI的可在本地部署的开放权重LLM的实际有效性。研究结果表明，最近的较小开放权重模型在总体准确性方面接近领先专有模型的表现，并需要企业级硬件来实现快速响应。所提议架构的演示实现可在<a target="_blank" rel="noopener" href="https://github.com/hansvdam/langbar">https://github.com/hansvdam/langbar</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06223v2">PDF</a> 24 pages, 19 figures, code available at   <a target="_blank" rel="noopener" href="https://github.com/hansvdam/langbar">https://github.com/hansvdam/langbar</a></p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的实时语音识别技术，现在可以通过自然语言驱动任何图形用户界面（GUI）的行动，并直接通过GUI获取系统回应。文章提出了一种具体架构，使GUI与基于LLM的语音助手进行交互。该架构通过模型上下文协议（MCP）提供应用程序的导航图和语义。文章还评价了关于隐私和数据安全的担忧，并发现小型开源模型在总体准确性上接近领先的专业模型，但需要企业级硬件来实现快速响应。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）和实时语音识别技术融合，可通过自然语言控制GUI，并获取系统反馈。</li>
<li>文章介绍了一种具体架构，该架构通过模型上下文协议（MCP）使GUI与基于LLM的语音助手交互。</li>
<li>ViewModel在MVVM模式中的作用是向语音助手展示应用程序的功能，提供当前视图可用的工具和全局工具。</li>
<li>该架构确保了语音输入与视觉界面的可靠对齐，并提供了跨模式的一致反馈。</li>
<li>文章还考虑了隐私和数据安全问题，评价了本地部署的开源LLM在语音赋能多模式UI中的实际效果。</li>
<li>研究发现，小型开源模型在总体准确性上表现出色，接近领先的专业模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-94b0fea6c6374283db76e8440ee8646b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126140&auth_key=1760126140-0-0-6cc1cab27140c55a72418753286d6de5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b36e1c34b643a1e0b2931b9238ba0c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126148&auth_key=1760126148-0-0-18e3ca5b4306f4ef73b3b556c7c3ae1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d132d84af904ccb013926774fcec1d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126155&auth_key=1760126155-0-0-d74be545d5f499665a0d6ceb0898909b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f9e3a9496979703fb802c7c56b9e7f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126161&auth_key=1760126161-0-0-f62fac9f03311778bfae8669e0818be6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4da75ead38342e3dff1d7e47d3cd6603~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126168&auth_key=1760126168-0-0-4892ddd2004cd4d741baef683fb9382b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b47d9116f17dbac147cb61397bb9229~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126174&auth_key=1760126174-0-0-37dcaa1f9eb5379945aecf963912fc91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Audio-Conditioned-Diffusion-LLMs-for-ASR-and-Deliberation-Processing"><a href="#Audio-Conditioned-Diffusion-LLMs-for-ASR-and-Deliberation-Processing" class="headerlink" title="Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing"></a>Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing</h2><p><strong>Authors:Mengqi Wang, Zhan Liu, Zengrui Jin, Guangzhi Sun, Chao Zhang, Philip C. Woodland</strong></p>
<p>Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%&#x2F;4.94% WER on test-clean&#x2F;test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements. </p>
<blockquote>
<p>基于扩散的大型语言模型（DLLMs）最近作为自回归解码器的替代方案而日益受到关注。在这项研究中，我们对使用基于扩散的大型语言模型LLaDA进行自动语音识别（ASR）进行了实证研究。我们首先研究其在基于外部深思处理模块的Whisper-LLaMA转录中的应用。通过利用LLaDA的双向注意力和去噪能力，我们探索了随机遮挡、低置信度遮挡和半自回归策略，结果表明Whisper-LLaDA与基线相比大幅降低了WER。在LibriSpeech上，最佳级联系统在测试干净&#x2F;其他测试上的WER达到2.25%&#x2F;4.94%，相对于Whisper-LLaMA基线，在其他测试分割上实现了12.3%的相对改进。相比之下，没有声学特征的纯文本LLaDA未能提高准确性，这凸显了音频条件嵌入的重要性。我们进一步评估了作为独立解码器的Whisper-LLaDA的ASR性能，采用扩散和半自回归解码。大多数实验配置实现了比Whisper-LLaMA基线更快的推理速度，尽管识别准确率略有下降。这些发现为基于扩散的LLM在ASR方面的应用提供了实证观点，并指出了改进的有希望的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16622v2">PDF</a> </p>
<p><strong>Summary</strong>：基于扩散的大型语言模型LLaDA在自动语音识别（ASR）中的应用进行了实证研究。将其作为Whisper-LLaMA的外部决策处理模块，利用LLaDA的双向注意力和去噪能力，通过随机掩蔽、低信心掩蔽和半自回归策略，显著降低了词错误率（WER）。在LibriSpeech数据集上，最佳级联系统达到了test-clean&#x2F;test-other的2.25%&#x2F;4.94% WER，相对于Whisper-LLaMA基线在test-other分割上实现了12.3%的相对改进。但纯文本LLaDA在不使用音频特征的情况下无法提高准确性，这强调了音频条件嵌入的重要性。此外，还评估了Whisper-LLaDA作为ASR的独立解码器的表现，大多数实验配置实现了比Whisper-LLaMA基线更快的推理速度，尽管识别准确率略有下降。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>基于扩散的大型语言模型LLaDA被用于自动语音识别（ASR）的实证研究中。</li>
<li>LLaDA作为Whisper-LLaMA的外部处理模块，通过特定策略显著降低了词错误率（WER）。</li>
<li>在LibriSpeech数据集上，级联系统的表现优于基线，特别是在test-other分割上实现了相对改进。</li>
<li>纯文本LLaDA在不使用音频特征时无法提高准确性，强调音频条件嵌入的重要性。</li>
<li>Whisper-LLaDA作为独立解码器在ASR中的表现被评估，多数配置实现快速推理，但识别准确率略有下降。</li>
<li>实证研究提供了关于扩散基础大型语言模型在ASR中应用的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6bf0a5840a3c3afdeb483b56919d55e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126182&auth_key=1760126182-0-0-8b53d4603a343f021bd15cd8f569e6fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7bea0571c1b4ce01ff63f098d63dac62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126189&auth_key=1760126189-0-0-fa03c8177fc9f20480b19d66d9424095&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5af576cacd857cc23f3a3b763f36df56~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126197&auth_key=1760126197-0-0-477902b496ad642e75afdc180b93f0e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e05e8329ebf3f8b915e8b152ce16eec2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126204&auth_key=1760126204-0-0-f1cca8d13f361f18410e1bf19131d9a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="I-2-RF-TFCKD-Intra-Inter-Representation-Fusion-with-Time-Frequency-Calibration-Knowledge-Distillation-for-Speech-Enhancement"><a href="#I-2-RF-TFCKD-Intra-Inter-Representation-Fusion-with-Time-Frequency-Calibration-Knowledge-Distillation-for-Speech-Enhancement" class="headerlink" title="I$^2$RF-TFCKD: Intra-Inter Representation Fusion with Time-Frequency   Calibration Knowledge Distillation for Speech Enhancement"></a>I$^2$RF-TFCKD: Intra-Inter Representation Fusion with Time-Frequency   Calibration Knowledge Distillation for Speech Enhancement</h2><p><strong>Authors:Jiaming Cheng, Ruiyu Liang, Ye Ni, Chao Xu, Jing Li, Wei Zhou, Rui Liu, Björn W. Schuller, Xiaoshuai Hao</strong></p>
<p>In this paper, we propose an intra-inter representation fusion knowledge distillation (KD) framework with time-frequency calibration (I$^2$RF-TFCKD) for SE, which achieves distillation through the fusion of multi-layer teacher-student feature flows. Different from previous distillation strategies for SE, the proposed framework fully utilizes the time-frequency differential information of speech while promoting global knowledge flow. Firstly, we construct a collaborative distillation paradigm for intra-set and inter-set correlations. Within a correlated set, multi-layer teacher-student features are pairwise matched for calibrated distillation. Subsequently, we generate representative features from each correlated set through residual fusion to form the fused feature set that enables inter-set knowledge interaction. Secondly, we propose a multi-layer interactive distillation based on dual-stream time-frequency cross-calibration, which calculates the teacher-student similarity calibration weights in the time and frequency domains respectively and performs cross-weighting, thus enabling refined allocation of distillation contributions across different layers according to speech characteristics. The proposed distillation strategy is applied to the dual-path dilated convolutional recurrent network (DPDCRN) that ranked first in the SE track of the L3DAS23 challenge. To evaluate the effectiveness of I$^2$RF-TFCKD, we conduct experiments on both single-channel and multi-channel SE datasets. Objective evaluations demonstrate that the proposed KD strategy consistently and effectively improves the performance of the low-complexity student model and outperforms other distillation schemes. </p>
<blockquote>
<p>本文提出了一种基于时间频率校准的内外表示融合知识蒸馏（KD）框架（称为I$^2$RF-TFCKD），用于SE。该框架通过融合多层教师学生特征流来实现蒸馏。不同于以前的SE蒸馏策略，该框架充分利用语音的时间频率差异信息，同时促进全局知识流。首先，我们为集合内部和集合之间的相关性构建了一种协作蒸馏模式。在相关集合内，多层教师学生特征进行配对校准蒸馏。然后，我们通过残差融合生成每个相关集合的代表特征，形成融合特征集，以实现集合间的知识交互。其次，我们提出了一种基于双流时间频率交叉校准的多层交互蒸馏方法，该方法分别计算教师和学生在时间和频率域中的相似度校准权重，并进行交叉加权，从而能够根据语音特征在不同的层次上精细分配蒸馏贡献。所提出的蒸馏策略应用于双路径扩张卷积循环网络（DPDCRN），在L3DAS23挑战的SE赛道上排名第一。为了评估I$^2$RF-TFCKD的有效性，我们在单通道和多通道SE数据集上进行了实验。客观评估表明，所提出的知识蒸馏策略持续有效地提高了低复杂度学生模型的性能，并优于其他蒸馏方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13127v2">PDF</a> submitted to Information Fusion</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于时间频率校准的跨内外表示融合知识蒸馏（I$^2$RF-TFCKD）框架，用于语音增强（SE）。该框架通过多层教师-学生特征流的融合实现蒸馏。与以往的SE蒸馏策略不同，该框架充分利用语音的时间频率差异信息，并促进全局知识流动。实验表明，该蒸馏策略在单通道和多通道SE数据集上均表现优秀，有效提升了低复杂度学生模型的性能并优于其他蒸馏方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于时间频率校准的跨内外表示融合知识蒸馏（I$^2$RF-TFCKD）框架。</li>
<li>该框架融合多层教师-学生特征流，实现语音增强（SE）中的蒸馏。</li>
<li>充分利用语音的时间频率差异信息，促进全局知识流动。</li>
<li>构建了协同蒸馏范式，实现数据集内部的跨集和内部特征匹配。</li>
<li>通过剩余融合生成代表性特征，实现跨集知识交互。</li>
<li>提出了基于双流时间频率交叉校准的多层交互蒸馏方法，根据语音特性进行不同层的精炼分配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f0caac560be936413123a81980e453e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126211&auth_key=1760126211-0-0-0fb92bcf8460abf685865d07e69b667e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2d6801afb558cd29fbe90509e98ce5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126218&auth_key=1760126218-0-0-86f2f8e89f989cfbb8a287d75183d7dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-584eeaa8eb21d95f748e4113ab0669e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126225&auth_key=1760126225-0-0-2314d7ad98ec429a53137cf7a0b19e5c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f462dfc08ddec06bd3ffdb61fa2f06d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126232&auth_key=1760126232-0-0-511ae7c6ba90fc8d90e1b2efd6c78150&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement"><a href="#SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement" class="headerlink" title="SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement"></a>SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement</h2><p><strong>Authors:Kuan-Yu Chen, Jeng-Lin Li, Jian-Jiun Ding</strong></p>
<p>With the fast development of zero-shot text-to-speech technologies, it is possible to generate high-quality speech signals that are indistinguishable from the real ones. Speech editing, including speech insertion and replacement, appeals to researchers due to its potential applications. However, existing studies only considered clean speech scenarios. In real-world applications, the existence of environmental noise could significantly degrade the quality of generation. In this study, we propose a noise-resilient speech editing framework, SeamlessEdit, for noisy speech editing. SeamlessEdit adopts a frequency-band-aware noise suppression module and an in-content refinement strategy. It can well address the scenario where the frequency bands of voice and background noise are not separated. The proposed SeamlessEdit framework outperforms state-of-the-art approaches in multiple quantitative and qualitative evaluations. </p>
<blockquote>
<p>随着零文本转语音技术的快速发展，生成高质量、难以与真实语音区分的语音信号成为可能。语音编辑，包括语音插入和替换，由于其潜在的应用价值而吸引了研究人员的关注。然而，现有研究仅涉及干净语音场景。在真实世界应用中，环境噪声的存在可能会显著降低生成语音的质量。本研究提出了一种用于噪声语音编辑的鲁棒性语音编辑框架SeamlessEdit。SeamlessEdit采用频带感知噪声抑制模块和内容内优化策略，能够很好地处理语音和背景噪声频率带未分离的场景。所提出的SeamlessEdit框架在多个定量和定性评估中均优于最新技术方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14066v2">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>     随着零样本文本转语音技术的快速发展，生成高质量、难以区分真假的语音信号成为可能。语音编辑，包括语音插入和替换，因其在各种应用场景中的潜力而受到研究人员的关注。然而，现有研究主要集中在干净语音场景。实际应用中，环境噪声的存在会显著影响生成语音的质量。本研究提出一种用于噪声语音编辑的稳健框架SeamlessEdit，采用频带感知噪声抑制模块和内容内优化策略，能够很好地处理语音和背景噪声频带不分离的场景。SeamlessEdit框架在多项定量和定性评估中优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本文本转语音技术快速发展，可生成高质量难以区分真假的语音信号。</li>
<li>语音编辑具有潜在应用价值，包括语音插入和替换。</li>
<li>现有研究主要集中在干净语音场景，忽略环境噪声的影响。</li>
<li>本研究提出一种用于噪声语音编辑的SeamlessEdit框架。</li>
<li>SeamlessEdit采用频带感知噪声抑制模块和内容内优化策略。</li>
<li>SeamlessEdit能处理语音和背景噪声频带不分离的场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14066">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-658b2cb3d588143d2131de1d0968c722~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126239&auth_key=1760126239-0-0-14fb062beb3c1369d942253faa54682f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33686396df5a918b34909ee013ebfbd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126247&auth_key=1760126247-0-0-6562ba9021b4d8dfbe6757bf5118fce0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9ec921c8018ee782a66f35be031746a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126253&auth_key=1760126253-0-0-0b08f50cb82ebdd986d69489f9c6fb4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a16c006506bf688ae62498df841d00c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126260&auth_key=1760126260-0-0-21014cd0bf3edc16bb3225811a639155&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Differentiable-Alignment-Framework-for-Sequence-to-Sequence-Modeling-via-Optimal-Transport"><a href="#A-Differentiable-Alignment-Framework-for-Sequence-to-Sequence-Modeling-via-Optimal-Transport" class="headerlink" title="A Differentiable Alignment Framework for Sequence-to-Sequence Modeling   via Optimal Transport"></a>A Differentiable Alignment Framework for Sequence-to-Sequence Modeling   via Optimal Transport</h2><p><strong>Authors:Yacouba Kaloga, Shashi Kumar, Petr Motlicek, Ina Kodrasi</strong></p>
<p>Accurate sequence-to-sequence (seq2seq) alignment is critical for applications like medical speech analysis and language learning tools relying on automatic speech recognition (ASR). State-of-the-art end-to-end (E2E) ASR systems, such as the Connectionist Temporal Classification (CTC) and transducer-based models, suffer from peaky behavior and alignment inaccuracies. In this paper, we propose a novel differentiable alignment framework based on one-dimensional optimal transport, enabling the model to learn a single alignment and perform ASR in an E2E manner. We introduce a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and discuss its theoretical properties. Based on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with CTC. Experimental results on the TIMIT, AMI, and LibriSpeech datasets show that our method considerably improves alignment performance compared to CTC and the more recently proposed Consistency-Regularized CTC, though with a trade-off in ASR performance. We believe this work opens new avenues for seq2seq alignment research, providing a solid foundation for further exploration and development within the community. </p>
<blockquote>
<p>精确序列到序列（seq2seq）对齐对于依赖自动语音识别（ASR）的医疗语音分析和语言学习工具等应用至关重要。目前最先进的端到端（E2E）ASR系统，如连接时序分类（CTC）和基于转换器模型的，都存在峰值行为和对齐不准确的问题。在本文中，我们提出了一种基于一维最优传输的新型可区分对齐框架，使模型能够以端到端的方式进行单一对齐并执行ASR。我们引入了序列最优传输距离（SOTD）的伪度量，并讨论了其理论属性。基于SOTD，我们提出了用于ASR的最优时序传输分类（OTTC）损失，并将其行为与CTC进行了对比。在TIMIT、AMI和LibriSpeech数据集上的实验结果表明，我们的方法在改进对齐性能方面与CTC和最近提出的一致性正则化CTC相比具有显著优势，尽管ASR性能存在权衡。我们相信这项工作开辟了seq2seq对齐研究的新途径，为社区内的进一步探索和发展提供了坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01588v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于一维最优传输的可微排列框架，用于解决序列到序列（seq2seq）对齐问题，如医疗语音分析和依赖自动语音识别（ASR）的语言学习工具中的对齐问题。传统端到端（E2E）ASR系统如连接定时分类（CTC）和基于转换器模型存在尖峰行为和排列不精确的问题。本文引入了一种序列最优传输距离（SOTD）的伪度量，并基于SOTD提出了最优时间传输分类（OTTC）损失函数用于ASR。实验结果表明，与CTC和最近提出的一致性正则化CTC相比，该方法在TIMIT、AMI和LibriSpeech数据集上的对齐性能有了显著提高，尽管ASR性能存在权衡。本文工作开创了seq2seq对齐研究的新途径，为社区内的进一步探索和发展提供了坚实的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>准确序列到序列对齐对于依赖自动语音识别（ASR）的应用至关重要，如医疗语音分析和语言学习工具。</li>
<li>当前端到端（E2E）ASR系统存在尖峰行为和排列不精确的问题。</li>
<li>引入了一种基于一维最优传输的可微排列框架，解决了seq2seq对齐问题。</li>
<li>提出了序列最优传输距离（SOTD）的伪度量，并基于SOTD构建了Optimal Temporal Transport Classification（OTTC）损失函数用于ASR。</li>
<li>实验结果表明，与CTC和其他方法相比，所提出的方法在对齐性能上有了显著提高。</li>
<li>虽然存在ASR性能的权衡，但这项工作为seq2seq对齐研究提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01588">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-33d36c1148b4d881a6410834f9c5b794~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126268&auth_key=1760126268-0-0-b1310e9239177cec639ff49a6721ceaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ac1ccf8900f9891e738c6dba833d734~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126275&auth_key=1760126275-0-0-d88da910edd6fa41c20d9cea5dd8f010&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2cc7453588744a9063fadccc54c2fb91~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126281&auth_key=1760126281-0-0-3a9421d73018efe92d82f0b1c7918236&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-Investigation-of-Incorporating-Mamba-for-Speech-Enhancement"><a href="#An-Investigation-of-Incorporating-Mamba-for-Speech-Enhancement" class="headerlink" title="An Investigation of Incorporating Mamba for Speech Enhancement"></a>An Investigation of Incorporating Mamba for Speech Enhancement</h2><p><strong>Authors:Rong Chao, Wen-Huang Cheng, Moreno La Quatra, Sabato Marco Siniscalchi, Chao-Han Huck Yang, Szu-Wei Fu, Yu Tsao</strong></p>
<p>This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions. </p>
<blockquote>
<p>本文旨在研究一种新提出的无注意力、可扩展的状态空间模型（SSM），名为Mamba，在语音增强（SE）任务中的应用。特别是，我们采用Mamba部署不同配置的基于回归的SE模型（SEMamba），包括基本、高级、因果和非因果配置。此外，还考虑了基于信号级距离或面向度量的损失函数。实验证据表明，SEMamba在VoiceBank-DEMAND数据集上使用高级非因果配置达到了有竞争力的PESQ 3.55。当SEMamba与感知对比度拉伸（PCS）结合时，报告了新的最高PESQ分数为3.69。与基于转换的等效SE解决方案相比，高级非因果配置下观察到显著的FLOPs减少，最高可达约12%。最后，SEMamba可以用作自动语音识别（ASR）之前的预处理步骤，显示出与最新的SE解决方案相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.06573v2">PDF</a> Accepted to IEEE SLT 2024</p>
<p><strong>Summary</strong><br>     本研究旨在探究一种新提出的无注意力、可扩展的状态空间模型（SSM），即Mamba在语音增强（SE）任务中的应用。研究采用Mamba部署不同配置的回归式SE模型（SEMamba），包括基本、高级、因果和非因果配置。同时考虑了基于信号级别距离或面向度量的损失函数。实验结果表明，SEMamba在VoiceBank-DEMAND数据集上获得有竞争力的PESQ分数3.55，其中高级非因果配置表现最佳。当SEMamba与感知对比度拉伸（PCS）结合时，报告了新的最先进的PESQ分数为3.69。与基于变换的等效SE解决方案相比，高级非因果配置实现了显著的FLOPs减少，最多可达约12%。最后，SEMamba可作为自动语音识别（ASR）之前的预处理步骤，与最新的SE解决方案相比表现有竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究使用了一种新提出的无注意力状态空间模型Mamba在语音增强任务中的应用。</li>
<li>通过Mamba部署了不同配置的回归式SE模型SEMamba，包括基本、高级、因果和非因果模型。</li>
<li>实验表明，SEMamba在VoiceBank-DEMAND数据集上获得有竞争力的PESQ分数。</li>
<li>当SEMamba与感知对比度拉伸（PCS）结合时，达到了新的最先进的PESQ分数。</li>
<li>与基于变换的SE解决方案相比，SEMamba在高级非因果配置下实现了显著的FLOPs减少。</li>
<li>SEMamba可以用作自动语音识别（ASR）的预处理步骤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.06573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fa9b613007c0b05b872d2682745f568c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126288&auth_key=1760126288-0-0-b31d58b6ff965f64bae4c92cb4391808&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-47e226ccfb0d8322d763b85d78aaa7c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126295&auth_key=1760126295-0-0-fdca9df16a2eb1c41197ebd7700d593b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-55242756c3ac145de5b5147ae80775d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126302&auth_key=1760126302-0-0-20df17b884ef0551da3facf36573587c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64eae1596c9742db8d2f0696e096d9e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126308&auth_key=1760126308-0-0-bef06ad740a5bd2698e6261f90050c9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-155af7e91e50e64a03226f74acc737b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760126315&auth_key=1760126315-0-0-a41a307abc57eb48c6301c8ec722a2fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-af916aa2015e25531947898af0f5b2e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760127175&auth_key=1760127175-0-0-b6af08e88a808f882d80ce7a8a125381&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-10-11  Large Scale Diffusion Distillation via Score-Regularized Continuous-Time   Consistency
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-94bc23fa748ef40ba5ce1317c1e646a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760125083&auth_key=1760125083-0-0-546445d909e72f1f3dd2165f13498427&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-10-11  CAST Contrastive Adaptation and Distillation for Semi-Supervised   Instance Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
