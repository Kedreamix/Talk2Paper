<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-11  AI-Driven Radiology Report Generation for Traumatic Brain Injuries">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-2b40a21bee6693aa28fcd120acfe463f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134196&auth_key=1760134196-0-0-2a1ca65148107875e5be856c1e2a00a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    88 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-11-更新"><a href="#2025-10-11-更新" class="headerlink" title="2025-10-11 更新"></a>2025-10-11 更新</h1><h2 id="AI-Driven-Radiology-Report-Generation-for-Traumatic-Brain-Injuries"><a href="#AI-Driven-Radiology-Report-Generation-for-Traumatic-Brain-Injuries" class="headerlink" title="AI-Driven Radiology Report Generation for Traumatic Brain Injuries"></a>AI-Driven Radiology Report Generation for Traumatic Brain Injuries</h2><p><strong>Authors:Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli</strong></p>
<p>Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries. </p>
<blockquote>
<p>创伤性脑损伤在急诊医学中构成了重要的诊断挑战，因为对医疗图像进行及时的解读对患者的治疗效果至关重要。在本文中，我们提出了一种新型的基于人工智能的方法，专为颅脑损伤病例设计自动放射学报告生成。我们的模型结合了AC-BiFPN与Transformer架构来捕获和处理复杂的医疗成像数据，如CT和MRI扫描。AC-BiFPN提取多尺度特征，能够检测颅内出血等复杂异常，而Transformer通过模拟远程依赖关系生成连贯且上下文相关的诊断报告。我们在RSNA颅内出血检测数据集上评估了我们模型的性能，与传统基于CNN的模型相比，它在诊断和报告生成方面的表现更为出色。这一解决方案不仅支持处于高压环境中的放射科医生，而且还为实习医生提供了强大的教育工具，提供实时反馈并增强他们的学习体验。我们的研究结果表明，将高级特征提取与基于Transformer的文本生成相结合，在创伤性脑损伤的诊断中可以提高临床决策制定的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08498v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于AI的自动放射学报告生成方法，用于颅脑外伤病例。该方法结合了AC-BiFPN和Transformer架构，能够处理复杂的医学成像数据，如CT和MRI扫描。AC-BiFPN提取多尺度特征，检测颅内出血等细微异常，而Transformer生成连贯、上下文相关的诊断报告，通过建模远程依赖关系。在RSNA颅内出血检测数据集上评估，该模型在诊断准确性和报告生成方面都优于传统的CNN模型。此解决方案不仅支持放射科医生在高压力环境下工作，还为医学生提供强大的教育工具，提供实时反馈，增强学习体验。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文介绍了在紧急医学中诊断颅脑损伤的挑战性，强调医学图像解读的及时性对患者结果至关重要。</li>
<li>提出了一种基于AI的自动放射学报告生成方法，专门用于颅脑损伤病例。</li>
<li>方法结合了AC-BiFPN和Transformer架构，能有效处理复杂的医学成像数据。</li>
<li>AC-BiFPN能够提取多尺度特征，检测颅内出血等细微异常。</li>
<li>Transformer能够生成连贯、上下文相关的诊断报告，建模远程依赖关系。</li>
<li>在RSNA颅内出血检测数据集上的评估表明，该模型在诊断准确性和报告生成方面优于传统CNN模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08498">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8c85bb3ea46b5d2afb1ea431d18f9d0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133924&auth_key=1760133924-0-0-8a220f53c6347bb9965f9b4b27ce1d71&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a14ab6a7ab551c3de935ac2bced4016~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133931&auth_key=1760133931-0-0-6ba3c7a8aeb8aadc7ca08908e6af04b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Robust-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation-based-on-Curriculum-Learning"><a href="#Robust-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation-based-on-Curriculum-Learning" class="headerlink" title="Robust Source-Free Domain Adaptation for Medical Image Segmentation   based on Curriculum Learning"></a>Robust Source-Free Domain Adaptation for Medical Image Segmentation   based on Curriculum Learning</h2><p><strong>Authors:Ziqi Zhang, Yuexiang Li, Yawen Huang, Nanjun He, Tao Xu, Liwei Lin, Yefeng Zheng, Shaoxin Li, Feiyue Huang</strong></p>
<p>Recent studies have uncovered a new research line, namely source-free domain adaptation, which adapts a model to target domains without using the source data. Such a setting can address the concerns on data privacy and security issues of medical images. However, current source-free domain adaptation frameworks mainly focus on the pseudo label refinement for target data without the consideration of learning procedure. Indeed, a progressive learning process from source to target domain will benefit the knowledge transfer during model adaptation. To this end, we propose a curriculum-based framework, namely learning from curriculum (LFC), for source-free domain adaptation, which consists of easy-to-hard and source-to-target curricula. Concretely, the former curriculum enables the framework to start learning with &#96;easy’ samples and gradually tune the optimization direction of model adaption by increasing the sample difficulty. While, the latter can stablize the adaptation process, which ensures smooth transfer of the model from the source domain to the target. We evaluate the proposed source-free domain adaptation approach on the public cross-domain datasets for fundus segmentation and polyp segmentation. The extensive experimental results show that our framework surpasses the existing approaches and achieves a new state-of-the-art. </p>
<blockquote>
<p>最近的研究揭示了一项新的研究线路，即无源域自适应（Source-Free Domain Adaptation），它能够在不使用源数据的情况下，使模型适应目标域。这种设置可以解决医疗图像数据隐私和安全问题。然而，当前的无源域自适应框架主要关注目标数据的伪标签优化，而没有考虑学习过程。实际上，从源域到目标域逐步学习过程将有助于模型适应过程中的知识转移。为此，我们提出了一种基于课程的学习框架，即无学习课程（Learning from Curriculum, LFC），用于无源域自适应，该框架包括由易到难和源到目标的课程。具体来说，前者课程使框架能够从“容易”的样本开始学习，并通过增加样本难度逐渐调整模型适应的优化方向。而后者则可以稳定适应过程，确保模型从源域到目标域的平稳转移。我们在公共的眼底分割和多息肉分割跨域数据集上评估了所提出的无源域自适应方法。大量的实验结果表明，我们的框架超越了现有方法，达到了新的先进水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08393v1">PDF</a> </p>
<p><strong>Summary</strong><br>     近期研究发现了一种新的研究思路——无源域自适应技术，该技术无需使用源数据即可将模型适应于目标域。该技术解决了医疗图像等数据隐私和安全问题。然而，当前的无源域自适应框架主要关注目标数据的伪标签优化，而忽略了学习过程的重要性。我们从源域到目标域进行渐进学习，有助于知识迁移过程中的模型适应。为此，我们提出了一种基于课程的学习框架——无学习课程（LFC）进行无源域自适应，其中包括从易到难和从源到目标的课程。我们从“简单”样本开始学习，通过增加样本难度逐渐调整模型优化的方向；同时确保模型从源域到目标域的平稳过渡。我们在公共跨域数据集上评估了所提出的无源域自适应方法，用于眼底分割和息肉分割。大量实验结果表明，我们的框架超越了现有方法，达到了新的先进水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一个新的概念——“无源域自适应”，旨在解决医疗图像等数据隐私和安全问题。</li>
<li>当前的无源域自适应框架主要关注目标数据的伪标签优化，但对学习过程考虑不足。</li>
<li>新的基于课程的学习框架——无学习课程（LFC），被提出来实现从源域到目标域的渐进学习。</li>
<li>该框架包括从易到难的课程，帮助模型从简单样本开始学习，逐渐增加样本难度，调整优化方向。</li>
<li>还包括从源到目标的课程，确保模型平稳地从源域过渡到目标域。</li>
<li>在公共跨域数据集上进行评估，该方法的眼底分割和息肉分割性能超越现有方法，达到新的先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d63dc20127aea78f30cfbce51841475f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133938&auth_key=1760133938-0-0-0fcd16f8a11379a3d2a53ba676b9ecec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0962b6b526c234f238a245d808f9cc73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133945&auth_key=1760133945-0-0-b8439a762c7c26e2d8efa97c5b9dcedf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Random-Window-Augmentations-for-Deep-Learning-Robustness-in-CT-and-Liver-Tumor-Segmentation"><a href="#Random-Window-Augmentations-for-Deep-Learning-Robustness-in-CT-and-Liver-Tumor-Segmentation" class="headerlink" title="Random Window Augmentations for Deep Learning Robustness in CT and Liver   Tumor Segmentation"></a>Random Window Augmentations for Deep Learning Robustness in CT and Liver   Tumor Segmentation</h2><p><strong>Authors:Eirik A. Østmo, Kristoffer K. Wickstrøm, Keyur Radiya, Michael C. Kampffmeyer, Karl Øyvind Mikalsen, Robert Jenssen</strong></p>
<p>Contrast-enhanced Computed Tomography (CT) is important for diagnosis and treatment planning for various medical conditions. Deep learning (DL) based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images, thereby reducing clinicians’ workload. Achieving generalization capabilities in limited data domains, such as radiology, requires modern DL models to be trained with image augmentation. However, naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality, where the intensities measure Hounsfield Units (HU) and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this, we propose a CT-specific augmentation technique, called Random windowing, that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrast-enhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets, and compare to, and outperform, state-of-the-art alternatives, while focusing on the challenge of liver tumor segmentation. </p>
<blockquote>
<p>增强计算机断层扫描（CT）对于各种疾病的诊断和治疗计划非常重要。基于深度学习的分割模型可能能够实现医疗图像自动化分析，检测并描绘CT图像中的肿瘤，从而减轻临床医生的工作量。在有限的领域（如放射学）实现模型的泛化能力，需要借助图像增强技术对现代深度学习模型进行训练。然而，将针对自然图像开发的增强方法直接应用于CT扫描时，往往会忽略CT的特性，其中强度以亨氏单位（HU）为单位测量并具有重要的物理意义。本文质疑在CT成像中使用此类强度增强的做法，并表明它们可能导致伪影和泛化性能不佳。为了缓解这一问题，我们提出了一种针对CT的特定增强技术，称为随机窗技术，该技术利用CT图像中可用的HU强度分布。随机窗技术有助于提高对比增强的稳健性，并对对比度较差或时间把控困难的图像显著提高模型性能。我们在多个数据集上进行了方法消融和分析，并与最新替代方案进行了比较，在解决肝脏肿瘤分割的挑战上表现更优。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08116v1">PDF</a> 10 pages, 9 figures. This work has been submitted to the IEEE for   possible publication</p>
<p><strong>Summary</strong></p>
<p>本文探讨了对比增强计算机断层扫描（CT）在医学诊断和治疗计划中的重要性。深度学习（DL）在CT图像中的肿瘤检测和分割方面具有潜力，可减轻医生的工作量。为了在有限的医学图像数据领域实现模型的泛化能力，训练深度模型需要使用图像增强技术。然而，针对自然图像开发的增强方法直接应用于CT扫描时，忽略了CT的特性，其强度单位为霍恩菲尔德单位（HU），具有物理意义。本文质疑了这种强度增强方法在CT影像中的应用，并指出其可能导致伪影和泛化性能不佳。为此，提出了一种针对CT的特定增强技术——随机窗技术，该技术利用CT图像中可用的HU强度分布。随机窗口技术可以提高模型对对比增强的稳健性，并在对比度过低或图像质量较差的情况下显著提高模型性能。我们对方法进行了多个数据集的消融与分析，并超越了其他最先进的方法，尤其针对肝脏肿瘤分割提出了挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比增强计算机断层扫描（CT）在医学诊断和治疗计划中起重要作用。</li>
<li>深度学习在自动化医学图像分析中有潜力，特别是在CT图像的肿瘤检测和分割方面。</li>
<li>在有限的医学图像数据领域实现模型泛化需要训练深度模型并使用图像增强技术。</li>
<li>应用于CT扫描的增强方法需要考虑到CT的特性，尤其是其强度单位为霍恩菲尔德单位（HU）。</li>
<li>通用强度增强方法可能导致CT影像中的伪影和模型泛化性能不佳。</li>
<li>提出了针对CT的特定增强技术——随机窗技术，该技术利用CT图像中的HU分布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08116">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0775995e49529faacd13c7e55fdeb8cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133953&auth_key=1760133953-0-0-c70b8091452a30f57add18d8b9a95081&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cbc92b73cc3b613128fddf8b2dbabf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133960&auth_key=1760133960-0-0-d1fcbfd02d82390b2da8b2baa47888e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10b9ae08170fa32c47be350848fe43ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133967&auth_key=1760133967-0-0-4f04c8007361e981e07049c13ddae7df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42a2ebf95ed1adf57480507385892aab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133973&auth_key=1760133973-0-0-ec843ac2e7e6d2ae81088e19f55372da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7abbd746953f3fc81ab5141528eeb44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133980&auth_key=1760133980-0-0-b3bb43934674fa71acd52ad31fbec8e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RASALoRE-Region-Aware-Spatial-Attention-with-Location-based-Random-Embeddings-for-Weakly-Supervised-Anomaly-Detection-in-Brain-MRI-Scans"><a href="#RASALoRE-Region-Aware-Spatial-Attention-with-Location-based-Random-Embeddings-for-Weakly-Supervised-Anomaly-Detection-in-Brain-MRI-Scans" class="headerlink" title="RASALoRE: Region Aware Spatial Attention with Location-based Random   Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans"></a>RASALoRE: Region Aware Spatial Attention with Location-based Random   Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans</h2><p><strong>Authors:Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan</strong></p>
<p>Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/">https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/</a>. </p>
<blockquote>
<p>在大脑MRI扫描中进行弱监督异常检测（WSAD）是一项重要挑战，在没有精确的像素级异常标注且只有弱标签（如切片级）可用的情况下，能迅速准确地检测大脑异常。在这项工作中，我们提出了RASALoRE：基于位置感知空间注意力的位置随机嵌入（Region Aware Spatial Attention with Location-based Random Embeddings），这是一种新颖的两阶段WSAD框架。在第一阶段，我们引入了一种判别式双重提示调整（DDPT）机制，它基于切片级标签生成高质量伪弱掩模，作为粗略定位线索。在第二阶段，我们提出了一种带有区域感知空间注意力机制的分割网络，该网络依赖于基于固定位置的随机嵌入。这种设计使模型能够有效地关注异常区域。我们的方法达到了最先进的异常检测性能，显著优于现有的WSAD方法，同时使用的参数少于8百万。在BraTS20、BraTS21、BraTS23和MSD数据集上的广泛评估证明了其性能显著提高以及计算复杂度的显著降低。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/">https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08052v1">PDF</a> Accepted in BMVC-2025</p>
<p><strong>Summary</strong></p>
<p>本摘要针对大脑MRI扫描的弱监督异常检测（WSAD）问题，提出了一种名为RASALoRE的新型两阶段WSAD框架。第一阶段通过判别式双提示调谐（DDPT）机制生成高质量伪弱掩膜，作为粗略定位线索。第二阶段采用具有基于固定位置随机嵌入的区域感知空间注意力机制的分割网络，实现对异常区域的有效关注。该方法实现了先进的异常检测性能，显著优于现有WSAD方法，同时参数使用量少于八百万。在BraTS20、BraTS21、BraTS23和MSD数据集上的广泛评估表明，该方法在性能上取得了实质性改进，同时大大降低了计算复杂度。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是该文本的主要见解：</p>
<ul>
<li>针对大脑MRI扫描的弱监督异常检测（WSAD）问题提出了一种新型框架RASALoRE。</li>
<li>该框架包含两个阶段：第一阶段生成伪弱掩膜作为粗略定位线索，第二阶段采用具有区域感知空间注意力机制的分割网络。</li>
<li>利用判别式双提示调谐（DDPT）机制生成高质量伪弱掩膜。</li>
<li>基于固定位置随机嵌入的区域感知空间注意力机制有助于模型有效关注异常区域。</li>
<li>该方法实现了先进的异常检测性能，显著优于现有WSAD方法。</li>
<li>该方法参数使用量少于八百万，同时在多个数据集上实现了性能提升和计算复杂度的降低。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f1d7735c805c2792d62bb95a64f6bcf0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133987&auth_key=1760133987-0-0-bd721a48fba861b5fbec52ab9474ba42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-494f4560263722f91be8845e598b5f17~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133994&auth_key=1760133994-0-0-e7800385c70e398442e80be30c5dd8d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-70580eeeeb83e25ffe7453cf85f90c45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134001&auth_key=1760134001-0-0-30f07fe3e5ead5648d4801c57b5cac05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43ce9c72a8fd0c328c4f42e565867344~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134008&auth_key=1760134008-0-0-7ad16a0c8a3d7f3df0cae441e6fd5f32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MRI-derived-quantification-of-hepatic-vessel-to-volume-ratios-in-chronic-liver-disease-using-a-deep-learning-approach"><a href="#MRI-derived-quantification-of-hepatic-vessel-to-volume-ratios-in-chronic-liver-disease-using-a-deep-learning-approach" class="headerlink" title="MRI-derived quantification of hepatic vessel-to-volume ratios in chronic   liver disease using a deep learning approach"></a>MRI-derived quantification of hepatic vessel-to-volume ratios in chronic   liver disease using a deep learning approach</h2><p><strong>Authors:Alexander Herold, Daniel Sobotka, Lucian Beer, Nina Bastati, Sarah Poetter-Lang, Michael Weber, Thomas Reiberger, Mattias Mandorfer, Georg Semmler, Benedikt Simbrunner, Barbara D. Wichtmann, Sami A. Ba-Ssalamah, Michael Trauner, Ahmed Ba-Ssalamah, Georg Langs</strong></p>
<p>Background: We aimed to quantify hepatic vessel volumes across chronic liver disease stages and healthy controls using deep learning-based magnetic resonance imaging (MRI) analysis, and assess correlations with biomarkers for liver (dys)function and fibrosis&#x2F;portal hypertension.   Methods: We assessed retrospectively healthy controls, non-advanced and advanced chronic liver disease (ACLD) patients using a 3D U-Net model for hepatic vessel segmentation on portal venous phase gadoxetic acid-enhanced 3-T MRI. Total (TVVR), hepatic (HVVR), and intrahepatic portal vein-to-volume ratios (PVVR) were compared between groups and correlated with: albumin-bilirubin (ALBI) and model for end-stage liver disease-sodium (MELD-Na) score, and fibrosis&#x2F;portal hypertension (Fibrosis-4 [FIB-4] score, liver stiffness measurement [LSM], hepatic venous pressure gradient [HVPG], platelet count [PLT], and spleen volume).   Results: We included 197 subjects, aged 54.9 $\pm$ 13.8 years (mean $\pm$ standard deviation), 111 males (56.3%): 35 healthy controls, 44 non-ACLD, and 118 ACLD patients. TVVR and HVVR were highest in controls (3.9; 2.1), intermediate in non-ACLD (2.8; 1.7), and lowest in ACLD patients (2.3; 1.0) ($p \leq 0.001$). PVVR was reduced in both non-ACLD and ACLD patients (both 1.2) compared to controls (1.7) ($p \leq 0.001$), but showed no difference between CLD groups ($p &#x3D; 0.999$). HVVR significantly correlated indirectly with FIB-4, ALBI, MELD-Na, LSM, and spleen volume ($\rho$ ranging from -0.27 to -0.40), and directly with PLT ($\rho &#x3D; 0.36$). TVVR and PVVR showed similar but weaker correlations.   Conclusions: Deep learning-based hepatic vessel volumetry demonstrated differences between healthy liver and chronic liver disease stages and shows correlations with established markers of disease severity. </p>
<blockquote>
<p>背景：本研究旨在利用基于深度学习的磁共振成像（MRI）分析来量化不同慢性肝病阶段和正常对照者的肝脏血管体积，并评估其与肝脏（异常）功能及纤维化&#x2F;门静脉高压的生物标志物之间的相关性。</p>
</blockquote>
<p>方法：我们回顾性地评估了正常对照组、非进展期慢性肝病患者和进展期慢性肝病患者，在静脉期的门脉期钆双胺酸增强的3T MRI上采用三维U-Net模型进行肝脏血管分割。组间比较总血管体积比（TVVR）、肝脏血管体积比（HVVR）和肝内门静脉与体积之比（PVVR），并与以下指标进行比较：白蛋白胆红素（ALBI）和末期肝病患者模型钠评分（MELD-Na）、纤维化指标&#x2F;门静脉高压（Fibrosis-4指数（FIB-4）、肝硬度测定值（LSM）、肝静脉压力梯度（HVPG）、血小板计数（PLT）和脾脏体积）。</p>
<p>结果：共纳入研究对象197人，平均年龄为±标准偏差的54.9岁，男性占56.3%，包括正常对照组35人，非ACLD患者44人，ACLD患者118人。TVVR和HVVR在对照组中最高（分别为3.9和2.1），在非ACLD患者中处于中等水平（分别为2.8和1.7），而在ACLD患者中最低（分别为2.3和1.0）（p≤0.001）。PVVR在非ACLD和ACLD患者中均低于对照组（分别为1.2和1.7）（p≤0.001），但两组间无显著差异（p&#x3D;0.999）。HVVR与FIB-4、ALBI、MELD-Na、LSM和脾脏体积的相关性间接且显著，其相关性强度从ρ&#x3D;-0.27到ρ&#x3D;-0.4变化不等；而与PLT呈直接显著相关性，其ρ为0.36。TVVR和PVVR显示了相似但较弱的关联性。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08039v1">PDF</a> ^Alexander Herold and Daniel Sobotka share first-authorship</p>
<p><strong>Summary</strong></p>
<p>基于深度学习技术的肝脏血管体积测量显示，健康人群与不同阶段的慢性肝病患者之间存在差异，并与疾病严重程度的标志存在相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究旨在利用深度学习技术进行肝脏血管体积的量化分析，评估慢性肝病阶段与健康对照者的差异。</li>
<li>采用3D U-Net模型对肝脏血管进行分割，利用磁共振成像（MRI）数据进行分析。</li>
<li>总血管体积比（TVVR）、肝脏血管体积比（HVVR）和肝内门静脉体积比（PVVR）在不同组间存在差异。</li>
<li>HVVR与肝纤维化、肝功能指标存在显著负相关，与血小板计数存在正相关。</li>
<li>TVVR和PVVR与疾病严重程度指标的相关性相对较弱。</li>
<li>深度学习技术在肝脏血管体积测量方面的应用有助于理解慢性肝病的病理生理机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08039">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2f0c5409766c4aa7958e7024cb1c1159~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134015&auth_key=1760134015-0-0-e054b5ea467b41c3c12721b6594ebb98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-966bbe9a5e57f1aac5c82353018bd67c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134022&auth_key=1760134022-0-0-714c85c6f2a7f4b6c77dbb740447878a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-efca1e3488b682b0c00e249b10828e6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134028&auth_key=1760134028-0-0-020b6255a4a3ec64aa98d44f5f219943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc9c1668621fc2c168fee732ec3292f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134035&auth_key=1760134035-0-0-86975cc8c85d7f37b7859e2b17a2bf34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Demystifying-Deep-Learning-based-Brain-Tumor-Segmentation-with-3D-UNets-and-Explainable-AI-XAI-A-Comparative-Analysis"><a href="#Demystifying-Deep-Learning-based-Brain-Tumor-Segmentation-with-3D-UNets-and-Explainable-AI-XAI-A-Comparative-Analysis" class="headerlink" title="Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets   and Explainable AI (XAI): A Comparative Analysis"></a>Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets   and Explainable AI (XAI): A Comparative Analysis</h2><p><strong>Authors:Ming Jie Ong, Sze Yinn Ung, Sim Kuan Goh, Jimmy Y. Zhong</strong></p>
<p>The current study investigated the use of Explainable Artificial Intelligence (XAI) to improve the accuracy of brain tumor segmentation in MRI images, with the goal of assisting physicians in clinical decision-making. The study focused on applying UNet models for brain tumor segmentation and using the XAI techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization to enhance the understanding of these models. Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet (AttUNet) - were evaluated to identify the best-performing model. XAI was employed with the aims of clarifying model decisions and increasing physicians’ trust in these models. We compared the performance of two UNet variants (ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM and attention-based visualization. Using the latest computer hardware, we trained and validated each model using the Adam optimizer and assessed their performance with respect to: (i) training, validation, and inference times, (ii) segmentation similarity coefficients and loss functions, and (iii) classification performance. Notably, during the final testing phase, ResUNet outperformed the other models with respect to Dice and Jaccard similarity scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided visuospatial insights into the tumor subregions each UNet model focused on while attention-based visualization provided valuable insights into the working mechanisms of AttUNet’s attention modules. These results demonstrated ResUNet as the best-performing model and we conclude by recommending its use for automated brain tumor segmentation in future clinical assessments. Our source code and checkpoint are available at <a target="_blank" rel="noopener" href="https://github.com/ethanong98/MultiModel-XAI-Brats2020">https://github.com/ethanong98/MultiModel-XAI-Brats2020</a> </p>
<blockquote>
<p>当前研究旨在利用可解释人工智能（XAI）提高MRI图像中脑肿瘤分割的准确性，从而帮助医生进行临床决策。该研究重点是将UNet模型应用于脑肿瘤分割，并使用梯度加权类激活映射（Grad-CAM）和基于注意力的可视化等XAI技术，以提高对这些模型的理解。我们评估了三种深度学习模型——UNet、残差UNet（ResUNet）和注意力UNet（AttUNet）——以确定表现最佳的模型。使用XAI的目的是阐明模型决策，增加医生对这些模型的信任。我们比较了两种UNet变体（ResUNet和AttUNet）与传统UNet在BraTS2020公共数据集上分割脑肿瘤的性能，并使用Grad-CAM和基于注意力的可视化分析模型预测。我们使用最新的计算机硬件，使用Adam优化器训练和验证每个模型，并就其以下方面评估其性能：（i）训练、验证和推理时间；（ii）分割相似度系数和损失函数；（iii）分类性能。值得注意的是，在最终测试阶段，ResUNet在Dice和Jaccard相似度得分以及准确性、召回率和F1得分方面表现出比其他模型更优秀的性能。Grad-CAM提供了关于每个UNet模型关注的肿瘤亚区的视觉空间见解，而基于注意力的可视化则提供了有关AttUNet注意力模块工作机制的宝贵见解。这些结果证明了ResUNet是表现最佳的模型，因此我们建议未来在临床评估中使用ResUNet进行自动脑肿瘤分割。我们的源代码和检查点可在<a target="_blank" rel="noopener" href="https://github.com/ethanong98/MultiModel-XAI-Brats2020%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ethanong98/MultiModel-XAI-Brats2020找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07785v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究运用可解释的的人工智能（XAI）技术来提升MRI图像中脑肿瘤分割的准确性，旨在帮助医生进行临床决策。研究聚焦于使用UNet模型进行脑肿瘤分割，并结合Grad-CAM和注意力可视化等XAI技术，以增强对模型的理解。对比了UNet、ResUNet和AttUNet三种深度学习模型的性能，XAI的引入旨在澄清模型决策，增加医生对模型的信任。最终测试阶段，ResUNet在Dice和Jaccard相似度得分、准确率、召回率和F1分数上表现最佳。</p>
<p><strong>Key Takeaways</strong><br>     1. 本研究使用Explainable Artificial Intelligence (XAI)以提高MRI图像中脑肿瘤分割的准确性。<br>     2. 研究聚焦于应用UNet模型进行脑肿瘤分割，并使用Grad-CAM和注意力可视化技术以增强对模型的理解。<br>     3. 对比了三种深度学习模型（UNet、ResUNet和AttUNet）的性能。<br>     4. XAI的引入旨在帮助医生更好地理解模型决策，并增加他们对模型的信任。<br>     5. 最终测试表明，ResUNet在各项评估指标上表现最佳。<br>     6. Grad-CAM提供了关于肿瘤子区域的信息，这些区域是每个UNet模型所关注的重点。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e1348b6b5d10550a8b11d95e035f2bc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134043&auth_key=1760134043-0-0-b0005fc88fb5d93ebff53aadb5494b5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ada5b7b77bf8b8621f3d7b45f3b6dbf4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134050&auth_key=1760134050-0-0-aeba971fc83ca00aacb2477772f815eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51024248f33c147baf9cbb89a4b2c605~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134057&auth_key=1760134057-0-0-301bef9e3dd29ff810036083987b3a05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TCIP-Threshold-Controlled-Iterative-Pyramid-Network-for-Deformable-Medical-Image-Registration"><a href="#TCIP-Threshold-Controlled-Iterative-Pyramid-Network-for-Deformable-Medical-Image-Registration" class="headerlink" title="TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable   Medical Image Registration"></a>TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable   Medical Image Registration</h2><p><strong>Authors:Heming Wu, Di Wang, Tai Ma, Peng Zhao, Yubin Xiao, Zhongke Wu, Xing-Ce Wang, Chuang Li, Xuan Wu, You Zhou</strong></p>
<p>Although pyramid networks have demonstrated superior performance in deformable medical image registration, their decoder architectures are inherently prone to propagating and accumulating anatomical structure misalignments. Moreover, most existing models do not adaptively determine the number of iterations for optimization under varying deformation requirements across images, resulting in either premature termination or excessive iterations that degrades registration accuracy. To effectively mitigate the accumulation of anatomical misalignments, we propose the Feature-Enhanced Residual Module (FERM) as the core component of each decoding layer in the pyramid network. FERM comprises three sequential blocks that extract anatomical semantic features, learn to suppress irrelevant features, and estimate the final deformation field, respectively. To adaptively determine the number of iterations for varying images, we propose the dual-stage Threshold-Controlled Iterative (TCI) strategy. In the first stage, TCI assesses registration stability and with asserted stability, it continues with the second stage to evaluate convergence. We coin the model that integrates FERM and TCI as Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP outperforms the state-of-the-art (SOTA) registration networks in terms of accuracy, while maintaining comparable inference speed and a compact model parameter size. Finally, we assess the generalizability of FERM and TCI by integrating them with existing registration networks and further conduct ablation studies to validate the effectiveness of these two proposed methods. </p>
<blockquote>
<p>尽管金字塔网络在可变形医学图像注册中表现出了卓越的性能，但其解码器架构固有的传播和累积解剖结构错位的问题。此外，大多数现有模型不能根据图像之间不同的变形要求自适应地确定优化迭代次数，导致过早终止或过度迭代，从而降低了注册准确性。为了有效缓解解剖结构错位的累积，我们提出了特征增强残差模块（FERM）作为金字塔网络中每个解码层的核心组件。FERM包含三个连续块，分别提取解剖语义特征、学习抑制无关特征和估计最终变形场。为了自适应地确定不同图像的迭代次数，我们提出了双阶段阈值控制迭代（TCI）策略。在第一阶段，TCI评估注册稳定性，并在稳定性得到确认后，进入第二阶段评估收敛性。我们将集成了FERM和TCI的模型称为阈值控制金字塔（TCIP）。在三个公共脑MRI数据集和一个腹部CT数据集上的广泛实验表明，TCIP在准确性方面超越了最先进的注册网络，同时保持了相当快的推理速度和紧凑的模型参数大小。最后，我们通过将FERM和TCI与现有注册网络集成来评估它们的通用性，并进行了剔除研究以验证这两种方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07666v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种用于医学图像注册的改进方法，包括用于金字塔网络解码层的特征增强残差模块（FERM）和双阶段阈值控制迭代（TCI）策略。FERM用于减轻解剖结构错位累积的问题，而TCI策略则能自适应确定不同图像优化所需的迭代次数。新方法在多个公开数据集上的实验结果表明，其在保证推理速度和模型参数大小的同时，提高了注册网络的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金字塔网络在可变形医学图像注册中表现出卓越性能，但其解码架构容易传播和累积解剖结构错位。</li>
<li>现有模型大多不能自适应确定不同图像优化所需的迭代次数，导致过早终止或过度迭代，影响注册准确性。</li>
<li>提出特征增强残差模块（FERM），作为金字塔网络每个解码层的核心组件，用于减轻解剖结构错位累积的问题。</li>
<li>FERM包含三个顺序块，分别提取解剖语义特征、学习抑制无关特征和估计最终变形场。</li>
<li>提出双阶段阈值控制迭代（TCI）策略，自适应确定不同图像的迭代次数。</li>
<li>TCI策略包括两个阶段：第一阶段评估注册稳定性，第二阶段评估收敛性。</li>
<li>实验结果表明，结合FERM和TCI的阈值控制迭代金字塔（TCIP）模型在准确性方面优于现有最先进的注册网络，同时保持相当的推理速度和模型参数大小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07666">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-df7cd6795c572ebbb3b23b6185cc85a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134065&auth_key=1760134065-0-0-14912bc25eed288df1a5a23bafc54f79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebae503596b9b87bda24b26427243ffc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134072&auth_key=1760134072-0-0-57ae1526170e74e480216a6c7d1ae3f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c19820090006f7b574536d8004d09f94~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134079&auth_key=1760134079-0-0-2c6d4d74f5d279858ceb0118b2f0d50f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37dd05c7c8794fba032bbc94ff0554fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134086&auth_key=1760134086-0-0-f25593a8918ecb89c3470702c28281a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e36cbe10e52583545005ce43a533389b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134093&auth_key=1760134093-0-0-93cc37e88be32a22ac86493fade420c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c4868ac3886b207fb8e5139cd8a0548f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134100&auth_key=1760134100-0-0-0c6dfd69659944af216cf8c4b4026576&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Denoising-Framework-for-Real-World-Ultra-Low-Dose-Lung-CT-Images-Based-on-an-Image-Purification-Strategy"><a href="#A-Denoising-Framework-for-Real-World-Ultra-Low-Dose-Lung-CT-Images-Based-on-an-Image-Purification-Strategy" class="headerlink" title="A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based   on an Image Purification Strategy"></a>A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based   on an Image Purification Strategy</h2><p><strong>Authors:Guoliang Gong, Man Yu</strong></p>
<p>Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/MonkeyDadLufy/flow-matching">https://github.com/MonkeyDadLufy/flow-matching</a>. </p>
<blockquote>
<p>超低剂量CT（uLDCT）显著降低了辐射暴露，但引入了严重的噪声和伪影。此外，它还导致uLDCT与常规剂量CT（NDCT）图像对之间出现较大的空间错位。这给直接应用现有合成噪声或对齐数据训练的降噪网络带来了挑战。针对uLDCT降噪中的这一核心挑战，本文提出了基于图像净化（IP）策略的创新降噪框架。首先，我们构建了真实的临床uLDCT肺部数据集。然后，我们提出了一种图像净化策略，生成结构对齐的uLDCT-NDCT图像对，为网络训练提供了高质量的数据基础。在此基础上，我们提出了频域流匹配（FFM）模型，该模型与IP策略协同工作，出色地保留了去噪图像的解剖结构完整性。在真实临床数据集上的实验表明，我们的IP策略显著提高了多个主流降噪模型在uLDCT任务上的性能。值得注意的是，我们提出的FFM模型与IP策略相结合，在解剖结构保留方面达到了最新水平（SOTA）。本研究为解决真实世界uLDCT降噪中的数据不匹配问题提供了有效解决方案。代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/MonkeyDadLufy/flow-matching%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MonkeyDadLufy/flow-matching获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07492v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于图像净化策略的降噪框架，以解决超低剂量CT（uLDCT）图像中的噪声和伪影问题。通过构建真实临床uLDCT肺部数据集和采用图像净化策略生成结构对齐的uLDCT-NDCT图像对，为网络训练提供高质量数据基础。在此基础上，提出频域流匹配模型，与图像净化策略协同工作，优秀地保留了解噪图像的解剖结构完整性。实验证明，该策略显著提高主流降噪模型在uLDCT任务上的性能，尤其是频域流匹配模型结合图像净化策略在解剖结构保留方面达到最佳效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>uLDCT显著减少辐射暴露，但引入严重噪声和伪影。</li>
<li>uLDCT与NDCT图像对之间存在空间不对准，给现有降噪网络的直接应用带来挑战。</li>
<li>提出基于图像净化策略的降噪框架，构建真实临床uLDCT肺部数据集。</li>
<li>引入频域流匹配模型，与图像净化策略协同，优秀地保留解噪图像的解剖结构完整性。</li>
<li>实验证明，图像净化策略显著提高主流降噪模型在uLDCT任务上的性能。</li>
<li>结合图像净化策略和频域流匹配模型达到最佳效果，在解剖结构保留方面为行业树立了新标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-30205cbcb17f9a23bf678997f9f6f4a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134107&auth_key=1760134107-0-0-17011393a6da659f8aad19ca97d5d3c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-031afa7e22350fb7e098885b936f8a14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134114&auth_key=1760134114-0-0-964647de99fece5e6835cda321a23a2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35cb7e17076619c8f510270b7c757211~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134121&auth_key=1760134121-0-0-dd3ead305fe5e3a2cc3e450801e687ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e682f9b498a8dc4418e83f5c2b2aa71c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134128&auth_key=1760134128-0-0-c466f2f716373dfd0d0a99877467a818&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f9257e51d861c95c777b8ea5d1405c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134134&auth_key=1760134134-0-0-9fa3a19c026b445fd02c1bab6fe13028&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a09a249e5de3de82e8096ed77062a4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134141&auth_key=1760134141-0-0-ed5204513234b7f183b820b1778d152a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5a580609045785a9bedfae3fb5edf5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134148&auth_key=1760134148-0-0-a82fb75dc9ceb1fa0e53ad7eabe6f075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-We-Won-BraTS-SSA-2025-Brain-Tumor-Segmentation-in-the-Sub-Saharan-African-Population-Using-Segmentation-Aware-Data-Augmentation-and-Model-Ensembling"><a href="#How-We-Won-BraTS-SSA-2025-Brain-Tumor-Segmentation-in-the-Sub-Saharan-African-Population-Using-Segmentation-Aware-Data-Augmentation-and-Model-Ensembling" class="headerlink" title="How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan   African Population Using Segmentation-Aware Data Augmentation and Model   Ensembling"></a>How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan   African Population Using Segmentation-Aware Data Augmentation and Model   Ensembling</h2><p><strong>Authors:Claudia Takyi Ankomah, Livingstone Eli Ayivor, Ireneaus Nyame, Leslie Wambo, Patrick Yeboah Bonsu, Aondona Moses Iorumbur, Raymond Confidence, Toufiq Musah</strong></p>
<p>Brain tumors, particularly gliomas, pose significant chall-enges due to their complex growth patterns, infiltrative nature, and the variability in brain structure across individuals, which makes accurate diagnosis and monitoring difficult. Deep learning models have been developed to accurately delineate these tumors. However, most of these models were trained on relatively homogenous high-resource datasets, limiting their robustness when deployed in underserved regions. In this study, we performed segmentation-aware offline data augmentation on the BraTS-Africa dataset to increase the data sample size and diversity to enhance generalization. We further constructed an ensemble of three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to leverage their complementary strengths. Our best-performing model, MedNeXt, was trained on 1000 epochs and achieved the highest average lesion-wise dice and normalized surface distance scores of 0.86 and 0.81 respectively. However, the ensemble model trained for 500 epochs produced the most balanced segmentation performance across the tumour subregions. This work demonstrates that a combination of advanced augmentation and model ensembling can improve segmentation accuracy and robustness on diverse and underrepresented datasets. Code available at: <a target="_blank" rel="noopener" href="https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti">https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti</a> </p>
<blockquote>
<p>脑肿瘤，特别是胶质瘤，由于其复杂的生长模式、浸润性以及脑结构在个体间的差异，给准确诊断和治疗带来了巨大挑战。深度学习模型已被开发出来精准地勾画这些肿瘤。然而，大多数模型是在相对均匀的高资源数据集上进行训练的，这在部署到服务不足的区域内时，限制了其稳健性。在这项研究中，我们对BraTS-Africa数据集进行了分割感知的离线数据增强，以增加数据样本量和多样性，从而提高模型的泛化能力。我们进一步构建了三种不同架构的集合模型，包括MedNeXt、SegMamba和Residual-Encoder U-Net，以利用它们的互补优势。表现最佳的模型MedNeXt经过1000个周期的训练，获得了最高的平均病变级别的Dice系数和归一化表面距离得分，分别为0.86和0.81。然而，经过500个周期训练的集成模型在肿瘤亚区的分割性能上表现最为均衡。这项工作表明，先进的增强技术与模型集成相结合，可以提高多样性和代表性不足的数据集的分割精度和稳健性。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti">https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03568v2">PDF</a> Brain Tumor Segmentation Challenge, International Medical Image   Computing and Computer Assisted Intervention (MICCAI) Conference, 11 Pages, 2   Figures, 2 Tables</p>
<p><strong>Summary</strong><br>     本研究针对非洲数据集BraTS-Africa进行线下数据增强，增加样本数量和多样性以提高模型泛化能力。结合三种不同架构的模型，实现肿瘤分割的精准诊断。其中MedNeXt模型表现最佳，平均病变级别的Dice系数和归一化表面距离得分分别为0.86和0.81。同时，通过模型集成提高了肿瘤各子区域的分割性能。研究证明了高级数据增强和模型集成的组合能提高分割精度和在多样性和代表性不足的数据集上的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脑肿瘤的复杂生长模式、浸润性和个体差异使得准确诊断和治疗具有挑战。</li>
<li>Deep learning模型已被开发用于精确界定这些肿瘤。</li>
<li>研究使用BraTS-Africa数据集进行线下数据增强，以提高样本多样性和模型泛化能力。</li>
<li>结合三种不同架构的模型，包括MedNeXt、SegMamba和Residual-Encoder U-Net，以提高分割准确性。</li>
<li>MedNeXt模型在训练了1000个周期后表现最佳，达到较高的Dice系数和归一化表面距离得分。</li>
<li>通过模型集成，提高了肿瘤各子区域的分割性能的最佳平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03568">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e9bb58f171e595e3e0b244a4100ab467~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134155&auth_key=1760134155-0-0-65aee3709542e28e9a5b5f26025c1a37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6d6c24fe0734fb5bf2d5b163a639868~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134162&auth_key=1760134162-0-0-2bd72478655e6ff59772273ba8900ccb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a549fbdcb3d06a028aca374ff8655a2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134168&auth_key=1760134168-0-0-729f5eb8145a4e5c6de865a548a190ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ProtoMedX-Towards-Explainable-Multi-Modal-Prototype-Learning-for-Bone-Health-Classification"><a href="#ProtoMedX-Towards-Explainable-Multi-Modal-Prototype-Learning-for-Bone-Health-Classification" class="headerlink" title="ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone   Health Classification"></a>ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone   Health Classification</h2><p><strong>Authors:Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns</strong></p>
<p>Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA&#x2F;X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal (multimodal) model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX’s prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods. </p>
<blockquote>
<p>骨骼健康研究在医学实践中对于骨量减少和骨质疏松症的早期检测和治疗至关重要。临床医生通常基于骨密度测定（DEXA扫描）和患者病史进行诊断。人工智能在该领域的应用仍在研究中。大多数成功的方法依赖于使用视觉的深度学习模型（DEXA&#x2F;X射线影像），侧重于预测准确性，而可解释性通常被忽视，并在事后对输入贡献进行评估。我们提出了ProtoMedX，一个使用DEXA扫描腰椎图像和患者记录的多模态（多模态）模型。ProtoMedX基于原型的设计是可解释的，这在医疗应用中至关重要，特别是在即将到来的欧盟人工智能法案的背景下，因为它允许对模型决策进行明确分析，包括错误的决策。ProtoMedX在骨骼健康分类方面表现出卓越的性能，同时提供了临床医生可以理解的解释。在使用包含4160名真实NHS患者的数据集进行的测试中，所提出的ProtoMedX在仅使用视觉任务的准确性达到87.58%，多模态变体的准确性为89.8%，均超过了现有已发布的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14830v2">PDF</a> ICCV 2025 (PHAROS-AFE-AIMI: Adaptation, Fairness, and Explainability   in Medical Imaging). 8 pages, 5 figures, 4 tables. Keywords: multi-modal,   multimodal, prototype learning, explainable AI, interpretable models,   case-based reasoning, medical imaging, DEXA, bone health, osteoporosis,   osteopenia, diagnosis, classification, clustering</p>
<p><strong>Summary</strong><br>医学实践中的骨骼健康研究对于早期发现和治疗骨质疏松及骨质减少至关重要。研究人员正在研究人工智能在该领域的应用，目前最成功的方法主要依赖于使用深度学习的视觉模型（DEXA或X射线影像），并关注预测的准确性。本研究提出了ProtoMedX多模态模型，结合了DEXA扫描的腰椎影像与患者记录。其原型设计具有可解释性，对于医疗应用至关重要，特别是在即将到来的欧盟人工智能法案背景下，该模型允许对决策进行明确分析，包括错误的决策。ProtoMedX在骨骼健康分类方面表现出卓越的性能，同时提供了临床医生可以理解的解释。在包含4,160名真实NHS患者的数据集上，ProtoMedX在仅使用视觉任务的准确率为87.58%，多模态版本的准确率为89.8%，均超过了已发布的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学实践中的骨骼健康研究有助于早期发现和治疗骨质疏松及骨质减少。</li>
<li>目前AI在骨骼健康领域的研究主要依赖于深度学习的视觉模型。</li>
<li>ProtoMedX模型结合了DEXA扫描的腰椎影像与患者记录，具有多模态特性。</li>
<li>ProtoMedX的原型设计具有可解释性，这对医疗应用至关重要。</li>
<li>欧盟人工智能法案强调模型决策的可解释性。</li>
<li>ProtoMedX在骨骼健康分类方面表现出卓越性能，准确率超过现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14830">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0a36cdd7934e799a9eee82c28192a374~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134176&auth_key=1760134176-0-0-e4708e50de33ce948f1d009a465a5b3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6760c2f8b0e2ce296f28264aa907cce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134183&auth_key=1760134183-0-0-a366eb5c3a258834a995035ee58b4b3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4f1775342f30bef6107f77e19d11481~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134189&auth_key=1760134189-0-0-50c5db754d178f94cbe6115f67dcf608&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b40a21bee6693aa28fcd120acfe463f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134196&auth_key=1760134196-0-0-2a1ca65148107875e5be856c1e2a00a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aab0a14c2ff2fd4a67c516253c7a239d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134203&auth_key=1760134203-0-0-00f139bad342c77458a1776ac4ce7334&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FireGNN-Neuro-Symbolic-Graph-Neural-Networks-with-Trainable-Fuzzy-Rules-for-Interpretable-Medical-Image-Classification"><a href="#FireGNN-Neuro-Symbolic-Graph-Neural-Networks-with-Trainable-Fuzzy-Rules-for-Interpretable-Medical-Image-Classification" class="headerlink" title="FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules   for Interpretable Medical Image Classification"></a>FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules   for Interpretable Medical Image Classification</h2><p><strong>Authors:Prajit Sengupta, Islem Rekik</strong></p>
<p>Medical image classification requires not only high predictive performance but also interpretability to ensure clinical trust and adoption. Graph Neural Networks (GNNs) offer a powerful framework for modeling relational structures within datasets; however, standard GNNs often operate as black boxes, limiting transparency and usability, particularly in clinical settings. In this work, we present an interpretable graph-based learning framework named FireGNN that integrates trainable fuzzy rules into GNNs for medical image classification. These rules embed topological descriptors - node degree, clustering coefficient, and label agreement - using learnable thresholds and sharpness parameters to enable intrinsic symbolic reasoning. Additionally, we explore auxiliary self-supervised tasks (e.g., homophily prediction, similarity entropy) as a benchmark to evaluate the contribution of topological learning. Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST benchmarks and the synthetic dataset MorphoMNIST, while also generating interpretable rule-based explanations. To our knowledge, this is the first integration of trainable fuzzy rules within a GNN. Source Code: <a target="_blank" rel="noopener" href="https://github.com/basiralab/FireGNN">https://github.com/basiralab/FireGNN</a> </p>
<blockquote>
<p>医疗图像分类不仅需要高预测性能，还需要可解释性，以确保临床上的信任和采用。图神经网络（GNNs）为数据集内的关系结构提供了强大的建模框架；然而，标准GNNs通常作为黑箱操作，限制了透明度和可用性，特别是在临床环境中。在这项工作中，我们提出了一个基于图的、可解释的学习框架，名为FireGNN，它将可训练的模糊规则集成到GNNs中，用于医学图像分类。这些规则使用可学习的阈值和尖锐度参数嵌入拓扑描述符（节点度、聚类系数和标签一致性），以实现内在符号推理。此外，我们探索了辅助的自我监督任务（例如，同源性预测、相似性熵）作为评估拓扑学习贡献的基准。我们的增强模糊规则模型在五个MedMNIST基准测试和合成数据集MorphoMNIST上取得了强大的性能表现，同时生成了基于规则的可解释解释。据我们所知，这是GNN中可训练模糊规则的首次集成。源代码：<a target="_blank" rel="noopener" href="https://github.com/basiralab/FireGNN">https://github.com/basiralab/FireGNN</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10510v2">PDF</a> Accepted at NeurIPS 2025 Conference (Workshop Track), San Diego, USA</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为FireGNN的可解释的基于图的学习框架，该框架将可训练的模糊规则集成到图神经网络（GNNs）中，用于医学图像分类。该框架使用拓扑描述符（如节点度、聚类系数和标签一致性）和可学习的阈值和尖锐度参数，实现了内在符号推理。此外，该研究还探讨了辅助自监督任务作为评估拓扑学习贡献的基准。该模糊规则增强的模型在五个MedMNIST基准测试和合成数据集MorphoMNIST上表现强劲，同时产生可解释的基于规则的解释。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分类需要高预测性能和可解释性，以确保临床信任和应用。</li>
<li>Graph Neural Networks (GNNs) 为数据集内的关系结构提供了强大的建模框架。</li>
<li>标准GNNs常常作为黑箱操作，限制了透明度和可用性，特别是在临床环境中。</li>
<li>FireGNN框架将可训练的模糊规则集成到GNNs中，实现医学图像分类。</li>
<li>该框架使用拓扑描述符进行符号推理，包括节点度、聚类系数和标签一致性。</li>
<li>辅助自监督任务被用作评估拓扑学习贡献的基准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8aff4be2351e0a7be67edcd94bf840fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134210&auth_key=1760134210-0-0-b8f666bbb489a862207f8d2ef4eaa888&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdade4dc9ba7c50a55677eec20e824a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134217&auth_key=1760134217-0-0-9c171dc7491888326259f948514241b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff52a14f4f059fe063e6dea4ff7c8272~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134224&auth_key=1760134224-0-0-d641ea7251867351e777a636f7e7d30c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VisionTS-Cross-Modal-Time-Series-Foundation-Model-with-Continual-Pre-trained-Vision-Backbones"><a href="#VisionTS-Cross-Modal-Time-Series-Foundation-Model-with-Continual-Pre-trained-Vision-Backbones" class="headerlink" title="VisionTS++: Cross-Modal Time Series Foundation Model with Continual   Pre-trained Vision Backbones"></a>VisionTS++: Cross-Modal Time Series Foundation Model with Continual   Pre-trained Vision Backbones</h2><p><strong>Authors:Lefei Shen, Mouxiang Chen, Xu Liu, Han Fu, Xiaoxue Ren, Jianling Sun, Zhuo Li, Chenghao Liu</strong></p>
<p>Recent studies have indicated that vision models pre-trained on images can serve as time series foundation models (TSFMs) by reformulating time series forecasting (TSF) as image reconstruction. However, effective cross-modal transfer from vision to time series remains challenging due to three discrepancies: (1) the data-modality gap between structured, bounded image data and unbounded, heterogeneous time series; (2) the multivariate-forecasting gap between fixed RGB-three-channel vision models and time series with arbitrary numbers of variates; and (3) the probabilistic-forecasting gap between the deterministic outputs of vision models and the requirement for uncertainty-aware probabilistic predictions. To bridge these gaps, we propose VisonTS++, a TSFM based on continual pre-training of a vision model on large-scale time series. Our approach introduces three key innovations: (1) vision-model-based filtering to identify high-quality sequences to stabilize pre-training and mitigate modality gap; (2) colorized multivariate conversion, encoding multivariate series as multi-subfigure RGB images to enhance cross-variate modeling; (3) multi-quantile forecasting, using parallel reconstruction heads to generate quantile forecasts without parametric assumptions. Experiments show that VisionTS++ achieves state-of-the-art performance in both in-distribution and out-of-distribution forecasting, outperforming specialized TSFMs by 6%-44% in MSE reduction and ranking first in GIFT-Eval benchmark which comprises 23 datasets across 7 domains. Our work demonstrates that with appropriate adaptation, vision models can effectively generalize to TSF, thus advancing the pursuit of universal TSFMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HALF111/VisionTSpp">https://github.com/HALF111/VisionTSpp</a>. </p>
<blockquote>
<p>近期研究表明，通过在图像上进行预训练的视觉模型，可以通过将时间序列预测（TSF）重新表述为图像重建，作为时间序列基础模型（TSFMs）。然而，由于三个差异，从视觉到时间序列的有效跨模态迁移仍然具有挑战性：（1）结构化、有界图像数据与无界、异质时间序列之间的数据模态差距；（2）固定RGB三通道视觉模型与具有任意变量数的时间序列之间的多元预测差距；（3）视觉模型的确定性输出与对不确定性感知的概率预测的要求之间的概率预测差距。为了弥合这些差距，我们提出了VisonTS++，一种基于视觉模型大规模时间序列持续预训练的时间序列基础模型。我们的方法引入了三个关键创新点：（1）基于视觉模型的过滤，以识别高质量序列，稳定预训练，减轻模态差距；（2）彩色多元转换，将多元序列编码为多子图RGB图像，以增强跨变量建模；（3）多分位预测，使用并行重建头生成分位预测，无需参数假设。实验表明，VisionTS++在内外分布预测方面达到了最先进的性能，在均方误差减少方面比专门的时间序列基础模型高出6%-44%，在由7个领域23个数据集组成的GIFT-Eval基准测试中排名第一。我们的工作证明，经过适当的适应，视觉模型可以有效地推广到TSF，从而推动通用TSFM的追求。代码可在<a target="_blank" rel="noopener" href="https://github.com/HALF111/VisionTSpp%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HALF111/VisionTSpp上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04379v2">PDF</a> 19 pages</p>
<p><strong>摘要</strong><br>    基于图像预训练的视觉模型通过改革时间序列预测为图像重建，可作为时间序列基础模型（TSFM）。然而，由于数据模态差距、多元预测差距和概率预测差距，从视觉到时间序列的有效跨模态迁移仍然具有挑战性。为缩小这些差距，我们提出VisionTS++模型，该模型基于视觉模型的大规模时间序列持续预训练，引入三个关键创新点：基于视觉模型的过滤以稳定预训练并缩小模态差距；彩色多元转换，将多元序列编码为多子图RGB图像，以增强跨变量建模；多分位预测，使用并行重建头生成分位预测，无需参数假设。实验表明，VisionTS++在分布内和分布外预测方面均达到最新性能水平，在MSE降低方面优于专业TSFM达6%-44%，在涵盖7个领域的23个数据集的GIFT-Eval基准测试中排名第一。我们的工作证明，经过适当的适应，视觉模型可以有效地推广到TSF，从而推动通用TSFM的追求。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究表明，基于图像预训练的视觉模型可以作为时间序列基础模型（TSFM）。</li>
<li>存在从视觉到时间序列的跨模态迁移挑战，主要由于数据模态、多元预测和概率预测的差距。</li>
<li>VisionTS++模型通过三个关键创新来解决这些问题：基于视觉模型的过滤、彩色多元转换和多分位预测。</li>
<li>VisionTS++在多种数据集上实现最先进的性能，表现出优秀的泛化能力。</li>
<li>该模型在分布内和分布外预测均表现优越，与专业TSFM相比，MSE降低达6%-44%。</li>
<li>VisionTS++在GIFT-Eval基准测试中排名第一，涵盖7个领域的23个数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fa456f582414c196ecd20bd9146bb6d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134232&auth_key=1760134232-0-0-ddebd43b62e09f77eff1e41b407f40eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c1a9cac7f87caed3ba9f57afb677359~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134239&auth_key=1760134239-0-0-3146ad5bb6d7ba19742c21d415712a4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0704bccd8332602e8d1a9eae0ff039c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134246&auth_key=1760134246-0-0-aea2313a748a7bc1c7d73c5489d9f5cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AutoMiSeg-Automatic-Medical-Image-Segmentation-via-Test-Time-Adaptation-of-Foundation-Models"><a href="#AutoMiSeg-Automatic-Medical-Image-Segmentation-via-Test-Time-Adaptation-of-Foundation-Models" class="headerlink" title="AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation   of Foundation Models"></a>AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation   of Foundation Models</h2><p><strong>Authors:Xingjian Li, Qifeng Wu, Adithya S. Ubaradka, Yiran Ding, Colleen Que, Runmin Jiang, Jianhua Xing, Tianyang Wang, Min Xu</strong></p>
<p>Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., “segment the optic disc in an eye fundus image”), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline not only substantially surpasses the previously best-performing method, yielding a 69% relative improvement in accuracy (Dice Score from 42.53 to 71.81), but also performs competitively with weakly-prompted interactive foundation models. </p>
<blockquote>
<p>医学图像分割对临床诊断至关重要，然而，当前的深度学习方法往往需要大量的专家精力，例如通过标注大量训练数据集或在推理时间为每个新病例提供提示。本文介绍了一种零样本自动分割管道，它结合了现成的视觉语言模型和分割基础模型。给定医学图像和任务定义（例如，“在眼底图像中分割视盘”），我们的方法使用定位模型生成初始边界框，然后通过视觉提示增强模块增强提示，最后由可提示的分割模型处理以产生最终掩码。为了解决领域差距和结果验证的挑战，我们引入了一个测试时间适应框架，该框架具有一组可学习的适配器，用于将医学输入与基础模型表示进行对齐。其超参数通过贝叶斯优化进行优化，由代理验证模型指导，无需真实标签。我们的管道为跨不同任务的零样本医学图像分割提供了一种高效且可扩展的解决方案。我们的管道在七个不同的医学成像数据集上进行了评估，并显示出有希望的结果。通过适当的分解和测试时间适应，我们的全自动管道不仅显著超越了之前性能最佳的方法，准确度相对提高了69%（Dice得分从42.53提高到71.81），而且与弱提示交互式基础模型的性能相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17931v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种零样本自动分割管道，结合了现成的视觉语言与分割基础模型。该方法只需医学图像和任务定义（如“在眼底图像中分割视盘”），无需大量专家标注数据或每个新病例的推理提示。通过初始边界框生成、视觉提示增强模块和可提示的分割模型处理，生成最终分割掩膜。为解决领域差距和结果验证问题，引入测试时自适应框架，通过优化超参数和代理验证模型，无需真实标签即可对齐医学输入与基础模型表示。该管道为跨不同任务的零样本医学图像分割提供了标注效率且可扩展的解决方案，并在七个不同的医学成像数据集上进行了评估，结果令人鼓舞。与传统方法相比，该管道不仅准确度相对提高了69%（Dice得分从42.53提高到71.81），而且与弱提示交互基础模型的性能具有竞争力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医学图像分割对临床诊断至关重要，但现有深度学习方法需要大量专家努力。</li>
<li>本文提出了一种零样本自动分割管道，结合了视觉语言和分割基础模型。</li>
<li>该方法通过生成初始边界框、增强视觉提示并处理可提示的分割模型来生成最终分割掩膜。</li>
<li>引入测试时自适应框架，通过优化超参数和代理验证模型解决领域差距和结果验证问题。</li>
<li>该管道在七个医学成像数据集上进行了评估，并实现了显著的准确性提高。</li>
<li>与传统方法相比，该管道的准确度相对提高了69%，并显示出与弱提示交互基础模型的竞争力。</li>
<li>该方法为实现全自动、高效的医学图像分割提供了新的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17931">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-13c7cb222ce3f7c4027634919a3b8253~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134253&auth_key=1760134253-0-0-186b87bfd09e913a2a1d65c1c401d370&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe144fb768b3ae7f90579b5daf66687e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134260&auth_key=1760134260-0-0-d19a8455a68ca03177cf64771234fbbb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db9038c169af92f3122f5872c731e32b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134267&auth_key=1760134267-0-0-e477780016b8294e3f28790ed790a328&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ac45edede43bac2333b49f0a2b18cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134274&auth_key=1760134274-0-0-e130502427e31213aa2cb35127ca3e51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Robust-Frequency-Domain-Full-Waveform-Inversion-via-HV-Geometry"><a href="#Robust-Frequency-Domain-Full-Waveform-Inversion-via-HV-Geometry" class="headerlink" title="Robust Frequency Domain Full-Waveform Inversion via HV-Geometry"></a>Robust Frequency Domain Full-Waveform Inversion via HV-Geometry</h2><p><strong>Authors:Zhijun Zeng, Matej Neumann, Yunan Yang</strong></p>
<p>Conventional frequency-domain full-waveform inversion (FWI) is typically implemented with an $L^2$ misfit function, which suffers from challenges such as cycle skipping and sensitivity to noise. While the Wasserstein metric has proven effective in addressing these issues in time-domain FWI, its applicability in frequency-domain FWI is limited due to the complex-valued nature of the data and reduced transport-like dependency on wave speed. To mitigate these challenges, we introduce the HV metric ($d_{\text{HV}}$), inspired by optimal transport theory, which compares signals based on horizontal and vertical changes without requiring the normalization of data. We implement $d_{\text{HV}}$ as the misfit function in frequency-domain FWI and evaluate its performance on synthetic and real-world datasets from seismic imaging and ultrasound computed tomography (USCT). Numerical experiments demonstrate that $d_{\text{HV}}$ outperforms the $L^2$ and Wasserstein metrics in scenarios with limited prior model information and high noise while robustly improving inversion results on clinical USCT data. </p>
<blockquote>
<p>传统的频域全波形反演（FWI）通常采用$L^2$不匹配函数进行实现，这面临着诸如周期跳变和对噪声敏感等挑战。虽然Wasserstein度量在时间域FWI中已被证明可以有效地解决这些问题，但由于数据的复值性和对波速的运输依赖降低，其在频域FWI中的应用受到限制。为了缓解这些挑战，我们引入了受最优传输理论启发的HV度量（$d_{\text{HV}}$），它基于水平变化和垂直变化比较信号，无需对数据进行归一化。我们将$d_{\text{HV}}$作为频域FWI中的不匹配函数进行实现，并评估其在地震成像和超声计算机断层扫描（USCT）的合成数据集和真实世界数据集上的性能。数值实验表明，在有限先验模型信息和高噪声的情况下，$d_{\text{HV}}$在场景中的表现优于$L^2$和Wasserstein度量，并且在临床USCT数据上稳健地改进了反演结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01817v2">PDF</a> </p>
<p><strong>Summary</strong><br>     传统频域全波形反演（FWI）通常使用L²不适定函数，面临循环跳过和噪声敏感等问题。受最优传输理论启发，我们引入HV度量（dHV），该度量基于水平垂直变化比较信号，无需数据归一化。在频域FWI中实现dHV作为不适定函数，并在地震成像和超声计算机断层扫描（USCT）的合成分组和现实世界数据集上评估其性能。数值实验表明，在有限先验模型信息和高噪声情况下，dHV优于L²和Wasserstein度量，并能在临床USCT数据上稳健地改进反演结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统频域全波形反演使用L²不适定函数面临挑战，如循环跳过和噪声敏感。</li>
<li>Wasserstein度量在频域FWI中的适用性有限，因为数据的复数性和运输依赖性的降低。</li>
<li>引入受最优传输理论启发的HV度量（dHV），无需数据归一化即可比较信号。</li>
<li>dHV基于水平垂直变化比较信号。</li>
<li>在合成和真实世界数据集上评估了dHV的性能，包括地震成像和超声计算机断层扫描（USCT）。</li>
<li>数值实验表明，在有限先验模型信息和高噪声情况下，dHV优于L²和Wasserstein度量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fd67fbb5f12f2df1b10ab17cf28fc12c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134281&auth_key=1760134281-0-0-d4ec4f6906457f2cb3ecf1840c58905a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ebeefdee468277653d55b5bf6d5f8f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134288&auth_key=1760134288-0-0-ff6b95861c4726cb784a4ffcbcd3a92f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d6faf51db223a4a895a0267d4db2b0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134294&auth_key=1760134294-0-0-268b210b732d400189ecdcc4b9ab26a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="From Gaze to Insight: Bridging Human Visual Attention and Vision   Language Model Explanation for Weakly-Supervised Medical Image Segmentation"></a>From Gaze to Insight: Bridging Human Visual Attention and Vision   Language Model Explanation for Weakly-Supervised Medical Image Segmentation</h2><p><strong>Authors:Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Vicente Grau, Jungong Han</strong></p>
<p>Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/jingkunchen/FGI">https://github.com/jingkunchen/FGI</a>. </p>
<blockquote>
<p>医学图像分割仍然是一个挑战，因为像素级标注的成本高昂，这增加了训练的难度。在弱监督的背景下，医生注视数据能够捕捉诊断时的感兴趣区域，但其稀疏性限制了其在分割中的应用。相比之下，视觉语言模型（VLM）通过文本描述提供语义上下文，但缺乏所需的解释精度。我们认识到，单一的来源不足以解决问题，因此提出一个整合注视和语言监督的教师-学生框架，发挥它们的互补优势。我们的关键见解是，注视数据指示医生在诊断时的关注区域，而VLM则解释这些区域为何重要。为实现这一点，教师模型首先从由VLM生成的病变形态描述增强的注视点中学习，为引导学生模型奠定基础。然后，教师通过三种策略引导学生：（1）多尺度特征对齐，融合视觉线索和文本语义；（2）置信度加权一致性约束，专注于可靠预测；（3）自适应掩码，以限制不确定区域的错误传播。在Kvasir-SEG、NCI-ISBI和ISIC数据集上的实验表明，我们的方法分别实现了80.78%、80.53%和84.22%的Dice得分，在注视基线的基础上提高了3-5%，同时没有增加标注负担。通过保留预测、注视数据和病变描述之间的相关性，我们的框架还保持了临床可解释性。这项工作说明了如何将人类视觉注意力与AI生成的语义上下文相结合，从而有效克服单个弱监督信号的局限性，推动部署高效、注释有效的医疗AI系统的发展。相关代码可访问：<a target="_blank" rel="noopener" href="https://github.com/jingkunchen/FGI%E3%80%82">https://github.com/jingkunchen/FGI。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11368v2">PDF</a> 11 pages, 4 figures</p>
<p><strong>摘要</strong><br>    本研究针对医学图像分割面临的挑战，提出了一种结合医生注视数据和视觉语言模型（VLM）的教师-学生框架。该框架利用两者互补的优势，通过教师模型学习医生注视点和VLM生成的病变形态描述，指导学生模型进行训练。通过多尺度特征对齐、置信度加权一致性约束和自适应掩模等策略，实现医学图像分割的精准预测。实验结果表明，该方法在Kvasir-SEG、NCI-ISBI和ISIC数据集上的Dice得分有所提高，同时保持临床可解释性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医学图像分割面临训练像素级注释的高成本挑战。</li>
<li>医生注视数据能捕捉诊断时的感兴趣区域，但其稀疏性限制了其在分割中的应用。</li>
<li>视觉语言模型（VLM）通过文本描述提供语义上下文，但缺乏解释精度。</li>
<li>提出了一种教师-学生框架，结合了注视和语言的监督，利用它们的互补优势。</li>
<li>教师模型通过结合医生注视点和VLM生成的描述建立基础，指导学生模型训练。</li>
<li>通过多尺度特征对齐等策略实现精准预测，并在多个数据集上取得改进结果。</li>
<li>该方法保持临床可解释性，通过整合人类视觉注意和AI生成的语义上下文，克服单一弱监督信号的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0a491ba654777f0b00530d3222459c09~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134302&auth_key=1760134302-0-0-81e2c7d5433072d4b9859bddd5973898&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1211f2f914a4a421178243f97155964~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134310&auth_key=1760134310-0-0-a4c6925ac12dba34ec93416618a34705&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6e6064cb5d640eaf5ac4a36678cf720~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134316&auth_key=1760134316-0-0-819cdfcfaaf3559b591578806e6057a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Optimizing-Breast-Cancer-Detection-in-Mammograms-A-Comprehensive-Study-of-Transfer-Learning-Resolution-Reduction-and-Multi-View-Classification"><a href="#Optimizing-Breast-Cancer-Detection-in-Mammograms-A-Comprehensive-Study-of-Transfer-Learning-Resolution-Reduction-and-Multi-View-Classification" class="headerlink" title="Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study   of Transfer Learning, Resolution Reduction, and Multi-View Classification"></a>Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study   of Transfer Learning, Resolution Reduction, and Multi-View Classification</h2><p><strong>Authors:Daniel G. P. Petrini, Hae Yong Kim</strong></p>
<p>Mammography, an X-ray-based imaging technique, remains central to the early detection of breast cancer. Recent advances in artificial intelligence have enabled increasingly sophisticated computer-aided diagnostic methods, evolving from patch-based classifiers to whole-image approaches and then to multi-view architectures that jointly analyze complementary projections. Despite this progress, several critical questions remain unanswered. In this study, we systematically investigate these issues by addressing five key research questions: (1) the role of patch classifiers in performance, (2) the transferability of natural-image-trained backbones, (3) the advantages of learn-to-resize over conventional downscaling, (4) the contribution of multi-view integration, and (5) the robustness of findings across varying image quality. Beyond benchmarking, our experiments demonstrate clear performance gains over prior work. For the CBIS-DDSM dataset, we improved single-view AUC from 0.8153 to 0.8343, and multiple-view AUC from 0.8483 to 0.8658. Using a new comparative method, we also observed a 0.0217 AUC increase when extending from single to multiple-view analysis. On the complete VinDr-Mammo dataset, the multiple-view approach further improved results, achieving a 0.0492 AUC increase over single view and reaching 0.8511 AUC overall. These results establish new state-of-the-art benchmarks, providing clear evidence of the advantages of multi-view architectures for mammogram interpretation. Beyond performance, our analysis offers principled insights into model design and transfer learning strategies, contributing to the development of more accurate and reliable breast cancer screening tools. The inference code and trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/dpetrini/multiple-view">https://github.com/dpetrini/multiple-view</a>. </p>
<blockquote>
<p>乳腺X线摄影（即乳腺摄影术）仍是早期发现乳腺癌的核心技术。人工智能领域的最新进展使得计算机辅助诊断方法越来越精细，从基于补丁的分类器发展到全图像方法，再发展到多视图架构，联合分析互补投影。尽管取得了进展，但仍有一些关键问题尚未得到解答。在这项研究中，我们通过解决五个关键研究问题来系统地研究这些问题：（1）补丁分类器在性能中的作用；（2）自然图像训练后骨的可转移性；（3）与学习调整大小相比，传统缩小尺寸的优势；（4）多视图集成的贡献；以及（5）不同图像质量下研究结果的稳健性。除了基准测试外，我们的实验还证明了与之前工作的明确性能提升。对于CBIS-DDSM数据集，我们将单视图AUC从0.8153提高到0.8343，多视图AUC从0.8483提高到0.8658。使用一种新的比较方法，我们还观察到从单视图分析扩展到多视图分析时的AUC增加了0.0217。在完整的VinDr-Mammo数据集上，多视图方法进一步改善了结果，相对于单视图实现了0.0492的AUC增加，总体达到0.8511的AUC。这些结果建立了新的最先进的基准，充分证明了多视图架构在乳腺钼靶解读中的优势。除了性能之外，我们的分析还为模型设计和迁移学习策略提供了原则性的见解，为开发更准确可靠的乳腺癌筛查工具做出了贡献。推理代码和训练模型可在<a target="_blank" rel="noopener" href="https://github.com/dpetrini/multiple-view%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dpetrini/multiple-view公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19945v3">PDF</a> 31 pages</p>
<p><strong>Summary</strong></p>
<p>本文探讨了人工智能在乳腺癌检测中的应用，特别是多视角架构在乳腺X光图像分析中的优势。研究通过系统实验回答了五个关键问题，并在CBIS-DDSM和VinDr-Mammo数据集上取得了新的先进性能。多视角分析方法的引入有效提升了诊断准确性，为乳腺癌筛查工具的开发提供了理论见解和实践指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在乳腺癌检测中的最新进展，特别是多视角架构的应用，能联合分析互补投影，提高诊断准确性。</li>
<li>研究通过解决五个关键问题，包括patch分类器的作用、自然图像训练骨架的迁移性、学习调整大小的优势、多视角整合的贡献以及不同图像质量下结果的稳定性，进行了系统的实验调查。</li>
<li>在CBIS-DDSM数据集上，多视角分析方法相较于单视角提高了诊断性能，AUC值从0.8153提升至0.8658。</li>
<li>在VinDr-Mammo数据集上，多视角方法实现了更显著的AUC值提升，从单视角的0.8提升至整体的0.8511。</li>
<li>研究结果确立了新的性能基准，并提供了模型设计和迁移学习策略的见解，有助于开发更准确可靠的乳腺癌筛查工具。</li>
<li>公开可用的推理代码和训练模型有助于进一步研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19945">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c544a5ae3021a8dcef9af7d3a514704b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134323&auth_key=1760134323-0-0-5d3be39233e7d7c1dc25d26aba4dbb55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e16a81dd49aee3d1f10c1344a31328a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134330&auth_key=1760134330-0-0-70a4b2540ce6384bf508d42287e3a984&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Submillimeter-Accurate-3D-Lumbar-Spine-Reconstruction-from-Biplanar-X-Ray-Images-Incorporating-a-Multi-Task-Network-and-Landmark-Weighted-Loss"><a href="#Submillimeter-Accurate-3D-Lumbar-Spine-Reconstruction-from-Biplanar-X-Ray-Images-Incorporating-a-Multi-Task-Network-and-Landmark-Weighted-Loss" class="headerlink" title="Submillimeter-Accurate 3D Lumbar Spine Reconstruction from Biplanar   X-Ray Images: Incorporating a Multi-Task Network and Landmark-Weighted Loss"></a>Submillimeter-Accurate 3D Lumbar Spine Reconstruction from Biplanar   X-Ray Images: Incorporating a Multi-Task Network and Landmark-Weighted Loss</h2><p><strong>Authors:Wanxin Yu, Zhemin Zhu, Cong Wang, Yihang Bao, Chunjie Xia, Rongshan Cheng, Yan Yu, Tsung-Yuan Tsai</strong></p>
<p>To meet the clinical demand for accurate 3D lumbar spine assessment in a weight-bearing position, this study presents a novel, fully automatic framework for high-precision 3D reconstruction from biplanar X-ray images, overcoming the limitations of existing methods. The core of this method involves a novel multi-task deep learning network that simultaneously performs lumbar decomposition and landmark detection on the original biplanar radiographs. The decomposition effectively eliminates interference from surrounding tissues, simplifying subsequent image registration, while the landmark detection provides an initial pose estimation for the Statistical Shape Model (SSM), enhancing the efficiency and robustness of the registration process. Building on this, we introduce a landmark-weighted 2D-3D registration strategy. By assigning higher weights to complex posterior structures like the transverse and spinous processes during optimization, this strategy significantly enhances the reconstruction accuracy of the posterior arch. Our method was validated against a gold standard derived from registering CT segmentations to the biplanar X-rays. It sets a new benchmark by achieving sub-millimeter accuracy and completes the full reconstruction and measurement workflow in under 20 seconds, establishing a state-of-the-art combination of precision and speed. This fast and low-dose pipeline provides a powerful automated tool for diagnosing lumbar conditions such as spondylolisthesis and scoliosis in their functional, weight-bearing state. </p>
<blockquote>
<p>为满足在承重状态下对精确三维腰椎评估的临床需求，本研究提出了一种新型全自动高精度三维重建框架，可从双平面X射线图像中构建，克服了现有方法的局限性。该方法的核心在于一种新型多任务深度学习网络，可同时执行腰椎分解和原始双平面放射影像上的地标检测。分解有效地消除了周围组织的干扰，简化了后续图像注册，而地标检测为统计形状模型（SSM）提供了初步姿态估计，提高了注册过程的效率和稳健性。在此基础上，我们引入了一种地标加权2D-3D注册策略。通过在优化过程中给横向过程和棘突等复杂后侧结构分配更高的权重，该策略大大提高了后侧结构的重建精度。我们的方法通过与从双平面X射线注册得到的CT分割的金标准进行比较来验证。它达到了亚毫米级的精度，并在不到20秒内完成了完整的重建和测量流程，建立了精确度和速度的最新组合。这一快速、低剂量的流程为在功能承重状态下诊断腰椎疾病（如脊柱滑脱和脊柱侧弯）提供了强大的自动化工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14573v3">PDF</a> 27 pages, 16 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>本研究提出了一种新型全自动框架，用于从双平面X射线图像进行高精度3D重建，以满足临床对负重状态下准确3D腰椎评估的需求。该研究的核心在于一种多任务深度学习网络，可同时执行腰椎分解和地标检测，从而消除周围组织的干扰，简化图像注册，并提供初始姿态估计，增强统计形状模型的效率和稳健性。此外，引入地标加权2D-3D注册策略，优化过程中给复杂后结构如横突和棘突赋予更高的权重，显著提高后拱重建的准确性。该研究实现了亚毫米级精度，并在20秒内完成完整重建和测量流程，为诊断腰椎疾病如椎体滑脱和脊柱侧凸提供了强大的自动化工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一种全自动框架，从双平面X射线图像进行高精度3D腰椎重建。</li>
<li>多任务深度学习网络同时执行腰椎分解和地标检测，消除周围组织干扰，简化图像注册。</li>
<li>地标检测提供初始姿态估计，增强统计形状模型的效率和稳健性。</li>
<li>引入地标加权2D-3D注册策略，优化复杂后结构的重建准确性。</li>
<li>研究实现了亚毫米级精度，快速完成重建和测量流程。</li>
<li>该方法可用于诊断腰椎疾病，如椎体滑脱和脊柱侧凸。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c645fbd933ee18cbc08b60e860a7226f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134337&auth_key=1760134337-0-0-ef98139e6106f84302636700054c527f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Graph-Based-Framework-for-Interpretable-Whole-Slide-Image-Analysis"><a href="#A-Graph-Based-Framework-for-Interpretable-Whole-Slide-Image-Analysis" class="headerlink" title="A Graph-Based Framework for Interpretable Whole Slide Image Analysis"></a>A Graph-Based Framework for Interpretable Whole Slide Image Analysis</h2><p><strong>Authors:Alexander Weers, Alexander H. Berger, Laurin Lux, Peter Schüffler, Daniel Rueckert, Johannes C. Paetzold</strong></p>
<p>The histopathological analysis of whole-slide images (WSIs) is fundamental to cancer diagnosis but is a time-consuming and expert-driven process. While deep learning methods show promising results, dominant patch-based methods artificially fragment tissue, ignore biological boundaries, and produce black-box predictions. We overcome these limitations with a novel framework that transforms gigapixel WSIs into biologically-informed graph representations and is interpretable by design. Our approach builds graph nodes from tissue regions that respect natural structures, not arbitrary grids. We introduce an adaptive graph coarsening technique, guided by learned embeddings, to efficiently merge homogeneous regions while preserving diagnostically critical details in heterogeneous areas. Each node is enriched with a compact, interpretable feature set capturing clinically-motivated priors. A graph attention network then performs diagnosis on this compact representation. We demonstrate strong performance on challenging cancer staging and survival prediction tasks. Crucially, our resource-efficient model ($&gt;$13x fewer parameters and $&gt;$300x less data) achieves results competitive with a massive foundation model, while offering full interpretability through feature attribution. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HistoGraph31/pix2pathology">https://github.com/HistoGraph31/pix2pathology</a>. </p>
<blockquote>
<p>全切片图像（WSI）的组织病理学分析对癌症诊断至关重要，但这是一个耗时且依赖专家的过程。虽然深度学习的方法显示出有前景的结果，但主流的基于补丁的方法人为地分割组织，忽略生物边界，并产生黑箱预测。我们通过一个新型框架克服了这些限制，该框架将吉像素WSI转换为具有生物学信息的图形表示，并通过设计具有可解释性。我们的方法根据自然结构而不是任意网格构建图形节点。我们引入了一种自适应图形粗化技术，通过学习的嵌入进行引导，以有效地合并同质区域，同时保留异质区域的诊断细节。每个节点都通过捕捉临床驱动的先验知识，配备了一套紧凑且可解释的特征。然后，图注意力网络在此紧凑表示上进行诊断。我们在具有挑战性的癌症分期和生存预测任务上表现出强劲的性能。关键的是，我们的资源高效型模型（参数少13倍以上，数据少300倍以上）实现了与大型基础模型相当的结果，同时通过特征归属提供完全的可解释性。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/HistoGraph31/pix2pathology">https://github.com/HistoGraph31/pix2pathology</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11846v2">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种全新的用于全幻灯片图像（WSI）病理分析的框架，它能够将巨像素WSI转化为具有生物学信息的图形表示，通过构建尊重自然结构的图形节点，实现更准确的癌症诊断。该框架引入了一种自适应的图简化技术，能够在保留诊断细节的同时，有效地合并同质区域。此外，该模型具有紧凑、可解释的特征集，并通过图注意力网络进行诊断。该模型在癌症分期和生存预测任务上表现出强大的性能，且资源利用率高，可实现特征归属的完全可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>病理分析在全幻灯片图像（WSI）中至关重要，但过程耗时且依赖专家。</li>
<li>当前深度学习方法在病理分析中存在局限性，如基于补丁的方法可能会人为地破坏组织，忽视生物边界，并产生黑箱预测。</li>
<li>提出了一种新的框架，将巨像素WSI转化为具有生物学信息的图形表示，尊重自然结构，提高诊断准确性。</li>
<li>引入自适应图简化技术，通过学习的嵌入进行引导，有效合并同质区域，同时保留异质区域的诊断细节。</li>
<li>每个节点都包含紧凑、可解释的特征集，这些特征集捕捉了临床动机的先验知识。</li>
<li>使用图注意力网络进行诊断，并在具有挑战性的癌症分期和生存预测任务上表现出强大的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11846">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ad39be852ed86510f61ca329fb3830e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134345&auth_key=1760134345-0-0-93dd11f2dcc3c511172f16a6298a5df6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c34f0784e1846959ac19741b5428314e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134352&auth_key=1760134352-0-0-e73cd7d5875750ab5b28f70e30f8afc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdee39bdc0a3b927b0c5d575893121a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134359&auth_key=1760134359-0-0-36416279630d5883d3d529446a88590a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c33a255586c6ecc7491c7a6452779876~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134365&auth_key=1760134365-0-0-b6818b05691e6f44d2d4d66ebf37d36b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba0de929bbb054ad994690369d8c0496~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134372&auth_key=1760134372-0-0-b249e533ad27aa72a9efae86d43315aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bea3abad3ed3fa41c016f152d4e7431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134378&auth_key=1760134378-0-0-6ec0f4e9745d41baf6789376d0beec1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging"><a href="#H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging" class="headerlink" title="H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging"></a>H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging</h2><p><strong>Authors:Zhen Huang, Tao Tang, Ronghao Xu, Yangbo Wei, Wenkai Yang, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao</strong></p>
<p>3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.   In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection \textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights. </p>
<blockquote>
<p>三维（3D）标志点检测是医学图像分析中的一项关键任务，准确检测解剖标志点对于后续医学成像任务至关重要。然而，该领域的主流深度学习方法难以在精细局部特征捕捉、全局空间关系建模、准确性和计算效率之间保持平衡。局部特征提取需要捕捉精细的解剖细节，而全局建模则需要理解复杂解剖结构内的空间关系。此外，由于标志点稀疏分布，三维体积的高维性质进一步加剧了这些挑战，导致计算成本较高。因此，实现高效且精确的3D标志点检测仍然是医学图像分析中的一个紧迫挑战。在本研究中，我们提出了一种混合三维检测网络（Hybrid 3D Detection Net，简称H3DE-Net）。这是一个新颖框架，结合了卷积神经网络（CNN）用于局部特征提取和一个轻量级注意力机制，旨在高效捕捉三维体积数据中的全局依赖关系。该机制采用分层路由策略来降低计算成本，同时保持全局上下文建模。据我们所知，H3DE-Net是第一个将此类轻量级注意力机制与CNN相结合的3D标志点检测模型。此外，通过整合多尺度特征融合进一步提高了检测精度和稳健性。在公共CT数据集上的实验结果表明，H3DE-Net达到了最先进的性能，特别是在缺失标志点或复杂解剖结构变异的情况下，显著提高了准确性和稳健性。我们已经开源了我们的项目，包括代码、数据和模型权重。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14221v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种混合三维检测网络（H3DE-Net），结合卷积神经网络（CNN）进行局部特征提取，并采用轻量级注意力机制高效捕捉三维体积数据中的全局依赖关系。该方法采用分层路由策略，降低计算成本的同时保持全局上下文建模。实验结果表明，H3DE-Net在公开CT数据集上达到最佳性能，尤其在缺失地标或复杂解剖变异情况下，提高准确性和鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D landmark检测在医学图像分析中至关重要，但主流深度学习方法难以平衡精细局部特征与全局空间关系的捕捉。</li>
<li>局部特征提取需要捕捉细致的解剖细节，而全局建模需要理解复杂解剖结构内的空间关系。</li>
<li>H3DE-Net结合CNN和轻量级注意力机制，高效捕捉三维体积数据中的全局依赖。</li>
<li>分层路由策略降低计算成本，同时保持全局上下文建模。</li>
<li>H3DE-Net是首个结合轻量级注意力机制和CNN的3D landmark检测模型。</li>
<li>多尺度特征融合进一步提高检测准确性和鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-915501af2f98253bae1c0a691393a7bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134386&auth_key=1760134386-0-0-d4c469a814462ca11e907e381862669a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43ea667fa313cad50fa94792383c3e76~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134393&auth_key=1760134393-0-0-438c011f9b80fac58de93b9736b528c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-44c29e4a7f8a0890e727266b7e798f95~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134399&auth_key=1760134399-0-0-33c361c4347c7099e4b9cc1fe601e702&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2421b59cbea368c5f99c3ecfbff3b9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134406&auth_key=1760134406-0-0-05fe887ea352c998e4eeb4de22638326&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-68489d6d976bde2a48736db2a974242e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134413&auth_key=1760134413-0-0-527fe8c762191000817bc6cebb840b10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-179e82adbe16f3f4ed6f5b3cc246c54c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134419&auth_key=1760134419-0-0-f0c94b2adc3b18cbe3c6bb65c6e09a2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b1e1df5b33f5a558477ed2ee69aca01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134426&auth_key=1760134426-0-0-f41888b80b82bb8a8bf3667a181179f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention"><a href="#Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention" class="headerlink" title="Medical Image Classification with KAN-Integrated Transformers and   Dilated Neighborhood Attention"></a>Medical Image Classification with KAN-Integrated Transformers and   Dilated Neighborhood Attention</h2><p><strong>Authors:Omid Nejati Manzari, Hojat Asgariandehkordi, Taha Koleilat, Yiming Xiao, Hassan Rivaz</strong></p>
<p>Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6% on MedMNIST, 5.8% on NonMNIST, and 13.4% on the MedMNIST-C benchmark. </p>
<blockquote>
<p>卷积网络、变压器、混合模型和基于Mamba的架构已在各种医学图像分类任务中表现出强大的性能。然而，这些方法主要是为使用带标签数据对干净图像进行分类而设计的。相比之下，现实世界的临床数据通常涉及多中心研究独有的图像损坏问题，以及来自不同制造商的成像设备之间的差异所导致的图像损坏问题。在本文中，我们引入了医疗视觉转换器（MedViTV2），这是一种新型架构，首次将Kolmogorov-Arnold网络（KAN）层纳入转换器架构中，旨在实现通用的医学图像分类。我们开发了一个高效的KAN块，以减少计算负载同时提高原始MedViT的准确性。此外，为了抵消我们MedViT在扩大规模时的脆弱性，我们提出了一种增强的膨胀邻域注意力（DiNA），这是对高效的融合点积注意力核心的适应，能够捕获全局上下文并扩展感受野，以有效地扩展模型并解决特征崩溃问题。此外，我们介绍了一种分层混合策略，以有效的方式堆叠我们的局部特征感知和全局特征感知块，这平衡了局部和全局特征感知以提高性能。在17个医学图像分类数据集和12个损坏医学图像数据集上的大量实验表明，MedViTV2在29次实验中的27次达到了最新水平的结果，并且计算复杂度降低。MedViTV2的计算效率比前一个版本高出44％，并且大大提高了准确性，在MedMNIST上提高了4.6％，在NonMNIST上提高了5.8％，在MedMNIST-C基准测试上提高了13.4％。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13693v3">PDF</a> </p>
<p><strong>摘要</strong><br>    本文介绍了Medical Vision Transformer V2（MedViTV2）架构，该架构首次将Kolmogorov-Arnold网络（KAN）层融入transformer架构中，旨在实现通用的医学图像分类。通过开发高效的KAN块，减少了计算负载，提高了原始MedViT的准确性。为应对模型放大后的脆弱性，提出了增强的Dilated Neighborhood Attention（DiNA），其能捕捉全局上下文并扩展感受野，有效扩展模型并解决特征崩溃问题。此外，还介绍了分层混合策略，以平衡本地和全局特征感知块，提高性能。在17个医学图像分类数据集和12个腐败医学图像数据集上的大量实验表明，MedViTV2在27项实验中获得了最新结果，计算复杂度降低。与前一版本相比，MedViTV2计算效率提高了44%，并且在MedMNIST、NonMNIST和MedMNIST-C基准测试上的准确率分别提高了4.6%、5.8%和13.4%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>MedViTV2结合了卷积网络、transformer、混合模型和基于Mamba的架构的优势，用于医学图像分类任务。</li>
<li>引入Kolmogorov-Arnold网络（KAN）层以提高模型的准确性和效率。</li>
<li>提出Dilated Neighborhood Attention（DiNA）以增强模型的健壮性并扩大感受野。</li>
<li>采用分层混合策略平衡本地和全局特征感知，进一步提升性能。</li>
<li>在多个医学图像分类数据集上进行广泛实验，证明MedViTV2具有卓越的性能和计算效率。</li>
<li>与先前的模型相比，MedViTV2在计算效率和准确率方面均有显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a4a0e0023f890c0b230693aac78fdc9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134434&auth_key=1760134434-0-0-72d8c7b8d0a6c4693bcb65a331661596&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65093cf0573e4d4e02293e38fa2fc882~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134441&auth_key=1760134441-0-0-59261ec397d265177c7657cc11ce2e15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34be78d7dcefba8d7265a1e2e265150f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134448&auth_key=1760134448-0-0-8900481b4ccbf5f46e633276d6045398&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-33686396df5a918b34909ee013ebfbd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135725&auth_key=1760135725-0-0-eb6e51862bc1f8cf44096ab0ae86febc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-10-11  DialoSpeech Dual-Speaker Dialogue Generation with LLM and Flow Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-96b0b56ffe742bd505738bdd8e536a8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760132053&auth_key=1760132053-0-0-f5e21e90ce190179da897f7d41423e8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-10-11  Biology-driven assessment of deep learning super-resolution imaging of   the porosity network in dentin
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
