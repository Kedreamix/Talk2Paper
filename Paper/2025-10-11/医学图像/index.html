<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  AI-Driven Radiology Report Generation for Traumatic Brain Injuries">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-2b40a21bee6693aa28fcd120acfe463f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134196&auth_key=1760134196-0-0-2a1ca65148107875e5be856c1e2a00a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-11-æ›´æ–°"><a href="#2025-10-11-æ›´æ–°" class="headerlink" title="2025-10-11 æ›´æ–°"></a>2025-10-11 æ›´æ–°</h1><h2 id="AI-Driven-Radiology-Report-Generation-for-Traumatic-Brain-Injuries"><a href="#AI-Driven-Radiology-Report-Generation-for-Traumatic-Brain-Injuries" class="headerlink" title="AI-Driven Radiology Report Generation for Traumatic Brain Injuries"></a>AI-Driven Radiology Report Generation for Traumatic Brain Injuries</h2><p><strong>Authors:Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli</strong></p>
<p>Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries. </p>
<blockquote>
<p>åˆ›ä¼¤æ€§è„‘æŸä¼¤åœ¨æ€¥è¯ŠåŒ»å­¦ä¸­æ„æˆäº†é‡è¦çš„è¯Šæ–­æŒ‘æˆ˜ï¼Œå› ä¸ºå¯¹åŒ»ç–—å›¾åƒè¿›è¡ŒåŠæ—¶çš„è§£è¯»å¯¹æ‚£è€…çš„æ²»ç–—æ•ˆæœè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºäººå·¥æ™ºèƒ½çš„æ–¹æ³•ï¼Œä¸“ä¸ºé¢…è„‘æŸä¼¤ç—…ä¾‹è®¾è®¡è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†AC-BiFPNä¸Transformeræ¶æ„æ¥æ•è·å’Œå¤„ç†å¤æ‚çš„åŒ»ç–—æˆåƒæ•°æ®ï¼Œå¦‚CTå’ŒMRIæ‰«æã€‚AC-BiFPNæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œèƒ½å¤Ÿæ£€æµ‹é¢…å†…å‡ºè¡€ç­‰å¤æ‚å¼‚å¸¸ï¼Œè€ŒTransformeré€šè¿‡æ¨¡æ‹Ÿè¿œç¨‹ä¾èµ–å…³ç³»ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„è¯Šæ–­æŠ¥å‘Šã€‚æˆ‘ä»¬åœ¨RSNAé¢…å†…å‡ºè¡€æ£€æµ‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ä¼ ç»ŸåŸºäºCNNçš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨è¯Šæ–­å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚è¿™ä¸€è§£å†³æ–¹æ¡ˆä¸ä»…æ”¯æŒå¤„äºé«˜å‹ç¯å¢ƒä¸­çš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè€Œä¸”è¿˜ä¸ºå®ä¹ åŒ»ç”Ÿæä¾›äº†å¼ºå¤§çš„æ•™è‚²å·¥å…·ï¼Œæä¾›å®æ—¶åé¦ˆå¹¶å¢å¼ºä»–ä»¬çš„å­¦ä¹ ä½“éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†é«˜çº§ç‰¹å¾æå–ä¸åŸºäºTransformerçš„æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆï¼Œåœ¨åˆ›ä¼¤æ€§è„‘æŸä¼¤çš„è¯Šæ–­ä¸­å¯ä»¥æé«˜ä¸´åºŠå†³ç­–åˆ¶å®šçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08498v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºAIçš„è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºé¢…è„‘å¤–ä¼¤ç—…ä¾‹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†AC-BiFPNå’ŒTransformeræ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„åŒ»å­¦æˆåƒæ•°æ®ï¼Œå¦‚CTå’ŒMRIæ‰«æã€‚AC-BiFPNæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œæ£€æµ‹é¢…å†…å‡ºè¡€ç­‰ç»†å¾®å¼‚å¸¸ï¼Œè€ŒTransformerç”Ÿæˆè¿è´¯ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„è¯Šæ–­æŠ¥å‘Šï¼Œé€šè¿‡å»ºæ¨¡è¿œç¨‹ä¾èµ–å…³ç³»ã€‚åœ¨RSNAé¢…å†…å‡ºè¡€æ£€æµ‹æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œè¯¥æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿçš„CNNæ¨¡å‹ã€‚æ­¤è§£å†³æ–¹æ¡ˆä¸ä»…æ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨é«˜å‹åŠ›ç¯å¢ƒä¸‹å·¥ä½œï¼Œè¿˜ä¸ºåŒ»å­¦ç”Ÿæä¾›å¼ºå¤§çš„æ•™è‚²å·¥å…·ï¼Œæä¾›å®æ—¶åé¦ˆï¼Œå¢å¼ºå­¦ä¹ ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†åœ¨ç´§æ€¥åŒ»å­¦ä¸­è¯Šæ–­é¢…è„‘æŸä¼¤çš„æŒ‘æˆ˜æ€§ï¼Œå¼ºè°ƒåŒ»å­¦å›¾åƒè§£è¯»çš„åŠæ—¶æ€§å¯¹æ‚£è€…ç»“æœè‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºAIçš„è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œä¸“é—¨ç”¨äºé¢…è„‘æŸä¼¤ç—…ä¾‹ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†AC-BiFPNå’ŒTransformeræ¶æ„ï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤æ‚çš„åŒ»å­¦æˆåƒæ•°æ®ã€‚</li>
<li>AC-BiFPNèƒ½å¤Ÿæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œæ£€æµ‹é¢…å†…å‡ºè¡€ç­‰ç»†å¾®å¼‚å¸¸ã€‚</li>
<li>Transformerèƒ½å¤Ÿç”Ÿæˆè¿è´¯ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„è¯Šæ–­æŠ¥å‘Šï¼Œå»ºæ¨¡è¿œç¨‹ä¾èµ–å…³ç³»ã€‚</li>
<li>åœ¨RSNAé¢…å†…å‡ºè¡€æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢ä¼˜äºä¼ ç»ŸCNNæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c85bb3ea46b5d2afb1ea431d18f9d0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133924&auth_key=1760133924-0-0-8a220f53c6347bb9965f9b4b27ce1d71&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a14ab6a7ab551c3de935ac2bced4016~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133931&auth_key=1760133931-0-0-6ba3c7a8aeb8aadc7ca08908e6af04b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Robust-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation-based-on-Curriculum-Learning"><a href="#Robust-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation-based-on-Curriculum-Learning" class="headerlink" title="Robust Source-Free Domain Adaptation for Medical Image Segmentation   based on Curriculum Learning"></a>Robust Source-Free Domain Adaptation for Medical Image Segmentation   based on Curriculum Learning</h2><p><strong>Authors:Ziqi Zhang, Yuexiang Li, Yawen Huang, Nanjun He, Tao Xu, Liwei Lin, Yefeng Zheng, Shaoxin Li, Feiyue Huang</strong></p>
<p>Recent studies have uncovered a new research line, namely source-free domain adaptation, which adapts a model to target domains without using the source data. Such a setting can address the concerns on data privacy and security issues of medical images. However, current source-free domain adaptation frameworks mainly focus on the pseudo label refinement for target data without the consideration of learning procedure. Indeed, a progressive learning process from source to target domain will benefit the knowledge transfer during model adaptation. To this end, we propose a curriculum-based framework, namely learning from curriculum (LFC), for source-free domain adaptation, which consists of easy-to-hard and source-to-target curricula. Concretely, the former curriculum enables the framework to start learning with &#96;easyâ€™ samples and gradually tune the optimization direction of model adaption by increasing the sample difficulty. While, the latter can stablize the adaptation process, which ensures smooth transfer of the model from the source domain to the target. We evaluate the proposed source-free domain adaptation approach on the public cross-domain datasets for fundus segmentation and polyp segmentation. The extensive experimental results show that our framework surpasses the existing approaches and achieves a new state-of-the-art. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†ä¸€é¡¹æ–°çš„ç ”ç©¶çº¿è·¯ï¼Œå³æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSource-Free Domain Adaptationï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨æºæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä½¿æ¨¡å‹é€‚åº”ç›®æ ‡åŸŸã€‚è¿™ç§è®¾ç½®å¯ä»¥è§£å†³åŒ»ç–—å›¾åƒæ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ— æºåŸŸè‡ªé€‚åº”æ¡†æ¶ä¸»è¦å…³æ³¨ç›®æ ‡æ•°æ®çš„ä¼ªæ ‡ç­¾ä¼˜åŒ–ï¼Œè€Œæ²¡æœ‰è€ƒè™‘å­¦ä¹ è¿‡ç¨‹ã€‚å®é™…ä¸Šï¼Œä»æºåŸŸåˆ°ç›®æ ‡åŸŸé€æ­¥å­¦ä¹ è¿‡ç¨‹å°†æœ‰åŠ©äºæ¨¡å‹é€‚åº”è¿‡ç¨‹ä¸­çš„çŸ¥è¯†è½¬ç§»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹çš„å­¦ä¹ æ¡†æ¶ï¼Œå³æ— å­¦ä¹ è¯¾ç¨‹ï¼ˆLearning from Curriculum, LFCï¼‰ï¼Œç”¨äºæ— æºåŸŸè‡ªé€‚åº”ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ç”±æ˜“åˆ°éš¾å’Œæºåˆ°ç›®æ ‡çš„è¯¾ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå‰è€…è¯¾ç¨‹ä½¿æ¡†æ¶èƒ½å¤Ÿä»â€œå®¹æ˜“â€çš„æ ·æœ¬å¼€å§‹å­¦ä¹ ï¼Œå¹¶é€šè¿‡å¢åŠ æ ·æœ¬éš¾åº¦é€æ¸è°ƒæ•´æ¨¡å‹é€‚åº”çš„ä¼˜åŒ–æ–¹å‘ã€‚è€Œåè€…åˆ™å¯ä»¥ç¨³å®šé€‚åº”è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹ä»æºåŸŸåˆ°ç›®æ ‡åŸŸçš„å¹³ç¨³è½¬ç§»ã€‚æˆ‘ä»¬åœ¨å…¬å…±çš„çœ¼åº•åˆ†å‰²å’Œå¤šæ¯è‚‰åˆ†å‰²è·¨åŸŸæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ— æºåŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08393v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸç ”ç©¶å‘ç°äº†ä¸€ç§æ–°çš„ç ”ç©¶æ€è·¯â€”â€”æ— æºåŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ— éœ€ä½¿ç”¨æºæ•°æ®å³å¯å°†æ¨¡å‹é€‚åº”äºç›®æ ‡åŸŸã€‚è¯¥æŠ€æœ¯è§£å†³äº†åŒ»ç–—å›¾åƒç­‰æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ— æºåŸŸè‡ªé€‚åº”æ¡†æ¶ä¸»è¦å…³æ³¨ç›®æ ‡æ•°æ®çš„ä¼ªæ ‡ç­¾ä¼˜åŒ–ï¼Œè€Œå¿½ç•¥äº†å­¦ä¹ è¿‡ç¨‹çš„é‡è¦æ€§ã€‚æˆ‘ä»¬ä»æºåŸŸåˆ°ç›®æ ‡åŸŸè¿›è¡Œæ¸è¿›å­¦ä¹ ï¼Œæœ‰åŠ©äºçŸ¥è¯†è¿ç§»è¿‡ç¨‹ä¸­çš„æ¨¡å‹é€‚åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹çš„å­¦ä¹ æ¡†æ¶â€”â€”æ— å­¦ä¹ è¯¾ç¨‹ï¼ˆLFCï¼‰è¿›è¡Œæ— æºåŸŸè‡ªé€‚åº”ï¼Œå…¶ä¸­åŒ…æ‹¬ä»æ˜“åˆ°éš¾å’Œä»æºåˆ°ç›®æ ‡çš„è¯¾ç¨‹ã€‚æˆ‘ä»¬ä»â€œç®€å•â€æ ·æœ¬å¼€å§‹å­¦ä¹ ï¼Œé€šè¿‡å¢åŠ æ ·æœ¬éš¾åº¦é€æ¸è°ƒæ•´æ¨¡å‹ä¼˜åŒ–çš„æ–¹å‘ï¼›åŒæ—¶ç¡®ä¿æ¨¡å‹ä»æºåŸŸåˆ°ç›®æ ‡åŸŸçš„å¹³ç¨³è¿‡æ¸¡ã€‚æˆ‘ä»¬åœ¨å…¬å…±è·¨åŸŸæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ— æºåŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œç”¨äºçœ¼åº•åˆ†å‰²å’Œæ¯è‚‰åˆ†å‰²ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µâ€”â€”â€œæ— æºåŸŸè‡ªé€‚åº”â€ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å›¾åƒç­‰æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜ã€‚</li>
<li>å½“å‰çš„æ— æºåŸŸè‡ªé€‚åº”æ¡†æ¶ä¸»è¦å…³æ³¨ç›®æ ‡æ•°æ®çš„ä¼ªæ ‡ç­¾ä¼˜åŒ–ï¼Œä½†å¯¹å­¦ä¹ è¿‡ç¨‹è€ƒè™‘ä¸è¶³ã€‚</li>
<li>æ–°çš„åŸºäºè¯¾ç¨‹çš„å­¦ä¹ æ¡†æ¶â€”â€”æ— å­¦ä¹ è¯¾ç¨‹ï¼ˆLFCï¼‰ï¼Œè¢«æå‡ºæ¥å®ç°ä»æºåŸŸåˆ°ç›®æ ‡åŸŸçš„æ¸è¿›å­¦ä¹ ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹ï¼Œå¸®åŠ©æ¨¡å‹ä»ç®€å•æ ·æœ¬å¼€å§‹å­¦ä¹ ï¼Œé€æ¸å¢åŠ æ ·æœ¬éš¾åº¦ï¼Œè°ƒæ•´ä¼˜åŒ–æ–¹å‘ã€‚</li>
<li>è¿˜åŒ…æ‹¬ä»æºåˆ°ç›®æ ‡çš„è¯¾ç¨‹ï¼Œç¡®ä¿æ¨¡å‹å¹³ç¨³åœ°ä»æºåŸŸè¿‡æ¸¡åˆ°ç›®æ ‡åŸŸã€‚</li>
<li>åœ¨å…¬å…±è·¨åŸŸæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ–¹æ³•çš„çœ¼åº•åˆ†å‰²å’Œæ¯è‚‰åˆ†å‰²æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d63dc20127aea78f30cfbce51841475f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133938&auth_key=1760133938-0-0-0fcd16f8a11379a3d2a53ba676b9ecec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0962b6b526c234f238a245d808f9cc73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133945&auth_key=1760133945-0-0-b8439a762c7c26e2d8efa97c5b9dcedf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Random-Window-Augmentations-for-Deep-Learning-Robustness-in-CT-and-Liver-Tumor-Segmentation"><a href="#Random-Window-Augmentations-for-Deep-Learning-Robustness-in-CT-and-Liver-Tumor-Segmentation" class="headerlink" title="Random Window Augmentations for Deep Learning Robustness in CT and Liver   Tumor Segmentation"></a>Random Window Augmentations for Deep Learning Robustness in CT and Liver   Tumor Segmentation</h2><p><strong>Authors:Eirik A. Ã˜stmo, Kristoffer K. WickstrÃ¸m, Keyur Radiya, Michael C. Kampffmeyer, Karl Ã˜yvind Mikalsen, Robert Jenssen</strong></p>
<p>Contrast-enhanced Computed Tomography (CT) is important for diagnosis and treatment planning for various medical conditions. Deep learning (DL) based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images, thereby reducing cliniciansâ€™ workload. Achieving generalization capabilities in limited data domains, such as radiology, requires modern DL models to be trained with image augmentation. However, naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality, where the intensities measure Hounsfield Units (HU) and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this, we propose a CT-specific augmentation technique, called Random windowing, that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrast-enhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets, and compare to, and outperform, state-of-the-art alternatives, while focusing on the challenge of liver tumor segmentation. </p>
<blockquote>
<p>å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å¯¹äºå„ç§ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’éå¸¸é‡è¦ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æ¨¡å‹å¯èƒ½èƒ½å¤Ÿå®ç°åŒ»ç–—å›¾åƒè‡ªåŠ¨åŒ–åˆ†æï¼Œæ£€æµ‹å¹¶æç»˜CTå›¾åƒä¸­çš„è‚¿ç˜¤ï¼Œä»è€Œå‡è½»ä¸´åºŠåŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚åœ¨æœ‰é™çš„é¢†åŸŸï¼ˆå¦‚æ”¾å°„å­¦ï¼‰å®ç°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å€ŸåŠ©å›¾åƒå¢å¼ºæŠ€æœ¯å¯¹ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå°†é’ˆå¯¹è‡ªç„¶å›¾åƒå¼€å‘çš„å¢å¼ºæ–¹æ³•ç›´æ¥åº”ç”¨äºCTæ‰«ææ—¶ï¼Œå¾€å¾€ä¼šå¿½ç•¥CTçš„ç‰¹æ€§ï¼Œå…¶ä¸­å¼ºåº¦ä»¥äº¨æ°å•ä½ï¼ˆHUï¼‰ä¸ºå•ä½æµ‹é‡å¹¶å…·æœ‰é‡è¦çš„ç‰©ç†æ„ä¹‰ã€‚æœ¬æ–‡è´¨ç–‘åœ¨CTæˆåƒä¸­ä½¿ç”¨æ­¤ç±»å¼ºåº¦å¢å¼ºçš„åšæ³•ï¼Œå¹¶è¡¨æ˜å®ƒä»¬å¯èƒ½å¯¼è‡´ä¼ªå½±å’Œæ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹CTçš„ç‰¹å®šå¢å¼ºæŠ€æœ¯ï¼Œç§°ä¸ºéšæœºçª—æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨CTå›¾åƒä¸­å¯ç”¨çš„HUå¼ºåº¦åˆ†å¸ƒã€‚éšæœºçª—æŠ€æœ¯æœ‰åŠ©äºæé«˜å¯¹æ¯”å¢å¼ºçš„ç¨³å¥æ€§ï¼Œå¹¶å¯¹å¯¹æ¯”åº¦è¾ƒå·®æˆ–æ—¶é—´æŠŠæ§å›°éš¾çš„å›¾åƒæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ–¹æ³•æ¶ˆèå’Œåˆ†æï¼Œå¹¶ä¸æœ€æ–°æ›¿ä»£æ–¹æ¡ˆè¿›è¡Œäº†æ¯”è¾ƒï¼Œåœ¨è§£å†³è‚è„è‚¿ç˜¤åˆ†å‰²çš„æŒ‘æˆ˜ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08116v1">PDF</a> 10 pages, 9 figures. This work has been submitted to the IEEE for   possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¯¹æ¯”å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨åŒ»å­¦è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­çš„é‡è¦æ€§ã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨CTå›¾åƒä¸­çš„è‚¿ç˜¤æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯å‡è½»åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚ä¸ºäº†åœ¨æœ‰é™çš„åŒ»å­¦å›¾åƒæ•°æ®é¢†åŸŸå®ç°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè®­ç»ƒæ·±åº¦æ¨¡å‹éœ€è¦ä½¿ç”¨å›¾åƒå¢å¼ºæŠ€æœ¯ã€‚ç„¶è€Œï¼Œé’ˆå¯¹è‡ªç„¶å›¾åƒå¼€å‘çš„å¢å¼ºæ–¹æ³•ç›´æ¥åº”ç”¨äºCTæ‰«ææ—¶ï¼Œå¿½ç•¥äº†CTçš„ç‰¹æ€§ï¼Œå…¶å¼ºåº¦å•ä½ä¸ºéœæ©è²å°”å¾·å•ä½ï¼ˆHUï¼‰ï¼Œå…·æœ‰ç‰©ç†æ„ä¹‰ã€‚æœ¬æ–‡è´¨ç–‘äº†è¿™ç§å¼ºåº¦å¢å¼ºæ–¹æ³•åœ¨CTå½±åƒä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºå…¶å¯èƒ½å¯¼è‡´ä¼ªå½±å’Œæ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹CTçš„ç‰¹å®šå¢å¼ºæŠ€æœ¯â€”â€”éšæœºçª—æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨CTå›¾åƒä¸­å¯ç”¨çš„HUå¼ºåº¦åˆ†å¸ƒã€‚éšæœºçª—å£æŠ€æœ¯å¯ä»¥æé«˜æ¨¡å‹å¯¹å¯¹æ¯”å¢å¼ºçš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¯¹æ¯”åº¦è¿‡ä½æˆ–å›¾åƒè´¨é‡è¾ƒå·®çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹æ–¹æ³•è¿›è¡Œäº†å¤šä¸ªæ•°æ®é›†çš„æ¶ˆèä¸åˆ†æï¼Œå¹¶è¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶é’ˆå¯¹è‚è„è‚¿ç˜¤åˆ†å‰²æå‡ºäº†æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨åŒ»å­¦è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–åŒ»å­¦å›¾åƒåˆ†æä¸­æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨CTå›¾åƒçš„è‚¿ç˜¤æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢ã€‚</li>
<li>åœ¨æœ‰é™çš„åŒ»å­¦å›¾åƒæ•°æ®é¢†åŸŸå®ç°æ¨¡å‹æ³›åŒ–éœ€è¦è®­ç»ƒæ·±åº¦æ¨¡å‹å¹¶ä½¿ç”¨å›¾åƒå¢å¼ºæŠ€æœ¯ã€‚</li>
<li>åº”ç”¨äºCTæ‰«æçš„å¢å¼ºæ–¹æ³•éœ€è¦è€ƒè™‘åˆ°CTçš„ç‰¹æ€§ï¼Œå°¤å…¶æ˜¯å…¶å¼ºåº¦å•ä½ä¸ºéœæ©è²å°”å¾·å•ä½ï¼ˆHUï¼‰ã€‚</li>
<li>é€šç”¨å¼ºåº¦å¢å¼ºæ–¹æ³•å¯èƒ½å¯¼è‡´CTå½±åƒä¸­çš„ä¼ªå½±å’Œæ¨¡å‹æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹CTçš„ç‰¹å®šå¢å¼ºæŠ€æœ¯â€”â€”éšæœºçª—æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨CTå›¾åƒä¸­çš„HUåˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0775995e49529faacd13c7e55fdeb8cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133953&auth_key=1760133953-0-0-c70b8091452a30f57add18d8b9a95081&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cbc92b73cc3b613128fddf8b2dbabf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133960&auth_key=1760133960-0-0-d1fcbfd02d82390b2da8b2baa47888e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10b9ae08170fa32c47be350848fe43ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133967&auth_key=1760133967-0-0-4f04c8007361e981e07049c13ddae7df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42a2ebf95ed1adf57480507385892aab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133973&auth_key=1760133973-0-0-ec843ac2e7e6d2ae81088e19f55372da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7abbd746953f3fc81ab5141528eeb44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133980&auth_key=1760133980-0-0-b3bb43934674fa71acd52ad31fbec8e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RASALoRE-Region-Aware-Spatial-Attention-with-Location-based-Random-Embeddings-for-Weakly-Supervised-Anomaly-Detection-in-Brain-MRI-Scans"><a href="#RASALoRE-Region-Aware-Spatial-Attention-with-Location-based-Random-Embeddings-for-Weakly-Supervised-Anomaly-Detection-in-Brain-MRI-Scans" class="headerlink" title="RASALoRE: Region Aware Spatial Attention with Location-based Random   Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans"></a>RASALoRE: Region Aware Spatial Attention with Location-based Random   Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans</h2><p><strong>Authors:Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan</strong></p>
<p>Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/">https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/</a>. </p>
<blockquote>
<p>åœ¨å¤§è„‘MRIæ‰«æä¸­è¿›è¡Œå¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆWSADï¼‰æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ï¼Œåœ¨æ²¡æœ‰ç²¾ç¡®çš„åƒç´ çº§å¼‚å¸¸æ ‡æ³¨ä¸”åªæœ‰å¼±æ ‡ç­¾ï¼ˆå¦‚åˆ‡ç‰‡çº§ï¼‰å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œèƒ½è¿…é€Ÿå‡†ç¡®åœ°æ£€æµ‹å¤§è„‘å¼‚å¸¸ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RASALoREï¼šåŸºäºä½ç½®æ„ŸçŸ¥ç©ºé—´æ³¨æ„åŠ›çš„ä½ç½®éšæœºåµŒå…¥ï¼ˆRegion Aware Spatial Attention with Location-based Random Embeddingsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µWSADæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ¤åˆ«å¼åŒé‡æç¤ºè°ƒæ•´ï¼ˆDDPTï¼‰æœºåˆ¶ï¼Œå®ƒåŸºäºåˆ‡ç‰‡çº§æ ‡ç­¾ç”Ÿæˆé«˜è´¨é‡ä¼ªå¼±æ©æ¨¡ï¼Œä½œä¸ºç²—ç•¥å®šä½çº¿ç´¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰åŒºåŸŸæ„ŸçŸ¥ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å‰²ç½‘ç»œï¼Œè¯¥ç½‘ç»œä¾èµ–äºåŸºäºå›ºå®šä½ç½®çš„éšæœºåµŒå…¥ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å…³æ³¨å¼‚å¸¸åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„WSADæ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°å°‘äº8ç™¾ä¸‡ã€‚åœ¨BraTS20ã€BraTS21ã€BraTS23å’ŒMSDæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†å…¶æ€§èƒ½æ˜¾è‘—æé«˜ä»¥åŠè®¡ç®—å¤æ‚åº¦çš„æ˜¾è‘—é™ä½ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/">https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08052v1">PDF</a> Accepted in BMVC-2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹å¤§è„‘MRIæ‰«æçš„å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆWSADï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRASALoREçš„æ–°å‹ä¸¤é˜¶æ®µWSADæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆ¤åˆ«å¼åŒæç¤ºè°ƒè°ï¼ˆDDPTï¼‰æœºåˆ¶ç”Ÿæˆé«˜è´¨é‡ä¼ªå¼±æ©è†œï¼Œä½œä¸ºç²—ç•¥å®šä½çº¿ç´¢ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å…·æœ‰åŸºäºå›ºå®šä½ç½®éšæœºåµŒå…¥çš„åŒºåŸŸæ„ŸçŸ¥ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å‰²ç½‘ç»œï¼Œå®ç°å¯¹å¼‚å¸¸åŒºåŸŸçš„æœ‰æ•ˆå…³æ³¨ã€‚è¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰WSADæ–¹æ³•ï¼ŒåŒæ—¶å‚æ•°ä½¿ç”¨é‡å°‘äºå…«ç™¾ä¸‡ã€‚åœ¨BraTS20ã€BraTS21ã€BraTS23å’ŒMSDæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå–å¾—äº†å®è´¨æ€§æ”¹è¿›ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„ä¸»è¦è§è§£ï¼š</p>
<ul>
<li>é’ˆå¯¹å¤§è„‘MRIæ‰«æçš„å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆWSADï¼‰é—®é¢˜æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶RASALoREã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”Ÿæˆä¼ªå¼±æ©è†œä½œä¸ºç²—ç•¥å®šä½çº¿ç´¢ï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨å…·æœ‰åŒºåŸŸæ„ŸçŸ¥ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å‰²ç½‘ç»œã€‚</li>
<li>åˆ©ç”¨åˆ¤åˆ«å¼åŒæç¤ºè°ƒè°ï¼ˆDDPTï¼‰æœºåˆ¶ç”Ÿæˆé«˜è´¨é‡ä¼ªå¼±æ©è†œã€‚</li>
<li>åŸºäºå›ºå®šä½ç½®éšæœºåµŒå…¥çš„åŒºåŸŸæ„ŸçŸ¥ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºæ¨¡å‹æœ‰æ•ˆå…³æ³¨å¼‚å¸¸åŒºåŸŸã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰WSADæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å‚æ•°ä½¿ç”¨é‡å°‘äºå…«ç™¾ä¸‡ï¼ŒåŒæ—¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ€§èƒ½æå‡å’Œè®¡ç®—å¤æ‚åº¦çš„é™ä½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f1d7735c805c2792d62bb95a64f6bcf0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133987&auth_key=1760133987-0-0-bd721a48fba861b5fbec52ab9474ba42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-494f4560263722f91be8845e598b5f17~resize:0:q75.jpg?source=1f5c5e47&expiration=1760133994&auth_key=1760133994-0-0-e7800385c70e398442e80be30c5dd8d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-70580eeeeb83e25ffe7453cf85f90c45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134001&auth_key=1760134001-0-0-30f07fe3e5ead5648d4801c57b5cac05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43ce9c72a8fd0c328c4f42e565867344~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134008&auth_key=1760134008-0-0-7ad16a0c8a3d7f3df0cae441e6fd5f32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MRI-derived-quantification-of-hepatic-vessel-to-volume-ratios-in-chronic-liver-disease-using-a-deep-learning-approach"><a href="#MRI-derived-quantification-of-hepatic-vessel-to-volume-ratios-in-chronic-liver-disease-using-a-deep-learning-approach" class="headerlink" title="MRI-derived quantification of hepatic vessel-to-volume ratios in chronic   liver disease using a deep learning approach"></a>MRI-derived quantification of hepatic vessel-to-volume ratios in chronic   liver disease using a deep learning approach</h2><p><strong>Authors:Alexander Herold, Daniel Sobotka, Lucian Beer, Nina Bastati, Sarah Poetter-Lang, Michael Weber, Thomas Reiberger, Mattias Mandorfer, Georg Semmler, Benedikt Simbrunner, Barbara D. Wichtmann, Sami A. Ba-Ssalamah, Michael Trauner, Ahmed Ba-Ssalamah, Georg Langs</strong></p>
<p>Background: We aimed to quantify hepatic vessel volumes across chronic liver disease stages and healthy controls using deep learning-based magnetic resonance imaging (MRI) analysis, and assess correlations with biomarkers for liver (dys)function and fibrosis&#x2F;portal hypertension.   Methods: We assessed retrospectively healthy controls, non-advanced and advanced chronic liver disease (ACLD) patients using a 3D U-Net model for hepatic vessel segmentation on portal venous phase gadoxetic acid-enhanced 3-T MRI. Total (TVVR), hepatic (HVVR), and intrahepatic portal vein-to-volume ratios (PVVR) were compared between groups and correlated with: albumin-bilirubin (ALBI) and model for end-stage liver disease-sodium (MELD-Na) score, and fibrosis&#x2F;portal hypertension (Fibrosis-4 [FIB-4] score, liver stiffness measurement [LSM], hepatic venous pressure gradient [HVPG], platelet count [PLT], and spleen volume).   Results: We included 197 subjects, aged 54.9 $\pm$ 13.8 years (mean $\pm$ standard deviation), 111 males (56.3%): 35 healthy controls, 44 non-ACLD, and 118 ACLD patients. TVVR and HVVR were highest in controls (3.9; 2.1), intermediate in non-ACLD (2.8; 1.7), and lowest in ACLD patients (2.3; 1.0) ($p \leq 0.001$). PVVR was reduced in both non-ACLD and ACLD patients (both 1.2) compared to controls (1.7) ($p \leq 0.001$), but showed no difference between CLD groups ($p &#x3D; 0.999$). HVVR significantly correlated indirectly with FIB-4, ALBI, MELD-Na, LSM, and spleen volume ($\rho$ ranging from -0.27 to -0.40), and directly with PLT ($\rho &#x3D; 0.36$). TVVR and PVVR showed similar but weaker correlations.   Conclusions: Deep learning-based hepatic vessel volumetry demonstrated differences between healthy liver and chronic liver disease stages and shows correlations with established markers of disease severity. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†ææ¥é‡åŒ–ä¸åŒæ…¢æ€§è‚ç—…é˜¶æ®µå’Œæ­£å¸¸å¯¹ç…§è€…çš„è‚è„è¡€ç®¡ä½“ç§¯ï¼Œå¹¶è¯„ä¼°å…¶ä¸è‚è„ï¼ˆå¼‚å¸¸ï¼‰åŠŸèƒ½åŠçº¤ç»´åŒ–&#x2F;é—¨é™è„‰é«˜å‹çš„ç”Ÿç‰©æ ‡å¿—ç‰©ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬å›é¡¾æ€§åœ°è¯„ä¼°äº†æ­£å¸¸å¯¹ç…§ç»„ã€éè¿›å±•æœŸæ…¢æ€§è‚ç—…æ‚£è€…å’Œè¿›å±•æœŸæ…¢æ€§è‚ç—…æ‚£è€…ï¼Œåœ¨é™è„‰æœŸçš„é—¨è„‰æœŸé’†åŒèƒºé…¸å¢å¼ºçš„3T MRIä¸Šé‡‡ç”¨ä¸‰ç»´U-Netæ¨¡å‹è¿›è¡Œè‚è„è¡€ç®¡åˆ†å‰²ã€‚ç»„é—´æ¯”è¾ƒæ€»è¡€ç®¡ä½“ç§¯æ¯”ï¼ˆTVVRï¼‰ã€è‚è„è¡€ç®¡ä½“ç§¯æ¯”ï¼ˆHVVRï¼‰å’Œè‚å†…é—¨é™è„‰ä¸ä½“ç§¯ä¹‹æ¯”ï¼ˆPVVRï¼‰ï¼Œå¹¶ä¸ä»¥ä¸‹æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒï¼šç™½è›‹ç™½èƒ†çº¢ç´ ï¼ˆALBIï¼‰å’Œæœ«æœŸè‚ç—…æ‚£è€…æ¨¡å‹é’ è¯„åˆ†ï¼ˆMELD-Naï¼‰ã€çº¤ç»´åŒ–æŒ‡æ ‡&#x2F;é—¨é™è„‰é«˜å‹ï¼ˆFibrosis-4æŒ‡æ•°ï¼ˆFIB-4ï¼‰ã€è‚ç¡¬åº¦æµ‹å®šå€¼ï¼ˆLSMï¼‰ã€è‚é™è„‰å‹åŠ›æ¢¯åº¦ï¼ˆHVPGï¼‰ã€è¡€å°æ¿è®¡æ•°ï¼ˆPLTï¼‰å’Œè„¾è„ä½“ç§¯ï¼‰ã€‚</p>
<p>ç»“æœï¼šå…±çº³å…¥ç ”ç©¶å¯¹è±¡197äººï¼Œå¹³å‡å¹´é¾„ä¸ºÂ±æ ‡å‡†åå·®çš„54.9å²ï¼Œç”·æ€§å 56.3%ï¼ŒåŒ…æ‹¬æ­£å¸¸å¯¹ç…§ç»„35äººï¼ŒéACLDæ‚£è€…44äººï¼ŒACLDæ‚£è€…118äººã€‚TVVRå’ŒHVVRåœ¨å¯¹ç…§ç»„ä¸­æœ€é«˜ï¼ˆåˆ†åˆ«ä¸º3.9å’Œ2.1ï¼‰ï¼Œåœ¨éACLDæ‚£è€…ä¸­å¤„äºä¸­ç­‰æ°´å¹³ï¼ˆåˆ†åˆ«ä¸º2.8å’Œ1.7ï¼‰ï¼Œè€Œåœ¨ACLDæ‚£è€…ä¸­æœ€ä½ï¼ˆåˆ†åˆ«ä¸º2.3å’Œ1.0ï¼‰ï¼ˆpâ‰¤0.001ï¼‰ã€‚PVVRåœ¨éACLDå’ŒACLDæ‚£è€…ä¸­å‡ä½äºå¯¹ç…§ç»„ï¼ˆåˆ†åˆ«ä¸º1.2å’Œ1.7ï¼‰ï¼ˆpâ‰¤0.001ï¼‰ï¼Œä½†ä¸¤ç»„é—´æ— æ˜¾è‘—å·®å¼‚ï¼ˆp&#x3D;0.999ï¼‰ã€‚HVVRä¸FIB-4ã€ALBIã€MELD-Naã€LSMå’Œè„¾è„ä½“ç§¯çš„ç›¸å…³æ€§é—´æ¥ä¸”æ˜¾è‘—ï¼Œå…¶ç›¸å…³æ€§å¼ºåº¦ä»Ï&#x3D;-0.27åˆ°Ï&#x3D;-0.4å˜åŒ–ä¸ç­‰ï¼›è€Œä¸PLTå‘ˆç›´æ¥æ˜¾è‘—ç›¸å…³æ€§ï¼Œå…¶Ïä¸º0.36ã€‚TVVRå’ŒPVVRæ˜¾ç¤ºäº†ç›¸ä¼¼ä½†è¾ƒå¼±çš„å…³è”æ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08039v1">PDF</a> ^Alexander Herold and Daniel Sobotka share first-authorship</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è‚è„è¡€ç®¡ä½“ç§¯æµ‹é‡æ˜¾ç¤ºï¼Œå¥åº·äººç¾¤ä¸ä¸åŒé˜¶æ®µçš„æ…¢æ€§è‚ç—…æ‚£è€…ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œå¹¶ä¸ç–¾ç—…ä¸¥é‡ç¨‹åº¦çš„æ ‡å¿—å­˜åœ¨ç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œè‚è„è¡€ç®¡ä½“ç§¯çš„é‡åŒ–åˆ†æï¼Œè¯„ä¼°æ…¢æ€§è‚ç—…é˜¶æ®µä¸å¥åº·å¯¹ç…§è€…çš„å·®å¼‚ã€‚</li>
<li>é‡‡ç”¨3D U-Netæ¨¡å‹å¯¹è‚è„è¡€ç®¡è¿›è¡Œåˆ†å‰²ï¼Œåˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®è¿›è¡Œåˆ†æã€‚</li>
<li>æ€»è¡€ç®¡ä½“ç§¯æ¯”ï¼ˆTVVRï¼‰ã€è‚è„è¡€ç®¡ä½“ç§¯æ¯”ï¼ˆHVVRï¼‰å’Œè‚å†…é—¨é™è„‰ä½“ç§¯æ¯”ï¼ˆPVVRï¼‰åœ¨ä¸åŒç»„é—´å­˜åœ¨å·®å¼‚ã€‚</li>
<li>HVVRä¸è‚çº¤ç»´åŒ–ã€è‚åŠŸèƒ½æŒ‡æ ‡å­˜åœ¨æ˜¾è‘—è´Ÿç›¸å…³ï¼Œä¸è¡€å°æ¿è®¡æ•°å­˜åœ¨æ­£ç›¸å…³ã€‚</li>
<li>TVVRå’ŒPVVRä¸ç–¾ç—…ä¸¥é‡ç¨‹åº¦æŒ‡æ ‡çš„ç›¸å…³æ€§ç›¸å¯¹è¾ƒå¼±ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è‚è„è¡€ç®¡ä½“ç§¯æµ‹é‡æ–¹é¢çš„åº”ç”¨æœ‰åŠ©äºç†è§£æ…¢æ€§è‚ç—…çš„ç—…ç†ç”Ÿç†æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2f0c5409766c4aa7958e7024cb1c1159~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134015&auth_key=1760134015-0-0-e054b5ea467b41c3c12721b6594ebb98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-966bbe9a5e57f1aac5c82353018bd67c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134022&auth_key=1760134022-0-0-714c85c6f2a7f4b6c77dbb740447878a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-efca1e3488b682b0c00e249b10828e6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134028&auth_key=1760134028-0-0-020b6255a4a3ec64aa98d44f5f219943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc9c1668621fc2c168fee732ec3292f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134035&auth_key=1760134035-0-0-86975cc8c85d7f37b7859e2b17a2bf34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Demystifying-Deep-Learning-based-Brain-Tumor-Segmentation-with-3D-UNets-and-Explainable-AI-XAI-A-Comparative-Analysis"><a href="#Demystifying-Deep-Learning-based-Brain-Tumor-Segmentation-with-3D-UNets-and-Explainable-AI-XAI-A-Comparative-Analysis" class="headerlink" title="Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets   and Explainable AI (XAI): A Comparative Analysis"></a>Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets   and Explainable AI (XAI): A Comparative Analysis</h2><p><strong>Authors:Ming Jie Ong, Sze Yinn Ung, Sim Kuan Goh, Jimmy Y. Zhong</strong></p>
<p>The current study investigated the use of Explainable Artificial Intelligence (XAI) to improve the accuracy of brain tumor segmentation in MRI images, with the goal of assisting physicians in clinical decision-making. The study focused on applying UNet models for brain tumor segmentation and using the XAI techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization to enhance the understanding of these models. Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet (AttUNet) - were evaluated to identify the best-performing model. XAI was employed with the aims of clarifying model decisions and increasing physiciansâ€™ trust in these models. We compared the performance of two UNet variants (ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM and attention-based visualization. Using the latest computer hardware, we trained and validated each model using the Adam optimizer and assessed their performance with respect to: (i) training, validation, and inference times, (ii) segmentation similarity coefficients and loss functions, and (iii) classification performance. Notably, during the final testing phase, ResUNet outperformed the other models with respect to Dice and Jaccard similarity scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided visuospatial insights into the tumor subregions each UNet model focused on while attention-based visualization provided valuable insights into the working mechanisms of AttUNetâ€™s attention modules. These results demonstrated ResUNet as the best-performing model and we conclude by recommending its use for automated brain tumor segmentation in future clinical assessments. Our source code and checkpoint are available at <a target="_blank" rel="noopener" href="https://github.com/ethanong98/MultiModel-XAI-Brats2020">https://github.com/ethanong98/MultiModel-XAI-Brats2020</a> </p>
<blockquote>
<p>å½“å‰ç ”ç©¶æ—¨åœ¨åˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æé«˜MRIå›¾åƒä¸­è„‘è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œä»è€Œå¸®åŠ©åŒ»ç”Ÿè¿›è¡Œä¸´åºŠå†³ç­–ã€‚è¯¥ç ”ç©¶é‡ç‚¹æ˜¯å°†UNetæ¨¡å‹åº”ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰å’ŒåŸºäºæ³¨æ„åŠ›çš„å¯è§†åŒ–ç­‰XAIæŠ€æœ¯ï¼Œä»¥æé«˜å¯¹è¿™äº›æ¨¡å‹çš„ç†è§£ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§æ·±åº¦å­¦ä¹ æ¨¡å‹â€”â€”UNetã€æ®‹å·®UNetï¼ˆResUNetï¼‰å’Œæ³¨æ„åŠ›UNetï¼ˆAttUNetï¼‰â€”â€”ä»¥ç¡®å®šè¡¨ç°æœ€ä½³çš„æ¨¡å‹ã€‚ä½¿ç”¨XAIçš„ç›®çš„æ˜¯é˜æ˜æ¨¡å‹å†³ç­–ï¼Œå¢åŠ åŒ»ç”Ÿå¯¹è¿™äº›æ¨¡å‹çš„ä¿¡ä»»ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§UNetå˜ä½“ï¼ˆResUNetå’ŒAttUNetï¼‰ä¸ä¼ ç»ŸUNetåœ¨BraTS2020å…¬å…±æ•°æ®é›†ä¸Šåˆ†å‰²è„‘è‚¿ç˜¤çš„æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨Grad-CAMå’ŒåŸºäºæ³¨æ„åŠ›çš„å¯è§†åŒ–åˆ†ææ¨¡å‹é¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€æ–°çš„è®¡ç®—æœºç¡¬ä»¶ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨è®­ç»ƒå’ŒéªŒè¯æ¯ä¸ªæ¨¡å‹ï¼Œå¹¶å°±å…¶ä»¥ä¸‹æ–¹é¢è¯„ä¼°å…¶æ€§èƒ½ï¼šï¼ˆiï¼‰è®­ç»ƒã€éªŒè¯å’Œæ¨ç†æ—¶é—´ï¼›ï¼ˆiiï¼‰åˆ†å‰²ç›¸ä¼¼åº¦ç³»æ•°å’ŒæŸå¤±å‡½æ•°ï¼›ï¼ˆiiiï¼‰åˆ†ç±»æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æœ€ç»ˆæµ‹è¯•é˜¶æ®µï¼ŒResUNetåœ¨Diceå’ŒJaccardç›¸ä¼¼åº¦å¾—åˆ†ä»¥åŠå‡†ç¡®æ€§ã€å¬å›ç‡å’ŒF1å¾—åˆ†æ–¹é¢è¡¨ç°å‡ºæ¯”å…¶ä»–æ¨¡å‹æ›´ä¼˜ç§€çš„æ€§èƒ½ã€‚Grad-CAMæä¾›äº†å…³äºæ¯ä¸ªUNetæ¨¡å‹å…³æ³¨çš„è‚¿ç˜¤äºšåŒºçš„è§†è§‰ç©ºé—´è§è§£ï¼Œè€ŒåŸºäºæ³¨æ„åŠ›çš„å¯è§†åŒ–åˆ™æä¾›äº†æœ‰å…³AttUNetæ³¨æ„åŠ›æ¨¡å—å·¥ä½œæœºåˆ¶çš„å®è´µè§è§£ã€‚è¿™äº›ç»“æœè¯æ˜äº†ResUNetæ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®æœªæ¥åœ¨ä¸´åºŠè¯„ä¼°ä¸­ä½¿ç”¨ResUNetè¿›è¡Œè‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ethanong98/MultiModel-XAI-Brats2020%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ethanong98/MultiModel-XAI-Brats2020æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07785v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶è¿ç”¨å¯è§£é‡Šçš„çš„äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯æ¥æå‡MRIå›¾åƒä¸­è„‘è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œæ—¨åœ¨å¸®åŠ©åŒ»ç”Ÿè¿›è¡Œä¸´åºŠå†³ç­–ã€‚ç ”ç©¶èšç„¦äºä½¿ç”¨UNetæ¨¡å‹è¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå¹¶ç»“åˆGrad-CAMå’Œæ³¨æ„åŠ›å¯è§†åŒ–ç­‰XAIæŠ€æœ¯ï¼Œä»¥å¢å¼ºå¯¹æ¨¡å‹çš„ç†è§£ã€‚å¯¹æ¯”äº†UNetã€ResUNetå’ŒAttUNetä¸‰ç§æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼ŒXAIçš„å¼•å…¥æ—¨åœ¨æ¾„æ¸…æ¨¡å‹å†³ç­–ï¼Œå¢åŠ åŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»ã€‚æœ€ç»ˆæµ‹è¯•é˜¶æ®µï¼ŒResUNetåœ¨Diceå’ŒJaccardç›¸ä¼¼åº¦å¾—åˆ†ã€å‡†ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. æœ¬ç ”ç©¶ä½¿ç”¨Explainable Artificial Intelligence (XAI)ä»¥æé«˜MRIå›¾åƒä¸­è„‘è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚<br>     2. ç ”ç©¶èšç„¦äºåº”ç”¨UNetæ¨¡å‹è¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå¹¶ä½¿ç”¨Grad-CAMå’Œæ³¨æ„åŠ›å¯è§†åŒ–æŠ€æœ¯ä»¥å¢å¼ºå¯¹æ¨¡å‹çš„ç†è§£ã€‚<br>     3. å¯¹æ¯”äº†ä¸‰ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆUNetã€ResUNetå’ŒAttUNetï¼‰çš„æ€§èƒ½ã€‚<br>     4. XAIçš„å¼•å…¥æ—¨åœ¨å¸®åŠ©åŒ»ç”Ÿæ›´å¥½åœ°ç†è§£æ¨¡å‹å†³ç­–ï¼Œå¹¶å¢åŠ ä»–ä»¬å¯¹æ¨¡å‹çš„ä¿¡ä»»ã€‚<br>     5. æœ€ç»ˆæµ‹è¯•è¡¨æ˜ï¼ŒResUNetåœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚<br>     6. Grad-CAMæä¾›äº†å…³äºè‚¿ç˜¤å­åŒºåŸŸçš„ä¿¡æ¯ï¼Œè¿™äº›åŒºåŸŸæ˜¯æ¯ä¸ªUNetæ¨¡å‹æ‰€å…³æ³¨çš„é‡ç‚¹ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e1348b6b5d10550a8b11d95e035f2bc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134043&auth_key=1760134043-0-0-b0005fc88fb5d93ebff53aadb5494b5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ada5b7b77bf8b8621f3d7b45f3b6dbf4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134050&auth_key=1760134050-0-0-aeba971fc83ca00aacb2477772f815eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51024248f33c147baf9cbb89a4b2c605~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134057&auth_key=1760134057-0-0-301bef9e3dd29ff810036083987b3a05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TCIP-Threshold-Controlled-Iterative-Pyramid-Network-for-Deformable-Medical-Image-Registration"><a href="#TCIP-Threshold-Controlled-Iterative-Pyramid-Network-for-Deformable-Medical-Image-Registration" class="headerlink" title="TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable   Medical Image Registration"></a>TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable   Medical Image Registration</h2><p><strong>Authors:Heming Wu, Di Wang, Tai Ma, Peng Zhao, Yubin Xiao, Zhongke Wu, Xing-Ce Wang, Chuang Li, Xuan Wu, You Zhou</strong></p>
<p>Although pyramid networks have demonstrated superior performance in deformable medical image registration, their decoder architectures are inherently prone to propagating and accumulating anatomical structure misalignments. Moreover, most existing models do not adaptively determine the number of iterations for optimization under varying deformation requirements across images, resulting in either premature termination or excessive iterations that degrades registration accuracy. To effectively mitigate the accumulation of anatomical misalignments, we propose the Feature-Enhanced Residual Module (FERM) as the core component of each decoding layer in the pyramid network. FERM comprises three sequential blocks that extract anatomical semantic features, learn to suppress irrelevant features, and estimate the final deformation field, respectively. To adaptively determine the number of iterations for varying images, we propose the dual-stage Threshold-Controlled Iterative (TCI) strategy. In the first stage, TCI assesses registration stability and with asserted stability, it continues with the second stage to evaluate convergence. We coin the model that integrates FERM and TCI as Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP outperforms the state-of-the-art (SOTA) registration networks in terms of accuracy, while maintaining comparable inference speed and a compact model parameter size. Finally, we assess the generalizability of FERM and TCI by integrating them with existing registration networks and further conduct ablation studies to validate the effectiveness of these two proposed methods. </p>
<blockquote>
<p>å°½ç®¡é‡‘å­—å¡”ç½‘ç»œåœ¨å¯å˜å½¢åŒ»å­¦å›¾åƒæ³¨å†Œä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶è§£ç å™¨æ¶æ„å›ºæœ‰çš„ä¼ æ’­å’Œç´¯ç§¯è§£å‰–ç»“æ„é”™ä½çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¸èƒ½æ ¹æ®å›¾åƒä¹‹é—´ä¸åŒçš„å˜å½¢è¦æ±‚è‡ªé€‚åº”åœ°ç¡®å®šä¼˜åŒ–è¿­ä»£æ¬¡æ•°ï¼Œå¯¼è‡´è¿‡æ—©ç»ˆæ­¢æˆ–è¿‡åº¦è¿­ä»£ï¼Œä»è€Œé™ä½äº†æ³¨å†Œå‡†ç¡®æ€§ã€‚ä¸ºäº†æœ‰æ•ˆç¼“è§£è§£å‰–ç»“æ„é”™ä½çš„ç´¯ç§¯ï¼Œæˆ‘ä»¬æå‡ºäº†ç‰¹å¾å¢å¼ºæ®‹å·®æ¨¡å—ï¼ˆFERMï¼‰ä½œä¸ºé‡‘å­—å¡”ç½‘ç»œä¸­æ¯ä¸ªè§£ç å±‚çš„æ ¸å¿ƒç»„ä»¶ã€‚FERMåŒ…å«ä¸‰ä¸ªè¿ç»­å—ï¼Œåˆ†åˆ«æå–è§£å‰–è¯­ä¹‰ç‰¹å¾ã€å­¦ä¹ æŠ‘åˆ¶æ— å…³ç‰¹å¾å’Œä¼°è®¡æœ€ç»ˆå˜å½¢åœºã€‚ä¸ºäº†è‡ªé€‚åº”åœ°ç¡®å®šä¸åŒå›¾åƒçš„è¿­ä»£æ¬¡æ•°ï¼Œæˆ‘ä»¬æå‡ºäº†åŒé˜¶æ®µé˜ˆå€¼æ§åˆ¶è¿­ä»£ï¼ˆTCIï¼‰ç­–ç•¥ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒTCIè¯„ä¼°æ³¨å†Œç¨³å®šæ€§ï¼Œå¹¶åœ¨ç¨³å®šæ€§å¾—åˆ°ç¡®è®¤åï¼Œè¿›å…¥ç¬¬äºŒé˜¶æ®µè¯„ä¼°æ”¶æ•›æ€§ã€‚æˆ‘ä»¬å°†é›†æˆäº†FERMå’ŒTCIçš„æ¨¡å‹ç§°ä¸ºé˜ˆå€¼æ§åˆ¶é‡‘å­—å¡”ï¼ˆTCIPï¼‰ã€‚åœ¨ä¸‰ä¸ªå…¬å…±è„‘MRIæ•°æ®é›†å’Œä¸€ä¸ªè…¹éƒ¨CTæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTCIPåœ¨å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ³¨å†Œç½‘ç»œï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“å¿«çš„æ¨ç†é€Ÿåº¦å’Œç´§å‡‘çš„æ¨¡å‹å‚æ•°å¤§å°ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†FERMå’ŒTCIä¸ç°æœ‰æ³¨å†Œç½‘ç»œé›†æˆæ¥è¯„ä¼°å®ƒä»¬çš„é€šç”¨æ€§ï¼Œå¹¶è¿›è¡Œäº†å‰”é™¤ç ”ç©¶ä»¥éªŒè¯è¿™ä¸¤ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07666v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒæ³¨å†Œçš„æ”¹è¿›æ–¹æ³•ï¼ŒåŒ…æ‹¬ç”¨äºé‡‘å­—å¡”ç½‘ç»œè§£ç å±‚çš„ç‰¹å¾å¢å¼ºæ®‹å·®æ¨¡å—ï¼ˆFERMï¼‰å’ŒåŒé˜¶æ®µé˜ˆå€¼æ§åˆ¶è¿­ä»£ï¼ˆTCIï¼‰ç­–ç•¥ã€‚FERMç”¨äºå‡è½»è§£å‰–ç»“æ„é”™ä½ç´¯ç§¯çš„é—®é¢˜ï¼Œè€ŒTCIç­–ç•¥åˆ™èƒ½è‡ªé€‚åº”ç¡®å®šä¸åŒå›¾åƒä¼˜åŒ–æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ã€‚æ–°æ–¹æ³•åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨ä¿è¯æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹å‚æ•°å¤§å°çš„åŒæ—¶ï¼Œæé«˜äº†æ³¨å†Œç½‘ç»œçš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘å­—å¡”ç½‘ç»œåœ¨å¯å˜å½¢åŒ»å­¦å›¾åƒæ³¨å†Œä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶è§£ç æ¶æ„å®¹æ˜“ä¼ æ’­å’Œç´¯ç§¯è§£å‰–ç»“æ„é”™ä½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¤§å¤šä¸èƒ½è‡ªé€‚åº”ç¡®å®šä¸åŒå›¾åƒä¼˜åŒ–æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ï¼Œå¯¼è‡´è¿‡æ—©ç»ˆæ­¢æˆ–è¿‡åº¦è¿­ä»£ï¼Œå½±å“æ³¨å†Œå‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºç‰¹å¾å¢å¼ºæ®‹å·®æ¨¡å—ï¼ˆFERMï¼‰ï¼Œä½œä¸ºé‡‘å­—å¡”ç½‘ç»œæ¯ä¸ªè§£ç å±‚çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå‡è½»è§£å‰–ç»“æ„é”™ä½ç´¯ç§¯çš„é—®é¢˜ã€‚</li>
<li>FERMåŒ…å«ä¸‰ä¸ªé¡ºåºå—ï¼Œåˆ†åˆ«æå–è§£å‰–è¯­ä¹‰ç‰¹å¾ã€å­¦ä¹ æŠ‘åˆ¶æ— å…³ç‰¹å¾å’Œä¼°è®¡æœ€ç»ˆå˜å½¢åœºã€‚</li>
<li>æå‡ºåŒé˜¶æ®µé˜ˆå€¼æ§åˆ¶è¿­ä»£ï¼ˆTCIï¼‰ç­–ç•¥ï¼Œè‡ªé€‚åº”ç¡®å®šä¸åŒå›¾åƒçš„è¿­ä»£æ¬¡æ•°ã€‚</li>
<li>TCIç­–ç•¥åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µè¯„ä¼°æ³¨å†Œç¨³å®šæ€§ï¼Œç¬¬äºŒé˜¶æ®µè¯„ä¼°æ”¶æ•›æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆFERMå’ŒTCIçš„é˜ˆå€¼æ§åˆ¶è¿­ä»£é‡‘å­—å¡”ï¼ˆTCIPï¼‰æ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ³¨å†Œç½‘ç»œï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹å‚æ•°å¤§å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-df7cd6795c572ebbb3b23b6185cc85a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134065&auth_key=1760134065-0-0-14912bc25eed288df1a5a23bafc54f79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebae503596b9b87bda24b26427243ffc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134072&auth_key=1760134072-0-0-57ae1526170e74e480216a6c7d1ae3f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c19820090006f7b574536d8004d09f94~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134079&auth_key=1760134079-0-0-2c6d4d74f5d279858ceb0118b2f0d50f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37dd05c7c8794fba032bbc94ff0554fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134086&auth_key=1760134086-0-0-f25593a8918ecb89c3470702c28281a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e36cbe10e52583545005ce43a533389b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134093&auth_key=1760134093-0-0-93cc37e88be32a22ac86493fade420c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c4868ac3886b207fb8e5139cd8a0548f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134100&auth_key=1760134100-0-0-0c6dfd69659944af216cf8c4b4026576&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Denoising-Framework-for-Real-World-Ultra-Low-Dose-Lung-CT-Images-Based-on-an-Image-Purification-Strategy"><a href="#A-Denoising-Framework-for-Real-World-Ultra-Low-Dose-Lung-CT-Images-Based-on-an-Image-Purification-Strategy" class="headerlink" title="A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based   on an Image Purification Strategy"></a>A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based   on an Image Purification Strategy</h2><p><strong>Authors:Guoliang Gong, Man Yu</strong></p>
<p>Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/MonkeyDadLufy/flow-matching">https://github.com/MonkeyDadLufy/flow-matching</a>. </p>
<blockquote>
<p>è¶…ä½å‰‚é‡CTï¼ˆuLDCTï¼‰æ˜¾è‘—é™ä½äº†è¾å°„æš´éœ²ï¼Œä½†å¼•å…¥äº†ä¸¥é‡çš„å™ªå£°å’Œä¼ªå½±ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯¼è‡´uLDCTä¸å¸¸è§„å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒå¯¹ä¹‹é—´å‡ºç°è¾ƒå¤§çš„ç©ºé—´é”™ä½ã€‚è¿™ç»™ç›´æ¥åº”ç”¨ç°æœ‰åˆæˆå™ªå£°æˆ–å¯¹é½æ•°æ®è®­ç»ƒçš„é™å™ªç½‘ç»œå¸¦æ¥äº†æŒ‘æˆ˜ã€‚é’ˆå¯¹uLDCTé™å™ªä¸­çš„è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå›¾åƒå‡€åŒ–ï¼ˆIPï¼‰ç­–ç•¥çš„åˆ›æ–°é™å™ªæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†çœŸå®çš„ä¸´åºŠuLDCTè‚ºéƒ¨æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒå‡€åŒ–ç­–ç•¥ï¼Œç”Ÿæˆç»“æ„å¯¹é½çš„uLDCT-NDCTå›¾åƒå¯¹ï¼Œä¸ºç½‘ç»œè®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„æ•°æ®åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†é¢‘åŸŸæµåŒ¹é…ï¼ˆFFMï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸IPç­–ç•¥ååŒå·¥ä½œï¼Œå‡ºè‰²åœ°ä¿ç•™äº†å»å™ªå›¾åƒçš„è§£å‰–ç»“æ„å®Œæ•´æ€§ã€‚åœ¨çœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„IPç­–ç•¥æ˜¾è‘—æé«˜äº†å¤šä¸ªä¸»æµé™å™ªæ¨¡å‹åœ¨uLDCTä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„FFMæ¨¡å‹ä¸IPç­–ç•¥ç›¸ç»“åˆï¼Œåœ¨è§£å‰–ç»“æ„ä¿ç•™æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆSOTAï¼‰ã€‚æœ¬ç ”ç©¶ä¸ºè§£å†³çœŸå®ä¸–ç•ŒuLDCTé™å™ªä¸­çš„æ•°æ®ä¸åŒ¹é…é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MonkeyDadLufy/flow-matching%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MonkeyDadLufy/flow-matchingè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07492v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾åƒå‡€åŒ–ç­–ç•¥çš„é™å™ªæ¡†æ¶ï¼Œä»¥è§£å†³è¶…ä½å‰‚é‡CTï¼ˆuLDCTï¼‰å›¾åƒä¸­çš„å™ªå£°å’Œä¼ªå½±é—®é¢˜ã€‚é€šè¿‡æ„å»ºçœŸå®ä¸´åºŠuLDCTè‚ºéƒ¨æ•°æ®é›†å’Œé‡‡ç”¨å›¾åƒå‡€åŒ–ç­–ç•¥ç”Ÿæˆç»“æ„å¯¹é½çš„uLDCT-NDCTå›¾åƒå¯¹ï¼Œä¸ºç½‘ç»œè®­ç»ƒæä¾›é«˜è´¨é‡æ•°æ®åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºé¢‘åŸŸæµåŒ¹é…æ¨¡å‹ï¼Œä¸å›¾åƒå‡€åŒ–ç­–ç•¥ååŒå·¥ä½œï¼Œä¼˜ç§€åœ°ä¿ç•™äº†è§£å™ªå›¾åƒçš„è§£å‰–ç»“æ„å®Œæ•´æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æé«˜ä¸»æµé™å™ªæ¨¡å‹åœ¨uLDCTä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯é¢‘åŸŸæµåŒ¹é…æ¨¡å‹ç»“åˆå›¾åƒå‡€åŒ–ç­–ç•¥åœ¨è§£å‰–ç»“æ„ä¿ç•™æ–¹é¢è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>uLDCTæ˜¾è‘—å‡å°‘è¾å°„æš´éœ²ï¼Œä½†å¼•å…¥ä¸¥é‡å™ªå£°å’Œä¼ªå½±ã€‚</li>
<li>uLDCTä¸NDCTå›¾åƒå¯¹ä¹‹é—´å­˜åœ¨ç©ºé—´ä¸å¯¹å‡†ï¼Œç»™ç°æœ‰é™å™ªç½‘ç»œçš„ç›´æ¥åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåŸºäºå›¾åƒå‡€åŒ–ç­–ç•¥çš„é™å™ªæ¡†æ¶ï¼Œæ„å»ºçœŸå®ä¸´åºŠuLDCTè‚ºéƒ¨æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥é¢‘åŸŸæµåŒ¹é…æ¨¡å‹ï¼Œä¸å›¾åƒå‡€åŒ–ç­–ç•¥ååŒï¼Œä¼˜ç§€åœ°ä¿ç•™è§£å™ªå›¾åƒçš„è§£å‰–ç»“æ„å®Œæ•´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œå›¾åƒå‡€åŒ–ç­–ç•¥æ˜¾è‘—æé«˜ä¸»æµé™å™ªæ¨¡å‹åœ¨uLDCTä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆå›¾åƒå‡€åŒ–ç­–ç•¥å’Œé¢‘åŸŸæµåŒ¹é…æ¨¡å‹è¾¾åˆ°æœ€ä½³æ•ˆæœï¼Œåœ¨è§£å‰–ç»“æ„ä¿ç•™æ–¹é¢ä¸ºè¡Œä¸šæ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-30205cbcb17f9a23bf678997f9f6f4a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134107&auth_key=1760134107-0-0-17011393a6da659f8aad19ca97d5d3c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-031afa7e22350fb7e098885b936f8a14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134114&auth_key=1760134114-0-0-964647de99fece5e6835cda321a23a2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35cb7e17076619c8f510270b7c757211~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134121&auth_key=1760134121-0-0-dd3ead305fe5e3a2cc3e450801e687ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e682f9b498a8dc4418e83f5c2b2aa71c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134128&auth_key=1760134128-0-0-c466f2f716373dfd0d0a99877467a818&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f9257e51d861c95c777b8ea5d1405c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134134&auth_key=1760134134-0-0-9fa3a19c026b445fd02c1bab6fe13028&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a09a249e5de3de82e8096ed77062a4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134141&auth_key=1760134141-0-0-ed5204513234b7f183b820b1778d152a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5a580609045785a9bedfae3fb5edf5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134148&auth_key=1760134148-0-0-a82fb75dc9ceb1fa0e53ad7eabe6f075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-We-Won-BraTS-SSA-2025-Brain-Tumor-Segmentation-in-the-Sub-Saharan-African-Population-Using-Segmentation-Aware-Data-Augmentation-and-Model-Ensembling"><a href="#How-We-Won-BraTS-SSA-2025-Brain-Tumor-Segmentation-in-the-Sub-Saharan-African-Population-Using-Segmentation-Aware-Data-Augmentation-and-Model-Ensembling" class="headerlink" title="How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan   African Population Using Segmentation-Aware Data Augmentation and Model   Ensembling"></a>How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan   African Population Using Segmentation-Aware Data Augmentation and Model   Ensembling</h2><p><strong>Authors:Claudia Takyi Ankomah, Livingstone Eli Ayivor, Ireneaus Nyame, Leslie Wambo, Patrick Yeboah Bonsu, Aondona Moses Iorumbur, Raymond Confidence, Toufiq Musah</strong></p>
<p>Brain tumors, particularly gliomas, pose significant chall-enges due to their complex growth patterns, infiltrative nature, and the variability in brain structure across individuals, which makes accurate diagnosis and monitoring difficult. Deep learning models have been developed to accurately delineate these tumors. However, most of these models were trained on relatively homogenous high-resource datasets, limiting their robustness when deployed in underserved regions. In this study, we performed segmentation-aware offline data augmentation on the BraTS-Africa dataset to increase the data sample size and diversity to enhance generalization. We further constructed an ensemble of three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to leverage their complementary strengths. Our best-performing model, MedNeXt, was trained on 1000 epochs and achieved the highest average lesion-wise dice and normalized surface distance scores of 0.86 and 0.81 respectively. However, the ensemble model trained for 500 epochs produced the most balanced segmentation performance across the tumour subregions. This work demonstrates that a combination of advanced augmentation and model ensembling can improve segmentation accuracy and robustness on diverse and underrepresented datasets. Code available at: <a target="_blank" rel="noopener" href="https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti">https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti</a> </p>
<blockquote>
<p>è„‘è‚¿ç˜¤ï¼Œç‰¹åˆ«æ˜¯èƒ¶è´¨ç˜¤ï¼Œç”±äºå…¶å¤æ‚çš„ç”Ÿé•¿æ¨¡å¼ã€æµ¸æ¶¦æ€§ä»¥åŠè„‘ç»“æ„åœ¨ä¸ªä½“é—´çš„å·®å¼‚ï¼Œç»™å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹å·²è¢«å¼€å‘å‡ºæ¥ç²¾å‡†åœ°å‹¾ç”»è¿™äº›è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹æ˜¯åœ¨ç›¸å¯¹å‡åŒ€çš„é«˜èµ„æºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™åœ¨éƒ¨ç½²åˆ°æœåŠ¡ä¸è¶³çš„åŒºåŸŸå†…æ—¶ï¼Œé™åˆ¶äº†å…¶ç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹BraTS-Africaæ•°æ®é›†è¿›è¡Œäº†åˆ†å‰²æ„ŸçŸ¥çš„ç¦»çº¿æ•°æ®å¢å¼ºï¼Œä»¥å¢åŠ æ•°æ®æ ·æœ¬é‡å’Œå¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†ä¸‰ç§ä¸åŒæ¶æ„çš„é›†åˆæ¨¡å‹ï¼ŒåŒ…æ‹¬MedNeXtã€SegMambaå’ŒResidual-Encoder U-Netï¼Œä»¥åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹MedNeXtç»è¿‡1000ä¸ªå‘¨æœŸçš„è®­ç»ƒï¼Œè·å¾—äº†æœ€é«˜çš„å¹³å‡ç—…å˜çº§åˆ«çš„Diceç³»æ•°å’Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»å¾—åˆ†ï¼Œåˆ†åˆ«ä¸º0.86å’Œ0.81ã€‚ç„¶è€Œï¼Œç»è¿‡500ä¸ªå‘¨æœŸè®­ç»ƒçš„é›†æˆæ¨¡å‹åœ¨è‚¿ç˜¤äºšåŒºçš„åˆ†å‰²æ€§èƒ½ä¸Šè¡¨ç°æœ€ä¸ºå‡è¡¡ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå…ˆè¿›çš„å¢å¼ºæŠ€æœ¯ä¸æ¨¡å‹é›†æˆç›¸ç»“åˆï¼Œå¯ä»¥æé«˜å¤šæ ·æ€§å’Œä»£è¡¨æ€§ä¸è¶³çš„æ•°æ®é›†çš„åˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti">https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03568v2">PDF</a> Brain Tumor Segmentation Challenge, International Medical Image   Computing and Computer Assisted Intervention (MICCAI) Conference, 11 Pages, 2   Figures, 2 Tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é’ˆå¯¹éæ´²æ•°æ®é›†BraTS-Africaè¿›è¡Œçº¿ä¸‹æ•°æ®å¢å¼ºï¼Œå¢åŠ æ ·æœ¬æ•°é‡å’Œå¤šæ ·æ€§ä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ç»“åˆä¸‰ç§ä¸åŒæ¶æ„çš„æ¨¡å‹ï¼Œå®ç°è‚¿ç˜¤åˆ†å‰²çš„ç²¾å‡†è¯Šæ–­ã€‚å…¶ä¸­MedNeXtæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå¹³å‡ç—…å˜çº§åˆ«çš„Diceç³»æ•°å’Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»å¾—åˆ†åˆ†åˆ«ä¸º0.86å’Œ0.81ã€‚åŒæ—¶ï¼Œé€šè¿‡æ¨¡å‹é›†æˆæé«˜äº†è‚¿ç˜¤å„å­åŒºåŸŸçš„åˆ†å‰²æ€§èƒ½ã€‚ç ”ç©¶è¯æ˜äº†é«˜çº§æ•°æ®å¢å¼ºå’Œæ¨¡å‹é›†æˆçš„ç»„åˆèƒ½æé«˜åˆ†å‰²ç²¾åº¦å’Œåœ¨å¤šæ ·æ€§å’Œä»£è¡¨æ€§ä¸è¶³çš„æ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤çš„å¤æ‚ç”Ÿé•¿æ¨¡å¼ã€æµ¸æ¶¦æ€§å’Œä¸ªä½“å·®å¼‚ä½¿å¾—å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>Deep learningæ¨¡å‹å·²è¢«å¼€å‘ç”¨äºç²¾ç¡®ç•Œå®šè¿™äº›è‚¿ç˜¤ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨BraTS-Africaæ•°æ®é›†è¿›è¡Œçº¿ä¸‹æ•°æ®å¢å¼ºï¼Œä»¥æé«˜æ ·æœ¬å¤šæ ·æ€§å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç»“åˆä¸‰ç§ä¸åŒæ¶æ„çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬MedNeXtã€SegMambaå’ŒResidual-Encoder U-Netï¼Œä»¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>MedNeXtæ¨¡å‹åœ¨è®­ç»ƒäº†1000ä¸ªå‘¨æœŸåè¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°è¾ƒé«˜çš„Diceç³»æ•°å’Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»å¾—åˆ†ã€‚</li>
<li>é€šè¿‡æ¨¡å‹é›†æˆï¼Œæé«˜äº†è‚¿ç˜¤å„å­åŒºåŸŸçš„åˆ†å‰²æ€§èƒ½çš„æœ€ä½³å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e9bb58f171e595e3e0b244a4100ab467~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134155&auth_key=1760134155-0-0-65aee3709542e28e9a5b5f26025c1a37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6d6c24fe0734fb5bf2d5b163a639868~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134162&auth_key=1760134162-0-0-2bd72478655e6ff59772273ba8900ccb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a549fbdcb3d06a028aca374ff8655a2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134168&auth_key=1760134168-0-0-729f5eb8145a4e5c6de865a548a190ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ProtoMedX-Towards-Explainable-Multi-Modal-Prototype-Learning-for-Bone-Health-Classification"><a href="#ProtoMedX-Towards-Explainable-Multi-Modal-Prototype-Learning-for-Bone-Health-Classification" class="headerlink" title="ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone   Health Classification"></a>ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone   Health Classification</h2><p><strong>Authors:Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns</strong></p>
<p>Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA&#x2F;X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal (multimodal) model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedXâ€™s prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods. </p>
<blockquote>
<p>éª¨éª¼å¥åº·ç ”ç©¶åœ¨åŒ»å­¦å®è·µä¸­å¯¹äºéª¨é‡å‡å°‘å’Œéª¨è´¨ç–æ¾ç—‡çš„æ—©æœŸæ£€æµ‹å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚ä¸´åºŠåŒ»ç”Ÿé€šå¸¸åŸºäºéª¨å¯†åº¦æµ‹å®šï¼ˆDEXAæ‰«æï¼‰å’Œæ‚£è€…ç—…å²è¿›è¡Œè¯Šæ–­ã€‚äººå·¥æ™ºèƒ½åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ä»åœ¨ç ”ç©¶ä¸­ã€‚å¤§å¤šæ•°æˆåŠŸçš„æ–¹æ³•ä¾èµ–äºä½¿ç”¨è§†è§‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆDEXA&#x2F;Xå°„çº¿å½±åƒï¼‰ï¼Œä¾§é‡äºé¢„æµ‹å‡†ç¡®æ€§ï¼Œè€Œå¯è§£é‡Šæ€§é€šå¸¸è¢«å¿½è§†ï¼Œå¹¶åœ¨äº‹åå¯¹è¾“å…¥è´¡çŒ®è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ProtoMedXï¼Œä¸€ä¸ªä½¿ç”¨DEXAæ‰«æè…°æ¤å›¾åƒå’Œæ‚£è€…è®°å½•çš„å¤šæ¨¡æ€ï¼ˆå¤šæ¨¡æ€ï¼‰æ¨¡å‹ã€‚ProtoMedXåŸºäºåŸå‹çš„è®¾è®¡æ˜¯å¯è§£é‡Šçš„ï¼Œè¿™åœ¨åŒ»ç–—åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å³å°†åˆ°æ¥çš„æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆçš„èƒŒæ™¯ä¸‹ï¼Œå› ä¸ºå®ƒå…è®¸å¯¹æ¨¡å‹å†³ç­–è¿›è¡Œæ˜ç¡®åˆ†æï¼ŒåŒ…æ‹¬é”™è¯¯çš„å†³ç­–ã€‚ProtoMedXåœ¨éª¨éª¼å¥åº·åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†ä¸´åºŠåŒ»ç”Ÿå¯ä»¥ç†è§£çš„è§£é‡Šã€‚åœ¨ä½¿ç”¨åŒ…å«4160åçœŸå®NHSæ‚£è€…çš„æ•°æ®é›†è¿›è¡Œçš„æµ‹è¯•ä¸­ï¼Œæ‰€æå‡ºçš„ProtoMedXåœ¨ä»…ä½¿ç”¨è§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§è¾¾åˆ°87.58%ï¼Œå¤šæ¨¡æ€å˜ä½“çš„å‡†ç¡®æ€§ä¸º89.8%ï¼Œå‡è¶…è¿‡äº†ç°æœ‰å·²å‘å¸ƒçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14830v2">PDF</a> ICCV 2025 (PHAROS-AFE-AIMI: Adaptation, Fairness, and Explainability   in Medical Imaging). 8 pages, 5 figures, 4 tables. Keywords: multi-modal,   multimodal, prototype learning, explainable AI, interpretable models,   case-based reasoning, medical imaging, DEXA, bone health, osteoporosis,   osteopenia, diagnosis, classification, clustering</p>
<p><strong>Summary</strong><br>åŒ»å­¦å®è·µä¸­çš„éª¨éª¼å¥åº·ç ”ç©¶å¯¹äºæ—©æœŸå‘ç°å’Œæ²»ç–—éª¨è´¨ç–æ¾åŠéª¨è´¨å‡å°‘è‡³å…³é‡è¦ã€‚ç ”ç©¶äººå‘˜æ­£åœ¨ç ”ç©¶äººå·¥æ™ºèƒ½åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ï¼Œç›®å‰æœ€æˆåŠŸçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºä½¿ç”¨æ·±åº¦å­¦ä¹ çš„è§†è§‰æ¨¡å‹ï¼ˆDEXAæˆ–Xå°„çº¿å½±åƒï¼‰ï¼Œå¹¶å…³æ³¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ProtoMedXå¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆäº†DEXAæ‰«æçš„è…°æ¤å½±åƒä¸æ‚£è€…è®°å½•ã€‚å…¶åŸå‹è®¾è®¡å…·æœ‰å¯è§£é‡Šæ€§ï¼Œå¯¹äºåŒ»ç–—åº”ç”¨è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å³å°†åˆ°æ¥çš„æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆèƒŒæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹å…è®¸å¯¹å†³ç­–è¿›è¡Œæ˜ç¡®åˆ†æï¼ŒåŒ…æ‹¬é”™è¯¯çš„å†³ç­–ã€‚ProtoMedXåœ¨éª¨éª¼å¥åº·åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†ä¸´åºŠåŒ»ç”Ÿå¯ä»¥ç†è§£çš„è§£é‡Šã€‚åœ¨åŒ…å«4,160åçœŸå®NHSæ‚£è€…çš„æ•°æ®é›†ä¸Šï¼ŒProtoMedXåœ¨ä»…ä½¿ç”¨è§†è§‰ä»»åŠ¡çš„å‡†ç¡®ç‡ä¸º87.58%ï¼Œå¤šæ¨¡æ€ç‰ˆæœ¬çš„å‡†ç¡®ç‡ä¸º89.8%ï¼Œå‡è¶…è¿‡äº†å·²å‘å¸ƒçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å®è·µä¸­çš„éª¨éª¼å¥åº·ç ”ç©¶æœ‰åŠ©äºæ—©æœŸå‘ç°å’Œæ²»ç–—éª¨è´¨ç–æ¾åŠéª¨è´¨å‡å°‘ã€‚</li>
<li>ç›®å‰AIåœ¨éª¨éª¼å¥åº·é¢†åŸŸçš„ç ”ç©¶ä¸»è¦ä¾èµ–äºæ·±åº¦å­¦ä¹ çš„è§†è§‰æ¨¡å‹ã€‚</li>
<li>ProtoMedXæ¨¡å‹ç»“åˆäº†DEXAæ‰«æçš„è…°æ¤å½±åƒä¸æ‚£è€…è®°å½•ï¼Œå…·æœ‰å¤šæ¨¡æ€ç‰¹æ€§ã€‚</li>
<li>ProtoMedXçš„åŸå‹è®¾è®¡å…·æœ‰å¯è§£é‡Šæ€§ï¼Œè¿™å¯¹åŒ»ç–—åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆå¼ºè°ƒæ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ProtoMedXåœ¨éª¨éª¼å¥åº·åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0a36cdd7934e799a9eee82c28192a374~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134176&auth_key=1760134176-0-0-e4708e50de33ce948f1d009a465a5b3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6760c2f8b0e2ce296f28264aa907cce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134183&auth_key=1760134183-0-0-a366eb5c3a258834a995035ee58b4b3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4f1775342f30bef6107f77e19d11481~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134189&auth_key=1760134189-0-0-50c5db754d178f94cbe6115f67dcf608&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b40a21bee6693aa28fcd120acfe463f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134196&auth_key=1760134196-0-0-2a1ca65148107875e5be856c1e2a00a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aab0a14c2ff2fd4a67c516253c7a239d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134203&auth_key=1760134203-0-0-00f139bad342c77458a1776ac4ce7334&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FireGNN-Neuro-Symbolic-Graph-Neural-Networks-with-Trainable-Fuzzy-Rules-for-Interpretable-Medical-Image-Classification"><a href="#FireGNN-Neuro-Symbolic-Graph-Neural-Networks-with-Trainable-Fuzzy-Rules-for-Interpretable-Medical-Image-Classification" class="headerlink" title="FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules   for Interpretable Medical Image Classification"></a>FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules   for Interpretable Medical Image Classification</h2><p><strong>Authors:Prajit Sengupta, Islem Rekik</strong></p>
<p>Medical image classification requires not only high predictive performance but also interpretability to ensure clinical trust and adoption. Graph Neural Networks (GNNs) offer a powerful framework for modeling relational structures within datasets; however, standard GNNs often operate as black boxes, limiting transparency and usability, particularly in clinical settings. In this work, we present an interpretable graph-based learning framework named FireGNN that integrates trainable fuzzy rules into GNNs for medical image classification. These rules embed topological descriptors - node degree, clustering coefficient, and label agreement - using learnable thresholds and sharpness parameters to enable intrinsic symbolic reasoning. Additionally, we explore auxiliary self-supervised tasks (e.g., homophily prediction, similarity entropy) as a benchmark to evaluate the contribution of topological learning. Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST benchmarks and the synthetic dataset MorphoMNIST, while also generating interpretable rule-based explanations. To our knowledge, this is the first integration of trainable fuzzy rules within a GNN. Source Code: <a target="_blank" rel="noopener" href="https://github.com/basiralab/FireGNN">https://github.com/basiralab/FireGNN</a> </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†ç±»ä¸ä»…éœ€è¦é«˜é¢„æµ‹æ€§èƒ½ï¼Œè¿˜éœ€è¦å¯è§£é‡Šæ€§ï¼Œä»¥ç¡®ä¿ä¸´åºŠä¸Šçš„ä¿¡ä»»å’Œé‡‡ç”¨ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸ºæ•°æ®é›†å†…çš„å…³ç³»ç»“æ„æä¾›äº†å¼ºå¤§çš„å»ºæ¨¡æ¡†æ¶ï¼›ç„¶è€Œï¼Œæ ‡å‡†GNNsé€šå¸¸ä½œä¸ºé»‘ç®±æ“ä½œï¼Œé™åˆ¶äº†é€æ˜åº¦å’Œå¯ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠç¯å¢ƒä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå›¾çš„ã€å¯è§£é‡Šçš„å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºFireGNNï¼Œå®ƒå°†å¯è®­ç»ƒçš„æ¨¡ç³Šè§„åˆ™é›†æˆåˆ°GNNsä¸­ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¿™äº›è§„åˆ™ä½¿ç”¨å¯å­¦ä¹ çš„é˜ˆå€¼å’Œå°–é”åº¦å‚æ•°åµŒå…¥æ‹“æ‰‘æè¿°ç¬¦ï¼ˆèŠ‚ç‚¹åº¦ã€èšç±»ç³»æ•°å’Œæ ‡ç­¾ä¸€è‡´æ€§ï¼‰ï¼Œä»¥å®ç°å†…åœ¨ç¬¦å·æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¾…åŠ©çš„è‡ªæˆ‘ç›‘ç£ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼ŒåŒæºæ€§é¢„æµ‹ã€ç›¸ä¼¼æ€§ç†µï¼‰ä½œä¸ºè¯„ä¼°æ‹“æ‰‘å­¦ä¹ è´¡çŒ®çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„å¢å¼ºæ¨¡ç³Šè§„åˆ™æ¨¡å‹åœ¨äº”ä¸ªMedMNISTåŸºå‡†æµ‹è¯•å’Œåˆæˆæ•°æ®é›†MorphoMNISTä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶ç”Ÿæˆäº†åŸºäºè§„åˆ™çš„å¯è§£é‡Šè§£é‡Šã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯GNNä¸­å¯è®­ç»ƒæ¨¡ç³Šè§„åˆ™çš„é¦–æ¬¡é›†æˆã€‚æºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/basiralab/FireGNN">https://github.com/basiralab/FireGNN</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10510v2">PDF</a> Accepted at NeurIPS 2025 Conference (Workshop Track), San Diego, USA</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFireGNNçš„å¯è§£é‡Šçš„åŸºäºå›¾çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¯è®­ç»ƒçš„æ¨¡ç³Šè§„åˆ™é›†æˆåˆ°å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸­ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ¡†æ¶ä½¿ç”¨æ‹“æ‰‘æè¿°ç¬¦ï¼ˆå¦‚èŠ‚ç‚¹åº¦ã€èšç±»ç³»æ•°å’Œæ ‡ç­¾ä¸€è‡´æ€§ï¼‰å’Œå¯å­¦ä¹ çš„é˜ˆå€¼å’Œå°–é”åº¦å‚æ•°ï¼Œå®ç°äº†å†…åœ¨ç¬¦å·æ¨ç†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†è¾…åŠ©è‡ªç›‘ç£ä»»åŠ¡ä½œä¸ºè¯„ä¼°æ‹“æ‰‘å­¦ä¹ è´¡çŒ®çš„åŸºå‡†ã€‚è¯¥æ¨¡ç³Šè§„åˆ™å¢å¼ºçš„æ¨¡å‹åœ¨äº”ä¸ªMedMNISTåŸºå‡†æµ‹è¯•å’Œåˆæˆæ•°æ®é›†MorphoMNISTä¸Šè¡¨ç°å¼ºåŠ²ï¼ŒåŒæ—¶äº§ç”Ÿå¯è§£é‡Šçš„åŸºäºè§„åˆ™çš„è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»éœ€è¦é«˜é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œä»¥ç¡®ä¿ä¸´åºŠä¿¡ä»»å’Œåº”ç”¨ã€‚</li>
<li>Graph Neural Networks (GNNs) ä¸ºæ•°æ®é›†å†…çš„å…³ç³»ç»“æ„æä¾›äº†å¼ºå¤§çš„å»ºæ¨¡æ¡†æ¶ã€‚</li>
<li>æ ‡å‡†GNNså¸¸å¸¸ä½œä¸ºé»‘ç®±æ“ä½œï¼Œé™åˆ¶äº†é€æ˜åº¦å’Œå¯ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠç¯å¢ƒä¸­ã€‚</li>
<li>FireGNNæ¡†æ¶å°†å¯è®­ç»ƒçš„æ¨¡ç³Šè§„åˆ™é›†æˆåˆ°GNNsä¸­ï¼Œå®ç°åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨æ‹“æ‰‘æè¿°ç¬¦è¿›è¡Œç¬¦å·æ¨ç†ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹åº¦ã€èšç±»ç³»æ•°å’Œæ ‡ç­¾ä¸€è‡´æ€§ã€‚</li>
<li>è¾…åŠ©è‡ªç›‘ç£ä»»åŠ¡è¢«ç”¨ä½œè¯„ä¼°æ‹“æ‰‘å­¦ä¹ è´¡çŒ®çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8aff4be2351e0a7be67edcd94bf840fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134210&auth_key=1760134210-0-0-b8f666bbb489a862207f8d2ef4eaa888&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdade4dc9ba7c50a55677eec20e824a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134217&auth_key=1760134217-0-0-9c171dc7491888326259f948514241b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff52a14f4f059fe063e6dea4ff7c8272~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134224&auth_key=1760134224-0-0-d641ea7251867351e777a636f7e7d30c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VisionTS-Cross-Modal-Time-Series-Foundation-Model-with-Continual-Pre-trained-Vision-Backbones"><a href="#VisionTS-Cross-Modal-Time-Series-Foundation-Model-with-Continual-Pre-trained-Vision-Backbones" class="headerlink" title="VisionTS++: Cross-Modal Time Series Foundation Model with Continual   Pre-trained Vision Backbones"></a>VisionTS++: Cross-Modal Time Series Foundation Model with Continual   Pre-trained Vision Backbones</h2><p><strong>Authors:Lefei Shen, Mouxiang Chen, Xu Liu, Han Fu, Xiaoxue Ren, Jianling Sun, Zhuo Li, Chenghao Liu</strong></p>
<p>Recent studies have indicated that vision models pre-trained on images can serve as time series foundation models (TSFMs) by reformulating time series forecasting (TSF) as image reconstruction. However, effective cross-modal transfer from vision to time series remains challenging due to three discrepancies: (1) the data-modality gap between structured, bounded image data and unbounded, heterogeneous time series; (2) the multivariate-forecasting gap between fixed RGB-three-channel vision models and time series with arbitrary numbers of variates; and (3) the probabilistic-forecasting gap between the deterministic outputs of vision models and the requirement for uncertainty-aware probabilistic predictions. To bridge these gaps, we propose VisonTS++, a TSFM based on continual pre-training of a vision model on large-scale time series. Our approach introduces three key innovations: (1) vision-model-based filtering to identify high-quality sequences to stabilize pre-training and mitigate modality gap; (2) colorized multivariate conversion, encoding multivariate series as multi-subfigure RGB images to enhance cross-variate modeling; (3) multi-quantile forecasting, using parallel reconstruction heads to generate quantile forecasts without parametric assumptions. Experiments show that VisionTS++ achieves state-of-the-art performance in both in-distribution and out-of-distribution forecasting, outperforming specialized TSFMs by 6%-44% in MSE reduction and ranking first in GIFT-Eval benchmark which comprises 23 datasets across 7 domains. Our work demonstrates that with appropriate adaptation, vision models can effectively generalize to TSF, thus advancing the pursuit of universal TSFMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HALF111/VisionTSpp">https://github.com/HALF111/VisionTSpp</a>. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åœ¨å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡å°†æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é‡æ–°è¡¨è¿°ä¸ºå›¾åƒé‡å»ºï¼Œä½œä¸ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰ã€‚ç„¶è€Œï¼Œç”±äºä¸‰ä¸ªå·®å¼‚ï¼Œä»è§†è§‰åˆ°æ—¶é—´åºåˆ—çš„æœ‰æ•ˆè·¨æ¨¡æ€è¿ç§»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šï¼ˆ1ï¼‰ç»“æ„åŒ–ã€æœ‰ç•Œå›¾åƒæ•°æ®ä¸æ— ç•Œã€å¼‚è´¨æ—¶é—´åºåˆ—ä¹‹é—´çš„æ•°æ®æ¨¡æ€å·®è·ï¼›ï¼ˆ2ï¼‰å›ºå®šRGBä¸‰é€šé“è§†è§‰æ¨¡å‹ä¸å…·æœ‰ä»»æ„å˜é‡æ•°çš„æ—¶é—´åºåˆ—ä¹‹é—´çš„å¤šå…ƒé¢„æµ‹å·®è·ï¼›ï¼ˆ3ï¼‰è§†è§‰æ¨¡å‹çš„ç¡®å®šæ€§è¾“å‡ºä¸å¯¹ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ¦‚ç‡é¢„æµ‹çš„è¦æ±‚ä¹‹é—´çš„æ¦‚ç‡é¢„æµ‹å·®è·ã€‚ä¸ºäº†å¼¥åˆè¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†VisonTS++ï¼Œä¸€ç§åŸºäºè§†è§‰æ¨¡å‹å¤§è§„æ¨¡æ—¶é—´åºåˆ—æŒç»­é¢„è®­ç»ƒçš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰åŸºäºè§†è§‰æ¨¡å‹çš„è¿‡æ»¤ï¼Œä»¥è¯†åˆ«é«˜è´¨é‡åºåˆ—ï¼Œç¨³å®šé¢„è®­ç»ƒï¼Œå‡è½»æ¨¡æ€å·®è·ï¼›ï¼ˆ2ï¼‰å½©è‰²å¤šå…ƒè½¬æ¢ï¼Œå°†å¤šå…ƒåºåˆ—ç¼–ç ä¸ºå¤šå­å›¾RGBå›¾åƒï¼Œä»¥å¢å¼ºè·¨å˜é‡å»ºæ¨¡ï¼›ï¼ˆ3ï¼‰å¤šåˆ†ä½é¢„æµ‹ï¼Œä½¿ç”¨å¹¶è¡Œé‡å»ºå¤´ç”Ÿæˆåˆ†ä½é¢„æµ‹ï¼Œæ— éœ€å‚æ•°å‡è®¾ã€‚å®éªŒè¡¨æ˜ï¼ŒVisionTS++åœ¨å†…å¤–åˆ†å¸ƒé¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡æ–¹è¯¯å·®å‡å°‘æ–¹é¢æ¯”ä¸“é—¨çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹é«˜å‡º6%-44%ï¼Œåœ¨ç”±7ä¸ªé¢†åŸŸ23ä¸ªæ•°æ®é›†ç»„æˆçš„GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œç»è¿‡é€‚å½“çš„é€‚åº”ï¼Œè§†è§‰æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°TSFï¼Œä»è€Œæ¨åŠ¨é€šç”¨TSFMçš„è¿½æ±‚ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HALF111/VisionTSpp%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HALF111/VisionTSppä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04379v2">PDF</a> 19 pages</p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºå›¾åƒé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹é€šè¿‡æ”¹é©æ—¶é—´åºåˆ—é¢„æµ‹ä¸ºå›¾åƒé‡å»ºï¼Œå¯ä½œä¸ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMï¼‰ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æ¨¡æ€å·®è·ã€å¤šå…ƒé¢„æµ‹å·®è·å’Œæ¦‚ç‡é¢„æµ‹å·®è·ï¼Œä»è§†è§‰åˆ°æ—¶é—´åºåˆ—çš„æœ‰æ•ˆè·¨æ¨¡æ€è¿ç§»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºç¼©å°è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºVisionTS++æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºè§†è§‰æ¨¡å‹çš„å¤§è§„æ¨¡æ—¶é—´åºåˆ—æŒç»­é¢„è®­ç»ƒï¼Œå¼•å…¥ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŸºäºè§†è§‰æ¨¡å‹çš„è¿‡æ»¤ä»¥ç¨³å®šé¢„è®­ç»ƒå¹¶ç¼©å°æ¨¡æ€å·®è·ï¼›å½©è‰²å¤šå…ƒè½¬æ¢ï¼Œå°†å¤šå…ƒåºåˆ—ç¼–ç ä¸ºå¤šå­å›¾RGBå›¾åƒï¼Œä»¥å¢å¼ºè·¨å˜é‡å»ºæ¨¡ï¼›å¤šåˆ†ä½é¢„æµ‹ï¼Œä½¿ç”¨å¹¶è¡Œé‡å»ºå¤´ç”Ÿæˆåˆ†ä½é¢„æµ‹ï¼Œæ— éœ€å‚æ•°å‡è®¾ã€‚å®éªŒè¡¨æ˜ï¼ŒVisionTS++åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–é¢„æµ‹æ–¹é¢å‡è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨MSEé™ä½æ–¹é¢ä¼˜äºä¸“ä¸šTSFMè¾¾6%-44%ï¼Œåœ¨æ¶µç›–7ä¸ªé¢†åŸŸçš„23ä¸ªæ•°æ®é›†çš„GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œç»è¿‡é€‚å½“çš„é€‚åº”ï¼Œè§†è§‰æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°TSFï¼Œä»è€Œæ¨åŠ¨é€šç”¨TSFMçš„è¿½æ±‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå›¾åƒé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹å¯ä»¥ä½œä¸ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMï¼‰ã€‚</li>
<li>å­˜åœ¨ä»è§†è§‰åˆ°æ—¶é—´åºåˆ—çš„è·¨æ¨¡æ€è¿ç§»æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ•°æ®æ¨¡æ€ã€å¤šå…ƒé¢„æµ‹å’Œæ¦‚ç‡é¢„æµ‹çš„å·®è·ã€‚</li>
<li>VisionTS++æ¨¡å‹é€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°æ¥è§£å†³è¿™äº›é—®é¢˜ï¼šåŸºäºè§†è§‰æ¨¡å‹çš„è¿‡æ»¤ã€å½©è‰²å¤šå…ƒè½¬æ¢å’Œå¤šåˆ†ä½é¢„æµ‹ã€‚</li>
<li>VisionTS++åœ¨å¤šç§æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¡¨ç°å‡ºä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–é¢„æµ‹å‡è¡¨ç°ä¼˜è¶Šï¼Œä¸ä¸“ä¸šTSFMç›¸æ¯”ï¼ŒMSEé™ä½è¾¾6%-44%ã€‚</li>
<li>VisionTS++åœ¨GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ï¼Œæ¶µç›–7ä¸ªé¢†åŸŸçš„23ä¸ªæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fa456f582414c196ecd20bd9146bb6d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134232&auth_key=1760134232-0-0-ddebd43b62e09f77eff1e41b407f40eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c1a9cac7f87caed3ba9f57afb677359~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134239&auth_key=1760134239-0-0-3146ad5bb6d7ba19742c21d415712a4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0704bccd8332602e8d1a9eae0ff039c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134246&auth_key=1760134246-0-0-aea2313a748a7bc1c7d73c5489d9f5cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AutoMiSeg-Automatic-Medical-Image-Segmentation-via-Test-Time-Adaptation-of-Foundation-Models"><a href="#AutoMiSeg-Automatic-Medical-Image-Segmentation-via-Test-Time-Adaptation-of-Foundation-Models" class="headerlink" title="AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation   of Foundation Models"></a>AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation   of Foundation Models</h2><p><strong>Authors:Xingjian Li, Qifeng Wu, Adithya S. Ubaradka, Yiran Ding, Colleen Que, Runmin Jiang, Jianhua Xing, Tianyang Wang, Min Xu</strong></p>
<p>Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., â€œsegment the optic disc in an eye fundus imageâ€), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline not only substantially surpasses the previously best-performing method, yielding a 69% relative improvement in accuracy (Dice Score from 42.53 to 71.81), but also performs competitively with weakly-prompted interactive foundation models. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œå½“å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„ä¸“å®¶ç²¾åŠ›ï¼Œä¾‹å¦‚é€šè¿‡æ ‡æ³¨å¤§é‡è®­ç»ƒæ•°æ®é›†æˆ–åœ¨æ¨ç†æ—¶é—´ä¸ºæ¯ä¸ªæ–°ç—…ä¾‹æä¾›æç¤ºã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é›¶æ ·æœ¬è‡ªåŠ¨åˆ†å‰²ç®¡é“ï¼Œå®ƒç»“åˆäº†ç°æˆçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚ç»™å®šåŒ»å­¦å›¾åƒå’Œä»»åŠ¡å®šä¹‰ï¼ˆä¾‹å¦‚ï¼Œâ€œåœ¨çœ¼åº•å›¾åƒä¸­åˆ†å‰²è§†ç›˜â€ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å®šä½æ¨¡å‹ç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ï¼Œç„¶åé€šè¿‡è§†è§‰æç¤ºå¢å¼ºæ¨¡å—å¢å¼ºæç¤ºï¼Œæœ€åç”±å¯æç¤ºçš„åˆ†å‰²æ¨¡å‹å¤„ç†ä»¥äº§ç”Ÿæœ€ç»ˆæ©ç ã€‚ä¸ºäº†è§£å†³é¢†åŸŸå·®è·å’Œç»“æœéªŒè¯çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶é—´é€‚åº”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸€ç»„å¯å­¦ä¹ çš„é€‚é…å™¨ï¼Œç”¨äºå°†åŒ»å­¦è¾“å…¥ä¸åŸºç¡€æ¨¡å‹è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚å…¶è¶…å‚æ•°é€šè¿‡è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ï¼Œç”±ä»£ç†éªŒè¯æ¨¡å‹æŒ‡å¯¼ï¼Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸ºè·¨ä¸åŒä»»åŠ¡çš„é›¶æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç®¡é“åœ¨ä¸ƒä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚é€šè¿‡é€‚å½“çš„åˆ†è§£å’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼Œæˆ‘ä»¬çš„å…¨è‡ªåŠ¨ç®¡é“ä¸ä»…æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰æ€§èƒ½æœ€ä½³çš„æ–¹æ³•ï¼Œå‡†ç¡®åº¦ç›¸å¯¹æé«˜äº†69%ï¼ˆDiceå¾—åˆ†ä»42.53æé«˜åˆ°71.81ï¼‰ï¼Œè€Œä¸”ä¸å¼±æç¤ºäº¤äº’å¼åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17931v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é›¶æ ·æœ¬è‡ªåŠ¨åˆ†å‰²ç®¡é“ï¼Œç»“åˆäº†ç°æˆçš„è§†è§‰è¯­è¨€ä¸åˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚è¯¥æ–¹æ³•åªéœ€åŒ»å­¦å›¾åƒå’Œä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œåœ¨çœ¼åº•å›¾åƒä¸­åˆ†å‰²è§†ç›˜â€ï¼‰ï¼Œæ— éœ€å¤§é‡ä¸“å®¶æ ‡æ³¨æ•°æ®æˆ–æ¯ä¸ªæ–°ç—…ä¾‹çš„æ¨ç†æç¤ºã€‚é€šè¿‡åˆå§‹è¾¹ç•Œæ¡†ç”Ÿæˆã€è§†è§‰æç¤ºå¢å¼ºæ¨¡å—å’Œå¯æç¤ºçš„åˆ†å‰²æ¨¡å‹å¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œã€‚ä¸ºè§£å†³é¢†åŸŸå·®è·å’Œç»“æœéªŒè¯é—®é¢˜ï¼Œå¼•å…¥æµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è¶…å‚æ•°å’Œä»£ç†éªŒè¯æ¨¡å‹ï¼Œæ— éœ€çœŸå®æ ‡ç­¾å³å¯å¯¹é½åŒ»å­¦è¾“å…¥ä¸åŸºç¡€æ¨¡å‹è¡¨ç¤ºã€‚è¯¥ç®¡é“ä¸ºè·¨ä¸åŒä»»åŠ¡çš„é›¶æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ ‡æ³¨æ•ˆç‡ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨ä¸ƒä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœä»¤äººé¼“èˆã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®¡é“ä¸ä»…å‡†ç¡®åº¦ç›¸å¯¹æé«˜äº†69%ï¼ˆDiceå¾—åˆ†ä»42.53æé«˜åˆ°71.81ï¼‰ï¼Œè€Œä¸”ä¸å¼±æç¤ºäº¤äº’åŸºç¡€æ¨¡å‹çš„æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡ä¸“å®¶åŠªåŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬è‡ªåŠ¨åˆ†å‰²ç®¡é“ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€å’Œåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ã€å¢å¼ºè§†è§‰æç¤ºå¹¶å¤„ç†å¯æç¤ºçš„åˆ†å‰²æ¨¡å‹æ¥ç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œã€‚</li>
<li>å¼•å…¥æµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è¶…å‚æ•°å’Œä»£ç†éªŒè¯æ¨¡å‹è§£å†³é¢†åŸŸå·®è·å’Œç»“æœéªŒè¯é—®é¢˜ã€‚</li>
<li>è¯¥ç®¡é“åœ¨ä¸ƒä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æé«˜ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®¡é“çš„å‡†ç¡®åº¦ç›¸å¯¹æé«˜äº†69%ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¸å¼±æç¤ºäº¤äº’åŸºç¡€æ¨¡å‹çš„ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºå®ç°å…¨è‡ªåŠ¨ã€é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-13c7cb222ce3f7c4027634919a3b8253~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134253&auth_key=1760134253-0-0-186b87bfd09e913a2a1d65c1c401d370&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe144fb768b3ae7f90579b5daf66687e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134260&auth_key=1760134260-0-0-d19a8455a68ca03177cf64771234fbbb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db9038c169af92f3122f5872c731e32b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134267&auth_key=1760134267-0-0-e477780016b8294e3f28790ed790a328&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ac45edede43bac2333b49f0a2b18cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134274&auth_key=1760134274-0-0-e130502427e31213aa2cb35127ca3e51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Robust-Frequency-Domain-Full-Waveform-Inversion-via-HV-Geometry"><a href="#Robust-Frequency-Domain-Full-Waveform-Inversion-via-HV-Geometry" class="headerlink" title="Robust Frequency Domain Full-Waveform Inversion via HV-Geometry"></a>Robust Frequency Domain Full-Waveform Inversion via HV-Geometry</h2><p><strong>Authors:Zhijun Zeng, Matej Neumann, Yunan Yang</strong></p>
<p>Conventional frequency-domain full-waveform inversion (FWI) is typically implemented with an $L^2$ misfit function, which suffers from challenges such as cycle skipping and sensitivity to noise. While the Wasserstein metric has proven effective in addressing these issues in time-domain FWI, its applicability in frequency-domain FWI is limited due to the complex-valued nature of the data and reduced transport-like dependency on wave speed. To mitigate these challenges, we introduce the HV metric ($d_{\text{HV}}$), inspired by optimal transport theory, which compares signals based on horizontal and vertical changes without requiring the normalization of data. We implement $d_{\text{HV}}$ as the misfit function in frequency-domain FWI and evaluate its performance on synthetic and real-world datasets from seismic imaging and ultrasound computed tomography (USCT). Numerical experiments demonstrate that $d_{\text{HV}}$ outperforms the $L^2$ and Wasserstein metrics in scenarios with limited prior model information and high noise while robustly improving inversion results on clinical USCT data. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„é¢‘åŸŸå…¨æ³¢å½¢åæ¼”ï¼ˆFWIï¼‰é€šå¸¸é‡‡ç”¨$L^2$ä¸åŒ¹é…å‡½æ•°è¿›è¡Œå®ç°ï¼Œè¿™é¢ä¸´ç€è¯¸å¦‚å‘¨æœŸè·³å˜å’Œå¯¹å™ªå£°æ•æ„Ÿç­‰æŒ‘æˆ˜ã€‚è™½ç„¶Wassersteinåº¦é‡åœ¨æ—¶é—´åŸŸFWIä¸­å·²è¢«è¯æ˜å¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ç”±äºæ•°æ®çš„å¤å€¼æ€§å’Œå¯¹æ³¢é€Ÿçš„è¿è¾“ä¾èµ–é™ä½ï¼Œå…¶åœ¨é¢‘åŸŸFWIä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºäº†ç¼“è§£è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å—æœ€ä¼˜ä¼ è¾“ç†è®ºå¯å‘çš„HVåº¦é‡ï¼ˆ$d_{\text{HV}}$ï¼‰ï¼Œå®ƒåŸºäºæ°´å¹³å˜åŒ–å’Œå‚ç›´å˜åŒ–æ¯”è¾ƒä¿¡å·ï¼Œæ— éœ€å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ã€‚æˆ‘ä»¬å°†$d_{\text{HV}}$ä½œä¸ºé¢‘åŸŸFWIä¸­çš„ä¸åŒ¹é…å‡½æ•°è¿›è¡Œå®ç°ï¼Œå¹¶è¯„ä¼°å…¶åœ¨åœ°éœ‡æˆåƒå’Œè¶…å£°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆUSCTï¼‰çš„åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™å…ˆéªŒæ¨¡å‹ä¿¡æ¯å’Œé«˜å™ªå£°çš„æƒ…å†µä¸‹ï¼Œ$d_{\text{HV}}$åœ¨åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äº$L^2$å’ŒWassersteinåº¦é‡ï¼Œå¹¶ä¸”åœ¨ä¸´åºŠUSCTæ•°æ®ä¸Šç¨³å¥åœ°æ”¹è¿›äº†åæ¼”ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01817v2">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¼ ç»Ÿé¢‘åŸŸå…¨æ³¢å½¢åæ¼”ï¼ˆFWIï¼‰é€šå¸¸ä½¿ç”¨LÂ²ä¸é€‚å®šå‡½æ•°ï¼Œé¢ä¸´å¾ªç¯è·³è¿‡å’Œå™ªå£°æ•æ„Ÿç­‰é—®é¢˜ã€‚å—æœ€ä¼˜ä¼ è¾“ç†è®ºå¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥HVåº¦é‡ï¼ˆdHVï¼‰ï¼Œè¯¥åº¦é‡åŸºäºæ°´å¹³å‚ç›´å˜åŒ–æ¯”è¾ƒä¿¡å·ï¼Œæ— éœ€æ•°æ®å½’ä¸€åŒ–ã€‚åœ¨é¢‘åŸŸFWIä¸­å®ç°dHVä½œä¸ºä¸é€‚å®šå‡½æ•°ï¼Œå¹¶åœ¨åœ°éœ‡æˆåƒå’Œè¶…å£°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆUSCTï¼‰çš„åˆæˆåˆ†ç»„å’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™å…ˆéªŒæ¨¡å‹ä¿¡æ¯å’Œé«˜å™ªå£°æƒ…å†µä¸‹ï¼ŒdHVä¼˜äºLÂ²å’ŒWassersteinåº¦é‡ï¼Œå¹¶èƒ½åœ¨ä¸´åºŠUSCTæ•°æ®ä¸Šç¨³å¥åœ°æ”¹è¿›åæ¼”ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿé¢‘åŸŸå…¨æ³¢å½¢åæ¼”ä½¿ç”¨LÂ²ä¸é€‚å®šå‡½æ•°é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¾ªç¯è·³è¿‡å’Œå™ªå£°æ•æ„Ÿã€‚</li>
<li>Wassersteinåº¦é‡åœ¨é¢‘åŸŸFWIä¸­çš„é€‚ç”¨æ€§æœ‰é™ï¼Œå› ä¸ºæ•°æ®çš„å¤æ•°æ€§å’Œè¿è¾“ä¾èµ–æ€§çš„é™ä½ã€‚</li>
<li>å¼•å…¥å—æœ€ä¼˜ä¼ è¾“ç†è®ºå¯å‘çš„HVåº¦é‡ï¼ˆdHVï¼‰ï¼Œæ— éœ€æ•°æ®å½’ä¸€åŒ–å³å¯æ¯”è¾ƒä¿¡å·ã€‚</li>
<li>dHVåŸºäºæ°´å¹³å‚ç›´å˜åŒ–æ¯”è¾ƒä¿¡å·ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°äº†dHVçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ°éœ‡æˆåƒå’Œè¶…å£°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆUSCTï¼‰ã€‚</li>
<li>æ•°å€¼å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™å…ˆéªŒæ¨¡å‹ä¿¡æ¯å’Œé«˜å™ªå£°æƒ…å†µä¸‹ï¼ŒdHVä¼˜äºLÂ²å’ŒWassersteinåº¦é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fd67fbb5f12f2df1b10ab17cf28fc12c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134281&auth_key=1760134281-0-0-d4ec4f6906457f2cb3ecf1840c58905a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ebeefdee468277653d55b5bf6d5f8f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134288&auth_key=1760134288-0-0-ff6b95861c4726cb784a4ffcbcd3a92f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d6faf51db223a4a895a0267d4db2b0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134294&auth_key=1760134294-0-0-268b210b732d400189ecdcc4b9ab26a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="From Gaze to Insight: Bridging Human Visual Attention and Vision   Language Model Explanation for Weakly-Supervised Medical Image Segmentation"></a>From Gaze to Insight: Bridging Human Visual Attention and Vision   Language Model Explanation for Weakly-Supervised Medical Image Segmentation</h2><p><strong>Authors:Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Vicente Grau, Jungong Han</strong></p>
<p>Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/jingkunchen/FGI">https://github.com/jingkunchen/FGI</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºåƒç´ çº§æ ‡æ³¨çš„æˆæœ¬é«˜æ˜‚ï¼Œè¿™å¢åŠ äº†è®­ç»ƒçš„éš¾åº¦ã€‚åœ¨å¼±ç›‘ç£çš„èƒŒæ™¯ä¸‹ï¼ŒåŒ»ç”Ÿæ³¨è§†æ•°æ®èƒ½å¤Ÿæ•æ‰è¯Šæ–­æ—¶çš„æ„Ÿå…´è¶£åŒºåŸŸï¼Œä½†å…¶ç¨€ç–æ€§é™åˆ¶äº†å…¶åœ¨åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡æ–‡æœ¬æè¿°æä¾›è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œä½†ç¼ºä¹æ‰€éœ€çš„è§£é‡Šç²¾åº¦ã€‚æˆ‘ä»¬è®¤è¯†åˆ°ï¼Œå•ä¸€çš„æ¥æºä¸è¶³ä»¥è§£å†³é—®é¢˜ï¼Œå› æ­¤æå‡ºä¸€ä¸ªæ•´åˆæ³¨è§†å’Œè¯­è¨€ç›‘ç£çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œå‘æŒ¥å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œæ³¨è§†æ•°æ®æŒ‡ç¤ºåŒ»ç”Ÿåœ¨è¯Šæ–­æ—¶çš„å…³æ³¨åŒºåŸŸï¼Œè€ŒVLMåˆ™è§£é‡Šè¿™äº›åŒºåŸŸä¸ºä½•é‡è¦ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæ•™å¸ˆæ¨¡å‹é¦–å…ˆä»ç”±VLMç”Ÿæˆçš„ç—…å˜å½¢æ€æè¿°å¢å¼ºçš„æ³¨è§†ç‚¹ä¸­å­¦ä¹ ï¼Œä¸ºå¼•å¯¼å­¦ç”Ÿæ¨¡å‹å¥ å®šåŸºç¡€ã€‚ç„¶åï¼Œæ•™å¸ˆé€šè¿‡ä¸‰ç§ç­–ç•¥å¼•å¯¼å­¦ç”Ÿï¼šï¼ˆ1ï¼‰å¤šå°ºåº¦ç‰¹å¾å¯¹é½ï¼Œèåˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬è¯­ä¹‰ï¼›ï¼ˆ2ï¼‰ç½®ä¿¡åº¦åŠ æƒä¸€è‡´æ€§çº¦æŸï¼Œä¸“æ³¨äºå¯é é¢„æµ‹ï¼›ï¼ˆ3ï¼‰è‡ªé€‚åº”æ©ç ï¼Œä»¥é™åˆ¶ä¸ç¡®å®šåŒºåŸŸçš„é”™è¯¯ä¼ æ’­ã€‚åœ¨Kvasir-SEGã€NCI-ISBIå’ŒISICæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«å®ç°äº†80.78%ã€80.53%å’Œ84.22%çš„Diceå¾—åˆ†ï¼Œåœ¨æ³¨è§†åŸºçº¿çš„åŸºç¡€ä¸Šæé«˜äº†3-5%ï¼ŒåŒæ—¶æ²¡æœ‰å¢åŠ æ ‡æ³¨è´Ÿæ‹…ã€‚é€šè¿‡ä¿ç•™é¢„æµ‹ã€æ³¨è§†æ•°æ®å’Œç—…å˜æè¿°ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¿˜ä¿æŒäº†ä¸´åºŠå¯è§£é‡Šæ€§ã€‚è¿™é¡¹å·¥ä½œè¯´æ˜äº†å¦‚ä½•å°†äººç±»è§†è§‰æ³¨æ„åŠ›ä¸AIç”Ÿæˆçš„è¯­ä¹‰ä¸Šä¸‹æ–‡ç›¸ç»“åˆï¼Œä»è€Œæœ‰æ•ˆå…‹æœå•ä¸ªå¼±ç›‘ç£ä¿¡å·çš„å±€é™æ€§ï¼Œæ¨åŠ¨éƒ¨ç½²é«˜æ•ˆã€æ³¨é‡Šæœ‰æ•ˆçš„åŒ»ç–—AIç³»ç»Ÿçš„å‘å±•ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/jingkunchen/FGI%E3%80%82">https://github.com/jingkunchen/FGIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11368v2">PDF</a> 11 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåŒ»ç”Ÿæ³¨è§†æ•°æ®å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸¤è€…äº’è¡¥çš„ä¼˜åŠ¿ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹å­¦ä¹ åŒ»ç”Ÿæ³¨è§†ç‚¹å’ŒVLMç”Ÿæˆçš„ç—…å˜å½¢æ€æè¿°ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¤šå°ºåº¦ç‰¹å¾å¯¹é½ã€ç½®ä¿¡åº¦åŠ æƒä¸€è‡´æ€§çº¦æŸå’Œè‡ªé€‚åº”æ©æ¨¡ç­‰ç­–ç•¥ï¼Œå®ç°åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç²¾å‡†é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Kvasir-SEGã€NCI-ISBIå’ŒISICæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†æœ‰æ‰€æé«˜ï¼ŒåŒæ—¶ä¿æŒä¸´åºŠå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è®­ç»ƒåƒç´ çº§æ³¨é‡Šçš„é«˜æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>åŒ»ç”Ÿæ³¨è§†æ•°æ®èƒ½æ•æ‰è¯Šæ–­æ—¶çš„æ„Ÿå…´è¶£åŒºåŸŸï¼Œä½†å…¶ç¨€ç–æ€§é™åˆ¶äº†å…¶åœ¨åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡æ–‡æœ¬æè¿°æä¾›è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œä½†ç¼ºä¹è§£é‡Šç²¾åº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œç»“åˆäº†æ³¨è§†å’Œè¯­è¨€çš„ç›‘ç£ï¼Œåˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹é€šè¿‡ç»“åˆåŒ»ç”Ÿæ³¨è§†ç‚¹å’ŒVLMç”Ÿæˆçš„æè¿°å»ºç«‹åŸºç¡€ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹è®­ç»ƒã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦ç‰¹å¾å¯¹é½ç­‰ç­–ç•¥å®ç°ç²¾å‡†é¢„æµ‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—æ”¹è¿›ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•ä¿æŒä¸´åºŠå¯è§£é‡Šæ€§ï¼Œé€šè¿‡æ•´åˆäººç±»è§†è§‰æ³¨æ„å’ŒAIç”Ÿæˆçš„è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œå…‹æœå•ä¸€å¼±ç›‘ç£ä¿¡å·çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0a491ba654777f0b00530d3222459c09~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134302&auth_key=1760134302-0-0-81e2c7d5433072d4b9859bddd5973898&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1211f2f914a4a421178243f97155964~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134310&auth_key=1760134310-0-0-a4c6925ac12dba34ec93416618a34705&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6e6064cb5d640eaf5ac4a36678cf720~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134316&auth_key=1760134316-0-0-819cdfcfaaf3559b591578806e6057a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Optimizing-Breast-Cancer-Detection-in-Mammograms-A-Comprehensive-Study-of-Transfer-Learning-Resolution-Reduction-and-Multi-View-Classification"><a href="#Optimizing-Breast-Cancer-Detection-in-Mammograms-A-Comprehensive-Study-of-Transfer-Learning-Resolution-Reduction-and-Multi-View-Classification" class="headerlink" title="Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study   of Transfer Learning, Resolution Reduction, and Multi-View Classification"></a>Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study   of Transfer Learning, Resolution Reduction, and Multi-View Classification</h2><p><strong>Authors:Daniel G. P. Petrini, Hae Yong Kim</strong></p>
<p>Mammography, an X-ray-based imaging technique, remains central to the early detection of breast cancer. Recent advances in artificial intelligence have enabled increasingly sophisticated computer-aided diagnostic methods, evolving from patch-based classifiers to whole-image approaches and then to multi-view architectures that jointly analyze complementary projections. Despite this progress, several critical questions remain unanswered. In this study, we systematically investigate these issues by addressing five key research questions: (1) the role of patch classifiers in performance, (2) the transferability of natural-image-trained backbones, (3) the advantages of learn-to-resize over conventional downscaling, (4) the contribution of multi-view integration, and (5) the robustness of findings across varying image quality. Beyond benchmarking, our experiments demonstrate clear performance gains over prior work. For the CBIS-DDSM dataset, we improved single-view AUC from 0.8153 to 0.8343, and multiple-view AUC from 0.8483 to 0.8658. Using a new comparative method, we also observed a 0.0217 AUC increase when extending from single to multiple-view analysis. On the complete VinDr-Mammo dataset, the multiple-view approach further improved results, achieving a 0.0492 AUC increase over single view and reaching 0.8511 AUC overall. These results establish new state-of-the-art benchmarks, providing clear evidence of the advantages of multi-view architectures for mammogram interpretation. Beyond performance, our analysis offers principled insights into model design and transfer learning strategies, contributing to the development of more accurate and reliable breast cancer screening tools. The inference code and trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/dpetrini/multiple-view">https://github.com/dpetrini/multiple-view</a>. </p>
<blockquote>
<p>ä¹³è…ºXçº¿æ‘„å½±ï¼ˆå³ä¹³è…ºæ‘„å½±æœ¯ï¼‰ä»æ˜¯æ—©æœŸå‘ç°ä¹³è…ºç™Œçš„æ ¸å¿ƒæŠ€æœ¯ã€‚äººå·¥æ™ºèƒ½é¢†åŸŸçš„æœ€æ–°è¿›å±•ä½¿å¾—è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•è¶Šæ¥è¶Šç²¾ç»†ï¼Œä»åŸºäºè¡¥ä¸çš„åˆ†ç±»å™¨å‘å±•åˆ°å…¨å›¾åƒæ–¹æ³•ï¼Œå†å‘å±•åˆ°å¤šè§†å›¾æ¶æ„ï¼Œè”åˆåˆ†æäº’è¡¥æŠ•å½±ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†ä»æœ‰ä¸€äº›å…³é”®é—®é¢˜å°šæœªå¾—åˆ°è§£ç­”ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³äº”ä¸ªå…³é”®ç ”ç©¶é—®é¢˜æ¥ç³»ç»Ÿåœ°ç ”ç©¶è¿™äº›é—®é¢˜ï¼šï¼ˆ1ï¼‰è¡¥ä¸åˆ†ç±»å™¨åœ¨æ€§èƒ½ä¸­çš„ä½œç”¨ï¼›ï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒè®­ç»ƒåéª¨çš„å¯è½¬ç§»æ€§ï¼›ï¼ˆ3ï¼‰ä¸å­¦ä¹ è°ƒæ•´å¤§å°ç›¸æ¯”ï¼Œä¼ ç»Ÿç¼©å°å°ºå¯¸çš„ä¼˜åŠ¿ï¼›ï¼ˆ4ï¼‰å¤šè§†å›¾é›†æˆçš„è´¡çŒ®ï¼›ä»¥åŠï¼ˆ5ï¼‰ä¸åŒå›¾åƒè´¨é‡ä¸‹ç ”ç©¶ç»“æœçš„ç¨³å¥æ€§ã€‚é™¤äº†åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜è¯æ˜äº†ä¸ä¹‹å‰å·¥ä½œçš„æ˜ç¡®æ€§èƒ½æå‡ã€‚å¯¹äºCBIS-DDSMæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å•è§†å›¾AUCä»0.8153æé«˜åˆ°0.8343ï¼Œå¤šè§†å›¾AUCä»0.8483æé«˜åˆ°0.8658ã€‚ä½¿ç”¨ä¸€ç§æ–°çš„æ¯”è¾ƒæ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ä»å•è§†å›¾åˆ†ææ‰©å±•åˆ°å¤šè§†å›¾åˆ†ææ—¶çš„AUCå¢åŠ äº†0.0217ã€‚åœ¨å®Œæ•´çš„VinDr-Mammoæ•°æ®é›†ä¸Šï¼Œå¤šè§†å›¾æ–¹æ³•è¿›ä¸€æ­¥æ”¹å–„äº†ç»“æœï¼Œç›¸å¯¹äºå•è§†å›¾å®ç°äº†0.0492çš„AUCå¢åŠ ï¼Œæ€»ä½“è¾¾åˆ°0.8511çš„AUCã€‚è¿™äº›ç»“æœå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„åŸºå‡†ï¼Œå……åˆ†è¯æ˜äº†å¤šè§†å›¾æ¶æ„åœ¨ä¹³è…ºé’¼é¶è§£è¯»ä¸­çš„ä¼˜åŠ¿ã€‚é™¤äº†æ€§èƒ½ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¿˜ä¸ºæ¨¡å‹è®¾è®¡å’Œè¿ç§»å­¦ä¹ ç­–ç•¥æä¾›äº†åŸåˆ™æ€§çš„è§è§£ï¼Œä¸ºå¼€å‘æ›´å‡†ç¡®å¯é çš„ä¹³è…ºç™Œç­›æŸ¥å·¥å…·åšå‡ºäº†è´¡çŒ®ã€‚æ¨ç†ä»£ç å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dpetrini/multiple-view%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dpetrini/multiple-viewå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19945v3">PDF</a> 31 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨ä¹³è…ºç™Œæ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¤šè§†è§’æ¶æ„åœ¨ä¹³è…ºXå…‰å›¾åƒåˆ†æä¸­çš„ä¼˜åŠ¿ã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿå®éªŒå›ç­”äº†äº”ä¸ªå…³é”®é—®é¢˜ï¼Œå¹¶åœ¨CBIS-DDSMå’ŒVinDr-Mammoæ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„å…ˆè¿›æ€§èƒ½ã€‚å¤šè§†è§’åˆ†ææ–¹æ³•çš„å¼•å…¥æœ‰æ•ˆæå‡äº†è¯Šæ–­å‡†ç¡®æ€§ï¼Œä¸ºä¹³è…ºç™Œç­›æŸ¥å·¥å…·çš„å¼€å‘æä¾›äº†ç†è®ºè§è§£å’Œå®è·µæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨ä¹³è…ºç™Œæ£€æµ‹ä¸­çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤šè§†è§’æ¶æ„çš„åº”ç”¨ï¼Œèƒ½è”åˆåˆ†æäº’è¡¥æŠ•å½±ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è§£å†³äº”ä¸ªå…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬patchåˆ†ç±»å™¨çš„ä½œç”¨ã€è‡ªç„¶å›¾åƒè®­ç»ƒéª¨æ¶çš„è¿ç§»æ€§ã€å­¦ä¹ è°ƒæ•´å¤§å°çš„ä¼˜åŠ¿ã€å¤šè§†è§’æ•´åˆçš„è´¡çŒ®ä»¥åŠä¸åŒå›¾åƒè´¨é‡ä¸‹ç»“æœçš„ç¨³å®šæ€§ï¼Œè¿›è¡Œäº†ç³»ç»Ÿçš„å®éªŒè°ƒæŸ¥ã€‚</li>
<li>åœ¨CBIS-DDSMæ•°æ®é›†ä¸Šï¼Œå¤šè§†è§’åˆ†ææ–¹æ³•ç›¸è¾ƒäºå•è§†è§’æé«˜äº†è¯Šæ–­æ€§èƒ½ï¼ŒAUCå€¼ä»0.8153æå‡è‡³0.8658ã€‚</li>
<li>åœ¨VinDr-Mammoæ•°æ®é›†ä¸Šï¼Œå¤šè§†è§’æ–¹æ³•å®ç°äº†æ›´æ˜¾è‘—çš„AUCå€¼æå‡ï¼Œä»å•è§†è§’çš„0.8æå‡è‡³æ•´ä½“çš„0.8511ã€‚</li>
<li>ç ”ç©¶ç»“æœç¡®ç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œå¹¶æä¾›äº†æ¨¡å‹è®¾è®¡å’Œè¿ç§»å­¦ä¹ ç­–ç•¥çš„è§è§£ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å‡†ç¡®å¯é çš„ä¹³è…ºç™Œç­›æŸ¥å·¥å…·ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„æ¨ç†ä»£ç å’Œè®­ç»ƒæ¨¡å‹æœ‰åŠ©äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c544a5ae3021a8dcef9af7d3a514704b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134323&auth_key=1760134323-0-0-5d3be39233e7d7c1dc25d26aba4dbb55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e16a81dd49aee3d1f10c1344a31328a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134330&auth_key=1760134330-0-0-70a4b2540ce6384bf508d42287e3a984&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Submillimeter-Accurate-3D-Lumbar-Spine-Reconstruction-from-Biplanar-X-Ray-Images-Incorporating-a-Multi-Task-Network-and-Landmark-Weighted-Loss"><a href="#Submillimeter-Accurate-3D-Lumbar-Spine-Reconstruction-from-Biplanar-X-Ray-Images-Incorporating-a-Multi-Task-Network-and-Landmark-Weighted-Loss" class="headerlink" title="Submillimeter-Accurate 3D Lumbar Spine Reconstruction from Biplanar   X-Ray Images: Incorporating a Multi-Task Network and Landmark-Weighted Loss"></a>Submillimeter-Accurate 3D Lumbar Spine Reconstruction from Biplanar   X-Ray Images: Incorporating a Multi-Task Network and Landmark-Weighted Loss</h2><p><strong>Authors:Wanxin Yu, Zhemin Zhu, Cong Wang, Yihang Bao, Chunjie Xia, Rongshan Cheng, Yan Yu, Tsung-Yuan Tsai</strong></p>
<p>To meet the clinical demand for accurate 3D lumbar spine assessment in a weight-bearing position, this study presents a novel, fully automatic framework for high-precision 3D reconstruction from biplanar X-ray images, overcoming the limitations of existing methods. The core of this method involves a novel multi-task deep learning network that simultaneously performs lumbar decomposition and landmark detection on the original biplanar radiographs. The decomposition effectively eliminates interference from surrounding tissues, simplifying subsequent image registration, while the landmark detection provides an initial pose estimation for the Statistical Shape Model (SSM), enhancing the efficiency and robustness of the registration process. Building on this, we introduce a landmark-weighted 2D-3D registration strategy. By assigning higher weights to complex posterior structures like the transverse and spinous processes during optimization, this strategy significantly enhances the reconstruction accuracy of the posterior arch. Our method was validated against a gold standard derived from registering CT segmentations to the biplanar X-rays. It sets a new benchmark by achieving sub-millimeter accuracy and completes the full reconstruction and measurement workflow in under 20 seconds, establishing a state-of-the-art combination of precision and speed. This fast and low-dose pipeline provides a powerful automated tool for diagnosing lumbar conditions such as spondylolisthesis and scoliosis in their functional, weight-bearing state. </p>
<blockquote>
<p>ä¸ºæ»¡è¶³åœ¨æ‰¿é‡çŠ¶æ€ä¸‹å¯¹ç²¾ç¡®ä¸‰ç»´è…°æ¤è¯„ä¼°çš„ä¸´åºŠéœ€æ±‚ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å…¨è‡ªåŠ¨é«˜ç²¾åº¦ä¸‰ç»´é‡å»ºæ¡†æ¶ï¼Œå¯ä»åŒå¹³é¢Xå°„çº¿å›¾åƒä¸­æ„å»ºï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºä¸€ç§æ–°å‹å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œå¯åŒæ—¶æ‰§è¡Œè…°æ¤åˆ†è§£å’ŒåŸå§‹åŒå¹³é¢æ”¾å°„å½±åƒä¸Šçš„åœ°æ ‡æ£€æµ‹ã€‚åˆ†è§£æœ‰æ•ˆåœ°æ¶ˆé™¤äº†å‘¨å›´ç»„ç»‡çš„å¹²æ‰°ï¼Œç®€åŒ–äº†åç»­å›¾åƒæ³¨å†Œï¼Œè€Œåœ°æ ‡æ£€æµ‹ä¸ºç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ï¼ˆSSMï¼‰æä¾›äº†åˆæ­¥å§¿æ€ä¼°è®¡ï¼Œæé«˜äº†æ³¨å†Œè¿‡ç¨‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åœ°æ ‡åŠ æƒ2D-3Dæ³¨å†Œç­–ç•¥ã€‚é€šè¿‡åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç»™æ¨ªå‘è¿‡ç¨‹å’Œæ£˜çªç­‰å¤æ‚åä¾§ç»“æ„åˆ†é…æ›´é«˜çš„æƒé‡ï¼Œè¯¥ç­–ç•¥å¤§å¤§æé«˜äº†åä¾§ç»“æ„çš„é‡å»ºç²¾åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸ä»åŒå¹³é¢Xå°„çº¿æ³¨å†Œå¾—åˆ°çš„CTåˆ†å‰²çš„é‡‘æ ‡å‡†è¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯ã€‚å®ƒè¾¾åˆ°äº†äºšæ¯«ç±³çº§çš„ç²¾åº¦ï¼Œå¹¶åœ¨ä¸åˆ°20ç§’å†…å®Œæˆäº†å®Œæ•´çš„é‡å»ºå’Œæµ‹é‡æµç¨‹ï¼Œå»ºç«‹äº†ç²¾ç¡®åº¦å’Œé€Ÿåº¦çš„æœ€æ–°ç»„åˆã€‚è¿™ä¸€å¿«é€Ÿã€ä½å‰‚é‡çš„æµç¨‹ä¸ºåœ¨åŠŸèƒ½æ‰¿é‡çŠ¶æ€ä¸‹è¯Šæ–­è…°æ¤ç–¾ç—…ï¼ˆå¦‚è„ŠæŸ±æ»‘è„±å’Œè„ŠæŸ±ä¾§å¼¯ï¼‰æä¾›äº†å¼ºå¤§çš„è‡ªåŠ¨åŒ–å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14573v3">PDF</a> 27 pages, 16 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å…¨è‡ªåŠ¨æ¡†æ¶ï¼Œç”¨äºä»åŒå¹³é¢Xå°„çº¿å›¾åƒè¿›è¡Œé«˜ç²¾åº¦3Dé‡å»ºï¼Œä»¥æ»¡è¶³ä¸´åºŠå¯¹è´Ÿé‡çŠ¶æ€ä¸‹å‡†ç¡®3Dè…°æ¤è¯„ä¼°çš„éœ€æ±‚ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºä¸€ç§å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œå¯åŒæ—¶æ‰§è¡Œè…°æ¤åˆ†è§£å’Œåœ°æ ‡æ£€æµ‹ï¼Œä»è€Œæ¶ˆé™¤å‘¨å›´ç»„ç»‡çš„å¹²æ‰°ï¼Œç®€åŒ–å›¾åƒæ³¨å†Œï¼Œå¹¶æä¾›åˆå§‹å§¿æ€ä¼°è®¡ï¼Œå¢å¼ºç»Ÿè®¡å½¢çŠ¶æ¨¡å‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥åœ°æ ‡åŠ æƒ2D-3Dæ³¨å†Œç­–ç•¥ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸­ç»™å¤æ‚åç»“æ„å¦‚æ¨ªçªå’Œæ£˜çªèµ‹äºˆæ›´é«˜çš„æƒé‡ï¼Œæ˜¾è‘—æé«˜åæ‹±é‡å»ºçš„å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶å®ç°äº†äºšæ¯«ç±³çº§ç²¾åº¦ï¼Œå¹¶åœ¨20ç§’å†…å®Œæˆå®Œæ•´é‡å»ºå’Œæµ‹é‡æµç¨‹ï¼Œä¸ºè¯Šæ–­è…°æ¤ç–¾ç—…å¦‚æ¤ä½“æ»‘è„±å’Œè„ŠæŸ±ä¾§å‡¸æä¾›äº†å¼ºå¤§çš„è‡ªåŠ¨åŒ–å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨æ¡†æ¶ï¼Œä»åŒå¹³é¢Xå°„çº¿å›¾åƒè¿›è¡Œé«˜ç²¾åº¦3Dè…°æ¤é‡å»ºã€‚</li>
<li>å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ ç½‘ç»œåŒæ—¶æ‰§è¡Œè…°æ¤åˆ†è§£å’Œåœ°æ ‡æ£€æµ‹ï¼Œæ¶ˆé™¤å‘¨å›´ç»„ç»‡å¹²æ‰°ï¼Œç®€åŒ–å›¾åƒæ³¨å†Œã€‚</li>
<li>åœ°æ ‡æ£€æµ‹æä¾›åˆå§‹å§¿æ€ä¼°è®¡ï¼Œå¢å¼ºç»Ÿè®¡å½¢çŠ¶æ¨¡å‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥åœ°æ ‡åŠ æƒ2D-3Dæ³¨å†Œç­–ç•¥ï¼Œä¼˜åŒ–å¤æ‚åç»“æ„çš„é‡å»ºå‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶å®ç°äº†äºšæ¯«ç±³çº§ç²¾åº¦ï¼Œå¿«é€Ÿå®Œæˆé‡å»ºå’Œæµ‹é‡æµç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ç”¨äºè¯Šæ–­è…°æ¤ç–¾ç—…ï¼Œå¦‚æ¤ä½“æ»‘è„±å’Œè„ŠæŸ±ä¾§å‡¸ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c645fbd933ee18cbc08b60e860a7226f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134337&auth_key=1760134337-0-0-ef98139e6106f84302636700054c527f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Graph-Based-Framework-for-Interpretable-Whole-Slide-Image-Analysis"><a href="#A-Graph-Based-Framework-for-Interpretable-Whole-Slide-Image-Analysis" class="headerlink" title="A Graph-Based Framework for Interpretable Whole Slide Image Analysis"></a>A Graph-Based Framework for Interpretable Whole Slide Image Analysis</h2><p><strong>Authors:Alexander Weers, Alexander H. Berger, Laurin Lux, Peter SchÃ¼ffler, Daniel Rueckert, Johannes C. Paetzold</strong></p>
<p>The histopathological analysis of whole-slide images (WSIs) is fundamental to cancer diagnosis but is a time-consuming and expert-driven process. While deep learning methods show promising results, dominant patch-based methods artificially fragment tissue, ignore biological boundaries, and produce black-box predictions. We overcome these limitations with a novel framework that transforms gigapixel WSIs into biologically-informed graph representations and is interpretable by design. Our approach builds graph nodes from tissue regions that respect natural structures, not arbitrary grids. We introduce an adaptive graph coarsening technique, guided by learned embeddings, to efficiently merge homogeneous regions while preserving diagnostically critical details in heterogeneous areas. Each node is enriched with a compact, interpretable feature set capturing clinically-motivated priors. A graph attention network then performs diagnosis on this compact representation. We demonstrate strong performance on challenging cancer staging and survival prediction tasks. Crucially, our resource-efficient model ($&gt;$13x fewer parameters and $&gt;$300x less data) achieves results competitive with a massive foundation model, while offering full interpretability through feature attribution. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HistoGraph31/pix2pathology">https://github.com/HistoGraph31/pix2pathology</a>. </p>
<blockquote>
<p>å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç»„ç»‡ç—…ç†å­¦åˆ†æå¯¹ç™Œç—‡è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”ä¾èµ–ä¸“å®¶çš„è¿‡ç¨‹ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ä¸»æµçš„åŸºäºè¡¥ä¸çš„æ–¹æ³•äººä¸ºåœ°åˆ†å‰²ç»„ç»‡ï¼Œå¿½ç•¥ç”Ÿç‰©è¾¹ç•Œï¼Œå¹¶äº§ç”Ÿé»‘ç®±é¢„æµ‹ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ–°å‹æ¡†æ¶å…‹æœäº†è¿™äº›é™åˆ¶ï¼Œè¯¥æ¡†æ¶å°†å‰åƒç´ WSIè½¬æ¢ä¸ºå…·æœ‰ç”Ÿç‰©å­¦ä¿¡æ¯çš„å›¾å½¢è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è®¾è®¡å…·æœ‰å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¹æ®è‡ªç„¶ç»“æ„è€Œä¸æ˜¯ä»»æ„ç½‘æ ¼æ„å»ºå›¾å½¢èŠ‚ç‚¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å›¾å½¢ç²—åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å­¦ä¹ çš„åµŒå…¥è¿›è¡Œå¼•å¯¼ï¼Œä»¥æœ‰æ•ˆåœ°åˆå¹¶åŒè´¨åŒºåŸŸï¼ŒåŒæ—¶ä¿ç•™å¼‚è´¨åŒºåŸŸçš„è¯Šæ–­ç»†èŠ‚ã€‚æ¯ä¸ªèŠ‚ç‚¹éƒ½é€šè¿‡æ•æ‰ä¸´åºŠé©±åŠ¨çš„å…ˆéªŒçŸ¥è¯†ï¼Œé…å¤‡äº†ä¸€å¥—ç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾ã€‚ç„¶åï¼Œå›¾æ³¨æ„åŠ›ç½‘ç»œåœ¨æ­¤ç´§å‡‘è¡¨ç¤ºä¸Šè¿›è¡Œè¯Šæ–­ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç™Œç—‡åˆ†æœŸå’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„èµ„æºé«˜æ•ˆå‹æ¨¡å‹ï¼ˆå‚æ•°å°‘13å€ä»¥ä¸Šï¼Œæ•°æ®å°‘300å€ä»¥ä¸Šï¼‰å®ç°äº†ä¸å¤§å‹åŸºç¡€æ¨¡å‹ç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶é€šè¿‡ç‰¹å¾å½’å±æä¾›å®Œå…¨çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/HistoGraph31/pix2pathology">https://github.com/HistoGraph31/pix2pathology</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11846v2">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…¨æ–°çš„ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ç—…ç†åˆ†æçš„æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿå°†å·¨åƒç´ WSIè½¬åŒ–ä¸ºå…·æœ‰ç”Ÿç‰©å­¦ä¿¡æ¯çš„å›¾å½¢è¡¨ç¤ºï¼Œé€šè¿‡æ„å»ºå°Šé‡è‡ªç„¶ç»“æ„çš„å›¾å½¢èŠ‚ç‚¹ï¼Œå®ç°æ›´å‡†ç¡®çš„ç™Œç—‡è¯Šæ–­ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„å›¾ç®€åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™è¯Šæ–­ç»†èŠ‚çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°åˆå¹¶åŒè´¨åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…·æœ‰ç´§å‡‘ã€å¯è§£é‡Šçš„ç‰¹å¾é›†ï¼Œå¹¶é€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œè¯Šæ–­ã€‚è¯¥æ¨¡å‹åœ¨ç™Œç—‡åˆ†æœŸå’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¸”èµ„æºåˆ©ç”¨ç‡é«˜ï¼Œå¯å®ç°ç‰¹å¾å½’å±çš„å®Œå…¨å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†åˆ†æåœ¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­è‡³å…³é‡è¦ï¼Œä½†è¿‡ç¨‹è€—æ—¶ä¸”ä¾èµ–ä¸“å®¶ã€‚</li>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç—…ç†åˆ†æä¸­å­˜åœ¨å±€é™æ€§ï¼Œå¦‚åŸºäºè¡¥ä¸çš„æ–¹æ³•å¯èƒ½ä¼šäººä¸ºåœ°ç ´åç»„ç»‡ï¼Œå¿½è§†ç”Ÿç‰©è¾¹ç•Œï¼Œå¹¶äº§ç”Ÿé»‘ç®±é¢„æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†å·¨åƒç´ WSIè½¬åŒ–ä¸ºå…·æœ‰ç”Ÿç‰©å­¦ä¿¡æ¯çš„å›¾å½¢è¡¨ç¤ºï¼Œå°Šé‡è‡ªç„¶ç»“æ„ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”å›¾ç®€åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å­¦ä¹ çš„åµŒå…¥è¿›è¡Œå¼•å¯¼ï¼Œæœ‰æ•ˆåˆå¹¶åŒè´¨åŒºåŸŸï¼ŒåŒæ—¶ä¿ç•™å¼‚è´¨åŒºåŸŸçš„è¯Šæ–­ç»†èŠ‚ã€‚</li>
<li>æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«ç´§å‡‘ã€å¯è§£é‡Šçš„ç‰¹å¾é›†ï¼Œè¿™äº›ç‰¹å¾é›†æ•æ‰äº†ä¸´åºŠåŠ¨æœºçš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œè¯Šæ–­ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç™Œç—‡åˆ†æœŸå’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ad39be852ed86510f61ca329fb3830e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134345&auth_key=1760134345-0-0-93dd11f2dcc3c511172f16a6298a5df6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c34f0784e1846959ac19741b5428314e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134352&auth_key=1760134352-0-0-e73cd7d5875750ab5b28f70e30f8afc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdee39bdc0a3b927b0c5d575893121a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134359&auth_key=1760134359-0-0-36416279630d5883d3d529446a88590a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c33a255586c6ecc7491c7a6452779876~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134365&auth_key=1760134365-0-0-b6818b05691e6f44d2d4d66ebf37d36b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba0de929bbb054ad994690369d8c0496~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134372&auth_key=1760134372-0-0-b249e533ad27aa72a9efae86d43315aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bea3abad3ed3fa41c016f152d4e7431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134378&auth_key=1760134378-0-0-6ec0f4e9745d41baf6789376d0beec1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging"><a href="#H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging" class="headerlink" title="H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging"></a>H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging</h2><p><strong>Authors:Zhen Huang, Tao Tang, Ronghao Xu, Yangbo Wei, Wenkai Yang, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao</strong></p>
<p>3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.   In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection \textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights. </p>
<blockquote>
<p>ä¸‰ç»´ï¼ˆ3Dï¼‰æ ‡å¿—ç‚¹æ£€æµ‹æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œå‡†ç¡®æ£€æµ‹è§£å‰–æ ‡å¿—ç‚¹å¯¹äºåç»­åŒ»å­¦æˆåƒä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„ä¸»æµæ·±åº¦å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨ç²¾ç»†å±€éƒ¨ç‰¹å¾æ•æ‰ã€å…¨å±€ç©ºé—´å…³ç³»å»ºæ¨¡ã€å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´ä¿æŒå¹³è¡¡ã€‚å±€éƒ¨ç‰¹å¾æå–éœ€è¦æ•æ‰ç²¾ç»†çš„è§£å‰–ç»†èŠ‚ï¼Œè€Œå…¨å±€å»ºæ¨¡åˆ™éœ€è¦ç†è§£å¤æ‚è§£å‰–ç»“æ„å†…çš„ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œç”±äºæ ‡å¿—ç‚¹ç¨€ç–åˆ†å¸ƒï¼Œä¸‰ç»´ä½“ç§¯çš„é«˜ç»´æ€§è´¨è¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›æŒ‘æˆ˜ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œå®ç°é«˜æ•ˆä¸”ç²¾ç¡®çš„3Dæ ‡å¿—ç‚¹æ£€æµ‹ä»ç„¶æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€ä¸ªç´§è¿«æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¸‰ç»´æ£€æµ‹ç½‘ç»œï¼ˆHybrid 3D Detection Netï¼Œç®€ç§°H3DE-Netï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç”¨äºå±€éƒ¨ç‰¹å¾æå–å’Œä¸€ä¸ªè½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨é«˜æ•ˆæ•æ‰ä¸‰ç»´ä½“ç§¯æ•°æ®ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚è¯¥æœºåˆ¶é‡‡ç”¨åˆ†å±‚è·¯ç”±ç­–ç•¥æ¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒH3DE-Netæ˜¯ç¬¬ä¸€ä¸ªå°†æ­¤ç±»è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ä¸CNNç›¸ç»“åˆçš„3Dæ ‡å¿—ç‚¹æ£€æµ‹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ•´åˆå¤šå°ºåº¦ç‰¹å¾èåˆè¿›ä¸€æ­¥æé«˜äº†æ£€æµ‹ç²¾åº¦å’Œç¨³å¥æ€§ã€‚åœ¨å…¬å…±CTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒH3DE-Netè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºå¤±æ ‡å¿—ç‚¹æˆ–å¤æ‚è§£å‰–ç»“æ„å˜å¼‚çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬å·²ç»å¼€æºäº†æˆ‘ä»¬çš„é¡¹ç›®ï¼ŒåŒ…æ‹¬ä»£ç ã€æ•°æ®å’Œæ¨¡å‹æƒé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14221v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ··åˆä¸‰ç»´æ£€æµ‹ç½‘ç»œï¼ˆH3DE-Netï¼‰ï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶é«˜æ•ˆæ•æ‰ä¸‰ç»´ä½“ç§¯æ•°æ®ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚è·¯ç”±ç­–ç•¥ï¼Œé™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH3DE-Netåœ¨å…¬å¼€CTæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç¼ºå¤±åœ°æ ‡æˆ–å¤æ‚è§£å‰–å˜å¼‚æƒ…å†µä¸‹ï¼Œæé«˜å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D landmarkæ£€æµ‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­è‡³å…³é‡è¦ï¼Œä½†ä¸»æµæ·±åº¦å­¦ä¹ æ–¹æ³•éš¾ä»¥å¹³è¡¡ç²¾ç»†å±€éƒ¨ç‰¹å¾ä¸å…¨å±€ç©ºé—´å…³ç³»çš„æ•æ‰ã€‚</li>
<li>å±€éƒ¨ç‰¹å¾æå–éœ€è¦æ•æ‰ç»†è‡´çš„è§£å‰–ç»†èŠ‚ï¼Œè€Œå…¨å±€å»ºæ¨¡éœ€è¦ç†è§£å¤æ‚è§£å‰–ç»“æ„å†…çš„ç©ºé—´å…³ç³»ã€‚</li>
<li>H3DE-Netç»“åˆCNNå’Œè½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œé«˜æ•ˆæ•æ‰ä¸‰ç»´ä½“ç§¯æ•°æ®ä¸­çš„å…¨å±€ä¾èµ–ã€‚</li>
<li>åˆ†å±‚è·¯ç”±ç­–ç•¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</li>
<li>H3DE-Netæ˜¯é¦–ä¸ªç»“åˆè½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶å’ŒCNNçš„3D landmarkæ£€æµ‹æ¨¡å‹ã€‚</li>
<li>å¤šå°ºåº¦ç‰¹å¾èåˆè¿›ä¸€æ­¥æé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-915501af2f98253bae1c0a691393a7bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134386&auth_key=1760134386-0-0-d4c469a814462ca11e907e381862669a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43ea667fa313cad50fa94792383c3e76~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134393&auth_key=1760134393-0-0-438c011f9b80fac58de93b9736b528c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-44c29e4a7f8a0890e727266b7e798f95~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134399&auth_key=1760134399-0-0-33c361c4347c7099e4b9cc1fe601e702&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2421b59cbea368c5f99c3ecfbff3b9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134406&auth_key=1760134406-0-0-05fe887ea352c998e4eeb4de22638326&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-68489d6d976bde2a48736db2a974242e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134413&auth_key=1760134413-0-0-527fe8c762191000817bc6cebb840b10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-179e82adbe16f3f4ed6f5b3cc246c54c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134419&auth_key=1760134419-0-0-f0c94b2adc3b18cbe3c6bb65c6e09a2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b1e1df5b33f5a558477ed2ee69aca01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134426&auth_key=1760134426-0-0-f41888b80b82bb8a8bf3667a181179f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention"><a href="#Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention" class="headerlink" title="Medical Image Classification with KAN-Integrated Transformers and   Dilated Neighborhood Attention"></a>Medical Image Classification with KAN-Integrated Transformers and   Dilated Neighborhood Attention</h2><p><strong>Authors:Omid Nejati Manzari, Hojat Asgariandehkordi, Taha Koleilat, Yiming Xiao, Hassan Rivaz</strong></p>
<p>Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6% on MedMNIST, 5.8% on NonMNIST, and 13.4% on the MedMNIST-C benchmark. </p>
<blockquote>
<p>å·ç§¯ç½‘ç»œã€å˜å‹å™¨ã€æ··åˆæ¨¡å‹å’ŒåŸºäºMambaçš„æ¶æ„å·²åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦æ˜¯ä¸ºä½¿ç”¨å¸¦æ ‡ç­¾æ•°æ®å¯¹å¹²å‡€å›¾åƒè¿›è¡Œåˆ†ç±»è€Œè®¾è®¡çš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç°å®ä¸–ç•Œçš„ä¸´åºŠæ•°æ®é€šå¸¸æ¶‰åŠå¤šä¸­å¿ƒç ”ç©¶ç‹¬æœ‰çš„å›¾åƒæŸåé—®é¢˜ï¼Œä»¥åŠæ¥è‡ªä¸åŒåˆ¶é€ å•†çš„æˆåƒè®¾å¤‡ä¹‹é—´çš„å·®å¼‚æ‰€å¯¼è‡´çš„å›¾åƒæŸåé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»ç–—è§†è§‰è½¬æ¢å™¨ï¼ˆMedViTV2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé¦–æ¬¡å°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚çº³å…¥è½¬æ¢å™¨æ¶æ„ä¸­ï¼Œæ—¨åœ¨å®ç°é€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„KANå—ï¼Œä»¥å‡å°‘è®¡ç®—è´Ÿè½½åŒæ—¶æé«˜åŸå§‹MedViTçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æŠµæ¶ˆæˆ‘ä»¬MedViTåœ¨æ‰©å¤§è§„æ¨¡æ—¶çš„è„†å¼±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„è†¨èƒ€é‚»åŸŸæ³¨æ„åŠ›ï¼ˆDiNAï¼‰ï¼Œè¿™æ˜¯å¯¹é«˜æ•ˆçš„èåˆç‚¹ç§¯æ³¨æ„åŠ›æ ¸å¿ƒçš„é€‚åº”ï¼Œèƒ½å¤Ÿæ•è·å…¨å±€ä¸Šä¸‹æ–‡å¹¶æ‰©å±•æ„Ÿå—é‡ï¼Œä»¥æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å¹¶è§£å†³ç‰¹å¾å´©æºƒé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ†å±‚æ··åˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆçš„æ–¹å¼å †å æˆ‘ä»¬çš„å±€éƒ¨ç‰¹å¾æ„ŸçŸ¥å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥å—ï¼Œè¿™å¹³è¡¡äº†å±€éƒ¨å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥ä»¥æé«˜æ€§èƒ½ã€‚åœ¨17ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†å’Œ12ä¸ªæŸååŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedViTV2åœ¨29æ¬¡å®éªŒä¸­çš„27æ¬¡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå¹¶ä¸”è®¡ç®—å¤æ‚åº¦é™ä½ã€‚MedViTV2çš„è®¡ç®—æ•ˆç‡æ¯”å‰ä¸€ä¸ªç‰ˆæœ¬é«˜å‡º44ï¼…ï¼Œå¹¶ä¸”å¤§å¤§æé«˜äº†å‡†ç¡®æ€§ï¼Œåœ¨MedMNISTä¸Šæé«˜äº†4.6ï¼…ï¼Œåœ¨NonMNISTä¸Šæé«˜äº†5.8ï¼…ï¼Œåœ¨MedMNIST-CåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†13.4ï¼…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13693v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†Medical Vision Transformer V2ï¼ˆMedViTV2ï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„é¦–æ¬¡å°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚èå…¥transformeræ¶æ„ä¸­ï¼Œæ—¨åœ¨å®ç°é€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚é€šè¿‡å¼€å‘é«˜æ•ˆçš„KANå—ï¼Œå‡å°‘äº†è®¡ç®—è´Ÿè½½ï¼Œæé«˜äº†åŸå§‹MedViTçš„å‡†ç¡®æ€§ã€‚ä¸ºåº”å¯¹æ¨¡å‹æ”¾å¤§åçš„è„†å¼±æ€§ï¼Œæå‡ºäº†å¢å¼ºçš„Dilated Neighborhood Attentionï¼ˆDiNAï¼‰ï¼Œå…¶èƒ½æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å¹¶æ‰©å±•æ„Ÿå—é‡ï¼Œæœ‰æ•ˆæ‰©å±•æ¨¡å‹å¹¶è§£å†³ç‰¹å¾å´©æºƒé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åˆ†å±‚æ··åˆç­–ç•¥ï¼Œä»¥å¹³è¡¡æœ¬åœ°å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥å—ï¼Œæé«˜æ€§èƒ½ã€‚åœ¨17ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†å’Œ12ä¸ªè…è´¥åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedViTV2åœ¨27é¡¹å®éªŒä¸­è·å¾—äº†æœ€æ–°ç»“æœï¼Œè®¡ç®—å¤æ‚åº¦é™ä½ã€‚ä¸å‰ä¸€ç‰ˆæœ¬ç›¸æ¯”ï¼ŒMedViTV2è®¡ç®—æ•ˆç‡æé«˜äº†44%ï¼Œå¹¶ä¸”åœ¨MedMNISTã€NonMNISTå’ŒMedMNIST-CåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†4.6%ã€5.8%å’Œ13.4%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MedViTV2ç»“åˆäº†å·ç§¯ç½‘ç»œã€transformerã€æ··åˆæ¨¡å‹å’ŒåŸºäºMambaçš„æ¶æ„çš„ä¼˜åŠ¿ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚ä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æå‡ºDilated Neighborhood Attentionï¼ˆDiNAï¼‰ä»¥å¢å¼ºæ¨¡å‹çš„å¥å£®æ€§å¹¶æ‰©å¤§æ„Ÿå—é‡ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚æ··åˆç­–ç•¥å¹³è¡¡æœ¬åœ°å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜MedViTV2å…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>ä¸å…ˆå‰çš„æ¨¡å‹ç›¸æ¯”ï¼ŒMedViTV2åœ¨è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®ç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a4a0e0023f890c0b230693aac78fdc9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134434&auth_key=1760134434-0-0-72d8c7b8d0a6c4693bcb65a331661596&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65093cf0573e4d4e02293e38fa2fc882~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134441&auth_key=1760134441-0-0-59261ec397d265177c7657cc11ce2e15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34be78d7dcefba8d7265a1e2e265150f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134448&auth_key=1760134448-0-0-8900481b4ccbf5f46e633276d6045398&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-33686396df5a918b34909ee013ebfbd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135725&auth_key=1760135725-0-0-eb6e51862bc1f8cf44096ab0ae86febc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  DialoSpeech Dual-Speaker Dialogue Generation with LLM and Flow Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-96b0b56ffe742bd505738bdd8e536a8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760132053&auth_key=1760132053-0-0-f5e21e90ce190179da897f7d41423e8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  Biology-driven assessment of deep learning super-resolution imaging of   the porosity network in dentin
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
