<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  VideoCanvas Unified Video Completion from Arbitrary Spatiotemporal   Patches via In-Context Conditioning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-dc65fdb07455bd0cd939dd3c7bd174d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131310&auth_key=1760131310-0-0-ed7b1512ebb0040a9136e03995fe88a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-11-æ›´æ–°"><a href="#2025-10-11-æ›´æ–°" class="headerlink" title="2025-10-11 æ›´æ–°"></a>2025-10-11 æ›´æ–°</h1><h2 id="VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning"><a href="#VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning" class="headerlink" title="VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal   Patches via In-Context Conditioning"></a>VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal   Patches via In-Context Conditioning</h2><p><strong>Authors:Minghong Cai, Qiulin Wang, Zongli Ye, Wenze Liu, Quande Liu, Weicai Ye, Xintao Wang, Pengfei Wan, Kun Gai, Xiangyu Yue</strong></p>
<p>We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasksâ€“including first-frame image-to-video, inpainting, extension, and interpolationâ€“under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAEâ€™s temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨çš„çš„ä»»åŠ¡ï¼Œå³ç”Ÿæˆä¸€ä¸ªè§†é¢‘ï¼Œè¯¥è§†é¢‘ç”±ç”¨æˆ·æŒ‡å®šçš„ä»»æ„è¡¥ä¸åœ¨ä»»æ„ç©ºé—´ä½ç½®å’Œæ—¶é—´æˆ³æ”¾ç½®ï¼Œå°±åƒåœ¨è§†é¢‘ç”»å¸ƒä¸Šç»˜ç”»ä¸€æ ·ã€‚è¿™ç§çµæ´»çš„å…¬å¼è‡ªç„¶åœ°ç»Ÿä¸€äº†è®¸å¤šç°æœ‰çš„å¯æ§è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬ä»ç¬¬ä¸€å¸§å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆã€å›¾åƒä¿®å¤ã€æ‰©å±•å’Œæ’å€¼ï¼Œåœ¨ä¸€ä¸ªè¿è´¯çš„æ¨¡å¼ä¸‹ã€‚ç„¶è€Œï¼Œå®ç°è¿™ä¸€æ„¿æ™¯é¢ä¸´ç€ç°ä»£æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æ ¹æœ¬éšœç¢ï¼šå› æœè‡ªç¼–ç å™¨å¼•å…¥çš„æ—¶é—´æ¨¡ç³Šï¼Œå¤šä¸ªåƒç´ å¸§è¢«å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„æ½œåœ¨è¡¨ç¤ºï¼Œä½¿å¾—ç²¾ç¡®å¸§çº§çš„æ¡ä»¶ç»“æ„åŒ–å˜å¾—å›°éš¾ã€‚æˆ‘ä»¬é€šè¿‡VideoCanvasæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ä¸Šä¸‹æ–‡æ¡ä»¶ï¼ˆICCï¼‰èŒƒå¼é€‚åº”äºè¿™é¡¹ç²¾ç»†çš„æ§åˆ¶ä»»åŠ¡ï¼Œæ— éœ€ä»»ä½•æ–°å‚æ•°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¡ä»¶ç­–ç•¥ï¼Œå°†ç©ºé—´å’Œæ—¶é—´æ§åˆ¶è§£è€¦ï¼šç©ºé—´ä½ç½®é€šè¿‡é›¶å¡«å……å¤„ç†ï¼Œè€Œæ—¶é—´å¯¹é½åˆ™é€šè¿‡Temporal RoPEæ’å€¼å®ç°ï¼Œä¸ºæ¯ä¸ªæ¡ä»¶åœ¨æ½œåœ¨åºåˆ—å†…åˆ†é…ä¸€ä¸ªè¿ç»­çš„åˆ†æ•°ä½ç½®ã€‚è¿™è§£å†³äº†è‡ªç¼–ç å™¨çš„æ—¶åºæ¨¡ç³Šé—®é¢˜ï¼Œå¹¶åœ¨å†»ç»“çš„éª¨å¹²ç½‘ä¸Šå®ç°äº†åƒç´ å¸§æ„ŸçŸ¥æ§åˆ¶ã€‚ä¸ºäº†è¯„ä¼°è¿™ç§æ–°èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†VideoCanvasBenchï¼Œè¿™æ˜¯ä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†åœºæ™¯å†…çš„ä¿çœŸåº¦å’Œåœºæ™¯é—´çš„åˆ›é€ åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒVideoCanvasæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¡ä»¶èŒƒå¼ï¼Œåœ¨çµæ´»ç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆæ–¹é¢å»ºç«‹äº†æ–°çš„æŠ€æœ¯é¢†å…ˆåœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08555v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://onevfall.github.io/project_page/videocanvas">https://onevfall.github.io/project_page/videocanvas</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨çš„ç”Ÿæˆä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ä»»æ„ç©ºé—´ä½ç½®å’Œç‰¹å®šæ—¶é—´æˆ³ç”Ÿæˆè§†é¢‘ç‰‡æ®µçš„æ–°æŠ€æœ¯ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºVideoCanvasçš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨é›¶å‚æ•°ä¸Šä¸‹æ–‡æ¡ä»¶æ³•è§£å†³äº†ç°æœ‰æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´æ¨¡ç³Šé—®é¢˜ã€‚é€šè¿‡ç©ºé—´é›¶å¡«å……å’Œæ—¶é—´RoPEæ’å€¼æŠ€æœ¯ï¼Œå®ç°äº†åƒç´ çº§ç²¾ç»†æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒVideoCanvasåœ¨ä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¡ä»¶æ¨¡å‹ï¼Œå»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨ä»»åŠ¡ä»‹ç»ï¼šè¯¥æŠ€æœ¯å…è®¸åœ¨è§†é¢‘çš„ä»»æ„ä½ç½®å’Œæ—¶é—´ç”Ÿæˆæ–°çš„ç‰‡æ®µã€‚</li>
<li>VideoCanvasæ¡†æ¶è§£å†³äº†ç°æœ‰æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>VideoCanvasåˆ©ç”¨é›¶å‚æ•°ä¸Šä¸‹æ–‡æ¡ä»¶æ³•å®ç°ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>ç©ºé—´æ§åˆ¶é€šè¿‡é›¶å¡«å……å®ç°ï¼Œæ—¶é—´æ§åˆ¶åˆ™é€šè¿‡Temporal RoPEæ’å€¼å®ç°ã€‚</li>
<li>VideoCanvasæ˜¾è‘—æé«˜äº†ä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>VideoCanvasBenchä½œä¸ºé¦–ä¸ªé’ˆå¯¹ä»»æ„æ—¶ç©ºè§†é¢‘è¡¥å…¨ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œåˆ›æ–°æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3753d817739986b0d9a50c0ba0dc0b4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130746&auth_key=1760130746-0-0-448595a9fa35173e3924244377d67839&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ca6dfe44d877537795c7c38fd0f7626~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130753&auth_key=1760130753-0-0-b82b4e31cd068937eae16ac3b8014643&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1b47cc32ec615cc48e664958e7591c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130760&auth_key=1760130760-0-0-60e6a428b1aff9ebdff032477b919275&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="X2Video-Adapting-Diffusion-Models-for-Multimodal-Controllable-Neural-Video-Rendering"><a href="#X2Video-Adapting-Diffusion-Models-for-Multimodal-Controllable-Neural-Video-Rendering" class="headerlink" title="X2Video: Adapting Diffusion Models for Multimodal Controllable Neural   Video Rendering"></a>X2Video: Adapting Diffusion Models for Multimodal Controllable Neural   Video Rendering</h2><p><strong>Authors:Zhitong Huang, Mohan Zhang, Renhan Wang, Rui Tang, Hao Zhu, Jing Liao</strong></p>
<p>We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: <a target="_blank" rel="noopener" href="https://luckyhzt.github.io/x2video">https://luckyhzt.github.io/x2video</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†X2Videoï¼Œè¿™æ˜¯ä¸€æ¬¾é¦–ä¸ªé‡‡ç”¨å†…åœ¨é€šé“å¼•å¯¼ï¼ˆåŒ…æ‹¬äº®åº¦ã€æ³•çº¿ã€ç²—ç³™åº¦ã€é‡‘å±æ€§å’Œè¾ç…§åº¦ï¼‰çš„å…‰ç…§å†™å®è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚å®ƒæ”¯æŒä½¿ç”¨å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹å…¨å±€å’Œå±€éƒ¨åŒºåŸŸè¿›è¡Œç›´è§‚çš„å¤šæ¨¡å¼æ§åˆ¶ã€‚å†…åœ¨å¼•å¯¼å…è®¸å‡†ç¡®æ“ä½œé¢œè‰²ã€æè´¨ã€å‡ ä½•å’Œå…‰ç…§ï¼Œè€Œå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºåˆ™åœ¨ç¼ºä¹å†…åœ¨ä¿¡æ¯çš„æƒ…å†µä¸‹æä¾›ç›´è§‚çš„è°ƒæ•´ã€‚ä¸ºäº†å®ç°è¿™äº›åŠŸèƒ½ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨æ–°é¢–ä¸”é«˜æ•ˆæ··åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å†…åœ¨å¼•å¯¼å›¾åƒç”Ÿæˆæ¨¡å‹XRGBæ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆã€‚è¿™ç¡®ä¿äº†è§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ï¼Œå¹¶æé«˜äº†å¯¹å‚è€ƒå›¾åƒçš„ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†é®ç½©äº¤å‰æ³¨æ„åŠ›ï¼Œä»¥åŒºåˆ†å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬æç¤ºï¼Œå¹¶å°†å…¶æœ‰æ•ˆåº”ç”¨äºå„è‡ªçš„å±€éƒ¨å’Œå…¨å±€åŒºåŸŸã€‚ä¸ºäº†ç”Ÿæˆé•¿è§†é¢‘ï¼Œæˆ‘ä»¬çš„æ–°é¢–é€’å½’é‡‡æ ·æ–¹æ³•ç»“åˆäº†æ¸è¿›å¸§é‡‡æ ·ï¼Œç»“åˆå…³é”®å¸§é¢„æµ‹å’Œå¸§æ’å€¼ï¼Œä»¥ä¿æŒé•¿æ—¶ç¨‹çš„æ—¶é—´è¿è´¯æ€§ï¼ŒåŒæ—¶é˜²æ­¢è¯¯å·®ç´¯ç§¯ã€‚ä¸ºäº†æ”¯æŒX2Videoçš„è®­ç»ƒï¼Œæˆ‘ä»¬ç»„è£…äº†ä¸€ä¸ªåä¸ºInteriorVideoçš„è§†é¢‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ª295ä¸ªå®¤å†…åœºæ™¯çš„1154ä¸ªæˆ¿é—´ï¼Œå…·å¤‡å¯é çš„åœ°é¢çœŸå®å†…åœ¨é€šé“åºåˆ—å’Œå¹³æ»‘çš„ç›¸æœºè½¨è¿¹ã€‚å®šæ€§å’Œå®šé‡è¯„ä¼°å‡è¡¨æ˜ï¼ŒX2Videoå¯ä»¥ç”Ÿæˆç”±å†…åœ¨æ¡ä»¶å¼•å¯¼çš„é•¿ã€æ—¶é—´è¿è´¯ä¸”å…‰ç…§å†™å®çš„è§†é¢‘ã€‚æ­¤å¤–ï¼ŒX2Videoæœ‰æ•ˆåœ°é€‚åº”äº†å¤šæ¨¡å¼æ§åˆ¶ï¼Œä½¿ç”¨å‚è€ƒå›¾åƒã€å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬æç¤ºï¼ŒåŒæ—¶æ”¯æŒé€šè¿‡å‚æ•°è°ƒæ•´è¿›è¡Œé¢œè‰²ã€æè´¨ã€å‡ ä½•å’Œå…‰ç…§ç¼–è¾‘ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://luckyhzt.github.io/x2video">https://luckyhzt.github.io/x2video</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08530v1">PDF</a> Code, model, and dataset will be released at project page soon:   <a target="_blank" rel="noopener" href="https://luckyhzt.github.io/x2video">https://luckyhzt.github.io/x2video</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†X2Videoï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡å†…åœ¨é€šé“ï¼ˆåŒ…æ‹¬æè´¨ã€å…‰ç…§ç­‰ï¼‰å¼•å¯¼çš„å…‰å†™å®è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ”¯æŒç›´è§‚çš„å¤šæ¨¡å¼æ§åˆ¶ï¼Œå¯ä»¥é€šè¿‡å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹å…¨å±€å’Œå±€éƒ¨åŒºåŸŸè¿›è¡Œæ“æ§ã€‚å†…åœ¨å¼•å¯¼å¯å®ç°é¢œè‰²ã€æè´¨ã€å‡ ä½•å’Œå…‰ç…§çš„ç²¾ç¡®æ“æ§ã€‚ä¸ºæ”¯æŒè¿™äº›åŠŸèƒ½ï¼Œæœ¬æ–‡æ‰©å±•äº†å†…åœ¨å¼•å¯¼å›¾åƒç”Ÿæˆæ¨¡å‹XRGBï¼Œå°†å…¶åº”ç”¨äºè§†é¢‘ç”Ÿæˆã€‚é‡‡ç”¨é«˜æ•ˆæ··åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿è§†é¢‘å¸§é—´çš„æ—¶åºä¸€è‡´æ€§å¹¶æé«˜å‚è€ƒå›¾åƒçš„ä¿çœŸåº¦ã€‚è¿›ä¸€æ­¥å‘å±•äº†æ©è†œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬æç¤ºæœ‰æ•ˆåœ°åº”ç”¨äºç›¸åº”åŒºåŸŸã€‚å¯¹äºé•¿è§†é¢‘çš„ç”Ÿæˆï¼Œæœ¬æ–‡æå‡ºé€’å½’é‡‡æ ·æ–¹æ³•ï¼Œç»“åˆå…³é”®å¸§é¢„æµ‹å’Œå¸§æ’å€¼ï¼Œç»´æŒé•¿æ—¶åºä¸€è‡´æ€§å¹¶é˜²æ­¢è¯¯å·®ç´¯ç§¯ã€‚ä¸ºæ”¯æŒX2Videoçš„è®­ç»ƒï¼Œæœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªåä¸ºInteriorVideoçš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª295ä¸ªå†…éƒ¨åœºæ™¯çš„1,154ä¸ªæˆ¿é—´çš„å¯é åœ°é¢çœŸå®å†…åœ¨é€šé“åºåˆ—å’Œå¹³æ»‘ç›¸æœºè½¨è¿¹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒX2Videoå¯ç”Ÿæˆé•¿æ—¶é—´ä¸€è‡´ä¸”å…‰å†™å®æ€§çš„è§†é¢‘ï¼Œå¹¶æœ‰æ•ˆå®ç°å¤šæ¨¡å¼æ§åˆ¶åŠå…¨å±€å’Œå±€éƒ¨æ–‡æœ¬æç¤ºçš„ç¼–è¾‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>X2Videoæ˜¯é¦–ä¸ªç»“åˆå†…åœ¨é€šé“ï¼ˆåŒ…æ‹¬æè´¨ã€å…‰ç…§ç­‰ï¼‰çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå…‰å†™å®è§†é¢‘ã€‚</li>
<li>æ¨¡å‹æ”¯æŒé€šè¿‡å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹å…¨å±€å’Œå±€éƒ¨åŒºåŸŸè¿›è¡Œç›´è§‚è°ƒæ•´ã€‚</li>
<li>é‡‡ç”¨é«˜æ•ˆæ··åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿è§†é¢‘å¸§é—´çš„æ—¶åºä¸€è‡´æ€§ã€‚</li>
<li>æ©è†œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä½¿å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬æç¤ºèƒ½æ›´ç²¾å‡†ä½œç”¨äºç›¸åº”åŒºåŸŸã€‚</li>
<li>æå‡ºé€’å½’é‡‡æ ·æ–¹æ³•ä»¥ç”Ÿæˆé•¿è§†é¢‘ï¼Œç»´æŒé•¿æ—¶é—´çš„ä¸€è‡´æ€§å’Œé˜²æ­¢è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>åˆ›å»ºäº†InteriorVideoæ•°æ®é›†ï¼Œæä¾›å¯é åœ°é¢çœŸå®å†…åœ¨é€šé“åºåˆ—å’Œå¹³æ»‘ç›¸æœºè½¨è¿¹ã€‚</li>
<li>X2Videoåœ¨ç”Ÿæˆé•¿ã€æ—¶é—´ä¸Šä¸€è‡´ä¸”å…‰å†™å®è§†é¢‘æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶æ”¯æŒå¤šæ¨¡å¼æ§åˆ¶å’Œç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ae1ac4519ab58418147ddf0b02c82a69~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130767&auth_key=1760130767-0-0-5f734da77fdb44841dd7494ea8ae49be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af802126ee563bc593d48b49532df268~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130774&auth_key=1760130774-0-0-15f72dc9a5ee368d2f74e6c1f7164a10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8d678a4ee2009c3835a140ecd66d60b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130780&auth_key=1760130780-0-0-4defdf441d2790fa8be32761f1d226a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cdf50d3a4c96a24e41a3c970a2cf19f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130787&auth_key=1760130787-0-0-67ebb1c4274d489a372801312d7e4a32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance"><a href="#InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance" class="headerlink" title="InstructX: Towards Unified Visual Editing with MLLM Guidance"></a>InstructX: Towards Unified Visual Editing with MLLM Guidance</h2><p><strong>Authors:Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He</strong></p>
<p>With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›æ—¥ç›Šæ˜¾ç°ï¼Œäººä»¬å¯¹å…¶æ”¹è¿›æ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ€§èƒ½çš„å…´è¶£ä¹Ÿåœ¨å¢é•¿ã€‚å°½ç®¡è¿›å±•è¿…é€Ÿï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ç¼ºä¹å¯¹MLLMè®¾è®¡é€‰æ‹©çš„æ·±å…¥åˆ†æã€‚æ­¤å¤–ï¼Œå°†MLLMså’Œæ‰©æ•£æ¨¡å‹é›†æˆåœ¨ä¸€èµ·åœ¨æŸäº›å›°éš¾çš„ä»»åŠ¡ï¼ˆå¦‚è§†é¢‘ç¼–è¾‘ï¼‰ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†InstructXï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå›¾åƒå’Œè§†é¢‘ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹é›†æˆMLLMså’Œæ‰©æ•£æ¨¡å‹ä»¥è¿›è¡ŒæŒ‡ä»¤é©±åŠ¨çš„è·¨å¤šç§ä»»åŠ¡çš„ç¼–è¾‘è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚åŸºäºè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ†æäº†å›¾åƒå’Œè§†é¢‘åœ¨ç»Ÿä¸€å»ºæ¨¡ä¸­çš„åˆä½œå’ŒåŒºåˆ«ã€‚ï¼ˆ1ï¼‰æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥åœ¨æ²¡æœ‰æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹äº§ç”Ÿè§†é¢‘ç¼–è¾‘èƒ½åŠ›ï¼Œä»è€Œå‡è½»äº†å› è§†é¢‘è®­ç»ƒæ•°æ®ç¨€ç¼ºè€Œå—åˆ°çš„çº¦æŸã€‚ï¼ˆ2ï¼‰é€šè¿‡èå…¥æ¨¡æ€ç‰¹å®šçš„MLLMç‰¹æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¤„ç†å¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ï¼Œåˆ©ç”¨å…¶æå‡æ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ€§èƒ½æˆä¸ºäº†ç ”ç©¶çƒ­ç‚¹ã€‚æœ¬æ–‡æå‡ºInstructXæ¡†æ¶ï¼Œç”¨äºå›¾åƒå’Œè§†é¢‘ç¼–è¾‘çš„æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶MLLMså’Œæ‰©æ•£æ¨¡å‹çš„æ•´åˆï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­è¿›è¡ŒæŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€‚è®­ç»ƒå›¾åƒæ•°æ®å¯ä»¥æå‡è§†é¢‘ç¼–è¾‘èƒ½åŠ›ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£ï¼›ç»“åˆæ¨¡æ€ç‰¹å®šçš„MLLMç‰¹æ€§ï¼Œå®ç°äº†å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„ç»Ÿä¸€å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—è¿›å±•ï¼Œæœ‰åŠ©äºæå‡æ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ€§èƒ½ã€‚</li>
<li>InstructXæ¡†æ¶ç”¨äºå›¾åƒå’Œè§†é¢‘ç¼–è¾‘çš„æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>æ•´åˆMLLMså’Œæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶å¯¹äºè§£å†³è§†é¢‘ç¼–è¾‘ç­‰å›°éš¾ä»»åŠ¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è®­ç»ƒå›¾åƒæ•°æ®å¯ä»¥æå‡è§†é¢‘ç¼–è¾‘èƒ½åŠ›ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚</li>
<li>InstructXæ¡†æ¶é€šè¿‡ç»“åˆæ¨¡æ€ç‰¹å®šçš„MLLMç‰¹æ€§ï¼Œå®ç°äº†å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„ç»Ÿä¸€å»ºæ¨¡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4bc8d3a7c558ea5a8120c4757cea3a75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130794&auth_key=1760130794-0-0-155dc2b1b9f94b8bcf55d0554a969aa5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-afc26c2a7a2e1d4b68777f798f0935b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130801&auth_key=1760130801-0-0-9e206da13c1515352dbfa55b42e03679&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97399e335a9ea984d56386d3e3bd9cf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130808&auth_key=1760130808-0-0-cc8652d4c1d7d061d3895c83832c4315&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f1020c946a37973f231072e01c4450c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130815&auth_key=1760130815-0-0-08190583dfdbab41bf832d8fd0552cfc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d255793f2e47ee8b95ffad0c69e3a50c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130822&auth_key=1760130822-0-0-c399c2dd1f8b97892bd0d4a8406e14cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33567bcefd0370480eaba7aafc3b11bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130828&auth_key=1760130828-0-0-651a5ffdf93a41fccdac12a95ae30916&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b35b1ab4a5866e0514d47e3dcf7405d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130835&auth_key=1760130835-0-0-42bd34c784a57b3fe6d9d6c2b3d97c89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a533e74bd38a606086428a5aa5c12b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130842&auth_key=1760130842-0-0-894e9c161c7963afeb37cfe4f1430cf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency"><a href="#Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency" class="headerlink" title="Large Scale Diffusion Distillation via Score-Regularized Continuous-Time   Consistency"></a>Large Scale Diffusion Distillation via Score-Regularized Continuous-Time   Consistency</h2><p><strong>Authors:Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</strong></p>
<p>This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the â€œmode-coveringâ€ nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the â€œmode-seekingâ€ reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling by $15\times\sim50\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation. </p>
<blockquote>
<p>æœ¬æ–‡ä»£è¡¨äº†å°†è¿ç»­æ—¶é—´ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯æ‰©å±•åˆ°é€šç”¨åº”ç”¨çº§å›¾åƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„é¦–æ¬¡å°è¯•ã€‚å°½ç®¡è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆsCMï¼‰åœ¨ç†è®ºä¸Šå…·æœ‰åŸåˆ™æ€§ï¼Œå¹¶ä¸”åœ¨åŠ é€Ÿå­¦æœ¯è§„æ¨¡çš„æ‰©æ•£æ–¹é¢å…·æœ‰å®è¯æ•ˆåŠ›ï¼Œä½†ç”±äºé›…å¯æ¯”å‘é‡ç§¯ï¼ˆJVPï¼‰è®¡ç®—çš„åŸºç¡€è®¾æ–½æŒ‘æˆ˜ä»¥åŠæ ‡å‡†è¯„ä¼°åŸºå‡†çš„é™åˆ¶ï¼Œå…¶åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ä¸Šçš„é€‚ç”¨æ€§ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªä¸å¹¶è¡Œæ€§å…¼å®¹çš„FlashAttention-2 JVPå†…æ ¸ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰è¶…è¿‡10äº¿å‚æ•°å’Œé«˜ç»´è§†é¢‘ä»»åŠ¡çš„æ¨¡å‹ä¸Šè¿›è¡ŒsCMè®­ç»ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°sCMåœ¨ç»†èŠ‚ç”Ÿæˆæ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§çš„è´¨é‡é™åˆ¶ï¼Œæˆ‘ä»¬å°†å…¶å½’å› äºè¯¯å·®ç´¯ç§¯å’Œå…¶å‰å‘å‘æ•£ç›®æ ‡çš„â€œæ¨¡å¼è¦†ç›–â€æ€§è´¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¾—åˆ†æ­£åˆ™åŒ–çš„è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆrCMï¼‰ï¼Œå®ƒå°†å¾—åˆ†è’¸é¦ä½œä¸ºé•¿è·³æ­£åˆ™åŒ–å™¨ã€‚è¿™ç§é›†æˆä¸sCMçš„â€œæ¨¡å¼å¯»æ±‚â€åå‘å‘æ•£ç›¸ç»“åˆï¼Œåœ¨ä¿æŒé«˜ç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°æé«˜äº†è§†è§‰è´¨é‡ã€‚ç»è¿‡éªŒè¯ï¼Œåœ¨å¤§è§„æ¨¡æ¨¡å‹ï¼ˆCosmos-Predict2ã€Wan2.1ï¼‰ä¸Šï¼ŒrCMçš„å‚æ•°é«˜è¾¾14Bï¼Œè§†é¢‘æ—¶é•¿ä¸º5ç§’ï¼Œå®ƒåœ¨è´¨é‡æŒ‡æ ‡ä¸Šä¸æœ€æ–°çš„è’¸é¦æ–¹æ³•DMD2ç›¸åŒ¹é…æˆ–è¶…è¶Šäº†å®ƒï¼ŒåŒæ—¶åœ¨å¤šæ ·æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæ‰€æœ‰è¿™äº›æˆæœéƒ½æ˜¯åœ¨æ²¡æœ‰ä½¿ç”¨GANå¾®è°ƒæˆ–å¹¿æ³›è¶…å‚æ•°æœç´¢çš„æƒ…å†µä¸‹å–å¾—çš„ã€‚è’¸é¦åçš„æ¨¡å‹åœ¨ä»…1è‡³4æ­¥å†…ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ï¼Œå°†æ‰©æ•£é‡‡æ ·çš„é€Ÿåº¦æé«˜äº†15è‡³50å€ã€‚è¿™äº›ç»“æœå°†rCMå®šä½ä¸ºæ¨è¿›å¤§è§„æ¨¡æ‰©æ•£è’¸é¦çš„å®é™…ä¸”ç†è®ºæ‰å®çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08431v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶é¦–æ¬¡å®ç°äº†è¿ç»­æ—¶é—´ä¸€è‡´æ€§è’¸é¦åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¼€å‘å¹¶è¡Œå…¼å®¹çš„FlashAttention-2 JVPå†…æ ¸ï¼Œå®ç°äº†åœ¨è¶…è¿‡10äº¿å‚æ•°å’Œé«˜ç»´è§†é¢‘ä»»åŠ¡ä¸Šçš„sCMè®­ç»ƒã€‚ç ”ç©¶å‘ç°sCMåœ¨ç»†èŠ‚ç”Ÿæˆä¸Šå­˜åœ¨è´¨é‡å±€é™ï¼Œæå‡ºé€šè¿‡å¼•å…¥è¯„åˆ†æ­£åˆ™åŒ–çš„è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆrCMï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚rCMé€šè¿‡ç»“åˆsCMçš„å‰å‘å‘æ•£ç›®æ ‡å’Œè¯„åˆ†è’¸é¦çš„é•¿æœŸè·³è·ƒæ­£åˆ™åŒ–ï¼Œåœ¨ä¿æŒé«˜ç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶ï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€‚åœ¨å¤§å‹æ¨¡å‹ï¼ˆCosmos-Predict2ã€Wan2.1ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒrCMåœ¨è´¨é‡æŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°çš„è’¸é¦æ–¹æ³•DMD2ï¼Œä¸”åœ¨å¤šæ ·æ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæ— éœ€GANè°ƒæ•´å’Œå¹¿æ³›çš„è¶…å‚æ•°æœç´¢ã€‚è’¸é¦åçš„æ¨¡å‹åœ¨1<del>4æ­¥å†…ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ï¼Œå°†æ‰©æ•£é‡‡æ ·çš„é€Ÿåº¦æé«˜äº†15</del>50å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ‰©å±•äº†è¿ç»­æ—¶é—´ä¸€è‡´æ€§è’¸é¦åœ¨é€šç”¨åº”ç”¨çº§åˆ«çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>å¼€å‘äº†FlashAttention-2 JVPå†…æ ¸ï¼Œæ”¯æŒåœ¨å¤§å‹æ¨¡å‹å’Œé«˜ç»´è§†é¢‘ä»»åŠ¡ä¸Šè¿›è¡ŒsCMè®­ç»ƒã€‚</li>
<li>æ­ç¤ºäº†sCMåœ¨ç»†èŠ‚ç”Ÿæˆæ–¹é¢çš„è´¨é‡å±€é™ï¼Œå½’å› äºè¯¯å·®ç´¯ç§¯å’Œå®ƒçš„å‰å‘å‘æ•£ç›®æ ‡çš„â€œæ¨¡å¼è¦†ç›–â€ç‰¹æ€§ã€‚</li>
<li>æå‡ºäº†è¯„åˆ†æ­£åˆ™åŒ–çš„è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆrCMï¼‰ï¼Œé€šè¿‡ç»“åˆsCMå’Œè¯„åˆ†è’¸é¦ï¼Œæé«˜äº†è§†è§‰è´¨é‡å¹¶ä¿æŒé«˜ç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
<li>rCMåœ¨å¤§å‹æ¨¡å‹ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰çš„è’¸é¦æ–¹æ³•DMD2ï¼Œç‰¹åˆ«æ˜¯åœ¨è´¨é‡æŒ‡æ ‡å’Œå¤šæ ·æ€§ä¸Šã€‚</li>
<li>rCMä¸éœ€è¦å¤æ‚çš„GANè°ƒæ•´å’Œè¶…å‚æ•°æœç´¢ï¼Œç®€åŒ–äº†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>è’¸é¦åçš„æ¨¡å‹èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ­¥éª¤å†…ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ï¼Œæ˜¾è‘—åŠ é€Ÿäº†æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c316938763dae2ed516da9ae15997fa1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130849&auth_key=1760130849-0-0-2bbc34c9b22a4fdafdb98f89751c7823&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af916aa2015e25531947898af0f5b2e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130855&auth_key=1760130855-0-0-88a6e4308f14129b5a28e3ead61d4023&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c424927706436691e6194048863c5440~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130861&auth_key=1760130861-0-0-2f1b0068eef4fb5550ac891cdee1771a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f62c7892381b45b032b2260d4d609c77~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130868&auth_key=1760130868-0-0-fc13655b035bc680abd417c42fb9aef3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hyperspectral-data-augmentation-with-transformer-based-diffusion-models"><a href="#Hyperspectral-data-augmentation-with-transformer-based-diffusion-models" class="headerlink" title="Hyperspectral data augmentation with transformer-based diffusion models"></a>Hyperspectral data augmentation with transformer-based diffusion models</h2><p><strong>Authors:Mattia Ferrari, Lorenzo Bruzzone</strong></p>
<p>The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation. </p>
<blockquote>
<p>æ–°ä¸€ä»£é«˜å…‰è°±å«æ˜Ÿä¼ æ„Ÿå™¨çš„å¼•å…¥ï¼Œç»“åˆæ·±åº¦å­¦ä¹ æ–¹æ³•çš„è¿›æ­¥ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ä¸­ç­‰è‡³è¾ƒå¤§è§„æ¨¡ä¸Šè¯†åˆ«è¯¦ç»†åœŸåœ°è¦†ç›–ç±»åˆ«çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯ï¼Œåœ¨ä½¿ç”¨å°å‹æ ‡è®°æ•°æ®é›†è®­ç»ƒç½‘ç»œæ—¶å­˜åœ¨è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚ä¸ºäº†æœ‰æ•ˆåœ°åœ¨æœ‰é™æ•°é‡çš„æ ‡è®°æ ·æœ¬ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶æ•è·æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªè½»é‡çº§çš„å˜å‹å™¨ç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„åŠ æƒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–åçš„ä½™å¼¦æ–¹å·®è°ƒåº¦å™¨ï¼Œæœ‰åŠ©äºåœ¨å°å‹æ•°æ®é›†ä¸Šè¿›è¡Œå¿«é€Ÿæœ‰æ•ˆçš„è®­ç»ƒã€‚æˆ‘ä»¬ä»¥æ£®æ—åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œåˆ©ç”¨PRISMAå«æ˜Ÿè·å–çš„é«˜å…‰è°±å›¾åƒå¯¹10ç§ä¸åŒç±»å‹çš„æ£®æ—è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®ç‡å’ŒåŠ æƒå¹³å‡å‡†ç¡®ç‡ä¸Šå‡ä¼˜äºå…¶ä»–æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿˜ä½“ç°åœ¨æ¨¡å‹çš„ç¨³å®šè®­ç»ƒè¡Œä¸ºä¸Šï¼Œè§£å†³äº†æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®å¢å¼ºå®é™…åº”ç”¨ä¸­çš„å¸¸è§å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08363v1">PDF</a> 10 pages, 2 figures, accepted at SPIE REMOTE SENSING conference 16-20   September 2024 Edinburgh, United Kingdom</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£é«˜å…‰è°±å«æ˜Ÿä¼ æ„Ÿå™¨ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»“åˆï¼Œæé«˜äº†å¯¹ä¸­ç­‰è‡³å¤§è§„æ¨¡åœŸåœ°è¦†ç›–ç±»åˆ«çš„é‰´åˆ«èƒ½åŠ›ã€‚é’ˆå¯¹å°æ ‡ç­¾æ•°æ®é›†è®­ç»ƒç½‘ç»œæ—¶æ˜“å‡ºç°çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¹¶å®ç°äº†è½»é‡çº§å˜å‹å™¨ç½‘ç»œä»¥æ•è·æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ã€‚é€šè¿‡æ”¹è¿›åŠ æƒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–ä½™å¼¦æ–¹å·®è°ƒåº¦å™¨ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨æœ‰é™æ ‡ç­¾æ ·æœ¬ä¸Šå¿«é€Ÿæœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹ã€‚åœ¨åˆ©ç”¨PRISMAå«æ˜Ÿè·å–çš„é«˜å…‰è°±å›¾åƒè¿›è¡Œçš„æ£®æ—åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å’ŒåŠ æƒå¹³å‡ç²¾åº¦ä¸Šå‡ä¼˜äºå…¶ä»–æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¹¶å±•ç¤ºäº†ç¨³å®šçš„è®­ç»ƒè¡Œä¸ºï¼Œè§£å†³äº†æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„å¸¸è§å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°ä¸€ä»£é«˜å…‰è°±å«æ˜Ÿä¼ æ„Ÿå™¨ä¸æ·±åº¦å­¦ä¹ ç»“åˆï¼Œæé«˜äº†å¯¹åœŸåœ°è¦†ç›–ç±»åˆ«çš„é‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹å°æ ‡ç­¾æ•°æ®é›†è®­ç»ƒç½‘ç»œæ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>å®ç°äº†è½»é‡çº§å˜å‹å™¨ç½‘ç»œä»¥æ›´æœ‰æ•ˆåœ°æ•è·æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ã€‚</li>
<li>æ”¹è¿›äº†åŠ æƒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–äº†ä½™å¼¦æ–¹å·®è°ƒåº¦å™¨ï¼Œä¿ƒè¿›æ¨¡å‹çš„å¿«é€Ÿæœ‰æ•ˆè®­ç»ƒã€‚</li>
<li>åœ¨æ£®æ—åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å’ŒåŠ æƒå¹³å‡ç²¾åº¦ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç¤ºçš„ç¨³å®šè®­ç»ƒè¡Œä¸ºè§£å†³äº†æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¸¸è§å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ae696f057fd860a338c6184cc545e3fc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130875&auth_key=1760130875-0-0-587b2e146fed94c9c1afc08c07a5e783&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37ab153a9e833ac89081097cd33b5bfd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130883&auth_key=1760130883-0-0-3963676990c45ce2abdebd6fda3d5c33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="One-Stone-with-Two-Birds-A-Null-Text-Null-Frequency-Aware-Diffusion-Models-for-Text-Guided-Image-Inpainting"><a href="#One-Stone-with-Two-Birds-A-Null-Text-Null-Frequency-Aware-Diffusion-Models-for-Text-Guided-Image-Inpainting" class="headerlink" title="One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion   Models for Text-Guided Image Inpainting"></a>One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion   Models for Text-Guided Image Inpainting</h2><p><strong>Authors:Haipeng Liu, Yang Wang, Meng Wang</strong></p>
<p>Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from <a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff">https://github.com/htyjers/NTN-Diff</a>. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤æ—¨åœ¨æ ¹æ®æ–‡æœ¬æç¤ºé‡å»ºé®æŒ¡åŒºåŸŸï¼Œé•¿æœŸä»¥æ¥çš„æŒ‘æˆ˜åœ¨äºä¿ç•™æœªé®æŒ¡åŒºåŸŸï¼ŒåŒæ—¶å®ç°æœªé®æŒ¡åŒºåŸŸå’Œä¿®å¤é®æŒ¡åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä»¥å‰çš„æ–¹æ³•æœªèƒ½åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œé€šå¸¸åªèƒ½è§£å†³å…¶ä¸­ä¹‹ä¸€ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™ç§æƒ…å†µæºäºæ··åˆï¼ˆä¾‹å¦‚ä¸­ä½é¢‘ï¼‰é¢‘å¸¦ä¹‹é—´çš„çº ç¼ ï¼Œè¿™äº›é¢‘å¸¦ç¼–ç äº†ä¸åŒçš„å›¾åƒå±æ€§ï¼Œåœ¨é™å™ªè¿‡ç¨‹ä¸­å¯¹ä¸åŒæ–‡æœ¬æç¤ºè¡¨ç°å‡ºä¸åŒçš„ç¨³å¥æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤çš„é›¶æ–‡æœ¬é›¶é¢‘ç‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸ºNTN-Diffã€‚å®ƒé€šè¿‡åˆ†è§£é®æŒ¡å’Œæœªé®æŒ¡åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œé’ˆå¯¹æ¯ä¸ªé¢‘å¸¦è¿›è¡Œä¸€è‡´æ€§å¤„ç†ï¼ŒåŒæ—¶ä¿ç•™æœªé®æŒ¡åŒºåŸŸï¼Œä»è€Œè¿ç»­è§£å†³ä¸¤ä¸ªæŒ‘æˆ˜ã€‚åŸºäºæ‰©æ•£è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†é™å™ªè¿‡ç¨‹è¿›ä¸€æ­¥åˆ†ä¸ºæ—©æœŸï¼ˆé«˜çº§å™ªå£°ï¼‰å’Œæ™šæœŸï¼ˆä½çº§å™ªå£°ï¼‰é˜¶æ®µï¼Œå…¶ä¸­ä¸­ä½é¢‘å¸¦åœ¨é™å™ªè¿‡ç¨‹ä¸­è¢«è§£å¼€ã€‚è§‚å¯Ÿåˆ°ç¨³å®šçš„ä¸­é¢‘å¸¦åœ¨æ–‡æœ¬å¼•å¯¼çš„é™å™ªè¿‡ç¨‹ä¸­é€æ¸å»å™ªä»¥å®ç°è¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶ä½œä¸ºé›¶æ–‡æœ¬é™å™ªè¿‡ç¨‹çš„æŒ‡å¯¼æ¥å»é™¤é®æŒ¡åŒºåŸŸçš„ä½é¢‘å¸¦å™ªå£°ï¼Œéšååœ¨åæœŸè¿›è¡Œåç»­çš„æ–‡æœ¬å¼•å¯¼é™å™ªè¿‡ç¨‹ï¼Œä»¥å®ç°é®æŒ¡å’Œæœªé®æŒ¡åŒºåŸŸä¹‹é—´çš„ä¸­ä½é¢‘å¸¦çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™æœªé®æŒ¡åŒºåŸŸã€‚å¤§é‡å®éªŒéªŒè¯äº†NTN-Diffåœ¨æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff">https://github.com/htyjers/NTN-Diff</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08273v1">PDF</a> 25 pages, 11 figures, to appear NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤æ˜¯æ ¹æ®æ–‡æœ¬æç¤ºé‡æ„è¢«é®è”½åŒºåŸŸçš„è¿‡ç¨‹ï¼Œå…¶é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä¿æŒæœªé®è”½åŒºåŸŸçš„å®Œæ•´æ€§ï¼ŒåŒæ—¶åœ¨æœªé®è”½å’Œä¿®å¤çš„è¢«é®è”½åŒºåŸŸä¹‹é—´å®ç°è¯­ä¹‰ä¸€è‡´æ€§ã€‚å…ˆå‰çš„æ–¹æ³•æ— æ³•è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå¸¸å¸¸åªèƒ½è§£å†³å…¶ä¸­ä¹‹ä¸€ã€‚è¿™ç§æƒ…å†µæºäºæ··åˆé¢‘ç‡æ³¢æ®µï¼ˆå¦‚ä¸­ä½é¢‘ï¼‰çš„çº ç¼ ï¼Œè¿™äº›é¢‘ç‡æ³¢æ®µç¼–ç äº†ä¸åŒçš„å›¾åƒå±æ€§ï¼Œåœ¨é™å™ªè¿‡ç¨‹ä¸­å¯¹æ–‡æœ¬æç¤ºçš„å“åº”ä¸åŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤çš„é›¶æ–‡æœ¬é›¶é¢‘ç‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼ˆNTN-Diffï¼‰ï¼Œé€šè¿‡åˆ†è§£è¢«é®è”½å’Œæœªé®è”½åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§åˆ°æ¯ä¸ªé¢‘ç‡æ³¢æ®µï¼ŒåŒæ—¶ä¿æŒæœªé®è”½åŒºåŸŸçš„å®Œæ•´æ€§ï¼Œä»¥æ­¤è¿ç»­è§£å†³ä¸¤ä¸ªæŒ‘æˆ˜ã€‚åŸºäºæ‰©æ•£è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†é™å™ªè¿‡ç¨‹åˆ†ä¸ºæ—©æœŸï¼ˆé«˜çº§å™ªå£°ï¼‰å’Œæ™šæœŸï¼ˆä½çº§å™ªå£°ï¼‰é˜¶æ®µï¼Œå…¶ä¸­ä¸­ä½é¢‘æ³¢æ®µåœ¨é™å™ªè¿‡ç¨‹ä¸­è¢«åˆ†ç¦»ã€‚è§‚å¯Ÿåˆ°ç¨³å®šçš„ä¸­é¢‘æ³¢æ®µåœ¨æ–‡æœ¬å¼•å¯¼çš„é™å™ªè¿‡ç¨‹ä¸­é€æ­¥å»å™ªä»¥å®ç°è¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶ä½œä¸ºé›¶æ–‡æœ¬é™å™ªè¿‡ç¨‹çš„æŒ‡å¯¼æ¥ä¿®å¤è¢«é®è”½åŒºåŸŸçš„ä½é¢‘æ³¢æ®µï¼Œéšååœ¨æ™šæœŸé˜¶æ®µè¿›è¡Œåç»­çš„æ–‡æœ¬å¼•å¯¼é™å™ªè¿‡ç¨‹ï¼Œä»¥å®ç°è¢«é®è”½å’Œæœªé®è”½åŒºåŸŸä¹‹é—´çš„ä¸­ä½é¢‘æ³¢æ®µçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒæœªé®è”½åŒºåŸŸçš„å®Œæ•´æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†NTN-Diffç›¸è¾ƒäºå…ˆè¿›çš„æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/htyjers/NTN-Diffè®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤æ—¨åœ¨æ ¹æ®æ–‡æœ¬æç¤ºé‡å»ºè¢«é®è”½åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæœªé®è”½åŒºåŸŸçš„å®Œæ•´æ€§ã€‚</li>
<li>æ­¤å‰çš„æ–¹æ³•åœ¨ä¸­ä½é¢‘æ³¢æ®µçš„çº ç¼ é—®é¢˜ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´æ— æ³•åŒæ—¶è§£å†³è¯­ä¹‰ä¸€è‡´æ€§å’Œæœªé®è”½åŒºåŸŸä¿æŒçš„é—®é¢˜ã€‚</li>
<li>NTN-Diffæ¨¡å‹é€šè¿‡åˆ†è§£è¯­ä¹‰ä¸€è‡´æ€§åˆ°æ¯ä¸ªé¢‘ç‡æ³¢æ®µï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>NTN-Diffæ¨¡å‹å°†é™å™ªè¿‡ç¨‹åˆ†ä¸ºæ—©æœŸå’Œæ™šæœŸé˜¶æ®µï¼Œå¹¶å‘ç°ç¨³å®šçš„ä¸­é¢‘æ³¢æ®µåœ¨æ–‡æœ¬å¼•å¯¼çš„é™å™ªè¿‡ç¨‹ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>é›¶æ–‡æœ¬é™å™ªè¿‡ç¨‹ç”¨äºä¿®å¤è¢«é®è”½åŒºåŸŸçš„ä½é¢‘æ³¢æ®µï¼Œéšåè¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„ç¬¬äºŒé˜¶æ®µé™å™ªã€‚</li>
<li>é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒNTN-Diffå®ç°äº†è¢«é®è”½å’Œæœªé®è”½åŒºåŸŸä¹‹é—´çš„ä¸­ä½é¢‘æ³¢æ®µçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-04b24c3ff8b2b5975ea60d245ebf40e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130890&auth_key=1760130890-0-0-145eadab9cedf331055be0d2ce3f488b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-49a8ae7cd43afc9fa0e604c29753f7b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130898&auth_key=1760130898-0-0-016d7a02b0e71328bea82eae155e8333&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac9ec4873f47b2cd85c0c20665bf7012~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130904&auth_key=1760130904-0-0-39f01cf7bb36ceb602af79cbd72fb257&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d3ba07ccf9825ce15ead0887b1a5b40~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130911&auth_key=1760130911-0-0-d71e078bdb37f370af71004070c157b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16ca20f748a75b8ca00c8adfd3c44244~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130918&auth_key=1760130918-0-0-23b91836e940749306320e2eab2f4b56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation"><a href="#SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation" class="headerlink" title="SViM3D: Stable Video Material Diffusion for Single Image 3D Generation"></a>SViM3D: Stable Video Material Diffusion for Single Image 3D Generation</h2><p><strong>Authors:Andreas Engelhardt, Mark Boss, Vikram Voletti, Chun-Han Yao, Hendrik P. A. Lensch, Varun Jampani</strong></p>
<p>We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR&#x2F;VR, movies, games and other visual media. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ç¨³å®šè§†é¢‘ææ–™3Dï¼ˆSViM3Dï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»å•å¼ å›¾åƒé¢„æµ‹å¤šè§†è§’ä¸€è‡´æ€§çš„åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™ã€‚æœ€è¿‘ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹å·²æˆåŠŸç”¨äºä»å•å¼ å›¾åƒé«˜æ•ˆé‡å»º3Då¯¹è±¡ã€‚ç„¶è€Œï¼Œåå°„ä»ç”±ç®€å•çš„ææ–™æ¨¡å‹è¡¨ç¤ºï¼Œæˆ–è€…éœ€è¦è¿›è¡Œé¢å¤–çš„æ­¥éª¤æ¥ä¼°è®¡ï¼Œä»¥å®ç°é‡æ–°ç…§æ˜å’Œæ§åˆ¶å¤–è§‚ç¼–è¾‘ã€‚æˆ‘ä»¬æ‰©å±•äº†æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥è¾“å‡ºç©ºé—´å˜åŒ–çš„PBRå‚æ•°å’Œè¡¨é¢æ³•çº¿ï¼Œä»¥åŠä¸æ¯ä¸ªç”Ÿæˆè§†å›¾åŸºäºæ˜¾å¼ç›¸æœºæ§åˆ¶çš„è”åˆè¾“å‡ºã€‚è¿™ç§ç‹¬ç‰¹çš„è®¾ç½®å…è®¸ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ä½œä¸ºç¥ç»å…ˆéªŒæ¥è¿›è¡Œé‡æ–°ç…§æ˜å’Œç”Ÿæˆ3Dèµ„äº§ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸ªç®¡é“å¼•å…¥äº†å„ç§æœºåˆ¶ï¼Œåœ¨è¿™ä¸ªä¸é€‚å®šçš„ç¯å¢ƒä¸­æé«˜äº†è´¨é‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„é‡æ–°ç…§æ˜å’Œæ–°è§†è§’åˆæˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå„ç§è¾“å…¥ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯ç”¨äºAR&#x2F;VRã€ç”µå½±ã€æ¸¸æˆå’Œå…¶ä»–è§†è§‰åª’ä½“çš„é‡æ–°ç…§æ˜çš„3Dèµ„äº§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08271v1">PDF</a> Accepted by International Conference on Computer Vision (ICCV 2025).   Project page: <a target="_blank" rel="noopener" href="http://svim3d.aengelhardt.com/">http://svim3d.aengelhardt.com</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºStable Video Materials 3Dï¼ˆSViM3Dï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å•å¼ å›¾åƒé¢„æµ‹å¤šè§†è§’ä¸€è‡´æ€§çš„ç‰©ç†åŸºç¡€æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™ã€‚é€šè¿‡æ‰©å±•æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè”åˆç”Ÿæˆæ¯ä¸ªè§†è§’çš„ç©ºé—´å˜åŒ–PBRå‚æ•°å’Œè¡¨é¢æ³•çº¿ï¼Œå¹¶åŸºäºæ˜ç¡®çš„ç›¸æœºæ§åˆ¶è¿›è¡Œè¾“å‡ºã€‚è¿™ä¸€ç‹¬ç‰¹è®¾ç½®å…è®¸ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ä½œä¸ºç¥ç»å…ˆéªŒæ¥è¿›è¡Œé‡æ–°ç…§æ˜å’Œç”Ÿæˆ3Dèµ„äº§ã€‚æœ¬æ–‡ä»‹ç»äº†å¤šç§æœºåˆ¶ï¼Œå¯ä»¥åœ¨è¿™ä¸ªä¸é€‚å®šçš„ç¯å¢ƒä¸­æé«˜è´¨é‡ã€‚åœ¨å¤šä¸ªä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºäº†æœ€å…ˆè¿›çš„é‡æ–°ç…§æ˜å’Œæ–°è§†è§’åˆæˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°å„ç§è¾“å…¥ï¼Œç”Ÿæˆçš„é‡æ–°ç…§æ˜çš„3Dèµ„äº§å¯ç”¨äºAR&#x2F;VRã€ç”µå½±ã€æ¸¸æˆå’Œå…¶ä»–è§†è§‰åª’ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SViM3Dæ¡†æ¶èƒ½ä»å•å¼ å›¾åƒé¢„æµ‹å¤šè§†è§’ä¸€è‡´æ€§çš„PBRææ–™ã€‚</li>
<li>æ¡†æ¶æ‰©å±•äº†æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥è”åˆç”Ÿæˆç©ºé—´å˜åŒ–çš„PBRå‚æ•°å’Œè¡¨é¢æ³•çº¿ã€‚</li>
<li>é€šè¿‡æ˜ç¡®çš„ç›¸æœºæ§åˆ¶è¿›è¡Œè¾“å‡ºï¼Œå…è®¸é‡æ–°ç…§æ˜å’Œç”Ÿæˆ3Dèµ„äº§ã€‚</li>
<li>å¼•å…¥å¤šç§æœºåˆ¶ä»¥æé«˜åœ¨ä¸é€‚å®šç¯å¢ƒä¸­çš„è´¨é‡ã€‚</li>
<li>åœ¨å¤šä¸ªç‰©ä½“ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šæ˜¾ç¤ºäº†æœ€å…ˆè¿›çš„é‡æ–°ç…§æ˜å’Œæ–°è§†è§’åˆæˆæ€§èƒ½ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°å¤šç§è¾“å…¥ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ea5c89e76c2e305007253914c17facac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130925&auth_key=1760130925-0-0-332013aa3ef97db290e279357b4cd871&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f102e258caf40840cbd1ad5f0875c8d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130932&auth_key=1760130932-0-0-83caa7bcc1d1e79512e4a99c0259d610&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9de9783005b9f4df8a0ee8c7c119a630~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130939&auth_key=1760130939-0-0-be1afc162f9d13d38a85058b3448e2cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-29908801b7c2b92575cfe018d1da3f76~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130946&auth_key=1760130946-0-0-78b5413cba28a125ebd54935cff173c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution"><a href="#UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution" class="headerlink" title="UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video   Super-Resolution"></a>UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video   Super-Resolution</h2><p><strong>Authors:Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji</strong></p>
<p>Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques. </p>
<blockquote>
<p>çº§è”è§†é¢‘è¶…åˆ†è¾¨ç‡æŠ€æœ¯å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œç”¨äºå‡è½»åœ¨ä½¿ç”¨å¤§å‹åŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†é¢‘æ—¶æ‰€å¸¦æ¥çš„è®¡ç®—è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¤§å¤šå±€é™äºæ–‡æœ¬åˆ°è§†é¢‘çš„ä»»åŠ¡ï¼Œæœªèƒ½åˆ©ç”¨æ–‡æœ¬ä»¥å¤–çš„å…¶ä»–ç”Ÿæˆæ¡ä»¶ï¼Œè¿™å¯¹äºç¡®ä¿å¤šæ¨¡å¼è§†é¢‘ç”Ÿæˆçš„ä¿çœŸåº¦è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºUniMMVSRæ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒUniMMVSRæ˜¯ç¬¬ä¸€ä¸ªç»“åˆæ··åˆæ¨¡å¼æ¡ä»¶çš„ç»Ÿä¸€ç”Ÿæˆè§†é¢‘è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚æˆ‘ä»¬åœ¨æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å…¨é¢æ¢ç´¢äº†æ¡ä»¶æ³¨å…¥ç­–ç•¥ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ•°æ®æ··åˆæŠ€æœ¯ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯è®¾è®¡ç‹¬ç‰¹çš„æ•°æ®æ„å»ºå’Œæ¡ä»¶åˆ©ç”¨æ–¹æ³•ï¼Œä»¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®åˆ©ç”¨æ‰€æœ‰æ¡ä»¶ç±»å‹ï¼Œè€ƒè™‘åˆ°å®ƒä»¬ä¸ç›®æ ‡è§†é¢‘çš„å„å¼‚å…³è”æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒUniMMVSRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„è§†é¢‘ç»†èŠ‚æ›´åŠ ä¼˜ç§€ï¼Œå¹¶ä¸”æ›´èƒ½ç¬¦åˆå¤šæ¨¡å¼æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†å°†UniMMVSRä¸åŸºç¡€æ¨¡å‹ç›¸ç»“åˆç”Ÿæˆ4Kè§†é¢‘çš„å¯è¡Œæ€§ï¼Œè¿™æ˜¯ç°æœ‰æŠ€æœ¯æ— æ³•è¾¾åˆ°çš„æˆå°±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08143v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºUniMMVSRçš„ç»Ÿä¸€è§†é¢‘è¶…åˆ†è¾¨ç‡ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€æ¡ä»¶ï¼Œè§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸»è¦å±€é™äºæ–‡æœ¬åˆ°è§†é¢‘çš„å•ä¸€ä»»åŠ¡ï¼Œå¹¶åœ¨æ··åˆæ¨¡æ€æ¡ä»¶ä¸‹çš„è§†é¢‘ç”Ÿæˆä¸­ä¿è¯ä¿çœŸåº¦çš„é—®é¢˜ã€‚é€šè¿‡æ½œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç ”ç©¶å›¢é˜Ÿæ·±å…¥æ¢è®¨äº†æ¡ä»¶æ³¨å…¥ç­–ç•¥ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ•°æ®æ··åˆæŠ€æœ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒUniMMVSRåœ¨åˆ©ç”¨ä¸åŒæ¡ä»¶ç±»å‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”èƒ½å¤Ÿç»“åˆåŸºç¡€æ¨¡å‹ç”Ÿæˆç¬¦åˆå¤šæ¨¡æ€æŒ‡å¯¼çš„4Kè§†é¢‘ã€‚è¯¥æ¡†æ¶æ˜¾è‘—æé«˜è§†é¢‘ç»†èŠ‚åº¦å’Œå¯¹å¤šæ¨¡æ€æ¡ä»¶çš„ç¬¦åˆç¨‹åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniMMVSRæ˜¯é¦–ä¸ªç»“åˆå¤šç§æ¨¡æ€æ¡ä»¶çš„ç»Ÿä¸€è§†é¢‘è¶…åˆ†è¾¨ç‡ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ¢è®¨äº†æ¡ä»¶æ³¨å…¥ç­–ç•¥ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ•°æ®æ··åˆæŠ€æœ¯ã€‚</li>
<li>è®¾è®¡äº†ç‹¬ç‰¹çš„æ•°æ®æ„å»ºå’Œåˆ©ç”¨æ¡ä»¶çš„æ–¹æ³•ï¼Œä»¥åˆ©ç”¨ä¸åŒæ¨¡æ€ä¸ç›®æ ‡çš„è§†é¢‘çš„ç›¸å…³æ€§ã€‚</li>
<li>UniMMVSRæ˜¾è‘—æé«˜äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ï¼Œå¹¶åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒUniMMVSRèƒ½å¤Ÿç”Ÿæˆæ›´ç²¾ç»†çš„è§†é¢‘å†…å®¹ã€‚</li>
<li>UniMMVSRèƒ½ä¸åŸºç¡€æ¨¡å‹ç»“åˆï¼Œå®ç°å¤šæ¨¡æ€æŒ‡å¯¼ä¸‹çš„4Kè§†é¢‘ç”Ÿæˆï¼Œè¿™æ˜¯ä»¥å‰çš„æŠ€æœ¯æ— æ³•è¾¾åˆ°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0074bba4bc706c6dfa73c048e109d756~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130953&auth_key=1760130953-0-0-5222200adabe1aacf7e27dd67b443ae4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a91a326a393f319d8b2b27fe692b4de1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130960&auth_key=1760130960-0-0-b464f8495edb9038cc1bf70a495f26d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e744eb0508f93a082dd207f683ea2fef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130967&auth_key=1760130967-0-0-d18cec9083c8cb12e3f680e1fce1757e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Real-Time-Motion-Controllable-Autoregressive-Video-Diffusion"><a href="#Real-Time-Motion-Controllable-Autoregressive-Video-Diffusion" class="headerlink" title="Real-Time Motion-Controllable Autoregressive Video Diffusion"></a>Real-Time Motion-Controllable Autoregressive Video Diffusion</h2><p><strong>Authors:Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang</strong></p>
<p>Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: <a target="_blank" rel="noopener" href="https://kesenzhao.github.io/AR-Drag.github.io/">https://kesenzhao.github.io/AR-Drag.github.io/</a>. </p>
<blockquote>
<p>å®æ—¶è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºåŒå‘æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰å»¶è¿Ÿå’Œç¼ºä¹æœ‰æ•ˆçš„è‡ªå›å½’ï¼ˆARï¼‰æ–¹æ³•ã€‚ç°æœ‰çš„ARè§†é¢‘æ‰©æ•£æ¨¡å‹ä»…é™äºç®€å•çš„æ§åˆ¶ä¿¡å·æˆ–æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼Œå¹¶ä¸”åœ¨å°‘æ­¥éª¤ç”Ÿæˆä¸­ç»å¸¸é­å—è´¨é‡ä¸‹é™å’Œè¿åŠ¨ä¼ªå½±çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AR-Dragï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ çš„å°‘æ•°æ­¥éª¤ARè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå®æ—¶å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆå…·æœ‰å¤šç§è¿åŠ¨æ§åˆ¶ã€‚æˆ‘ä»¬é¦–å…ˆå¾®è°ƒåŸºç¡€I2Væ¨¡å‹ä»¥æ”¯æŒåŸºæœ¬è¿åŠ¨æ§åˆ¶ï¼Œç„¶åé€šè¿‡åŸºäºè½¨è¿¹çš„å¥–åŠ±æ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›ã€‚æˆ‘ä»¬çš„è®¾è®¡é€šè¿‡è‡ªæˆ‘æ»šåŠ¨æœºåˆ¶ä¿ç•™äº†é©¬å°”å¯å¤«å±æ€§ï¼Œå¹¶é€šè¿‡åœ¨é™å™ªæ­¥éª¤ä¸­é€‰æ‹©æ€§å¼•å…¥éšæœºæ€§æ¥åŠ é€Ÿè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAR-Dragå®ç°äº†é«˜è§†è§‰ä¿çœŸåº¦å’Œç²¾ç¡®çš„è¿åŠ¨å¯¹é½ï¼Œä¸æœ€å…ˆè¿›çš„è¿åŠ¨æ§åˆ¶VDMç›¸æ¯”ï¼Œå»¶è¿Ÿæ—¶é—´å¤§å¤§é™ä½ï¼Œè€Œå‚æ•°ä»…ä½¿ç”¨1.3Bã€‚æ›´å¤šå¯è§†åŒ–å†…å®¹è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://kesenzhao.github.io/AR-Drag.github.io/">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08131v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å®æ—¶è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŒå‘æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰å»¶è¿Ÿå’Œç¼ºä¹æœ‰æ•ˆçš„è‡ªå›å½’ï¼ˆARï¼‰æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†AR-Dragæ¨¡å‹ã€‚å®ƒæ˜¯é¦–ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯çš„å°‘æ­¥éª¤è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒå®æ—¶å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆå¹¶å…·æœ‰å¤šæ ·çš„è¿åŠ¨æ§åˆ¶ã€‚é€šè¿‡å¾®è°ƒåŸºç¡€I2Væ¨¡å‹å¹¶å€ŸåŠ©åŸºäºè½¨è¿¹çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒAR-Dragè®¾è®¡ä¿ç•™äº†é©¬å°”å¯å¤«å±æ€§ï¼ŒåŒæ—¶é€šè¿‡é€‰æ‹©æ€§å¼•å…¥å»å™ªæ­¥éª¤ä¸­çš„éšæœºæ€§æ¥åŠ é€Ÿè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒAR-Dragåœ¨è§†è§‰ä¿çœŸåº¦å’Œè¿åŠ¨å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºå½“å‰å…ˆè¿›çš„è¿åŠ¨æ§åˆ¶VDMæ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼Œä»…ä½¿ç”¨1.3Bå‚æ•°ã€‚æ›´å¤šå¯è§†åŒ–å†…å®¹è¯·è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR-Dragæ˜¯é¦–ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å°‘æ­¥éª¤è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒå®æ—¶å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆå¹¶å…·æœ‰å¤šæ ·çš„è¿åŠ¨æ§åˆ¶ã€‚</li>
<li>AR-Dragé€šè¿‡å¾®è°ƒåŸºç¡€I2Væ¨¡å‹ä»¥å®ç°åŸºæœ¬è¿åŠ¨æ§åˆ¶ï¼Œå¹¶å€ŸåŠ©åŸºäºè½¨è¿¹çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>æ¨¡å‹è®¾è®¡ä¿ç•™äº†é©¬å°”å¯å¤«å±æ€§ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§å¼•å…¥å»å™ªæ­¥éª¤ä¸­çš„éšæœºæ€§æ¥åŠ é€Ÿè®­ç»ƒã€‚</li>
<li>AR-Dragå®ç°äº†é«˜è§†è§‰ä¿çœŸå’Œç²¾ç¡®è¿åŠ¨å¯¹é½ã€‚</li>
<li>ä¸ç°æœ‰è¿åŠ¨æ§åˆ¶VDMç›¸æ¯”ï¼ŒAR-Dragæ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚</li>
<li>AR-Dragæ¨¡å‹ä»…ä½¿ç”¨1.3Bå‚æ•°ï¼Œå®ç°äº†é«˜æ•ˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d831921cefdced602c0dc2f44969201b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130975&auth_key=1760130975-0-0-15ee0eb5159271cdf9fe5903f4add4a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10718804e10e9362ee445d463a9511d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130982&auth_key=1760130982-0-0-17787665d7e8de7d6250da545493d192&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5133b353722a6e4509656a4ba0d4e17a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130988&auth_key=1760130988-0-0-8d5c80ea7d924e5ba75837155bd3b90c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ComGS-Efficient-3D-Object-Scene-Composition-via-Surface-Octahedral-Probes"><a href="#ComGS-Efficient-3D-Object-Scene-Composition-via-Surface-Octahedral-Probes" class="headerlink" title="ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral   Probes"></a>ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral   Probes</h2><p><strong>Authors:Jian Gao, Mengqi Yuan, Yifei Zeng, Chang Zeng, Zhihao Li, Zhenyu Chen, Weichao Qiu, Xiao-Xiao Long, Hao Zhu, Xun Cao, Yao Yao</strong></p>
<p>Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the objectâ€™s appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the objectâ€™s placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at <a target="_blank" rel="noopener" href="https://nju-3dv.github.io/projects/ComGS/">https://nju-3dv.github.io/projects/ComGS/</a>. </p>
<blockquote>
<p>é«˜æ–¯è´´å›¾ï¼ˆGSï¼‰æŠ€æœ¯èƒ½å¤Ÿå®ç°æ²‰æµ¸å¼æ¸²æŸ“ï¼Œä½†ç°å®3Då¯¹è±¡åœºæ™¯ç»„åˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚GSè¾å°„åœºä¸­çš„çƒ˜ç„™å¤–è§‚å’Œé˜´å½±ä¿¡æ¯åœ¨ç»„åˆå¯¹è±¡å’Œåœºæ™¯æ—¶ä¼šå¯¼è‡´ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œå¯é‡æ–°ç…§æ˜çš„å¯¹è±¡é‡å»ºå’Œåœºæ™¯ç…§æ˜ä¼°è®¡ã€‚å¯¹äºå¯é‡æ–°ç…§æ˜çš„å¯¹è±¡é‡å»ºï¼Œç°æœ‰çš„åŸºäºé«˜æ–¯å€¼çš„é€†å‘æ¸²æŸ“æ–¹æ³•é€šå¸¸ä¾èµ–äºå…‰çº¿è¿½è¸ªï¼Œå¯¼è‡´æ•ˆç‡è¾ƒä½ã€‚æˆ‘ä»¬å¼•å…¥äº†è¡¨é¢å…«é¢æ¢é’ˆï¼ˆSOPsï¼‰ï¼Œå…¶èƒ½å¤Ÿå­˜å‚¨ç…§æ˜å’Œé®æŒ¡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ’å€¼å®ç°é«˜æ•ˆ3DæŸ¥è¯¢ï¼Œé¿å…äº†æ˜‚è´µçš„å…‰çº¿è¿½è¸ªã€‚SOPsè‡³å°‘å¯å°†é‡å»ºé€Ÿåº¦æé«˜2å€ï¼Œå¹¶åœ¨é«˜æ–¯åœºæ™¯ä¸­å®ç°å®æ—¶é˜´å½±è®¡ç®—ã€‚å¯¹äºç…§æ˜ä¼°è®¡ï¼Œç°æœ‰çš„åŸºäºé«˜æ–¯çš„é€†å‘æ¸²æŸ“æ–¹æ³•åœ¨å¤æ‚çš„å…‰çº¿ä¼ è¾“å»ºæ¨¡æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­ç»å¸¸å¤±æ•ˆï¼Œè€ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•åˆ™ä»å•ä¸€å›¾åƒé¢„æµ‹ç…§æ˜ï¼Œå¹¶å¯¹è§‚ç‚¹æ•æ„Ÿã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œ3Då¯¹è±¡åœºæ™¯ç»„åˆä¸»è¦å…³æ³¨å¯¹è±¡çš„å¤–è§‚å’Œé™„è¿‘çš„é˜´å½±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡å…³æ³¨å¯¹è±¡æ”¾ç½®å¤„çš„ç¯å¢ƒç…§æ˜æ¥ç®€åŒ–å®Œæ•´çš„åœºæ™¯ç…§æ˜ä¼°è®¡è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨è¯¥ä½ç½®æ•è·åœºæ™¯çš„360åº¦é‡å»ºè¾å°„åœºï¼Œå¹¶å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å®Œæˆç…§æ˜ã€‚åŸºäºè¿™äº›è¿›å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ComGSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Då¯¹è±¡åœºæ™¯ç»„åˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥å¤§çº¦28å¸§æ¯ç§’çš„é€Ÿåº¦å®ç°é«˜è´¨é‡å®æ—¶æ¸²æŸ“ï¼Œäº§ç”Ÿè§†è§‰å’Œè°ã€é˜´å½±ç”ŸåŠ¨çš„ç»“æœï¼Œç¼–è¾‘ä»…éœ€36ç§’ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://nju-3dv.github.io/projects/ComGS/%E8%8E%B7%E5%8F%96%E3%80%82">https://nju-3dv.github.io/projects/ComGS/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07729v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†Gaussian Splattingï¼ˆGSï¼‰åœ¨æ²‰æµ¸å¼æ¸²æŸ“æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶é’ˆå¯¹3Då¯¹è±¡åœºæ™¯ç»„åˆçš„æŒ‘æˆ˜æ€§è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚æ–‡ç« é€šè¿‡å¼•å…¥Surface Octahedral Probesï¼ˆSOPsï¼‰æ¥è§£å†³å¯¹è±¡å’Œåœºæ™¯ç»„åˆæ—¶çš„ä¸ä¸€è‡´æ€§ï¼Œæé«˜äº†é‡å»ºå’Œå®æ—¶é˜´å½±è®¡ç®—æ•ˆç‡ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§ç®€åŒ–åœºæ™¯ç…§æ˜ä¼°è®¡çš„æ–¹æ³•ï¼Œä¸“æ³¨äºå¯¹è±¡æ”¾ç½®å¤„çš„ç¯å¢ƒç…§æ˜ï¼Œå¹¶ç»“åˆæ‰©æ•£æ¨¡å‹å®Œæˆç…§æ˜ã€‚æœ€ç»ˆï¼Œæ–‡ç« æå‡ºäº†ComGSï¼Œä¸€ä¸ªæ–°å‹çš„3Då¯¹è±¡åœºæ™¯ç»„åˆæ¡†æ¶ï¼Œå®ç°äº†é«˜è´¨é‡ã€å®æ—¶çš„æ¸²æŸ“æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Gaussian Splattingï¼ˆGSï¼‰åœ¨æ²‰æµ¸å¼æ¸²æŸ“ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†3Då¯¹è±¡åœºæ™¯ç»„åˆå­˜åœ¨æŒ‘æˆ˜ã€‚<br>2.SOPsçš„å¼•å…¥è§£å†³äº†å¯¹è±¡å’Œåœºæ™¯ç»„åˆæ—¶çš„ä¸ä¸€è‡´æ€§ï¼Œé€šè¿‡å­˜å‚¨å…‰ç…§å’Œé®æŒ¡ä¿¡æ¯ï¼Œå…è®¸é€šè¿‡æ’å€¼è¿›è¡Œé«˜æ•ˆ3DæŸ¥è¯¢ï¼Œé¿å…æ˜‚è´µçš„å…‰çº¿è¿½è¸ªã€‚<br>3.ç°æœ‰åŸºäºGaussiançš„é€†å‘æ¸²æŸ“æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­çš„å…‰ç…§å»ºæ¨¡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œå­¦ä¹ åŸºäºæ–¹æ³•çš„æ–¹æ³•åˆ™ä»å•ä¸€å›¾åƒé¢„æµ‹å…‰ç…§ï¼Œå¯¹è§‚ç‚¹æ•æ„Ÿã€‚<br>4.æ–‡ç« é€šè¿‡ä¸“æ³¨äºå¯¹è±¡æ”¾ç½®å¤„çš„ç¯å¢ƒç…§æ˜ï¼Œç®€åŒ–äº†å¤æ‚çš„åœºæ™¯ç…§æ˜ä¼°è®¡ä»»åŠ¡ã€‚<br>5.ç»“åˆæ‰©æ•£æ¨¡å‹å®Œæˆç…§æ˜ï¼Œæé«˜äº†æ¸²æŸ“è´¨é‡å’Œå®æ—¶æ€§ã€‚<br>6.æå‡ºäº†ComGSï¼Œä¸€ä¸ªæ–°å‹çš„3Då¯¹è±¡åœºæ™¯ç»„åˆæ¡†æ¶ï¼Œå®ç°äº†é«˜è´¨é‡ã€å®æ—¶çš„æ¸²æŸ“ï¼Œå¹¶å…·å¤‡ç¼–è¾‘åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-631cabeab6e53f860a30b22be62e6e7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760130995&auth_key=1760130995-0-0-fc245cbd9d085ec153074bc2754c0a70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff176087408bb7dd36390232a08588c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131003&auth_key=1760131003-0-0-dc7a4ecb454df9d903af8490a186eb09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07f190f3bfa44ddba6ff3602f68ba829~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131009&auth_key=1760131009-0-0-24173b01f9728e0ede0c9582049d1b63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57be79e0bc77ecd7b902daae7606e7ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131016&auth_key=1760131016-0-0-75b6e0cf3655110f9674243d8744b10a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rectified-CFG-for-Flow-Based-Models"><a href="#Rectified-CFG-for-Flow-Based-Models" class="headerlink" title="Rectified-CFG++ for Flow Based Models"></a>Rectified-CFG++ for Flow Based Models</h2><p><strong>Authors:Shreshth Saini, Shashank Gupta, Alan C. Bovik</strong></p>
<p>Classifier-free guidance (CFG) is the workhorse for steering large diffusion models toward text-conditioned targets, yet its native application to rectified flow (RF) based models provokes severe off-manifold drift, yielding visual artifacts, text misalignment, and brittle behaviour. We present Rectified-CFG++, an adaptive predictor-corrector guidance that couples the deterministic efficiency of rectified flows with a geometry-aware conditioning rule. Each inference step first executes a conditional RF update that anchors the sample near the learned transport path, then applies a weighted conditional correction that interpolates between conditional and unconditional velocity fields. We prove that the resulting velocity field is marginally consistent and that its trajectories remain within a bounded tubular neighbourhood of the data manifold, ensuring stability across a wide range of guidance strengths. Extensive experiments on large-scale text-to-image models (Flux, Stable Diffusion 3&#x2F;3.5, Lumina) show that Rectified-CFG++ consistently outperforms standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and T2I-CompBench. Project page: <a target="_blank" rel="noopener" href="https://rectified-cfgpp.github.io/">https://rectified-cfgpp.github.io/</a> </p>
<blockquote>
<p>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯æ¨åŠ¨å¤§å‹æ‰©æ•£æ¨¡å‹å‘æ–‡æœ¬æ¡ä»¶ç›®æ ‡å‘å±•çš„ä¸»è¦æ–¹æ³•ï¼Œç„¶è€Œå®ƒç›´æ¥åº”ç”¨äºåŸºäºæ­£è§„åŒ–æµï¼ˆRFï¼‰çš„æ¨¡å‹ä¼šå¼•å‘ä¸¥é‡çš„æµå½¢æ¼‚ç§»é—®é¢˜ï¼Œå¯¼è‡´è§†è§‰ä¼ªå½±ã€æ–‡æœ¬é”™ä½å’Œæ¨¡å‹è¡¨ç°è„†å¼±ã€‚æˆ‘ä»¬æå‡ºäº†æ­£è§„åŒ–æµ-CFG++ï¼ˆRectified-CFG++ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”çš„é¢„æµ‹æ ¡æ­£å¼•å¯¼æ³•ï¼Œå®ƒå°†æ­£è§„åŒ–æµçš„ç¡®å®šæ€§æ•ˆç‡ä¸å‡ ä½•æ„ŸçŸ¥æ¡ä»¶è§„åˆ™ç›¸ç»“åˆã€‚æ¯æ¬¡æ¨ç†æ­¥éª¤é¦–å…ˆæ‰§è¡Œæ¡ä»¶RFæ›´æ–°ï¼Œå°†æ ·æœ¬å›ºå®šåœ¨æ¥è¿‘å­¦ä¹ åˆ°çš„ä¼ è¾“è·¯å¾„ä¸Šï¼Œç„¶ååº”ç”¨åŠ æƒæ¡ä»¶æ ¡æ­£ï¼Œåœ¨æ¡ä»¶å’Œæ— æ¡ä»¶é€Ÿåº¦åœºä¹‹é—´è¿›è¡Œæ’å€¼ã€‚æˆ‘ä»¬è¯æ˜å¾—åˆ°çš„é€Ÿåº¦åœºå…·æœ‰è¾¹é™…ä¸€è‡´æ€§ï¼Œå…¶è½¨è¿¹ä¿æŒåœ¨æ•°æ®æµå½¢çš„æœ‰ç•Œç®¡çŠ¶é‚»åŸŸå†…ï¼Œç¡®ä¿äº†åœ¨ä¸åŒå¼ºåº¦çš„å¼•å¯¼ä¸‹çš„ç¨³å®šæ€§ã€‚åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼ˆFluxã€ç¨³å®šæ‰©æ•£3&#x2F;3.5ã€Luminaï¼‰è¡¨æ˜ï¼ŒRectified-CFG++åœ¨MS-COCOã€LAION-Aestheticå’ŒT2I-CompBenchç­‰åŸºå‡†æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºæ ‡å‡†CFGã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://rectified-cfgpp.github.io/%EF%BC%88%E6%AD%A4%E9%83%A8%E5%88%86%E4%B8%8D%E7%BF%BB%E8%AF%91%EF%BC%89">https://rectified-cfgpp.github.io/ï¼ˆæ­¤éƒ¨åˆ†ä¸ç¿»è¯‘ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07631v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ç”¨äºå¤§å°ºåº¦æ‰©æ•£æ¨¡å‹çš„æ”¹è¿›å‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æ–¹æ³•â€”â€”Rectified-CFG++ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¡®å®šæ€§é«˜çš„rectifiedæµå’Œå‡ ä½•æ„ŸçŸ¥æ¡ä»¶è§„åˆ™ï¼Œæ—¨åœ¨è§£å†³åŸæœ‰æ–¹æ³•åœ¨æ–‡æœ¬æ¡ä»¶ä¸‹çš„ç›®æ ‡åº”ç”¨ä¸­å­˜åœ¨çš„è§†è§‰ä¼ªå½±ã€æ–‡æœ¬ä¸åŒ¹é…ç­‰é—®é¢˜ï¼Œé€šè¿‡æ¯ä¸€æ­¥æ¨ç†å…ˆè¿›è¡Œæ¡ä»¶æ€§RFæ›´æ–°ä»¥å°†æ ·æœ¬é”šå®šåœ¨å­¦ä¹ è·¯å¾„é™„è¿‘ï¼Œå†åº”ç”¨åŠ æƒæ¡ä»¶æ ¡æ­£ä»¥åœ¨æ¡ä»¶é€Ÿåº¦åœºå’Œæ— æ¡ä»¶é€Ÿåº¦åœºä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œä»è€Œç¡®ä¿æ ·æœ¬çš„ç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒRectified-CFG++åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ ‡å‡†CFGåœ¨MS-COCOã€LAION-Aestheticå’ŒT2I-CompBenchç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rectified-CFG++è§£å†³äº†åŸæœ‰æ–¹æ³•åœ¨æ–‡æœ¬æ¡ä»¶ä¸‹çš„ç›®æ ‡åº”ç”¨ä¸­çš„è§†è§‰ä¼ªå½±å’Œæ–‡æœ¬ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç¡®å®šæ€§é«˜çš„rectifiedæµå’Œå‡ ä½•æ„ŸçŸ¥æ¡ä»¶è§„åˆ™ï¼Œæ—¨åœ¨æé«˜å¤§å°ºåº¦æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Rectified-CFG++é€šè¿‡é¢„æµ‹å’Œæ ¡æ­£æœºåˆ¶ç¡®ä¿æ ·æœ¬çš„ç¨³å®šæ€§ï¼Œå³ä½¿åœ¨ä¸åŒå¼ºåº¦çš„å¼•å¯¼ä¸‹ä¹Ÿèƒ½ä¿æŒç¨³å®šæ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRectified-CFG++åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†CFGã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¦‚MS-COCOã€LAION-Aestheticå’ŒT2I-CompBenchç­‰ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š<a target="_blank" rel="noopener" href="https://rectified-cfgpp.github.io/">https://rectified-cfgpp.github.io/</a>ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f0d2cb9fc4d38124ee13dfbde82ce3e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131023&auth_key=1760131023-0-0-7d80e1bac4c41549abcc49c785279f4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b9d23177ecbdca729d33805c76aff458~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131030&auth_key=1760131030-0-0-940956aea4603f618ade328750664c65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32274e7490c918058c57629235583548~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131036&auth_key=1760131036-0-0-430be57fca3311f87bf096932ac94342&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2152a4ab062e7bbc5278c64743a50a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131043&auth_key=1760131043-0-0-fed179682ed34ffce78f43ad02a70d84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3fd1b942b15a72641c0f07264e50c05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131049&auth_key=1760131049-0-0-8d7300c2d5918f6b865659aa822e1def&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09e76a136293cf8c59154b908288dc64~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131056&auth_key=1760131056-0-0-94479b25f933242d39c39dae52bc340d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EMPalm-Exfiltrating-Palm-Biometric-Data-via-Electromagnetic-Side-Channels"><a href="#EMPalm-Exfiltrating-Palm-Biometric-Data-via-Electromagnetic-Side-Channels" class="headerlink" title="EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic   Side-Channels"></a>EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic   Side-Channels</h2><p><strong>Authors:Haowen Xu, Tianya Zhao, Xuyu Wang, Lei Ma, Jun Dai, Alexander Wyglinski, Xiaoyan Sun</strong></p>
<p>Palm recognition has emerged as a dominant biometric authentication technology in critical infrastructure. These systems operate in either single-modal form, using palmprint or palmvein individually, or dual-modal form, fusing the two modalities. Despite this diversity, they share similar hardware architectures that inadvertently emit electromagnetic (EM) signals during operation. Our research reveals that these EM emissions leak palm biometric information, motivating us to develop EMPalmâ€“an attack framework that covertly recovers both palmprint and palmvein images from eavesdropped EM signals. Specifically, we first separate the interleaved transmissions of the two modalities, identify and combine their informative frequency bands, and reconstruct the images. To further enhance fidelity, we employ a diffusion model to restore fine-grained biometric features unique to each domain. Evaluations on seven prototype and two commercial palm acquisition devices show that EMPalm can recover palm biometric information with high visual fidelity, achieving SSIM scores up to 0.79, PSNR up to 29.88 dB, and FID scores as low as 6.82 across all tested devices, metrics that collectively demonstrate strong structural similarity, high signal quality, and low perceptual discrepancy. To assess the practical implications of the attack, we further evaluate it against four state-of-the-art palm recognition models, achieving a model-wise average spoofing success rate of 65.30% over 6,000 samples from 100 distinct users. </p>
<blockquote>
<p>æŒçº¹è¯†åˆ«å·²åœ¨å…³é”®åŸºç¡€è®¾æ–½ä¸­å´­éœ²å¤´è§’ï¼Œæˆä¸ºä¸»è¦çš„ç”Ÿç‰©è®¤è¯æŠ€æœ¯ã€‚è¿™äº›ç³»ç»Ÿä»¥å•æ¨¡æ€å½¢å¼è¿è¡Œï¼Œå•ç‹¬ä½¿ç”¨æŒçº¹æˆ–æŒé™è„‰ï¼Œæˆ–ä»¥åŒæ¨¡æ€å½¢å¼è¿è¡Œï¼Œèåˆè¿™ä¸¤ç§æ¨¡å¼ã€‚å°½ç®¡å­˜åœ¨è¿™ç§å¤šæ ·æ€§ï¼Œä½†å®ƒä»¬å´æ‹¥æœ‰ç±»ä¼¼çš„ç¡¬ä»¶æ¶æ„ï¼Œåœ¨è¿è¡Œæ—¶æ— æ„ä¸­å‘å‡ºç”µç£ï¼ˆEMï¼‰ä¿¡å·ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›EMæ’æ”¾ä¼šæ³„éœ²æŒçº¹ç”Ÿç‰©è¯†åˆ«ä¿¡æ¯ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬å¼€å‘å‡ºEMPalmâ€”â€”ä¸€ç§æ”»å‡»æ¡†æ¶ï¼Œå¯ä»¥éšç§˜åœ°ä»çªƒå¬çš„EMä¿¡å·ä¸­æ¢å¤æŒçº¹å’ŒæŒé™è„‰å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†ç¦»ä¸¤ç§æ¨¡å¼çš„äº¤é”™ä¼ è¾“ï¼Œè¯†åˆ«å¹¶ç»“åˆå…¶ä¿¡æ¯é¢‘æ®µï¼Œç„¶åé‡å»ºå›¾åƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä¿çœŸåº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥æ¢å¤æ¯ä¸ªé¢†åŸŸçš„ç‹¬ç‰¹ç»†å¾®ç”Ÿç‰©ç‰¹å¾ã€‚åœ¨ä¸ƒä¸ªåŸå‹å’Œä¸¤ä¸ªå•†ç”¨æŒçº¹é‡‡é›†è®¾å¤‡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒEMPalmå¯ä»¥é«˜åº¦é€¼çœŸåœ°æ¢å¤æŒçº¹ç”Ÿç‰©è¯†åˆ«ä¿¡æ¯ï¼Œå®ç°é«˜è¾¾0.79çš„ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰åˆ†æ•°ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰é«˜è¾¾29.88åˆ†è´ï¼Œä»¥åŠæ‰€æœ‰æµ‹è¯•è®¾å¤‡ä¸­FIDåˆ†æ•°ä½è‡³6.82ã€‚è¿™äº›æŒ‡æ ‡å…±åŒä½“ç°äº†å¼ºå¤§çš„ç»“æ„ç›¸ä¼¼æ€§ã€é«˜ä¿¡å·è´¨é‡å’Œä½æ„ŸçŸ¥å·®å¼‚ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ”»å‡»çš„å®é™…å½±å“ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åœ¨å››ä¸ªæœ€å…ˆè¿›çš„æŒçº¹è¯†åˆ«æ¨¡å‹ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨æ¥è‡ª100ä¸ªä¸åŒç”¨æˆ·çš„6000ä¸ªæ ·æœ¬ä¸Šï¼Œæ¨¡å‹çº§çš„å¹³å‡å‡å†’æˆåŠŸç‡è¾¾åˆ°65.30%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07533v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ‰‹éƒ¨ç”Ÿç‰©ç‰¹å¾è¯†åˆ«æŠ€æœ¯çš„ç”µç£æ³„éœ²é—®é¢˜ã€‚ç ”ç©¶ä¸­å‘ç°ï¼Œæ‰‹è¯†åˆ«ç³»ç»Ÿåœ¨æ“ä½œè¿‡ç¨‹ä¸­ä¼šæ— æ„ä¸­å‘å‡ºåŒ…å«æ‰‹æŒçº¹ç†ä¿¡æ¯çš„ç”µç£ä¿¡å·ï¼Œå¹¶ç”±æ­¤æå‡ºäº†EMPalmæ”»å‡»æ¡†æ¶ï¼Œå¯ä»¥ä»çªƒå¬çš„ç”µç£ä¿¡å·ä¸­æ¢å¤æ‰‹æŒçº¹è·¯å’ŒæŒé™è„‰å›¾åƒã€‚å®éªŒè¯æ˜ï¼ŒEMPalmå¯ä»¥åœ¨å„ç§è®¾å¤‡ä¸Šå®ç°é«˜ä¿çœŸåº¦çš„ä¿¡æ¯æ¢å¤ï¼Œå¹¶èƒ½æˆåŠŸæ”»å‡»å¤šç§æ‰‹æŒè¯†åˆ«æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹éƒ¨è¯†åˆ«æŠ€æœ¯æˆä¸ºå…³é”®åŸºç¡€è®¾æ–½ä¸­çš„ä¸»è¦ç”Ÿç‰©è®¤è¯æŠ€æœ¯ï¼Œæœ‰å•æ¨¡æ€å’ŒåŒæ¨¡æ€ä¸¤ç§å½¢å¼ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†æ‰‹éƒ¨ç”Ÿç‰©ç‰¹å¾è¯†åˆ«ç³»ç»Ÿæ“ä½œä¸­ä¼šæ— æ„å‘å‡ºç”µç£ä¿¡å·ï¼Œå…¶ä¸­åŒ…å«ç”Ÿç‰©ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>EMPalmæ¡†æ¶å¯ä»¥ä»çªƒå¬çš„ç”µç£ä¿¡å·ä¸­æ¢å¤æ‰‹æŒçº¹è·¯å’ŒæŒé™è„‰å›¾åƒã€‚</li>
<li>EMPalmé€šè¿‡åˆ†ç¦»ä¸¤ç§æ¨¡æ€çš„ä¼ è¾“ä¿¡å·ã€è¯†åˆ«å¹¶ç»“åˆå…¶ä¿¡æ¯é¢‘æ®µï¼Œè¿›è€Œé‡å»ºå›¾åƒã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¢å¼ºå›¾åƒçš„ç»†èŠ‚æ¢å¤ï¼Œæé«˜å„é¢†åŸŸçš„ç”Ÿç‰©ç‰¹å¾è¾¨è¯†åº¦ã€‚</li>
<li>å®éªŒè¯æ˜EMPalmåœ¨å¤šç§è®¾å¤‡ä¸Šçš„ä¿¡æ¯æ¢å¤å…·æœ‰é«˜è´¨é‡ï¼ŒåŒ…æ‹¬é«˜ç»“æ„ç›¸ä¼¼æ€§ã€é«˜ä¿¡å·è´¨é‡å’Œä½æ„ŸçŸ¥å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6367f39a044db3818e884d1c45e072f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131064&auth_key=1760131064-0-0-59bb31e98789e3ebbb2d53145bd0abec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b46476c60a410de64d02e633ab3bed71~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131071&auth_key=1760131071-0-0-e8df8f28f2c250529cc451bebaf86b37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aaf4e4f31cd2af993e29ffb29a23c489~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131077&auth_key=1760131077-0-0-939624884926b82332c001ee7940bb8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c481abbb4472509f5e63f04dc39bbb2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131084&auth_key=1760131084-0-0-755e0b25ebaffec27c31c0c6cccb9462&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ca4294c6efd68abee3bdb123f12d095~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131091&auth_key=1760131091-0-0-19e2800fab1a934851683ea295db5860&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1718610ff2b7c753f5aeda85ab27f42d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131097&auth_key=1760131097-0-0-7a0e1372b40898dc378af977184541ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DiffEye-Diffusion-Based-Continuous-Eye-Tracking-Data-Generation-Conditioned-on-Natural-Images"><a href="#DiffEye-Diffusion-Based-Continuous-Eye-Tracking-Data-Generation-Conditioned-on-Natural-Images" class="headerlink" title="DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation   Conditioned on Natural Images"></a>DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation   Conditioned on Natural Images</h2><p><strong>Authors:Ozgur Kara, Harris Nisar, James M. Rehg</strong></p>
<p>Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: <a target="_blank" rel="noopener" href="https://diff-eye.github.io/">https://diff-eye.github.io/</a> </p>
<blockquote>
<p>é’ˆå¯¹è§†çº¿è½¨è¿¹å’Œæ˜¾è‘—æ€§é¢„æµ‹ï¼Œå·²ç»å¼€å‘äº†è®¸å¤šæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸åŸºäºè§†çº¿è½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œå°†çœ¼çƒè¿åŠ¨æ¨¡æ‹Ÿä¸ºä¸€ç³»åˆ—ç”±çœ¼è·³è¿æ¥çš„ç¦»æ•£æ³¨è§†ç‚¹åºåˆ—ï¼Œè€ŒåŸå§‹è½¨è¿¹ä¸­åŒ…å«çš„ä¸°å¯Œä¿¡æ¯é€šå¸¸è¢«ä¸¢å¼ƒã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•æ— æ³•æ•æ‰åŒä¸€å›¾åƒä¸­ä¸åŒäººç±»ä¸»ä½“ä¹‹é—´çš„å·®å¼‚æ€§ã€‚å®ƒä»¬é€šå¸¸é¢„æµ‹å…·æœ‰å›ºå®šé¢„å®šä¹‰é•¿åº¦çš„å•ä¸€è§†çº¿è½¨è¿¹ï¼Œè¿™ä¸çœŸå®ä¸–ç•Œè§†è§‰æ³¨æ„åŠ›çš„å†…åœ¨å¤šæ ·æ€§å’Œéšæœºæ€§ç›¸å†²çªã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffEyeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿåœ¨è‡ªç„¶å›¾åƒè‡ªç”±è§‚çœ‹æœŸé—´çš„è¿ç»­å’Œå¤šæ ·çš„çœ¼çƒè¿åŠ¨è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºè§†è§‰åˆºæ¿€çš„æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ä¸Šæ„å»ºï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºç›¸åº”ä½ç½®åµŒå…¥ï¼ˆCPEï¼‰çš„æ–°ç»„ä»¶ï¼Œå®ƒå°†ç©ºé—´å‡è§†ä¿¡æ¯ä¸è§†è§‰è¾“å…¥çš„åŸºäºè¡¥ä¸çš„è¯­ä¹‰ç‰¹å¾å¯¹é½ã€‚DiffEyeé€šè¿‡åˆ©ç”¨åŸå§‹çš„çœ¼åŠ¨è½¨è¿¹ï¼Œè€Œä¸æ˜¯ä¾èµ–äºè§†çº¿è½¨è¿¹ï¼Œæ¥æ•æ‰äººç±»å‡è§†è¡Œä¸ºçš„å†…åœ¨å˜åŒ–ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„ã€ç°å®çš„çœ¼åŠ¨æ¨¡å¼ï¼Œå°½ç®¡å®ƒæ˜¯åœ¨ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚ç”Ÿæˆçš„è½¨è¿¹è¿˜å¯ä»¥è½¬æ¢ä¸ºè§†çº¿è½¨è¿¹å’Œæ˜¾è‘—æ€§åœ°å›¾ï¼Œä»è€Œäº§ç”Ÿæ›´å‡†ç¡®åœ°åæ˜ äººç±»è§†è§‰æ³¨æ„åŠ›åˆ†å¸ƒçš„è¾“å±±ã€‚DiffEyeæ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†è‡ªç„¶å›¾åƒä»»åŠ¡çš„æ–¹æ³•ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨äº†åŸå§‹çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®çš„ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDiffEyeä¸ä»…åœ¨è§†çº¿è½¨è¿¹ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè€Œä¸”é¦–æ¬¡å®ç°äº†è¿ç»­çš„çœ¼åŠ¨è½¨è¿¹ç”Ÿæˆã€‚é¡¹ç›®ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="https://diff-eye.github.io/">https://diff-eye.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16767v2">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰æ³¨è§†è·¯å¾„é¢„æµ‹çš„æ–°æ¨¡å‹DiffEyeã€‚è¯¥æ¨¡å‹åŸºäºæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿè¿ç»­ä¸”å¤šæ ·çš„çœ¼åŠ¨è½¨è¿¹ã€‚æ¨¡å‹é€šè¿‡å¼•å…¥CPEï¼ˆå¯¹åº”ä½ç½®åµŒå…¥ï¼‰æ¥å°†ç©ºé—´æ³¨è§†ä¿¡æ¯ä¸è§†è§‰è¾“å…¥çš„è¯­ä¹‰ç‰¹å¾å¯¹é½ã€‚ä½¿ç”¨åŸå§‹çœ¼åŠ¨è½¨è¿¹æ•°æ®ï¼ŒDiffEyeèƒ½å¤Ÿæ•æ‰äººç±»æ³¨è§†è¡Œä¸ºçš„å†…åœ¨å˜åŒ–ï¼Œç”Ÿæˆé€¼çœŸçš„çœ¼åŠ¨æ¨¡å¼ï¼Œå¹¶åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒä¹Ÿèƒ½è¾¾åˆ°é«˜è´¨é‡çš„ç»“æœã€‚DiffEyeä¸ä»…èƒ½å¤Ÿç”Ÿæˆæ‰«æè·¯å¾„å’Œæ˜¾è‘—æ€§åœ°å›¾ï¼Œè€Œä¸”èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ äººç±»è§†è§‰æ³¨æ„çš„åˆ†å¸ƒã€‚å®ƒåœ¨è‡ªç„¶å›¾åƒä»»åŠ¡ä¸­è¾¾åˆ°äº†åˆ©ç”¨åŸå§‹çœ¼åŠ¨è½¨è¿¹æ•°æ®çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffEyeæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿè‡ªç„¶å›¾åƒè§‚çœ‹æ—¶çš„è¿ç»­å’Œå¤šæ ·åŒ–çš„çœ¼åŠ¨è½¨è¿¹ã€‚</li>
<li>è¯¥æ¨¡å‹å¼•å…¥CPEï¼ˆå¯¹åº”ä½ç½®åµŒå…¥ï¼‰ï¼Œå°†ç©ºé—´æ³¨è§†ä¿¡æ¯ä¸è§†è§‰è¾“å…¥çš„è¯­ä¹‰ç‰¹å¾å¯¹é½ã€‚</li>
<li>DiffEyeä½¿ç”¨åŸå§‹çœ¼åŠ¨è½¨è¿¹æ•°æ®ï¼Œèƒ½å¤Ÿæ•æ‰äººç±»æ³¨è§†è¡Œä¸ºçš„å†…åœ¨å˜åŒ–å¹¶ç”Ÿæˆé«˜è´¨é‡çš„çœ¼åŠ¨æ¨¡å¼ã€‚</li>
<li>DiffEyeèƒ½å¤Ÿåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„çœ¼åŠ¨è½¨è¿¹ã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥ç”Ÿæˆæ‰«æè·¯å¾„å’Œæ˜¾è‘—æ€§åœ°å›¾ï¼Œæ›´å‡†ç¡®åæ˜ äººç±»è§†è§‰æ³¨æ„çš„åˆ†å¸ƒã€‚</li>
<li>DiffEyeæ˜¯é¦–ä¸ªåœ¨è‡ªç„¶å›¾åƒä»»åŠ¡ä¸­ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¹¶å……åˆ†åˆ©ç”¨åŸå§‹çœ¼åŠ¨è½¨è¿¹æ•°æ®çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-755578d95da9ad59c1b93ccbce9aca53~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131105&auth_key=1760131105-0-0-605e8108756f9430d9ac751c30d048b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3111b1bd42c50cc4ca11889c765fdf53~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131112&auth_key=1760131112-0-0-5f63fe6d69ef602a6e5b968142f865a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9be5d21f003ef093d954ba69f9e02060~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131118&auth_key=1760131118-0-0-93f3869494c414949cfa5c1e8f12dbf7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6712f8a6ddf2a0c2fddcb819de22a2e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131125&auth_key=1760131125-0-0-1413bf140cc889d6326028c51564c135&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-125ee86ce60b86387db395e5340bf2eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131132&auth_key=1760131132-0-0-9b95d529317f3771d418d788acf705df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MAMBO-High-Resolution-Generative-Approach-for-Mammography-Images"><a href="#MAMBO-High-Resolution-Generative-Approach-for-Mammography-Images" class="headerlink" title="MAMBO: High-Resolution Generative Approach for Mammography Images"></a>MAMBO: High-Resolution Generative Approach for Mammography Images</h2><p><strong>Authors:Milica Å kipina, Nikola JoviÅ¡iÄ‡, Nicola Dallâ€™Asen, Vanja Å venda, Anil Osman Tur, Slobodan IliÄ‡, Elisa Ricci, Dubravko Ä†ulibrk</strong></p>
<p>Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final model, significantly aiding the noise removal process. This design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly segmentation. Experiments, both numerical and radiologist validation, assess MAMBOâ€™s capabilities in image generation, super-resolution, and anomaly segmentation, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection. The source code used in this study is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/iai-rs/mambo">https://github.com/iai-rs/mambo</a>. </p>
<blockquote>
<p>ä¹³è…ºXå…‰æ‘„å½±æ˜¯æ£€æµ‹å’Œè¯Šæ–­ä¹³è…ºç™Œçš„é‡‘æ ‡å‡†ã€‚é€šè¿‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è½¯ä»¶å¯ä»¥æ˜¾è‘—å¢å¼ºè¿™ä¸€ç¨‹åºçš„æ•ˆæœï¼Œå¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¯†åˆ«å¼‚å¸¸ã€‚ç„¶è€Œï¼Œè®­ç»ƒAIç³»ç»Ÿéœ€è¦å¤§å‹ä¸”å¤šæ ·çš„æ•°æ®é›†ï¼Œç”±äºéšç§å’Œé“å¾·çº¦æŸï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€éš¾ä»¥è·å¾—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†MAMmography ensemBle mOdelï¼ˆMAMBOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¡¥ä¸çš„æ‰©æ•£æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå…¨åˆ†è¾¨ç‡ä¹³è…ºXçº¿å½±åƒã€‚æ‰©æ•£æ¨¡å‹åœ¨çœŸå®å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§æˆæœï¼Œä½†å¯¹ä¹³è…ºXçº¿å½±åƒçš„ç ”ç©¶å¾ˆå°‘ï¼Œä¸”å°šæ— ç ”ç©¶èƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºï¼Œä»¥æ•æ‰å°ç—…ç¶çš„ç²¾ç»†ç‰¹å¾ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒMAMBOé›†æˆäº†å•ç‹¬çš„æ‰©æ•£æ¨¡å‹æ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ï¼ˆå›¾åƒçº§åˆ«ï¼‰çš„ä¸Šä¸‹æ–‡ã€‚ç„¶åå°†ä¸Šä¸‹æ–‡ä¿¡æ¯è¾“å…¥åˆ°æœ€ç»ˆæ¨¡å‹ä¸­ï¼Œæå¤§åœ°æœ‰åŠ©äºå»å™ªè¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡ä½¿MAMBOèƒ½å¤Ÿç”Ÿæˆé«˜è¾¾3840x3840åƒç´ çš„éå¸¸é€¼çœŸçš„ä¹³è…ºXçº¿å½±åƒã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•å¯ç”¨äºå¢å¼ºåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶å¯æ‰©å±•åˆ°å¼‚å¸¸åˆ†å‰²ã€‚å®éªŒåŒ…æ‹¬æ•°å€¼éªŒè¯å’Œæ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯ï¼Œè¯„ä¼°äº†MAMBOåœ¨å›¾åƒç”Ÿæˆã€è¶…åˆ†è¾¨ç‡å’Œå¼‚å¸¸åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ï¼Œçªæ˜¾å…¶åœ¨æé«˜ä¹³è…ºXçº¿æ‘„å½±åˆ†æã€æ›´å‡†ç¡®çš„è¯Šæ–­å’Œæ›´æ—©çš„ç—…ç¶æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iai-rs/mambo">https://github.com/iai-rs/mambo</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08677v3">PDF</a> 21 pages, 14 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è¾…åŠ©çš„ä¹³è…ºXå…‰æ‘„å½±ï¼ˆmammographyï¼‰èƒ½æ˜¾è‘—æé«˜ä¹³è…ºç™Œçš„æ£€æµ‹å’Œè¯Šæ–­æ°´å¹³ã€‚ä¸ºè§£å†³AIç³»ç»Ÿè®­ç»ƒæ‰€éœ€çš„å¤§å‹å¤šæ ·æ•°æ®é›†éš¾ä»¥è·å–çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†MAMmography ensemBle mOdelï¼ˆMAMBOï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¹³è…ºXå…‰å›¾åƒï¼Œé€šè¿‡é›†æˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œåˆ†è¾¨ç‡é«˜è¾¾3840x3840åƒç´ ã€‚è¯¥æ¨¡å‹å¯åº”ç”¨äºåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒå¢å¼ºå’Œå¼‚å¸¸åˆ†å‰²ï¼Œæœ‰æœ›æé«˜ä¹³è…ºXå…‰æ‘„å½±çš„åˆ†æå‡†ç¡®æ€§ï¼Œå®ç°æ›´æ—©çš„ç—…å˜æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨ä¹³è…ºXå…‰æ‘„å½±ä¸­çš„åº”ç”¨èƒ½æ˜¾è‘—æé«˜ä¹³è…ºç™Œçš„æ£€æµ‹å’Œè¯Šæ–­æ°´å¹³ã€‚</li>
<li>MAMBOæ¨¡å‹æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¹³è…ºXå…‰å›¾åƒã€‚</li>
<li>MAMBOé€šè¿‡é›†æˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒã€‚</li>
<li>MAMBOæ¨¡å‹å¯åº”ç”¨äºåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒå¢å¼ºã€‚</li>
<li>MAMBOæ¨¡å‹æœ‰åŠ©äºå®ç°å¼‚å¸¸åˆ†å‰²ï¼Œæé«˜ä¹³è…ºXå…‰æ‘„å½±çš„åˆ†æå‡†ç¡®æ€§ã€‚</li>
<li>MAMBOæ¨¡å‹çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b52a2b73dc42aaedcf1fefa978c2f585~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131139&auth_key=1760131139-0-0-eaa668b14d83a19f9dc44b548f424530&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bbf2698ec76c89ecc7491b79c522236b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131146&auth_key=1760131146-0-0-11c74f1fae4f1e2d22b05cc59cba0252&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d5daa69f586c4df8ea48185cc4cfd1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131152&auth_key=1760131152-0-0-e66f8f1343ab42553204d53643e94f4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd496a1d999ba6d25ca4845f3294c363~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131158&auth_key=1760131158-0-0-ba884bd5cc21ac6b5e1a8ae115c29499&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f6d6bdeb176598123b57ec1dc512611c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131165&auth_key=1760131165-0-0-0c1f3de9ed104e2664ae2e2fb2369c72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Feedback-Guidance-of-Diffusion-Models"><a href="#Feedback-Guidance-of-Diffusion-Models" class="headerlink" title="Feedback Guidance of Diffusion Models"></a>Feedback Guidance of Diffusion Models</h2><p><strong>Authors:Felix Koulischer, Florian Handke, Johannes Deleu, Thomas Demeester, Luca Ambrogioni</strong></p>
<p>While Classifier-Free Guidance (CFG) has become standard for improving sample fidelity in conditional diffusion models, it can harm diversity and induce memorization by applying constant guidance regardless of whether a particular sample needs correction. We propose FeedBack Guidance (FBG), which uses a state-dependent coefficient to self-regulate guidance amounts based on need. Our approach is derived from first principles by assuming the learned conditional distribution is linearly corrupted by the unconditional distribution, contrasting with CFGâ€™s implicit multiplicative assumption. Our scheme relies on feedback of its own predictions about the conditional signal informativeness to adapt guidance dynamically during inference, challenging the view of guidance as a fixed hyperparameter. The approach is benchmarked on ImageNet512x512, where it significantly outperforms Classifier-Free Guidance and is competitive to Limited Interval Guidance (LIG) while benefitting from a strong mathematical framework. On Text-To-Image generation, we demonstrate that, as anticipated, our approach automatically applies higher guidance scales for complex prompts than for simpler ones and that it can be easily combined with existing guidance schemes such as CFG or LIG. </p>
<blockquote>
<p>è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰å·²æˆä¸ºæ”¹å–„æ¡ä»¶æ‰©æ•£æ¨¡å‹æ ·æœ¬ä¿çœŸåº¦çš„æ ‡å‡†æ–¹æ³•ï¼Œä½†å®ƒå¯èƒ½ä¼šæŸå®³å¤šæ ·æ€§å¹¶å¯¼è‡´è®°å¿†å›ºåŒ–ï¼Œå› ä¸ºå®ƒä¼šä¸æ–­åº”ç”¨å¼•å¯¼ï¼Œè€Œä¸è€ƒè™‘ç‰¹å®šæ ·æœ¬æ˜¯å¦éœ€è¦æ ¡æ­£ã€‚æˆ‘ä»¬æå‡ºäº†åé¦ˆå¼•å¯¼ï¼ˆFBGï¼‰æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¾èµ–ç³»æ•°æ¥æ ¹æ®éœ€æ±‚è‡ªæˆ‘è°ƒèŠ‚å¼•å¯¼é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºåŸºæœ¬å‡è®¾ï¼Œå³å­¦ä¹ åˆ°çš„æ¡ä»¶åˆ†å¸ƒè¢«æ— æ¡ä»¶åˆ†å¸ƒçº¿æ€§è…èš€ï¼Œè¿™ä¸CFGçš„éšæ€§ä¹˜æ³•å‡è®¾å½¢æˆå¯¹æ¯”ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆä¾èµ–äºå¯¹å…¶å…³äºæ¡ä»¶ä¿¡å·ä¿¡æ¯æ€§çš„é¢„æµ‹åé¦ˆï¼Œä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‚åº”å¼•å¯¼ï¼Œä»è€ŒæŒ‘æˆ˜äº†å°†å¼•å¯¼è§†ä¸ºå›ºå®šè¶…å‚æ•°çš„è§‚ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ImageNet512x512ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œåœ¨é‚£é‡Œå®ƒæ˜¾è‘—ä¼˜äºæ— åˆ†ç±»å™¨å¼•å¯¼å¹¶å…·æœ‰ä¸æœ‰é™é—´éš”å¼•å¯¼ï¼ˆLIGï¼‰çš„ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶è¿˜å—ç›Šäºå¼ºå¤§çš„æ•°å­¦æ¡†æ¶ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è‡ªåŠ¨ä¸ºå¤æ‚æç¤ºåº”ç”¨æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹è€Œä¸æ˜¯ç®€å•æç¤ºï¼Œå¹¶ä¸”å®ƒå¯ä»¥è½»æ¾ä¸ç°æœ‰çš„å¼•å¯¼æ–¹æ¡ˆå¦‚CFGæˆ–LIGç»“åˆä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06085v2">PDF</a> Article accepeted as poster at the 39th Annual Conference on Neural   Information Processing Systems (NeurIPS25). Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/FelixKoulischer/FBG_using_edm2">https://github.com/FelixKoulischer/FBG_using_edm2</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åé¦ˆæŒ‡å¯¼ï¼ˆFBGï¼‰åœ¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æ”¹è¿›æ ·æœ¬ä¿çœŸåº¦å¹¶æé«˜å¤šæ ·æ€§ã€‚FBGä½¿ç”¨çŠ¶æ€ä¾èµ–ç³»æ•°æ¥æ ¹æ®éœ€æ±‚è‡ªæˆ‘è°ƒèŠ‚æŒ‡å¯¼é‡ï¼Œä¸Classifier-Free Guidanceï¼ˆCFGï¼‰ä¸åŒï¼ŒFBGé€šè¿‡å‡è®¾å­¦ä¹ çš„æ¡ä»¶åˆ†å¸ƒè¢«æ— æ¡ä»¶åˆ†å¸ƒçº¿æ€§ç ´åæ¥å®ç°ã€‚è¿™ç§æ–¹æ³•ä¾é è‡ªèº«å¯¹æ¡ä»¶ä¿¡å·é‡è¦æ€§çš„é¢„æµ‹åé¦ˆæ¥åŠ¨æ€è°ƒæ•´æŒ‡å¯¼è¿‡ç¨‹ï¼Œæ‰“ç ´äº†å°†æŒ‡å¯¼ä½œä¸ºå›ºå®šè¶…å‚æ•°çš„çœ‹æ³•ã€‚åœ¨ImageNetå’Œæ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä»»åŠ¡çš„æµ‹è¯•ä¸­ï¼ŒFBGè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¯è‡ªåŠ¨å¯¹å¤æ‚æç¤ºåº”ç”¨æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹ï¼Œå¹¶å¯è½»æ¾ä¸å…¶ä»–æŒ‡å¯¼æ–¹æ¡ˆç»“åˆä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FBGæ—¨åœ¨æ”¹è¿›æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚</li>
<li>FBGä½¿ç”¨çŠ¶æ€ä¾èµ–ç³»æ•°æ¥æ ¹æ®éœ€æ±‚è‡ªæˆ‘è°ƒèŠ‚æŒ‡å¯¼é‡ã€‚</li>
<li>ä¸CFGä¸åŒï¼ŒFBGåŸºäºå‡è®¾å­¦ä¹ çš„æ¡ä»¶åˆ†å¸ƒè¢«æ— æ¡ä»¶åˆ†å¸ƒçº¿æ€§ç ´åæ¥å®ç°ã€‚</li>
<li>FBGä¾é è‡ªèº«é¢„æµ‹åé¦ˆæ¥åŠ¨æ€è°ƒæ•´æŒ‡å¯¼è¿‡ç¨‹ã€‚</li>
<li>FBGåœ¨ImageNetä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºCFGï¼Œä¸Limited Interval Guidanceï¼ˆLIGï¼‰å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>FBGå¯è‡ªåŠ¨å¯¹å¤æ‚æç¤ºåº”ç”¨æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-eac83eb195c48aa568713f88c1018bba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131172&auth_key=1760131172-0-0-9147e37d12d71c852e8102878e090746&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-81330a33c319279c16e67cbbfdd35265~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131178&auth_key=1760131178-0-0-9c8d6d467de69b2afbb7d83886586976&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-81167319a243fc49bdb5caa0b0bfc2ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131185&auth_key=1760131185-0-0-3cb24ccf6c37c608d1862258c165aef3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="IMAGHarmony-Controllable-Image-Editing-with-Consistent-Object-Quantity-and-Layout"><a href="#IMAGHarmony-Controllable-Image-Editing-with-Consistent-Object-Quantity-and-Layout" class="headerlink" title="IMAGHarmony: Controllable Image Editing with Consistent Object Quantity   and Layout"></a>IMAGHarmony: Controllable Image Editing with Consistent Object Quantity   and Layout</h2><p><strong>Authors:Fei Shen, Yutong Gao, Jian Yu, Xiaoyu Du, Jinhui Tang</strong></p>
<p>Recent diffusion models have advanced image editing by improving fidelity and controllability across creative and personalized applications. However, multi-object scenes remain challenging, as reliable control over object categories, counts, and spatial layout is difficult to achieve. For that, we first study quantity and layout consistent image editing, abbreviated as QL-Edit, which targets control of object quantity and spatial layout in multi-object scenes. Then, we present IMAGHarmony, a straightforward framework featuring a plug-and-play harmony aware (HA) module that fuses perception semantics while modeling object counts and locations, resulting in accurate edits and strong structural consistency. We further observe that diffusion models are sensitive to the choice of initial noise and tend to prefer certain noise patterns. Based on this finding, we present a preference-guided noise selection (PNS) strategy that selects semantically aligned initial noise through vision and language matching, thereby further improving generation stability and layout consistency in multiple object editing. To support evaluation, we develop HarmonyBench, a comprehensive benchmark that covers a diverse range of quantity and layout control scenarios. Extensive experiments demonstrate that IMAGHarmony outperforms prior methods in both structural alignment and semantic accuracy, utilizing only 200 training images and 10.6M of trainable parameters. Code, models, and data are available at <a target="_blank" rel="noopener" href="https://github.com/muzishen/IMAGHarmony">https://github.com/muzishen/IMAGHarmony</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹é€šè¿‡æé«˜åˆ›æ„å’Œä¸ªæ€§åŒ–åº”ç”¨ç¨‹åºä¸­çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ï¼Œæ¨åŠ¨äº†å›¾åƒç¼–è¾‘çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤šç›®æ ‡åœºæ™¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ç°å¯¹ç›®æ ‡ç±»åˆ«ã€è®¡æ•°å’Œç©ºé—´å¸ƒå±€çš„å¯æ§æ€§ä»ç„¶å¾ˆéš¾åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆç ”ç©¶æ•°é‡ä¸å¸ƒå±€ä¸€è‡´çš„å›¾åƒç¼–è¾‘ï¼Œç®€ç§°ä¸ºQL-Editï¼Œæ—¨åœ¨æ§åˆ¶å¤šç›®æ ‡åœºæ™¯ä¸­çš„ç›®æ ‡æ•°é‡å’Œç©ºé—´å¸ƒå±€ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¨å‡ºäº†IMAGHarmonyæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç®€æ´ç›´è§‚ï¼Œå¹¶å¸¦æœ‰ä¸€ä¸ªéšæ’éšç”¨çš„å’Œè°æ„ŸçŸ¥ï¼ˆHAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å»ºæ¨¡å¯¹è±¡è®¡æ•°å’Œä½ç½®æ—¶èåˆäº†æ„ŸçŸ¥è¯­ä¹‰ï¼Œä»è€Œå®ç°ç²¾å‡†ç¼–è¾‘å’Œå¼ºå¤§çš„ç»“æ„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°æ‰©æ•£æ¨¡å‹å¯¹åˆå§‹å™ªå£°çš„é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œå¹¶å€¾å‘äºåå¥½æŸäº›å™ªå£°æ¨¡å¼ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åå¥½å¼•å¯¼å™ªå£°é€‰æ‹©ï¼ˆPNSï¼‰ç­–ç•¥ï¼Œé€šè¿‡è§†è§‰å’Œè¯­è¨€åŒ¹é…é€‰æ‹©è¯­ä¹‰å¯¹é½çš„åˆå§‹å™ªå£°ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜å¤šç›®æ ‡ç¼–è¾‘ä¸­çš„ç”Ÿæˆç¨³å®šæ€§å’Œå¸ƒå±€ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†HarmonyBenchç»¼åˆåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–å¤šç§æ•°é‡å’Œå¸ƒå±€æ§åˆ¶åœºæ™¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIMAGHarmonyåœ¨ç»“æ„å¯¹é½å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨200å¼ è®­ç»ƒå›¾åƒå’Œ106ä¸‡ä¸ªå¯è®­ç»ƒå‚æ•°ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/muzishen/IMAGHarmony">https://github.com/muzishen/IMAGHarmony</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01949v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ‰©æ•£æ¨¡å‹é€šè¿‡æé«˜å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ï¼Œæ¨åŠ¨äº†åˆ›æ„å’Œä¸ªæ€§åŒ–åº”ç”¨çš„å‘å±•ã€‚åœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç›®æ ‡æ•°é‡ä¸å¸ƒå±€ä¸€è‡´çš„å›¾åƒç¼–è¾‘ï¼ˆQL-Editï¼‰ï¼Œå¹¶æ¨å‡ºäº†IMAGHarmonyæ¡†æ¶ï¼Œå…·æœ‰å³æ’å³ç”¨çš„å’Œè°æ„ŸçŸ¥ï¼ˆHAï¼‰æ¨¡å—ï¼Œèƒ½èåˆæ„ŸçŸ¥è¯­ä¹‰åŒæ—¶å»ºæ¨¡å¯¹è±¡è®¡æ•°å’Œä½ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£æ¨¡å‹å¯¹åˆå§‹å™ªå£°çš„é€‰æ‹©å¾ˆæ•æ„Ÿï¼Œäºæ˜¯æå‡ºäº†åŸºäºè§†è§‰å’Œè¯­è¨€åŒ¹é…çš„åå¥½å¼•å¯¼å™ªå£°é€‰æ‹©ï¼ˆPNSï¼‰ç­–ç•¥ã€‚ä¸ºäº†æ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†HarmonyBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–å¤šç§æ•°é‡ä¸å¸ƒå±€æ§åˆ¶åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼ŒIMAGHarmonyåœ¨ç»“æ„å¯¹é½å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä»…ä½¿ç”¨200å¼ è®­ç»ƒå›¾åƒå’Œ10.6Må¯è®­ç»ƒå‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æé«˜äº†å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ï¼Œä¿ƒè¿›äº†åˆ›æ„å’Œä¸ªæ€§åŒ–åº”ç”¨çš„å‘å±•ã€‚</li>
<li>åœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­ï¼Œå®ç°å¯¹è±¡ç±»åˆ«ã€æ•°é‡å’Œç©ºé—´å¸ƒå±€çš„å¯æ§æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>QL-EditæŠ€æœ¯é’ˆå¯¹å¤šå¯¹è±¡åœºæ™¯ä¸­çš„å¯¹è±¡æ•°é‡å’Œç©ºé—´å¸ƒå±€æ§åˆ¶è¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>IMAGHarmonyæ¡†æ¶é€šè¿‡èåˆæ„ŸçŸ¥è¯­ä¹‰åŒæ—¶å»ºæ¨¡å¯¹è±¡è®¡æ•°å’Œä½ç½®ï¼Œæé«˜äº†å›¾åƒç¼–è¾‘çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯¹åˆå§‹å™ªå£°çš„é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œå› æ­¤æå‡ºäº†åå¥½å¼•å¯¼å™ªå£°é€‰æ‹©ï¼ˆPNSï¼‰ç­–ç•¥ã€‚</li>
<li>HarmonyBenchåŸºå‡†æµ‹è¯•å¹³å°ç”¨äºè¯„ä¼°å›¾åƒç¼–è¾‘çš„æ•ˆæœï¼Œæ¶µç›–å¤šç§æ•°é‡ä¸å¸ƒå±€æ§åˆ¶åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a77c2451299dbb360c8c9f802370563a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131192&auth_key=1760131192-0-0-ee10fb30988b89f92a4322192e3fdc2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27bf777f8aef1a4650ed75900e413881~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131199&auth_key=1760131199-0-0-0bdcfa68156cbfaf60b2140e7a4540af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb5fc08ed845e1152a3c332cc16411b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131206&auth_key=1760131206-0-0-f6144cbc894f76f105b3f063e71c3d67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8725e1d28fa620dd983c3ea149471126~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131213&auth_key=1760131213-0-0-c0b04d31e6562c5e81c3053d79e50b10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04b3c014e491b0254b1187cc422bad52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131220&auth_key=1760131220-0-0-1929cbe65ad530498781a717f33b8bd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DvD-Unleashing-a-Generative-Paradigm-for-Document-Dewarping-via-Coordinates-based-Diffusion-Model"><a href="#DvD-Unleashing-a-Generative-Paradigm-for-Document-Dewarping-via-Coordinates-based-Diffusion-Model" class="headerlink" title="DvD: Unleashing a Generative Paradigm for Document Dewarping via   Coordinates-based Diffusion Model"></a>DvD: Unleashing a Generative Paradigm for Document Dewarping via   Coordinates-based Diffusion Model</h2><p><strong>Authors:Weiguang Zhang, Huangcheng Lu, Maizhen Ning, Xiaowei Huang, Wei Wang, Kaizhu Huang, Qiufeng Wang</strong></p>
<p>Document dewarping aims to rectify deformations in photographic document images, thus improving text readability, which has attracted much attention and made great progress, but it is still challenging to preserve document structures. Given recent advances in diffusion models, it is natural for us to consider their potential applicability to document dewarping. However, it is far from straightforward to adopt diffusion models in document dewarping due to their unfaithful control on highly complex document images (e.g., 2000$times$3000 resolution). In this paper, we propose DvD, the first generative model to tackle document Dewarping via a Diffusion framework. To be specific, DvD introduces a coordinate-level denoising instead of typical pixel-level denoising, generating a mapping for deformation rectification. In addition, we further propose a time-variant condition refinement mechanism to enhance the preservation of document structures. In experiments, we find that current document dewarping benchmarks can not evaluate dewarping models comprehensively. To this end, we present AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark comprising 6,300 real image pairs across three distinct domains, enabling fine-grained evaluation of dewarping models. Comprehensive experiments demonstrate that our proposed DvD can achieve state-of-the-art performance with acceptable computational efficiency on multiple metrics across various benchmarks, including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/hanquansanren/DvD">https://github.com/hanquansanren/DvD</a>. </p>
<blockquote>
<p>æ–‡æ¡£å»æ‰­æ›²æ—¨åœ¨çº æ­£æ‘„å½±æ–‡æ¡£å›¾åƒä¸­çš„å˜å½¢ï¼Œä»è€Œæé«˜æ–‡æœ¬çš„å¯è¯»æ€§ï¼Œè¿™å·²å¼•èµ·äº†å¾ˆå¤šå…³æ³¨å¹¶å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œä½†åœ¨ä¿ç•™æ–‡æ¡£ç»“æ„æ–¹é¢ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è€ƒè™‘åˆ°æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼Œæˆ‘ä»¬è‡ªç„¶ä¼šè€ƒè™‘å®ƒä»¬åœ¨æ–‡æ¡£å»æ‰­æ›²æ–¹é¢çš„æ½œåœ¨é€‚ç”¨æ€§ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹å¯¹é«˜åº¦å¤æ‚çš„æ–‡æ¡£å›¾åƒï¼ˆä¾‹å¦‚2000x3000åˆ†è¾¨ç‡ï¼‰æ§åˆ¶ä¸å¿ å®ï¼Œå› æ­¤åœ¨æ–‡æ¡£å»æ‰­æ›²ä¸­é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¹¶ä¸ç®€å•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DvDï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡æ‰©æ•£æ¡†æ¶è§£å†³æ–‡æ¡£å»æ‰­æ›²çš„ç”Ÿæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒDvDå¼•å…¥äº†åæ ‡çº§å»å™ªï¼Œè€Œä¸æ˜¯å…¸å‹çš„åƒç´ çº§å»å™ªï¼Œä¸ºå˜å½¢æ ¡æ­£ç”Ÿæˆæ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ—¶é—´å¯å˜æ¡ä»¶ç»†åŒ–æœºåˆ¶ï¼Œä»¥æé«˜æ–‡æ¡£ç»“æ„çš„ä¿ç•™ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AnyPhotoDoc6300ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼è®¾è®¡çš„å¤§è§„æ¨¡æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„6300å¼ çœŸå®å›¾åƒå¯¹ï¼Œèƒ½å¤Ÿå¯¹å»æ‰­æ›²æ¨¡å‹è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„DvDå¯ä»¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°å“è¶Šçš„æ€§èƒ½å’Œå¯æ¥å—çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒ…æ‹¬DocUNetã€DIR300å’ŒAnyPhotoDoc6300ã€‚æ–°åŸºå‡†æµ‹è¯•å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanquansanren/DvD%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/hanquansanren/DvDä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21975v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ–‡æ¡£å›¾åƒå»æ‰­æ›²æŠ€æœ¯ï¼Œæ—¨åœ¨æ”¹å–„æ–‡æ¡£å›¾åƒçš„æ–‡æœ¬å¯è¯»æ€§ã€‚æ–‡ç« æå‡ºäº†DvDï¼Œé¦–ä¸ªé€šè¿‡æ‰©æ•£æ¨¡å‹è§£å†³æ–‡æ¡£å»æ‰­æ›²é—®é¢˜çš„ç”Ÿæˆæ¨¡å‹ã€‚DvDå¼•å…¥åæ ‡çº§åˆ«çš„å»å™ªï¼Œè€Œéå…¸å‹çš„åƒç´ çº§åˆ«å»å™ªï¼Œä¸ºå˜å½¢çŸ«æ­£ç”Ÿæˆæ˜ å°„ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºæ—¶é—´å¯å˜æ¡ä»¶ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™æ–‡æ¡£ç»“æ„ã€‚å®éªŒå‘ç°ç°æœ‰æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ï¼Œå› æ­¤æ¨å‡ºäº†AnyPhotoDoc6300ï¼Œä¸€ä¸ªä¸¥æ ¼è®¾è®¡çš„å¤§å‹æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«6300å¼ çœŸå®å›¾åƒå¯¹ï¼Œå¯ç²¾ç»†è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDvDæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—çš„è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æ¡£å›¾åƒå»æ‰­æ›²æŠ€æœ¯æ—¨åœ¨æé«˜æ–‡æ¡£å›¾åƒçš„æ–‡æœ¬å¯è¯»æ€§ï¼Œå·²å¼•èµ·å¹¿æ³›å…³æ³¨å¹¶å–å¾—é‡å¤§è¿›å±•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æ¡£å»æ‰­æ›²ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨é«˜åº¦å¤æ‚çš„æ–‡æ¡£å›¾åƒæ§åˆ¶ä¸Šä¸å¿ å®ã€‚</li>
<li>DvDæ¨¡å‹æ˜¯é¦–ä¸ªé€šè¿‡æ‰©æ•£æ¨¡å‹è§£å†³æ–‡æ¡£å»æ‰­æ›²é—®é¢˜çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>DvDå¼•å…¥åæ ‡çº§åˆ«å»å™ªï¼Œç”Ÿæˆå˜å½¢çŸ«æ­£æ˜ å°„ã€‚</li>
<li>æå‡ºæ—¶é—´å¯å˜æ¡ä»¶ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™æ–‡æ¡£ç»“æ„ã€‚</li>
<li>ç°æœ‰æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ï¼Œéœ€è¦æ–°çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>æ¨å‡ºAnyPhotoDoc6300å¤§å‹æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•ï¼Œå¯ç²¾ç»†è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ï¼ŒDvDæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å…ˆè¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-477c9baa5f2c2597ac87cfd7a77a1c65~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131228&auth_key=1760131228-0-0-7b44b24a464368cf803b0702f1f73030&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31eb40b2a1f146c5d074013b350e3368~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131235&auth_key=1760131235-0-0-697268a4542d69cdb8d4fa6244734ba9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c7c9c6307f963c31d890a84f8403162~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131241&auth_key=1760131241-0-0-584dd60439a599c102912a29facd8b49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4fc609ae24521a985a9fba8396a17f4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131248&auth_key=1760131248-0-0-557f94fc3f043e29a2954033c3d6508e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c2c3b2e0aacd7a8098cdab46e88bc4d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131255&auth_key=1760131255-0-0-5dc3b2c70f5bab85247135067dd22323&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82413075fb373dcf108447bd737f04cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131261&auth_key=1760131261-0-0-cda912bce490aa7c18413f64aed23be6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fabf195a3b892e02412deb0712084b33~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131268&auth_key=1760131268-0-0-fea5cb0ce75d29a64bfa6fa7a83df99a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c605ce365f3f3f4b5e31bd9ee5d67623~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131275&auth_key=1760131275-0-0-393a318406b4a3c6d113451f38fe4434&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Diffusion-Guided-Refinement-of-3D-Scenes"><a href="#Uncertainty-Aware-Diffusion-Guided-Refinement-of-3D-Scenes" class="headerlink" title="Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes"></a>Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes</h2><p><strong>Authors:Sarosij Bose, Arindam Dutta, Sayak Nag, Junge Zhang, Jiachen Li, Konstantinos Karydis, Amit K. Roy Chowdhury</strong></p>
<p>Reconstructing 3D scenes from a single image is a fundamentally ill-posed task due to the severely under-constrained nature of the problem. Consequently, when the scene is rendered from novel camera views, existing single image to 3D reconstruction methods render incoherent and blurry views. This problem is exacerbated when the unseen regions are far away from the input camera. In this work, we address these inherent limitations in existing single image-to-3D scene feedforward networks. To alleviate the poor performance due to insufficient information beyond the input imageâ€™s view, we leverage a strong generative prior in the form of a pre-trained latent video diffusion model, for iterative refinement of a coarse scene represented by optimizable Gaussian parameters. To ensure that the style and texture of the generated images align with that of the input image, we incorporate on-the-fly Fourier-style transfer between the generated images and the input image. Additionally, we design a semantic uncertainty quantification module that calculates the per-pixel entropy and yields uncertainty maps used to guide the refinement process from the most confident pixels while discarding the remaining highly uncertain ones. We conduct extensive experiments on real-world scene datasets, including in-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach can provide more realistic and high-fidelity novel view synthesis results compared to existing state-of-the-art methods. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒé‡å»º3Dåœºæ™¯æ˜¯ä¸€ä¸ªæ ¹æœ¬ä¸Šçš„ä¸é€‚å®šä»»åŠ¡ï¼Œå› ä¸ºè¿™ä¸ªé—®é¢˜å…·æœ‰ä¸¥é‡çš„çº¦æŸä¸è¶³çš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œå½“åœºæ™¯ä»æ–°è§†è§’å‘ˆç°æ—¶ï¼Œç°æœ‰çš„å•ä¸€å›¾åƒåˆ°3Dé‡å»ºæ–¹æ³•ä¼šäº§ç”Ÿä¸è¿è´¯å’Œæ¨¡ç³Šçš„è§†å›¾ã€‚å½“æœªè§çš„åŒºåŸŸè¿œç¦»è¾“å…¥ç›¸æœºæ—¶ï¼Œè¿™ä¸ªé—®é¢˜æ›´åŠ ä¸¥é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†ç°æœ‰å•ä¸€å›¾åƒåˆ°3Dåœºæ™¯å‰é¦ˆç½‘ç»œçš„è¿™äº›å›ºæœ‰å±€é™æ€§ã€‚ä¸ºäº†ç¼“è§£å› è¾“å…¥å›¾åƒè§†å›¾ä»¥å¤–çš„ä¿¡æ¯ä¸è¶³è€Œå¯¼è‡´çš„æ€§èƒ½ä¸ä½³ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºç”Ÿæˆå…ˆéªŒï¼Œå¯¹ç”±å¯ä¼˜åŒ–é«˜æ–¯å‚æ•°è¡¨ç¤ºçš„ç²—ç•¥åœºæ™¯è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å›¾åƒçš„é£æ ¼å’Œçº¹ç†ä¸è¾“å…¥å›¾åƒä¸€è‡´ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆçš„å›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´è¿›è¡Œäº†å®æ—¶çš„å‚…é‡Œå¶é£æ ¼è½¬æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—è®¡ç®—åƒç´ ç†µå¹¶äº§ç”Ÿä¸ç¡®å®šæ€§æ˜ å°„ï¼Œç”¨äºæŒ‡å¯¼ä»æœ€å…·ä¿¡å¿ƒçš„åƒç´ è¿›è¡Œç»†åŒ–è¿‡ç¨‹ï¼ŒåŒæ—¶ä¸¢å¼ƒå‰©ä½™çš„é«˜åº¦ä¸ç¡®å®šçš„åƒç´ ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬åŸŸå†…RealEstate-10Kå’ŒåŸŸå¤–KITTI-v2çš„çœŸå®åœºæ™¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æ›´çœŸå®ã€æ›´é«˜ä¿çœŸåº¦çš„æ–°è§†è§’åˆæˆç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15742v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡è§£å†³äº†ä»å•å¼ å›¾åƒé‡å»º3Dåœºæ™¯æ—¶é¢ä¸´çš„å›ºæœ‰å±€é™æ€§ï¼Œå¦‚æ¸²æŸ“ä¸æ¸…æ™°ã€è§†è§’ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚é€šè¿‡å¼•å…¥é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†åœºæ™¯çš„è¿­ä»£ä¼˜åŒ–ï¼›åˆ©ç”¨å‚…ç«‹å¶é£æ ¼è½¬ç§»æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸è¾“å…¥å›¾åƒçš„é£æ ¼å’Œçº¹ç†ä¸€è‡´ï¼›åŒæ—¶è®¾è®¡äº†è¯­ä¹‰ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œç”ŸæˆçœŸå®åœºæ™¯çš„ç†µå›¾æŒ‡å¯¼æœ€å‡†ç¡®çš„åƒç´ ç»†åŒ–è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œæ­¤æ–¹æ³•æ¯”ç°æœ‰æŠ€æœ¯æ›´ä¸ºçœŸå®å’Œé«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³äº†ä»å•å¼ å›¾åƒé‡å»ºçš„3Dåœºæ™¯å‡ºç°çš„æ¸²æŸ“ä¸æ¸…æ™°å’Œè§†è§’ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜åœºæ™¯é‡å»ºçš„è´¨é‡ã€‚</li>
<li>åˆ©ç”¨å‚…ç«‹å¶é£æ ¼è½¬ç§»æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸è¾“å…¥å›¾åƒé£æ ¼å’Œçº¹ç†çš„ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†è¯­ä¹‰ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œç”Ÿæˆç†µå›¾ä»¥æŒ‡å¯¼æœ€å‡†ç¡®çš„åƒç´ ç»†åŒ–è¿‡ç¨‹ã€‚</li>
<li>æ–¹æ³•å¯åº”ç”¨äºçœŸå®åœºæ™¯æ•°æ®é›†ï¼Œå¦‚RealEstate-10Kå’ŒKITTI-v2æ•°æ®é›†ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´çœŸå®ã€é«˜ä¿çœŸåº¦çš„è§†è§’åˆæˆç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-171238d12a2d509df9620413156fe8e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131282&auth_key=1760131282-0-0-e1da53a96dd7dfdd7cae35978b70ed32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34a15cb78edeb5c1122d11b1ccb2cce3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131290&auth_key=1760131290-0-0-31612d1d30150cad5b1d7a5b43ed37db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2311f415312f48487a969afc071af25a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131297&auth_key=1760131297-0-0-2361091f92f4b1a04d3b8e3b281379ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd368b1d5a58a53ea416a6badc61e49d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131303&auth_key=1760131303-0-0-2e2df656e147d022a934601361b29878&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DICEPTION-A-Generalist-Diffusion-Model-for-Visual-Perceptual-Tasks"><a href="#DICEPTION-A-Generalist-Diffusion-Model-for-Visual-Perceptual-Tasks" class="headerlink" title="DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks"></a>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</h2><p><strong>Authors:Canyu Zhao, Yanlong Sun, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, Chunhua Shen</strong></p>
<p>This paperâ€™s primary objective is to develop a robust generalist perception model capable of addressing multiple tasks under constraints of computational resources and limited training data. We leverage text-to-image diffusion models pre-trained on billions of images and successfully introduce our DICEPTION, a visual generalist model. Exhaustive evaluations demonstrate that DICEPTION effectively tackles diverse perception tasks, even achieving performance comparable to SOTA single-task specialist models. Specifically, we achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs.\ 1B pixel-level annotated images). We designed comprehensive experiments on architectures and input paradigms, demonstrating that the key to successfully re-purposing a single diffusion model for multiple perception tasks lies in maximizing the preservation of the pre-trained modelâ€™s prior knowledge. Consequently, DICEPTION can be trained with substantially lower computational costs than conventional models requiring training from scratch. Furthermore, adapting DICEPTION to novel tasks is highly efficient, necessitating fine-tuning on as few as 50 images and approximately 1% of its parameters. Finally, we demonstrate that a subtle application of classifier-free guidance can improve the modelâ€™s performance on depth and normal estimation. We also show that pixel-aligned training, as is characteristic of perception tasks, significantly enhances the modelâ€™s ability to preserve fine details. DICEPTION offers valuable insights and presents a promising direction for the development of advanced diffusion-based visual generalist models. Code and Model: <a target="_blank" rel="noopener" href="https://github.com/aim-uofa/Diception">https://github.com/aim-uofa/Diception</a> </p>
<blockquote>
<p>æœ¬æ–‡çš„ä¸»è¦ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªç¨³å¥çš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºæœ‰é™å’Œè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨åœ¨æ•°åäº¿å›¾åƒä¸Šé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶æˆåŠŸæ¨å‡ºäº†æˆ‘ä»¬çš„é€šç”¨è§†è§‰æ¨¡å‹DICEPTIONã€‚è¯¦å°½çš„è¯„ä¼°è¡¨æ˜ï¼ŒDICEPTIONèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§æ„ŸçŸ¥ä»»åŠ¡ï¼Œç”šè‡³å®ç°äº†ä¸æœ€æ–°å•ä»»åŠ¡ä¸“ä¸šæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨å…¶0.06%çš„æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œ600Kå¯¹æ¯”1Båƒç´ çº§æ³¨é‡Šå›¾åƒï¼‰å°±è¾¾åˆ°äº†ä¸SAM-vit-hç›¸å½“çš„ç»“æœã€‚æˆ‘ä»¬å¯¹æ¶æ„å’Œè¾“å…¥èŒƒå¼è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒæˆåŠŸåœ°å°†å•ä¸ªæ‰©æ•£æ¨¡å‹é‡æ–°ç”¨äºå¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡çš„å…³é”®åœ¨äºæœ€å¤§é™åº¦åœ°ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚å› æ­¤ï¼ŒDICEPTIONçš„è®¡ç®—æˆæœ¬è¿œä½äºéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒçš„å¸¸è§„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå°†DICEPTIONé€‚åº”æ–°ä»»åŠ¡éå¸¸é«˜æ•ˆï¼Œåªéœ€å¯¹å¤§çº¦50ä¸ªå›¾åƒè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”åªéœ€è¦å…¶å¤§çº¦1%çš„å‚æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼çš„ç²¾å¦™åº”ç”¨å¯ä»¥æé«˜æ¨¡å‹åœ¨æ·±åº¦å’Œæ­£å¸¸ä¼°è®¡æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ„ŸçŸ¥ä»»åŠ¡ç‰¹æœ‰çš„åƒç´ å¯¹é½è®­ç»ƒæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„ç»†èŠ‚ä¿ç•™èƒ½åŠ›ã€‚DICEPTIONæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºåŸºäºæ‰©æ•£çš„é«˜çº§è§†è§‰é€šç”¨æ¨¡å‹çš„å‘å±•æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/aim-uofa/Diception">https://github.com/aim-uofa/Diception</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17157v3">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Diception">https://aim-uofa.github.io/Diception</a>, Code and Model:   <a target="_blank" rel="noopener" href="https://github.com/aim-uofa/Diception">https://github.com/aim-uofa/Diception</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªç¨³å¥çš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºæœ‰é™å’Œè®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹é¢„è®­ç»ƒäºæ•°åäº¿å›¾åƒæ•°æ®ï¼ŒæˆåŠŸæ¨å‡ºè§†è§‰é€šç”¨æ¨¡å‹DICEPTIONã€‚è¯„ä¼°è¡¨æ˜ï¼ŒDICEPTIONèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤šæ ·åŒ–çš„æ„ŸçŸ¥ä»»åŠ¡ï¼Œç”šè‡³è¾¾åˆ°å•ä»»åŠ¡ä¸“ä¸šæ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚åœ¨ä»…ä½¿ç”¨0.06%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå…¶æ•ˆæœä¸SAM-vit-hç›¸å½“ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨600K vs. 1Båƒç´ çº§æ ‡æ³¨å›¾åƒï¼‰ã€‚å®éªŒè¯æ˜ï¼ŒæˆåŠŸå°†ä¸€ä¸ªå•ä¸€çš„æ‰©æ•£æ¨¡å‹ç”¨äºå¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡çš„å…³é”®åœ¨äºæœ€å¤§é™åº¦åœ°ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚å› æ­¤ï¼ŒDICEPTIONçš„è®¡ç®—æˆæœ¬è¿œä½äºä¼ ç»Ÿæ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€‚åº”DICEPTIONè¿›è¡Œæ–°ä»»åŠ¡éå¸¸é«˜æ•ˆï¼Œä»…éœ€å¾®è°ƒå°‘é‡å›¾åƒå’Œçº¦1%çš„å‚æ•°ã€‚æœ€åï¼Œç ”ç©¶è¯æ˜é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼å¯æå‡æ¨¡å‹åœ¨æ·±åº¦å’Œæ­£å¸¸ä¼°è®¡æ–¹é¢çš„æ€§èƒ½ã€‚åƒç´ å¯¹é½è®­ç»ƒä¹Ÿæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ç»†èŠ‚çš„ä¿ç•™èƒ½åŠ›ã€‚DICEPTIONä¸ºæ‰©æ•£å¼è§†è§‰é€šç”¨æ¨¡å‹çš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œå‰æ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªèƒ½åœ¨è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®é™åˆ¶ä¸‹å¤„ç†å¤šä¸ªä»»åŠ¡çš„ç¨³å¥é€šç”¨æ„ŸçŸ¥æ¨¡å‹ã€‚</li>
<li>å¼•å…¥è§†è§‰é€šç”¨æ¨¡å‹DICEPTIONï¼Œé€šè¿‡åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹é¢„è®­ç»ƒäºå¤§é‡å›¾åƒæ•°æ®ã€‚</li>
<li>DICEPTIONèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„æ„ŸçŸ¥ä»»åŠ¡ï¼Œæ€§èƒ½ä¸å…ˆè¿›çš„ä¸“ä¸šæ¨¡å‹ç›¸å½“ã€‚</li>
<li>ä½¿ç”¨æå°‘é‡æ•°æ®ï¼ˆä»…0.06%ï¼‰å³å¯è¾¾åˆ°ä¸SAM-vit-hç›¸å½“çš„æ•ˆæœã€‚</li>
<li>æˆåŠŸå°†å•ä¸€æ‰©æ•£æ¨¡å‹ç”¨äºå¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡çš„å…³é”®åœ¨äºä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>DICEPTIONçš„è®¡ç®—æˆæœ¬è¿œä½äºä¼ ç»Ÿæ¨¡å‹ï¼Œé€‚åº”æ–°ä»»åŠ¡æ›´åŠ é«˜æ•ˆã€‚</li>
<li>é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼å’Œåƒç´ å¯¹é½è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹åœ¨æ·±åº¦å’Œæ­£å¸¸ä¼°è®¡ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å¢å¼ºäº†ç»†èŠ‚ä¿ç•™èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dc65fdb07455bd0cd939dd3c7bd174d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131310&auth_key=1760131310-0-0-ed7b1512ebb0040a9136e03995fe88a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ec0f185556e3cf808bfc5ffed169d10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131319&auth_key=1760131319-0-0-ba4e058d67d27a8ce27f65a9aa58f65c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d01a38c0c1d8e5720df849816a308fab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131325&auth_key=1760131325-0-0-cae11196ff7b96f2e8e22a886bc2c0c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="The-Poisson-Midpoint-Method-for-Langevin-Dynamics-Provably-Efficient-Discretization-for-Diffusion-Models"><a href="#The-Poisson-Midpoint-Method-for-Langevin-Dynamics-Provably-Efficient-Discretization-for-Diffusion-Models" class="headerlink" title="The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient   Discretization for Diffusion Models"></a>The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient   Discretization for Diffusion Models</h2><p><strong>Authors:Saravanan Kandasamy, Dheeraj Nagaraj</strong></p>
<p>Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization. Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm. LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples. This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps. Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions. However, important applications such as diffusion models involve non-log concave densities and contain time varying drift. We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes. We prove that this can obtain a quadratic speed up of LMC under very weak assumptions. We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 1000 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute. </p>
<blockquote>
<p>æœ—ç»´åŠ¨åŠ›å­¦æ˜¯é‡‡æ ·å’Œç”Ÿæˆå»ºæ¨¡ä¸­çš„æ ¸å¿ƒéšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰ï¼Œé€šè¿‡æ—¶é—´ç¦»æ•£åŒ–å®ç°ã€‚åŸºäºæ¬§æ‹‰-ç›é²äºšé©¬ç¦»æ•£çš„æœ—ç»´è’™ç‰¹å¡æ´›ï¼ˆLMCï¼‰æ˜¯æœ€ç®€å•ä¸”ç ”ç©¶æœ€å¤šçš„ç®—æ³•ã€‚LMCå­˜åœ¨æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ï¼Œéœ€è¦å¤§é‡çš„å°æ­¥é•¿æ­¥éª¤æ‰èƒ½è·å¾—é«˜è´¨é‡çš„æ ·æœ¬ã€‚åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œè¿™ç§æƒ…å†µæ›´åŠ æ˜æ˜¾ï¼Œå¤§é‡çš„æ­¥éª¤èƒ½ç»™å‡ºæœ€å¥½çš„æ ·æœ¬ï¼Œä½†æ­¥éª¤æ•°é‡å‡å°‘æ—¶ï¼Œè´¨é‡è¿…é€Ÿä¸‹é™ã€‚æœ€è¿‘æå‡ºäº†éšæœºä¸­ç‚¹æ³•ä½œä¸ºæœ—ç»´åŠ¨åŠ›å­¦çš„æ›´å¥½ç¦»æ•£åŒ–æ–¹æ³•ï¼Œç”¨äºä»å¼ºå¯¹æ•°å‡¹åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚ç„¶è€Œï¼Œé‡è¦çš„åº”ç”¨ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰æ¶‰åŠéå¯¹æ•°å‡¹å¯†åº¦å’Œéšæ—¶é—´å˜åŒ–çš„æ¼‚ç§»ã€‚æˆ‘ä»¬æå‡ºäº†å®ƒçš„å˜ä½“â€”â€”æ³Šæ¾ä¸­ç‚¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨å¤§æ­¥é•¿è¿‘ä¼¼å°æ­¥é•¿çš„LMCã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å¾ˆå¼±çš„å‡è®¾ä¸‹ï¼Œè¿™å¯ä»¥è·å¾—LMCçš„äºŒæ¬¡åŠ é€Ÿã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºå›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¡¨æ˜åœ¨ä»…ä½¿ç”¨50-80ä¸ªç¥ç»ç½‘ç»œè°ƒç”¨çš„æƒ…å†µä¸‹ï¼Œå®ƒèƒ½åœ¨ä¿æŒDDPMè´¨é‡çš„åŒæ—¶ï¼Œå°†ç¥ç»ç½‘ç»œè°ƒç”¨æ¬¡æ•°ä»1000æ¬¡å‡å°‘åˆ°åªéœ€å‡ åæ¬¡ï¼Œå¹¶ä¸”ä¼˜äºç±»ä¼¼çš„è®¡ç®—é‡çš„åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ç»„çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17068v3">PDF</a> â€œOne often meets his destiny on the road he takes to avoid itâ€ -   Master Oogway. My destiny seems to be to write triangle inequalities for the   rest of my life</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLangevinåŠ¨åŠ›å­¦éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„é‡‡æ ·å’Œç”Ÿæˆæ¨¡å‹ä¸­çš„å…³é”®è¦ç´ ï¼Œåˆ©ç”¨æ—¶é—´ç¦»æ•£åŒ–å®æ–½ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”Poissonä¸­ç‚¹æ³•ï¼Œç”¨äºè¿‘ä¼¼å°æ­¥é•¿çš„Langevin Monte Carloæ–¹æ³•ï¼Œå¹¶å¯åœ¨å¤§å‹æ­¥é•¿ä¸‹å®ç°äºŒæ¬¡åŠ é€Ÿã€‚åœ¨å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ä¸­åº”ç”¨æ­¤æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†ç¥ç»ç½‘ç»œè°ƒç”¨çš„æ•°é‡ï¼Œå¹¶ä¿æŒé«˜è´¨é‡çš„ç”Ÿæˆç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Langevin Dynamicsæ˜¯ä¸€ç§åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„é‡‡æ ·å’Œç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒæ–¹æ³•ï¼Œé€šè¿‡æ—¶é—´ç¦»æ•£åŒ–å®ç°ã€‚</li>
<li>Langevin Monte Carloï¼ˆLMCï¼‰è™½ç„¶æ˜¯æœ€ç®€å•ä¸”è¢«å¹¿æ³›ç ”ç©¶çš„æ–¹æ³•ï¼Œä½†å…¶æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­éœ€è¦å¤§é‡æ­¥éª¤æ‰èƒ½è·å¾—é«˜è´¨é‡çš„æ ·æœ¬ã€‚</li>
<li>å¯¹äºå¼ºå¯¹æ•°å‡¹åˆ†å¸ƒï¼Œé‡‡ç”¨éšæœºä¸­ç‚¹æ³•ä½œä¸ºå¯¹LangevinåŠ¨åŠ›å­¦çš„æ›´å¥½ç¦»æ•£åŒ–æ–¹æ³•ã€‚</li>
<li>ç„¶è€Œï¼Œé‡è¦åº”ç”¨å¦‚æ‰©æ•£æ¨¡å‹æ¶‰åŠéå¯¹æ•°å‡¹å¯†åº¦å’Œéšæ—¶é—´å˜åŒ–çš„æ¼‚ç§»ï¼Œæå‡ºäº†Poissonä¸­ç‚¹æ–¹æ³•ä½œä¸ºè¿‘ä¼¼å°æ­¥é•¿LMCçš„æ–°æ‰‹æ®µï¼Œèƒ½åœ¨å¤§å‹æ­¥é•¿ä¸‹å·¥ä½œã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥å®ç°åœ¨éå¸¸å¼±çš„å‡è®¾ä¸‹LMCçš„äºŒæ¬¡åŠ é€Ÿæ•ˆæœã€‚</li>
<li>åœ¨å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ä¸­åº”ç”¨Poissonä¸­ç‚¹æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†ç¥ç»ç½‘ç»œè°ƒç”¨çš„æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e829c9c97dc4b026fe47721974231944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131332&auth_key=1760131332-0-0-ebb12953102daa0944d19cbf07e5c082&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c822cd7ea5dc7f3430946ce2f7ffd24~resize:0:q75.jpg?source=1f5c5e47&expiration=1760131340&auth_key=1760131340-0-0-d1e8ca1420b7c2da19b1a04e60b49419&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-96b0b56ffe742bd505738bdd8e536a8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760132053&auth_key=1760132053-0-0-f5e21e90ce190179da897f7d41423e8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  Biology-driven assessment of deep learning super-resolution imaging of   the porosity network in dentin
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c43c7abf8a697134a190d75c5547494c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760129174&auth_key=1760129174-0-0-03e9af97ac704a03e8ad6969742f68c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  Splat the Net Radiance Fields with Splattable Neural Primitives
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31180k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
