<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-10-11  DialoSpeech Dual-Speaker Dialogue Generation with LLM and Flow Matching">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-33686396df5a918b34909ee013ebfbd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135725&auth_key=1760135725-0-0-eb6e51862bc1f8cf44096ab0ae86febc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-11-更新"><a href="#2025-10-11-更新" class="headerlink" title="2025-10-11 更新"></a>2025-10-11 更新</h1><h2 id="DialoSpeech-Dual-Speaker-Dialogue-Generation-with-LLM-and-Flow-Matching"><a href="#DialoSpeech-Dual-Speaker-Dialogue-Generation-with-LLM-and-Flow-Matching" class="headerlink" title="DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching"></a>DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching</h2><p><strong>Authors:Hanke Xie, Dake Guo, Chengyou Wang, Yue Li, Wenjie Tian, Xinfa Zhu, Xinsheng Wang, Xiulin Li, Guanqiong Miao, Bo Liu, Lei Xie</strong></p>
<p>Recent advances in text-to-speech (TTS) synthesis, particularly those leveraging large language models (LLMs), have significantly improved expressiveness and naturalness. However, generating human-like, interactive dialogue speech remains challenging. Current systems face limitations due to the scarcity of dual-track data and difficulties in achieving naturalness, contextual coherence, and interactional dynamics, such as turn-taking, overlapping speech, and speaker consistency, in multi-turn conversations. To address these challenges, we propose DialoSpeech, a dual-track architecture combining a large language model with Chunked Flow Matching for expressive, human-like dialogue speech synthesis. DialoSpeech generates natural multi-turn conversations with coherent speaker turns and natural overlaps, supporting both Chinese and English and cross-lingual speech synthesis. We introduce a data processing pipeline to construct dual-track dialogue datasets, facilitating scalable training and experimental validation. Experiments show that our model outperforms baselines, offering a solution for generating human-like spoken dialogues. Audio samples are available at <a target="_blank" rel="noopener" href="https://tiamojames.github.io/DialoSpeech">https://tiamojames.github.io/DialoSpeech</a> </p>
<blockquote>
<p>文本转语音（TTS）合成的最新进展，特别是那些利用大型语言模型（LLM）的技术，显著提高了表达能力和自然度。然而，生成类似人类的交互式对话语音仍然具有挑战性。当前的系统面临着双重轨道数据稀缺和难以实现自然性、上下文连贯性和交互动态性的困难，例如在多轮对话中的轮流发言、重叠语音和说话人一致性。为了解决这些挑战，我们提出了DialoSpeech，这是一种结合大型语言模型和Chunked Flow Matching的双轨架构，用于表达性、类似人类的对话语音合成。DialoSpeech生成了具有连贯的发言人和自然的重叠的自然多轮对话，支持中文和英文以及跨语言语音合成。我们引入了一个数据处理管道来构建双轨对话数据集，便于可扩展的训练和实验验证。实验表明，我们的模型优于基线模型，为解决类似人类口语对话的生成提供了解决方案。音频样本可在<a target="_blank" rel="noopener" href="https://tiamojames.github.io/DialoSpeech%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://tiamojames.github.io/DialoSpeech上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08373v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期文本转语音（TTS）合成技术的进展，特别是利用大型语言模型（LLM）的技术，显著提高了表达的自然度。然而，生成人类般的、互动的对话语音仍具挑战。当前系统面临多轨对话数据的稀缺性，以及在实现自然性、上下文连贯性和交互动态性方面的困难，如话轮转换、重叠语音和说话人一致性等。为解决这些挑战，我们提出了DialoSpeech，这是一种结合大型语言模型和Chunked Flow Matching的双轨架构，用于表达人类般的对话语音合成。DialoSpeech可以生成自然的多轮对话，支持中文和英文以及跨语言语音合成。我们引入了一条数据处理管道来构建双轨对话数据集，便于可扩展的训练和实验验证。实验表明，我们的模型优于基线模型，为解决生成人类般的对话语音提供了解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS合成技术借助大型语言模型（LLM）显著提高了表达的自然度和生动性。</li>
<li>生成人类般的互动对话语音存在挑战，尤其在实现多轮对话的自然性、连贯性和交互动态性方面。</li>
<li>DialoSpeech是一个双轨架构，结合了大型语言模型和Chunked Flow Matching技术，旨在解决这些挑战。</li>
<li>DialoSpeech支持中文和英文以及跨语言的语音合成。</li>
<li>提出的数据处理管道有助于构建双轨对话数据集，促进了模型的可扩展训练和实验验证。</li>
<li>实验显示DialoSpeech模型性能优于基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d16b99f2a10567363070c07df11c38ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135493&auth_key=1760135493-0-0-98100116b2d0e2e59c6fd21aaad6c182&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f96287b8f80266aa56e315022fa5b5b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135500&auth_key=1760135500-0-0-fa674ee4864fc85a2d4821dad77c9c1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e96a86514d10e12f26c4ecd9b41a302a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135506&auth_key=1760135506-0-0-985e379528f5d0f67b13567a1d862591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-913572ca9d8424c8442a1dafffd4ca9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135513&auth_key=1760135513-0-0-54c31c14fd847487876731d6c414bc9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad6e182cd24e57d6a679b2e8385bc5e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135519&auth_key=1760135519-0-0-5d482cb3590ad777c4a6a38a0b26acec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3649b9a4c5def1a29e66b8deb42ab5e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135526&auth_key=1760135526-0-0-442c635b11451e47fa4bd7eef1ce713f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f51effa4304222a5e70cb0b39782e8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135532&auth_key=1760135532-0-0-ef1e6580c0a96bf25a3d634d96927686&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="IntMeanFlow-Few-step-Speech-Generation-with-Integral-Velocity-Distillation"><a href="#IntMeanFlow-Few-step-Speech-Generation-with-Integral-Velocity-Distillation" class="headerlink" title="IntMeanFlow: Few-step Speech Generation with Integral Velocity   Distillation"></a>IntMeanFlow: Few-step Speech Generation with Integral Velocity   Distillation</h2><p><strong>Authors:Wei Wang, Rong Cao, Yi Guo, Zhengyang Chen, Kuan Chen, Yuanyuan Huo</strong></p>
<p>Flow-based generative models have greatly improved text-to-speech (TTS) synthesis quality, but inference speed remains limited by the iterative sampling process and multiple function evaluations (NFE). The recent MeanFlow model accelerates generation by modeling average velocity instead of instantaneous velocity. However, its direct application to TTS encounters challenges, including GPU memory overhead from Jacobian-vector products (JVP) and training instability due to self-bootstrap processes. To address these issues, we introduce IntMeanFlow, a framework for few-step speech generation with integral velocity distillation. By approximating average velocity with the teacher’s instantaneous velocity over a temporal interval, IntMeanFlow eliminates the need for JVPs and self-bootstrap, improving stability and reducing GPU memory usage. We also propose the Optimal Step Sampling Search (O3S) algorithm, which identifies the model-specific optimal sampling steps, improving speech synthesis without additional inference overhead. Experiments show that IntMeanFlow achieves 1-NFE inference for token-to-spectrogram and 3-NFE for text-to-spectrogram tasks while maintaining high-quality synthesis. Demo samples are available at <a target="_blank" rel="noopener" href="https://vvwangvv.github.io/intmeanflow">https://vvwangvv.github.io/intmeanflow</a>. </p>
<blockquote>
<p>基于流的生成模型极大地提高了文本到语音（TTS）合成的质量，但推理速度仍然受到迭代采样过程和多次函数评估（NFE）的限制。最近的MeanFlow模型通过建模平均速度而不是瞬时速度来加速生成。然而，它直接应用于TTS时面临挑战，包括由雅可比向量积（JVP）导致的GPU内存开销和由于自举过程导致的训练不稳定。为了解决这些问题，我们引入了IntMeanFlow，这是一个具有积分速度蒸馏功能的几步语音生成框架。通过用教师在时间间隔内的瞬时速度来近似平均速度，IntMeanFlow消除了对JVP和自举的需要，提高了稳定性并减少了GPU内存使用。我们还提出了最佳步采样搜索（O3S）算法，该算法可以识别模型特定的最佳采样步骤，在不需要额外的推理开销的情况下提高语音合成质量。实验表明，IntMeanFlow在令牌到光谱图任务中实现1-NFE推理，在文本到光谱图任务中实现3-NFE推理，同时保持高质量合成。演示样本可在<a target="_blank" rel="noopener" href="https://vvwangvv.github.io/intmeanflow%E6%9F%A5%E7%9C%8B%E3%80%82">https://vvwangvv.github.io/intmeanflow查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07979v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于流生成模型的文本转语音（TTS）合成质量显著提高，但推理速度受限于迭代采样过程和多次函数评估（NFE）。最近提出的MeanFlow模型通过建模平均速度来加速生成，但直接应用于TTS面临挑战，如GPU内存开销大的雅可比向量积（JVP）和自举过程导致的训练不稳定。为解决这些问题，我们推出IntMeanFlow框架，通过用教师的瞬时速度近似平均速度进行少量步骤的语音生成。IntMeanFlow消除了对JVP和自举的需要，提高了稳定性和GPU内存使用效率。我们还提出了最优步采样搜索（O3S）算法，该算法能识别模型特定的最优采样步骤，在不影响合成质量的同时提高语音合成的推理速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流生成模型在TTS中虽能提高合成质量，但推理速度受限。</li>
<li>MeanFlow模型通过建模平均速度加速生成，但直接应用于TTS存在挑战。</li>
<li>IntMeanFlow框架通过教师的瞬时速度近似平均速度进行少量步骤的语音生成，解决GPU内存开销和训练不稳定问题。</li>
<li>IntMeanFlow消除了对雅可比向量积和自举的需要。</li>
<li>最优步采样搜索（O3S）算法能识别模型特定的最优采样步骤，提高语音合成的推理速度。</li>
<li>IntMeanFlow在token-to-spectrogram任务中实现1-NFE推理，在text-to-spectrogram任务中实现3-NFE。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-49cf954c30b027c377cba248e70618c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135540&auth_key=1760135540-0-0-2ac61ed15fb86bbd94acb2fed7eb0fc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdb5393f78d89efa67af2fcde9d311b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135547&auth_key=1760135547-0-0-f5e424cdc33c626990e1d3fd8213fcf2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d60562b1b097eece05a5505a8714a93~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135554&auth_key=1760135554-0-0-a61c998ee481e23fd347f332e1cefd2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d33a33fd28fd72a443490da2b9db7df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135561&auth_key=1760135561-0-0-e6ae9ee8e87dc145865b1c7fb2e4cb4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00d5bb361fb122546bbf0ad1a3e873ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135568&auth_key=1760135568-0-0-9cd59a019305f62b03f8c05266cd1a63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models"><a href="#Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models" class="headerlink" title="Parallel Test-Time Scaling for Latent Reasoning Models"></a>Parallel Test-Time Scaling for Latent Reasoning Models</h2><p><strong>Authors:Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li</strong></p>
<p>Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at <a target="_blank" rel="noopener" href="https://github.com/YRYangang/LatentTTS">https://github.com/YRYangang/LatentTTS</a>. </p>
<blockquote>
<p>并行测试时间缩放（TTS）是增强大型语言模型（LLM）的关键方法，通常通过并行采样多个基于标记的思维链，并通过投票或搜索聚合结果来实现。最近，潜在推理的进展，其中中间推理在连续向量空间中展开，为明确的思维链提供了更有效的替代方案。然而，潜在模型是否也能从并行TTS中受益仍然是一个悬而未决的问题，这主要是因为连续空间中缺少采样机制，以及缺乏用于高级轨迹聚合的概率信号。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07745v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了一种将平行测试时间缩放（Parallel Test-Time Scaling，简称TTS）应用于潜在推理模型的方法。针对采样和聚合问题，引入两种不确定性启发策略进行采样，并利用Latent Reward Model进行轨迹评分和引导。实验和分析显示，这些策略可有效扩展计算，同时LatentRM可实现有效的轨迹选择。为连续空间的可扩展推理开辟了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>平行测试时间缩放（TTS）用于增强大型语言模型（LLMs）。</li>
<li>通过采样多个基于token的并行思维链并投票或搜索聚合结果。</li>
<li>潜在推理模型中的连续向量空间展开提供了更高效的替代方案。</li>
<li>引入两种不确定性启发策略采样方法：Monte Carlo Dropout和Additive Gaussian Noise。</li>
<li>设计Latent Reward Model（LatentRM）进行轨迹评分和引导，使用逐步对比目标进行训练。</li>
<li>实验和可视化分析表明，采样策略在算力上有效扩展，展现出不同的探索动态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-faf8993114877c568f0a810fee81963c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135576&auth_key=1760135576-0-0-9b01ca47b0b914e5978e1b3a6f82a475&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfd7fee28378e1dd31e20d8577a56d9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135583&auth_key=1760135583-0-0-ba3caba48566ea8b565d6a58f3b809ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-451d80afbbbb115dfc019f5b794490d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135591&auth_key=1760135591-0-0-52a86fe81f0d7a633cc160d292c71d93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AsyncSpade-Efficient-Test-Time-Scaling-with-Asynchronous-Sparse-Decoding"><a href="#AsyncSpade-Efficient-Test-Time-Scaling-with-Asynchronous-Sparse-Decoding" class="headerlink" title="AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse   Decoding"></a>AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse   Decoding</h2><p><strong>Authors:Shuqing Luo, Yilin Guan, Pingzhi Li, Hanrui Wang, Tianlong Chen</strong></p>
<p>Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT), but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM decoding. Query-aware page-level sparse decoding can achieve state-of-the-art performance under constrained FLOPs budgets, but is limited by both sequential-dependent page filtering and coarse-grained token selection, hampering serving efficiency and model performance on TTS tasks under high concurrency and long CoT scenarios (consuming even higher runtime than the forward pipeline itself). In this paper, we first find that the current-step query state can be accurately approximated in a unified manner from a short window of recent queries, enabling training-free query-aware sparsity without waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework for efficient TTS built on two core components: (1) a novel light-weight temporal-regressive module that predicts the next-token query state; (2) an asynchronous and disaggregated framework that decouples the KV cache filtering from the auto-regressive decoding loop, overlapping the token-level KV selection with the forward inference computation through asynchronism. To our knowledge, AsyncSpade is the first to eliminate the sequential dependence without sacrificing model performance. We validate the effectiveness of AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade fully overlaps KV-cache operations with the inference pipeline, achieving theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and at least 50% TPOT reduction compared to full attention on Qwen3-8B and Qwen3-32B models, while matching or surpassing their accuracy on various TTS benchmarks (AIME-24&#x2F;25, GPQA-Diamond, MATH-500). </p>
<blockquote>
<p>测试时缩放（TTS）通过长思考链（CoT）提升大型语言模型（LLM）的推理能力，但线性KV缓存增长放大了大型语言模型解码的内存瓶颈。查询感知的页面级稀疏解码可以在有限的浮点运算预算下实现最先进的性能，但受限于顺序依赖的页面过滤和粗粒度令牌选择，这阻碍了在高并发和长思考链场景下的TTS任务的服务效率和模型性能（甚至消耗比前向管道更高的运行时间）。在本文中，我们首先发现当前步骤的查询状态可以以统一的方式从最近的查询的短窗口中准确近似，从而在解码循环中无需等待即可实现训练感知的查询感知稀疏性。我们提出了AsyncSpade，这是一个高效的TTS异步框架，它包括两个核心组件：（1）一种新型轻量级的时间回归模块，用于预测下一个令牌查询状态；（2）一个异步和分散的框架，它将KV缓存过滤与自回归解码循环解耦，通过异步性将令牌级别的KV选择与前向推理计算重叠。据我所知，AsyncSpade是第一个在不牺牲模型性能的情况下消除了顺序依赖性的方法。我们在使用A100节点的常见大型语言模型服务环境中验证了AsyncSpade的有效性，在该环境中，AsyncSpade完全将KV缓存操作与推理管道重叠，实现了理论上的最佳每输出令牌时间（TPOT）。具体来说，AsyncSpade相对于最新技术基准（例如Quest）减少了超过20%的TPOT，并且在Qwen3-8B和Qwen3-32B模型上至少减少了50%的TPOT，同时在各种TTS基准测试中达到了与其相当或更高的准确率（AIME-24&#x2F;25、GPQA-Diamond、MATH-500）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07486v1">PDF</a> 14 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>该文本探讨了测试时缩放（TTS）如何通过长链思维（CoT）增强大型语言模型（LLM）的推理能力，但同时也面临着内存瓶颈问题。文章提出了一种新的异步框架AsyncSpade，通过两个核心组件解决现有问题：一是轻量级的时序回归模块，用于预测下一个令牌查询状态；二是异步和分散的框架，将KV缓存过滤与自动回归解码循环解耦，通过异步性将令牌级KV选择与前向推理计算重叠。AsyncSpade消除了顺序依赖性，同时不牺牲模型性能，并在常见LLM服务设置上进行了验证，实现了理论上的最佳每输出令牌时间（TPOT）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时缩放（TTS）增强了大型语言模型（LLM）的推理能力，通过长链思维（CoT）。</li>
<li>线性KV-cache增长放大了LLM解码的内存限制瓶颈。</li>
<li>查询感知的页面级稀疏解码在受限的FLOPs预算下实现了先进性能，但受限于顺序依赖的页面过滤和粗粒度的令牌选择。</li>
<li>当前步骤的查询状态可以准确地从近期的短查询窗口中近似，实现了训练免费的查询感知稀疏性，无需等待解码循环。</li>
<li>AsyncSpade是一个新的异步框架，旨在解决TTS的效率问题，包括两个核心组件：轻量级时序回归模块和异步分散框架。</li>
<li>AsyncSpade消除了顺序依赖性，实现了理论上的最佳每输出令牌时间（TPOT），在某些模型上实现了超过20%的TPOT减少。</li>
<li>AsyncSpade在多种TTS基准测试上达到了或超越了现有模型的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07486">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-92e82ea0b6685d46e1a92b2c8ca160ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135598&auth_key=1760135598-0-0-9be60045426f3ba8e2baf78dc3a1d7ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5388c924bb4ea87287c31fdbd831ee0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135605&auth_key=1760135605-0-0-5e4e502950c659849e93e19183cffde2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8c3cc2e0463b8fd05069b40069b3f14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135611&auth_key=1760135611-0-0-75126e3f06d3f12fe47de43b0798da83&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4813adae008ff0e48050c65d1fed0be6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135618&auth_key=1760135618-0-0-6c9835254ffb6dcdd8a29ee55911b98e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e73189e94326334a487666de89baee7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135625&auth_key=1760135625-0-0-105010a618dfd28819ba146e58e583bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7f3d3ae54e90deb21a7c6add05eb634~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135632&auth_key=1760135632-0-0-1bf2acb56d54a58450246d803896407e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-315ef975557cd671e9521e76fc8a0d1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135638&auth_key=1760135638-0-0-bd62f8afdc93bc12bc8f34617f27386a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Paper2Video-Automatic-Video-Generation-from-Scientific-Papers"><a href="#Paper2Video-Automatic-Video-Generation-from-Scientific-Papers" class="headerlink" title="Paper2Video: Automatic Video Generation from Scientific Papers"></a>Paper2Video: Automatic Video Generation from Scientific Papers</h2><p><strong>Authors:Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou</strong></p>
<p>Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics–Meta Similarity, PresentArena, PresentQuiz, and IP Memory–to measure how videos convey the paper’s information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at <a target="_blank" rel="noopener" href="https://github.com/showlab/Paper2Video">https://github.com/showlab/Paper2Video</a>. </p>
<blockquote>
<p>学术报告视频已成为研究交流的重要媒介，但其制作仍然高度依赖人工，通常需要数小时的幻灯片设计、录制和编辑才能制作出短短2到10分钟的视频。与常规视频不同，演示视频生成面临一些独特挑战：如研究论文的输入、密集的多模式信息（文本、图表、表格），以及需要协调多个对齐的通道（如幻灯片、字幕、语音和人类说话者）。为了应对这些挑战，我们推出了Paper2Video，这是由作者创建的第一个包含研究报告视频、幻灯片以及演讲者元数据的基准测试集，包含101篇研究论文。我们进一步设计了四个专门的评估指标——Meta Similarity、PresentArena、PresentQuiz和IP Memory，来衡量视频如何向观众传达论文的信息。在此基础上，我们提出了学术报告视频生成的首个多代理框架PaperTalker。它整合了幻灯片生成与有效的布局优化，通过新颖的有效树搜索视觉选择、光标定位、字幕、语音合成和说话人渲染等技术，同时并行幻灯片生成以提高效率。在Paper2Video上的实验表明，我们的方法生成的报告视频比现有基线更忠实和富有信息，朝着自动化和即用的学术视频生成迈出了实用的一步。我们的数据集、代理和代码可在<a target="_blank" rel="noopener" href="https://github.com/showlab/Paper2Video%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/showlab/Paper2Video找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05096v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://showlab.github.io/Paper2Video/">https://showlab.github.io/Paper2Video/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了学术报告视频的重要性及其制作过程中的挑战。为应对这些挑战，研究者推出了Paper2Video数据集，包括研究论文与作者制作的报告视频、幻灯片及演讲者元数据。基于该数据集，提出了PaperTalker框架，实现了幻灯片生成、布局优化、字幕、语音合成和真人演讲渲染等功能，且可并行生成幻灯片以提高效率。实验证明其生成的报告视频更具准确性和信息性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学术报告视频已成为研究交流的重要媒介，但制作过程中存在高度劳动密集的问题。</li>
<li>Paper2Video数据集包含研究论文、作者制作的报告视频、幻灯片及演讲者元数据，为解决制作过程中的挑战提供了基础。</li>
<li>推出了PaperTalker框架，集成了幻灯片生成、布局优化、字幕、语音合成和真人演讲渲染等功能。</li>
<li>PaperTalker框架通过有效树状视觉搜索、光标定位等技术实现高效制作。</li>
<li>Paper2Video数据集上的实验证明，PaperTalker生成的报告视频相比现有方法更忠实且信息更丰富。</li>
<li>PaperTalker框架可实现幻灯片并行生成，提高制作效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-80f0a10ce384247742fe32c2aacaafe1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135646&auth_key=1760135646-0-0-57a5144c940a1982125e5460e58ed1ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d40d333a67bd787daf8a676abd9b8649~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135653&auth_key=1760135653-0-0-14c8c862c536924285d25ac295dc9c10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f71caecb1f38bae74629fe1551696ad7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135660&auth_key=1760135660-0-0-d075f48ca75919b2b7dc1dc64d4bbe2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3446f984fd03ece428cc782d312cc98f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135667&auth_key=1760135667-0-0-75d1adc60fb6dc3253b665ad576de155&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72495efc5bb1891aaf2c5199772232db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135673&auth_key=1760135673-0-0-c27f6ad0d4836b55e0d3a772c654e8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a927d8a3c411167e73facd194a696d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135680&auth_key=1760135680-0-0-040788319c2c716b26b40a021816a771&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a810cd52c05e8a66af95936ba8d9998c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135687&auth_key=1760135687-0-0-ae15008ec1cfba824a4be07e9dc771b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models"><a href="#Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models" class="headerlink" title="Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models"></a>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models</h2><p><strong>Authors:Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</strong></p>
<p>Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</a> </p>
<blockquote>
<p>视频理解是计算机视觉领域最具挑战性的前沿方向，它要求模型能够推理复杂的时空关系、长期依赖关系和多模态证据。最近出现的视频大型多模态模型（Video-LMMs），集成了视觉编码器和基于解码器的强大语言模型，在视频理解任务中表现出了卓越的能力。然而，将这些模型从基本感知系统转变为先进推理引擎的关键阶段——后训练，在文献中仍然分散。这篇综述提供了对视频大型多模态模型后训练方法的首次全面研究，涵盖了三个基本支柱：通过思维链进行有监督微调（SFT）、基于可验证目标的强化学习（RL）以及通过增强推理计算进行测试时缩放（TTS）。我们提出了一个结构化分类法，阐明了这些方法的作用、相互关联以及针对视频的特定适应，应对独特的挑战，如时间定位、时空定位、长视频效率和多模态证据融合。通过对代表性方法的系统分析，我们综合了关键的设计原则、见解和评估协议，同时确定了奖励设计、可扩展性和成本性能优化方面的关键开放挑战。我们还整理了重要的基准测试、数据集和指标，以便对后训练的有效性进行严格评估。本综述旨在为研究人员和实践者提供一个统一框架，以推进视频大型多模态模型的能力。更多资源和更新信息请访问：<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post%E7%BB%83%E5%AE%B9">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05034v3">PDF</a> The 1st version</p>
<p><strong>摘要</strong></p>
<p>视频理解是计算机视觉领域最具挑战性的前沿课题，需要模型推理复杂的时空关系、长期依赖性和多模态证据。视频大型多模态模型（Video-LMMs）的兴起，通过整合视觉编码器和基于语言模型的解码器，展现了视频理解任务的卓越能力。然而，将这些模型从基本感知系统转变为先进推理引擎的后续训练阶段，在文献中仍显得零散。本文首次全面调查了Video-LMMs的后续训练方法论，包括三个基本支柱：借助思维链的监督微调（SFT）、基于可验证目标的强化学习（RL）以及通过增强推理计算进行测试时缩放（TTS）。本文提出了一个结构化分类法，阐明了这些方法在视频理解中的角色、相互联系和特定适应性，并解决了诸如时间定位、时空定位、长视频效率和多模态证据融合等独特挑战。通过对代表性方法的系统分析，我们总结了关键设计原则、见解和评估协议，同时确定了奖励设计、可扩展性和成本性能优化方面的关键开放挑战。此外，我们还整理了重要的基准测试、数据集和度量标准，以便对后续训练的有效性进行严格评估。本文旨在为研究人员和从业者提供一个推进Video-LMM能力的统一框架。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频理解是当前计算机视觉领域最具挑战性的任务之一，需要模型处理复杂的时空关系、长期依赖和多模态证据。</li>
<li>Video-LMMs通过结合视觉编码器与语言模型解码器，已在视频理解方面展现出显著能力。</li>
<li>后续训练是将Video-LMMs从基本感知系统转变为先进推理引擎的关键阶段。</li>
<li>论文详细介绍了三种主要的后续训练策略：监督微调（SFT）、强化学习（RL）和测试时缩放（TTS）。</li>
<li>这些策略面临一些独特挑战，如时间定位、时空定位、长视频效率以及多模态证据融合。</li>
<li>论文还提供了一系列关键设计原则、见解和评估协议，以及对奖励设计、可扩展性和成本性能优化等关键开放挑战的识别。</li>
<li>为了方便对后续训练的有效性进行严格评估，论文还整理了重要的基准测试、数据集和度量标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c29e512707eea138803c7e3f8dc3078c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135695&auth_key=1760135695-0-0-7db36685d674461393d5a793f1b34a9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85a61878c24e7a8ad77236b533d85f14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135702&auth_key=1760135702-0-0-d71db1e714f417482075ba78847f8734&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54c7a4120c0b449b18e193ab28f346b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135709&auth_key=1760135709-0-0-c898f834f42b233fc1b5e2a06195a1cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement"><a href="#SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement" class="headerlink" title="SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement"></a>SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement</h2><p><strong>Authors:Kuan-Yu Chen, Jeng-Lin Li, Jian-Jiun Ding</strong></p>
<p>With the fast development of zero-shot text-to-speech technologies, it is possible to generate high-quality speech signals that are indistinguishable from the real ones. Speech editing, including speech insertion and replacement, appeals to researchers due to its potential applications. However, existing studies only considered clean speech scenarios. In real-world applications, the existence of environmental noise could significantly degrade the quality of generation. In this study, we propose a noise-resilient speech editing framework, SeamlessEdit, for noisy speech editing. SeamlessEdit adopts a frequency-band-aware noise suppression module and an in-content refinement strategy. It can well address the scenario where the frequency bands of voice and background noise are not separated. The proposed SeamlessEdit framework outperforms state-of-the-art approaches in multiple quantitative and qualitative evaluations. </p>
<blockquote>
<p>随着零样本文本到语音技术的快速发展，生成高质量、与真实语音信号无法区分的语音信号已成为可能。语音编辑，包括语音插入和替换，因其潜在的应用价值而吸引研究人员。然而，现有研究仅涉及清洁语音场景。在真实世界应用中，环境噪声的存在可能会显著降低生成语音的质量。本研究提出一种噪声鲁棒的语音编辑框架SeamlessEdit，用于噪声语音编辑。SeamlessEdit采用频带感知噪声抑制模块和内容细化策略。它能很好地处理语音和背景噪声频率带未分离的场景。所提出的SeamlessEdit框架在多个定量和定性评估中均优于最新技术方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14066v2">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>随着零样本文本转语音技术的快速发展，生成高质量、与现实语音难以区分的语音信号成为可能。语音编辑，包括语音插入和替换，因其潜在的应用价值而吸引研究者关注。然而，现有研究主要集中在干净语音场景。实际应用中，环境噪声会对语音生成质量造成显著影响。本研究提出一种噪声鲁棒的语音编辑框架SeamlessEdit，用于噪声语音编辑。SeamlessEdit采用频带感知噪声抑制模块和内容优化策略，能够应对语音和背景噪声频带不分离的场景。SeamlessEdit框架在多项定量和定性评估中表现优于现有先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本文本转语音技术快速发展，可生成高质量与现实语音难以区分的语音信号。</li>
<li>语音编辑具有潜在应用价值，但现有研究主要集中在干净语音场景。</li>
<li>实际应用中，环境噪声对语音生成质量有显著影响。</li>
<li>提出一种噪声鲁棒的语音编辑框架SeamlessEdit，用于噪声语音编辑。</li>
<li>SeamlessEdit采用频带感知噪声抑制模块，能应对语音和背景噪声频带不分离的情况。</li>
<li>SeamlessEdit框架在多项评估中表现优于现有先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14066">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-658b2cb3d588143d2131de1d0968c722~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135717&auth_key=1760135717-0-0-6cfd2de0284471b50f211a35b47246ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33686396df5a918b34909ee013ebfbd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135725&auth_key=1760135725-0-0-eb6e51862bc1f8cf44096ab0ae86febc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9ec921c8018ee782a66f35be031746a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135731&auth_key=1760135731-0-0-6d9b8efbcc37278e3ebfedd14c56230c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a16c006506bf688ae62498df841d00c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760135738&auth_key=1760135738-0-0-8be944474ae23f816c861f1b71366e25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ad6e182cd24e57d6a679b2e8385bc5e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760136244&auth_key=1760136244-0-0-7554c11aad096b6f04256381193a4e48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-10-11  DialoSpeech Dual-Speaker Dialogue Generation with LLM and Flow Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2b40a21bee6693aa28fcd120acfe463f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760134196&auth_key=1760134196-0-0-2a1ca65148107875e5be856c1e2a00a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-11  AI-Driven Radiology Report Generation for Traumatic Brain Injuries
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
