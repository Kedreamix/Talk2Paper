<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-10-11  NaViL Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-a7ef7465fa414c8bad36ed951b897930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145180&auth_key=1760145180-0-0-1a40e4df6f93262ee4e95af44576d06e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-11-更新"><a href="#2025-10-11-更新" class="headerlink" title="2025-10-11 更新"></a>2025-10-11 更新</h1><h2 id="NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints"><a href="#NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints" class="headerlink" title="NaViL: Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints"></a>NaViL: Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints</h2><p><strong>Authors:Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai</strong></p>
<p>Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）的实际范式通常采用组合训练，其中通过连续的多模态预训练将预训练的视觉编码器和预训练的语言模型连接起来。然而，由于训练是分离的，该范式的多模态扩展属性仍然难以探索。本文重点关注MLLMs的端到端原生训练，并在实际设置（即数据约束）下系统地研究其设计空间和扩展属性。通过对MLLM中各种选择的研究，我们获得了最佳元架构，该架构在性能和训练成本之间达到了最佳平衡。之后，我们进一步探索了原生MLLM的扩展属性，并指出了视觉编码器和LLMs之间正相关的扩展关系。基于这些发现，我们提出了一种名为NaViL的原生MLLM，并结合了一种简单且成本效益高的配方。在14个多模态基准测试上的实验结果证明了NaViL与现有MLLMs的竞争性能。除此之外，我们的发现和结果还为未来原生MLLM的研究提供了深入见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08565v1">PDF</a> Accepted by NeurIPS 2025. 22 pages, link:   <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/NaViL">https://github.com/OpenGVLab/NaViL</a></p>
<p><strong>Summary</strong><br>     本文研究了多模态大语言模型（MLLMs）的原生训练方式，并系统地探讨了其在实用场景下的设计空间和扩展属性。通过对MLLM中各种选择的研究，获得了能最佳平衡性能和训练成本的元架构。此外，文章进一步探索了原生MLLM的扩展属性，并提出了一个名为NaViL的原生MLLM，它在多个模态基准测试上表现出竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大语言模型（MLLMs）通常采用组合训练范式，通过连续多模态预训练将预训练的视觉编码器和预训练的语言模型连接起来。</li>
<li>原生训练方式是MLLMs的一种重要训练范式，本文对其进行研究并系统地探讨了其设计空间和扩展属性。</li>
<li>在数据受限的实用场景下，本文获得了能最佳平衡性能和训练成本的元架构。</li>
<li>文章进一步探索了原生MLLM的扩展属性，发现视觉编码器和语言模型之间存在正相关关系。</li>
<li>基于研究，提出了一个名为NaViL的原生MLLM模型，该模型在多个模态基准测试上表现出竞争力。</li>
<li>NaViL模型具有简单且经济实惠的特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fffa53aafc2dd31fab067e1d0077084a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145187&auth_key=1760145187-0-0-d2465ccbee04b35c67796e2f80ef9c8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73343bac7a7291d33aa09ae469326982~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145195&auth_key=1760145195-0-0-d217fc4abab39232130d3792462736e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9456766951dc75cc5c567cd68e558284~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145202&auth_key=1760145202-0-0-eea3a50838de6c1c4ec97b95b690a348&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-493cb0f288ee301e28c6d94611f182e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145209&auth_key=1760145209-0-0-24aaab62a6f8d8904e2d71c853d1ab22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c4ed9fb4370e1b9f682306cb7aa1597~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145215&auth_key=1760145215-0-0-026865b30d3ecafadfcd898a141a86e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62c2fe3d5be6c240eae88c1f3788b56d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145222&auth_key=1760145222-0-0-4c28aaade5718bf715d63fee69b0eeda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-562c1b87ac6b13f3c4b82f07865d50a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145228&auth_key=1760145228-0-0-8c72f8e7b92a4de964e5bae39d757591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Reasoning-for-Diffusion-Language-Models-via-Group-Diffusion-Policy-Optimization"><a href="#Improving-Reasoning-for-Diffusion-Language-Models-via-Group-Diffusion-Policy-Optimization" class="headerlink" title="Improving Reasoning for Diffusion Language Models via Group Diffusion   Policy Optimization"></a>Improving Reasoning for Diffusion Language Models via Group Diffusion   Policy Optimization</h2><p><strong>Authors:Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</strong></p>
<p>Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce \textbf{Group Diffusion Policy Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks. </p>
<blockquote>
<p>扩散语言模型（DLMs）能够实现并行、无序生成和迭代优化，为自回归大型语言模型（LLMs）提供了灵活的替代方案。然而，由于概率计算不可行，将强化学习（RL）微调适应到DLMs仍然是一个挑战。开创性工作如diffu-GRPO通过一步去掩码来估计token级概率。虽然计算效率高，但这种方法存在严重偏见。更原则性的基础在于序列级概率，其中证据下限（ELBO）作为替代物。然而，尽管有这种清晰的数学联系，基于ELBO的方法由于概率评估的昂贵成本而应用有限。在这项工作中，我们重新审视了ELBO估计并分解了其方差来源。这种分解促使我们通过一些关键维度上的快速确定性积分近似来减少方差。基于这一见解，我们引入了针对DLMs定制的全新RL算法——**Group Diffusion Policy Optimization (GDPO)**。GDPO利用简单有效的半确定性蒙特卡洛方案，缓解了ELBO估计器在纯双重蒙特卡洛采样下的方差爆炸问题，在严格的评估预算下，提供了一个可证明的低方差估计器。经验上，GDPO在预训练检查点上实现了持续的收益，并在大多数数学、推理和编码基准测试上超越了最先进的基线之一diffu-GRPO。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散语言模型（DLMs）通过并行、顺序无关生成和迭代优化提供了一种灵活的替代自回归大型语言模型（LLMs）的方法。然而，由于不确定性太大，强化学习（RL）微调在DLMs中的应用仍然是一个挑战。目前大多数工作如diffu-GRPO都是基于一步解掩码进行token级别的可能性估计，这虽计算高效，但有严重偏见。基于序列级别可能性的证据下限（ELBO）更为原则性，但由于评估可能性成本高昂，其应用受到限制。本文重新审视了ELBO估计并分解了其方差来源。基于此分解，我们引入了针对DLMs的新的RL算法——集团扩散策略优化（GDPO）。GDPO利用简单有效的半确定性蒙特卡洛方案，缓解了朴素双重蒙特卡洛采样下ELBO估计量的方差爆炸问题，在严格评估预算下提供了一个方差更低的估计量。经验上，GDPO在预训练检查点上取得了持续的收益，并在大多数数学、推理和编码基准测试中表现优于当前最先进的基线之一diffu-GRPO。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散语言模型（DLMs）提供灵活的生成方式，可以通过并行处理进行迭代优化。</li>
<li>强化学习（RL）在DLMs中的应用面临不确定性问题。</li>
<li>当前方法如diffu-GRPO主要基于token级别的可能性估计，存在偏见问题。</li>
<li>基于序列级别的ELBO估计更为原则性，但由于成本高昂应用受限。</li>
<li>本文分解了ELBO估计的方差来源，并引入了新的RL算法GDPO来解决DLMs中的强化学习问题。</li>
<li>GDPO利用半确定性蒙特卡洛方案缓解方差爆炸问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cb590f5a4a7334f71436a8a106aba1a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145235&auth_key=1760145235-0-0-b1bd92dc5c4d58eb0aa6cbd6e7a41932&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94ffa7476b939b90c924578864a82ee1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145242&auth_key=1760145242-0-0-bdc42c8f883101e67bc055240d134a41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards"><a href="#CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards" class="headerlink" title="CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"></a>CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</h2><p><strong>Authors:Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai</strong></p>
<p>Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent’s policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents. </p>
<blockquote>
<p>自我进化是使基于大型语言模型（LLM）的代理在预训练后能够持续提高其能力的一个核心研究课题。最近的研究见证了从非强化学习（RL）到基于RL的方法的转变。当前的基于RL的方法要么依赖于密集的外部奖励信号，要么从LLM本身中提取内在奖励信号。然而，这些方法与人类智能中的自我进化机制相悖，个人通过相互讨论和协作来学习和提高。在这项工作中，我们引入了协同进化多智能体系统（CoMAS），这是一种新型框架，能够使智能体通过从智能体之间的交互学习来提高自主性，而无需外部监督。CoMAS从丰富的讨论动态中产生内在奖励，采用LLM作为裁判机制来制定这些奖励，并通过RL优化每个智能体的策略，从而实现去中心化和可扩展的协同进化。实验结果表明，CoMAS始终优于未训练的代理，并在大多数评估环境中达到最新技术水平。消融研究证实了基于交互的奖励信号的必要性，并显示出随着智能体数量和多样性的增加，其前景广阔的可扩展性。这些发现确立了CoMAS在LLM代理中的自我进化的新颖而有效的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08529v1">PDF</a> </p>
<p><strong>Summary</strong><br>自我进化是使基于大型语言模型（LLM）的代理在预训练后持续提高其能力的一个核心研究课题。最近的研究经历了从无强化学习（RL）到基于RL的方法的转变。当前基于RL的方法要么依赖于密集的外部奖励信号，要么从LLM本身中提取内在奖励信号。然而，这些方法偏离了人类智能中的自我进化机制，个人通过相互讨论和协作学习和提高。在此工作中，我们引入了协同进化多智能体系统（CoMAS），这是一个新型框架，使智能体能够通过相互间的交互学习而自主提高，无需外部监督。CoMAS从丰富的讨论动态中产生内在奖励，采用LLM作为法官的机制来制定这些奖励，并通过RL优化每个智能体的策略，从而实现分散和可扩展的协同进化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于LLM的代理的自我进化是研究的重点，旨在使其在预训练后持续提高能力。</li>
<li>当前方法从密集外部奖励信号或LLM本身提取内在奖励信号。</li>
<li>人类智能中的自我进化机制涉及个体通过相互讨论和协作学习和提高。</li>
<li>CoMAS框架使智能体能够通过相互间的交互学习而自主提高，无需外部监督。</li>
<li>CoMAS生成内在奖励来自丰富的讨论动态，并采用LLM作为法官来制定奖励。</li>
<li>CoMAS通过RL优化每个智能体的策略，实现分散和可扩展的协同进化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08529">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c6c34837bff0f4ad8d24310829b02e6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145250&auth_key=1760145250-0-0-467a9b45de2491db22741d24d2b05f95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-949738b7676f3792c403fa91a7b3982e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145257&auth_key=1760145257-0-0-9146d0d7123b1c187a4b31dd5ef31525&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e70f4c8c542fc2eed746a939aef97ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145263&auth_key=1760145263-0-0-e80edce6d02c3bf353decde88f7dda66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AutoMLGen-Navigating-Fine-Grained-Optimization-for-Coding-Agents"><a href="#AutoMLGen-Navigating-Fine-Grained-Optimization-for-Coding-Agents" class="headerlink" title="AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents"></a>AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</h2><p><strong>Authors:Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai</strong></p>
<p>Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-Innovator/InternAgent">https://github.com/Alpha-Innovator/InternAgent</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在一般编程任务中表现出了令人印象深刻的性能。然而，在机器学习工程（MLE）场景，如自动化机器学习（AutoML）和Kaggle竞赛中，实现高性能很大程度上依赖于专家的介入和重复调整，而不是简单地生成正确的代码。当直接应用于这些任务时，LLM往往缺乏精细的域先验知识，而现有的MLE方法使用线性或树状搜索，将知识转移限制在相邻的层次链接上。因此，它们无法利用过去的完整轨迹或在各分支之间共享信息，限制了自我进化能力和搜索空间的多样性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08511v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在一般编程任务中表现出色，但在机器学习工程（MLE）场景如AutoML和Kaggle竞赛中，实现高性能更多地依赖于专家干预和重复调整，而非仅生成正确代码。针对LLM在这些任务中的局限性，提出了基于LLM的编码代理AutoMLGen，通过整合领域知识库和蒙特卡洛图搜索（MCGS）来提高性能。MCGS保留了蒙特卡洛树搜索的树结构指导探索，同时在扩展阶段嵌入图结构，实现动态路径重组、历史轨迹复用和多解融合，支持自我进化和协作学习。在MLE-Bench上的评估显示，AutoMLGen在平均奖牌率和有效提交率等多个维度实现了卓越性能，且在12小时预算内（仅为标准运行时的一半）达到了业界领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在MLE场景中面临专家干预和重复调整的需求较高，直接应用时缺乏精细领域先验知识。</li>
<li>现有MLE方法使用线性或树状结构搜索，限制了知识转移和搜索空间多样性。</li>
<li>AutoMLGen是一个基于LLM的编码代理，集成了领域知识库和蒙特卡洛图搜索（MCGS）以提高性能。</li>
<li>MCGS结合了树结构指导的探索和图结构嵌入，实现动态路径重组、历史轨迹复用和多解融合。</li>
<li>AutoMLGen支持自我进化和协作学习，提高了稳定性并加速了收敛。</li>
<li>在MLE-Bench上的评估显示，AutoMLGen在多个维度达到了业界领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08511">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2f78f8ab4f3917a7f0f379e1d3b0dd5e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145271&auth_key=1760145271-0-0-cdce8cd0dc472737a2b2ca7add330b0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d63880a4d4f492ba270657dbc5b72de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145279&auth_key=1760145279-0-0-5b8ad3e4816fd254b34c63bcb9245ec7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance"><a href="#InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance" class="headerlink" title="InstructX: Towards Unified Visual Editing with MLLM Guidance"></a>InstructX: Towards Unified Visual Editing with MLLM Guidance</h2><p><strong>Authors:Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He</strong></p>
<p>With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance. </p>
<blockquote>
<p>随着多模态大语言模型（MLLMs）的近期进展，显示出强大的视觉理解和推理能力，人们越来越有兴趣将它们用于提高扩散模型的编辑性能。尽管进展迅速，但大多数研究缺乏对MLLM设计选择的深入分析。此外，将MLLMs和扩散模型集成在一起在某些困难的任务（如视频编辑）中仍然是一个开放性的挑战。在本文中，我们提出了InstructX，一个用于图像和视频编辑的统一框架。具体来说，我们对集成MLLMs和扩散模型以进行指令驱动编辑进行了全面研究，涉及多种任务。在此基础上，我们分析了图像和视频在统一建模中的合作和区别。（1）我们表明，在图像数据上进行训练可以在没有显式监督的情况下导致出现视频编辑能力，从而减轻了稀缺视频训练数据带来的约束。（2）通过融入模态特定的MLLM特性，我们的方法有效地将图像和视频编辑任务统一到一个单一模型中。大量实验表明，我们的方法可以处理广泛的图像和视频编辑任务，并实现了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着多模态大语言模型（MLLMs）的近期进展，其在视觉理解和推理方面表现出强大的能力，人们开始关注将其用于提高扩散模型的编辑性能。本文提出了InstructX，一个用于图像和视频编辑的统一框架，深入研究了将MLLMs和扩散模型集成在一起，以进行指令驱动的编辑任务。研究表明，在图像数据上进行训练可以涌现出视频编辑能力，且通过融入模态特定MLLM特征，能够统一图像和视频编辑任务。实验证明，该方法能处理多种图像和视频编辑任务，并达到最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大语言模型（MLLMs）在视觉理解和推理方面表现出强大能力。</li>
<li>InstructX框架被提出，用于整合MLLMs和扩散模型，以实现指令驱动的图像和视频编辑。</li>
<li>在图像数据上训练可以使得模型具备视频编辑能力，无需明确的视频监督。</li>
<li>通过融入模态特定的MLLM特征，图像和视频编辑任务可以在单一模型内得到统一处理。</li>
<li>InstructX框架能够处理多种图像和视频编辑任务。</li>
<li>该方法在实验上达到了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08485">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4bc8d3a7c558ea5a8120c4757cea3a75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145286&auth_key=1760145286-0-0-9d05afd3cc7cf13f5efa8ab440eed045&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-afc26c2a7a2e1d4b68777f798f0935b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145293&auth_key=1760145293-0-0-cbcf8944ef0a4bf8f515fe0c7b5038d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97399e335a9ea984d56386d3e3bd9cf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145300&auth_key=1760145300-0-0-d7c7453e5d59e8ff098aaad84c12a287&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f1020c946a37973f231072e01c4450c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145306&auth_key=1760145306-0-0-59fe9eb100731578f018c810a9ffbe46&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d255793f2e47ee8b95ffad0c69e3a50c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145313&auth_key=1760145313-0-0-ccb70c412308d62d12b775696b665eba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33567bcefd0370480eaba7aafc3b11bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145319&auth_key=1760145319-0-0-0caf110c7b753ebab097ad6505751afe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b35b1ab4a5866e0514d47e3dcf7405d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145325&auth_key=1760145325-0-0-c14a50cebf00255b871d49f732607209&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a533e74bd38a606086428a5aa5c12b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145332&auth_key=1760145332-0-0-ad0b308f90c80b5336c2d817631700e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy"><a href="#DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy" class="headerlink" title="DeepPrune: Parallel Scaling without Inter-trace Redundancy"></a>DeepPrune: Parallel Scaling without Inter-trace Redundancy</h2><p><strong>Authors:Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li</strong></p>
<p>Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy – our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: <a target="_blank" rel="noopener" href="https://deepprune.github.io/">https://deepprune.github.io/</a> </p>
<blockquote>
<p>并行扩展已经成为一种强大的范式，通过同时生成多个思维链追踪（CoT），增强大型语言模型（LLM）的推理能力。然而，这种方法由于追踪之间的冗余而引入了重大的计算效率低下——我们的分析表明，超过80%的并行推理轨迹产生了相同的最终答案，代表了大量的计算浪费。为了解决这一关键的效率瓶颈，我们提出了DeepPrune，一个通过动态修剪实现高效并行扩展的新型框架。我们的方法以一个使用焦点损失和过采样技术训练的专用法官模型为特色，可以准确地预测从部分推理轨迹得出的答案是否等效。此模型实现了预测等效的受试者工作特征曲线下面积值（AUROC）达到0.87，同时结合在线贪婪聚类算法，在保留答案多样性的同时动态修剪冗余路径。在三个具有挑战性的基准测试（AIME 2024、AIME 2025和GPQA）以及多个推理模型上的综合评估表明，DeepPrune在大多数情况下实现了超过80%的令牌减少，与传统共识采样相比取得了显著成绩，同时保持了3个百分点以内的竞争力准确度。我们的工作为高效并行推理建立了新标准，使高性能推理更加高效。我们的代码和数据可在此处找到：[<a target="_blank" rel="noopener" href="https://deepprune.github.io/]">https://deepprune.github.io/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08483v1">PDF</a> 15 pages, 4 figures, please check out the project page:   <a target="_blank" rel="noopener" href="https://deepprune.github.io/">https://deepprune.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>并行缩放已成为增强大型语言模型（LLM）推理能力的一种强大范式，通过同时生成多个思维链（CoT）轨迹来实现。然而，这种方法由于轨迹间的冗余而引入了重大的计算效率问题——我们的分析显示，超过80%的并行推理轨迹得出了相同的最终答案，造成了大量的计算浪费。为解决这一关键效率瓶颈，我们提出了DeepPrune框架，通过动态剪枝实现高效并行缩放。该方法采用专门训练的法官模型，结合焦点损失和过采样技术，准确预测部分推理轨迹的答案是否相同，实现0.87的等价预测值。结合在线贪婪聚类算法，动态删除冗余路径的同时保持答案多样性。在三个具有挑战性的基准测试（AIME 2024、AIME 2025和GPQA）和多个推理模型上的综合评估表明，DeepPrune在大多数情况下实现了超过80%的令牌减少，同时保持竞争性的准确率在3个百分点以内。我们的工作为高效并行推理建立了新标准，使高性能推理更加高效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>平行缩放增强LLM推理能力，通过生成多个思维链轨迹实现。</li>
<li>平行缩放存在计算效率低的问题，超过80%的轨迹得出相同答案，造成计算浪费。</li>
<li>提出DeepPrune框架解决效率问题，通过动态剪枝实现高效并行推理。</li>
<li>DeepPrune采用专门训练的法官模型，结合焦点损失和过采样技术预测答案等价性。</li>
<li>DeepPrune实现了超过80%的令牌减少，同时保持竞争性的准确率。</li>
<li>综合评估表明DeepPrune在多个基准测试和推理模型上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d3f3c0b39aea7be40be5ba5125a06f82~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145340&auth_key=1760145340-0-0-74194a9c6f3984a961fbbe7802dfdca6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d52756d94c37872766de5e5e8e34b01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145347&auth_key=1760145347-0-0-b3bcd66cd3b9322d37ebb912eec1a56a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6db0e48ef10bacd8e4c7aac52d639525~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145354&auth_key=1760145354-0-0-7529ad397f781039df87ca2bbe590e28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28351f2ca282f623675cce5cd8c443f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145361&auth_key=1760145361-0-0-7811dca8c7aeac3ecb017d576ecbe2c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-STAR-Reinforcing-Open-Vocabulary-Action-Recognition-with-Tools"><a href="#Video-STAR-Reinforcing-Open-Vocabulary-Action-Recognition-with-Tools" class="headerlink" title="Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools"></a>Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools</h2><p><strong>Authors:Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li</strong></p>
<p>Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在弥合视觉和文本推理方面表现出显著潜力，但它们对以文本为中心的先验的依赖往往限制了它们在开放词汇场景中区分语义上相似的动作的能力。为了解决这个问题，我们提出了Video-STAR框架，该框架将上下文子运动分解与工具增强强化学习相结合，用于开放词汇动作识别（OVAR）。不同于将动作视为单一实体的先前方法，我们的方法创新地将动作分解为具有区分性的子动作，以实现精细匹配，同时动态调用特定领域的工具进行跨模态交织，从而启用特定类别的推理能力并减少跨模态幻觉。此外，通过设计一种平衡的层次奖励来平衡工具使用效率、子动作相关性和推理中的结构连贯性，我们的方法能够自主地利用外部工具来优先处理子动作模式，无需明确监督，实现从以文本为中心的推理到以视觉为基础的推理。在HMDB-51、UCF-101、SSv2、Kinetics-400和Kinetics-600数据集上的广泛评估表明，我们的最新技术在区分精细动作和处理跨模态幻觉方面表现出卓越的性能，验证了我们的稳健性和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMs在多模态大型语言模型中的视觉和文本推理方面具有显著潜力，但其文本为中心的先验知识限制了其在开放词汇场景中对语义相似动作的辨别能力。为解决此问题，提出了Video-STAR框架，该框架结合语境子运动分解与工具辅助强化学习进行开放词汇动作识别（OVAR）。不同于以往将动作视为单一实体的方法，Video-STAR创新地将动作分解为具有鉴别力的子运动，进行精细匹配，并动态调用特定工具进行跨模态交织，从而提升类别特定推理能力并减少跨模态幻觉。通过设计平衡工具使用效率、子运动相关性和推理结构连贯性的分层奖励，该方法可自主利用外部工具来优先子运动模式，从文本中心推理转向视觉基础推理。在HMDB-51、UCF-101、SSv2、Kinetics-400和Kinetics-600数据集上的广泛评估表明，该方法在区分精细动作和处理跨模态幻觉方面达到了最新技术水平，验证了其卓越的稳定性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）融合了视觉和文本推理，但在开放词汇场景中识别语义相似动作时存在限制。</li>
<li>Video-STAR框架通过子运动分解和工具辅助强化学习来解决这一问题。</li>
<li>Video-STAR将动作分解为子运动，实现精细匹配，并动态利用特定工具进行跨模态交织。</li>
<li>框架通过设计分层奖励来平衡工具使用效率和推理结构连贯性。</li>
<li>外部工具的使用有助于优先处理子运动模式，实现从文本中心推理到视觉基础推理的转变。</li>
<li>Video-STAR在多个数据集上表现出最新技术水平，尤其在区分精细动作和处理跨模态幻觉方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08480">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-512b3fd5d536e9d8b6b660b0e09c84d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145368&auth_key=1760145368-0-0-6c107f40665d722abb68496eada30b7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ef1b3d28c5fe1ec91052b1b2826854a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145375&auth_key=1760145375-0-0-ae84b39a041267e2dccf493d9554fd77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-736ccc17eae14cf8c7b1ee128db2ae5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145382&auth_key=1760145382-0-0-94fb65bb26790c82679d90fd812f9aa2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b80ee5dbebaae39dc7b3d1bd124a5f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145389&auth_key=1760145389-0-0-9ac2b4825289f2a2e18a3c2f89672e9c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Single-layer-tiny-Co-4-outpaces-GPT-2-and-GPT-BERT"><a href="#Single-layer-tiny-Co-4-outpaces-GPT-2-and-GPT-BERT" class="headerlink" title="Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT"></a>Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT</h2><p><strong>Authors:Noor Ul Zain, Mohsin Raza, Ahsan Adeel</strong></p>
<p>We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws. </p>
<blockquote>
<p>我们展示了一个小型Co$^4$机器（Adeel，2025）的单层、两个头部和8M参数，在近似于$O(N)$的计算成本（其中$N$是输入令牌的数量）下，仅经过两个周期的训练就超过了BabyLM挑战基准的GPT-2（1.2亿，12层，$O(N^2)$）和GPT-BERT（3亿，12层，$O(N^2)$）。在训练了十个周期后，Co$^4$在训练效率上实现了高达数倍的突破，展示了高度样本高效的预训练。使用BabyLM挑战评估管道进行复杂基准测试时，Co$^4$在SuperGLUE任务上表现出强大的零样本和微调性能。具体来说，Co$^4$在零样本指标的7个中有5个优于GPT-2，在微调任务的7个中有6个优于GPT-2，并且在两种情况下都有4个指标优于GPT-BERT。这些结果提示我们需要重新思考现有的深度学习范式和相关的扩展定律。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08404v1">PDF</a> </p>
<p><strong>Summary</strong><br>     基于Adeel在2025年提出的Co$^4$机器模型，该模型具有单层、两个头和8M参数，在近似于O(N)的计算成本下（其中N为输入标记的数量），经过仅两个周期的训练便超越了BabyLM挑战基线模型GPT-2和GPT-BERT。在包含复杂基准测试的BabyLM挑战评估管道上，Co$^4$模型表现出强大的零样本和微调性能。尤其是在SuperGLUE任务上，Co$^4$在零样本情况下优于GPT-2的五个指标，在微调任务中优于六个指标；对比GPT-BERT，它在七个指标中有四个表现更优。这些结果挑战了现有的深度学习范式及其相关扩展定律。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>Co$^4$机器模型具备出色的训练效率，可在仅两个周期内超越基线模型GPT-2和GPT-BERT的训练表现。这两个基线模型分别使用了更复杂的网络结构和更多训练周期（训练了十个周期）。这表明Co$^4$具有很高的样本预处理效率。</p>
</li>
<li><p>在基准测试中，Co$^4$模型的性能优异，尤其是相较于当前流行的大型模型如GPT-2和GPT-BERT的表现更有优势。其在零样本和微调任务上的表现都超过了这些基线模型。这证明了Co$^4$模型在处理复杂任务时的优越性。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08404">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b02286ef5eb4d93582283519633b60ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145396&auth_key=1760145396-0-0-0d4dc03f6661bd0169f3be390a3146ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdd262253c1380da62d7e10101789fac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145404&auth_key=1760145404-0-0-13a3e587be8a54a05ea5250d8a0b0235&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c8d97daf58b0d8efe65b37c76be294e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145411&auth_key=1760145411-0-0-a658d427dba8884a6f5dc6428f14c578&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-badf98306beb19cac6bb04a0175cecc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145418&auth_key=1760145418-0-0-45c7767d2f1facb528578e1bd0d11120&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts"><a href="#FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts" class="headerlink" title="FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts"></a>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts</h2><p><strong>Authors:Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains – general knowledge understanding, scientific question answering, mathematical reasoning, and code generation – demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA">https://github.com/gfyddha/FlyLoRA</a>. </p>
<blockquote>
<p>低秩适应（LoRA）是一种广泛应用于基础模型的参数高效微调方法，但存在参数干扰的问题，导致性能不佳。虽然基于混合专家（MoE）的LoRA变体在单任务指令调整中显示出缓解任务内关联的潜力，但它们引入了额外的路由器参数，并且在多任务模型合并中仍然无效，这里会产生任务间干扰。受果蝇嗅觉电路的启发，我们提出了FlyLoRA，这是一种基于隐式MoE的LoRA变体，它引入了：（1）上投影矩阵中的秩专家激活；（2）一种隐式路由器，统一专家路由和下投影，其中冻结的稀疏随机投影矩阵取代了传统的密集可训练版本。这种设计通过不需要显式路由器解决了任务内去相关和计算效率之间的权衡，同时由于其随机矩阵的正交性，固有地减轻了任务间的干扰。在四个领域——包括通用知识理解、科学问题回答、数学推理和代码生成——的广泛实验证明，与现有方法相比，FlyLoRA在性能上实现了持续的提升。除了实证收益外，FlyLoRA还突出了生物结构如何激发人工智能技术的创新。代码可在<a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gfyddha/FlyLoRA上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08396v1">PDF</a> NeurIPS 2025 accepted paper</p>
<p><strong>Summary</strong></p>
<p>基于仿生设计的新LoRA变体FlyLoRA解决了参数干扰问题，提高了性能。它通过引入排名专家激活和隐性路由器，消除了传统MoE方法中存在的路由问题，并通过随机矩阵的正交性有效地减轻了跨任务的干扰。在四个不同领域的大量实验中，FlyLoRA显示出优于现有方法的性能提升。代码可在GitHub上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FlyLoRA是一种基于隐式MoE的LoRA变体，解决了参数干扰问题。</li>
<li>它通过引入排名专家激活和隐性路由器设计解决了传统MoE方法的缺点。</li>
<li>FlyLoRA消除了对显式路由器的需求，同时通过随机矩阵的正交性减轻了跨任务干扰。</li>
<li>在四个不同领域的实验中，FlyLoRA表现出性能提升。</li>
<li>该方法不仅提高了性能，而且展示了生物学结构如何启发AI技术的创新。</li>
<li>FlyLoRA具有潜力改善多任务的模型合并效果，有助于更有效地应用在大规模模型和大规模数据集上。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-49ea1c2a9cb8333300a75733cd10a794~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145425&auth_key=1760145425-0-0-bec13721b04e87d2f2fc5211ea3c5e86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d41ed60610223d6aeb8776f73cc27110~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145432&auth_key=1760145432-0-0-a10601a02c6ef064b309a620541ed612&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b6ef07a2c98c73372937f9512da163b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145439&auth_key=1760145439-0-0-83a80f190020ff891a1234c2616defae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c25a82fe3f965c063516f972603997a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145446&auth_key=1760145446-0-0-ea7cae5c07ee29d84a44a6ab6a3dd52c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Detecting-Legend-Items-on-Historical-Maps-Using-GPT-4o-with-In-Context-Learning"><a href="#Detecting-Legend-Items-on-Historical-Maps-Using-GPT-4o-with-In-Context-Learning" class="headerlink" title="Detecting Legend Items on Historical Maps Using GPT-4o with In-Context   Learning"></a>Detecting Legend Items on Historical Maps Using GPT-4o with In-Context   Learning</h2><p><strong>Authors:Sofia Kirsanova, Yao-Yi Chiang, Weiwei Duan</strong></p>
<p>Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles. </p>
<blockquote>
<p>地图图例解读对于解释地图符号至关重要。然而，其布局的不一致性和非结构化形式使得自动提取工作面临挑战。早期的研究主要集中在分割或一般的光学字符识别（OCR）上，很少有方法能够有效地以结构化方式将图例符号与相应的描述相匹配。我们提出了一种方法，结合LayoutLMv3进行布局检测，并使用GPT-4通过上下文学习来检测并连接图例项及其描述，通过边界框预测进行匹配。我们的实验表明，使用结构化JSON提示的GPT-4在F-1得分上超过了基线水平，达到了88%，IoU达到了85%，并揭示了提示设计、示例数量和布局对齐如何影响性能。这种方法支持可扩展的、具有布局意识的图例解析，提高了各种视觉风格的历史地图的索引和可搜索性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08385v1">PDF</a> </p>
<p><strong>Summary</strong>：历史地图的图例解读对于理解地图符号至关重要。然而，由于图例布局的不一致性和非结构化格式，自动提取面临挑战。先前的研究主要集中在分割或一般的光学字符识别（OCR）上，而很少有一种方法能够有效地以结构化方式将图例符号与它们的描述相匹配。本文提出了一种结合LayoutLMv3进行布局检测的方法，并使用GPT-4o通过上下文学习进行图例项及其描述的检测与链接，通过边界框预测来实现。实验表明，GPT-4与结构化JSON提示的组合相比基线表现出色，达到了88%的F-1和85%的IoU，并揭示了提示设计、示例计数和布局对齐对性能的影响。该方法支持可扩展的布局感知图例解析，提高了历史地图的索引和可搜索性，适用于各种视觉风格。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>历史地图的图例解读是理解地图符号的关键。</li>
<li>现有的自动提取图例方法存在挑战，主要由于图例布局的不一致性和非结构化格式。</li>
<li>先前的研究主要集中在分割或OCR上，而结构化匹配图例符号与描述的方法较少。</li>
<li>本文提出了一种结合LayoutLMv3和GPT-4o的方法，通过上下文学习和边界框预测来检测并链接图例项及其描述。</li>
<li>实验结果显示GPT-4与结构化JSON提示的组合性能优异，达到了88%的F-1和85%的IoU。</li>
<li>这种方法提高了历史地图的索引和可搜索性，支持各种视觉风格的地图。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08385">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-271205413fa08c429b16d2d155c36529~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145453&auth_key=1760145453-0-0-833a86e00fe076224f888827f7f711f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6db6171ae29c28a5baf04431c96cbe14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145461&auth_key=1760145461-0-0-cc7e3e4c321af209d9b556dc345ba755&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f4e0c9855b1b43d07df660bbfc850ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145467&auth_key=1760145467-0-0-a419bf4561f06cb8284a55696b1a005a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-725ed2227236cac674df464d077b2c9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145474&auth_key=1760145474-0-0-757b51b0207bdc1c5af00a1471cea80a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ChatGPT-as-a-Translation-Engine-A-Case-Study-on-Japanese-English"><a href="#ChatGPT-as-a-Translation-Engine-A-Case-Study-on-Japanese-English" class="headerlink" title="ChatGPT as a Translation Engine: A Case Study on Japanese-English"></a>ChatGPT as a Translation Engine: A Case Study on Japanese-English</h2><p><strong>Authors:Vincent Michael Sutanto, Giovanni Gatti De Giacomo, Toshiaki Nakazawa, Masaru Yamada</strong></p>
<p>This study investigates ChatGPT for Japanese-English translation, exploring simple and enhanced prompts and comparing against commercially available translation engines. Performing both automatic and MQM-based human evaluations, we found that document-level translation outperforms sentence-level translation for ChatGPT. On the other hand, we were not able to determine if enhanced prompts performed better than simple prompts in our experiments. We also discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly, ChatGPT yields competitive results against two widely-known translation systems. </p>
<blockquote>
<p>本研究调查了ChatGPT在日英翻译方面的表现，探索了简单提示和增强提示，并与市面上可用的翻译引擎进行了比较。我们进行了自动和人类评价，发现ChatGPT在文档级别的翻译上表现优于句子级别的翻译。然而，在我们的实验中，我们未能确定增强提示是否优于简单提示。我们还发现自动评估更青睐ChatGPT-3.5，但在准确性与流畅性之间存在权衡（ChatGPT-3.5更准，而ChatGPT-4更流畅）。最后，ChatGPT在与两个广为人知的翻译系统的对比中表现具有竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08042v1">PDF</a> </p>
<p><strong>Summary</strong>：本研究探讨了ChatGPT在日英翻译方面的应用，研究了简单提示和高级提示的效果，并与市面上可用的翻译引擎进行了比较。通过自动和基于MQM的人类评估，发现ChatGPT在文档级别的翻译表现优于句子级别。然而，实验未能确定高级提示是否优于简单提示。此外，自动评估更青睐ChatGPT-3.5，但在准确性和流畅性之间存在权衡。最后，ChatGPT与两个广为人知的翻译系统的结果相当。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究对比了ChatGPT的简单提示和高级提示在日英翻译中的应用效果。</li>
<li>文档级别的ChatGPT翻译表现优于句子级别。</li>
<li>实验未能确定高级提示是否优于简单提示。</li>
<li>自动评估更青睐ChatGPT-3.5，但在准确性与流畅性之间存在权衡。</li>
<li>ChatGPT与市面上两个广受欢迎的翻译系统表现相当。</li>
<li>研究采用了自动和基于MQM的人类评估方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c290c437810b81da5dd0ed1fbc16e39b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145481&auth_key=1760145481-0-0-76a15ed84d493df1302a3b06d39a0d67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c30bcb56b496f1da553d1d9d2885e3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145488&auth_key=1760145488-0-0-ba55eb68f88bdb4d2c89691618502c56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b196ec37ea7541b60adffe0dbedf9ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145495&auth_key=1760145495-0-0-9ad97c2ea43ba300e9e98c77735e7bdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8ea80c310b18db20399b3f9ad7db7d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145502&auth_key=1760145502-0-0-c536b631fcb18abe6e4bb476e4efd941&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7ef7465fa414c8bad36ed951b897930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145508&auth_key=1760145508-0-0-41bce07085ebb7ac99179353c41770b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d1551a9a40c5c021bde83bfb9ebce3a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145515&auth_key=1760145515-0-0-5f66a28cee6a1c8b39a0ab4c02b6df0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AppForge-From-Assistant-to-Independent-Developer-–-Are-GPTs-Ready-for-Software-Development"><a href="#AppForge-From-Assistant-to-Independent-Developer-–-Are-GPTs-Ready-for-Software-Development" class="headerlink" title="AppForge: From Assistant to Independent Developer – Are GPTs Ready for   Software Development?"></a>AppForge: From Assistant to Independent Developer – Are GPTs Ready for   Software Development?</h2><p><strong>Authors:Dezhi Ran, Yuan Cao, Mengzhou Wu, Simin Chen, Yuzhe Guo, Jun Ren, Zihe Song, Hao Yu, Jialei Wei, Linyi Li, Wei Yang, Baishakhi Ray, Tao Xie</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. Unlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch. To address this gap, we propose APPFORGE, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with implementing the functionality into an Android app from scratch. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct APPFORGE, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, APPFORGE incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8% functionally correct applications, highlighting fundamental limitations in current models’ ability to handle complex, multi-component software engineering challenges. </p>
<blockquote>
<p>大型语言模型（LLM）在功能级别的代码生成任务中表现出了显著的能力。不同于孤立的函数，现实世界的应用程序需要在整个软件系统进行推理：开发者必须协调不同组件的交互，随时间保持状态的一致性，并确保应用程序在生命周期和框架约束内表现正确。然而，现有的基准测试并未充分评估LLM是否能够弥补这一差距并从零开始构建整个软件系统。为了填补这一空白，我们提出了APPFORGE基准测试，它包含从真实世界的Android应用程序中抽取的101个软件开发问题。给定详细描述应用程序功能的自然语言规范，语言模型的任务是从零开始将功能实现为Android应用程序。从零开发Android应用程序需要理解和协调应用程序状态、生命周期管理和异步操作，要求LLM生成上下文感知、稳健和可维护的代码。为了构建APPFORGE，我们设计了一个多代理系统，自动从应用程序文档中总结主要功能和导航应用程序以合成验证应用程序实现功能正确性的测试用例。经过Android开发专家的严格人工验证后，APPFORGE将测试用例纳入自动化评估框架，实现了无需人工干预即可进行可重复评估，使其易于在未来研究中采用。我们对12款旗舰LLM的评估显示，所有评估的模型效果都很差，表现最佳的模型（GPT-5）开发的只有18.8%功能正确的应用程序，这凸显了当前模型在处理复杂多组件软件工程挑战时的根本局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07740v1">PDF</a> Under Review. Benchmark and leadboards at   <a target="_blank" rel="noopener" href="https://appforge-bench.github.io/">https://appforge-bench.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在函数级别的代码生成任务上展现出卓越的能力。然而，在现实世界的软件开发过程中，需要对整个软件系统进行推理，包括协调不同组件的交互、保持状态的持续一致性以及在生命周期和框架约束内确保应用程序的正确行为。针对这一差距，提出了APPFORGE基准测试，它包含从真实世界Android应用程序中抽取的101个软件开发问题。给定详细的应用程序功能自然语言规范，语言模型需要从零开始实现这些功能到Android应用程序中。在构建APPFORGE时，设计了一个多代理系统来自动总结应用程序文档的主要功能并导航应用程序以合成测试用例，验证应用程序实现的正确性。评估结果显示，所有评估的LLM模型的有效性均较低，最佳模型GPT-5仅开发出18.8%功能正确的应用程序，凸显了当前模型在处理复杂的多组件软件工程挑战时的基本局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在函数级别的代码生成任务上表现出色，但在现实世界的软件开发方面存在差距。</li>
<li>APPFORGE基准测试包含从真实Android应用程序中抽取的101个软件开发问题。</li>
<li>LLMs需理解和协调应用程序的状态、生命周期管理和异步操作。</li>
<li>多代理系统用于自动总结应用程序文档的主要功能并合成测试用例。</li>
<li>严格的手动验证确保了APPFORGE的评估准确性。</li>
<li>评估结果显示所有LLM模型的有效性较低，最佳模型GPT-5的开发效果有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07740">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d15a15aa58926c02e39466f82486d5a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145522&auth_key=1760145522-0-0-937cfafe57ff5cd793a9abee24cacf70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f882d9694f4523df35200b1067e55d96~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145529&auth_key=1760145529-0-0-c3286fa5075e702fb3cfd4e4968324e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fb09ce8b7432daaa3fd17e48c11764a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145535&auth_key=1760145535-0-0-f649fc39d9413f9f1b64d7770a33e58f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration"><a href="#More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration" class="headerlink" title="More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration"></a>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration</h2><p><strong>Authors:Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Heng Tao Shen</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This “guidance-on-demand” approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a>. </p>
<blockquote>
<p>强化学习与可验证奖励（RLVR）是一种有望增强大型语言模型（LLM）推理能力的范式。然而，现有的方法主要依赖于自我探索或单一的离线策略教师来激发长链思维（LongCoT）推理，这可能会引入内在的模型偏见并限制探索，最终限制推理的多样性和性能。我们从知识蒸馏中的多教师策略中汲取灵感，引入了自适应多引导策略优化（AMPO），这是一种新的框架，自适应地利用多个熟练教师模型的指导，但仅在在线策略模型无法生成正确解决方案时才使用。这种“按需指导”的方法在保持自我发现价值的同时扩大了探索范围。此外，AMPO 融入了一种基于理解的选择机制，促使学生学习最有可能被其理解的推理路径，从而在广泛探索与有效利用之间取得平衡。大量实验表明，AMPO 显著优于强基线（GRPO），在数学推理任务上提高了4.3%，在非常规任务上提高了12.2%，同时显著提高了Pass@k 性能并实现了更多样化的探索。值得注意的是，使用四位同行大小的教师，我们的方法与利用单个更强大教师的方法（例如，DeepSeek-R1）相比取得了相当的结果，并且我们的方法使用了更多的数据。这些结果证明了一条更高效、更可扩展的通往卓越推理和泛化能力的道路。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02227v2">PDF</a> 20 pages, 5 figures</p>
<p><strong>摘要</strong></p>
<p>强化学习与可验证奖励（RLVR）范式有望增强大型语言模型（LLM）的推理能力。然而，当前主流方法主要依赖于自我探索或单一离线策略教师来激发长链思维（LongCoT）推理，这可能引入模型内在偏见并限制探索，最终限制推理的多样性和性能。受知识蒸馏中多教师策略的启发，我们引入了自适应多指导策略优化（AMPO），这是一个新的框架，能自适应地利用多个熟练教师模型的指导，但仅在策略内模型无法产生正确解决方案时才进行。这种“按需指导”的方法在扩大探索的同时保留了自我发现的价值。此外，AMPO还采用了一种基于理解的筛选机制，鼓励学生从它最可能理解的推理路径中学习，从而在广泛的探索与有效的利用之间取得平衡。实验表明，AMPO显著优于强大的基线（GRPO），在数学推理任务上提高了4.3%，在超出分布的任务上提高了12.2%，同时大大提高了Pass@k的性能并实现了更多样化的探索。值得注意的是，使用四位同级教师，我们的方法可以达到与利用单一更强大教师的方法（例如，DeepSeek-R1）相当的结果，而且我们的方法需要的数据更少。这些结果证明了一条更高效、更可扩展的实现卓越推理和泛化的路径。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SII-Enigma/AMPO上找到。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>强化学习与可验证奖励（RLVR）范式有望提升大型语言模型（LLM）的推理能力。</li>
<li>当前方法可能引入模型内在偏见并限制探索。</li>
<li>AMPO框架通过自适应地利用多个熟练教师模型的指导来改进现有方法。</li>
<li>AMPO在自我发现和探索之间找到了平衡，通过按需指导和基于理解的筛选机制来优化学习。</li>
<li>AMPO在数学和超出分布的任务上实现了显著的性能提升。</li>
<li>AMPO方法与使用单一更强大教师的方法相比具有相当的结果，但所需数据更少。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02227">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b844c96d963b1a485f000c3b7f74321d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145542&auth_key=1760145542-0-0-c9fcb7f2f0efb43dac08e0ccadf0583e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80b65aedb635e441d5057037ef27720a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145550&auth_key=1760145550-0-0-0d56f79328f8dfb3241b7dc93db3b8fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1570915f2e22666acbf89398f0dc69ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145557&auth_key=1760145557-0-0-a408a2415511a9f8db50370331e57ad4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Kimi-Dev-Agentless-Training-as-Skill-Prior-for-SWE-Agents"><a href="#Kimi-Dev-Agentless-Training-as-Skill-Prior-for-SWE-Agents" class="headerlink" title="Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents"></a>Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents</h2><p><strong>Authors:Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, Yanhao Li, Yue Liu, Zhenxing Hu, Kaitai Zhang, Shuyi Wang, Huarong Chen, Flood Sung, Yang Liu, Yang Gao, Zhilin Yang, Tianyu Liu</strong></p>
<p>Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents. </p>
<blockquote>
<p>大型语言模型（LLM）越来越多地被应用于软件工程（SWE），而SWE-bench是一个关键的基准测试。解决方案分为SWE-Agent框架，具有多轮互动，以及基于工作流的无需Agent的方法，具有单轮可验证步骤。我们认为这两种范式并不是相互排斥的：推理密集型的无需Agent的训练会引发技能优先权，包括定位、代码编辑和自我反思，这能够实现对SWE-Agent的有效和高效适应。在这项工作中，我们首先制定无需Agent的训练配方，并展示了Kimi-Dev，这是一个开源的SWE LLM，在SWE-bench Verified上达到60.4%，是工作流程方法中的最佳表现。通过对5k个公开可用的轨迹进行额外的SFT适应，Kimi-Dev使SWE-Agents达到48.6%的pass@1，与Claude 3.5 Sonnet（241022版本）持平。这些结果表明，来自无需Agent的训练的结构化技能优先权可以弥合工作流程和代理框架之间的差距，为可转移编码代理提供支撑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23045v2">PDF</a> 58 pages</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在软件工程（SWE）中的应用日益广泛，SWE-bench是其中的一项关键基准测试。解决方案包括基于多轮交互的SWE-Agent框架和基于工作流的无需Agent的单轮可验证步骤方法。本文认为这两种范式并非相互排斥：注重推理的无需Agent的训练会诱导技能优先权，包括定位、代码编辑和自我反思等功能，从而实现对SWE-Agent的有效适应。在此工作中，我们首先制定了无需Agent的训练配方，并推出了Kimi-Dev这一开源SWE LLM，在SWE-bench Verified上的表现达到60.4%，在工作流程方法中表现最佳。通过额外对5k公开轨迹进行SFT适应，Kimi-Dev驱动的SWE-Agents达到48.6%的pass@1，与Claude 3.5 Sonnet（241022版本）相当。结果表明，来自无需Agent的训练的结构化技能优先权可以在工作流程和agentic框架之间架起桥梁，为可转移的编码agent提供支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在软件工程中的应用日益重要，SWE-bench是评估其性能的关键基准。</li>
<li>软件工程的语言模型解决方案包括SWE-Agent框架和Agentless方法。</li>
<li>无需Agent的训练能诱导技能优先权，包括定位、代码编辑和自我反思等功能。</li>
<li>Kimi-Dev是一个开源的SWE LLM，在SWE-bench Verified上的表现优异。</li>
<li>Kimi-Dev通过SFT适应提高了SWE-Agents的性能，与最新技术相当。</li>
<li>结构化的技能优先权有助于弥合工作流程和agentic框架之间的差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cd2440f08bbf59a316cd59e6d78b8102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145564&auth_key=1760145564-0-0-4f57c303598ff3051b3994661683fa1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b9f3ec2f3f23d4b01d457b5530fe78a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145571&auth_key=1760145571-0-0-07cfe4a40eae1645a068ca029323f1c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-350aba9d3f02d29d655d9f39d6b7f47b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145577&auth_key=1760145577-0-0-732788fbc84c4f3b0266eb57846c345d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c66af6bf5ea5e152ecff9cc47d8d7efc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145584&auth_key=1760145584-0-0-9e83664e059b46e2107d17b0ceb42caa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f2de0cca086d805c8f5d2f8dd117e15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145590&auth_key=1760145590-0-0-b1fc4e36522f6d40a09c5cf888549c0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers"><a href="#Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers" class="headerlink" title="Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers"></a>Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers</h2><p><strong>Authors:Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao</strong></p>
<p>The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08% and 41.4% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search. </p>
<blockquote>
<p>将大型语言模型（LLM）集成到自动化定理证明中已显示出巨大的潜力，然而，它受到训练时间强化学习（RL）和推理时间计算扩展挑战的根本限制。本文介绍了<code>BFS-Prover-V2</code>系统，该系统旨在解决这种双重扩展问题。我们提出了两个主要的创新点。第一个是新型的多轮离线策略强化学习框架，旨在不断提高训练时LLM逐步证明的性能。该框架受到AlphaZero原则的启发，采用多阶段专家迭代管道，具有自适应战术级数据过滤和定期重新训练的功能，以克服通常限制长期RL的性能瓶颈，在基于LLM的代理中。第二个创新点是一个增强规划的多智能体搜索架构，该架构在推理时间扩展推理能力。该架构采用通用推理模型作为高级规划器，将复杂的定理迭代地分解为一系列更简单的子目标。这种分层方法大大减少了搜索空间，使一组并行证明智能体能够通过利用共享证明缓存来有效地协作。我们证明了这种双重扩展方法产生了最先进的成果，在正式的数学基准测试上表现出卓越性能。<code>BFS-Prover-V2</code>在MiniF2F和ProofNet测试集上分别达到了95.08%和41.4%的准确率。虽然该工作展示的是在形式数学领域的应用，但本工作提出的强化学习和推理技术具有更广泛的兴趣，并可应用于需要长期多轮推理和复杂搜索的其他领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06493v2">PDF</a> </p>
<p><strong>摘要</strong><br>LLMs融入自动化定理证明展现巨大潜力，但仍面临训练时强化学习（RL）和推理时计算的双重扩展挑战。《BFS-Prover-V2系统》旨在解决这一问题。该系统引入两大创新点：一是全新的多轮离线策略RL框架，旨在持续提高训练时LLM的性能；二是规划增强型多智能体搜索架构，提升推理能力。前者采用AlphaZero原理的迭代管道设计，利用自适应战术级数据过滤和定期再训练克服性能瓶颈；后者则通过通用推理模型作为高级规划器分解复杂定理序列。本文的RL和推理技术虽用于形式数学领域，但其广泛的应用潜力有望为其他领域带来启示。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>LLM在自动化定理证明中的集成展现出巨大潜力，面临的主要挑战是训练和推理时间规模的扩大。</li>
<li>引入的新型多轮离线策略RL框架通过模仿AlphaZero原理提高了LLM的性能。通过自适应战术级数据过滤和定期再训练来克服性能瓶颈。</li>
<li>规划增强型多智能体搜索架构能够扩展推理能力，该架构采用通用推理模型作为高级规划器分解复杂定理序列，显著减少搜索空间。</li>
<li>通过在形式数学领域的测试集上的实验，验证了系统的有效性，展示了在定理证明领域的先进成果。这种双策略方法在标准数学基准测试中达到了很高的表现水平。同时强调了其在其他领域的应用潜力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2077e21fd42b0dc8a812878ce7c79d2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145598&auth_key=1760145598-0-0-113deb51e98856b21bd07949104d6c8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84858c4f548f5303404a32f3ae383431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145606&auth_key=1760145606-0-0-e0d3c4137799e04751e95297b288c5ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf9d216a016536e180306c0133831217~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145613&auth_key=1760145613-0-0-4dcad429edf965ecd548ae626a5d9ddf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67fc817e10239aa01fd578650e9e33d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145620&auth_key=1760145620-0-0-206e324bfa7890849eab53833ffa280d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Flora-Effortless-Context-Construction-to-Arbitrary-Length-and-Scale"><a href="#Flora-Effortless-Context-Construction-to-Arbitrary-Length-and-Scale" class="headerlink" title="Flora: Effortless Context Construction to Arbitrary Length and Scale"></a>Flora: Effortless Context Construction to Arbitrary Length and Scale</h2><p><strong>Authors:Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu</strong></p>
<p>Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human&#x2F;LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/txchen-USTC/Flora%7D%7Bhttps://github.com/txchen-USTC/Flora%7D">https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}</a>. </p>
<blockquote>
<p>有效地处理长语境对于大型语言模型（LLM）来说是一个挑战，因为长文本较为罕见、计算需求高，并且短语境能力的遗忘也较多。近期的方法尝试构建长语境进行指令调整，但这些方法通常需要大型语言模型或人工干预，二者成本高昂，并且在长度和多样性方面存在限制。此外，当前长语境大型语言模型在短语境中的表现下降仍然显著。在本文中，我们介绍了Flora，这是一种无需人工&#x2F;大型语言模型参与的长语境构建策略。Flora可以通过基于类别任意组合短指令，并指令大型语言模型基于长语境元指令生成响应，从而显著增强大型语言模型的长语境表现。这使得Flora能够产生任意长度和规模、具有丰富多样性的上下文，同时只稍微影响短语境的表现。在Llama3-8B-Instruct和QwQ-32B上的实验表明，经过Flora增强的大型语言模型在三个长语境基准测试中表现出色，同时在短语境任务中保持强劲表现。我们的数据构建代码可在<a target="_blank" rel="noopener" href="https://github.com/txchen-USTC/Flora%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/txchen-USTC/Flora上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19786v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Flora的长语境构建策略，它可以在不依赖人工或大型语言模型的情况下，通过组合短指令和基于长语境元指令生成响应，有效提高大型语言模型处理长语境的性能。该方法能够构建任意长度和多样性的语境，同时只略微影响短语境的性能。实验表明，使用Flora增强的大型语言模型在长语境任务中表现优异，同时保持短语境任务的强性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在处理长语境时面临挑战，主要由于长文本稀少、计算需求高以及短语境能力的遗忘。</li>
<li>现有构建长语境的方法需要大型语言模型或人工参与，成本高昂且受限。</li>
<li>Flora是一种新型长语境构建策略，通过组合短指令和基于长语境的元指令生成响应。</li>
<li>Flora能构建任意长度和多样性的语境，同时只略微影响短语境性能。</li>
<li>实验证明，使用Flora增强的大型语言模型在长语境任务中表现优异。</li>
<li>Flora提供的数据构建代码可公开访问。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19786">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-92368b2743395338938ed651b568380b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145627&auth_key=1760145627-0-0-337f5cbec3365b85d58285d4fea5f7eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93ca07a36434365edab79dfc1e6dbcd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145634&auth_key=1760145634-0-0-6810337fa8f4938e5ec911772b54e10d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fcc7526047885bf32e2d600547c39ef5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145641&auth_key=1760145641-0-0-bc14bf71a2a942b25023092f9d1c1ffc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab931d5ef0b4478bcc98c1b1e799dd47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145648&auth_key=1760145648-0-0-0e933056bcd0dfa1860eaf198c043ae7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dca80d2fb7bb6c3ed7d3b00088b56d9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145655&auth_key=1760145655-0-0-331d4b0ad59188e26834d8bfe7dd270d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LDI-Localized-Data-Imputation-for-Text-Rich-Tables"><a href="#LDI-Localized-Data-Imputation-for-Text-Rich-Tables" class="headerlink" title="LDI: Localized Data Imputation for Text-Rich Tables"></a>LDI: Localized Data Imputation for Text-Rich Tables</h2><p><strong>Authors:Soroush Omidvartehrani, Davood Rafiei</strong></p>
<p>Missing values are pervasive in real-world tabular data and can significantly impair downstream analysis. Imputing them is especially challenging in text-rich tables, where dependencies are implicit, complex, and dispersed across long textual fields. Recent work has explored using Large Language Models (LLMs) for data imputation, yet existing approaches typically process entire tables or loosely related contexts, which can compromise accuracy, scalability, and explainability. We introduce LDI, a novel framework that leverages LLMs through localized reasoning, selecting a compact, contextually relevant subset of attributes and tuples for each missing value. This targeted selection reduces noise, improves scalability, and provides transparent attribution by revealing which data influenced each prediction. Through extensive experiments on real and synthetic datasets, we demonstrate that LDI consistently outperforms state-of-the-art imputation methods, achieving up to 8% higher accuracy with hosted LLMs and even greater gains with local models. The improved interpretability and robustness also make LDI well-suited for high-stakes data management applications. </p>
<blockquote>
<p>现实世界中的表格数据普遍存在缺失值，这可能会显著影响下游分析。在富含文本的表格中对缺失值进行估算尤其具有挑战性，因为这些表格中的依赖关系隐蔽、复杂且分散在较长的文本字段中。最近的工作已经探索使用大型语言模型（LLM）进行数据估算，但现有方法通常处理整个表格或松散相关的上下文，这可能会损害准确性、可扩展性和可解释性。我们介绍了LDI，这是一个新型框架，它通过局部推理利用LLM，为每个缺失值选择紧凑且上下文相关的属性子集和元组。这种有针对性的选择减少了噪声，提高了可扩展性，并通过揭示影响每个预测的数据，提供了透明的归属关系。我们在真实和合成数据集上进行了大量实验，结果表明，LDI始终优于最先进的估算方法，在使用托管LLM的情况下，其准确率提高了高达8%，使用本地模型则获得更大的收益。改进的可解释性和稳健性也使LDI非常适合于高风险数据管理应用程序。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16616v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了真实世界表格数据中缺失值的问题及其对数据分析和处理的负面影响。针对文本丰富的表格数据中的缺失值填充问题，提出了一种新型框架LDI，该框架利用局部推理选择相关属性子集和元组进行预测，提高了准确性、可扩展性和解释性。实验证明，LDI在真实和合成数据集上表现优异，相较于现有方法可提高高达8%的准确度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>缺失值在真实世界的表格数据中普遍存在，会对下游分析产生重大影响。</li>
<li>在文本丰富的表格中进行数据填充是一个挑战，因为依赖关系复杂且分散在较长的文本字段中。</li>
<li>LDI框架利用大型语言模型（LLMs）进行局部推理，针对每个缺失值选择相关的属性子集和元组进行处理。</li>
<li>LDI通过减少噪声、提高可扩展性和揭示预测背后的数据影响来提高准确性。</li>
<li>实验证明，LDI在真实和合成数据集上的表现优于现有的填充方法，准确率提高高达8%。</li>
<li>LDI通过利用托管的大型语言模型和本地模型，在高性能的同时也表现出了强大的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8ef6c79282d5edcffa4c14e5d0477c49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145663&auth_key=1760145663-0-0-89eb106553120cc46c1264ed63660d44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f76609c414cc79b591938fa5d6711357~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145670&auth_key=1760145670-0-0-f96842539e102b60f4d20bcde3e6d27e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82e8689a43f37d7079ba2eb308100b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145676&auth_key=1760145676-0-0-fe7d96d3a323368a2d4ccb353c9697ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3ad893d591d252082ac7d0dd75c6598~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145683&auth_key=1760145683-0-0-7c62c8ad794da09ac28e4ae7b2d8484c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56d5c1b5fc969405e9f6c09a876c2862~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145689&auth_key=1760145689-0-0-8c893eb62b710badc3d580c246709c0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Play-to-Generalize-Learning-to-Reason-Through-Game-Play"><a href="#Play-to-Generalize-Learning-to-Reason-Through-Game-Play" class="headerlink" title="Play to Generalize: Learning to Reason Through Game Play"></a>Play to Generalize: Learning to Reason Through Game Play</h2><p><strong>Authors:Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei</strong></p>
<p>Developing reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by literature suggesting that gameplay promotes transferable reasoning skills, we propose a novel post-training method, Visual Game Learning (ViGaL), where MLLMs develop generalizable reasoning skills through playing arcade-like games. Specifically, we show that training a 7B-parameter MLLM via reinforcement learning (RL) on simple games like Snake significantly enhances the downstream performance on multimodal math benchmarks like MathVista, on multi-discipline questions like MMMU and on 3D spatial reasoning benchmarks like VSI-Bench, without seeing any worked solutions, equations, or diagrams during RL. Remarkably, our model outperforms specialist models post-trained on benchmark-oriented multimodal reasoning data, while preserving the model’s performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest that multimodal reasoning can emerge from gameplay, pointing to a promising strategy of designing surrogate tasks for RL post-training. </p>
<blockquote>
<p>在多模态大型语言模型（MLLMs）中培养推理能力仍然是一个挑战。受文献启发，文献表明游戏玩法可以促进可迁移的推理技能，我们提出了一种新型的后训练方法——视觉游戏学习（ViGaL），其中MLLMs可以通过玩类似街机游戏来发展可推广的推理技能。具体来说，我们展示了通过强化学习（RL）在简单的游戏（如Snake）上训练一个拥有7B参数的MLLM，可以显著增强在诸如MathVista等多模态数学基准测试、MMMU等多学科问题以及VSI-Bench等3D空间推理基准测试上的下游性能，而无需在RL过程中查看任何解决方案、方程式或图表。值得注意的是，我们的模型在面向基准测试的多模态推理数据上进行了后训练，超越了专业模型的表现，同时保持了模型在一般视觉基准测试上的性能，这是一个专业模型经常表现不足的挑战。我们的研究结果表明，多模态推理可以源自游戏玩法，这指向了一种为RL后训练设计替代任务的有前途的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08011v4">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yunfeixie233.github.io/ViGaL/">https://yunfeixie233.github.io/ViGaL/</a></p>
<p><strong>Summary</strong></p>
<p>基于文献表明游戏可以促进可转移推理能力的观点，本研究提出了一种新型的基于游戏的后训练方式（ViGaL方法），通过类似于电玩游戏的活动来提升多模态大型语言模型（MLLMs）的通用推理能力。训练包含使用强化学习（RL）对含有如蛇梯游戏等简单游戏的7B参数MLLM进行训练，显著提高了在如MathVista等多模态数学基准测试、MMMU等多学科问题测试以及VSI-Bench的3D空间推理基准测试中的下游性能。值得注意的是，即使在RL过程中未见任何解决方案、方程式或图表，模型依然表现出卓越性能，甚至在某些情况下超越了专门针对基准测试的多模态推理数据后训练的模型，同时保持了模型在常规视觉基准测试上的性能表现。此发现预示着游戏对于训练具有强大推理能力的大型语言模型的应用前景，同时提供了为RL后训练设计替代任务的可行策略。</p>
<p><strong>Key Takeaways</strong></p>
<p>一、该研究提出了一种名为Visual Game Learning（ViGaL）的新型后训练方法，旨在通过类似于电玩游戏的活动提升多模态大型语言模型（MLLMs）的通用推理能力。<br>二、研究结果显示，使用强化学习（RL）训练模型，在简单的游戏如蛇梯游戏中进行训练，能显著提高模型在多个基准测试中的性能。<br>三、即使在没有接触任何解决方案、方程式或图表的情况下，模型依然能在各类测试中展现出卓越性能。<br>四、ViGaL方法在某些情况下超越了专门针对基准测试的多模态推理数据后训练的模型。<br>五、该模型在保持一般视觉基准测试性能的同时，能够在特定的多模态推理任务中展现出优异表现。<br>六、研究结果表明，游戏能够有助于训练具备强大推理能力的大型语言模型的开发，这对于未来的AI研究具有启示意义。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08011">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ac4ee991959206e5710079a8e250eb31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145696&auth_key=1760145696-0-0-20dc9d59ecd866ed69372892e21cbca2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cee785f65f3935b88b6db8cbd25e7e1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145704&auth_key=1760145704-0-0-903e8090b82a9eedcc116d0fddbacb82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f49dfdb0e8a00fd97d771febdcb53e7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145711&auth_key=1760145711-0-0-a254701bf68b4282bb75fc0f6f0383e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cac789441040cc27ccd0a2e10953bf68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145718&auth_key=1760145718-0-0-81fffbb322589b8e37d2a7eaef90f4e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OASIS-Online-Sample-Selection-for-Continual-Visual-Instruction-Tuning"><a href="#OASIS-Online-Sample-Selection-for-Continual-Visual-Instruction-Tuning" class="headerlink" title="OASIS: Online Sample Selection for Continual Visual Instruction Tuning"></a>OASIS: Online Sample Selection for Continual Visual Instruction Tuning</h2><p><strong>Authors:Minjae Lee, Minhyuk Seo, Tingyu Qu, Tinne Tuytelaars, Jonghyun Choi</strong></p>
<p>In continual instruction tuning (CIT) scenarios, where new instruction tuning data continuously arrive in an online streaming manner, training delays from large-scale data significantly hinder real-time adaptation. Data selection can mitigate this overhead, but existing strategies often rely on pretrained reference models, which are impractical in CIT setups since future data are unknown. Recent reference model-free online sample selection methods address this, but typically select a fixed number of samples per batch (e.g., top-k), making them vulnerable to distribution shifts where informativeness varies across batches. To address these limitations, we propose OASIS, an adaptive online sample selection approach for CIT that (1) selects informative samples by estimating each sample’s informativeness relative to all previously seen data, beyond batch-level constraints, and (2) minimizes informative redundancy of selected samples through iterative selection score updates. Experiments on various large foundation models show that OASIS, using only 25 percent of the data, achieves comparable performance to full-data training and outperforms the state-of-the-art sampling methods. </p>
<blockquote>
<p>在持续指令微调（CIT）场景中，新的指令微调数据以在线流的方式持续到达，大规模数据导致的训练延迟严重阻碍了实时适应。数据选择可以减轻这种开销，但现有策略通常依赖于预先训练的参考模型，这在CIT设置中并不实用，因为未来的数据是未知的。最近的无需参考模型的在线样本选择方法解决了这个问题，但通常每批选择固定数量的样本（例如，前k名），这使得它们在信息性随批次变化而变化的分布转移中容易受到影响。为了解决这些限制，我们提出了OASIS，这是一种适用于CIT的自适应在线样本选择方法，它（1）通过估计每个样本相对于所有之前数据的信息量来选择信息性样本，超越批次级别的约束；（2）通过迭代选择分数更新来最小化所选样本的信息冗余。在各种大型基础模型上的实验表明，OASIS仅使用25%的数据就能达到与全数据培训相当的性能，并优于最先进的采样方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02011v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在线连续指令调整（CIT）场景中，新指令调整数据以在线流方式持续到达时，大规模数据的训练延迟严重阻碍了实时适应。数据选择可以缓解这种开销，但现有策略通常依赖于预先训练好的参考模型，这在CIT设置中并不实用，因为未来数据未知。最近的参考模型无关的在线样本选择方法可以解决这一问题，但它们通常按批次选择固定数量的样本（例如前k名），这使得它们在信息分布跨批次变化时容易受到干扰。为解决这些限制，我们提出了OASIS，这是一种适用于CIT的自适应在线样本选择方法，它（1）通过估计每个样本相对于所有已见数据的信息量来选择信息样本，超越批次级别的约束；（2）通过迭代选择分数更新来减少所选样本的信息冗余。在大型基础模型上的实验表明，OASIS仅使用25%的数据即可实现与全数据训练相当的性能，并优于最新的采样方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在连续指令调整（CIT）场景中，训练延迟是一个主要问题，影响实时适应。</li>
<li>数据选择是缓解训练延迟的有效方法，但现有策略依赖于预训练参考模型，这在CIT中不实用。</li>
<li>最近的在线样本选择方法试图解决这一问题，但它们易受分布变化的干扰。</li>
<li>OASIS是一种自适应的在线样本选择方法，它通过估计每个样本的信息量并选择信息样本来解决上述问题。</li>
<li>OASIS超越批次级别约束，并通过迭代选择分数更新来减少信息冗余。</li>
<li>实验表明，OASIS使用较少的数据即可实现与全数据训练相当的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02011">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d5bda6702cea7b2c63cb2dfc7820f8d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145726&auth_key=1760145726-0-0-442fd54245333d3072058cf64c94cf21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a7904d0638d1c942c8b6c1faf5868a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145733&auth_key=1760145733-0-0-bb2a331ced0912f2f65f0a35074f0f32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7639ee03b02bcc86ff95a0f8b5b57b84~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145740&auth_key=1760145740-0-0-f8e59c35802ab4d5bcd0c23148f06c16&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Sherkala-Chat-Building-a-State-of-the-Art-LLM-for-Kazakh-in-a-Moderately-Resourced-Setting"><a href="#Sherkala-Chat-Building-a-State-of-the-Art-LLM-for-Kazakh-in-a-Moderately-Resourced-Setting" class="headerlink" title="Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a   Moderately Resourced Setting"></a>Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a   Moderately Resourced Setting</h2><p><strong>Authors:Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokulakrishnan Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Preslav Nakov</strong></p>
<p>Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outper-forming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. To ensure effective and responsible alignment, we leverage translated instruction datasets, a Kazakhstan-specific instruction dataset that is automatically constructed and manually verified, and Kazakh-specific safety data. We release Sherkala-Chat (8B) as an open-weight model, along with a detailed description of its training, alignment, and evaluation, to support research and real-world applications for Kazakh speakers. </p>
<blockquote>
<p>Llama-3.1-Sherkala-8B-Chat，简称Sherkala-Chat（8B），是一款针对哈萨克语设计的最新指令调优开放生成大型语言模型（LLM）。Sherkala-Chat（8B）旨在提高哈萨克语使用者对LLM进步的包容性。该模型改编自LLaMA-3.1-8B模型，并在哈萨克语、英语、俄语和土耳其语的45.3B标记上进行训练。拥有8亿参数，它在哈萨克语方面表现出强大的知识和推理能力，在类似规模的开放哈萨克语和多语种模型中表现卓越，同时在英语方面也有竞争力。为确保有效和负责任的对齐，我们利用翻译指令数据集、自动构建和人工验证的哈萨克斯坦特定指令数据集以及哈萨克特定安全数据。我们将Sherkala-Chat（8B）作为开放权重模型发布，同时提供其训练、对齐和评估的详细描述，以支持哈萨克语使用者的研究和实际应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01493v2">PDF</a> Accepted at COLM 2025</p>
<p><strong>Summary</strong></p>
<pre><code>Sherkala-Chat（8B）是一款面向哈萨克语的先进指令调优开源生成式大型语言模型（LLM）。它旨在提高哈萨克语说话者使用LLM的包容性。该模型基于LLaMA-3.1-8B模型改编，经过哈萨克语、英语、俄语和土耳其语共45.3B令牌训练。拥有8亿参数，在哈萨克语知识展现和推理能力上表现卓越，与类似规模的开源哈萨克语和多语种模型相比具有显著优势，同时在英语方面也有竞争力表现。为确保有效和负责任的对齐，研究团队利用翻译指令数据集、自动构建和人工验证的哈萨克斯坦特定指令数据集以及哈萨克特定安全数据。我们公开发布Sherkala-Chat（8B）作为开源权重模型，并详细介绍了其训练、对齐和评估的详细信息，以支持哈萨克语使用者的研究和实际应用。
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sherkala-Chat（8B）是一款面向哈萨克语的先进指令调优大型语言模型。</li>
<li>模型基于LLaMA-3.1-8B，进行了多语言训练，包括哈萨克语、英语、俄语和土耳其语。</li>
<li>Sherkala-Chat（8B）拥有强大的知识和推理能力，特别是在哈萨克语方面表现卓越。</li>
<li>与类似规模的模型相比，Sherkala-Chat（8B）在哈萨克语方面的性能具有显著优势，同时在英语方面也有竞争力。</li>
<li>为了确保模型的有效性和安全性，研究团队采用了多种数据来源，包括翻译指令数据集、特定指令数据集和安全数据。</li>
<li>Sherkala-Chat（8B）作为开源权重模型发布，支持研究和实际应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b51fa5bfd06495dd9978045968a3010f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145748&auth_key=1760145748-0-0-eface0193d2d326ac1ec768b500349d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-580b06d9a3b6c7494f3d0a1a0ebd752a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145755&auth_key=1760145755-0-0-01cf46340dbe25bc4c16c2cfdcaca7eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed7458ac1c5b66e0fa326c776609b86a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145761&auth_key=1760145761-0-0-3c5aba623a39936a0d0d795a517f2354&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-032121491e3224b8e9447a818f11faf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145768&auth_key=1760145768-0-0-c12fae783c952534446a1edee36a27a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-10-11  MATRIX Multimodal Agent Tuning for Robust Tool-Use Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d9d1ddb3c6472ed06fbff1a2c61fdce1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144625&auth_key=1760144625-0-0-affe5ab05b4f88b2c1413b36abaeda32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-11  MATRIX Multimodal Agent Tuning for Robust Tool-Use Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
