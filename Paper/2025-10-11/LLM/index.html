<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  NaViL Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-a7ef7465fa414c8bad36ed951b897930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145180&auth_key=1760145180-0-0-1a40e4df6f93262ee4e95af44576d06e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-11-æ›´æ–°"><a href="#2025-10-11-æ›´æ–°" class="headerlink" title="2025-10-11 æ›´æ–°"></a>2025-10-11 æ›´æ–°</h1><h2 id="NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints"><a href="#NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints" class="headerlink" title="NaViL: Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints"></a>NaViL: Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints</h2><p><strong>Authors:Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai</strong></p>
<p>Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å®é™…èŒƒå¼é€šå¸¸é‡‡ç”¨ç»„åˆè®­ç»ƒï¼Œå…¶ä¸­é€šè¿‡è¿ç»­çš„å¤šæ¨¡æ€é¢„è®­ç»ƒå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿æ¥èµ·æ¥ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ˜¯åˆ†ç¦»çš„ï¼Œè¯¥èŒƒå¼çš„å¤šæ¨¡æ€æ‰©å±•å±æ€§ä»ç„¶éš¾ä»¥æ¢ç´¢ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨MLLMsçš„ç«¯åˆ°ç«¯åŸç”Ÿè®­ç»ƒï¼Œå¹¶åœ¨å®é™…è®¾ç½®ï¼ˆå³æ•°æ®çº¦æŸï¼‰ä¸‹ç³»ç»Ÿåœ°ç ”ç©¶å…¶è®¾è®¡ç©ºé—´å’Œæ‰©å±•å±æ€§ã€‚é€šè¿‡å¯¹MLLMä¸­å„ç§é€‰æ‹©çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è·å¾—äº†æœ€ä½³å…ƒæ¶æ„ï¼Œè¯¥æ¶æ„åœ¨æ€§èƒ½å’Œè®­ç»ƒæˆæœ¬ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚ä¹‹åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”ŸMLLMçš„æ‰©å±•å±æ€§ï¼Œå¹¶æŒ‡å‡ºäº†è§†è§‰ç¼–ç å™¨å’ŒLLMsä¹‹é—´æ­£ç›¸å…³çš„æ‰©å±•å…³ç³»ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºNaViLçš„åŸç”ŸMLLMï¼Œå¹¶ç»“åˆäº†ä¸€ç§ç®€å•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„é…æ–¹ã€‚åœ¨14ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†NaViLä¸ç°æœ‰MLLMsçš„ç«äº‰æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å‘ç°å’Œç»“æœè¿˜ä¸ºæœªæ¥åŸç”ŸMLLMçš„ç ”ç©¶æä¾›äº†æ·±å…¥è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08565v1">PDF</a> Accepted by NeurIPS 2025. 22 pages, link:   <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/NaViL">https://github.com/OpenGVLab/NaViL</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åŸç”Ÿè®­ç»ƒæ–¹å¼ï¼Œå¹¶ç³»ç»Ÿåœ°æ¢è®¨äº†å…¶åœ¨å®ç”¨åœºæ™¯ä¸‹çš„è®¾è®¡ç©ºé—´å’Œæ‰©å±•å±æ€§ã€‚é€šè¿‡å¯¹MLLMä¸­å„ç§é€‰æ‹©çš„ç ”ç©¶ï¼Œè·å¾—äº†èƒ½æœ€ä½³å¹³è¡¡æ€§èƒ½å’Œè®­ç»ƒæˆæœ¬çš„å…ƒæ¶æ„ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”ŸMLLMçš„æ‰©å±•å±æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºNaViLçš„åŸç”ŸMLLMï¼Œå®ƒåœ¨å¤šä¸ªæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šå¸¸é‡‡ç”¨ç»„åˆè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡è¿ç»­å¤šæ¨¡æ€é¢„è®­ç»ƒå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿æ¥èµ·æ¥ã€‚</li>
<li>åŸç”Ÿè®­ç»ƒæ–¹å¼æ˜¯MLLMsçš„ä¸€ç§é‡è¦è®­ç»ƒèŒƒå¼ï¼Œæœ¬æ–‡å¯¹å…¶è¿›è¡Œç ”ç©¶å¹¶ç³»ç»Ÿåœ°æ¢è®¨äº†å…¶è®¾è®¡ç©ºé—´å’Œæ‰©å±•å±æ€§ã€‚</li>
<li>åœ¨æ•°æ®å—é™çš„å®ç”¨åœºæ™¯ä¸‹ï¼Œæœ¬æ–‡è·å¾—äº†èƒ½æœ€ä½³å¹³è¡¡æ€§èƒ½å’Œè®­ç»ƒæˆæœ¬çš„å…ƒæ¶æ„ã€‚</li>
<li>æ–‡ç« è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”ŸMLLMçš„æ‰©å±•å±æ€§ï¼Œå‘ç°è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚</li>
<li>åŸºäºç ”ç©¶ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºNaViLçš„åŸç”ŸMLLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>NaViLæ¨¡å‹å…·æœ‰ç®€å•ä¸”ç»æµå®æƒ çš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fffa53aafc2dd31fab067e1d0077084a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145187&auth_key=1760145187-0-0-d2465ccbee04b35c67796e2f80ef9c8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73343bac7a7291d33aa09ae469326982~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145195&auth_key=1760145195-0-0-d217fc4abab39232130d3792462736e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9456766951dc75cc5c567cd68e558284~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145202&auth_key=1760145202-0-0-eea3a50838de6c1c4ec97b95b690a348&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-493cb0f288ee301e28c6d94611f182e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145209&auth_key=1760145209-0-0-24aaab62a6f8d8904e2d71c853d1ab22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c4ed9fb4370e1b9f682306cb7aa1597~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145215&auth_key=1760145215-0-0-026865b30d3ecafadfcd898a141a86e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62c2fe3d5be6c240eae88c1f3788b56d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145222&auth_key=1760145222-0-0-4c28aaade5718bf715d63fee69b0eeda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-562c1b87ac6b13f3c4b82f07865d50a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145228&auth_key=1760145228-0-0-8c72f8e7b92a4de964e5bae39d757591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Reasoning-for-Diffusion-Language-Models-via-Group-Diffusion-Policy-Optimization"><a href="#Improving-Reasoning-for-Diffusion-Language-Models-via-Group-Diffusion-Policy-Optimization" class="headerlink" title="Improving Reasoning for Diffusion Language Models via Group Diffusion   Policy Optimization"></a>Improving Reasoning for Diffusion Language Models via Group Diffusion   Policy Optimization</h2><p><strong>Authors:Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</strong></p>
<p>Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce \textbf{Group Diffusion Policy Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks. </p>
<blockquote>
<p>æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰èƒ½å¤Ÿå®ç°å¹¶è¡Œã€æ— åºç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–ï¼Œä¸ºè‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºæ¦‚ç‡è®¡ç®—ä¸å¯è¡Œï¼Œå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒé€‚åº”åˆ°DLMsä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¼€åˆ›æ€§å·¥ä½œå¦‚diffu-GRPOé€šè¿‡ä¸€æ­¥å»æ©ç æ¥ä¼°è®¡tokençº§æ¦‚ç‡ã€‚è™½ç„¶è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä¸¥é‡åè§ã€‚æ›´åŸåˆ™æ€§çš„åŸºç¡€åœ¨äºåºåˆ—çº§æ¦‚ç‡ï¼Œå…¶ä¸­è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ä½œä¸ºæ›¿ä»£ç‰©ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰è¿™ç§æ¸…æ™°çš„æ•°å­¦è”ç³»ï¼ŒåŸºäºELBOçš„æ–¹æ³•ç”±äºæ¦‚ç‡è¯„ä¼°çš„æ˜‚è´µæˆæœ¬è€Œåº”ç”¨æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ELBOä¼°è®¡å¹¶åˆ†è§£äº†å…¶æ–¹å·®æ¥æºã€‚è¿™ç§åˆ†è§£ä¿ƒä½¿æˆ‘ä»¬é€šè¿‡ä¸€äº›å…³é”®ç»´åº¦ä¸Šçš„å¿«é€Ÿç¡®å®šæ€§ç§¯åˆ†è¿‘ä¼¼æ¥å‡å°‘æ–¹å·®ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹DLMså®šåˆ¶çš„å…¨æ–°RLç®—æ³•â€”â€”**Group Diffusion Policy Optimization (GDPO)**ã€‚GDPOåˆ©ç”¨ç®€å•æœ‰æ•ˆçš„åŠç¡®å®šæ€§è’™ç‰¹å¡æ´›æ–¹æ¡ˆï¼Œç¼“è§£äº†ELBOä¼°è®¡å™¨åœ¨çº¯åŒé‡è’™ç‰¹å¡æ´›é‡‡æ ·ä¸‹çš„æ–¹å·®çˆ†ç‚¸é—®é¢˜ï¼Œåœ¨ä¸¥æ ¼çš„è¯„ä¼°é¢„ç®—ä¸‹ï¼Œæä¾›äº†ä¸€ä¸ªå¯è¯æ˜çš„ä½æ–¹å·®ä¼°è®¡å™¨ã€‚ç»éªŒä¸Šï¼ŒGDPOåœ¨é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šå®ç°äº†æŒç»­çš„æ”¶ç›Šï¼Œå¹¶åœ¨å¤§å¤šæ•°æ•°å­¦ã€æ¨ç†å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿ä¹‹ä¸€diffu-GRPOã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰é€šè¿‡å¹¶è¡Œã€é¡ºåºæ— å…³ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–æä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºä¸ç¡®å®šæ€§å¤ªå¤§ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒåœ¨DLMsä¸­çš„åº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç›®å‰å¤§å¤šæ•°å·¥ä½œå¦‚diffu-GRPOéƒ½æ˜¯åŸºäºä¸€æ­¥è§£æ©ç è¿›è¡Œtokençº§åˆ«çš„å¯èƒ½æ€§ä¼°è®¡ï¼Œè¿™è™½è®¡ç®—é«˜æ•ˆï¼Œä½†æœ‰ä¸¥é‡åè§ã€‚åŸºäºåºåˆ—çº§åˆ«å¯èƒ½æ€§çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰æ›´ä¸ºåŸåˆ™æ€§ï¼Œä½†ç”±äºè¯„ä¼°å¯èƒ½æ€§æˆæœ¬é«˜æ˜‚ï¼Œå…¶åº”ç”¨å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†ELBOä¼°è®¡å¹¶åˆ†è§£äº†å…¶æ–¹å·®æ¥æºã€‚åŸºäºæ­¤åˆ†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹DLMsçš„æ–°çš„RLç®—æ³•â€”â€”é›†å›¢æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆGDPOï¼‰ã€‚GDPOåˆ©ç”¨ç®€å•æœ‰æ•ˆçš„åŠç¡®å®šæ€§è’™ç‰¹å¡æ´›æ–¹æ¡ˆï¼Œç¼“è§£äº†æœ´ç´ åŒé‡è’™ç‰¹å¡æ´›é‡‡æ ·ä¸‹ELBOä¼°è®¡é‡çš„æ–¹å·®çˆ†ç‚¸é—®é¢˜ï¼Œåœ¨ä¸¥æ ¼è¯„ä¼°é¢„ç®—ä¸‹æä¾›äº†ä¸€ä¸ªæ–¹å·®æ›´ä½çš„ä¼°è®¡é‡ã€‚ç»éªŒä¸Šï¼ŒGDPOåœ¨é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šå–å¾—äº†æŒç»­çš„æ”¶ç›Šï¼Œå¹¶åœ¨å¤§å¤šæ•°æ•°å­¦ã€æ¨ç†å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºçº¿ä¹‹ä¸€diffu-GRPOã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰æä¾›çµæ´»çš„ç”Ÿæˆæ–¹å¼ï¼Œå¯ä»¥é€šè¿‡å¹¶è¡Œå¤„ç†è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨DLMsä¸­çš„åº”ç”¨é¢ä¸´ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚diffu-GRPOä¸»è¦åŸºäºtokençº§åˆ«çš„å¯èƒ½æ€§ä¼°è®¡ï¼Œå­˜åœ¨åè§é—®é¢˜ã€‚</li>
<li>åŸºäºåºåˆ—çº§åˆ«çš„ELBOä¼°è®¡æ›´ä¸ºåŸåˆ™æ€§ï¼Œä½†ç”±äºæˆæœ¬é«˜æ˜‚åº”ç”¨å—é™ã€‚</li>
<li>æœ¬æ–‡åˆ†è§£äº†ELBOä¼°è®¡çš„æ–¹å·®æ¥æºï¼Œå¹¶å¼•å…¥äº†æ–°çš„RLç®—æ³•GDPOæ¥è§£å†³DLMsä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚</li>
<li>GDPOåˆ©ç”¨åŠç¡®å®šæ€§è’™ç‰¹å¡æ´›æ–¹æ¡ˆç¼“è§£æ–¹å·®çˆ†ç‚¸é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cb590f5a4a7334f71436a8a106aba1a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145235&auth_key=1760145235-0-0-b1bd92dc5c4d58eb0aa6cbd6e7a41932&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94ffa7476b939b90c924578864a82ee1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145242&auth_key=1760145242-0-0-bdc42c8f883101e67bc055240d134a41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards"><a href="#CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards" class="headerlink" title="CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"></a>CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</h2><p><strong>Authors:Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai</strong></p>
<p>Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agentâ€™s policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents. </p>
<blockquote>
<p>è‡ªæˆ‘è¿›åŒ–æ˜¯ä½¿åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†åœ¨é¢„è®­ç»ƒåèƒ½å¤ŸæŒç»­æé«˜å…¶èƒ½åŠ›çš„ä¸€ä¸ªæ ¸å¿ƒç ”ç©¶è¯¾é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶è§è¯äº†ä»éå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ°åŸºäºRLçš„æ–¹æ³•çš„è½¬å˜ã€‚å½“å‰çš„åŸºäºRLçš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºå¯†é›†çš„å¤–éƒ¨å¥–åŠ±ä¿¡å·ï¼Œè¦ä¹ˆä»LLMæœ¬èº«ä¸­æå–å†…åœ¨å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸äººç±»æ™ºèƒ½ä¸­çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶ç›¸æ‚–ï¼Œä¸ªäººé€šè¿‡ç›¸äº’è®¨è®ºå’Œåä½œæ¥å­¦ä¹ å’Œæé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ååŒè¿›åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆCoMASï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿æ™ºèƒ½ä½“é€šè¿‡ä»æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’å­¦ä¹ æ¥æé«˜è‡ªä¸»æ€§ï¼Œè€Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚CoMASä»ä¸°å¯Œçš„è®¨è®ºåŠ¨æ€ä¸­äº§ç”Ÿå†…åœ¨å¥–åŠ±ï¼Œé‡‡ç”¨LLMä½œä¸ºè£åˆ¤æœºåˆ¶æ¥åˆ¶å®šè¿™äº›å¥–åŠ±ï¼Œå¹¶é€šè¿‡RLä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œä»è€Œå®ç°å»ä¸­å¿ƒåŒ–å’Œå¯æ‰©å±•çš„ååŒè¿›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMASå§‹ç»ˆä¼˜äºæœªè®­ç»ƒçš„ä»£ç†ï¼Œå¹¶åœ¨å¤§å¤šæ•°è¯„ä¼°ç¯å¢ƒä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¯å®äº†åŸºäºäº¤äº’çš„å¥–åŠ±ä¿¡å·çš„å¿…è¦æ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºéšç€æ™ºèƒ½ä½“æ•°é‡å’Œå¤šæ ·æ€§çš„å¢åŠ ï¼Œå…¶å‰æ™¯å¹¿é˜”çš„å¯æ‰©å±•æ€§ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†CoMASåœ¨LLMä»£ç†ä¸­çš„è‡ªæˆ‘è¿›åŒ–çš„æ–°é¢–è€Œæœ‰æ•ˆçš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08529v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªæˆ‘è¿›åŒ–æ˜¯ä½¿åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†åœ¨é¢„è®­ç»ƒåæŒç»­æé«˜å…¶èƒ½åŠ›çš„ä¸€ä¸ªæ ¸å¿ƒç ”ç©¶è¯¾é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶ç»å†äº†ä»æ— å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ°åŸºäºRLçš„æ–¹æ³•çš„è½¬å˜ã€‚å½“å‰åŸºäºRLçš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºå¯†é›†çš„å¤–éƒ¨å¥–åŠ±ä¿¡å·ï¼Œè¦ä¹ˆä»LLMæœ¬èº«ä¸­æå–å†…åœ¨å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åç¦»äº†äººç±»æ™ºèƒ½ä¸­çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶ï¼Œä¸ªäººé€šè¿‡ç›¸äº’è®¨è®ºå’Œåä½œå­¦ä¹ å’Œæé«˜ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ååŒè¿›åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆCoMASï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡ç›¸äº’é—´çš„äº¤äº’å­¦ä¹ è€Œè‡ªä¸»æé«˜ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚CoMASä»ä¸°å¯Œçš„è®¨è®ºåŠ¨æ€ä¸­äº§ç”Ÿå†…åœ¨å¥–åŠ±ï¼Œé‡‡ç”¨LLMä½œä¸ºæ³•å®˜çš„æœºåˆ¶æ¥åˆ¶å®šè¿™äº›å¥–åŠ±ï¼Œå¹¶é€šè¿‡RLä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œä»è€Œå®ç°åˆ†æ•£å’Œå¯æ‰©å±•çš„ååŒè¿›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºLLMçš„ä»£ç†çš„è‡ªæˆ‘è¿›åŒ–æ˜¯ç ”ç©¶çš„é‡ç‚¹ï¼Œæ—¨åœ¨ä½¿å…¶åœ¨é¢„è®­ç»ƒåæŒç»­æé«˜èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä»å¯†é›†å¤–éƒ¨å¥–åŠ±ä¿¡å·æˆ–LLMæœ¬èº«æå–å†…åœ¨å¥–åŠ±ä¿¡å·ã€‚</li>
<li>äººç±»æ™ºèƒ½ä¸­çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶æ¶‰åŠä¸ªä½“é€šè¿‡ç›¸äº’è®¨è®ºå’Œåä½œå­¦ä¹ å’Œæé«˜ã€‚</li>
<li>CoMASæ¡†æ¶ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡ç›¸äº’é—´çš„äº¤äº’å­¦ä¹ è€Œè‡ªä¸»æé«˜ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚</li>
<li>CoMASç”Ÿæˆå†…åœ¨å¥–åŠ±æ¥è‡ªä¸°å¯Œçš„è®¨è®ºåŠ¨æ€ï¼Œå¹¶é‡‡ç”¨LLMä½œä¸ºæ³•å®˜æ¥åˆ¶å®šå¥–åŠ±ã€‚</li>
<li>CoMASé€šè¿‡RLä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œå®ç°åˆ†æ•£å’Œå¯æ‰©å±•çš„ååŒè¿›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c6c34837bff0f4ad8d24310829b02e6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145250&auth_key=1760145250-0-0-467a9b45de2491db22741d24d2b05f95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-949738b7676f3792c403fa91a7b3982e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145257&auth_key=1760145257-0-0-9146d0d7123b1c187a4b31dd5ef31525&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e70f4c8c542fc2eed746a939aef97ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145263&auth_key=1760145263-0-0-e80edce6d02c3bf353decde88f7dda66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AutoMLGen-Navigating-Fine-Grained-Optimization-for-Coding-Agents"><a href="#AutoMLGen-Navigating-Fine-Grained-Optimization-for-Coding-Agents" class="headerlink" title="AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents"></a>AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</h2><p><strong>Authors:Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai</strong></p>
<p>Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-Innovator/InternAgent">https://github.com/Alpha-Innovator/InternAgent</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æœºå™¨å­¦ä¹ å·¥ç¨‹ï¼ˆMLEï¼‰åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰å’ŒKaggleç«èµ›ä¸­ï¼Œå®ç°é«˜æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºä¸“å®¶çš„ä»‹å…¥å’Œé‡å¤è°ƒæ•´ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ç”Ÿæˆæ­£ç¡®çš„ä»£ç ã€‚å½“ç›´æ¥åº”ç”¨äºè¿™äº›ä»»åŠ¡æ—¶ï¼ŒLLMå¾€å¾€ç¼ºä¹ç²¾ç»†çš„åŸŸå…ˆéªŒçŸ¥è¯†ï¼Œè€Œç°æœ‰çš„MLEæ–¹æ³•ä½¿ç”¨çº¿æ€§æˆ–æ ‘çŠ¶æœç´¢ï¼Œå°†çŸ¥è¯†è½¬ç§»é™åˆ¶åœ¨ç›¸é‚»çš„å±‚æ¬¡é“¾æ¥ä¸Šã€‚å› æ­¤ï¼Œå®ƒä»¬æ— æ³•åˆ©ç”¨è¿‡å»çš„å®Œæ•´è½¨è¿¹æˆ–åœ¨å„åˆ†æ”¯ä¹‹é—´å…±äº«ä¿¡æ¯ï¼Œé™åˆ¶äº†è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›å’Œæœç´¢ç©ºé—´çš„å¤šæ ·æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08511v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æœºå™¨å­¦ä¹ å·¥ç¨‹ï¼ˆMLEï¼‰åœºæ™¯å¦‚AutoMLå’ŒKaggleç«èµ›ä¸­ï¼Œå®ç°é«˜æ€§èƒ½æ›´å¤šåœ°ä¾èµ–äºä¸“å®¶å¹²é¢„å’Œé‡å¤è°ƒæ•´ï¼Œè€Œéä»…ç”Ÿæˆæ­£ç¡®ä»£ç ã€‚é’ˆå¯¹LLMåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†åŸºäºLLMçš„ç¼–ç ä»£ç†AutoMLGenï¼Œé€šè¿‡æ•´åˆé¢†åŸŸçŸ¥è¯†åº“å’Œè’™ç‰¹å¡æ´›å›¾æœç´¢ï¼ˆMCGSï¼‰æ¥æé«˜æ€§èƒ½ã€‚MCGSä¿ç•™äº†è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æ ‘ç»“æ„æŒ‡å¯¼æ¢ç´¢ï¼ŒåŒæ—¶åœ¨æ‰©å±•é˜¶æ®µåµŒå…¥å›¾ç»“æ„ï¼Œå®ç°åŠ¨æ€è·¯å¾„é‡ç»„ã€å†å²è½¨è¿¹å¤ç”¨å’Œå¤šè§£èåˆï¼Œæ”¯æŒè‡ªæˆ‘è¿›åŒ–å’Œåä½œå­¦ä¹ ã€‚åœ¨MLE-Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAutoMLGenåœ¨å¹³å‡å¥–ç‰Œç‡å’Œæœ‰æ•ˆæäº¤ç‡ç­‰å¤šä¸ªç»´åº¦å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä¸”åœ¨12å°æ—¶é¢„ç®—å†…ï¼ˆä»…ä¸ºæ ‡å‡†è¿è¡Œæ—¶çš„ä¸€åŠï¼‰è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨MLEåœºæ™¯ä¸­é¢ä¸´ä¸“å®¶å¹²é¢„å’Œé‡å¤è°ƒæ•´çš„éœ€æ±‚è¾ƒé«˜ï¼Œç›´æ¥åº”ç”¨æ—¶ç¼ºä¹ç²¾ç»†é¢†åŸŸå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ç°æœ‰MLEæ–¹æ³•ä½¿ç”¨çº¿æ€§æˆ–æ ‘çŠ¶ç»“æ„æœç´¢ï¼Œé™åˆ¶äº†çŸ¥è¯†è½¬ç§»å’Œæœç´¢ç©ºé—´å¤šæ ·æ€§ã€‚</li>
<li>AutoMLGenæ˜¯ä¸€ä¸ªåŸºäºLLMçš„ç¼–ç ä»£ç†ï¼Œé›†æˆäº†é¢†åŸŸçŸ¥è¯†åº“å’Œè’™ç‰¹å¡æ´›å›¾æœç´¢ï¼ˆMCGSï¼‰ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>MCGSç»“åˆäº†æ ‘ç»“æ„æŒ‡å¯¼çš„æ¢ç´¢å’Œå›¾ç»“æ„åµŒå…¥ï¼Œå®ç°åŠ¨æ€è·¯å¾„é‡ç»„ã€å†å²è½¨è¿¹å¤ç”¨å’Œå¤šè§£èåˆã€‚</li>
<li>AutoMLGenæ”¯æŒè‡ªæˆ‘è¿›åŒ–å’Œåä½œå­¦ä¹ ï¼Œæé«˜äº†ç¨³å®šæ€§å¹¶åŠ é€Ÿäº†æ”¶æ•›ã€‚</li>
<li>åœ¨MLE-Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAutoMLGenåœ¨å¤šä¸ªç»´åº¦è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2f78f8ab4f3917a7f0f379e1d3b0dd5e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145271&auth_key=1760145271-0-0-cdce8cd0dc472737a2b2ca7add330b0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d63880a4d4f492ba270657dbc5b72de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145279&auth_key=1760145279-0-0-5b8ad3e4816fd254b34c63bcb9245ec7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance"><a href="#InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance" class="headerlink" title="InstructX: Towards Unified Visual Editing with MLLM Guidance"></a>InstructX: Towards Unified Visual Editing with MLLM Guidance</h2><p><strong>Authors:Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He</strong></p>
<p>With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿‘æœŸè¿›å±•ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œäººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£å°†å®ƒä»¬ç”¨äºæé«˜æ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ€§èƒ½ã€‚å°½ç®¡è¿›å±•è¿…é€Ÿï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ç¼ºä¹å¯¹MLLMè®¾è®¡é€‰æ‹©çš„æ·±å…¥åˆ†æã€‚æ­¤å¤–ï¼Œå°†MLLMså’Œæ‰©æ•£æ¨¡å‹é›†æˆåœ¨ä¸€èµ·åœ¨æŸäº›å›°éš¾çš„ä»»åŠ¡ï¼ˆå¦‚è§†é¢‘ç¼–è¾‘ï¼‰ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†InstructXï¼Œä¸€ä¸ªç”¨äºå›¾åƒå’Œè§†é¢‘ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹é›†æˆMLLMså’Œæ‰©æ•£æ¨¡å‹ä»¥è¿›è¡ŒæŒ‡ä»¤é©±åŠ¨ç¼–è¾‘è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œæ¶‰åŠå¤šç§ä»»åŠ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åˆ†æäº†å›¾åƒå’Œè§†é¢‘åœ¨ç»Ÿä¸€å»ºæ¨¡ä¸­çš„åˆä½œå’ŒåŒºåˆ«ã€‚ï¼ˆ1ï¼‰æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥åœ¨æ²¡æœ‰æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å¯¼è‡´å‡ºç°è§†é¢‘ç¼–è¾‘èƒ½åŠ›ï¼Œä»è€Œå‡è½»äº†ç¨€ç¼ºè§†é¢‘è®­ç»ƒæ•°æ®å¸¦æ¥çš„çº¦æŸã€‚ï¼ˆ2ï¼‰é€šè¿‡èå…¥æ¨¡æ€ç‰¹å®šçš„MLLMç‰¹æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¤„ç†å¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿‘æœŸè¿›å±•ï¼Œå…¶åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œäººä»¬å¼€å§‹å…³æ³¨å°†å…¶ç”¨äºæé«˜æ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†InstructXï¼Œä¸€ä¸ªç”¨äºå›¾åƒå’Œè§†é¢‘ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ·±å…¥ç ”ç©¶äº†å°†MLLMså’Œæ‰©æ•£æ¨¡å‹é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥è¿›è¡ŒæŒ‡ä»¤é©±åŠ¨çš„ç¼–è¾‘ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥æ¶Œç°å‡ºè§†é¢‘ç¼–è¾‘èƒ½åŠ›ï¼Œä¸”é€šè¿‡èå…¥æ¨¡æ€ç‰¹å®šMLLMç‰¹å¾ï¼Œèƒ½å¤Ÿç»Ÿä¸€å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤„ç†å¤šç§å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>InstructXæ¡†æ¶è¢«æå‡ºï¼Œç”¨äºæ•´åˆMLLMså’Œæ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ã€‚</li>
<li>åœ¨å›¾åƒæ•°æ®ä¸Šè®­ç»ƒå¯ä»¥ä½¿å¾—æ¨¡å‹å…·å¤‡è§†é¢‘ç¼–è¾‘èƒ½åŠ›ï¼Œæ— éœ€æ˜ç¡®çš„è§†é¢‘ç›‘ç£ã€‚</li>
<li>é€šè¿‡èå…¥æ¨¡æ€ç‰¹å®šçš„MLLMç‰¹å¾ï¼Œå›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡å¯ä»¥åœ¨å•ä¸€æ¨¡å‹å†…å¾—åˆ°ç»Ÿä¸€å¤„ç†ã€‚</li>
<li>InstructXæ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤šç§å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®éªŒä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4bc8d3a7c558ea5a8120c4757cea3a75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145286&auth_key=1760145286-0-0-9d05afd3cc7cf13f5efa8ab440eed045&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-afc26c2a7a2e1d4b68777f798f0935b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145293&auth_key=1760145293-0-0-cbcf8944ef0a4bf8f515fe0c7b5038d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97399e335a9ea984d56386d3e3bd9cf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145300&auth_key=1760145300-0-0-d7c7453e5d59e8ff098aaad84c12a287&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f1020c946a37973f231072e01c4450c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145306&auth_key=1760145306-0-0-59fe9eb100731578f018c810a9ffbe46&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d255793f2e47ee8b95ffad0c69e3a50c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145313&auth_key=1760145313-0-0-ccb70c412308d62d12b775696b665eba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33567bcefd0370480eaba7aafc3b11bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145319&auth_key=1760145319-0-0-0caf110c7b753ebab097ad6505751afe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b35b1ab4a5866e0514d47e3dcf7405d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145325&auth_key=1760145325-0-0-c14a50cebf00255b871d49f732607209&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a533e74bd38a606086428a5aa5c12b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145332&auth_key=1760145332-0-0-ad0b308f90c80b5336c2d817631700e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy"><a href="#DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy" class="headerlink" title="DeepPrune: Parallel Scaling without Inter-trace Redundancy"></a>DeepPrune: Parallel Scaling without Inter-trace Redundancy</h2><p><strong>Authors:Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li</strong></p>
<p>Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy â€“ our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: <a target="_blank" rel="noopener" href="https://deepprune.github.io/">https://deepprune.github.io/</a> </p>
<blockquote>
<p>å¹¶è¡Œæ‰©å±•å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œé€šè¿‡åŒæ—¶ç”Ÿæˆå¤šä¸ªæ€ç»´é“¾è¿½è¸ªï¼ˆCoTï¼‰ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç”±äºè¿½è¸ªä¹‹é—´çš„å†—ä½™è€Œå¼•å…¥äº†é‡å¤§çš„è®¡ç®—æ•ˆç‡ä½ä¸‹â€”â€”æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¶…è¿‡80%çš„å¹¶è¡Œæ¨ç†è½¨è¿¹äº§ç”Ÿäº†ç›¸åŒçš„æœ€ç»ˆç­”æ¡ˆï¼Œä»£è¡¨äº†å¤§é‡çš„è®¡ç®—æµªè´¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®çš„æ•ˆç‡ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†DeepPruneï¼Œä¸€ä¸ªé€šè¿‡åŠ¨æ€ä¿®å‰ªå®ç°é«˜æ•ˆå¹¶è¡Œæ‰©å±•çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ä¸€ä¸ªä½¿ç”¨ç„¦ç‚¹æŸå¤±å’Œè¿‡é‡‡æ ·æŠ€æœ¯è®­ç»ƒçš„ä¸“ç”¨æ³•å®˜æ¨¡å‹ä¸ºç‰¹è‰²ï¼Œå¯ä»¥å‡†ç¡®åœ°é¢„æµ‹ä»éƒ¨åˆ†æ¨ç†è½¨è¿¹å¾—å‡ºçš„ç­”æ¡ˆæ˜¯å¦ç­‰æ•ˆã€‚æ­¤æ¨¡å‹å®ç°äº†é¢„æµ‹ç­‰æ•ˆçš„å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯å€¼ï¼ˆAUROCï¼‰è¾¾åˆ°0.87ï¼ŒåŒæ—¶ç»“åˆåœ¨çº¿è´ªå©ªèšç±»ç®—æ³•ï¼Œåœ¨ä¿ç•™ç­”æ¡ˆå¤šæ ·æ€§çš„åŒæ—¶åŠ¨æ€ä¿®å‰ªå†—ä½™è·¯å¾„ã€‚åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆAIME 2024ã€AIME 2025å’ŒGPQAï¼‰ä»¥åŠå¤šä¸ªæ¨ç†æ¨¡å‹ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒDeepPruneåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å®ç°äº†è¶…è¿‡80%çš„ä»¤ç‰Œå‡å°‘ï¼Œä¸ä¼ ç»Ÿå…±è¯†é‡‡æ ·ç›¸æ¯”å–å¾—äº†æ˜¾è‘—æˆç»©ï¼ŒåŒæ—¶ä¿æŒäº†3ä¸ªç™¾åˆ†ç‚¹ä»¥å†…çš„ç«äº‰åŠ›å‡†ç¡®åº¦ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºé«˜æ•ˆå¹¶è¡Œæ¨ç†å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œä½¿é«˜æ€§èƒ½æ¨ç†æ›´åŠ é«˜æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://deepprune.github.io/]">https://deepprune.github.io/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08483v1">PDF</a> 15 pages, 4 figures, please check out the project page:   <a target="_blank" rel="noopener" href="https://deepprune.github.io/">https://deepprune.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¹¶è¡Œç¼©æ”¾å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§èŒƒå¼ï¼Œé€šè¿‡åŒæ—¶ç”Ÿæˆå¤šä¸ªæ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹æ¥å®ç°ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç”±äºè½¨è¿¹é—´çš„å†—ä½™è€Œå¼•å…¥äº†é‡å¤§çš„è®¡ç®—æ•ˆç‡é—®é¢˜â€”â€”æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œè¶…è¿‡80%çš„å¹¶è¡Œæ¨ç†è½¨è¿¹å¾—å‡ºäº†ç›¸åŒçš„æœ€ç»ˆç­”æ¡ˆï¼Œé€ æˆäº†å¤§é‡çš„è®¡ç®—æµªè´¹ã€‚ä¸ºè§£å†³è¿™ä¸€å…³é”®æ•ˆç‡ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†DeepPruneæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å‰ªæå®ç°é«˜æ•ˆå¹¶è¡Œç¼©æ”¾ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸“é—¨è®­ç»ƒçš„æ³•å®˜æ¨¡å‹ï¼Œç»“åˆç„¦ç‚¹æŸå¤±å’Œè¿‡é‡‡æ ·æŠ€æœ¯ï¼Œå‡†ç¡®é¢„æµ‹éƒ¨åˆ†æ¨ç†è½¨è¿¹çš„ç­”æ¡ˆæ˜¯å¦ç›¸åŒï¼Œå®ç°0.87çš„ç­‰ä»·é¢„æµ‹å€¼ã€‚ç»“åˆåœ¨çº¿è´ªå©ªèšç±»ç®—æ³•ï¼ŒåŠ¨æ€åˆ é™¤å†—ä½™è·¯å¾„çš„åŒæ—¶ä¿æŒç­”æ¡ˆå¤šæ ·æ€§ã€‚åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆAIME 2024ã€AIME 2025å’ŒGPQAï¼‰å’Œå¤šä¸ªæ¨ç†æ¨¡å‹ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒDeepPruneåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å®ç°äº†è¶…è¿‡80%çš„ä»¤ç‰Œå‡å°‘ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„å‡†ç¡®ç‡åœ¨3ä¸ªç™¾åˆ†ç‚¹ä»¥å†…ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºé«˜æ•ˆå¹¶è¡Œæ¨ç†å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œä½¿é«˜æ€§èƒ½æ¨ç†æ›´åŠ é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹³è¡Œç¼©æ”¾å¢å¼ºLLMæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªæ€ç»´é“¾è½¨è¿¹å®ç°ã€‚</li>
<li>å¹³è¡Œç¼©æ”¾å­˜åœ¨è®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜ï¼Œè¶…è¿‡80%çš„è½¨è¿¹å¾—å‡ºç›¸åŒç­”æ¡ˆï¼Œé€ æˆè®¡ç®—æµªè´¹ã€‚</li>
<li>æå‡ºDeepPruneæ¡†æ¶è§£å†³æ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡åŠ¨æ€å‰ªæå®ç°é«˜æ•ˆå¹¶è¡Œæ¨ç†ã€‚</li>
<li>DeepPruneé‡‡ç”¨ä¸“é—¨è®­ç»ƒçš„æ³•å®˜æ¨¡å‹ï¼Œç»“åˆç„¦ç‚¹æŸå¤±å’Œè¿‡é‡‡æ ·æŠ€æœ¯é¢„æµ‹ç­”æ¡ˆç­‰ä»·æ€§ã€‚</li>
<li>DeepPruneå®ç°äº†è¶…è¿‡80%çš„ä»¤ç‰Œå‡å°‘ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„å‡†ç¡®ç‡ã€‚</li>
<li>ç»¼åˆè¯„ä¼°è¡¨æ˜DeepPruneåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨ç†æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d3f3c0b39aea7be40be5ba5125a06f82~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145340&auth_key=1760145340-0-0-74194a9c6f3984a961fbbe7802dfdca6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d52756d94c37872766de5e5e8e34b01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145347&auth_key=1760145347-0-0-b3bcd66cd3b9322d37ebb912eec1a56a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6db0e48ef10bacd8e4c7aac52d639525~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145354&auth_key=1760145354-0-0-7529ad397f781039df87ca2bbe590e28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28351f2ca282f623675cce5cd8c443f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145361&auth_key=1760145361-0-0-7811dca8c7aeac3ecb017d576ecbe2c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-STAR-Reinforcing-Open-Vocabulary-Action-Recognition-with-Tools"><a href="#Video-STAR-Reinforcing-Open-Vocabulary-Action-Recognition-with-Tools" class="headerlink" title="Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools"></a>Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools</h2><p><strong>Authors:Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li</strong></p>
<p>Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼¥åˆè§†è§‰å’Œæ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å…ˆéªŒçš„ä¾èµ–å¾€å¾€é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­åŒºåˆ†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„åŠ¨ä½œçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Video-STARæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä¸Šä¸‹æ–‡å­è¿åŠ¨åˆ†è§£ä¸å·¥å…·å¢å¼ºå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œç”¨äºå¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«ï¼ˆOVARï¼‰ã€‚ä¸åŒäºå°†åŠ¨ä½œè§†ä¸ºå•ä¸€å®ä½“çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ›æ–°åœ°å°†åŠ¨ä½œåˆ†è§£ä¸ºå…·æœ‰åŒºåˆ†æ€§çš„å­åŠ¨ä½œï¼Œä»¥å®ç°ç²¾ç»†åŒ¹é…ï¼ŒåŒæ—¶åŠ¨æ€è°ƒç”¨ç‰¹å®šé¢†åŸŸçš„å·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤ç»‡ï¼Œä»è€Œå¯ç”¨ç‰¹å®šç±»åˆ«çš„æ¨ç†èƒ½åŠ›å¹¶å‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡ä¸€ç§å¹³è¡¡çš„å±‚æ¬¡å¥–åŠ±æ¥å¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­åŠ¨ä½œç›¸å…³æ€§å’Œæ¨ç†ä¸­çš„ç»“æ„è¿è´¯æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªä¸»åœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥ä¼˜å…ˆå¤„ç†å­åŠ¨ä½œæ¨¡å¼ï¼Œæ— éœ€æ˜ç¡®ç›‘ç£ï¼Œå®ç°ä»ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¨ç†åˆ°ä»¥è§†è§‰ä¸ºåŸºç¡€çš„æ¨ç†ã€‚åœ¨HMDB-51ã€UCF-101ã€SSv2ã€Kinetics-400å’ŒKinetics-600æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æœ€æ–°æŠ€æœ¯åœ¨åŒºåˆ†ç²¾ç»†åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å’Œæ–‡æœ¬æ¨ç†æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†å…¶æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å…ˆéªŒçŸ¥è¯†é™åˆ¶äº†å…¶åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­å¯¹è¯­ä¹‰ç›¸ä¼¼åŠ¨ä½œçš„è¾¨åˆ«èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†Video-STARæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆè¯­å¢ƒå­è¿åŠ¨åˆ†è§£ä¸å·¥å…·è¾…åŠ©å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«ï¼ˆOVARï¼‰ã€‚ä¸åŒäºä»¥å¾€å°†åŠ¨ä½œè§†ä¸ºå•ä¸€å®ä½“çš„æ–¹æ³•ï¼ŒVideo-STARåˆ›æ–°åœ°å°†åŠ¨ä½œåˆ†è§£ä¸ºå…·æœ‰é‰´åˆ«åŠ›çš„å­è¿åŠ¨ï¼Œè¿›è¡Œç²¾ç»†åŒ¹é…ï¼Œå¹¶åŠ¨æ€è°ƒç”¨ç‰¹å®šå·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤ç»‡ï¼Œä»è€Œæå‡ç±»åˆ«ç‰¹å®šæ¨ç†èƒ½åŠ›å¹¶å‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚é€šè¿‡è®¾è®¡å¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­è¿åŠ¨ç›¸å…³æ€§å’Œæ¨ç†ç»“æ„è¿è´¯æ€§çš„åˆ†å±‚å¥–åŠ±ï¼Œè¯¥æ–¹æ³•å¯è‡ªä¸»åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥ä¼˜å…ˆå­è¿åŠ¨æ¨¡å¼ï¼Œä»æ–‡æœ¬ä¸­å¿ƒæ¨ç†è½¬å‘è§†è§‰åŸºç¡€æ¨ç†ã€‚åœ¨HMDB-51ã€UCF-101ã€SSv2ã€Kinetics-400å’ŒKinetics-600æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒºåˆ†ç²¾ç»†åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒéªŒè¯äº†å…¶å“è¶Šçš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œä½†åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼åŠ¨ä½œæ—¶å­˜åœ¨é™åˆ¶ã€‚</li>
<li>Video-STARæ¡†æ¶é€šè¿‡å­è¿åŠ¨åˆ†è§£å’Œå·¥å…·è¾…åŠ©å¼ºåŒ–å­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Video-STARå°†åŠ¨ä½œåˆ†è§£ä¸ºå­è¿åŠ¨ï¼Œå®ç°ç²¾ç»†åŒ¹é…ï¼Œå¹¶åŠ¨æ€åˆ©ç”¨ç‰¹å®šå·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤ç»‡ã€‚</li>
<li>æ¡†æ¶é€šè¿‡è®¾è®¡åˆ†å±‚å¥–åŠ±æ¥å¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡å’Œæ¨ç†ç»“æ„è¿è´¯æ€§ã€‚</li>
<li>å¤–éƒ¨å·¥å…·çš„ä½¿ç”¨æœ‰åŠ©äºä¼˜å…ˆå¤„ç†å­è¿åŠ¨æ¨¡å¼ï¼Œå®ç°ä»æ–‡æœ¬ä¸­å¿ƒæ¨ç†åˆ°è§†è§‰åŸºç¡€æ¨ç†çš„è½¬å˜ã€‚</li>
<li>Video-STARåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶åœ¨åŒºåˆ†ç²¾ç»†åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-512b3fd5d536e9d8b6b660b0e09c84d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145368&auth_key=1760145368-0-0-6c107f40665d722abb68496eada30b7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ef1b3d28c5fe1ec91052b1b2826854a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145375&auth_key=1760145375-0-0-ae84b39a041267e2dccf493d9554fd77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-736ccc17eae14cf8c7b1ee128db2ae5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145382&auth_key=1760145382-0-0-94fb65bb26790c82679d90fd812f9aa2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b80ee5dbebaae39dc7b3d1bd124a5f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145389&auth_key=1760145389-0-0-9ac2b4825289f2a2e18a3c2f89672e9c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Single-layer-tiny-Co-4-outpaces-GPT-2-and-GPT-BERT"><a href="#Single-layer-tiny-Co-4-outpaces-GPT-2-and-GPT-BERT" class="headerlink" title="Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT"></a>Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT</h2><p><strong>Authors:Noor Ul Zain, Mohsin Raza, Ahsan Adeel</strong></p>
<p>We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå°å‹Co$^4$æœºå™¨ï¼ˆAdeelï¼Œ2025ï¼‰çš„å•å±‚ã€ä¸¤ä¸ªå¤´éƒ¨å’Œ8Må‚æ•°ï¼Œåœ¨è¿‘ä¼¼äº$O(N)$çš„è®¡ç®—æˆæœ¬ï¼ˆå…¶ä¸­$N$æ˜¯è¾“å…¥ä»¤ç‰Œçš„æ•°é‡ï¼‰ä¸‹ï¼Œä»…ç»è¿‡ä¸¤ä¸ªå‘¨æœŸçš„è®­ç»ƒå°±è¶…è¿‡äº†BabyLMæŒ‘æˆ˜åŸºå‡†çš„GPT-2ï¼ˆ1.2äº¿ï¼Œ12å±‚ï¼Œ$O(N^2)$ï¼‰å’ŒGPT-BERTï¼ˆ3äº¿ï¼Œ12å±‚ï¼Œ$O(N^2)$ï¼‰ã€‚åœ¨è®­ç»ƒäº†åä¸ªå‘¨æœŸåï¼ŒCo$^4$åœ¨è®­ç»ƒæ•ˆç‡ä¸Šå®ç°äº†é«˜è¾¾æ•°å€çš„çªç ´ï¼Œå±•ç¤ºäº†é«˜åº¦æ ·æœ¬é«˜æ•ˆçš„é¢„è®­ç»ƒã€‚ä½¿ç”¨BabyLMæŒ‘æˆ˜è¯„ä¼°ç®¡é“è¿›è¡Œå¤æ‚åŸºå‡†æµ‹è¯•æ—¶ï¼ŒCo$^4$åœ¨SuperGLUEä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒCo$^4$åœ¨é›¶æ ·æœ¬æŒ‡æ ‡çš„7ä¸ªä¸­æœ‰5ä¸ªä¼˜äºGPT-2ï¼Œåœ¨å¾®è°ƒä»»åŠ¡çš„7ä¸ªä¸­æœ‰6ä¸ªä¼˜äºGPT-2ï¼Œå¹¶ä¸”åœ¨ä¸¤ç§æƒ…å†µä¸‹éƒ½æœ‰4ä¸ªæŒ‡æ ‡ä¼˜äºGPT-BERTã€‚è¿™äº›ç»“æœæç¤ºæˆ‘ä»¬éœ€è¦é‡æ–°æ€è€ƒç°æœ‰çš„æ·±åº¦å­¦ä¹ èŒƒå¼å’Œç›¸å…³çš„æ‰©å±•å®šå¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08404v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºAdeelåœ¨2025å¹´æå‡ºçš„Co$^4$æœºå™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰å•å±‚ã€ä¸¤ä¸ªå¤´å’Œ8Må‚æ•°ï¼Œåœ¨è¿‘ä¼¼äºO(N)çš„è®¡ç®—æˆæœ¬ä¸‹ï¼ˆå…¶ä¸­Nä¸ºè¾“å…¥æ ‡è®°çš„æ•°é‡ï¼‰ï¼Œç»è¿‡ä»…ä¸¤ä¸ªå‘¨æœŸçš„è®­ç»ƒä¾¿è¶…è¶Šäº†BabyLMæŒ‘æˆ˜åŸºçº¿æ¨¡å‹GPT-2å’ŒGPT-BERTã€‚åœ¨åŒ…å«å¤æ‚åŸºå‡†æµ‹è¯•çš„BabyLMæŒ‘æˆ˜è¯„ä¼°ç®¡é“ä¸Šï¼ŒCo$^4$æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨SuperGLUEä»»åŠ¡ä¸Šï¼ŒCo$^4$åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ä¼˜äºGPT-2çš„äº”ä¸ªæŒ‡æ ‡ï¼Œåœ¨å¾®è°ƒä»»åŠ¡ä¸­ä¼˜äºå…­ä¸ªæŒ‡æ ‡ï¼›å¯¹æ¯”GPT-BERTï¼Œå®ƒåœ¨ä¸ƒä¸ªæŒ‡æ ‡ä¸­æœ‰å››ä¸ªè¡¨ç°æ›´ä¼˜ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†ç°æœ‰çš„æ·±åº¦å­¦ä¹ èŒƒå¼åŠå…¶ç›¸å…³æ‰©å±•å®šå¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>Co$^4$æœºå™¨æ¨¡å‹å…·å¤‡å‡ºè‰²çš„è®­ç»ƒæ•ˆç‡ï¼Œå¯åœ¨ä»…ä¸¤ä¸ªå‘¨æœŸå†…è¶…è¶ŠåŸºçº¿æ¨¡å‹GPT-2å’ŒGPT-BERTçš„è®­ç»ƒè¡¨ç°ã€‚è¿™ä¸¤ä¸ªåŸºçº¿æ¨¡å‹åˆ†åˆ«ä½¿ç”¨äº†æ›´å¤æ‚çš„ç½‘ç»œç»“æ„å’Œæ›´å¤šè®­ç»ƒå‘¨æœŸï¼ˆè®­ç»ƒäº†åä¸ªå‘¨æœŸï¼‰ã€‚è¿™è¡¨æ˜Co$^4$å…·æœ‰å¾ˆé«˜çš„æ ·æœ¬é¢„å¤„ç†æ•ˆç‡ã€‚</p>
</li>
<li><p>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCo$^4$æ¨¡å‹çš„æ€§èƒ½ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯ç›¸è¾ƒäºå½“å‰æµè¡Œçš„å¤§å‹æ¨¡å‹å¦‚GPT-2å’ŒGPT-BERTçš„è¡¨ç°æ›´æœ‰ä¼˜åŠ¿ã€‚å…¶åœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†è¿™äº›åŸºçº¿æ¨¡å‹ã€‚è¿™è¯æ˜äº†Co$^4$æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b02286ef5eb4d93582283519633b60ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145396&auth_key=1760145396-0-0-0d4dc03f6661bd0169f3be390a3146ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdd262253c1380da62d7e10101789fac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145404&auth_key=1760145404-0-0-13a3e587be8a54a05ea5250d8a0b0235&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c8d97daf58b0d8efe65b37c76be294e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145411&auth_key=1760145411-0-0-a658d427dba8884a6f5dc6428f14c578&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-badf98306beb19cac6bb04a0175cecc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145418&auth_key=1760145418-0-0-45c7767d2f1facb528578e1bd0d11120&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts"><a href="#FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts" class="headerlink" title="FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts"></a>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts</h2><p><strong>Authors:Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains â€“ general knowledge understanding, scientific question answering, mathematical reasoning, and code generation â€“ demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA">https://github.com/gfyddha/FlyLoRA</a>. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºåŸºç¡€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä½†å­˜åœ¨å‚æ•°å¹²æ‰°çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚è™½ç„¶åŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„LoRAå˜ä½“åœ¨å•ä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ä¸­æ˜¾ç¤ºå‡ºç¼“è§£ä»»åŠ¡å†…å…³è”çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¼•å…¥äº†é¢å¤–çš„è·¯ç”±å™¨å‚æ•°ï¼Œå¹¶ä¸”åœ¨å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­ä»ç„¶æ— æ•ˆï¼Œè¿™é‡Œä¼šäº§ç”Ÿä»»åŠ¡é—´å¹²æ‰°ã€‚å—æœè‡å—…è§‰ç”µè·¯çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FlyLoRAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºéšå¼MoEçš„LoRAå˜ä½“ï¼Œå®ƒå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰ä¸ŠæŠ•å½±çŸ©é˜µä¸­çš„ç§©ä¸“å®¶æ¿€æ´»ï¼›ï¼ˆ2ï¼‰ä¸€ç§éšå¼è·¯ç”±å™¨ï¼Œç»Ÿä¸€ä¸“å®¶è·¯ç”±å’Œä¸‹æŠ•å½±ï¼Œå…¶ä¸­å†»ç»“çš„ç¨€ç–éšæœºæŠ•å½±çŸ©é˜µå–ä»£äº†ä¼ ç»Ÿçš„å¯†é›†å¯è®­ç»ƒç‰ˆæœ¬ã€‚è¿™ç§è®¾è®¡é€šè¿‡ä¸éœ€è¦æ˜¾å¼è·¯ç”±å™¨è§£å†³äº†ä»»åŠ¡å†…å»ç›¸å…³å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼ŒåŒæ—¶ç”±äºå…¶éšæœºçŸ©é˜µçš„æ­£äº¤æ€§ï¼Œå›ºæœ‰åœ°å‡è½»äº†ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚åœ¨å››ä¸ªé¢†åŸŸâ€”â€”åŒ…æ‹¬é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®é¢˜å›ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆâ€”â€”çš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFlyLoRAåœ¨æ€§èƒ½ä¸Šå®ç°äº†æŒç»­çš„æå‡ã€‚é™¤äº†å®è¯æ”¶ç›Šå¤–ï¼ŒFlyLoRAè¿˜çªå‡ºäº†ç”Ÿç‰©ç»“æ„å¦‚ä½•æ¿€å‘äººå·¥æ™ºèƒ½æŠ€æœ¯çš„åˆ›æ–°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gfyddha/FlyLoRAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08396v1">PDF</a> NeurIPS 2025 accepted paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä»¿ç”Ÿè®¾è®¡çš„æ–°LoRAå˜ä½“FlyLoRAè§£å†³äº†å‚æ•°å¹²æ‰°é—®é¢˜ï¼Œæé«˜äº†æ€§èƒ½ã€‚å®ƒé€šè¿‡å¼•å…¥æ’åä¸“å®¶æ¿€æ´»å’Œéšæ€§è·¯ç”±å™¨ï¼Œæ¶ˆé™¤äº†ä¼ ç»ŸMoEæ–¹æ³•ä¸­å­˜åœ¨çš„è·¯ç”±é—®é¢˜ï¼Œå¹¶é€šè¿‡éšæœºçŸ©é˜µçš„æ­£äº¤æ€§æœ‰æ•ˆåœ°å‡è½»äº†è·¨ä»»åŠ¡çš„å¹²æ‰°ã€‚åœ¨å››ä¸ªä¸åŒé¢†åŸŸçš„å¤§é‡å®éªŒä¸­ï¼ŒFlyLoRAæ˜¾ç¤ºå‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½æå‡ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FlyLoRAæ˜¯ä¸€ç§åŸºäºéšå¼MoEçš„LoRAå˜ä½“ï¼Œè§£å†³äº†å‚æ•°å¹²æ‰°é—®é¢˜ã€‚</li>
<li>å®ƒé€šè¿‡å¼•å…¥æ’åä¸“å®¶æ¿€æ´»å’Œéšæ€§è·¯ç”±å™¨è®¾è®¡è§£å†³äº†ä¼ ç»ŸMoEæ–¹æ³•çš„ç¼ºç‚¹ã€‚</li>
<li>FlyLoRAæ¶ˆé™¤äº†å¯¹æ˜¾å¼è·¯ç”±å™¨çš„éœ€æ±‚ï¼ŒåŒæ—¶é€šè¿‡éšæœºçŸ©é˜µçš„æ­£äº¤æ€§å‡è½»äº†è·¨ä»»åŠ¡å¹²æ‰°ã€‚</li>
<li>åœ¨å››ä¸ªä¸åŒé¢†åŸŸçš„å®éªŒä¸­ï¼ŒFlyLoRAè¡¨ç°å‡ºæ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ€§èƒ½ï¼Œè€Œä¸”å±•ç¤ºäº†ç”Ÿç‰©å­¦ç»“æ„å¦‚ä½•å¯å‘AIæŠ€æœ¯çš„åˆ›æ–°ã€‚</li>
<li>FlyLoRAå…·æœ‰æ½œåŠ›æ”¹å–„å¤šä»»åŠ¡çš„æ¨¡å‹åˆå¹¶æ•ˆæœï¼Œæœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°åº”ç”¨åœ¨å¤§è§„æ¨¡æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®é›†ä¸Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-49ea1c2a9cb8333300a75733cd10a794~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145425&auth_key=1760145425-0-0-bec13721b04e87d2f2fc5211ea3c5e86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d41ed60610223d6aeb8776f73cc27110~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145432&auth_key=1760145432-0-0-a10601a02c6ef064b309a620541ed612&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b6ef07a2c98c73372937f9512da163b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145439&auth_key=1760145439-0-0-83a80f190020ff891a1234c2616defae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c25a82fe3f965c063516f972603997a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145446&auth_key=1760145446-0-0-ea7cae5c07ee29d84a44a6ab6a3dd52c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Detecting-Legend-Items-on-Historical-Maps-Using-GPT-4o-with-In-Context-Learning"><a href="#Detecting-Legend-Items-on-Historical-Maps-Using-GPT-4o-with-In-Context-Learning" class="headerlink" title="Detecting Legend Items on Historical Maps Using GPT-4o with In-Context   Learning"></a>Detecting Legend Items on Historical Maps Using GPT-4o with In-Context   Learning</h2><p><strong>Authors:Sofia Kirsanova, Yao-Yi Chiang, Weiwei Duan</strong></p>
<p>Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles. </p>
<blockquote>
<p>åœ°å›¾å›¾ä¾‹è§£è¯»å¯¹äºè§£é‡Šåœ°å›¾ç¬¦å·è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå…¶å¸ƒå±€çš„ä¸ä¸€è‡´æ€§å’Œéç»“æ„åŒ–å½¢å¼ä½¿å¾—è‡ªåŠ¨æå–å·¥ä½œé¢ä¸´æŒ‘æˆ˜ã€‚æ—©æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†å‰²æˆ–ä¸€èˆ¬çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä¸Šï¼Œå¾ˆå°‘æœ‰æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»¥ç»“æ„åŒ–æ–¹å¼å°†å›¾ä¾‹ç¬¦å·ä¸ç›¸åº”çš„æè¿°ç›¸åŒ¹é…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œç»“åˆLayoutLMv3è¿›è¡Œå¸ƒå±€æ£€æµ‹ï¼Œå¹¶ä½¿ç”¨GPT-4é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥æ£€æµ‹å¹¶è¿æ¥å›¾ä¾‹é¡¹åŠå…¶æè¿°ï¼Œé€šè¿‡è¾¹ç•Œæ¡†é¢„æµ‹è¿›è¡ŒåŒ¹é…ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç»“æ„åŒ–JSONæç¤ºçš„GPT-4åœ¨F-1å¾—åˆ†ä¸Šè¶…è¿‡äº†åŸºçº¿æ°´å¹³ï¼Œè¾¾åˆ°äº†88%ï¼ŒIoUè¾¾åˆ°äº†85%ï¼Œå¹¶æ­ç¤ºäº†æç¤ºè®¾è®¡ã€ç¤ºä¾‹æ•°é‡å’Œå¸ƒå±€å¯¹é½å¦‚ä½•å½±å“æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•æ”¯æŒå¯æ‰©å±•çš„ã€å…·æœ‰å¸ƒå±€æ„è¯†çš„å›¾ä¾‹è§£æï¼Œæé«˜äº†å„ç§è§†è§‰é£æ ¼çš„å†å²åœ°å›¾çš„ç´¢å¼•å’Œå¯æœç´¢æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08385v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå†å²åœ°å›¾çš„å›¾ä¾‹è§£è¯»å¯¹äºç†è§£åœ°å›¾ç¬¦å·è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå›¾ä¾‹å¸ƒå±€çš„ä¸ä¸€è‡´æ€§å’Œéç»“æ„åŒ–æ ¼å¼ï¼Œè‡ªåŠ¨æå–é¢ä¸´æŒ‘æˆ˜ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†å‰²æˆ–ä¸€èˆ¬çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä¸Šï¼Œè€Œå¾ˆå°‘æœ‰ä¸€ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»¥ç»“æ„åŒ–æ–¹å¼å°†å›¾ä¾‹ç¬¦å·ä¸å®ƒä»¬çš„æè¿°ç›¸åŒ¹é…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆLayoutLMv3è¿›è¡Œå¸ƒå±€æ£€æµ‹çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨GPT-4oé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œå›¾ä¾‹é¡¹åŠå…¶æè¿°çš„æ£€æµ‹ä¸é“¾æ¥ï¼Œé€šè¿‡è¾¹ç•Œæ¡†é¢„æµ‹æ¥å®ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒGPT-4ä¸ç»“æ„åŒ–JSONæç¤ºçš„ç»„åˆç›¸æ¯”åŸºçº¿è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†88%çš„F-1å’Œ85%çš„IoUï¼Œå¹¶æ­ç¤ºäº†æç¤ºè®¾è®¡ã€ç¤ºä¾‹è®¡æ•°å’Œå¸ƒå±€å¯¹é½å¯¹æ€§èƒ½çš„å½±å“ã€‚è¯¥æ–¹æ³•æ”¯æŒå¯æ‰©å±•çš„å¸ƒå±€æ„ŸçŸ¥å›¾ä¾‹è§£æï¼Œæé«˜äº†å†å²åœ°å›¾çš„ç´¢å¼•å’Œå¯æœç´¢æ€§ï¼Œé€‚ç”¨äºå„ç§è§†è§‰é£æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å†å²åœ°å›¾çš„å›¾ä¾‹è§£è¯»æ˜¯ç†è§£åœ°å›¾ç¬¦å·çš„å…³é”®ã€‚</li>
<li>ç°æœ‰çš„è‡ªåŠ¨æå–å›¾ä¾‹æ–¹æ³•å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå›¾ä¾‹å¸ƒå±€çš„ä¸ä¸€è‡´æ€§å’Œéç»“æ„åŒ–æ ¼å¼ã€‚</li>
<li>å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†å‰²æˆ–OCRä¸Šï¼Œè€Œç»“æ„åŒ–åŒ¹é…å›¾ä¾‹ç¬¦å·ä¸æè¿°çš„æ–¹æ³•è¾ƒå°‘ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆLayoutLMv3å’ŒGPT-4oçš„æ–¹æ³•ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè¾¹ç•Œæ¡†é¢„æµ‹æ¥æ£€æµ‹å¹¶é“¾æ¥å›¾ä¾‹é¡¹åŠå…¶æè¿°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºGPT-4ä¸ç»“æ„åŒ–JSONæç¤ºçš„ç»„åˆæ€§èƒ½ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†88%çš„F-1å’Œ85%çš„IoUã€‚</li>
<li>è¿™ç§æ–¹æ³•æé«˜äº†å†å²åœ°å›¾çš„ç´¢å¼•å’Œå¯æœç´¢æ€§ï¼Œæ”¯æŒå„ç§è§†è§‰é£æ ¼çš„åœ°å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-271205413fa08c429b16d2d155c36529~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145453&auth_key=1760145453-0-0-833a86e00fe076224f888827f7f711f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6db6171ae29c28a5baf04431c96cbe14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145461&auth_key=1760145461-0-0-cc7e3e4c321af209d9b556dc345ba755&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f4e0c9855b1b43d07df660bbfc850ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145467&auth_key=1760145467-0-0-a419bf4561f06cb8284a55696b1a005a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-725ed2227236cac674df464d077b2c9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145474&auth_key=1760145474-0-0-757b51b0207bdc1c5af00a1471cea80a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ChatGPT-as-a-Translation-Engine-A-Case-Study-on-Japanese-English"><a href="#ChatGPT-as-a-Translation-Engine-A-Case-Study-on-Japanese-English" class="headerlink" title="ChatGPT as a Translation Engine: A Case Study on Japanese-English"></a>ChatGPT as a Translation Engine: A Case Study on Japanese-English</h2><p><strong>Authors:Vincent Michael Sutanto, Giovanni Gatti De Giacomo, Toshiaki Nakazawa, Masaru Yamada</strong></p>
<p>This study investigates ChatGPT for Japanese-English translation, exploring simple and enhanced prompts and comparing against commercially available translation engines. Performing both automatic and MQM-based human evaluations, we found that document-level translation outperforms sentence-level translation for ChatGPT. On the other hand, we were not able to determine if enhanced prompts performed better than simple prompts in our experiments. We also discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly, ChatGPT yields competitive results against two widely-known translation systems. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è°ƒæŸ¥äº†ChatGPTåœ¨æ—¥è‹±ç¿»è¯‘æ–¹é¢çš„è¡¨ç°ï¼Œæ¢ç´¢äº†ç®€å•æç¤ºå’Œå¢å¼ºæç¤ºï¼Œå¹¶ä¸å¸‚é¢ä¸Šå¯ç”¨çš„ç¿»è¯‘å¼•æ“è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿›è¡Œäº†è‡ªåŠ¨å’Œäººç±»è¯„ä»·ï¼Œå‘ç°ChatGPTåœ¨æ–‡æ¡£çº§åˆ«çš„ç¿»è¯‘ä¸Šè¡¨ç°ä¼˜äºå¥å­çº§åˆ«çš„ç¿»è¯‘ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬æœªèƒ½ç¡®å®šå¢å¼ºæç¤ºæ˜¯å¦ä¼˜äºç®€å•æç¤ºã€‚æˆ‘ä»¬è¿˜å‘ç°è‡ªåŠ¨è¯„ä¼°æ›´é’çChatGPT-3.5ï¼Œä½†åœ¨å‡†ç¡®æ€§ä¸æµç•…æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼ˆChatGPT-3.5æ›´å‡†ï¼Œè€ŒChatGPT-4æ›´æµç•…ï¼‰ã€‚æœ€åï¼ŒChatGPTåœ¨ä¸ä¸¤ä¸ªå¹¿ä¸ºäººçŸ¥çš„ç¿»è¯‘ç³»ç»Ÿçš„å¯¹æ¯”ä¸­è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08042v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢è®¨äº†ChatGPTåœ¨æ—¥è‹±ç¿»è¯‘æ–¹é¢çš„åº”ç”¨ï¼Œç ”ç©¶äº†ç®€å•æç¤ºå’Œé«˜çº§æç¤ºçš„æ•ˆæœï¼Œå¹¶ä¸å¸‚é¢ä¸Šå¯ç”¨çš„ç¿»è¯‘å¼•æ“è¿›è¡Œäº†æ¯”è¾ƒã€‚é€šè¿‡è‡ªåŠ¨å’ŒåŸºäºMQMçš„äººç±»è¯„ä¼°ï¼Œå‘ç°ChatGPTåœ¨æ–‡æ¡£çº§åˆ«çš„ç¿»è¯‘è¡¨ç°ä¼˜äºå¥å­çº§åˆ«ã€‚ç„¶è€Œï¼Œå®éªŒæœªèƒ½ç¡®å®šé«˜çº§æç¤ºæ˜¯å¦ä¼˜äºç®€å•æç¤ºã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨è¯„ä¼°æ›´é’çChatGPT-3.5ï¼Œä½†åœ¨å‡†ç¡®æ€§å’Œæµç•…æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚æœ€åï¼ŒChatGPTä¸ä¸¤ä¸ªå¹¿ä¸ºäººçŸ¥çš„ç¿»è¯‘ç³»ç»Ÿçš„ç»“æœç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶å¯¹æ¯”äº†ChatGPTçš„ç®€å•æç¤ºå’Œé«˜çº§æç¤ºåœ¨æ—¥è‹±ç¿»è¯‘ä¸­çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>æ–‡æ¡£çº§åˆ«çš„ChatGPTç¿»è¯‘è¡¨ç°ä¼˜äºå¥å­çº§åˆ«ã€‚</li>
<li>å®éªŒæœªèƒ½ç¡®å®šé«˜çº§æç¤ºæ˜¯å¦ä¼˜äºç®€å•æç¤ºã€‚</li>
<li>è‡ªåŠ¨è¯„ä¼°æ›´é’çChatGPT-3.5ï¼Œä½†åœ¨å‡†ç¡®æ€§ä¸æµç•…æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>ChatGPTä¸å¸‚é¢ä¸Šä¸¤ä¸ªå¹¿å—æ¬¢è¿çš„ç¿»è¯‘ç³»ç»Ÿè¡¨ç°ç›¸å½“ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†è‡ªåŠ¨å’ŒåŸºäºMQMçš„äººç±»è¯„ä¼°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c290c437810b81da5dd0ed1fbc16e39b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145481&auth_key=1760145481-0-0-76a15ed84d493df1302a3b06d39a0d67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c30bcb56b496f1da553d1d9d2885e3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145488&auth_key=1760145488-0-0-ba55eb68f88bdb4d2c89691618502c56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b196ec37ea7541b60adffe0dbedf9ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145495&auth_key=1760145495-0-0-9ad97c2ea43ba300e9e98c77735e7bdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8ea80c310b18db20399b3f9ad7db7d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145502&auth_key=1760145502-0-0-c536b631fcb18abe6e4bb476e4efd941&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7ef7465fa414c8bad36ed951b897930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145508&auth_key=1760145508-0-0-41bce07085ebb7ac99179353c41770b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d1551a9a40c5c021bde83bfb9ebce3a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145515&auth_key=1760145515-0-0-5f66a28cee6a1c8b39a0ab4c02b6df0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AppForge-From-Assistant-to-Independent-Developer-â€“-Are-GPTs-Ready-for-Software-Development"><a href="#AppForge-From-Assistant-to-Independent-Developer-â€“-Are-GPTs-Ready-for-Software-Development" class="headerlink" title="AppForge: From Assistant to Independent Developer â€“ Are GPTs Ready for   Software Development?"></a>AppForge: From Assistant to Independent Developer â€“ Are GPTs Ready for   Software Development?</h2><p><strong>Authors:Dezhi Ran, Yuan Cao, Mengzhou Wu, Simin Chen, Yuzhe Guo, Jun Ren, Zihe Song, Hao Yu, Jialei Wei, Linyi Li, Wei Yang, Baishakhi Ray, Tao Xie</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. Unlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch. To address this gap, we propose APPFORGE, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with implementing the functionality into an Android app from scratch. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct APPFORGE, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, APPFORGE incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8% functionally correct applications, highlighting fundamental limitations in current modelsâ€™ ability to handle complex, multi-component software engineering challenges. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠŸèƒ½çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ä¸åŒäºå­¤ç«‹çš„å‡½æ•°ï¼Œç°å®ä¸–ç•Œçš„åº”ç”¨ç¨‹åºéœ€è¦åœ¨æ•´ä¸ªè½¯ä»¶ç³»ç»Ÿè¿›è¡Œæ¨ç†ï¼šå¼€å‘è€…å¿…é¡»åè°ƒä¸åŒç»„ä»¶çš„äº¤äº’ï¼Œéšæ—¶é—´ä¿æŒçŠ¶æ€çš„ä¸€è‡´æ€§ï¼Œå¹¶ç¡®ä¿åº”ç”¨ç¨‹åºåœ¨ç”Ÿå‘½å‘¨æœŸå’Œæ¡†æ¶çº¦æŸå†…è¡¨ç°æ­£ç¡®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªå……åˆ†è¯„ä¼°LLMæ˜¯å¦èƒ½å¤Ÿå¼¥è¡¥è¿™ä¸€å·®è·å¹¶ä»é›¶å¼€å§‹æ„å»ºæ•´ä¸ªè½¯ä»¶ç³»ç»Ÿã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†APPFORGEåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«ä»çœŸå®ä¸–ç•Œçš„Androidåº”ç”¨ç¨‹åºä¸­æŠ½å–çš„101ä¸ªè½¯ä»¶å¼€å‘é—®é¢˜ã€‚ç»™å®šè¯¦ç»†æè¿°åº”ç”¨ç¨‹åºåŠŸèƒ½çš„è‡ªç„¶è¯­è¨€è§„èŒƒï¼Œè¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ˜¯ä»é›¶å¼€å§‹å°†åŠŸèƒ½å®ç°ä¸ºAndroidåº”ç”¨ç¨‹åºã€‚ä»é›¶å¼€å‘Androidåº”ç”¨ç¨‹åºéœ€è¦ç†è§£å’Œåè°ƒåº”ç”¨ç¨‹åºçŠ¶æ€ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œå¼‚æ­¥æ“ä½œï¼Œè¦æ±‚LLMç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€ç¨³å¥å’Œå¯ç»´æŠ¤çš„ä»£ç ã€‚ä¸ºäº†æ„å»ºAPPFORGEï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šä»£ç†ç³»ç»Ÿï¼Œè‡ªåŠ¨ä»åº”ç”¨ç¨‹åºæ–‡æ¡£ä¸­æ€»ç»“ä¸»è¦åŠŸèƒ½å’Œå¯¼èˆªåº”ç”¨ç¨‹åºä»¥åˆæˆéªŒè¯åº”ç”¨ç¨‹åºå®ç°åŠŸèƒ½æ­£ç¡®æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚ç»è¿‡Androidå¼€å‘ä¸“å®¶çš„ä¸¥æ ¼äººå·¥éªŒè¯åï¼ŒAPPFORGEå°†æµ‹è¯•ç”¨ä¾‹çº³å…¥è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„å³å¯è¿›è¡Œå¯é‡å¤è¯„ä¼°ï¼Œä½¿å…¶æ˜“äºåœ¨æœªæ¥ç ”ç©¶ä¸­é‡‡ç”¨ã€‚æˆ‘ä»¬å¯¹12æ¬¾æ——èˆ°LLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹æ•ˆæœéƒ½å¾ˆå·®ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆGPT-5ï¼‰å¼€å‘çš„åªæœ‰18.8%åŠŸèƒ½æ­£ç¡®çš„åº”ç”¨ç¨‹åºï¼Œè¿™å‡¸æ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šç»„ä»¶è½¯ä»¶å·¥ç¨‹æŒ‘æˆ˜æ—¶çš„æ ¹æœ¬å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07740v1">PDF</a> Under Review. Benchmark and leadboards at   <a target="_blank" rel="noopener" href="https://appforge-bench.github.io/">https://appforge-bench.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å‡½æ•°çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„è½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¯¹æ•´ä¸ªè½¯ä»¶ç³»ç»Ÿè¿›è¡Œæ¨ç†ï¼ŒåŒ…æ‹¬åè°ƒä¸åŒç»„ä»¶çš„äº¤äº’ã€ä¿æŒçŠ¶æ€çš„æŒç»­ä¸€è‡´æ€§ä»¥åŠåœ¨ç”Ÿå‘½å‘¨æœŸå’Œæ¡†æ¶çº¦æŸå†…ç¡®ä¿åº”ç”¨ç¨‹åºçš„æ­£ç¡®è¡Œä¸ºã€‚é’ˆå¯¹è¿™ä¸€å·®è·ï¼Œæå‡ºäº†APPFORGEåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«ä»çœŸå®ä¸–ç•ŒAndroidåº”ç”¨ç¨‹åºä¸­æŠ½å–çš„101ä¸ªè½¯ä»¶å¼€å‘é—®é¢˜ã€‚ç»™å®šè¯¦ç»†çš„åº”ç”¨ç¨‹åºåŠŸèƒ½è‡ªç„¶è¯­è¨€è§„èŒƒï¼Œè¯­è¨€æ¨¡å‹éœ€è¦ä»é›¶å¼€å§‹å®ç°è¿™äº›åŠŸèƒ½åˆ°Androidåº”ç”¨ç¨‹åºä¸­ã€‚åœ¨æ„å»ºAPPFORGEæ—¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šä»£ç†ç³»ç»Ÿæ¥è‡ªåŠ¨æ€»ç»“åº”ç”¨ç¨‹åºæ–‡æ¡£çš„ä¸»è¦åŠŸèƒ½å¹¶å¯¼èˆªåº”ç”¨ç¨‹åºä»¥åˆæˆæµ‹è¯•ç”¨ä¾‹ï¼ŒéªŒè¯åº”ç”¨ç¨‹åºå®ç°çš„æ­£ç¡®æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰è¯„ä¼°çš„LLMæ¨¡å‹çš„æœ‰æ•ˆæ€§å‡è¾ƒä½ï¼Œæœ€ä½³æ¨¡å‹GPT-5ä»…å¼€å‘å‡º18.8%åŠŸèƒ½æ­£ç¡®çš„åº”ç”¨ç¨‹åºï¼Œå‡¸æ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šç»„ä»¶è½¯ä»¶å·¥ç¨‹æŒ‘æˆ˜æ—¶çš„åŸºæœ¬å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å‡½æ•°çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„è½¯ä»¶å¼€å‘æ–¹é¢å­˜åœ¨å·®è·ã€‚</li>
<li>APPFORGEåŸºå‡†æµ‹è¯•åŒ…å«ä»çœŸå®Androidåº”ç”¨ç¨‹åºä¸­æŠ½å–çš„101ä¸ªè½¯ä»¶å¼€å‘é—®é¢˜ã€‚</li>
<li>LLMséœ€ç†è§£å’Œåè°ƒåº”ç”¨ç¨‹åºçš„çŠ¶æ€ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œå¼‚æ­¥æ“ä½œã€‚</li>
<li>å¤šä»£ç†ç³»ç»Ÿç”¨äºè‡ªåŠ¨æ€»ç»“åº”ç”¨ç¨‹åºæ–‡æ¡£çš„ä¸»è¦åŠŸèƒ½å¹¶åˆæˆæµ‹è¯•ç”¨ä¾‹ã€‚</li>
<li>ä¸¥æ ¼çš„æ‰‹åŠ¨éªŒè¯ç¡®ä¿äº†APPFORGEçš„è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºæ‰€æœ‰LLMæ¨¡å‹çš„æœ‰æ•ˆæ€§è¾ƒä½ï¼Œæœ€ä½³æ¨¡å‹GPT-5çš„å¼€å‘æ•ˆæœæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d15a15aa58926c02e39466f82486d5a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145522&auth_key=1760145522-0-0-937cfafe57ff5cd793a9abee24cacf70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f882d9694f4523df35200b1067e55d96~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145529&auth_key=1760145529-0-0-c3286fa5075e702fb3cfd4e4968324e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fb09ce8b7432daaa3fd17e48c11764a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145535&auth_key=1760145535-0-0-f649fc39d9413f9f1b64d7770a33e58f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration"><a href="#More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration" class="headerlink" title="More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration"></a>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration</h2><p><strong>Authors:Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Heng Tao Shen</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This â€œguidance-on-demandâ€ approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯ä¸€ç§æœ‰æœ›å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºè‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€çš„ç¦»çº¿ç­–ç•¥æ•™å¸ˆæ¥æ¿€å‘é•¿é“¾æ€ç»´ï¼ˆLongCoTï¼‰æ¨ç†ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥å†…åœ¨çš„æ¨¡å‹åè§å¹¶é™åˆ¶æ¢ç´¢ï¼Œæœ€ç»ˆé™åˆ¶æ¨ç†çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬ä»çŸ¥è¯†è’¸é¦ä¸­çš„å¤šæ•™å¸ˆç­–ç•¥ä¸­æ±²å–çµæ„Ÿï¼Œå¼•å…¥äº†è‡ªé€‚åº”å¤šå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä½†ä»…åœ¨åœ¨çº¿ç­–ç•¥æ¨¡å‹æ— æ³•ç”Ÿæˆæ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶æ‰ä½¿ç”¨ã€‚è¿™ç§â€œæŒ‰éœ€æŒ‡å¯¼â€çš„æ–¹æ³•åœ¨ä¿æŒè‡ªæˆ‘å‘ç°ä»·å€¼çš„åŒæ—¶æ‰©å¤§äº†æ¢ç´¢èŒƒå›´ã€‚æ­¤å¤–ï¼ŒAMPO èå…¥äº†ä¸€ç§åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œä¿ƒä½¿å­¦ç”Ÿå­¦ä¹ æœ€æœ‰å¯èƒ½è¢«å…¶ç†è§£çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œåœ¨å¹¿æ³›æ¢ç´¢ä¸æœ‰æ•ˆåˆ©ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAMPO æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿ï¼ˆGRPOï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†4.3%ï¼Œåœ¨éå¸¸è§„ä»»åŠ¡ä¸Šæé«˜äº†12.2%ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†Pass@k æ€§èƒ½å¹¶å®ç°äº†æ›´å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨å››ä½åŒè¡Œå¤§å°çš„æ•™å¸ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸åˆ©ç”¨å•ä¸ªæ›´å¼ºå¤§æ•™å¸ˆçš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ï¼‰ç›¸æ¯”å–å¾—äº†ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨äº†æ›´å¤šçš„æ•°æ®ã€‚è¿™äº›ç»“æœè¯æ˜äº†ä¸€æ¡æ›´é«˜æ•ˆã€æ›´å¯æ‰©å±•çš„é€šå¾€å“è¶Šæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02227v2">PDF</a> 20 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èŒƒå¼æœ‰æœ›å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºè‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€ç¦»çº¿ç­–ç•¥æ•™å¸ˆæ¥æ¿€å‘é•¿é“¾æ€ç»´ï¼ˆLongCoTï¼‰æ¨ç†ï¼Œè¿™å¯èƒ½å¼•å…¥æ¨¡å‹å†…åœ¨åè§å¹¶é™åˆ¶æ¢ç´¢ï¼Œæœ€ç»ˆé™åˆ¶æ¨ç†çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚å—çŸ¥è¯†è’¸é¦ä¸­å¤šæ•™å¸ˆç­–ç•¥çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”å¤šæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œèƒ½è‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä½†ä»…åœ¨ç­–ç•¥å†…æ¨¡å‹æ— æ³•äº§ç”Ÿæ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶æ‰è¿›è¡Œã€‚è¿™ç§â€œæŒ‰éœ€æŒ‡å¯¼â€çš„æ–¹æ³•åœ¨æ‰©å¤§æ¢ç´¢çš„åŒæ—¶ä¿ç•™äº†è‡ªæˆ‘å‘ç°çš„ä»·å€¼ã€‚æ­¤å¤–ï¼ŒAMPOè¿˜é‡‡ç”¨äº†ä¸€ç§åŸºäºç†è§£çš„ç­›é€‰æœºåˆ¶ï¼Œé¼“åŠ±å­¦ç”Ÿä»å®ƒæœ€å¯èƒ½ç†è§£çš„æ¨ç†è·¯å¾„ä¸­å­¦ä¹ ï¼Œä»è€Œåœ¨å¹¿æ³›çš„æ¢ç´¢ä¸æœ‰æ•ˆçš„åˆ©ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒAMPOæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ˆGRPOï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†4.3%ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡ä¸Šæé«˜äº†12.2%ï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†Pass@kçš„æ€§èƒ½å¹¶å®ç°äº†æ›´å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨å››ä½åŒçº§æ•™å¸ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°ä¸åˆ©ç”¨å•ä¸€æ›´å¼ºå¤§æ•™å¸ˆçš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ï¼‰ç›¸å½“çš„ç»“æœï¼Œè€Œä¸”æˆ‘ä»¬çš„æ–¹æ³•éœ€è¦çš„æ•°æ®æ›´å°‘ã€‚è¿™äº›ç»“æœè¯æ˜äº†ä¸€æ¡æ›´é«˜æ•ˆã€æ›´å¯æ‰©å±•çš„å®ç°å“è¶Šæ¨ç†å’Œæ³›åŒ–çš„è·¯å¾„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SII-Enigma/AMPOä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èŒƒå¼æœ‰æœ›æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•å¯èƒ½å¼•å…¥æ¨¡å‹å†…åœ¨åè§å¹¶é™åˆ¶æ¢ç´¢ã€‚</li>
<li>AMPOæ¡†æ¶é€šè¿‡è‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥æ”¹è¿›ç°æœ‰æ–¹æ³•ã€‚</li>
<li>AMPOåœ¨è‡ªæˆ‘å‘ç°å’Œæ¢ç´¢ä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ï¼Œé€šè¿‡æŒ‰éœ€æŒ‡å¯¼å’ŒåŸºäºç†è§£çš„ç­›é€‰æœºåˆ¶æ¥ä¼˜åŒ–å­¦ä¹ ã€‚</li>
<li>AMPOåœ¨æ•°å­¦å’Œè¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>AMPOæ–¹æ³•ä¸ä½¿ç”¨å•ä¸€æ›´å¼ºå¤§æ•™å¸ˆçš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç›¸å½“çš„ç»“æœï¼Œä½†æ‰€éœ€æ•°æ®æ›´å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b844c96d963b1a485f000c3b7f74321d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145542&auth_key=1760145542-0-0-c9fcb7f2f0efb43dac08e0ccadf0583e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80b65aedb635e441d5057037ef27720a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145550&auth_key=1760145550-0-0-0d56f79328f8dfb3241b7dc93db3b8fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1570915f2e22666acbf89398f0dc69ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145557&auth_key=1760145557-0-0-a408a2415511a9f8db50370331e57ad4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Kimi-Dev-Agentless-Training-as-Skill-Prior-for-SWE-Agents"><a href="#Kimi-Dev-Agentless-Training-as-Skill-Prior-for-SWE-Agents" class="headerlink" title="Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents"></a>Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents</h2><p><strong>Authors:Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, Yanhao Li, Yue Liu, Zhenxing Hu, Kaitai Zhang, Shuyi Wang, Huarong Chen, Flood Sung, Yang Liu, Yang Gao, Zhilin Yang, Tianyu Liu</strong></p>
<p>Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºè½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ï¼Œè€ŒSWE-benchæ˜¯ä¸€ä¸ªå…³é”®çš„åŸºå‡†æµ‹è¯•ã€‚è§£å†³æ–¹æ¡ˆåˆ†ä¸ºSWE-Agentæ¡†æ¶ï¼Œå…·æœ‰å¤šè½®äº’åŠ¨ï¼Œä»¥åŠåŸºäºå·¥ä½œæµçš„æ— éœ€Agentçš„æ–¹æ³•ï¼Œå…·æœ‰å•è½®å¯éªŒè¯æ­¥éª¤ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸¤ç§èŒƒå¼å¹¶ä¸æ˜¯ç›¸äº’æ’æ–¥çš„ï¼šæ¨ç†å¯†é›†å‹çš„æ— éœ€Agentçš„è®­ç»ƒä¼šå¼•å‘æŠ€èƒ½ä¼˜å…ˆæƒï¼ŒåŒ…æ‹¬å®šä½ã€ä»£ç ç¼–è¾‘å’Œè‡ªæˆ‘åæ€ï¼Œè¿™èƒ½å¤Ÿå®ç°å¯¹SWE-Agentçš„æœ‰æ•ˆå’Œé«˜æ•ˆé€‚åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šæ— éœ€Agentçš„è®­ç»ƒé…æ–¹ï¼Œå¹¶å±•ç¤ºäº†Kimi-Devï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„SWE LLMï¼Œåœ¨SWE-bench Verifiedä¸Šè¾¾åˆ°60.4%ï¼Œæ˜¯å·¥ä½œæµç¨‹æ–¹æ³•ä¸­çš„æœ€ä½³è¡¨ç°ã€‚é€šè¿‡å¯¹5kä¸ªå…¬å¼€å¯ç”¨çš„è½¨è¿¹è¿›è¡Œé¢å¤–çš„SFTé€‚åº”ï¼ŒKimi-Devä½¿SWE-Agentsè¾¾åˆ°48.6%çš„pass@1ï¼Œä¸Claude 3.5 Sonnetï¼ˆ241022ç‰ˆæœ¬ï¼‰æŒå¹³ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ¥è‡ªæ— éœ€Agentçš„è®­ç»ƒçš„ç»“æ„åŒ–æŠ€èƒ½ä¼˜å…ˆæƒå¯ä»¥å¼¥åˆå·¥ä½œæµç¨‹å’Œä»£ç†æ¡†æ¶ä¹‹é—´çš„å·®è·ï¼Œä¸ºå¯è½¬ç§»ç¼–ç ä»£ç†æä¾›æ”¯æ’‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23045v2">PDF</a> 58 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒSWE-benchæ˜¯å…¶ä¸­çš„ä¸€é¡¹å…³é”®åŸºå‡†æµ‹è¯•ã€‚è§£å†³æ–¹æ¡ˆåŒ…æ‹¬åŸºäºå¤šè½®äº¤äº’çš„SWE-Agentæ¡†æ¶å’ŒåŸºäºå·¥ä½œæµçš„æ— éœ€Agentçš„å•è½®å¯éªŒè¯æ­¥éª¤æ–¹æ³•ã€‚æœ¬æ–‡è®¤ä¸ºè¿™ä¸¤ç§èŒƒå¼å¹¶éç›¸äº’æ’æ–¥ï¼šæ³¨é‡æ¨ç†çš„æ— éœ€Agentçš„è®­ç»ƒä¼šè¯±å¯¼æŠ€èƒ½ä¼˜å…ˆæƒï¼ŒåŒ…æ‹¬å®šä½ã€ä»£ç ç¼–è¾‘å’Œè‡ªæˆ‘åæ€ç­‰åŠŸèƒ½ï¼Œä»è€Œå®ç°å¯¹SWE-Agentçš„æœ‰æ•ˆé€‚åº”ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šäº†æ— éœ€Agentçš„è®­ç»ƒé…æ–¹ï¼Œå¹¶æ¨å‡ºäº†Kimi-Devè¿™ä¸€å¼€æºSWE LLMï¼Œåœ¨SWE-bench Verifiedä¸Šçš„è¡¨ç°è¾¾åˆ°60.4%ï¼Œåœ¨å·¥ä½œæµç¨‹æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ã€‚é€šè¿‡é¢å¤–å¯¹5kå…¬å¼€è½¨è¿¹è¿›è¡ŒSFTé€‚åº”ï¼ŒKimi-Devé©±åŠ¨çš„SWE-Agentsè¾¾åˆ°48.6%çš„pass@1ï¼Œä¸Claude 3.5 Sonnetï¼ˆ241022ç‰ˆæœ¬ï¼‰ç›¸å½“ã€‚ç»“æœè¡¨æ˜ï¼Œæ¥è‡ªæ— éœ€Agentçš„è®­ç»ƒçš„ç»“æ„åŒ–æŠ€èƒ½ä¼˜å…ˆæƒå¯ä»¥åœ¨å·¥ä½œæµç¨‹å’Œagenticæ¡†æ¶ä¹‹é—´æ¶èµ·æ¡¥æ¢ï¼Œä¸ºå¯è½¬ç§»çš„ç¼–ç agentæä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨æ—¥ç›Šé‡è¦ï¼ŒSWE-benchæ˜¯è¯„ä¼°å…¶æ€§èƒ½çš„å…³é”®åŸºå‡†ã€‚</li>
<li>è½¯ä»¶å·¥ç¨‹çš„è¯­è¨€æ¨¡å‹è§£å†³æ–¹æ¡ˆåŒ…æ‹¬SWE-Agentæ¡†æ¶å’ŒAgentlessæ–¹æ³•ã€‚</li>
<li>æ— éœ€Agentçš„è®­ç»ƒèƒ½è¯±å¯¼æŠ€èƒ½ä¼˜å…ˆæƒï¼ŒåŒ…æ‹¬å®šä½ã€ä»£ç ç¼–è¾‘å’Œè‡ªæˆ‘åæ€ç­‰åŠŸèƒ½ã€‚</li>
<li>Kimi-Devæ˜¯ä¸€ä¸ªå¼€æºçš„SWE LLMï¼Œåœ¨SWE-bench Verifiedä¸Šçš„è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Kimi-Devé€šè¿‡SFTé€‚åº”æé«˜äº†SWE-Agentsçš„æ€§èƒ½ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ã€‚</li>
<li>ç»“æ„åŒ–çš„æŠ€èƒ½ä¼˜å…ˆæƒæœ‰åŠ©äºå¼¥åˆå·¥ä½œæµç¨‹å’Œagenticæ¡†æ¶ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cd2440f08bbf59a316cd59e6d78b8102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145564&auth_key=1760145564-0-0-4f57c303598ff3051b3994661683fa1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b9f3ec2f3f23d4b01d457b5530fe78a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145571&auth_key=1760145571-0-0-07cfe4a40eae1645a068ca029323f1c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-350aba9d3f02d29d655d9f39d6b7f47b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145577&auth_key=1760145577-0-0-732788fbc84c4f3b0266eb57846c345d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c66af6bf5ea5e152ecff9cc47d8d7efc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145584&auth_key=1760145584-0-0-9e83664e059b46e2107d17b0ceb42caa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f2de0cca086d805c8f5d2f8dd117e15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145590&auth_key=1760145590-0-0-b1fc4e36522f6d40a09c5cf888549c0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers"><a href="#Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers" class="headerlink" title="Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers"></a>Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers</h2><p><strong>Authors:Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao</strong></p>
<p>The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08% and 41.4% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°è‡ªåŠ¨åŒ–å®šç†è¯æ˜ä¸­å·²æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œï¼Œå®ƒå—åˆ°è®­ç»ƒæ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•æŒ‘æˆ˜çš„æ ¹æœ¬é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†<code>BFS-Prover-V2</code>ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³è¿™ç§åŒé‡æ‰©å±•é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªä¸»è¦çš„åˆ›æ–°ç‚¹ã€‚ç¬¬ä¸€ä¸ªæ˜¯æ–°å‹çš„å¤šè½®ç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¸æ–­æé«˜è®­ç»ƒæ—¶LLMé€æ­¥è¯æ˜çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å—åˆ°AlphaZeroåŸåˆ™çš„å¯å‘ï¼Œé‡‡ç”¨å¤šé˜¶æ®µä¸“å®¶è¿­ä»£ç®¡é“ï¼Œå…·æœ‰è‡ªé€‚åº”æˆ˜æœ¯çº§æ•°æ®è¿‡æ»¤å’Œå®šæœŸé‡æ–°è®­ç»ƒçš„åŠŸèƒ½ï¼Œä»¥å…‹æœé€šå¸¸é™åˆ¶é•¿æœŸRLçš„æ€§èƒ½ç“¶é¢ˆï¼Œåœ¨åŸºäºLLMçš„ä»£ç†ä¸­ã€‚ç¬¬äºŒä¸ªåˆ›æ–°ç‚¹æ˜¯ä¸€ä¸ªå¢å¼ºè§„åˆ’çš„å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨æ¨ç†æ—¶é—´æ‰©å±•æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¶æ„é‡‡ç”¨é€šç”¨æ¨ç†æ¨¡å‹ä½œä¸ºé«˜çº§è§„åˆ’å™¨ï¼Œå°†å¤æ‚çš„å®šç†è¿­ä»£åœ°åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•çš„å­ç›®æ ‡ã€‚è¿™ç§åˆ†å±‚æ–¹æ³•å¤§å¤§å‡å°‘äº†æœç´¢ç©ºé—´ï¼Œä½¿ä¸€ç»„å¹¶è¡Œè¯æ˜æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡åˆ©ç”¨å…±äº«è¯æ˜ç¼“å­˜æ¥æœ‰æ•ˆåœ°åä½œã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§åŒé‡æ‰©å±•æ–¹æ³•äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨æ­£å¼çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚<code>BFS-Prover-V2</code>åœ¨MiniF2Få’ŒProofNetæµ‹è¯•é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†95.08%å’Œ41.4%çš„å‡†ç¡®ç‡ã€‚è™½ç„¶è¯¥å·¥ä½œå±•ç¤ºçš„æ˜¯åœ¨å½¢å¼æ•°å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œä½†æœ¬å·¥ä½œæå‡ºçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†æŠ€æœ¯å…·æœ‰æ›´å¹¿æ³›çš„å…´è¶£ï¼Œå¹¶å¯åº”ç”¨äºéœ€è¦é•¿æœŸå¤šè½®æ¨ç†å’Œå¤æ‚æœç´¢çš„å…¶ä»–é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06493v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>LLMsèå…¥è‡ªåŠ¨åŒ–å®šç†è¯æ˜å±•ç°å·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´è®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ—¶è®¡ç®—çš„åŒé‡æ‰©å±•æŒ‘æˆ˜ã€‚ã€ŠBFS-Prover-V2ç³»ç»Ÿã€‹æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥ç³»ç»Ÿå¼•å…¥ä¸¤å¤§åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯å…¨æ–°çš„å¤šè½®ç¦»çº¿ç­–ç•¥RLæ¡†æ¶ï¼Œæ—¨åœ¨æŒç»­æé«˜è®­ç»ƒæ—¶LLMçš„æ€§èƒ½ï¼›äºŒæ˜¯è§„åˆ’å¢å¼ºå‹å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚å‰è€…é‡‡ç”¨AlphaZeroåŸç†çš„è¿­ä»£ç®¡é“è®¾è®¡ï¼Œåˆ©ç”¨è‡ªé€‚åº”æˆ˜æœ¯çº§æ•°æ®è¿‡æ»¤å’Œå®šæœŸå†è®­ç»ƒå…‹æœæ€§èƒ½ç“¶é¢ˆï¼›åè€…åˆ™é€šè¿‡é€šç”¨æ¨ç†æ¨¡å‹ä½œä¸ºé«˜çº§è§„åˆ’å™¨åˆ†è§£å¤æ‚å®šç†åºåˆ—ã€‚æœ¬æ–‡çš„RLå’Œæ¨ç†æŠ€æœ¯è™½ç”¨äºå½¢å¼æ•°å­¦é¢†åŸŸï¼Œä½†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›æœ‰æœ›ä¸ºå…¶ä»–é¢†åŸŸå¸¦æ¥å¯ç¤ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMåœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ä¸­çš„é›†æˆå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯è®­ç»ƒå’Œæ¨ç†æ—¶é—´è§„æ¨¡çš„æ‰©å¤§ã€‚</li>
<li>å¼•å…¥çš„æ–°å‹å¤šè½®ç¦»çº¿ç­–ç•¥RLæ¡†æ¶é€šè¿‡æ¨¡ä»¿AlphaZeroåŸç†æé«˜äº†LLMçš„æ€§èƒ½ã€‚é€šè¿‡è‡ªé€‚åº”æˆ˜æœ¯çº§æ•°æ®è¿‡æ»¤å’Œå®šæœŸå†è®­ç»ƒæ¥å…‹æœæ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>è§„åˆ’å¢å¼ºå‹å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„èƒ½å¤Ÿæ‰©å±•æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¶æ„é‡‡ç”¨é€šç”¨æ¨ç†æ¨¡å‹ä½œä¸ºé«˜çº§è§„åˆ’å™¨åˆ†è§£å¤æ‚å®šç†åºåˆ—ï¼Œæ˜¾è‘—å‡å°‘æœç´¢ç©ºé—´ã€‚</li>
<li>é€šè¿‡åœ¨å½¢å¼æ•°å­¦é¢†åŸŸçš„æµ‹è¯•é›†ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨å®šç†è¯æ˜é¢†åŸŸçš„å…ˆè¿›æˆæœã€‚è¿™ç§åŒç­–ç•¥æ–¹æ³•åœ¨æ ‡å‡†æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å¾ˆé«˜çš„è¡¨ç°æ°´å¹³ã€‚åŒæ—¶å¼ºè°ƒäº†å…¶åœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2077e21fd42b0dc8a812878ce7c79d2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145598&auth_key=1760145598-0-0-113deb51e98856b21bd07949104d6c8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84858c4f548f5303404a32f3ae383431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145606&auth_key=1760145606-0-0-e0d3c4137799e04751e95297b288c5ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf9d216a016536e180306c0133831217~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145613&auth_key=1760145613-0-0-4dcad429edf965ecd548ae626a5d9ddf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67fc817e10239aa01fd578650e9e33d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145620&auth_key=1760145620-0-0-206e324bfa7890849eab53833ffa280d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Flora-Effortless-Context-Construction-to-Arbitrary-Length-and-Scale"><a href="#Flora-Effortless-Context-Construction-to-Arbitrary-Length-and-Scale" class="headerlink" title="Flora: Effortless Context Construction to Arbitrary Length and Scale"></a>Flora: Effortless Context Construction to Arbitrary Length and Scale</h2><p><strong>Authors:Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu</strong></p>
<p>Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human&#x2F;LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/txchen-USTC/Flora%7D%7Bhttps://github.com/txchen-USTC/Flora%7D">https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}</a>. </p>
<blockquote>
<p>æœ‰æ•ˆåœ°å¤„ç†é•¿è¯­å¢ƒå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºé•¿æ–‡æœ¬è¾ƒä¸ºç½•è§ã€è®¡ç®—éœ€æ±‚é«˜ï¼Œå¹¶ä¸”çŸ­è¯­å¢ƒèƒ½åŠ›çš„é—å¿˜ä¹Ÿè¾ƒå¤šã€‚è¿‘æœŸçš„æ–¹æ³•å°è¯•æ„å»ºé•¿è¯­å¢ƒè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹æˆ–äººå·¥å¹²é¢„ï¼ŒäºŒè€…æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”åœ¨é•¿åº¦å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨é™åˆ¶ã€‚æ­¤å¤–ï¼Œå½“å‰é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ­è¯­å¢ƒä¸­çš„è¡¨ç°ä¸‹é™ä»ç„¶æ˜¾è‘—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Floraï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€äººå·¥&#x2F;å¤§å‹è¯­è¨€æ¨¡å‹å‚ä¸çš„é•¿è¯­å¢ƒæ„å»ºç­–ç•¥ã€‚Floraå¯ä»¥é€šè¿‡åŸºäºç±»åˆ«ä»»æ„ç»„åˆçŸ­æŒ‡ä»¤ï¼Œå¹¶æŒ‡ä»¤å¤§å‹è¯­è¨€æ¨¡å‹åŸºäºé•¿è¯­å¢ƒå…ƒæŒ‡ä»¤ç”Ÿæˆå“åº”ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿è¯­å¢ƒè¡¨ç°ã€‚è¿™ä½¿å¾—Floraèƒ½å¤Ÿäº§ç”Ÿä»»æ„é•¿åº¦å’Œè§„æ¨¡ã€å…·æœ‰ä¸°å¯Œå¤šæ ·æ€§çš„ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶åªç¨å¾®å½±å“çŸ­è¯­å¢ƒçš„è¡¨ç°ã€‚åœ¨Llama3-8B-Instructå’ŒQwQ-32Bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡Floraå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‰ä¸ªé•¿è¯­å¢ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨çŸ­è¯­å¢ƒä»»åŠ¡ä¸­ä¿æŒå¼ºåŠ²è¡¨ç°ã€‚æˆ‘ä»¬çš„æ•°æ®æ„å»ºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/txchen-USTC/Flora%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/txchen-USTC/Floraä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19786v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFloraçš„é•¿è¯­å¢ƒæ„å»ºç­–ç•¥ï¼Œå®ƒå¯ä»¥åœ¨ä¸ä¾èµ–äººå·¥æˆ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç»„åˆçŸ­æŒ‡ä»¤å’ŒåŸºäºé•¿è¯­å¢ƒå…ƒæŒ‡ä»¤ç”Ÿæˆå“åº”ï¼Œæœ‰æ•ˆæé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿è¯­å¢ƒçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ„å»ºä»»æ„é•¿åº¦å’Œå¤šæ ·æ€§çš„è¯­å¢ƒï¼ŒåŒæ—¶åªç•¥å¾®å½±å“çŸ­è¯­å¢ƒçš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Floraå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è¯­å¢ƒä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒçŸ­è¯­å¢ƒä»»åŠ¡çš„å¼ºæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¯­å¢ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé•¿æ–‡æœ¬ç¨€å°‘ã€è®¡ç®—éœ€æ±‚é«˜ä»¥åŠçŸ­è¯­å¢ƒèƒ½åŠ›çš„é—å¿˜ã€‚</li>
<li>ç°æœ‰æ„å»ºé•¿è¯­å¢ƒçš„æ–¹æ³•éœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹æˆ–äººå·¥å‚ä¸ï¼Œæˆæœ¬é«˜æ˜‚ä¸”å—é™ã€‚</li>
<li>Floraæ˜¯ä¸€ç§æ–°å‹é•¿è¯­å¢ƒæ„å»ºç­–ç•¥ï¼Œé€šè¿‡ç»„åˆçŸ­æŒ‡ä»¤å’ŒåŸºäºé•¿è¯­å¢ƒçš„å…ƒæŒ‡ä»¤ç”Ÿæˆå“åº”ã€‚</li>
<li>Floraèƒ½æ„å»ºä»»æ„é•¿åº¦å’Œå¤šæ ·æ€§çš„è¯­å¢ƒï¼ŒåŒæ—¶åªç•¥å¾®å½±å“çŸ­è¯­å¢ƒæ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä½¿ç”¨Floraå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è¯­å¢ƒä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Floraæä¾›çš„æ•°æ®æ„å»ºä»£ç å¯å…¬å¼€è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-92368b2743395338938ed651b568380b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145627&auth_key=1760145627-0-0-337f5cbec3365b85d58285d4fea5f7eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93ca07a36434365edab79dfc1e6dbcd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145634&auth_key=1760145634-0-0-6810337fa8f4938e5ec911772b54e10d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fcc7526047885bf32e2d600547c39ef5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145641&auth_key=1760145641-0-0-bc14bf71a2a942b25023092f9d1c1ffc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab931d5ef0b4478bcc98c1b1e799dd47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145648&auth_key=1760145648-0-0-0e933056bcd0dfa1860eaf198c043ae7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dca80d2fb7bb6c3ed7d3b00088b56d9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145655&auth_key=1760145655-0-0-331d4b0ad59188e26834d8bfe7dd270d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LDI-Localized-Data-Imputation-for-Text-Rich-Tables"><a href="#LDI-Localized-Data-Imputation-for-Text-Rich-Tables" class="headerlink" title="LDI: Localized Data Imputation for Text-Rich Tables"></a>LDI: Localized Data Imputation for Text-Rich Tables</h2><p><strong>Authors:Soroush Omidvartehrani, Davood Rafiei</strong></p>
<p>Missing values are pervasive in real-world tabular data and can significantly impair downstream analysis. Imputing them is especially challenging in text-rich tables, where dependencies are implicit, complex, and dispersed across long textual fields. Recent work has explored using Large Language Models (LLMs) for data imputation, yet existing approaches typically process entire tables or loosely related contexts, which can compromise accuracy, scalability, and explainability. We introduce LDI, a novel framework that leverages LLMs through localized reasoning, selecting a compact, contextually relevant subset of attributes and tuples for each missing value. This targeted selection reduces noise, improves scalability, and provides transparent attribution by revealing which data influenced each prediction. Through extensive experiments on real and synthetic datasets, we demonstrate that LDI consistently outperforms state-of-the-art imputation methods, achieving up to 8% higher accuracy with hosted LLMs and even greater gains with local models. The improved interpretability and robustness also make LDI well-suited for high-stakes data management applications. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„è¡¨æ ¼æ•°æ®æ™®éå­˜åœ¨ç¼ºå¤±å€¼ï¼Œè¿™å¯èƒ½ä¼šæ˜¾è‘—å½±å“ä¸‹æ¸¸åˆ†æã€‚åœ¨å¯Œå«æ–‡æœ¬çš„è¡¨æ ¼ä¸­å¯¹ç¼ºå¤±å€¼è¿›è¡Œä¼°ç®—å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™äº›è¡¨æ ¼ä¸­çš„ä¾èµ–å…³ç³»éšè”½ã€å¤æ‚ä¸”åˆ†æ•£åœ¨è¾ƒé•¿çš„æ–‡æœ¬å­—æ®µä¸­ã€‚æœ€è¿‘çš„å·¥ä½œå·²ç»æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ•°æ®ä¼°ç®—ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸å¤„ç†æ•´ä¸ªè¡¨æ ¼æˆ–æ¾æ•£ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œè¿™å¯èƒ½ä¼šæŸå®³å‡†ç¡®æ€§ã€å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†LDIï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡å±€éƒ¨æ¨ç†åˆ©ç”¨LLMï¼Œä¸ºæ¯ä¸ªç¼ºå¤±å€¼é€‰æ‹©ç´§å‡‘ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„å±æ€§å­é›†å’Œå…ƒç»„ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„é€‰æ‹©å‡å°‘äº†å™ªå£°ï¼Œæé«˜äº†å¯æ‰©å±•æ€§ï¼Œå¹¶é€šè¿‡æ­ç¤ºå½±å“æ¯ä¸ªé¢„æµ‹çš„æ•°æ®ï¼Œæä¾›äº†é€æ˜çš„å½’å±å…³ç³»ã€‚æˆ‘ä»¬åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒLDIå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ä¼°ç®—æ–¹æ³•ï¼Œåœ¨ä½¿ç”¨æ‰˜ç®¡LLMçš„æƒ…å†µä¸‹ï¼Œå…¶å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾8%ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹åˆ™è·å¾—æ›´å¤§çš„æ”¶ç›Šã€‚æ”¹è¿›çš„å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ä¹Ÿä½¿LDIéå¸¸é€‚åˆäºé«˜é£é™©æ•°æ®ç®¡ç†åº”ç”¨ç¨‹åºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16616v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çœŸå®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸­ç¼ºå¤±å€¼çš„é—®é¢˜åŠå…¶å¯¹æ•°æ®åˆ†æå’Œå¤„ç†çš„è´Ÿé¢å½±å“ã€‚é’ˆå¯¹æ–‡æœ¬ä¸°å¯Œçš„è¡¨æ ¼æ•°æ®ä¸­çš„ç¼ºå¤±å€¼å¡«å……é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶LDIï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å±€éƒ¨æ¨ç†é€‰æ‹©ç›¸å…³å±æ€§å­é›†å’Œå…ƒç»„è¿›è¡Œé¢„æµ‹ï¼Œæé«˜äº†å‡†ç¡®æ€§ã€å¯æ‰©å±•æ€§å’Œè§£é‡Šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒLDIåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å¯æé«˜é«˜è¾¾8%çš„å‡†ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºå¤±å€¼åœ¨çœŸå®ä¸–ç•Œçš„è¡¨æ ¼æ•°æ®ä¸­æ™®éå­˜åœ¨ï¼Œä¼šå¯¹ä¸‹æ¸¸åˆ†æäº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
<li>åœ¨æ–‡æœ¬ä¸°å¯Œçš„è¡¨æ ¼ä¸­è¿›è¡Œæ•°æ®å¡«å……æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºä¾èµ–å…³ç³»å¤æ‚ä¸”åˆ†æ•£åœ¨è¾ƒé•¿çš„æ–‡æœ¬å­—æ®µä¸­ã€‚</li>
<li>LDIæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå±€éƒ¨æ¨ç†ï¼Œé’ˆå¯¹æ¯ä¸ªç¼ºå¤±å€¼é€‰æ‹©ç›¸å…³çš„å±æ€§å­é›†å’Œå…ƒç»„è¿›è¡Œå¤„ç†ã€‚</li>
<li>LDIé€šè¿‡å‡å°‘å™ªå£°ã€æé«˜å¯æ‰©å±•æ€§å’Œæ­ç¤ºé¢„æµ‹èƒŒåçš„æ•°æ®å½±å“æ¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒLDIåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¡«å……æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜é«˜è¾¾8%ã€‚</li>
<li>LDIé€šè¿‡åˆ©ç”¨æ‰˜ç®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹ï¼Œåœ¨é«˜æ€§èƒ½çš„åŒæ—¶ä¹Ÿè¡¨ç°å‡ºäº†å¼ºå¤§çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8ef6c79282d5edcffa4c14e5d0477c49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145663&auth_key=1760145663-0-0-89eb106553120cc46c1264ed63660d44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f76609c414cc79b591938fa5d6711357~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145670&auth_key=1760145670-0-0-f96842539e102b60f4d20bcde3e6d27e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82e8689a43f37d7079ba2eb308100b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145676&auth_key=1760145676-0-0-fe7d96d3a323368a2d4ccb353c9697ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3ad893d591d252082ac7d0dd75c6598~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145683&auth_key=1760145683-0-0-7c62c8ad794da09ac28e4ae7b2d8484c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56d5c1b5fc969405e9f6c09a876c2862~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145689&auth_key=1760145689-0-0-8c893eb62b710badc3d580c246709c0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Play-to-Generalize-Learning-to-Reason-Through-Game-Play"><a href="#Play-to-Generalize-Learning-to-Reason-Through-Game-Play" class="headerlink" title="Play to Generalize: Learning to Reason Through Game Play"></a>Play to Generalize: Learning to Reason Through Game Play</h2><p><strong>Authors:Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei</strong></p>
<p>Developing reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by literature suggesting that gameplay promotes transferable reasoning skills, we propose a novel post-training method, Visual Game Learning (ViGaL), where MLLMs develop generalizable reasoning skills through playing arcade-like games. Specifically, we show that training a 7B-parameter MLLM via reinforcement learning (RL) on simple games like Snake significantly enhances the downstream performance on multimodal math benchmarks like MathVista, on multi-discipline questions like MMMU and on 3D spatial reasoning benchmarks like VSI-Bench, without seeing any worked solutions, equations, or diagrams during RL. Remarkably, our model outperforms specialist models post-trained on benchmark-oriented multimodal reasoning data, while preserving the modelâ€™s performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest that multimodal reasoning can emerge from gameplay, pointing to a promising strategy of designing surrogate tasks for RL post-training. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­åŸ¹å…»æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å—æ–‡çŒ®å¯å‘ï¼Œæ–‡çŒ®è¡¨æ˜æ¸¸æˆç©æ³•å¯ä»¥ä¿ƒè¿›å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒæ–¹æ³•â€”â€”è§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ï¼Œå…¶ä¸­MLLMså¯ä»¥é€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆæ¥å‘å±•å¯æ¨å¹¿çš„æ¨ç†æŠ€èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç®€å•çš„æ¸¸æˆï¼ˆå¦‚Snakeï¼‰ä¸Šè®­ç»ƒä¸€ä¸ªæ‹¥æœ‰7Bå‚æ•°çš„MLLMï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºåœ¨è¯¸å¦‚MathVistaç­‰å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ã€MMMUç­‰å¤šå­¦ç§‘é—®é¢˜ä»¥åŠVSI-Benchç­‰3Dç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½ï¼Œè€Œæ— éœ€åœ¨RLè¿‡ç¨‹ä¸­æŸ¥çœ‹ä»»ä½•è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢å‘åŸºå‡†æµ‹è¯•çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ä¸Šè¿›è¡Œäº†åè®­ç»ƒï¼Œè¶…è¶Šäº†ä¸“ä¸šæ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸šæ¨¡å‹ç»å¸¸è¡¨ç°ä¸è¶³çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨ç†å¯ä»¥æºè‡ªæ¸¸æˆç©æ³•ï¼Œè¿™æŒ‡å‘äº†ä¸€ç§ä¸ºRLåè®­ç»ƒè®¾è®¡æ›¿ä»£ä»»åŠ¡çš„æœ‰å‰é€”çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08011v4">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yunfeixie233.github.io/ViGaL/">https://yunfeixie233.github.io/ViGaL/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡çŒ®è¡¨æ˜æ¸¸æˆå¯ä»¥ä¿ƒè¿›å¯è½¬ç§»æ¨ç†èƒ½åŠ›çš„è§‚ç‚¹ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ¸¸æˆçš„åè®­ç»ƒæ–¹å¼ï¼ˆViGaLæ–¹æ³•ï¼‰ï¼Œé€šè¿‡ç±»ä¼¼äºç”µç©æ¸¸æˆçš„æ´»åŠ¨æ¥æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚è®­ç»ƒåŒ…å«ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å«æœ‰å¦‚è›‡æ¢¯æ¸¸æˆç­‰ç®€å•æ¸¸æˆçš„7Bå‚æ•°MLLMè¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†åœ¨å¦‚MathVistaç­‰å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ã€MMMUç­‰å¤šå­¦ç§‘é—®é¢˜æµ‹è¯•ä»¥åŠVSI-Benchçš„3Dç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„ä¸‹æ¸¸æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨RLè¿‡ç¨‹ä¸­æœªè§ä»»ä½•è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ï¼Œæ¨¡å‹ä¾ç„¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä¸“é—¨é’ˆå¯¹åŸºå‡†æµ‹è¯•çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®åè®­ç»ƒçš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹åœ¨å¸¸è§„è§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å‘ç°é¢„ç¤ºç€æ¸¸æˆå¯¹äºè®­ç»ƒå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å‰æ™¯ï¼ŒåŒæ—¶æä¾›äº†ä¸ºRLåè®­ç»ƒè®¾è®¡æ›¿ä»£ä»»åŠ¡çš„å¯è¡Œç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºVisual Game Learningï¼ˆViGaLï¼‰çš„æ–°å‹åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç±»ä¼¼äºç”µç©æ¸¸æˆçš„æ´»åŠ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚<br>äºŒã€ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¨¡å‹ï¼Œåœ¨ç®€å•çš„æ¸¸æˆå¦‚è›‡æ¢¯æ¸¸æˆä¸­è¿›è¡Œè®­ç»ƒï¼Œèƒ½æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚<br>ä¸‰ã€å³ä½¿åœ¨æ²¡æœ‰æ¥è§¦ä»»ä½•è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¾ç„¶èƒ½åœ¨å„ç±»æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚<br>å››ã€ViGaLæ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä¸“é—¨é’ˆå¯¹åŸºå‡†æµ‹è¯•çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®åè®­ç»ƒçš„æ¨¡å‹ã€‚<br>äº”ã€è¯¥æ¨¡å‹åœ¨ä¿æŒä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šçš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜å¼‚è¡¨ç°ã€‚<br>å…­ã€ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¸¸æˆèƒ½å¤Ÿæœ‰åŠ©äºè®­ç»ƒå…·å¤‡å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ï¼Œè¿™å¯¹äºæœªæ¥çš„AIç ”ç©¶å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ac4ee991959206e5710079a8e250eb31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145696&auth_key=1760145696-0-0-20dc9d59ecd866ed69372892e21cbca2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cee785f65f3935b88b6db8cbd25e7e1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145704&auth_key=1760145704-0-0-903e8090b82a9eedcc116d0fddbacb82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f49dfdb0e8a00fd97d771febdcb53e7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145711&auth_key=1760145711-0-0-a254701bf68b4282bb75fc0f6f0383e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cac789441040cc27ccd0a2e10953bf68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145718&auth_key=1760145718-0-0-81fffbb322589b8e37d2a7eaef90f4e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OASIS-Online-Sample-Selection-for-Continual-Visual-Instruction-Tuning"><a href="#OASIS-Online-Sample-Selection-for-Continual-Visual-Instruction-Tuning" class="headerlink" title="OASIS: Online Sample Selection for Continual Visual Instruction Tuning"></a>OASIS: Online Sample Selection for Continual Visual Instruction Tuning</h2><p><strong>Authors:Minjae Lee, Minhyuk Seo, Tingyu Qu, Tinne Tuytelaars, Jonghyun Choi</strong></p>
<p>In continual instruction tuning (CIT) scenarios, where new instruction tuning data continuously arrive in an online streaming manner, training delays from large-scale data significantly hinder real-time adaptation. Data selection can mitigate this overhead, but existing strategies often rely on pretrained reference models, which are impractical in CIT setups since future data are unknown. Recent reference model-free online sample selection methods address this, but typically select a fixed number of samples per batch (e.g., top-k), making them vulnerable to distribution shifts where informativeness varies across batches. To address these limitations, we propose OASIS, an adaptive online sample selection approach for CIT that (1) selects informative samples by estimating each sampleâ€™s informativeness relative to all previously seen data, beyond batch-level constraints, and (2) minimizes informative redundancy of selected samples through iterative selection score updates. Experiments on various large foundation models show that OASIS, using only 25 percent of the data, achieves comparable performance to full-data training and outperforms the state-of-the-art sampling methods. </p>
<blockquote>
<p>åœ¨æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆCITï¼‰åœºæ™¯ä¸­ï¼Œæ–°çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ä»¥åœ¨çº¿æµçš„æ–¹å¼æŒç»­åˆ°è¾¾ï¼Œå¤§è§„æ¨¡æ•°æ®å¯¼è‡´çš„è®­ç»ƒå»¶è¿Ÿä¸¥é‡é˜»ç¢äº†å®æ—¶é€‚åº”ã€‚æ•°æ®é€‰æ‹©å¯ä»¥å‡è½»è¿™ç§å¼€é”€ï¼Œä½†ç°æœ‰ç­–ç•¥é€šå¸¸ä¾èµ–äºé¢„å…ˆè®­ç»ƒçš„å‚è€ƒæ¨¡å‹ï¼Œè¿™åœ¨CITè®¾ç½®ä¸­å¹¶ä¸å®ç”¨ï¼Œå› ä¸ºæœªæ¥çš„æ•°æ®æ˜¯æœªçŸ¥çš„ã€‚æœ€è¿‘çš„æ— éœ€å‚è€ƒæ¨¡å‹çš„åœ¨çº¿æ ·æœ¬é€‰æ‹©æ–¹æ³•è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†é€šå¸¸æ¯æ‰¹é€‰æ‹©å›ºå®šæ•°é‡çš„æ ·æœ¬ï¼ˆä¾‹å¦‚ï¼Œå‰kåï¼‰ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä¿¡æ¯æ€§éšæ‰¹æ¬¡å˜åŒ–è€Œå˜åŒ–çš„åˆ†å¸ƒè½¬ç§»ä¸­å®¹æ˜“å—åˆ°å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†OASISï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºCITçš„è‡ªé€‚åº”åœ¨çº¿æ ·æœ¬é€‰æ‹©æ–¹æ³•ï¼Œå®ƒï¼ˆ1ï¼‰é€šè¿‡ä¼°è®¡æ¯ä¸ªæ ·æœ¬ç›¸å¯¹äºæ‰€æœ‰ä¹‹å‰æ•°æ®çš„ä¿¡æ¯é‡æ¥é€‰æ‹©ä¿¡æ¯æ€§æ ·æœ¬ï¼Œè¶…è¶Šæ‰¹æ¬¡çº§åˆ«çš„çº¦æŸï¼›ï¼ˆ2ï¼‰é€šè¿‡è¿­ä»£é€‰æ‹©åˆ†æ•°æ›´æ–°æ¥æœ€å°åŒ–æ‰€é€‰æ ·æœ¬çš„ä¿¡æ¯å†—ä½™ã€‚åœ¨å„ç§å¤§å‹åŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOASISä»…ä½¿ç”¨25%çš„æ•°æ®å°±èƒ½è¾¾åˆ°ä¸å…¨æ•°æ®åŸ¹è®­ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºæœ€å…ˆè¿›çš„é‡‡æ ·æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02011v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿è¿ç»­æŒ‡ä»¤è°ƒæ•´ï¼ˆCITï¼‰åœºæ™¯ä¸­ï¼Œæ–°æŒ‡ä»¤è°ƒæ•´æ•°æ®ä»¥åœ¨çº¿æµæ–¹å¼æŒç»­åˆ°è¾¾æ—¶ï¼Œå¤§è§„æ¨¡æ•°æ®çš„è®­ç»ƒå»¶è¿Ÿä¸¥é‡é˜»ç¢äº†å®æ—¶é€‚åº”ã€‚æ•°æ®é€‰æ‹©å¯ä»¥ç¼“è§£è¿™ç§å¼€é”€ï¼Œä½†ç°æœ‰ç­–ç•¥é€šå¸¸ä¾èµ–äºé¢„å…ˆè®­ç»ƒå¥½çš„å‚è€ƒæ¨¡å‹ï¼Œè¿™åœ¨CITè®¾ç½®ä¸­å¹¶ä¸å®ç”¨ï¼Œå› ä¸ºæœªæ¥æ•°æ®æœªçŸ¥ã€‚æœ€è¿‘çš„å‚è€ƒæ¨¡å‹æ— å…³çš„åœ¨çº¿æ ·æœ¬é€‰æ‹©æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸æŒ‰æ‰¹æ¬¡é€‰æ‹©å›ºå®šæ•°é‡çš„æ ·æœ¬ï¼ˆä¾‹å¦‚å‰kåï¼‰ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä¿¡æ¯åˆ†å¸ƒè·¨æ‰¹æ¬¡å˜åŒ–æ—¶å®¹æ˜“å—åˆ°å¹²æ‰°ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†OASISï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºCITçš„è‡ªé€‚åº”åœ¨çº¿æ ·æœ¬é€‰æ‹©æ–¹æ³•ï¼Œå®ƒï¼ˆ1ï¼‰é€šè¿‡ä¼°è®¡æ¯ä¸ªæ ·æœ¬ç›¸å¯¹äºæ‰€æœ‰å·²è§æ•°æ®çš„ä¿¡æ¯é‡æ¥é€‰æ‹©ä¿¡æ¯æ ·æœ¬ï¼Œè¶…è¶Šæ‰¹æ¬¡çº§åˆ«çš„çº¦æŸï¼›ï¼ˆ2ï¼‰é€šè¿‡è¿­ä»£é€‰æ‹©åˆ†æ•°æ›´æ–°æ¥å‡å°‘æ‰€é€‰æ ·æœ¬çš„ä¿¡æ¯å†—ä½™ã€‚åœ¨å¤§å‹åŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOASISä»…ä½¿ç”¨25%çš„æ•°æ®å³å¯å®ç°ä¸å…¨æ•°æ®è®­ç»ƒç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºæœ€æ–°çš„é‡‡æ ·æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨è¿ç»­æŒ‡ä»¤è°ƒæ•´ï¼ˆCITï¼‰åœºæ™¯ä¸­ï¼Œè®­ç»ƒå»¶è¿Ÿæ˜¯ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼Œå½±å“å®æ—¶é€‚åº”ã€‚</li>
<li>æ•°æ®é€‰æ‹©æ˜¯ç¼“è§£è®­ç»ƒå»¶è¿Ÿçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰ç­–ç•¥ä¾èµ–äºé¢„è®­ç»ƒå‚è€ƒæ¨¡å‹ï¼Œè¿™åœ¨CITä¸­ä¸å®ç”¨ã€‚</li>
<li>æœ€è¿‘çš„åœ¨çº¿æ ·æœ¬é€‰æ‹©æ–¹æ³•è¯•å›¾è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬æ˜“å—åˆ†å¸ƒå˜åŒ–çš„å¹²æ‰°ã€‚</li>
<li>OASISæ˜¯ä¸€ç§è‡ªé€‚åº”çš„åœ¨çº¿æ ·æœ¬é€‰æ‹©æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¼°è®¡æ¯ä¸ªæ ·æœ¬çš„ä¿¡æ¯é‡å¹¶é€‰æ‹©ä¿¡æ¯æ ·æœ¬æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>OASISè¶…è¶Šæ‰¹æ¬¡çº§åˆ«çº¦æŸï¼Œå¹¶é€šè¿‡è¿­ä»£é€‰æ‹©åˆ†æ•°æ›´æ–°æ¥å‡å°‘ä¿¡æ¯å†—ä½™ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒOASISä½¿ç”¨è¾ƒå°‘çš„æ•°æ®å³å¯å®ç°ä¸å…¨æ•°æ®è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d5bda6702cea7b2c63cb2dfc7820f8d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145726&auth_key=1760145726-0-0-442fd54245333d3072058cf64c94cf21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a7904d0638d1c942c8b6c1faf5868a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145733&auth_key=1760145733-0-0-bb2a331ced0912f2f65f0a35074f0f32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7639ee03b02bcc86ff95a0f8b5b57b84~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145740&auth_key=1760145740-0-0-f8e59c35802ab4d5bcd0c23148f06c16&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Sherkala-Chat-Building-a-State-of-the-Art-LLM-for-Kazakh-in-a-Moderately-Resourced-Setting"><a href="#Sherkala-Chat-Building-a-State-of-the-Art-LLM-for-Kazakh-in-a-Moderately-Resourced-Setting" class="headerlink" title="Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a   Moderately Resourced Setting"></a>Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a   Moderately Resourced Setting</h2><p><strong>Authors:Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokulakrishnan Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Preslav Nakov</strong></p>
<p>Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outper-forming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. To ensure effective and responsible alignment, we leverage translated instruction datasets, a Kazakhstan-specific instruction dataset that is automatically constructed and manually verified, and Kazakh-specific safety data. We release Sherkala-Chat (8B) as an open-weight model, along with a detailed description of its training, alignment, and evaluation, to support research and real-world applications for Kazakh speakers. </p>
<blockquote>
<p>Llama-3.1-Sherkala-8B-Chatï¼Œç®€ç§°Sherkala-Chatï¼ˆ8Bï¼‰ï¼Œæ˜¯ä¸€æ¬¾é’ˆå¯¹å“ˆè¨å…‹è¯­è®¾è®¡çš„æœ€æ–°æŒ‡ä»¤è°ƒä¼˜å¼€æ”¾ç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚Sherkala-Chatï¼ˆ8Bï¼‰æ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­ä½¿ç”¨è€…å¯¹LLMè¿›æ­¥çš„åŒ…å®¹æ€§ã€‚è¯¥æ¨¡å‹æ”¹ç¼–è‡ªLLaMA-3.1-8Bæ¨¡å‹ï¼Œå¹¶åœ¨å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­çš„45.3Bæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ‹¥æœ‰8äº¿å‚æ•°ï¼Œå®ƒåœ¨å“ˆè¨å…‹è¯­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨ç±»ä¼¼è§„æ¨¡çš„å¼€æ”¾å“ˆè¨å…‹è¯­å’Œå¤šè¯­ç§æ¨¡å‹ä¸­è¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨è‹±è¯­æ–¹é¢ä¹Ÿæœ‰ç«äº‰åŠ›ã€‚ä¸ºç¡®ä¿æœ‰æ•ˆå’Œè´Ÿè´£ä»»çš„å¯¹é½ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¿»è¯‘æŒ‡ä»¤æ•°æ®é›†ã€è‡ªåŠ¨æ„å»ºå’Œäººå·¥éªŒè¯çš„å“ˆè¨å…‹æ–¯å¦ç‰¹å®šæŒ‡ä»¤æ•°æ®é›†ä»¥åŠå“ˆè¨å…‹ç‰¹å®šå®‰å…¨æ•°æ®ã€‚æˆ‘ä»¬å°†Sherkala-Chatï¼ˆ8Bï¼‰ä½œä¸ºå¼€æ”¾æƒé‡æ¨¡å‹å‘å¸ƒï¼ŒåŒæ—¶æä¾›å…¶è®­ç»ƒã€å¯¹é½å’Œè¯„ä¼°çš„è¯¦ç»†æè¿°ï¼Œä»¥æ”¯æŒå“ˆè¨å…‹è¯­ä½¿ç”¨è€…çš„ç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01493v2">PDF</a> Accepted at COLM 2025</p>
<p><strong>Summary</strong></p>
<pre><code>Sherkala-Chatï¼ˆ8Bï¼‰æ˜¯ä¸€æ¬¾é¢å‘å“ˆè¨å…‹è¯­çš„å…ˆè¿›æŒ‡ä»¤è°ƒä¼˜å¼€æºç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å®ƒæ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­è¯´è¯è€…ä½¿ç”¨LLMçš„åŒ…å®¹æ€§ã€‚è¯¥æ¨¡å‹åŸºäºLLaMA-3.1-8Bæ¨¡å‹æ”¹ç¼–ï¼Œç»è¿‡å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­å…±45.3Bä»¤ç‰Œè®­ç»ƒã€‚æ‹¥æœ‰8äº¿å‚æ•°ï¼Œåœ¨å“ˆè¨å…‹è¯­çŸ¥è¯†å±•ç°å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å“è¶Šï¼Œä¸ç±»ä¼¼è§„æ¨¡çš„å¼€æºå“ˆè¨å…‹è¯­å’Œå¤šè¯­ç§æ¨¡å‹ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶åœ¨è‹±è¯­æ–¹é¢ä¹Ÿæœ‰ç«äº‰åŠ›è¡¨ç°ã€‚ä¸ºç¡®ä¿æœ‰æ•ˆå’Œè´Ÿè´£ä»»çš„å¯¹é½ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç¿»è¯‘æŒ‡ä»¤æ•°æ®é›†ã€è‡ªåŠ¨æ„å»ºå’Œäººå·¥éªŒè¯çš„å“ˆè¨å…‹æ–¯å¦ç‰¹å®šæŒ‡ä»¤æ•°æ®é›†ä»¥åŠå“ˆè¨å…‹ç‰¹å®šå®‰å…¨æ•°æ®ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒSherkala-Chatï¼ˆ8Bï¼‰ä½œä¸ºå¼€æºæƒé‡æ¨¡å‹ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å…¶è®­ç»ƒã€å¯¹é½å’Œè¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯ï¼Œä»¥æ”¯æŒå“ˆè¨å…‹è¯­ä½¿ç”¨è€…çš„ç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sherkala-Chatï¼ˆ8Bï¼‰æ˜¯ä¸€æ¬¾é¢å‘å“ˆè¨å…‹è¯­çš„å…ˆè¿›æŒ‡ä»¤è°ƒä¼˜å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åŸºäºLLaMA-3.1-8Bï¼Œè¿›è¡Œäº†å¤šè¯­è¨€è®­ç»ƒï¼ŒåŒ…æ‹¬å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­ã€‚</li>
<li>Sherkala-Chatï¼ˆ8Bï¼‰æ‹¥æœ‰å¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å“ˆè¨å…‹è¯­æ–¹é¢è¡¨ç°å“è¶Šã€‚</li>
<li>ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼ŒSherkala-Chatï¼ˆ8Bï¼‰åœ¨å“ˆè¨å…‹è¯­æ–¹é¢çš„æ€§èƒ½å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶åœ¨è‹±è¯­æ–¹é¢ä¹Ÿæœ‰ç«äº‰åŠ›ã€‚</li>
<li>ä¸ºäº†ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå®‰å…¨æ€§ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†å¤šç§æ•°æ®æ¥æºï¼ŒåŒ…æ‹¬ç¿»è¯‘æŒ‡ä»¤æ•°æ®é›†ã€ç‰¹å®šæŒ‡ä»¤æ•°æ®é›†å’Œå®‰å…¨æ•°æ®ã€‚</li>
<li>Sherkala-Chatï¼ˆ8Bï¼‰ä½œä¸ºå¼€æºæƒé‡æ¨¡å‹å‘å¸ƒï¼Œæ”¯æŒç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b51fa5bfd06495dd9978045968a3010f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145748&auth_key=1760145748-0-0-eface0193d2d326ac1ec768b500349d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-580b06d9a3b6c7494f3d0a1a0ebd752a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145755&auth_key=1760145755-0-0-01cf46340dbe25bc4c16c2cfdcaca7eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed7458ac1c5b66e0fa326c776609b86a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145761&auth_key=1760145761-0-0-3c5aba623a39936a0d0d795a517f2354&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-032121491e3224b8e9447a818f11faf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145768&auth_key=1760145768-0-0-c12fae783c952534446a1edee36a27a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  MATRIX Multimodal Agent Tuning for Robust Tool-Use Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d9d1ddb3c6472ed06fbff1a2c61fdce1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144625&auth_key=1760144625-0-0-affe5ab05b4f88b2c1413b36abaeda32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  MATRIX Multimodal Agent Tuning for Robust Tool-Use Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
