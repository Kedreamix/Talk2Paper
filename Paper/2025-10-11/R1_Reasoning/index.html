<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  MATRIX Multimodal Agent Tuning for Robust Tool-Use Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d9d1ddb3c6472ed06fbff1a2c61fdce1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144625&auth_key=1760144625-0-0-affe5ab05b4f88b2c1413b36abaeda32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-11-æ›´æ–°"><a href="#2025-10-11-æ›´æ–°" class="headerlink" title="2025-10-11 æ›´æ–°"></a>2025-10-11 æ›´æ–°</h1><h2 id="MATRIX-Multimodal-Agent-Tuning-for-Robust-Tool-Use-Reasoning"><a href="#MATRIX-Multimodal-Agent-Tuning-for-Robust-Tool-Use-Reasoning" class="headerlink" title="MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning"></a>MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</h2><p><strong>Authors:Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/MATRIX">https://github.com/mbzuai-oryx/MATRIX</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºæ§åˆ¶å™¨ï¼Œå¯ä»¥è®¿é—®å¤–éƒ¨å·¥å…·è¿›è¡Œå¤æ‚çš„æ¨ç†å’Œå†³ç­–ï¼Œç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§ä»ç„¶å—åˆ°é«˜è´¨é‡å¤šæ¨¡å¼è½¨è¿¹ç¨€ç¼ºå’Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»£ç†è°ƒæ•´æ¡†æ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åˆæˆå¤šæ¨¡å¼è½¨è¿¹ï¼Œç”Ÿæˆåˆ†æ­¥åå¥½å¯¹ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªç”¨äºç¨³å¥å·¥å…·ä½¿ç”¨æ¨ç†çš„VLMæ§åˆ¶å™¨ã€‚æˆ‘ä»¬çš„ç®¡é“é¦–å…ˆæ„å»ºM-TRACEï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«28.5Kå¤šæ¨¡å¼ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«177Kä¸ªéªŒè¯è¿‡çš„è½¨è¿¹ï¼Œä¸ºå®ç°åŸºäºæ¨¡ä»¿çš„è½¨è¿¹è°ƒæ•´æä¾›äº†å¯èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†MATRIX Agentæ§åˆ¶å™¨ï¼Œåœ¨M-TRACEä¸Šè¿›è¡Œå¾®è°ƒä»¥å®ç°åˆ†æ­¥å·¥å…·æ¨ç†ã€‚ä¸ºäº†å®ç°æ›´ç²¾ç»†çš„å¯¹é½ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†Pref-Xï¼Œè¿™æ˜¯ä¸€ç»„è‡ªåŠ¨ç”Ÿæˆçš„åŒ…å«11Kä¸ªåå¥½å¯¹çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡é€æ­¥åå¥½å­¦ä¹ ä¼˜åŒ–MATRIXã€‚åœ¨Agent-Xã€GTAå’ŒGAIAä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMATRIXå§‹ç»ˆè¶…è¿‡äº†å¼€æºå’Œé—­æºçš„VLMsï¼Œè¯æ˜äº†å…¶å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆçš„å¤šæ¨¡å¼å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/MATRIX%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mbzuai-oryx/MATRIXæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08567v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´ç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€è½¨è¿¹å’Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»£ç†è°ƒæ•´æ¡†æ¶ï¼Œå¯è‡ªåŠ¨åˆæˆå¤šæ¨¡æ€è½¨è¿¹ã€ç”Ÿæˆåˆ†æ­¥åå¥½å¯¹ï¼Œå¹¶è®­ç»ƒVLMæ§åˆ¶å™¨è¿›è¡Œç¨³å¥çš„å·¥å…·ä½¿ç”¨æ¨ç†ã€‚æˆ‘ä»¬æ„å»ºäº†å¤§å‹æ•°æ®é›†M-TRACEï¼ŒåŒ…å«2.85ä¸‡å¤šé¡¹å¤šæ¨¡æ€ä»»åŠ¡å’Œ17.7ä¸‡å¤šæ¡éªŒè¯è½¨è¿¹ï¼Œä¸ºåŸºäºæ¨¡ä»¿çš„è½¨è¿¹è°ƒæ•´æ‰“ä¸‹åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘å‡ºMATRIX Agentæ§åˆ¶å™¨ï¼Œåœ¨M-TRACEä¸Šè¿›è¡Œå¾®è°ƒä»¥å®ç°é€æ­¥å·¥å…·æ¨ç†ã€‚ä¸ºè¾¾åˆ°æ›´ç²¾ç»†çš„å¯¹é½ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥è‡ªåŠ¨ç”Ÿæˆçš„åå¥½å¯¹Pref-Xï¼Œå¹¶é€šè¿‡é€æ­¥åå¥½å­¦ä¹ ä¼˜åŒ–MATRIXã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMATRIXåœ¨å¼€æºå’Œé—­æºVLMsä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå±•ç°å‡ºå¯ä¼¸ç¼©ä¸”æœ‰æ•ˆçš„å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsä½œä¸ºæ§åˆ¶å™¨åœ¨å¤æ‚æ¨ç†å’Œå†³ç­–åˆ¶å®šä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ä»é¢ä¸´ç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€è½¨è¿¹å’Œæ‰‹åŠ¨æ ‡æ³¨çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»£ç†è°ƒæ•´æ¡†æ¶ï¼Œè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€è½¨è¿¹å’Œç”Ÿæˆåˆ†æ­¥åå¥½å¯¹ï¼Œä»¥æé«˜VLMsçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ„å»ºå¤§å‹æ•°æ®é›†M-TRACEï¼Œä¸ºåŸºäºæ¨¡ä»¿çš„è½¨è¿¹è°ƒæ•´æä¾›åŸºç¡€ã€‚</li>
<li>å¼€å‘MATRIX Agentæ§åˆ¶å™¨ï¼Œå¯åœ¨M-TRACEä¸Šè¿›è¡Œå¾®è°ƒå¹¶å®ç°é€æ­¥å·¥å…·æ¨ç†ã€‚</li>
<li>é€šè¿‡å¼•å…¥è‡ªåŠ¨ç”Ÿæˆçš„åå¥½å¯¹Pref-Xå’Œé€æ­¥åå¥½å­¦ä¹ æ¥ä¼˜åŒ–MATRIXã€‚</li>
<li>MATRIXåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šå¼€æºå’Œé—­æºçš„VLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-28ddb784a489005dee88460b30fc8aeb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144633&auth_key=1760144633-0-0-41248a29cab3a698fa3e11b1dea8d330&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c051f3faa5b5b1e5db4c849cb1025d9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144640&auth_key=1760144640-0-0-469a768c1e6c88719809cfc3ae2a6d0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2369f652f8a3e7a5d0a026d2529cd795~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144647&auth_key=1760144647-0-0-1e8d17e0df37dbede373c85d9901d23d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models"><a href="#SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models" class="headerlink" title="SciVideoBench: Benchmarking Scientific Video Reasoning in Large   Multimodal Models"></a>SciVideoBench: Benchmarking Scientific Video Reasoning in Large   Multimodal Models</h2><p><strong>Authors:Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang</strong></p>
<p>Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception&#x2F;recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging modelsâ€™ higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å„ç§èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨ç§‘å­¦é¢†åŸŸçš„å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦é’ˆå¯¹é«˜åº¦ä¾èµ–æ„ŸçŸ¥&#x2F;è¯†åˆ«ä¸”ç›¸å¯¹ç®€å•çš„æ¨ç†ä»»åŠ¡çš„ä¸€èˆ¬åœºæ™¯ï¼Œå¯¼è‡´é¥±å’Œï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°é«˜çº§çš„å¤šæ¨¡æ€è®¤çŸ¥æŠ€èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SciVideoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ ‡å‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°ç§‘å­¦èƒŒæ™¯ä¸‹çš„é«˜çº§è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚SciVideoBenchåŒ…å«ä»å‰æ²¿çš„ç§‘å­¦å®éªŒè§†é¢‘ä¸­ç²¾å¿ƒåˆ¶ä½œçš„1000é“é€‰æ‹©é¢˜ï¼Œæ¶µç›–è¶…è¿‡25ä¸ªä¸“ä¸šçš„å­¦æœ¯ä¸»é¢˜ï¼Œå¹¶é€šè¿‡åŠè‡ªåŠ¨ç³»ç»Ÿè¿›è¡ŒéªŒè¯ã€‚æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ã€ç²¾ç¡®çš„æ—¶ç©ºæ„ŸçŸ¥å’Œå¤æ‚çš„é€»è¾‘æ¨ç†ï¼Œæœ‰æ•ˆåœ°æŒ‘æˆ˜äº†æ¨¡å‹çš„é«˜çº§è®¤çŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒåŒ…æ‹¬Gemini 2.5 Proå’ŒQwen2.5-VLåœ¨å†…çš„æœ€æ–°ä¸“æœ‰å’Œå¼€æºå¤§å‹æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ç¼ºé™·ï¼Œè¿™è¡¨æ˜åœ¨è§†é¢‘æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚å¯¹æ¨ç†å¤æ‚æ€§å’Œè§†è§‰å®šä½ç­‰å…³é”®å› ç´ çš„æ·±å…¥åˆ†æä¸ºå¤§å‹æ¨¡å‹çš„æœªæ¥å‘å±•æä¾›äº†å®è´µçš„è§è§£å’Œæ˜ç¡®çš„æ–¹å‘ï¼Œæ¨åŠ¨çœŸæ­£å…·å¤‡èƒ½åŠ›çš„å¤šæ¨¡æ€AIç§‘å­¦å®¶çš„å‡ºç°ã€‚æˆ‘ä»¬å¸Œæœ›SciVideoBenchèƒ½å¼•èµ·ç¤¾åŒºçš„å…´è¶£ï¼Œå¹¶å¸®åŠ©æ¨åŠ¨å‰æ²¿AIåœ¨è¾¹ç•Œç§‘å­¦æ–¹é¢çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08559v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SciVideoBenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦èƒŒæ™¯ä¸‹çš„é«˜çº§è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚SciVideoBenchåŒ…å«ä»å‰æ²¿ç§‘å­¦å®éªŒè§†é¢‘ä¸­ç²¾å¿ƒåˆ¶ä½œçš„æ¶µç›–25ä¸ªå­¦ç§‘çš„å¤šä¸ªé€‰æ‹©é¢˜ï¼Œæ—¨åœ¨æµ‹è¯•å¤æ‚çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å’Œç²¾ç¡®æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜å½“å‰å…ˆè¿›çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œè¿™ä¸ºæœªæ¥çš„æ¨¡å‹å‘å±•æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SciVideoBenchæ˜¯ä¸“ä¸ºè¯„ä¼°ç§‘å­¦èƒŒæ™¯ä¸‹çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é«˜çº§è§†é¢‘æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥æµ‹è¯•åŒ…å«ä»å‰æ²¿ç§‘å­¦å®éªŒè§†é¢‘ä¸­ç²¾å¿ƒåˆ¶ä½œçš„æ¶µç›–å¤šä¸ªå­¦ç§‘çš„é€‰æ‹©é¢˜ã€‚</li>
<li>æµ‹è¯•æ¶µç›–äº†å¤æ‚çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å’Œç²¾ç¡®æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚</li>
<li>SciVideoBenchçš„å®éªŒè¯„ä¼°ä¸ºæœªæ¥å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•æä¾›äº†æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ceba1284acee7caddb0c38010c79bff1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144655&auth_key=1760144655-0-0-470955100aff86ff68d46e59f20b272f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b9777f85b5939ecb2833618c4a1510e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144663&auth_key=1760144663-0-0-488f4329905ab87bb57f18873017819d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c210a2a7add07d4bfdc9074fc7bc48f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144670&auth_key=1760144670-0-0-139a0632c707bc7bcdd6e74b71fe559e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20a10630151de910bd1a54adf09a1d91~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144677&auth_key=1760144677-0-0-7383844ea15b25246c76a0c0602c3e63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cfb18b9d259592c5b99a7314ccd6381~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144683&auth_key=1760144683-0-0-8df7a70c2e4115d75d39af8bf357f909&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f55525b0a569d835168cf6a66f4865b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144691&auth_key=1760144691-0-0-3ea71a5b4c5b240f4703289f35c5ec43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Reasoning-for-Diffusion-Language-Models-via-Group-Diffusion-Policy-Optimization"><a href="#Improving-Reasoning-for-Diffusion-Language-Models-via-Group-Diffusion-Policy-Optimization" class="headerlink" title="Improving Reasoning for Diffusion Language Models via Group Diffusion   Policy Optimization"></a>Improving Reasoning for Diffusion Language Models via Group Diffusion   Policy Optimization</h2><p><strong>Authors:Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</strong></p>
<p>Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce \textbf{Group Diffusion Policy Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks. </p>
<blockquote>
<p>æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰æ”¯æŒå¹¶è¡Œã€é¡ºåºæ— å…³ç”Ÿæˆå’Œè¿­ä»£ç»†åŒ–ï¼Œä¸ºè‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºæ¦‚ç‡çš„ä¸å¯è®¡ç®—æ€§ï¼Œå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒé€‚åº”DLMsä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚å¼€åˆ›æ€§å·¥ä½œå¦‚diffu-GRPOé€šè¿‡ä¸€æ­¥å»æ©ç ä¼°è®¡ä»¤ç‰Œçº§æ¦‚ç‡ã€‚è™½ç„¶è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä¸¥é‡çš„åè§ã€‚æ›´åŸåˆ™æ€§çš„åŸºç¡€åœ¨äºåºåˆ—çº§æ¦‚ç‡ï¼Œå…¶ä¸­è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ä½œä¸ºæ›¿ä»£ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰è¿™ç§æ¸…æ™°çš„æ•°å­¦è”ç³»ï¼ŒåŸºäºELBOçš„æ–¹æ³•ç”±äºæ¦‚ç‡è¯„ä¼°çš„æ˜‚è´µæˆæœ¬è€Œåº”ç”¨æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†ELBOä¼°è®¡å¹¶åˆ†è§£å…¶æ–¹å·®æ¥æºã€‚è¿™ç§åˆ†è§£ä¿ƒä½¿æˆ‘ä»¬é€šè¿‡å‡ ä¸ªå…³é”®ç»´åº¦ä¸Šçš„å¿«é€Ÿç¡®å®šæ€§ç§¯åˆ†è¿‘ä¼¼æ¥å‡å°‘æ–¹å·®ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹DLMsé‡èº«å®šåˆ¶çš„\textbf{Group Diffusion Policy Optimization (GDPO)}è¿™ä¸€æ–°RLç®—æ³•ã€‚GDPOåˆ©ç”¨ç®€å•æœ‰æ•ˆçš„åŠç¡®å®šæ€§è’™ç‰¹å¡æ´›æ–¹æ¡ˆï¼Œç¼“è§£äº†çº¯åŒé‡è’™ç‰¹å¡æ´›é‡‡æ ·ä¸‹ELBOä¼°è®¡å™¨çš„æ–¹å·®çˆ†ç‚¸é—®é¢˜ï¼Œåœ¨ä¸¥æ ¼è¯„ä¼°é¢„ç®—ä¸‹äº§ç”Ÿäº†å¯è¯æ˜çš„ä½æ–¹å·®ä¼°è®¡å™¨ã€‚ç»éªŒä¸Šï¼ŒGDPOåœ¨é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šå®ç°äº†æŒç»­çš„æ”¶ç›Šï¼Œå¹¶åœ¨å¤§å¤šæ•°æ•°å­¦ã€æ¨ç†å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†æœ€å‰æ²¿çš„åŸºçº¿diffu-GRPOã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08554v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰æ”¯æŒå¹¶è¡Œã€é¡ºåºæ— å…³ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–ï¼Œä¸ºå¤§å‹è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹DLMsçš„å¾®è°ƒä»å­˜åœ¨å¼€æ”¾æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨ä¸å¯é¢„æµ‹çš„ä¼¼ç„¶æ€§ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†ELBOä¼°è®¡å¹¶åˆ†è§£äº†å…¶æ–¹å·®æ¥æºã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹DLMså®šåˆ¶çš„æ–°çš„RLç®—æ³•â€”â€”é›†å›¢æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆGDPOï¼‰ã€‚GDPOåˆ©ç”¨ç®€å•æœ‰æ•ˆçš„åŠç¡®å®šæ€§è’™ç‰¹å¡æ´›æ–¹æ¡ˆï¼Œç¼“è§£äº†ELBOä¼°è®¡å™¨åœ¨æ™®é€šåŒé‡è’™ç‰¹å¡æ´›é‡‡æ ·ä¸‹çš„æ–¹å·®çˆ†ç‚¸é—®é¢˜ï¼Œåœ¨æœ‰é™è¯„ä¼°é¢„ç®—ä¸‹å¾—åˆ°äº†æ–¹å·®æ›´å°çš„ä¼°è®¡å™¨ã€‚ç»éªŒä¸Šï¼ŒGDPOåœ¨é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šå–å¾—äº†æŒç»­çš„æ”¶ç›Šï¼Œå¹¶åœ¨å¤§å¤šæ•°æ•°å­¦ã€æ¨ç†å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å‰æ²¿çš„åŸºçº¿ä¹‹ä¸€â€”â€”diffu-GRPOã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰å…·æœ‰å¹¶è¡Œç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–çš„ç‰¹ç‚¹ï¼Œæä¾›å¤§å‹è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„çµæ´»æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹DLMsçš„å¾®è°ƒæ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨ä¸å¯é¢„æµ‹çš„ä¼¼ç„¶æ€§ã€‚</li>
<li>ELBOä¼°è®¡è¢«æè®®ä½œä¸ºåºåˆ—çº§ä¼¼ç„¶æ€§çš„åŸºç¡€ï¼Œä½†ç”±äºä¼¼ç„¶æ€§è¯„ä¼°çš„æ˜‚è´µæˆæœ¬ï¼Œå…¶åº”ç”¨å—åˆ°é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡åˆ†è§£ELBOä¼°è®¡çš„æ–¹å·®æ¥æºï¼Œä»‹ç»äº†æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”é›†å›¢æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆGDPOï¼‰ã€‚</li>
<li>GDPOåˆ©ç”¨åŠç¡®å®šæ€§è’™ç‰¹å¡æ´›æ–¹æ¡ˆç¼“è§£ELBOä¼°è®¡çš„æ–¹å·®é—®é¢˜ï¼Œåœ¨æœ‰é™è¯„ä¼°é¢„ç®—ä¸‹è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>GDPOåœ¨é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šå–å¾—æŒç»­æ”¶ç›Šï¼Œå¹¶åœ¨æ•°å­¦ã€æ¨ç†å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå½“å‰æœ€å‰æ²¿çš„åŸºçº¿æ–¹æ³•diffu-GRPOã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cb590f5a4a7334f71436a8a106aba1a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144698&auth_key=1760144698-0-0-49120ae39a47a95b631327eb5beff161&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94ffa7476b939b90c924578864a82ee1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144705&auth_key=1760144705-0-0-a8edac98ee7e81784da5047245c77f36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SpatialLadder-Progressive-Training-for-Spatial-Reasoning-in-Vision-Language-Models"><a href="#SpatialLadder-Progressive-Training-for-Spatial-Reasoning-in-Vision-Language-Models" class="headerlink" title="SpatialLadder: Progressive Training for Spatial Reasoning in   Vision-Language Models"></a>SpatialLadder: Progressive Training for Spatial Reasoning in   Vision-Language Models</h2><p><strong>Authors:Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang</strong></p>
<p>Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence. </p>
<blockquote>
<p>ç©ºé—´æ¨ç†ä»ç„¶æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»éš¾ä»¥å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸€å±€é™æ€§æºäºä¸€ä¸ªå…³é”®å·®è·ï¼šç°æœ‰æ–¹æ³•è¯•å›¾ç›´æ¥å­¦ä¹ ç©ºé—´æ¨ç†ï¼Œè€Œæ²¡æœ‰å»ºç«‹æ„ŸçŸ¥å’Œç†è§£çš„å±‚æ¬¡åŸºç¡€ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€æ­¥æ„å»ºç©ºé—´æ™ºåŠ›çš„ç»¼åˆæ–¹æ³•ã€‚æˆ‘ä»¬ä»‹ç»äº†SpatialLadder-26kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«26610ä¸ªæ ·æœ¬çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæ¶µç›–å¯¹è±¡å®šä½ã€å•å›¾åƒã€å¤šè§†å›¾å’Œè§†é¢‘ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡æ ‡å‡†åŒ–ç®¡é“æ„å»ºï¼Œç¡®ä¿è·¨æ¨¡å¼çš„ç³»ç»Ÿè¦†ç›–ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼ˆ1ï¼‰é€šè¿‡å¯¹è±¡å®šä½å»ºç«‹ç©ºé—´æ„ŸçŸ¥ï¼Œï¼ˆ2ï¼‰é€šè¿‡å¤šç»´ç©ºé—´ä»»åŠ¡å‘å±•ç©ºé—´ç†è§£ï¼Œä»¥åŠï¼ˆ3ï¼‰é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åŠ å¼ºå¤æ‚æ¨ç†ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†SpatialLadderæ¨¡å‹ï¼Œä¸€ä¸ªå…·æœ‰3Bå‚æ•°çš„æ¨¡å‹ï¼Œåœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹³å‡æ¯”åŸºç¡€æ¨¡å‹æé«˜23.4%ï¼Œæ¯”GPT-4oé«˜å‡º20.8%ï¼Œæ¯”Gemini-2.0-Flashé«˜å‡º10.1%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSpatialLadderåœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šçš„æ”¹è¿›è¾¾åˆ°äº†7.2%ï¼Œè¿™è¡¨æ˜ä»æ„ŸçŸ¥åˆ°æ¨ç†çš„æ¸è¿›å¼è®­ç»ƒå¯¹äºç¨³å¥çš„ç©ºé—´æ™ºåŠ›è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08531v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://zju-real.github.io/SpatialLadder/">https://zju-real.github.io/SpatialLadder/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/ZJU-REAL/SpatialLadder">https://github.com/ZJU-REAL/SpatialLadder</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æŒ‡å‡ºç©ºé—´æ¨ç†æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„åŸºæœ¬æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ç›´æ¥å­¦ä¹ ç©ºé—´æ¨ç†è€Œç¼ºä¹æ„ŸçŸ¥å’Œç†è§£å±‚æ¬¡çš„å»ºç«‹ï¼Œå› æ­¤å­˜åœ¨æ€§èƒ½å±€é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§æ¸è¿›å»ºç«‹ç©ºé—´æ™ºèƒ½çš„ç»¼åˆæ–¹æ³•ï¼ŒåŒ…æ‹¬æ„å»ºSpatialLadder-26kå¤šæ¨¡æ€æ•°æ®é›†å’Œåˆ†é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæœ€ç»ˆæå‡ºSpatialLadderæ¨¡å‹ï¼Œå®ç°äº†å¯¹ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•çš„å¹³å‡æ”¹è¿›ç‡è¾¾23.4%ï¼Œå¹¶ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´æ¨ç†æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ€§èƒ½å±€é™ã€‚</li>
<li>å±€é™æ€§çš„æ ¹æºåœ¨äºç°æœ‰æ–¹æ³•è¯•å›¾ç›´æ¥å­¦ä¹ ç©ºé—´æ¨ç†ï¼Œè€Œæ²¡æœ‰å»ºç«‹æ„ŸçŸ¥å’Œç†è§£çš„å±‚æ¬¡ç»“æ„ã€‚</li>
<li>ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ„å»ºç©ºé—´æ™ºèƒ½çš„æ¸è¿›ç»¼åˆæ–¹æ³•ã€‚</li>
<li>å¼•å…¥SpatialLadder-26kå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«26,610ä¸ªæ ·æœ¬ï¼Œæ¶µç›–å¯¹è±¡å®šä½ã€å•å›¾ã€å¤šè§†å›¾å’Œè§†é¢‘ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¸è¿›è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬é€šè¿‡å¯¹è±¡å®šä½å»ºç«‹ç©ºé—´æ„ŸçŸ¥ã€é€šè¿‡å¤šç»´ç©ºé—´ä»»åŠ¡å‘å±•ç©ºé—´ç†è§£ã€ä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±åŠ å¼ºå¤æ‚æ¨ç†ã€‚</li>
<li>æœ€ç»ˆæå‡ºSpatialLadderæ¨¡å‹ï¼Œå®ç°å¯¹ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•çš„å¹³å‡æ”¹è¿›ç‡è¾¾23.4%ï¼Œè¶…è¿‡åŸºå‡†æ¨¡å‹ã€GPT-4oå’ŒGemini-2.0-Flashã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0540aa302fcff0797c9a5af3c66411df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144713&auth_key=1760144713-0-0-ccd902f7d931dfb3d172a3bb9c1be4f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cbf2ca629128e9d60ab29868434004a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144720&auth_key=1760144720-0-0-ee13b0446e287dfbda298bd852376cd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ed543b391545320342738fa57bc88e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144727&auth_key=1760144727-0-0-afee58fd212f6ee4d7a119cc16ca2ea6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b17489fce37ffbba419064f06891091~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144734&auth_key=1760144734-0-0-75ba9452f75fb3e79ac2028cdda4aec4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards"><a href="#CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards" class="headerlink" title="CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"></a>CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</h2><p><strong>Authors:Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai</strong></p>
<p>Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agentâ€™s policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents. </p>
<blockquote>
<p>è‡ªæˆ‘è¿›åŒ–æ˜¯ä½¿åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨é¢„è®­ç»ƒåèƒ½å¤ŸæŒç»­æé«˜å…¶èƒ½åŠ›çš„ä¸€ä¸ªæ ¸å¿ƒç ”ç©¶ä¸»é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶è§è¯äº†ä»éå¼ºåŒ–å­¦ä¹ å‘åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•çš„è½¬å˜ã€‚å½“å‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºå¯†é›†çš„å¤–éƒ¨å¥–åŠ±ä¿¡å·ï¼Œè¦ä¹ˆä»è¯­è¨€æ¨¡å‹æœ¬èº«ä¸­æå–å†…åœ¨å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸äººç±»æ™ºèƒ½ä¸­è§‚å¯Ÿåˆ°çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶ç›¸æ‚–ï¼Œä¸ªäººé€šè¿‡ç›¸äº’è®¨è®ºå’Œåä½œæ¥å­¦ä¹ å’Œæé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ååŒè¿›åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆCoMASï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿè®©æ™ºèƒ½ä½“é€šè¿‡ä»æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’ä¸­å­¦ä¹ æ¥è‡ªä¸»æé«˜ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚CoMASä»ä¸°å¯Œçš„è®¨è®ºåŠ¨æ€ä¸­äº§ç”Ÿå†…åœ¨å¥–åŠ±ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„æœºåˆ¶æ¥åˆ¶å®šè¿™äº›å¥–åŠ±ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œä»è€Œå®ç°å»ä¸­å¿ƒåŒ–å’Œå¯æ‰©å±•çš„ååŒè¿›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMASå§‹ç»ˆä¼˜äºæœªç»è®­ç»ƒçš„æ™ºèƒ½ä½“ï¼Œå¹¶åœ¨å¤§å¤šæ•°è¯„ä¼°ç¯å¢ƒä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¯å®äº†åŸºäºäº¤äº’çš„å¥–åŠ±ä¿¡å·çš„å¿…è¦æ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºéšç€æ™ºèƒ½ä½“æ•°é‡å’Œå¤šæ ·æ€§çš„å¢åŠ ï¼Œå‰æ™¯ååˆ†å¹¿é˜”çš„å¯æ‰©å±•æ€§ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†CoMASåœ¨åŸºäºè¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“è‡ªæˆ‘è¿›åŒ–ä¸­çš„æ–°å‹æœ‰æ•ˆèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08529v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è‡ªæˆ‘è¿›åŒ–æ˜¯ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨é¢„è®­ç»ƒåæŒç»­æé«˜å…¶èƒ½åŠ›çš„é‡è¦è¯¾é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶ç»å†äº†ä»éå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ°åŸºäºRLçš„æ–¹æ³•çš„è½¬å˜ã€‚å½“å‰åŸºäºRLçš„æ–¹æ³•ä¾èµ–äºå¯†é›†çš„å¤–ç•Œå¥–åŠ±ä¿¡å·æˆ–ä»LLMæœ¬èº«æå–çš„å†…åœ¨å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸äººç±»æ™ºèƒ½ä¸­çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶ç›¸æ‚–ï¼Œäººç±»é€šè¿‡ç›¸äº’è®¨è®ºå’Œåä½œæ¥å­¦ä¹ å’Œæé«˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶CoMASï¼Œå®ƒä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡ä»æ™ºèƒ½ä½“é—´çš„äº¤äº’ä¸­å­¦ä¹ æ¥è‡ªä¸»æé«˜ï¼Œæ— éœ€å¤–ç•Œç›‘ç£ã€‚CoMASä»ä¸°å¯Œçš„è®¨è®ºåŠ¨æ€ä¸­äº§ç”Ÿå†…åœ¨å¥–åŠ±ï¼Œåˆ©ç”¨LLMä½œä¸ºè£åˆ¤æ¥åˆ¶å®šè¿™äº›å¥–åŠ±ï¼Œå¹¶é€šè¿‡RLä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œä»è€Œå®ç°åˆ†æ•£å’Œå¯æ‰©å±•çš„ååŒè¿›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMASæŒç»­è¶…è¶Šæœªç»è®­ç»ƒçš„æ™ºèƒ½ä½“ï¼Œå¹¶åœ¨å¤§å¤šæ•°è¯„ä¼°ç¯å¢ƒä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CoMASæ¡†æ¶å…è®¸LLMé©±åŠ¨çš„æ™ºèƒ½ä½“é€šè¿‡æ™ºèƒ½ä½“é—´çš„äº¤äº’æ¥è‡ªä¸»æé«˜èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†ä¸°å¯Œçš„è®¨è®ºåŠ¨æ€ä»¥ç”Ÿæˆå†…åœ¨å¥–åŠ±ã€‚</li>
<li>LLMä½œä¸ºè£åˆ¤åˆ¶å®šå¥–åŠ±ä¿¡å·ä»¥ä¿ƒè¿›æ™ºèƒ½ä½“çš„è¿›åŒ–ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜CoMASåœ¨å¤šæ•°è¯„ä¼°ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†åŸºäºäº¤äº’çš„å¥–åŠ±ä¿¡å·çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c6c34837bff0f4ad8d24310829b02e6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144742&auth_key=1760144742-0-0-97652679149944b217facba0a5fcf8d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-949738b7676f3792c403fa91a7b3982e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144749&auth_key=1760144749-0-0-7718435e9de8c7534f3c7765600fc002&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e70f4c8c542fc2eed746a939aef97ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144756&auth_key=1760144756-0-0-e66b515b3ff4ce52eae79cab921e2703&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-STAR-Reinforcing-Open-Vocabulary-Action-Recognition-with-Tools"><a href="#Video-STAR-Reinforcing-Open-Vocabulary-Action-Recognition-with-Tools" class="headerlink" title="Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools"></a>Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools</h2><p><strong>Authors:Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li</strong></p>
<p>Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬å¯¹ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å…ˆéªŒçš„ä¾èµ–å¸¸å¸¸é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­è§£å¼€è¯­ä¹‰ä¸Šç›¸ä¼¼çš„åŠ¨ä½œçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Video-STARæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡å­è¿åŠ¨åˆ†è§£ä¸å·¥å…·å¢å¼ºå¼ºåŒ–å­¦ä¹ å®ç°å¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«ï¼ˆOVARï¼‰ã€‚ä¸åŒäºå°†åŠ¨ä½œè§†ä¸ºå•ä¸€å®ä½“çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ›æ–°åœ°å°†åŠ¨ä½œåˆ†è§£ä¸ºå…·æœ‰åŒºåˆ†åŠ›çš„å­è¿åŠ¨ï¼Œè¿›è¡Œç²¾ç»†åŒ¹é…ï¼ŒåŒæ—¶åŠ¨æ€è°ƒç”¨ç‰¹å®šé¢†åŸŸçš„å·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤é”™ï¼Œä»è€Œå®ç°ç‰¹å®šç±»åˆ«çš„æ¨ç†èƒ½åŠ›å¹¶å‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡ä¸€ç§å¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­è¿åŠ¨ç›¸å…³æ€§å’Œæ¨ç†ä¸­ç»“æ„è¿è´¯æ€§çš„åˆ†å±‚å¥–åŠ±ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªä¸»åœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥ä¼˜å…ˆå¤„ç†å­è¿åŠ¨æ¨¡å¼ï¼Œè€Œæ— éœ€æ˜ç¡®ç›‘ç£ï¼Œå®ç°ä»ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¨ç†åˆ°ä»¥è§†è§‰ä¸ºåŸºç¡€çš„æ¨ç†ã€‚åœ¨HMDB-51ã€UCF-10 1ã€SSv2ã€Kinetics-400å’ŒKinetics-600æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ€§èƒ½å¤„äºæœ€æ–°æ°´å¹³ï¼Œåœ¨åŒºåˆ†ç²¾ç»†åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„å‡ºè‰²ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Video-STARæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡å­è¿åŠ¨åˆ†è§£ä¸å·¥å…·å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†å¼€æ”¾è¯æ±‡è¡¨åŠ¨ä½œè¯†åˆ«ï¼ˆOVARï¼‰ã€‚ä¸ä¼ ç»Ÿçš„å°†åŠ¨ä½œè§†ä¸ºå•ä¸€å®ä½“çš„æ–¹æ³•ä¸åŒï¼ŒVideo-STARåˆ›æ–°åœ°å°†åŠ¨ä½œåˆ†è§£ä¸ºå…·æœ‰é‰´åˆ«åŠ›çš„å­è¿åŠ¨ï¼Œè¿›è¡Œç²¾ç»†åŒ¹é…ï¼Œå¹¶åŠ¨æ€è°ƒç”¨ç‰¹å®šé¢†åŸŸçš„å·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤ç»‡ï¼Œä»è€Œæé«˜ç±»åˆ«ç‰¹å®šçš„æ¨ç†èƒ½åŠ›ï¼Œå‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚é€šè¿‡è®¾è®¡å¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­è¿åŠ¨ç›¸å…³æ€§å’Œæ¨ç†ç»“æ„è¿è´¯æ€§çš„åˆ†å±‚å¥–åŠ±ï¼Œè¯¥æ–¹æ³•å¯è‡ªä¸»åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥ä¼˜å…ˆå¤„ç†å­è¿åŠ¨æ¨¡å¼ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ç›‘ç£ã€‚å¹¿æ³›è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒVideo-STARåœ¨HMDB-51ã€UCF-101ã€SSv2ã€Kinetics-400å’ŒKinetics-600æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°äº†å…¶å“è¶Šçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-STARæ¡†æ¶èåˆäº†ä¸Šä¸‹æ–‡å­è¿åŠ¨åˆ†è§£ä¸å·¥å…·å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡è¡¨åŠ¨ä½œè¯†åˆ«ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒVideo-STARå°†åŠ¨ä½œåˆ†è§£ä¸ºå­è¿åŠ¨è¿›è¡Œç²¾ç»†åŒ¹é…ï¼Œæé«˜äº†è¯†åˆ«ç²¾åº¦ã€‚</li>
<li>Video-STARé€šè¿‡åŠ¨æ€è°ƒç”¨ç‰¹å®šé¢†åŸŸçš„å·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤ç»‡ï¼Œå¢å¼ºäº†ç±»åˆ«ç‰¹å®šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡çš„åˆ†å±‚å¥–åŠ±æœºåˆ¶å¯è‡ªä¸»åˆ©ç”¨å¤–éƒ¨å·¥å…·å¤„ç†å­è¿åŠ¨æ¨¡å¼ï¼Œæ— éœ€æ˜ç¡®ç›‘ç£ã€‚</li>
<li>Video-STARåœ¨å¤„ç†ç²¾ç»†åŠ¨ä½œè¯†åˆ«å’Œè·¨æ¨¡æ€å¹»è§‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Video-STARåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœå‡æ˜¾ç¤ºå…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-512b3fd5d536e9d8b6b660b0e09c84d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144763&auth_key=1760144763-0-0-13a5d5512d18c8234399f4291b039ad1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ef1b3d28c5fe1ec91052b1b2826854a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144771&auth_key=1760144771-0-0-b23e70f9468e077e3946563337375537&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-736ccc17eae14cf8c7b1ee128db2ae5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144778&auth_key=1760144778-0-0-0b50beb5e569b19209119bebb322d4e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b80ee5dbebaae39dc7b3d1bd124a5f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144785&auth_key=1760144785-0-0-78e4d42eba096019bac47be759bef5b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-Provenance-Completeness-with-a-Learning-Based-Linux-Scheduler"><a href="#Rethinking-Provenance-Completeness-with-a-Learning-Based-Linux-Scheduler" class="headerlink" title="Rethinking Provenance Completeness with a Learning-Based Linux Scheduler"></a>Rethinking Provenance Completeness with a Learning-Based Linux Scheduler</h2><p><strong>Authors:Jinsong Mao, Benjamin E. Ujcich, Shiqing Ma</strong></p>
<p>Provenance plays a critical role in maintaining traceability of a systemâ€™s actions for root cause analysis of security threats and impacts. Provenance collection is often incorporated into the reference monitor of systems to ensure that an audit trail exists of all events, that events are completely captured, and that logging of such events cannot be bypassed. However, recent research has questioned whether existing state-of-the-art provenance collection systems fail to ensure the security guarantees of a true reference monitor due to the â€˜super producer threatâ€™ in which provenance generation can overload a system to force the system to drop security-relevant events and allow an attacker to hide their actions. One approach towards solving this threat is to enforce resource isolation, but that does not fully solve the problems resulting from hardware dependencies and performance limitations.   In this paper, we show how an operating systemâ€™s kernel scheduler can mitigate this threat, and we introduce Venus, a learned scheduler for Linux specifically designed for provenance. Unlike conventional schedulers that ignore provenance completeness requirements, Venus leverages reinforcement learning to learn provenance task behavior and to dynamically optimize resource allocation. We evaluate Venusâ€™s efficacy and show that Venus significantly improves both the completeness and efficiency of provenance collection systems compared to traditional scheduling, while maintaining reasonable overheads and even improving overall runtime in certain cases compared to the default Linux scheduler. </p>
<blockquote>
<p>æº¯æºåœ¨ç³»ç»Ÿè¡Œä¸ºçš„æº¯æºè¿½è¸ªä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œç”¨äºåˆ†æå®‰å…¨å¨èƒå’Œå½±å“çš„æ ¹æœ¬åŸå› ã€‚æº¯æºæ”¶é›†é€šå¸¸è¢«çº³å…¥ç³»ç»Ÿçš„å‚è€ƒç›‘è§†å™¨ä¸­ï¼Œä»¥ç¡®ä¿æ‰€æœ‰äº‹ä»¶éƒ½æœ‰å®¡è®¡è·Ÿè¸ªï¼Œäº‹ä»¶è¢«å®Œå…¨æ•è·ï¼Œå¹¶ä¸”æ­¤ç±»äº‹ä»¶çš„æ—¥å¿—è®°å½•ä¸èƒ½è¢«ç»•è¿‡ã€‚ç„¶è€Œï¼Œæœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜ï¼Œç”±äºâ€œè¶…çº§ç”Ÿäº§è€…å¨èƒâ€ï¼Œç°æœ‰æœ€å…ˆè¿›çš„æº¯æºæ”¶é›†ç³»ç»Ÿæ— æ³•ä¿è¯çœŸæ­£å‚è€ƒç›‘è§†å™¨çš„å®‰å…¨ä¿è¯ã€‚åœ¨è¿™ç§å¨èƒä¸­ï¼Œæº¯æºç”Ÿæˆä¼šè¿‡è½½ç³»ç»Ÿï¼Œè¿«ä½¿ç³»ç»Ÿä¸¢å¼ƒä¸å®‰å…¨ç›¸å…³çš„äº‹ä»¶ï¼Œå¹¶å…è®¸æ”»å‡»è€…éšè—å…¶è¡ŒåŠ¨ã€‚è§£å†³æ­¤å¨èƒçš„ä¸€ç§æ–¹æ³•æ˜¯å¼ºåˆ¶æ‰§è¡Œèµ„æºéš”ç¦»ï¼Œä½†è¿™å¹¶ä¸èƒ½å®Œå…¨è§£å†³ç”±ç¡¬ä»¶ä¾èµ–å’Œæ€§èƒ½é™åˆ¶äº§ç”Ÿçš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ“ä½œç³»ç»Ÿå†…æ ¸è°ƒåº¦å™¨å¦‚ä½•å‡è½»è¿™ç§å¨èƒï¼Œå¹¶ä»‹ç»äº†Venusï¼Œä¸€ç§ä¸“ä¸ºæº¯æºè®¾è®¡çš„Linuxå­¦ä¹ è°ƒåº¦å™¨ã€‚ä¸å¿½ç•¥æº¯æºå®Œæ•´æ€§è¦æ±‚çš„ä¼ ç»Ÿè°ƒåº¦å™¨ä¸åŒï¼ŒVenusåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥äº†è§£æº¯æºä»»åŠ¡è¡Œä¸ºï¼Œå¹¶åŠ¨æ€ä¼˜åŒ–èµ„æºåˆ†é…ã€‚æˆ‘ä»¬è¯„ä¼°äº†Venusçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜ä¸ä¼ ç»Ÿè°ƒåº¦ç›¸æ¯”ï¼ŒVenusåœ¨æ˜¾è‘—æé«˜æº¯æºæ”¶é›†ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œæ•ˆç‡çš„åŒæ—¶ï¼Œè¿˜ä¿æŒäº†åˆç†çš„å¼€é”€ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³ä¸Linuxé»˜è®¤è°ƒåº¦å™¨ç›¸æ¯”æé«˜äº†æ€»ä½“è¿è¡Œæ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç³»ç»Ÿæº¯æºå¯¹äºç»´æŠ¤ç³»ç»Ÿè¡Œä¸ºçš„å¯è¿½æº¯æ€§ã€åˆ†æå®‰å…¨å¨èƒçš„æ ¹æºè‡³å…³é‡è¦ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰å…ˆè¿›çš„æº¯æºæ”¶é›†ç³»ç»Ÿå­˜åœ¨â€œè¶…çº§ç”Ÿäº§è€…å¨èƒâ€ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿä¸¢å¤±å®‰å…¨ç›¸å…³äº‹ä»¶å¹¶å…è®¸æ”»å‡»è€…éšè—è¡ŒåŠ¨ã€‚æœ¬æ–‡å±•ç¤ºå¦‚ä½•é€šè¿‡æ“ä½œç³»ç»Ÿå†…æ ¸è°ƒåº¦å™¨ç¼“è§£è¿™ä¸€å¨èƒï¼Œå¹¶ä»‹ç»ä¸“ä¸ºæº¯æºè®¾è®¡çš„Linuxå­¦ä¹ è°ƒåº¦å™¨Venusã€‚Venusåˆ©ç”¨å¼ºåŒ–å­¦ä¹ äº†è§£æº¯æºä»»åŠ¡è¡Œä¸ºï¼Œå¹¶åŠ¨æ€ä¼˜åŒ–èµ„æºåˆ†é…ï¼Œåœ¨ä¿æŒåˆç†å¼€é”€çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æº¯æºç³»ç»Ÿçš„å®Œæ•´æ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ç›¸æ¯”é»˜è®¤Linuxè°ƒåº¦å™¨æ”¹å–„äº†æ•´ä½“è¿è¡Œæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æº¯æºåœ¨ç³»ç»Ÿå®‰å…¨ä¸­èµ·å…³é”®ä½œç”¨ï¼Œæœ‰åŠ©äºè¿½è¸ªç³»ç»Ÿè¡Œä¸ºä»¥è¿›è¡Œå®‰å…¨å¨èƒçš„æ ¹æºåˆ†æã€‚</li>
<li>ç°æœ‰æº¯æºæ”¶é›†ç³»ç»Ÿé¢ä¸´â€œè¶…çº§ç”Ÿäº§è€…å¨èƒâ€ï¼Œå¯èƒ½å½±å“å…¶å®‰å…¨æ€§å’Œå®Œæ•´æ€§ã€‚</li>
<li>â€œVenusâ€è°ƒåº¦å™¨æ˜¯ä¸“ä¸ºLinuxè®¾è®¡çš„æº¯æºå­¦ä¹ è°ƒåº¦å™¨ï¼Œå¯ä¼˜åŒ–èµ„æºåˆ†é…ã€‚</li>
<li>Venusåˆ©ç”¨å¼ºåŒ–å­¦ä¹ äº†è§£æº¯æºä»»åŠ¡è¡Œä¸ºï¼Œä»¥æé«˜ç³»ç»Ÿçš„æº¯æºå®Œæ•´æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿè°ƒåº¦å™¨ç›¸æ¯”ï¼ŒVenusåœ¨ä¿æŒåˆç†å¼€é”€çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æº¯æºç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒVenusç”šè‡³å¯èƒ½åœ¨æ•´ä½“è¿è¡Œæ—¶é—´ä¸Šè¶…è¿‡é»˜è®¤Linuxè°ƒåº¦å™¨çš„è¡¨ç°ã€‚</li>
<li>Venusçš„å¼•å…¥ä¸ºç¼“è§£ç³»ç»Ÿå®‰å…¨å¨èƒæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d6237cb109feb0b327d4d50a35aca981~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144793&auth_key=1760144793-0-0-640c8cfc80416c40dee557a7f7104635&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a00841703b43e21b788396c6b8d67d2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144800&auth_key=1760144800-0-0-8690d850408a9ec87852613ca5851979&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d537425c539dd7738d4d07616e49a39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144807&auth_key=1760144807-0-0-5bbc694fad22bae25d1290c4c753908f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f02cd600a4217dbf8ed6dffa4aae070f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144813&auth_key=1760144813-0-0-5e466ca2ec841c6699d2a69f5ca53d2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-393fd67b596153045c20f2c062456291~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144820&auth_key=1760144820-0-0-36e6fe694b88aa0da3d2e6233fe2baea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization"><a href="#Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization" class="headerlink" title="Reinforcing Diffusion Models by Direct Group Preference Optimization"></a>Reinforcing Diffusion Models by Direct Group Preference Optimization</h2><p><strong>Authors:Yihong Luo, Tianyang Hu, Jing Tang</strong></p>
<p>While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/DGPO">https://github.com/Luo-Yihong/DGPO</a>. </p>
<blockquote>
<p>è™½ç„¶é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å·²ç»å¤§å¤§å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆæœï¼Œä½†å°†å…¶é€‚åº”æ‰©æ•£æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGRPOè¦æ±‚éšæœºç­–ç•¥ï¼Œè€Œæˆæœ¬æ•ˆç›Šæœ€é«˜çš„æ‰©æ•£é‡‡æ ·å™¨æ˜¯åŸºäºç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„ã€‚è¿‘æœŸçš„å·¥ä½œé€šè¿‡ä½¿ç”¨åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„é‡‡æ ·å™¨æ¥å¼•å…¥éšæœºæ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ç§å¯¹æ¨¡å‹ä¸å¯çŸ¥çš„é«˜æ–¯å™ªå£°çš„ä¾èµ–å¯¼è‡´äº†æ”¶æ•›é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†ç›´æ¥é›†å›¢åå¥½ä¼˜åŒ–ï¼ˆDGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒå®Œå…¨æ‘’å¼ƒäº†åŸºäºç­–ç•¥æ¢¯åº¦çš„æ¡†æ¶ã€‚DGPOç›´æ¥ä»ç¾¤ä½“å±‚é¢çš„åå¥½ä¸­å­¦ä¹ ï¼Œè¿™åˆ©ç”¨äº†ç¾¤ä½“å†…æ ·æœ¬çš„ç›¸å¯¹ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†å¯¹ä½æ•ˆéšæœºç­–ç•¥çš„éœ€æ±‚ï¼Œè§£é”äº†é«˜æ•ˆç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹é‡‡æ ·å™¨çš„ä½¿ç”¨ï¼Œå¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼ŒDGPOçš„è®­ç»ƒé€Ÿåº¦æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«çº¦20å€ï¼Œåœ¨åŸŸå†…å’ŒåŸŸå¤–å¥–åŠ±æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/DGPO">https://github.com/Luo-Yihong/DGPO</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08425v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¦‚é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å°†å…¶é€‚åº”æ‰©æ•£æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚GRPOè¦æ±‚ç­–ç•¥å…·æœ‰éšæœºæ€§ï¼Œè€Œæˆæœ¬æ•ˆç›Šæœ€é«˜çš„æ‰©æ•£é‡‡æ ·å™¨å´æ˜¯åŸºäºç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡ä½¿ç”¨åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰çš„é‡‡æ ·å™¨æ¥å¼•å…¥éšæœºæ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ä¾èµ–äºæ¨¡å‹æ— å…³çš„é«˜æ–¯å™ªå£°ï¼Œå¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢ã€‚ä¸ºè§£å†³è¿™ä¸€å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†ç›´æ¥é›†å›¢åå¥½ä¼˜åŒ–ï¼ˆDGPOï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒå®Œå…¨æ‘’å¼ƒäº†åŸºäºç­–ç•¥æ¢¯åº¦çš„æ¡†æ¶ã€‚DGPOç›´æ¥ä»é›†å›¢å±‚é¢çš„åå¥½ä¸­å­¦ä¹ ï¼Œåˆ©ç”¨é›†å›¢å†…æ ·æœ¬çš„ç›¸å¯¹ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†å¯¹ä½æ•ˆéšæœºç­–ç•¥çš„éœ€æ±‚ï¼Œå®ç°äº†é«˜æ•ˆç¡®å®šæ€§ODEé‡‡æ ·å™¨çš„ä½¿ç”¨å¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDGPOçš„è®­ç»ƒé€Ÿåº¦æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«çº¦20å€ï¼Œåœ¨åŸŸå†…å’ŒåŸŸå¤–å¥–åŠ±æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/DGPO">https://github.com/Luo-Yihong/DGPO</a> æ‰¾åˆ°ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€‚åº”æ‰©æ•£æ¨¡å‹æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚GRPOè¦æ±‚ç­–ç•¥å…·æœ‰éšæœºæ€§ï¼Œè€Œæœ€æœ‰æ•ˆçš„æ‰©æ•£é‡‡æ ·å™¨æ˜¯ç¡®å®šæ€§çš„ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶é€šè¿‡å¼•å…¥SDE-basedé‡‡æ ·å™¨æ¥å¢åŠ éšæœºæ€§ï¼Œä½†è¿™å¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢ä¸”ä¾èµ–äºæ¨¡å‹æ— å…³çš„é«˜æ–¯å™ªå£°ã€‚</li>
<li>DGPOç®—æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡ç›´æ¥ä»é›†å›¢å±‚é¢çš„åå¥½å­¦ä¹ ï¼Œæ‘’å¼ƒäº†ç­–ç•¥æ¢¯åº¦æ¡†æ¶ã€‚</li>
<li>DGPOçš„è®¾è®¡æ¶ˆé™¤äº†å¯¹éšæœºç­–ç•¥çš„éœ€æ±‚ï¼Œå…è®¸ä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§ODEé‡‡æ ·å™¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºDGPOè®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ€§èƒ½å“è¶Šï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ä¼˜åŠ¿æ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a4b3a563c5bc8556e9a5894fc08c3ba4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144828&auth_key=1760144828-0-0-0936dfa5e4a54442b0e41abbedb16deb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71a94678894374f1718fccd80acd7ff3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144835&auth_key=1760144835-0-0-1eab6c7de223b22362fa14118e48affd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts"><a href="#FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts" class="headerlink" title="FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts"></a>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts</h2><p><strong>Authors:Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains â€“ general knowledge understanding, scientific question answering, mathematical reasoning, and code generation â€“ demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA">https://github.com/gfyddha/FlyLoRA</a>. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºåŸºç¡€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä½†å®ƒå­˜åœ¨å‚æ•°å¹²æ‰°çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚è™½ç„¶åŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„LoRAå˜ä½“åœ¨å•ä»»åŠ¡æŒ‡ä»¤å¾®è°ƒä¸­æ˜¾ç¤ºå‡ºç¼“è§£ä»»åŠ¡å†…å…³è”çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¼•å…¥äº†é¢å¤–çš„è·¯ç”±å™¨å‚æ•°ï¼Œå¹¶ä¸”åœ¨å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­ä»ç„¶æ— æ•ˆï¼Œå› ä¸ºåœ¨è¿™é‡Œä¼šå‡ºç°ä»»åŠ¡é—´å¹²æ‰°ã€‚å—è‹è‡å—…è§‰å›è·¯çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FlyLoRAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºéšå¼MoEçš„LoRAå˜ä½“ï¼Œå®ƒå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰åœ¨æŠ•å½±çŸ©é˜µä¸­è¿›è¡Œç­‰çº§ä¸“å®¶æ¿€æ´»ï¼›ï¼ˆ2ï¼‰ä¸€ç§éšå¼è·¯ç”±å™¨ï¼Œç»Ÿä¸€ä¸“å®¶è·¯ç”±å’Œä¸‹æŠ•å½±ï¼Œå…¶ä¸­ç”¨å›ºå®šçš„ç¨€ç–éšæœºæŠ•å½±çŸ©é˜µä»£æ›¿ä¼ ç»Ÿçš„å¯†é›†å¯è®­ç»ƒç‰ˆæœ¬ã€‚è¿™ç§è®¾è®¡é€šè¿‡æ¶ˆé™¤å¯¹æ˜¾å¼è·¯ç”±å™¨çš„éœ€æ±‚ï¼Œè§£å†³äº†ä»»åŠ¡å†…å»ç›¸å…³å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼ŒåŒæ—¶ç”±äºå…¶éšæœºçŸ©é˜µçš„æ­£äº¤æ€§å±æ€§ï¼Œå›ºæœ‰åœ°å‡è½»äº†ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚åœ¨å››ä¸ªé¢†åŸŸâ€”â€”é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆâ€”â€”çš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFlyLoRAçš„æ€§èƒ½å¾—åˆ°äº†ä¸€è‡´çš„æå‡ã€‚é™¤äº†å®è¯æ”¶ç›Šå¤–ï¼ŒFlyLoRAè¿˜å¼ºè°ƒäº†ç”Ÿç‰©ç»“æ„å¦‚ä½•æ¿€å‘AIæŠ€æœ¯çš„åˆ›æ–°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gfyddha/FlyLoRAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08396v1">PDF</a> NeurIPS 2025 accepted paper</p>
<p><strong>Summary</strong><br>     é£æ´›æ‹‰ï¼ˆFlyLoRAï¼‰æ˜¯ä¸€ç§åŸºäºéšæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å˜ä½“ï¼Œé€šè¿‡å¼•å…¥æ’åä¸“å®¶æ¿€æ´»å’Œéšè·¯ç”±æœºåˆ¶ï¼Œè§£å†³äº†å‚æ•°å¹²æ‰°é—®é¢˜ã€‚å®ƒåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œå¦‚é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ã€‚å…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨éšæœºçŸ©é˜µçš„æ­£äº¤æ€§ï¼Œæœ‰æ•ˆç¼“è§£ä»»åŠ¡é—´å¹²æ‰°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é£æ´›æ‹‰ï¼ˆFlyLoRAï¼‰æ˜¯ä¸€ç§é’ˆå¯¹åŸºç¡€æ¨¡å‹çš„å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒæ–¹æ³•ï¼ŒåŸºäºéšæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ€æœ¯ã€‚</li>
<li>é£æ´›æ‹‰è§£å†³äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸­çš„å‚æ•°å¹²æ‰°é—®é¢˜ã€‚</li>
<li>é£æ´›æ‹‰é€šè¿‡å¼•å…¥æ’åä¸“å®¶æ¿€æ´»å’Œéšè·¯ç”±æœºåˆ¶ï¼Œåœ¨å•ä»»åŠ¡æŒ‡ä»¤å¾®è°ƒä¸­å‡è½»äº†ä»»åŠ¡å†…ç›¸å…³æ€§ã€‚</li>
<li>é£æ´›æ‹‰åˆ©ç”¨éšæœºçŸ©é˜µçš„å›ºæœ‰æ­£äº¤æ€§ï¼Œæœ‰æ•ˆç¼“è§£å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­çš„ä»»åŠ¡é—´å¹²æ‰°ã€‚</li>
<li>é£æ´›æ‹‰åœ¨å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é£æ´›æ‹‰çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-49ea1c2a9cb8333300a75733cd10a794~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144843&auth_key=1760144843-0-0-35a9d19b2900b6a1d84e5e19a3e59cdb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d41ed60610223d6aeb8776f73cc27110~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144850&auth_key=1760144850-0-0-053d5cf9fabd963f91a58ca45904844b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b6ef07a2c98c73372937f9512da163b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144857&auth_key=1760144857-0-0-c68d500d7f9ee3b31ad8f9142e693252&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c25a82fe3f965c063516f972603997a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144864&auth_key=1760144864-0-0-848beb966c9f7f8706c181c0aa204af2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Beyond-Pass-k-Breadth-Depth-Metrics-for-Reasoning-Boundaries"><a href="#Beyond-Pass-k-Breadth-Depth-Metrics-for-Reasoning-Boundaries" class="headerlink" title="Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries"></a>Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries</h2><p><strong>Authors:Marius Dragoi, Ioana Pintilie, Florin Gogianu, Florin Brad</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ç¼–ç¨‹ã€æ•°å­¦æˆ–é€»è¾‘ç­‰æ¨ç†ä»»åŠ¡ä¸Šæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†è¯„ä¼°æ¨ç†è¾¹ç•Œï¼ˆæ¨¡å‹å¯è§£å†³çš„é—®é¢˜éƒ¨åˆ†ï¼‰ï¼Œç ”ç©¶è€…é€šå¸¸ä¼šåœ¨å¤§é‡‡æ ·é¢„ç®—ä¸‹æŠ¥å‘ŠPass@kã€‚æœ€è¿‘çš„ç»“æœæ­ç¤ºäº†ä¸€ç§äº¤å‰ç°è±¡ï¼šå°½ç®¡åœ¨è¾ƒå°çš„kå€¼ä¸‹ï¼ŒRLVRæ¨¡å‹çš„è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†å½“é‡‡æ ·å¤§é‡çš„å®Œæˆæ—¶ï¼ŒåŸºç¡€æ¨¡å‹é€šå¸¸è¡¨ç°æ›´å¥½ã€‚è¿™è¢«è®¤ä¸ºæ˜¯åŸºç¡€æ¨¡å‹å…·æœ‰æ›´å¤§æ¨ç†è¾¹ç•Œçš„è¯æ®ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å…·æœ‰ç¦»æ•£ç­”æ¡ˆç©ºé—´çš„ä»»åŠ¡ï¼ˆå¦‚å…·æœ‰æ•°å­—è¾“å‡ºçš„æ•°å­¦ï¼‰ä¸­ï¼Œå¤§kå€¼çš„Pass@kåæ˜ äº†åœ¨è¯•éªŒæ¬¡æ•°æ— é™å¢åŠ çš„æƒ…å†µä¸‹è¶Šæ¥è¶Šé«˜çš„æˆåŠŸæœºä¼šï¼Œè€Œä¸æ˜¯çœŸæ­£çš„æ¨ç†ï¼Œå› æ­¤å¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†Cover@tauï¼Œå®ƒè¡¡é‡çš„æ˜¯æ¨¡å‹èƒ½å¤Ÿè§£å†³çš„é—®é¢˜ä¸­ï¼Œè‡³å°‘æœ‰ä¸€éƒ¨åˆ†ï¼ˆtauæ¯”ä¾‹ï¼‰çš„å®Œæˆæ˜¯æ­£ç¡®çš„ã€‚ä¸Pass@kä¸åŒï¼ŒCover@tauåœ¨æ˜ç¡®çš„å¯é æ€§é˜ˆå€¼ä¸‹æ•æ‰æ¨ç†ï¼šä¾èµ–éšæœºçŒœæµ‹çš„æ¨¡å‹éšç€tauçš„å¢åŠ è€Œè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºCover@tauçš„åº¦é‡æ ‡å‡†è¯„ä¼°äº†å¤šä¸ªRLVRæ¨¡å‹ï¼Œå¹¶è¯´æ˜äº†ä¸Pass@1ç›¸æ¯”ï¼Œæµè¡Œç®—æ³•ç›¸å¯¹æ’åçš„å˜åŒ–ï¼Œä¸ºæ¨ç†è¾¹ç•Œæä¾›äº†ä¸åŒçš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08325v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èŒƒå¼åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–ç¨‹ã€æ•°å­¦æˆ–é€»è¾‘ç­‰æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç ”ç©¶ä¸­å¸¸é€šè¿‡Pass@kæŒ‡æ ‡è¯„ä¼°æ¨¡å‹è§£å†³é—®é¢˜çš„è¾¹ç•Œï¼Œä½†åœ¨å¤§æ ·æœ¬é¢„ç®—ä¸‹å­˜åœ¨äº¤å‰ç°è±¡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨ç¦»æ•£ç­”æ¡ˆç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ•°å­¦ç­‰å…·æœ‰æ•°å€¼è¾“å‡ºçš„ä»»åŠ¡ï¼ŒPass@kæŒ‡æ ‡åœ¨å¤§å‹kå€¼ä¸Šåæ˜ çš„æ˜¯å°è¯•æ¬¡æ•°è¶Šå¤šæˆåŠŸæ¦‚ç‡è¶Šé«˜çš„æƒ…å†µï¼Œè€ŒéçœŸæ­£çš„æ¨ç†èƒ½åŠ›ï¼Œå¯èƒ½å…·æœ‰è¯¯å¯¼æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºCover@tauæŒ‡æ ‡ï¼Œè¡¡é‡æ¨¡å‹è§£å†³è‡³å°‘tauæ¯”ä¾‹é—®é¢˜çš„æ¯”ä¾‹ã€‚ä¸Pass@kä¸åŒï¼ŒCover@tauåœ¨æ˜ç¡®çš„å¯é æ€§é˜ˆå€¼ä¸‹è¡¡é‡æ¨ç†èƒ½åŠ›ï¼šä¾èµ–éšæœºçŒœæµ‹çš„æ¨¡å‹éšç€tauçš„å¢åŠ ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬å¯¹ä½¿ç”¨Cover@tauæŒ‡æ ‡çš„RLVRæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†ä¸Pass@1ç›¸æ¯”ç®—æ³•ç›¸å¯¹æ’åçš„å˜åŒ–ï¼Œä¸ºç†è§£æ¨ç†è¾¹ç•Œæä¾›äº†ä¸åŒè§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRèŒƒå¼åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
<li>Pass@kæŒ‡æ ‡åœ¨å¤§æ ·æœ¬é¢„ç®—ä¸‹å­˜åœ¨äº¤å‰ç°è±¡ï¼Œå¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹è§£å†³é—®é¢˜è¾¹ç•Œçš„è¯¯è§£ã€‚</li>
<li>åœ¨ç¦»æ•£ç­”æ¡ˆç©ºé—´çš„ä»»åŠ¡ä¸­ï¼ŒPass@kæŒ‡æ ‡å¯èƒ½æ— æ³•çœŸå®åæ˜ æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Cover@tauæŒ‡æ ‡èƒ½å¤Ÿè¡¡é‡æ¨¡å‹åœ¨æ˜ç¡®å¯é æ€§é˜ˆå€¼ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¾èµ–éšæœºçŒœæµ‹çš„æ¨¡å‹åœ¨Cover@tauæŒ‡æ ‡ä¸‹ä¼šè¿…é€Ÿä¸‹é™ã€‚</li>
<li>ä½¿ç”¨Cover@tauæŒ‡æ ‡çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºç®—æ³•ç›¸å¯¹æ’åçš„å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a5bcc7ddd367a7e3d71552aafbb7231b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144871&auth_key=1760144871-0-0-aa18a3a7013ee5673e2dd23ff3a0b60f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1661896138b2a092512efd36c6a9c3d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144879&auth_key=1760144879-0-0-836d8d2582c57ed3d9e2d11b738efed1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd42cf5e8f2cfe1f5dc94f0024f710ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144886&auth_key=1760144886-0-0-9ece8f4d932a3f07a34722669dbaa326&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b96734b1173402ac8129b9d80cf487e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144892&auth_key=1760144892-0-0-96ae0e27566d971acd35c5d755701950&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window"><a href="#Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window" class="headerlink" title="Beyond Turn Limits: Training Deep Search Agents with Dynamic Context   Window"></a>Beyond Turn Limits: Training Deep Search Agents with Dynamic Context   Window</h2><p><strong>Authors:Qiaoyu Tang, Hao Xiang, Le Yu, Bowen Yu, Yaojie Lu, Xianpei Han, Le Sun, WenJuan Zhang, Pengbo Wang, Shixuan Liu, Zhenru Zhang, Jianhong Tu, Hongyu Lin, Junyang Lin</strong></p>
<p>While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems. </p>
<blockquote>
<p>è™½ç„¶æœ€è¿‘çš„æ¨ç†æ¨¡å‹è¿›å±•é€šè¿‡å¼ºåŒ–å­¦ä¹ å±•ç¤ºäº†è®¤çŸ¥è¡Œä¸ºï¼Œä½†ç°æœ‰æ–¹æ³•å¾ˆéš¾åœ¨å…·æœ‰é•¿æœŸè§†é‡äº¤äº’çš„å¤šè½®ä»£ç†ä¸­æ¿€å‘æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†DeepMinerï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼•å…¥é«˜éš¾åº¦è®­ç»ƒä»»åŠ¡å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡çª—å£æ¥æ¿€å‘æ­¤ç±»èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚DeepMineré‡‡ç”¨åå‘æ„å»ºæ–¹æ³•ï¼Œä»çœŸå®ç½‘ç»œæ¥æºç”Ÿæˆå¤æ‚ä½†å¯éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¿™ç¡®ä¿äº†è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜æ€§å’Œå¯é æ€§ï¼ŒåŒæ—¶å°†è®¤çŸ¥èƒ½åŠ›æ³¨å…¥å¤šè½®æ¨ç†åœºæ™¯ä¸­ã€‚æˆ‘ä»¬è¿˜ä¸ºè®­ç»ƒå’Œæ¨ç†è®¾è®¡äº†ä¸€ä¸ªä¼˜é›…è€Œæœ‰æ•ˆçš„åŠ¨æ€ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ï¼Œåˆ©ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œæ¶ˆé™¤å¯¹å¤–éƒ¨æ‘˜è¦æ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œæœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹å¤„ç†ä¸æ–­æ‰©å±•çš„é•¿æœŸè§†é‡ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨Qwen3-32Bä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬å¼€å‘äº†DeepMiner-32Bï¼Œåœ¨å¤šä¸ªæœç´¢ä»£ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚DeepMineråœ¨BrowseComp-enä¸Šè¾¾åˆ°äº†33.5%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€ä½³çš„å¼€æºä»£ç†è¿‘20ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨BrowseComp-zhã€XBench-DeepSearchå’ŒGAIAä¸Šè¡¨ç°å‡ºæŒç»­çš„å¯æ”¹è¿›æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŠ¨æ€ä¸Šä¸‹æ–‡ç®¡ç†å¯å®ç°è¿‘100è½®çš„æ ‡å‡†3.2ä¸‡è¯­å¢ƒé•¿åº¦å†…çš„æŒç»­äº¤äº’ï¼Œæœ‰æ•ˆè§£å†³ç°æœ‰å¤šè½®äº¤äº’ç³»ç»Ÿçš„è¯­å¢ƒå±€é™æ€§é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08276v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦æŒ–æ˜ï¼ˆDeepMinerï¼‰æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥é«˜éš¾åº¦è®­ç»ƒä»»åŠ¡å’ŒåŠ¨æ€è¯­å¢ƒçª—å£ï¼Œæ¿€å‘å¤šè½®äº¤äº’ä¸­çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åå‘æ„å»ºæ–¹æ³•ç”Ÿæˆæ¥è‡ªçœŸå®ç½‘ç»œèµ„æºçš„å¤æ‚ä½†å¯éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜æ€§å’Œå¯é æ€§ï¼Œå¹¶æ³¨å…¥å¤šè½®æ¨ç†åœºæ™¯çš„è®¤çŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨Qwen3-32Bä¸Šçš„å®éªŒï¼ŒDeepMiner-32Båœ¨å¤šä¸ªæœç´¢ä»£ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepMineræ¡†æ¶é€šè¿‡å¼•å…¥é«˜éš¾åº¦è®­ç»ƒä»»åŠ¡å’ŒåŠ¨æ€è¯­å¢ƒçª—å£ï¼Œèƒ½å¤Ÿæ¿€å‘å¤šè½®äº¤äº’ä¸­çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨åå‘æ„å»ºæ–¹æ³•ç”ŸæˆçœŸå®ä¸”å¯éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜æ€§å’Œå¯é æ€§ã€‚</li>
<li>åŠ¨æ€è¯­å¢ƒç®¡ç†ç­–ç•¥ç”¨äºè®­ç»ƒå’Œæ¨ç†ï¼Œèƒ½æœ‰æ•ˆå¤„ç†ä¸æ–­æ‰©å±•çš„é•¿æœŸè¯­å¢ƒã€‚</li>
<li>DeepMineræ¡†æ¶æ— éœ€ä¾èµ–å¤–éƒ¨æ‘˜è¦æ¨¡å‹ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶æé«˜æ•ˆç‡ã€‚</li>
<li>é€šè¿‡åœ¨Qwen3-32Bä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒDeepMiner-32Bæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶åœ¨å¤šä¸ªæœç´¢ä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>DeepMineråœ¨BrowseComp-enä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†33.5%ï¼Œè¿œè¶…ä¹‹å‰å¼€æºä»£ç†çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-57ed116786cce55fc80da67e3e3bbd8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144900&auth_key=1760144900-0-0-0d011723ca4eb4ec9d07f2d46a30117b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-088612310744cec2fad6ddcfdac17a6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144907&auth_key=1760144907-0-0-d7060edc3668e569ff6c86e9ff91b0e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-114502022f12501b5b73219558e0b5b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144913&auth_key=1760144913-0-0-fa13974dfa4d20c5a865115a03ea5ca6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8f07f72a0894d94e5e79b993d8d256f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144920&auth_key=1760144920-0-0-95e3f04610e2c9bc9eac331ce886dda1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReasonEmbed-Enhanced-Text-Embeddings-for-Reasoning-Intensive-Document-Retrieval"><a href="#ReasonEmbed-Enhanced-Text-Embeddings-for-Reasoning-Intensive-Document-Retrieval" class="headerlink" title="ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document   Retrieval"></a>ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document   Retrieval</h2><p><strong>Authors:Jianlyu Chen, Junwei Lan, Chaofan Li, Defu Lian, Zheng Liu</strong></p>
<p>In this paper, we introduce ReasonEmbed, a novel text embedding model developed for reasoning-intensive document retrieval. Our work includes three key technical contributions. First, we propose ReMixer, a new data synthesis method that overcomes the triviality problem prevalent in previous synthetic datasets, enabling large-scale production of 82K high-quality training samples. Second, we design Redapter, a self-adaptive learning algorithm that dynamically adjusts training each sampleâ€™s weight based on its reasoning intensity. This allows the model to effectively capture the complex semantic relationships between queries and documents. Third, we implement ReasonEmbed across multiple backbones of varying sizes, all of which achieve superior performance on reasoning-intensive retrieval tasks. Notably, our ReasonEmbed-Qwen3-8B model offers a record-high nDCG@10 score of 38.1 on the BRIGHT benchmark, which significantly outperforms existing text embedding models. We will fully open-source our created resources in ReasonEmbed to push forward the research advancement in this field. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ReasonEmbedï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ¨ç†å¯†é›†å‹æ–‡æ¡£æ£€ç´¢è€Œå¼€å‘çš„æ–°å‹æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚æˆ‘ä»¬çš„å·¥ä½œåŒ…æ‹¬ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ReMixerï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œå…‹æœäº†ä»¥å‰åˆæˆæ•°æ®é›†ä¸­æ™®éå­˜åœ¨çš„å¹³åº¸é—®é¢˜ï¼Œèƒ½å¤Ÿå®ç°8.2ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å¤§è§„æ¨¡ç”Ÿäº§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†Redapterï¼Œä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„æ¨ç†å¼ºåº¦åŠ¨æ€è°ƒæ•´å…¶æƒé‡ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„å¤æ‚è¯­ä¹‰å…³ç³»ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬åœ¨ä¸åŒå¤§å°çš„ä¸»å¹²ç½‘ç»œä¸Šå®ç°äº†ReasonEmbedï¼Œæ‰€æœ‰è¿™äº›æ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ReasonEmbed-Qwen3-8Bæ¨¡å‹åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†åˆ›çºªå½•çš„nDCG@10å¾—åˆ†38.1ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚æˆ‘ä»¬å°†å…¨é¢å¼€æºReasonEmbedä¸­åˆ›å»ºçš„èµ„æºï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08252v1">PDF</a> 17 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ReasonEmbedï¼Œä¸€ç§ä¸ºæ¨ç†å¯†é›†å‹æ–‡æ¡£æ£€ç´¢å¼€å‘çš„æ–°å‹æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ï¼šæå‡ºReMixeræ–°æ•°æ®åˆæˆæ–¹æ³•ï¼Œå…‹æœä»¥å¾€åˆæˆæ•°æ®é›†çš„å¹³åº¸é—®é¢˜ï¼Œç”Ÿæˆ8.2ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ï¼›è®¾è®¡Redapterè‡ªé€‚åº”å­¦ä¹ ç®—æ³•ï¼Œæ ¹æ®æ ·æœ¬çš„æ¨ç†å¼ºåº¦åŠ¨æ€è°ƒæ•´è®­ç»ƒæƒé‡ï¼Œæœ‰æ•ˆæ•æ‰æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„å¤æ‚è¯­ä¹‰å…³ç³»ï¼›åœ¨å¤šå¤§å°ä¸åŒçš„backboneä¸Šå®ç°ReasonEmbedï¼Œåœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å…¶ä¸­ï¼ŒReasonEmbed-Qwen3-8Bæ¨¡å‹åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å–å¾—åˆ›çºªå½•çš„é«˜nDCG@10å¾—åˆ†38.1ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚æˆ‘ä»¬å°†å…¨é¢å¼€æºReasonEmbedèµ„æºï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ReasonEmbedæ¨¡å‹ï¼Œä¸“ä¸ºæ¨ç†å¯†é›†å‹æ–‡æ¡£æ£€ç´¢è®¾è®¡ã€‚</li>
<li>æå‡ºReMixeræ•°æ®åˆæˆæ–¹æ³•ï¼Œè§£å†³ä»¥å¾€åˆæˆæ•°æ®é›†çš„å¹³åº¸é—®é¢˜ï¼Œç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>è®¾è®¡Redapterè‡ªé€‚åº”å­¦ä¹ ç®—æ³•ï¼Œæ ¹æ®æ ·æœ¬æ¨ç†å¼ºåº¦åŠ¨æ€è°ƒæ•´è®­ç»ƒã€‚</li>
<li>ReasonEmbedæ¨¡å‹åœ¨å¤šå¤§å°ä¸åŒçš„backboneä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ReasonEmbed-Qwen3-8Bæ¨¡å‹åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å–å¾—æ˜¾è‘—é«˜å¾—åˆ†ã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ab403ebcb267f567bec2c12f7e1addc0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144928&auth_key=1760144928-0-0-568c16387a8958efe6d2f392f61ccec4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ebfefbf8ac12bb39f265629d52a1d4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144935&auth_key=1760144935-0-0-f4488d737941ee4ac7ace2a484889bea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ab7add262e9341a9f0bdd3a6eb5d0aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144942&auth_key=1760144942-0-0-c20c5647944a2771bdba36156dae3e01&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3e4161a5bf88682e545941ef411cefa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144949&auth_key=1760144949-0-0-8bb97c181fdf99b42a40448e5d4b27ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Reasoning-for-Diffusion-LLMs-via-Distribution-Matching-Policy-Optimization"><a href="#Enhancing-Reasoning-for-Diffusion-LLMs-via-Distribution-Matching-Policy-Optimization" class="headerlink" title="Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy   Optimization"></a>Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy   Optimization</h2><p><strong>Authors:Yuchen Zhu, Wei Guo, Jaemoo Choi, Petr Molodyk, Bo Yuan, Molei Tao, Yongxin Chen</strong></p>
<p>Diffusion large language models (dLLMs) are promising alternatives to autoregressive large language models (AR-LLMs), as they potentially allow higher inference throughput. Reinforcement learning (RL) is a crucial component for dLLMs to achieve comparable performance with AR-LLMs on important tasks, such as reasoning. However, RL algorithms that are well-suited for dLLMsâ€™ unique characteristics have yet to be developed. This paper proposes Distribution Matching Policy Optimization (DMPO), a principled and theoretically grounded RL fine-tuning method specifically designed to enhance the reasoning capabilities of dLLMs by matching the dLLM policy distribution to the optimal, reward-tilted one through cross-entropy optimization. We identify a key challenge in the implementation with a small training batch size and propose several effective solutions through a novel weight baseline subtraction technique. DMPO exhibits superior performance on multiple reasoning benchmarks without supervised fine-tuning, with an accuracy improvement of up to $42.9%$ over previously SOTA baselines and $55.8%$ over the base model, underscoring the effectiveness of the distribution matching framework. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yuchen-zhu-zyc/DMPO">https://github.com/yuchen-zhu-zyc/DMPO</a>. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºå¯¹è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAR-LLMsï¼‰çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰æ›´é«˜çš„æ¨ç†ååé‡æ½œåŠ›ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯dLLMsåœ¨æ¨ç†ç­‰é‡è¦ä»»åŠ¡ä¸Šå®ç°ä¸AR-LLMsç›¸å½“æ€§èƒ½çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œé€‚åˆdLLMsç‹¬ç‰¹ç‰¹æ€§çš„RLç®—æ³•å°šæœªå¼€å‘å‡ºæ¥ã€‚æœ¬æ–‡æå‡ºäº†åˆ†å¸ƒåŒ¹é…ç­–ç•¥ä¼˜åŒ–ï¼ˆDMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰åŸåˆ™ã€æœ‰ç†è®ºåŸºç¡€çš„RLå¾®è°ƒæ–¹æ³•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºé€šè¿‡äº¤å‰ç†µä¼˜åŒ–ï¼Œå°†dLLMç­–ç•¥åˆ†å¸ƒåŒ¹é…åˆ°æœ€ä¼˜ã€å€¾å‘å¥–åŠ±çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜dLLMsçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ç¡®å®šäº†åœ¨å®ç°è¿‡ç¨‹ä¸­å°è®­ç»ƒæ‰¹æ¬¡çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡ä¸€ç§æ–°çš„æƒé‡åŸºçº¿å‡æ³•æŠ€æœ¯æå‡ºäº†å‡ ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚DMPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼Œå…¶å‡†ç¡®ç‡è¾ƒä¹‹å‰çš„æœ€ä½³åŸºçº¿æé«˜äº†é«˜è¾¾42.9%ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æé«˜äº†55.8%ï¼Œçªæ˜¾äº†åˆ†å¸ƒåŒ¹é…æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuchen-zhu-zyc/DMPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yuchen-zhu-zyc/DMPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08233v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºå¯¹è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆAR-LLMsï¼‰çš„æ½œåœ¨æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œå®ƒä»¬å¯å®ç°æ›´é«˜çš„æ¨ç†ååé‡ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹äºdLLMsåœ¨é‡è¦ä»»åŠ¡ï¼ˆå¦‚æ¨ç†ï¼‰ä¸Šå®ç°ä¸AR-LLMsç›¸å½“çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé€‚åˆdLLMsç‹¬ç‰¹ç‰¹æ€§çš„RLç®—æ³•ä»æœ‰å¾…å¼€å‘ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ†å¸ƒåŒ¹é…ç­–ç•¥ä¼˜åŒ–ï¼ˆDMPOï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æœ‰åŸåˆ™ä¸”ç†è®ºä¸Šæœ‰ä¾æ®çš„RLå¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡äº¤å‰ç†µä¼˜åŒ–åŒ¹é…dLLMç­–ç•¥åˆ†å¸ƒåˆ°æœ€ä½³ã€å€¾æ–œå¥–åŠ±çš„åˆ†å¸ƒï¼Œä»è€Œå¢å¼ºdLLMsçš„æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è§£å†³äº†å®ç°è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå³å°è®­ç»ƒæ‰¹æ¬¡çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æƒé‡åŸºçº¿å‡æ³•æŠ€æœ¯ã€‚DMPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼Œå¯¹åŸºç¡€æ¨¡å‹çš„å‡†ç¡®åº¦æé«˜äº†é«˜è¾¾42.9%ï¼Œçªæ˜¾äº†åˆ†å¸ƒåŒ¹é…æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/yuchen-zhu-zyc/DMPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yuchen-zhu-zyc/DMPOè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰å…·æœ‰ä½œä¸ºè‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆAR-LLMsï¼‰æ›¿ä»£æ–¹æ¡ˆçš„å‰æ™¯ï¼Œå› ä¸ºå®ƒä»¬èƒ½æé«˜æ¨ç†ååé‡ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹äºæå‡dLLMsåœ¨é‡è¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å°šéœ€å¼€å‘é€‚åˆdLLMsç‹¬ç‰¹ç‰¹æ€§çš„RLç®—æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åˆ†å¸ƒåŒ¹é…ç­–ç•¥ä¼˜åŒ–ï¼ˆDMPOï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åŒ¹é…ç­–ç•¥åˆ†å¸ƒå¢å¼ºdLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>DMPOè§£å†³äº†å°è®­ç»ƒæ‰¹æ¬¡çš„é—®é¢˜ï¼Œå¹¶æå‡ºæœ‰æ•ˆçš„æƒé‡åŸºçº¿å‡æ³•æŠ€æœ¯ã€‚</li>
<li>DMPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šå…ˆå‰æœ€å…ˆè¿›åŸºçº¿ï¼Œå¯¹åŸºç¡€æ¨¡å‹çš„å‡†ç¡®åº¦æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b5bad44ff2799d05495147324adc2262~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144956&auth_key=1760144956-0-0-bea637fb7c25ffadd7187dfa0b436408&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf59fb5838cfde81b69c7d0e1a45b7e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144963&auth_key=1760144963-0-0-96f3fa14100e44c29d890084b6e0dbe7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Training-Free-Group-Relative-Policy-Optimization"><a href="#Training-Free-Group-Relative-Policy-Optimization" class="headerlink" title="Training-Free Group Relative Policy Optimization"></a>Training-Free Group Relative Policy Optimization</h2><p><strong>Authors:Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, Xing Sun</strong></p>
<p>Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººçš„è¿›å±•å±•ç¤ºå‡ºäº†å®ƒä»¬å…·æœ‰å‰æ™¯çš„é€šç”¨èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¸“ä¸šç°å®ä¸–ç•Œé¢†åŸŸçš„è¡¨ç°å¾€å¾€ä¼šå› ä¸ºæœ‰æ•ˆåœ°æ•´åˆå¤–éƒ¨å·¥å…·å’Œç‰¹å®šæç¤ºç­–ç•¥çš„æŒ‘æˆ˜è€Œé™ä½ã€‚è™½ç„¶æå‡ºäº†è¯¸å¦‚åŸºäºä»£ç†çš„å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„å‚æ•°æ›´æ–°ï¼Œä¾‹å¦‚é€šè¿‡é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µè¿›è¡Œåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥æ”¹å˜è¾“å‡ºåˆ†å¸ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºLLMå¯ä»¥é€šè¿‡å­¦ä¹ ä½“éªŒçŸ¥è¯†ä½œä¸ºä»¤ç‰Œå…ˆéªŒæ¥è¾¾åˆ°ç±»ä¼¼çš„è¾“å‡ºåˆ†å¸ƒæ•ˆæœï¼Œè¿™æ˜¯ä¸€ç§æ›´è½»ä¾¿çš„æ–¹æ³•ï¼Œä¸ä»…è§£å†³äº†å®é™…æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè€Œä¸”é¿å…äº†è¿‡æ‹Ÿåˆçš„å¸¸è§éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTraining-Free GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨æ— éœ€ä»»ä½•å‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹æé«˜LLMä»£ç†äººçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç»„å†…çš„ç›¸å¯¹è¯­ä¹‰ä¼˜åŠ¿ï¼Œè€Œä¸æ˜¯æ•°å€¼ä¼˜åŠ¿ï¼Œåœ¨å¤šè½®å­¦ä¹ çš„æ¯ä¸ªæ—¶æœŸä¸­è¿­ä»£æç‚¼é«˜è´¨é‡çš„ç»éªŒçŸ¥è¯†ï¼Œåœ¨æœ€å°‘çœŸå®æ•°æ®çš„åŸºç¡€ä¸Šã€‚è¿™ç§çŸ¥è¯†ä½œä¸ºå­¦ä¹ çš„ä»¤ç‰Œå…ˆéªŒï¼Œå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°LLM APIè°ƒç”¨ä¸­ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹çš„è¡Œä¸ºã€‚åœ¨æ•°å­¦æ¨ç†å’Œç½‘é¡µæœç´¢ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œå°†Training-Free GRPOåº”ç”¨äºDeepSeek-V3.1-Terminusæ—¶ï¼Œå…¶è·¨åŸŸæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚åªéœ€å‡ åä¸ªè®­ç»ƒæ ·æœ¬ï¼ŒTraining-Free GRPOå°±èƒ½ä»¥å¾®å°çš„è®­ç»ƒæ•°æ®å’Œæˆæœ¬ä¼˜åŠ¿è¶…è¶Šç²¾ç»†è°ƒæ•´çš„å°å‹LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08191v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨èƒ½åŠ›ä¸Šå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„ç°å®ä¸–ç•Œä¸­å¸¸é¢ä¸´æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºéš¾ä»¥æœ‰æ•ˆåœ°é›†æˆå¤–éƒ¨å·¥å…·å’Œç‰¹å®šçš„æç¤ºç­–ç•¥ã€‚è™½ç„¶å·²æœ‰æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„å‚æ•°æ›´æ–°è¿‡ç¨‹ï¼Œä¾‹å¦‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µè¿›è¡Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸»å¼ LLMèƒ½å¤Ÿé€šè¿‡å­¦ä¹ ç»éªŒçŸ¥è¯†ä½œä¸ºä»¤ç‰Œå…ˆéªŒæ¥å®ç°ç±»ä¼¼çš„è¾“å‡ºåˆ†å¸ƒæ•ˆæœï¼Œè¿™æ˜¯ä¸€ç§æ›´è½»é‡çº§çš„æ–¹æ³•ï¼Œä¸ä»…è§£å†³äº†å®é™…æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè€Œä¸”é¿å…äº†è¿‡åº¦æ‹Ÿåˆçš„å¸¸è§é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒå…è´¹ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTraining-Free GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºLLMä»£ç†æ€§èƒ½ä¸”æ— éœ€ä»»ä½•å‚æ•°æ›´æ–°çš„æˆæœ¬æ•ˆç›Šè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç»„å†…çš„ç›¸å¯¹è¯­ä¹‰ä¼˜åŠ¿è€Œä¸æ˜¯æ•°å€¼ä¼˜åŠ¿ï¼Œåœ¨å¤šè½®å­¦ä¹ çš„æ¯ä¸ªæ—¶ä»£ä¸­è¿­ä»£æç‚¼é«˜è´¨é‡çš„ç»éªŒçŸ¥è¯†ï¼ŒåŸºäºå°‘é‡çœŸå®æ•°æ®è¿›è¡Œã€‚è¿™ç§çŸ¥è¯†ä½œä¸ºå­¦åˆ°çš„ä»¤ç‰Œå…ˆéªŒæ— ç¼é›†æˆåˆ°LLM APIè°ƒç”¨ä¸­ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹è¡Œä¸ºã€‚åœ¨æ•°å­¦æ¨ç†å’Œç½‘é¡µæœç´¢ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸DeepSeek-V3.1-Terminusç›¸ç»“åˆæ—¶ï¼Œè®­ç»ƒå…è´¹çš„GRPOèƒ½æ˜¾è‘—æé«˜è·¨åŸŸæ€§èƒ½ï¼Œä»…ä½¿ç”¨å‡ åä¸ªè®­ç»ƒæ ·æœ¬å°±èƒ½è¶…è¶Šå¾®è°ƒçš„å°å‹LLMæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„ç°å®ä¸–ç•Œä¸­é¢ä¸´æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ è™½ç„¶èƒ½è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†éœ€è¦æ˜‚è´µçš„å‚æ•°æ›´æ–°è¿‡ç¨‹ã€‚</li>
<li>LLMå¯ä»¥é€šè¿‡å­¦ä¹ ç»éªŒçŸ¥è¯†ä½œä¸ºä»¤ç‰Œå…ˆéªŒæ¥å®ç°è¾“å‡ºåˆ†å¸ƒè°ƒæ•´ï¼Œè¿™æ˜¯ä¸€ç§æ›´è½»é‡çº§çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†è®­ç»ƒå…è´¹ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTraining-Free GRPOï¼‰ï¼Œèƒ½å¢å¼ºLLMæ€§èƒ½ä¸”æ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ã€‚</li>
<li>Training-Free GRPOåˆ©ç”¨ç»„å†…çš„ç›¸å¯¹è¯­ä¹‰ä¼˜åŠ¿ï¼Œåœ¨å¤šè½®å­¦ä¹ ä¸­æç‚¼ç»éªŒçŸ¥è¯†ã€‚</li>
<li>Training-Free GRPOé›†æˆäº†å­¦åˆ°çš„ä»¤ç‰Œå…ˆéªŒï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°LLM APIè°ƒç”¨ä¸­æŒ‡å¯¼æ¨¡å‹è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-99fd70c8e1da4ffbf0df00b343d2e325~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144970&auth_key=1760144970-0-0-76f004b7a0a88573f09734e89eae5ca2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bd11c49368ff867faa41c99e279da04~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144978&auth_key=1760144978-0-0-048f931a1f8c0e79c79baa28d1b0bd12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13d17065f0213158c36156592c6ff764~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144985&auth_key=1760144985-0-0-c2fbffdee62e5dd03fc5dfdd1fbf550a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c8430f3c9663699483942cd2c07d8dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144992&auth_key=1760144992-0-0-b1ec99b529f82362d8518e510a929cda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-Textual-CoT-Interleaved-Text-Image-Chains-with-Deep-Confidence-Reasoning-for-Image-Editing"><a href="#Beyond-Textual-CoT-Interleaved-Text-Image-Chains-with-Deep-Confidence-Reasoning-for-Image-Editing" class="headerlink" title="Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence   Reasoning for Image Editing"></a>Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence   Reasoning for Image Editing</h2><p><strong>Authors:Zhentao Zou, Zhengrong Yue, Kunpeng Du, Binlei Bao, Hanting Li, Haizhen Xie, Guozheng Xu, Yue Zhou, Yali Wang, Jie Hu, Xue Jiang, Xinghao Chen</strong></p>
<p>Image editing with natural language has gained significant popularity, yet existing methods struggle with intricate object intersections and fine-grained spatial relationships due to the lack of an explicit reasoning process. While Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual CoT or CoT augmented with coordinate information is fundamentally limited in its ability to represent intricate visual layouts and lacks the necessary visual cues to guide the generation of fine-grained, pixel-level details. To address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel framework that shifts the visual editing process from purely text-based reasoning to a series of interleaved textual and visual rationales. Our framework performs image editing using a natively multimodal, interleaved text-image CoT. This approach generates a step-by-step chain of reasoning where a textual description is followed by a corresponding visual cue, such as a positional mask that defined intended edited regions or a representation of new content. Furthermore, to mitigate the hallucination phenomenon of large language models, we introduce Multimodal Deep Confidence (MMDC) reasoning paradigm. This paradigm explores a tree of visual reasoning paths at each step. By pruning low-quality branches using a deep confidence score from a reward model, it ensures the model consistently follows a high-quality trajectory towards the final edited result. The proposed method decomposes complex editing tasks into interdependent sub-tasks, achieving greater precision at each stage and yielding high-fidelity edited results. We define the formulation for interleaved text-image chains and release the first CoT-Edit-14K dataset, comprising 14K high-quality editing examples. Extensive experiments show that our method yields significant improvements across three image editing benchmarks. </p>
<blockquote>
<p>å›¾åƒç¼–è¾‘ä¸è‡ªç„¶è¯­è¨€å·²ç»è·å¾—äº†å¹¿æ³›çš„å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„å¯¹è±¡äº¤å‰å’Œç²¾ç»†çš„ç©ºé—´å…³ç³»æ—¶é¢ä¸´å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å·²è¢«æ¢ç´¢ç”¨äºå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†çº¯ç²¹çš„æ–‡æœ¬CoTæˆ–ä¸åæ ‡ä¿¡æ¯ç›¸ç»“åˆçš„æ–¹æ³•åœ¨è¡¨ç¤ºå¤æ‚çš„è§†è§‰å¸ƒå±€æ–¹é¢å­˜åœ¨æ ¹æœ¬å±€é™æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¿…è¦çš„è§†è§‰çº¿ç´¢æ¥æŒ‡å¯¼ç²¾ç»†çš„åƒç´ çº§ç»†èŠ‚ç”Ÿæˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€æ¨ç†ç¼–è¾‘ï¼ˆMUREï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†è§†è§‰ç¼–è¾‘è¿‡ç¨‹ä»çº¯ç²¹çš„æ–‡æœ¬æ¨ç†è½¬å˜ä¸ºä¸€ç³»åˆ—äº¤ç»‡çš„æ–‡æœ¬å’Œè§†è§‰æ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨å¤©ç”Ÿçš„å¤šæ¨¡æ€ã€äº¤ç»‡çš„æ–‡æœ¬å›¾åƒCoTè¿›è¡Œå›¾åƒç¼–è¾‘ã€‚è¿™ç§æ–¹æ³•ç”Ÿæˆäº†ä¸€ä¸ªé€æ­¥æ¨ç†çš„é“¾ï¼Œå…¶ä¸­æ–‡æœ¬æè¿°ä¹‹åæ˜¯ç›¸åº”çš„è§†è§‰çº¿ç´¢ï¼Œå¦‚å®šä¹‰æ„å›¾ç¼–è¾‘åŒºåŸŸçš„å®šä½æ©è†œæˆ–æ–°å†…å®¹çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€æ·±åº¦ç½®ä¿¡ï¼ˆMMDCï¼‰æ¨ç†èŒƒå¼ã€‚è¯¥èŒƒå¼åœ¨æ¯ä¸€æ­¥æ¢ç´¢å¤šä¸ªè§†è§‰æ¨ç†è·¯å¾„çš„æ ‘ç»“æ„ã€‚é€šè¿‡ä½¿ç”¨å¥–åŠ±æ¨¡å‹çš„æ·±åº¦ç½®ä¿¡åˆ†æ•°æ¥ä¿®å‰ªä½è´¨é‡çš„åˆ†æ”¯ï¼Œå®ƒç¡®ä¿æ¨¡å‹å§‹ç»ˆéµå¾ªé«˜è´¨é‡è½¨è¿¹ä»¥è¾¾åˆ°æœ€ç»ˆçš„ç¼–è¾‘ç»“æœã€‚æ‰€æå‡ºçš„æ–¹æ³•å°†å¤æ‚çš„ç¼–è¾‘ä»»åŠ¡åˆ†è§£ä¸ºç›¸äº’ä¾å­˜çš„å­ä»»åŠ¡ï¼Œåœ¨æ¯ä¸ªé˜¶æ®µå®ç°æ›´é«˜çš„ç²¾åº¦ï¼Œå¹¶äº§ç”Ÿé«˜ä¿çœŸåº¦çš„ç¼–è¾‘ç»“æœã€‚æˆ‘ä»¬å®šä¹‰äº†äº¤ç»‡æ–‡æœ¬å›¾åƒé“¾çš„å…¬å¼ï¼Œå¹¶å‘å¸ƒäº†é¦–ä¸ªCoT-Edit-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Ké«˜è´¨é‡ç¼–è¾‘ç¤ºä¾‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªå›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08157v1">PDF</a> 25pages,20figures</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMultimodal Reasoning Editï¼ˆMUREï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºå›¾åƒç¼–è¾‘ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–‡æœ¬ä¸è§†è§‰äº¤æ›¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†ç²¾ç»†çš„åƒç´ çº§ç¼–è¾‘ã€‚ä¸ºå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡ï¼Œå¼•å…¥Multimodal Deep Confidenceï¼ˆMMDCï¼‰æ¨ç†èŒƒå¼ã€‚æ­¤æ–¹æ³•å°†å¤æ‚çš„ç¼–è¾‘ä»»åŠ¡åˆ†è§£ä¸ºç›¸äº’ä¾èµ–çš„å­ä»»åŠ¡ï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µå®ç°é«˜ç²¾åº¦ï¼Œäº§ç”Ÿé«˜ä¿çœŸåº¦çš„ç¼–è¾‘ç»“æœã€‚åŒæ—¶ï¼Œå‘å¸ƒCoT-Edit-14Kæ•°æ®é›†ç”¨äºå®éªŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Multimodal Reasoning Edit (MURE)æ¡†æ¶é‡‡ç”¨æ–‡æœ¬ä¸è§†è§‰äº¤æ›¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¤„ç†å›¾åƒç¼–è¾‘ä¸­çš„å¤æ‚ç©ºé—´å…³ç³»ã€‚</li>
<li>MUREé€šè¿‡ç»“åˆæ–‡æœ¬æè¿°å’Œç›¸åº”çš„è§†è§‰çº¿ç´¢ï¼ˆå¦‚ä½ç½®æ©ç å’Œæ–°å†…å®¹è¡¨ç¤ºï¼‰ï¼Œå®ç°äº†ç²¾ç»†çš„åƒç´ çº§ç¼–è¾‘ã€‚</li>
<li>Multimodal Deep Confidence (MMDC)æ¨ç†èŒƒå¼ç”¨äºå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡ï¼Œç¡®ä¿æ¨¡å‹éµå¾ªé«˜è´¨é‡è½¨è¿¹è¿›è¡Œç¼–è¾‘ã€‚</li>
<li>MUREå°†å¤æ‚çš„ç¼–è¾‘ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œæ¯ä¸ªé˜¶æ®µå®ç°é«˜ç²¾åº¦ï¼Œæœ€ç»ˆäº§ç”Ÿé«˜ä¿çœŸåº¦çš„ç¼–è¾‘ç»“æœã€‚</li>
<li>é¦–æ¬¡å‘å¸ƒCoT-Edit-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Ké«˜è´¨é‡ç¼–è¾‘ç¤ºä¾‹ï¼Œç”¨äºå®éªŒéªŒè¯å’Œæ¨¡å‹è®­ç»ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2a4559c33c025b238623705c4fa16d5f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144999&auth_key=1760144999-0-0-92c4417efee8f30a85818e80b916bd74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e74d72946fea748b001ae4c5087c8225~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145007&auth_key=1760145007-0-0-0d163ffe503580f91e92e4b13b61a45b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d3efd67a99ad39bae7293cdef595059~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145014&auth_key=1760145014-0-0-d55fd322b874139da9741fa189175dd8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d73be7137104278b826aab00c6a98f42~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145021&auth_key=1760145021-0-0-9decf30c83ee26b8de5f5e6b12007ca0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Detecting-and-Mitigating-Insertion-Hallucination-in-Video-to-Audio-Generation"><a href="#Detecting-and-Mitigating-Insertion-Hallucination-in-Video-to-Audio-Generation" class="headerlink" title="Detecting and Mitigating Insertion Hallucination in Video-to-Audio   Generation"></a>Detecting and Mitigating Insertion Hallucination in Video-to-Audio   Generation</h2><p><strong>Authors:Liyang Chen, Hongkai Chen, Yujun Cai, Sifan Li, Qingwen Ye, Yiwei Wang</strong></p>
<p>Video-to-Audio generation has made remarkable strides in automatically synthesizing sound for video. However, existing evaluation metrics, which focus on semantic and temporal alignment, overlook a critical failure mode: models often generate acoustic events, particularly speech and music, that have no corresponding visual source. We term this phenomenon Insertion Hallucination and identify it as a systemic risk driven by dataset biases, such as the prevalence of off-screen sounds, that remains completely undetected by current metrics. To address this challenge, we first develop a systematic evaluation framework that employs a majority-voting ensemble of multiple audio event detectors. We also introduce two novel metrics to quantify the prevalence and severity of this issue: IH@vid (the fraction of videos with hallucinations) and IH@dur (the fraction of hallucinated duration). Building on this, we propose Posterior Feature Correction, a novel training-free inference-time method that mitigates IH. PFC operates in a two-pass process: it first generates an initial audio output to detect hallucinated segments, and then regenerates the audio after masking the corresponding video features at those timestamps. Experiments on several mainstream V2A benchmarks first reveal that state-of-the-art models suffer from severe IH. In contrast, our PFC method reduces both the prevalence and duration of hallucinations by over 50% on average, without degrading, and in some cases even improving, conventional metrics for audio quality and temporal synchronization. Our work is the first to formally define, systematically measure, and effectively mitigate Insertion Hallucination, paving the way for more reliable and faithful V2A models. </p>
<blockquote>
<p>è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆåœ¨è‡ªåŠ¨ä¸ºè§†é¢‘åˆæˆå£°éŸ³æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ä¸»è¦é›†ä¸­åœ¨è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ä¸Šï¼Œå¿½ç•¥äº†ä¸€ç§å…³é”®çš„å¤±è´¥æ¨¡å¼ï¼šæ¨¡å‹ç»å¸¸ç”Ÿæˆæ²¡æœ‰ç›¸åº”è§†è§‰æ¥æºçš„å£°å­¦äº‹ä»¶ï¼Œç‰¹åˆ«æ˜¯è¯­éŸ³å’ŒéŸ³ä¹ã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œæ’å…¥å¹»è§‰â€ï¼Œå¹¶å°†å…¶è¯†åˆ«ä¸ºç”±æ•°æ®é›†åå·®é©±åŠ¨çš„ç³»ç»Ÿæ€§é£é™©ï¼Œä¾‹å¦‚å±å¹•å¤–å£°éŸ³æ™®éå­˜åœ¨ï¼Œè€Œå½“å‰æŒ‡æ ‡å®Œå…¨æ— æ³•æ£€æµ‹åˆ°æ­¤é£é™©ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šæ•°æŠ•ç¥¨åˆ¶çš„å¤šä¸ªéŸ³é¢‘äº‹ä»¶æ£€æµ‹å™¨é›†åˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§æ–°æŒ‡æ ‡æ¥é‡åŒ–æ­¤é—®é¢˜çš„æµè¡Œç¨‹åº¦å’Œä¸¥é‡æ€§ï¼šIH@vidï¼ˆæœ‰å¹»è§‰çš„è§†é¢‘æ¯”ä¾‹ï¼‰å’ŒIH@durï¼ˆå¹»è§‰æŒç»­æ—¶é—´æ¯”ä¾‹ï¼‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†â€œåç‰¹å¾æ ¡æ­£â€ï¼ˆPFCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹æ¨æ–­æ—¶é—´æ–¹æ³•ï¼Œå¯ä»¥ç¼“è§£IHã€‚PFCé‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šé¦–å…ˆç”Ÿæˆåˆå§‹éŸ³é¢‘è¾“å‡ºä»¥æ£€æµ‹å¹»è§‰ç‰‡æ®µï¼Œç„¶ååœ¨è¿™äº›æ—¶é—´æˆ³ä¸Šå±è”½ç›¸åº”è§†é¢‘ç‰¹å¾åé‡æ–°ç”ŸæˆéŸ³é¢‘ã€‚åœ¨å‡ ä¸ªä¸»æµV2AåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒé¦–å…ˆè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„IHã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„PFCæ–¹æ³•å¹³å‡å‡å°‘äº†è¶…è¿‡50%çš„å¹»è§‰å‘ç”Ÿç‡å’ŒæŒç»­æ—¶é—´ï¼ŒåŒæ—¶ä¸é™ä½ã€ç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ”¹è¿›äº†éŸ³é¢‘è´¨é‡å’Œæ—¶é—´åŒæ­¥çš„å¸¸è§„æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡æ­£å¼å®šä¹‰ã€ç³»ç»Ÿæµ‹é‡å’Œæœ‰æ•ˆç¼“è§£æ’å…¥å¹»è§‰ï¼Œä¸ºæ›´å¯é å’Œå¿ è¯šçš„V2Aæ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08078v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘è½¬éŸ³é¢‘ç”ŸæˆæŠ€æœ¯åœ¨è‡ªåŠ¨åˆæˆå£°éŸ³æ–¹é¢å·²å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ä¸»è¦é›†ä¸­åœ¨è¯­ä¹‰å’Œæ—¶åºå¯¹é½ä¸Šï¼Œå¿½ç•¥äº†ä¸€ç§å…³é”®çš„å¤±è´¥æ¨¡å¼ï¼šæ¨¡å‹ç»å¸¸ç”Ÿæˆæ²¡æœ‰ç›¸åº”è§†è§‰æ¥æºçš„å£°æºï¼Œç‰¹åˆ«æ˜¯è¯­éŸ³å’ŒéŸ³ä¹ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºæ’å…¥å¹»è§‰ï¼Œå¹¶è¢«è¯†åˆ«ä¸ºç³»ç»Ÿé£é™©ï¼Œç”±æ•°æ®é›†åå·®ï¼ˆå¦‚å±å¹•å¤–å£°éŸ³æ™®éå­˜åœ¨ï¼‰é©±åŠ¨ï¼Œè€Œå½“å‰æŒ‡æ ‡æ— æ³•æ£€æµ‹åˆ°å®ƒã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªé‡‡ç”¨å¤šæ•°æŠ•ç¥¨åˆ¶çš„éŸ³é¢‘äº‹ä»¶æ£€æµ‹å™¨é›†åˆçš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªæ–°æŒ‡æ ‡æ¥é‡åŒ–æ­¤é—®é¢˜çš„æ™®éæ€§å’Œä¸¥é‡æ€§ï¼šIH@vidï¼ˆå¸¦æœ‰å¹»è§‰çš„è§†é¢‘åˆ†æ•°ï¼‰å’ŒIH@durï¼ˆå¹»è§‰æŒç»­æ—¶é—´åˆ†æ•°ï¼‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åç‰¹å¾æ ¡æ­£ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„åæ¨ç†æ—¶é—´æ–¹æ³•ï¼Œå¯ä»¥ç¼“è§£IHã€‚PFCé‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šé¦–å…ˆç”Ÿæˆåˆå§‹éŸ³é¢‘è¾“å‡ºä»¥æ£€æµ‹å¹»è§‰ç‰‡æ®µï¼Œç„¶ååœ¨è¿™äº›æ—¶é—´æˆ³ä¸Šå±è”½ç›¸åº”è§†é¢‘ç‰¹å¾åé‡æ–°ç”ŸæˆéŸ³é¢‘ã€‚åœ¨ä¸»æµV2AåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„IHé—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„PFCæ–¹æ³•å¹³å‡å‡å°‘äº†è¶…è¿‡50%çš„å¹»è§‰å­˜åœ¨å’ŒæŒç»­æ—¶é—´ï¼ŒåŒæ—¶ä¸é™ä½ç”šè‡³æ”¹è¿›äº†éŸ³é¢‘è´¨é‡å’Œæ—¶åºåŒæ­¥çš„å¸¸è§„æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡æ­£å¼å®šä¹‰ã€ç³»ç»Ÿæµ‹é‡å’Œæœ‰æ•ˆç¼“è§£æ’å…¥å¹»è§‰ï¼Œä¸ºæ›´å¯é å’Œå¿ è¯šçš„V2Aæ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è§†é¢‘è½¬éŸ³é¢‘ç”ŸæˆæŠ€æœ¯å­˜åœ¨æ’å…¥å¹»è§‰é—®é¢˜ï¼Œå³æ¨¡å‹ä¼šç”Ÿæˆæ²¡æœ‰ç›¸åº”è§†è§‰æ¥æºçš„å£°æºã€‚</li>
<li>æ’å…¥å¹»è§‰æ˜¯ç³»ç»Ÿé£é™©ï¼Œç”±æ•°æ®é›†åå·®ï¼ˆå¦‚å±å¹•å¤–å£°éŸ³æ™®éå­˜åœ¨ï¼‰é©±åŠ¨ï¼Œå½“å‰è¯„ä¼°æŒ‡æ ‡æ— æ³•æ£€æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶å’ŒéŸ³é¢‘äº‹ä»¶æ£€æµ‹å™¨é›†åˆæ¥æ£€æµ‹æ’å…¥å¹»è§‰ã€‚</li>
<li>å¼•å…¥äº†ä¸¤ä¸ªæ–°æŒ‡æ ‡IH@vidå’ŒIH@duræ¥é‡åŒ–æ’å…¥å¹»è§‰çš„æ™®éæ€§å’Œä¸¥é‡æ€§ã€‚</li>
<li>æå‡ºäº†åç‰¹å¾æ ¡æ­£æ–¹æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹æ£€æµ‹å’Œç¼“è§£æ’å…¥å¹»è§‰ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œåç‰¹å¾æ ¡æ­£æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘æ’å…¥å¹»è§‰çš„å­˜åœ¨å’ŒæŒç»­æ—¶é—´ï¼ŒåŒæ—¶ä¸æŸå®³éŸ³é¢‘è´¨é‡å’Œæ—¶åºåŒæ­¥çš„å¸¸è§„æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4cf44688b31d85f79c910e491cbf591a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145028&auth_key=1760145028-0-0-7fc1ba25f52976aacbe340e6e8ce922a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b466bb06da55b41ad3ab91b09b0279ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145036&auth_key=1760145036-0-0-4e34628e071029453ff2fd766a5eeb55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edc0e397a9bef1e798e73e4be27dd404~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145042&auth_key=1760145042-0-0-3ef213b812466f95e5e1b43cdc08716f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ae54a33151af30bbfd69b77ae3f98102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145049&auth_key=1760145049-0-0-87592f1a5037b2f36a6acf9b277466a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TaoSR-AGRL-Adaptive-Guided-Reinforcement-Learning-Framework-for-E-commerce-Search-Relevance"><a href="#TaoSR-AGRL-Adaptive-Guided-Reinforcement-Learning-Framework-for-E-commerce-Search-Relevance" class="headerlink" title="TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for   E-commerce Search Relevance"></a>TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for   E-commerce Search Relevance</h2><p><strong>Authors:Jianhui Yang, Yiming Jin, Pengkun Jiao, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang</strong></p>
<p>Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users. </p>
<blockquote>
<p>æŸ¥è¯¢äº§å“ç›¸å…³æ€§é¢„æµ‹æ˜¯ç”µå­å•†åŠ¡æœç´¢çš„æ ¸å¿ƒåŸºç¡€ï¼Œåœ¨äººå·¥æ™ºèƒ½é©±åŠ¨è´­ç‰©çš„æ—¶ä»£ï¼Œå®ƒå˜å¾—æ›´ä¸ºé‡è¦ã€‚åœ¨è¿™ä¸ªæ—¶ä»£ï¼Œè¯­ä¹‰ç†è§£å’Œå¤æ‚æ¨ç†ç›´æ¥å½±å“äº†ç”¨æˆ·ä½“éªŒå’Œä¸šåŠ¡è½¬åŒ–ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿå®ç°åŸºäºæ¨ç†çš„ç”Ÿæˆæ–¹æ³•ï¼Œé€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¯¹åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼‰è¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œå•†ä¸šè§„åˆ™çš„æ—¥ç›Šå¤æ‚å’Œç”¨æˆ·æŸ¥è¯¢çš„å¤šæ ·åŒ–æš´éœ²å‡ºç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œå®ƒä»¬æ— æ³•ä½¿æ¨¡å‹å…·å¤‡é’ˆå¯¹é•¿å°¾å’Œå¤æ‚æƒ…å†µçš„ç¨³å¥æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„TaoSR-AGRLï¼ˆæ·˜å®æœç´¢ç›¸å…³æ€§ä¸­çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”å¼•å¯¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼‰æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚TaoSR-AGRLå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š1ï¼‰è§„åˆ™æ„ŸçŸ¥å¥–åŠ±å¡‘é€ ï¼Œå®ƒå°†æœ€ç»ˆçš„ç›¸å…³æ€§åˆ¤æ–­åˆ†è§£æˆå¯†é›†çš„ç»“æ„åŒ–å¥–åŠ±ï¼Œä¸ç‰¹å®šé¢†åŸŸçš„ç›¸å…³æ€§æ ‡å‡†å¯¹é½ï¼›2 ç»“äº†è®­ç»ƒè¿‡ç¨‹ä¸­ä½å‡†ç¡®ç‡çš„å›æ”¾å†…å®¹ï¼Œå¹¶æ³¨å…¥æœ‰é’ˆå¯¹æ€§çš„çœŸå®æŒ‡å¯¼ï¼Œä½¿ç­–ç•¥æ‘†è„±åœæ»ä¸å‰çš„è¿è§„æ¨ç†æ¨¡å¼ï¼Œè½¬å‘åˆè§„è½¨è¿¹ã€‚TaoSR-AGRLåœ¨å¤§å‹çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶é€šè¿‡æ·˜å®æœç´¢çš„åœ¨çº¿æ—ä¾§äººç±»è¯„ä¼°è¿›è¡Œäº†éªŒè¯ã€‚åœ¨çº¿ä¸‹å®éªŒä¸­ï¼Œå®ƒå§‹ç»ˆä¼˜äºDPOå’Œæ ‡å‡†GRPOåŸºå‡†æµ‹è¯•ï¼Œæé«˜äº†ç›¸å…³æ€§å‡†ç¡®æ€§ã€è§„åˆ™éµå¾ªæ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚ä½¿ç”¨TaoSR-AGRLè®­ç»ƒçš„æ¨¡å‹å·²æˆåŠŸéƒ¨ç½²åœ¨æ·˜å®ä¸»æœç´¢åœºæ™¯ä¸­ï¼Œä¸ºæ•°äº¿ç”¨æˆ·æä¾›æœåŠ¡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08048v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨AIé©±åŠ¨çš„è´­ç‰©æ—¶ä»£ï¼ŒæŸ¥è¯¢äº§å“ç›¸å…³æ€§é¢„æµ‹åœ¨ç”µå­å•†åŠ¡æœç´¢ä¸­çš„é‡è¦æ€§ã€‚æ–‡ç« æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆå’Œæ¨ç†çš„æ–¹æ³•æ¥è¿›è¡Œé¢„æµ‹ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å•†ä¸šè§„åˆ™å’Œé•¿å°¾æ¡ˆä¾‹æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¼•å¯¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶TaoSR-AGRLï¼Œç”¨äºæ·˜å®æœç´¢ç›¸å…³æ€§çš„LLMåŸºäºé¢„æµ‹ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè§„åˆ™æ„ŸçŸ¥å¥–åŠ±å¡‘é€ å’Œè‡ªé€‚åº”å¼•å¯¼å›æ”¾ï¼Œæ—¨åœ¨è§£å†³å¤šæ­¥æ¨ç†å’Œè§„åˆ™éµå®ˆé—®é¢˜ã€‚TaoSR-AGRLåœ¨å¤§å‹çœŸå®æ•°æ®é›†å’Œåœ¨çº¿äººç±»è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†ç›¸å…³æ€§å‡†ç¡®æ€§ã€è§„åˆ™éµå®ˆæ€§å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶å·²æˆåŠŸéƒ¨ç½²åœ¨æ·˜å®ä¸»è¦æœç´¢åœºæ™¯ä¸­ï¼Œä¸ºæ•°ç™¾ä¸‡ç”¨æˆ·æä¾›æœåŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢äº§å“ç›¸å…³æ€§é¢„æµ‹åœ¨AIé©±åŠ¨çš„è´­ç‰©æ—¶ä»£å¯¹ç”µå­å•†åŠ¡æœç´¢è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„æµ‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½†å¤„ç†å¤æ‚å•†ä¸šè§„åˆ™å’Œé•¿å°¾æ¡ˆä¾‹æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚Direct Preference Optimization (DPO)å’ŒGroup Relative Policy Optimization (GRPO)é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–ç»ˆç«¯å¥–åŠ±å’Œå¤šæ­¥æ¨ç†é—®é¢˜ã€‚</li>
<li>TaoSR-AGRLæ¡†æ¶é€šè¿‡è§„åˆ™æ„ŸçŸ¥å¥–åŠ±å¡‘é€ å’Œè‡ªé€‚åº”å¼•å¯¼å›æ”¾ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>TaoSR-AGRLåœ¨å¤§å‹çœŸå®æ•°æ®é›†å’Œåœ¨çº¿è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜ç›¸å…³æ€§å‡†ç¡®æ€§ã€è§„åˆ™éµå®ˆæ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>TaoSR-AGRLå·²æˆåŠŸéƒ¨ç½²åœ¨æ·˜å®æœç´¢åœºæ™¯ä¸­ï¼Œä¸ºæ•°ç™¾ä¸‡ç”¨æˆ·æä¾›æœåŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-418f94145029b236c1177ebce6c73f46~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145057&auth_key=1760145057-0-0-f406ba30a4d8e182cb30fa0fe154b5c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf8a9a13668b134decaa68d61757972a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145064&auth_key=1760145064-0-0-c23701b84e6ee4b3d8a9bf27763422dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5181bc94db26b2f2c5d465bfdd394609~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145071&auth_key=1760145071-0-0-cac03b998253ce19548e07cf42a314f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Do-We-Really-Need-SFT-Prompt-as-Policy-over-Knowledge-Graphs-for-Cold-start-Next-POI-Recommendation"><a href="#Do-We-Really-Need-SFT-Prompt-as-Policy-over-Knowledge-Graphs-for-Cold-start-Next-POI-Recommendation" class="headerlink" title="Do We Really Need SFT? Prompt-as-Policy over Knowledge Graphs for   Cold-start Next POI Recommendation"></a>Do We Really Need SFT? Prompt-as-Policy over Knowledge Graphs for   Cold-start Next POI Recommendation</h2><p><strong>Authors:Jinze Wang, Lu Zhang, Yiyang Cui, Zhishu Shen, Xingjun Ma, Jiong Jin, Tiehua Zhang</strong></p>
<p>Next point-of-interest (POI) recommendation is crucial for smart urban services such as tourism, dining, and transportation, yet most approaches struggle under cold-start conditions where user-POI interactions are sparse. Recent efforts leveraging large language models (LLMs) address this challenge through either supervised fine-tuning (SFT) or in-context learning (ICL). However, SFT demands costly annotations and fails to generalize to inactive users, while static prompts in ICL cannot adapt to diverse user contexts. To overcome these limitations, we propose Prompt-as-Policy over knowledge graphs, a reinforcement-guided prompting framework that learns to construct prompts dynamically through contextual bandit optimization. Our method treats prompt construction as a learnable policy that adaptively determines (i) which relational evidences to include, (ii) the number of evidence per candidate, and (iii) their organization and ordering within prompts. More specifically, we construct a knowledge graph (KG) to discover candidates and mine relational paths, which are transformed into evidence cards that summarize rationales for each candidate POI. The frozen LLM then acts as a reasoning engine, generating recommendations from the KG-discovered candidate set based on the policy-optimized prompts. Experiments on three real-world datasets demonstrate that Prompt-as-Policy consistently outperforms state-of-the-art baselines, achieving average 7.7% relative improvements in Acc@1 for inactive users, while maintaining competitive performance on active users, without requiring model fine-tuning. </p>
<blockquote>
<p>ä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èå¯¹äºæ™ºæ…§åŸå¸‚æœåŠ¡ï¼ˆå¦‚æ—…æ¸¸ã€é¤é¥®å’Œäº¤é€šï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨ç”¨æˆ·ä¸POIäº¤äº’ç¨€ç–çš„å†·å¯åŠ¨æ¡ä»¶ä¸‹è¡¨ç°å›°éš¾ã€‚æœ€è¿‘åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠªåŠ›é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒSFTéœ€è¦æ˜‚è´µçš„æ³¨é‡Šï¼Œå¹¶ä¸”æ— æ³•æ¨å¹¿åˆ°éæ´»è·ƒç”¨æˆ·ï¼Œè€ŒICLä¸­çš„é™æ€æç¤ºæ— æ³•é€‚åº”å¤šæ ·åŒ–çš„ç”¨æˆ·ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†å›¾çš„Prompt-as-Policyç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–æŒ‡å¯¼çš„æç¤ºæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å¼ºç›—ä¼˜åŒ–æ¥å­¦ä¹ åŠ¨æ€æ„å»ºæç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æç¤ºæ„å»ºè§†ä¸ºå¯å­¦ä¹ çš„ç­–ç•¥ï¼Œè‡ªé€‚åº”åœ°ç¡®å®šï¼ˆiï¼‰è¦åŒ…å«å“ªäº›å…³ç³»è¯æ®ï¼Œï¼ˆiiï¼‰æ¯ä¸ªå€™é€‰å¯¹è±¡çš„å…³ç³»è¯æ®æ•°é‡ï¼Œï¼ˆiiiï¼‰å®ƒä»¬åœ¨æç¤ºä¸­çš„ç»„ç»‡å’Œæ’åºã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†çŸ¥è¯†å›¾ï¼ˆKGï¼‰æ¥å‘ç°å€™é€‰å¯¹è±¡å¹¶æŒ–æ˜å…³ç³»è·¯å¾„ï¼Œè¿™äº›è·¯å¾„è¢«è½¬åŒ–ä¸ºè¯æ®å¡ç‰‡ï¼Œæ€»ç»“äº†æ¯ä¸ªå€™é€‰POIçš„åˆç†æ€§ã€‚ç„¶åï¼Œå†»ç»“çš„LLMå……å½“æ¨ç†å¼•æ“ï¼ŒåŸºäºç»è¿‡æ”¿ç­–ä¼˜åŒ–çš„æç¤ºï¼Œä»KGå‘ç°çš„å€™é€‰é›†ä¸­ç”Ÿæˆæ¨èã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPrompt-as-Policyå§‹ç»ˆä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨ä¸æ´»è·ƒç”¨æˆ·çš„å‡†ç¡®æ€§æ–¹é¢å¹³å‡æé«˜äº†7.7%ï¼ŒåŒæ—¶åœ¨æ´»è·ƒç”¨æˆ·ä¸Šä¿æŒç«äº‰æ€§èƒ½ï¼Œè€Œæ— éœ€è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08012v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åœ¨æ™ºèƒ½åŸå¸‚æœåŠ¡ï¼ˆå¦‚æ—…æ¸¸ã€é¤é¥®å’Œäº¤é€šï¼‰ä¸­ï¼Œä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰çš„æ¨èè‡³å…³é‡è¦ã€‚åœ¨é¢ä¸´å†·å¯åŠ¨æ¡ä»¶æ—¶ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¼šé¢ä¸´å›°éš¾ï¼Œå…¶ä¸­ç”¨æˆ·ä¸POIçš„äº’åŠ¨è¾ƒå°‘ã€‚è¿‘æœŸçš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒSFTéœ€è¦å¤§é‡æ˜‚è´µçš„æ³¨é‡Šå¹¶ä¸”æ— æ³•æ¨å¹¿åˆ°éæ´»è·ƒç”¨æˆ·ï¼Œè€ŒICLä¸­çš„é™æ€æç¤ºæ— æ³•é€‚åº”å¤šæ ·åŒ–çš„ç”¨æˆ·ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†å›¾è°±çš„â€œæç¤ºä½œä¸ºç­–ç•¥â€ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å¼•å¯¼æç¤ºæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸Šä¸‹æ–‡å¼ºç›—ä¼˜åŒ–åŠ¨æ€æ„å»ºæç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æç¤ºæ„å»ºè§†ä¸ºå¯å­¦ä¹ çš„ç­–ç•¥ï¼Œè‡ªé€‚åº”åœ°ç¡®å®šï¼ˆiï¼‰è¦åŒ…å«å“ªäº›å…³ç³»è¯æ®ï¼Œï¼ˆiiï¼‰æ¯ä¸ªå€™é€‰äººçš„è¯æ®æ•°é‡ï¼Œï¼ˆiiiï¼‰æç¤ºå†…è¯æ®çš„ç»„ç»‡å’Œæ’åºã€‚ç‰¹åˆ«æ˜¯åœ¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­ï¼Œæˆ‘ä»¬æ„å»ºå‘ç°å€™é€‰äººå’ŒæŒ–æ˜å…³ç³»è·¯å¾„ï¼Œå°†å…¶è½¬åŒ–ä¸ºè¯æ®å¡ç‰‡ï¼Œæ€»ç»“æ¯ä¸ªå€™é€‰POIçš„åˆç†æ€§ã€‚å†»ç»“çš„LLMåˆ™ä½œä¸ºæ¨ç†å¼•æ“ï¼Œæ ¹æ®ä¼˜åŒ–åçš„ç­–ç•¥æç¤ºä»çŸ¥è¯†å›¾è°±å‘ç°çš„å€™é€‰é›†ä¸­ç”Ÿæˆæ¨èã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œâ€œæç¤ºä½œä¸ºç­–ç•¥â€å§‹ç»ˆä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿ï¼Œåœ¨ä¸æ´»è·ƒç”¨æˆ·ä¸Šå¹³å‡æé«˜äº†7.7%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨æ´»è·ƒç”¨æˆ·ä¸Šä¿æŒç«äº‰åŠ›æ€§èƒ½ï¼Œä¸”æ— éœ€æ¨¡å‹å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åœ¨æ™ºèƒ½åŸå¸‚æœåŠ¡ä¸­ï¼Œä¸‹ä¸€ä¸ªå…´è¶£ç‚¹æ¨èå¯¹äºæ—…æ¸¸ã€é¤é¥®å’Œäº¤é€šç­‰æœåŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>åœ¨å†·å¯åŠ¨æ¡ä»¶ä¸‹ï¼Œå¤§å¤šæ•°æ¨èæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³ç”¨æˆ·ä¸POIä¹‹é—´äº’åŠ¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«ç”¨äºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨é™åˆ¶ï¼Œå¦‚ç›‘ç£å¾®è°ƒçš„é«˜æˆæœ¬å’Œéæ´»è·ƒç”¨æˆ·çš„æ¨å¹¿é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€œæç¤ºä½œä¸ºç­–ç•¥â€ï¼Œç»“åˆçŸ¥è¯†å›¾è°±å’Œå¼ºåŒ–å­¦ä¹ æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åŠ¨æ€æ„å»ºæç¤ºï¼Œå¹¶è‡ªé€‚åº”åœ°ç¡®å®šåº”åŒ…å«å“ªäº›å…³ç³»è¯æ®ã€è¯æ®æ•°é‡å’Œç»„ç»‡æ–¹å¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œâ€œæç¤ºä½œä¸ºç­–ç•¥â€åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸æ´»è·ƒç”¨æˆ·ä¸Šçš„å‡†ç¡®ç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c1fdc2ded07b49d6d73f00f2be30247~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145078&auth_key=1760145078-0-0-8e232ddb07bae0aca5d42eadc35c230a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dc6878c5edbc32e3e86c613cc93aa1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145085&auth_key=1760145085-0-0-b093461fe988969c82accc02a6ac7d65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f162e0e987af642906a0697ad04aec66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145092&auth_key=1760145092-0-0-eeeb70ca013aa94213370f90b740510c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cef0bb62e4aeb8c367ff5e9ec35bd2fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145098&auth_key=1760145098-0-0-844ae6fc7fc7e1dbdd8a94a47f3e2c9f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Active-Confusion-Expression-in-Large-Language-Models-Leveraging-World-Models-toward-Better-Social-Reasoning"><a href="#Active-Confusion-Expression-in-Large-Language-Models-Leveraging-World-Models-toward-Better-Social-Reasoning" class="headerlink" title="Active Confusion Expression in Large Language Models: Leveraging World   Models toward Better Social Reasoning"></a>Active Confusion Expression in Large Language Models: Leveraging World   Models toward Better Social Reasoning</h2><p><strong>Authors:Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu</strong></p>
<p>While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1â€™s reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like â€œtrickyâ€ and â€œconfusedâ€ when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agentsâ€™ subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦å’Œä»£ç æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä»¬åœ¨ç¤¾äº¤æ¨ç†ä»»åŠ¡ä¸Šé‡åˆ°äº†å›°éš¾ï¼Œè¡¨ç°å‡ºè®¤çŸ¥æ··æ·†ã€é€»è¾‘ä¸ä¸€è‡´ä»¥åŠå®¢è§‚ä¸–ç•ŒçŠ¶æ€å’Œä¸»è§‚ä¿¡å¿µçŠ¶æ€ä¹‹é—´çš„æ··æ·†ã€‚é€šè¿‡å¯¹DeepSeek-R1æ¨ç†è½¨è¿¹çš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬å‘ç°LLMç»å¸¸é‡åˆ°æ¨ç†éšœç¢ï¼Œåœ¨å¤„ç†å¤šå‚ä¸è€…å’Œæ—¶é—´çº¿çš„åœºæ™¯æ—¶ï¼Œå€¾å‘äºè¾“å‡ºâ€œæ£˜æ‰‹â€å’Œâ€œå›°æƒ‘â€ç­‰çŸ›ç›¾æœ¯è¯­ï¼Œä»è€Œå¯¼è‡´é”™è¯¯æ¨ç†æˆ–æ— é™å¾ªç¯ã€‚æ ¸å¿ƒé—®é¢˜æ˜¯å®ƒä»¬æ— æ³•å°†å®¢è§‚ç°å®ä¸ä»£ç†äººçš„ä¸»è§‚ä¿¡å¿µåŒºåˆ†å¼€æ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ä¸–ç•Œæ¨¡å‹å¢å¼ºçš„æ¨ç†æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ„å»ºäº†ä¸€ä¸ªåŠ¨æ€çš„æ–‡æœ¬ä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºè·Ÿè¸ªå®ä½“çŠ¶æ€å’Œæ—¶é—´åºåˆ—ã€‚å®ƒåŠ¨æ€ç›‘æ§æ¨ç†è½¨è¿¹ä¸­çš„æ··æ·†æŒ‡æ ‡ï¼Œå¹¶æä¾›æ¸…æ™°çš„ä¸–ç•ŒçŠ¶æ€æè¿°æ¥åŠæ—¶å¹²é¢„ï¼Œå¸®åŠ©æ¨¡å‹è§£å†³è®¤çŸ¥å›°å¢ƒã€‚è¯¥æœºåˆ¶æ¨¡ä»¿äº†äººç±»å¦‚ä½•ä½¿ç”¨éšå¼ä¸–ç•Œæ¨¡å‹æ¥åŒºåˆ†å¤–éƒ¨äº‹ä»¶å’Œå†…éƒ¨ä¿¡å¿µã€‚åœ¨ä¸‰ä¸ªç¤¾äº¤åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æœºåˆ¶åœ¨å‡†ç¡®æ€§æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ˆä¾‹å¦‚Hi-ToMä¸­çš„+10%ï¼‰ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼ˆæœ€å¤šå‡å°‘33.8%çš„ä»¤ç‰Œï¼‰ï¼Œä¸ºåœ¨ç¤¾äº¤ç¯å¢ƒä¸­éƒ¨ç½²LLMæä¾›äº†ç®€å•è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07974v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦å’Œä»£ç æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šé‡åˆ°å›°éš¾ï¼Œå­˜åœ¨è®¤çŸ¥æ··æ·†ã€é€»è¾‘ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚é€šè¿‡åˆ†æDeepSeek-R1çš„æ¨ç†è½¨è¿¹ï¼Œå‘ç°LLMsæ˜“é­é‡æ¨ç†å›°å¢ƒï¼Œåœ¨å¤„ç†å¤šå‚ä¸è€…å’Œæ—¶é—´çº¿çš„åœºæ™¯æ—¶ï¼Œä¼šè¾“å‡ºâ€œæ£˜æ‰‹â€å’Œâ€œå›°æƒ‘â€ç­‰çŸ›ç›¾æœ¯è¯­ï¼Œå¯¼è‡´é”™è¯¯æ¨ç†æˆ–æ— é™å¾ªç¯ã€‚æ ¸å¿ƒé—®é¢˜åœ¨äºæ— æ³•åŒºåˆ†å®¢è§‚ç°å®å’Œä»£ç†çš„ä¸»è§‚ä¿¡å¿µã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ä¸–ç•Œæ¨¡å‹å¢å¼ºçš„æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºåŠ¨æ€æ–‡æœ¬ä¸–ç•Œæ¨¡å‹æ¥è·Ÿè¸ªå®ä½“çŠ¶æ€å’Œæ—¶åºåºåˆ—ï¼ŒåŠæ—¶ç›‘æµ‹æ¨ç†è½¨è¿¹ä¸­çš„å›°æƒ‘æŒ‡æ ‡å¹¶æä¾›æ˜ç¡®çš„ä¸–ç•ŒçŠ¶æ€æè¿°ï¼Œå¸®åŠ©æ¨¡å‹è§£å†³è®¤çŸ¥å›°å¢ƒã€‚è¯¥æœºåˆ¶æ¨¡ä»¿äººç±»ä½¿ç”¨éšå¼ä¸–ç•Œæ¨¡å‹æ¥åŒºåˆ†å¤–éƒ¨äº‹ä»¶å’Œå†…éƒ¨ä¿¡å¿µã€‚åœ¨ä¸‰ä¸ªç¤¾ä¼šåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æœºåˆ¶åœ¨å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—æé«˜ï¼ˆä¾‹å¦‚Hi-ToMæé«˜10%ï¼‰ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ï¼ˆæœ€å¤šå‡å°‘33.8%çš„ä»¤ç‰Œæ•°ï¼‰ï¼Œä¸ºåœ¨ç¤¾ä¼šç¯å¢ƒä¸­éƒ¨ç½²LLMsæä¾›äº†ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ï¼Œå­˜åœ¨è®¤çŸ¥æ··æ·†å’Œé€»è¾‘ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>LLMsåœ¨å¤„ç†å¤šå‚ä¸è€…å’Œæ—¶é—´çº¿çš„åœºæ™¯æ—¶æ˜“é­é‡æ¨ç†å›°å¢ƒï¼Œè¾“å‡ºçŸ›ç›¾æœ¯è¯­ã€‚</li>
<li>æ ¸å¿ƒé—®é¢˜åœ¨äºLLMsæ— æ³•åŒºåˆ†å®¢è§‚ç°å®å’Œä»£ç†çš„ä¸»è§‚ä¿¡å¿µã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ä¸–ç•Œæ¨¡å‹å¢å¼ºçš„æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºåŠ¨æ€æ–‡æœ¬ä¸–ç•Œæ¨¡å‹æ¥è§£å†³LLMsçš„æ¨ç†é—®é¢˜ã€‚</li>
<li>è¯¥æœºåˆ¶èƒ½å®æ—¶ç›‘æµ‹å¹¶å¹²é¢„å›°æƒ‘çš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>æœºåˆ¶æ¨¡ä»¿äººç±»ä½¿ç”¨éšå¼ä¸–ç•Œæ¨¡å‹æ¥åŒºåˆ†å¤–éƒ¨äº‹ä»¶å’Œå†…éƒ¨ä¿¡å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d9d1ddb3c6472ed06fbff1a2c61fdce1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145106&auth_key=1760145106-0-0-80aabe3dbd5e2960736662b40a5232e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-112489b499b2538c3cd8717d03a11498~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145113&auth_key=1760145113-0-0-dd06f4b736a71f1b83925285d020a83c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86f6cb2f8ac3c982ff163829c09d38c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145119&auth_key=1760145119-0-0-cf498804f6d2ba4c41d60c616a44d3b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57f454212a18d0cd22dc76af70631a21~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145126&auth_key=1760145126-0-0-fc0fcf9c2ac53dca78e95d6af9cabe84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b04c515d91ffc8e75cdf54e9f4d4fe04~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145133&auth_key=1760145133-0-0-8b5fbf0db090edfe2477469e6134a186&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc317d624de4eddce39bda8b81ed1579~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145139&auth_key=1760145139-0-0-5f891d1b770bbfafe90dba96c54050dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LightReasoner-Can-Small-Language-Models-Teach-Large-Language-Models-Reasoning"><a href="#LightReasoner-Can-Small-Language-Models-Teach-Large-Language-Models-Reasoning" class="headerlink" title="LightReasoner: Can Small Language Models Teach Large Language Models   Reasoning?"></a>LightReasoner: Can Small Language Models Teach Large Language Models   Reasoning?</h2><p><strong>Authors:Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latterâ€™s unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expertâ€™s advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/LightReasoner">https://github.com/HKUDS/LightReasoner</a> </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¿™é€šå¸¸æ˜¯é€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰å®ç°çš„ã€‚ç„¶è€Œï¼ŒSFTèµ„æºå¯†é›†ï¼Œä¾èµ–äºå¤§é‡ç²¾é€‰æ•°æ®é›†ã€æ‹’ç»é‡‡æ ·æ¼”ç¤ºå’Œæ‰€æœ‰æ ‡è®°çš„ç»Ÿä¸€ä¼˜åŒ–ï¼Œå°½ç®¡åªæœ‰ä¸€å°éƒ¨åˆ†æºå¸¦äº†æœ‰æ„ä¹‰çš„å­¦ä¹ ä»·å€¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ä¸ªä¼¼æ˜¯è€Œéçš„æƒ³æ³•ï¼šæ˜¯å¦å¯ä»¥é€šè¿‡æ­ç¤ºåæ˜ å¤§å‹è¯­è¨€æ¨¡å‹ç‹¬ç‰¹ä¼˜åŠ¿çš„é«˜ä»·å€¼æ¨ç†æ—¶åˆ»ï¼Œè®©å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ•™æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Ÿæˆ‘ä»¬æå‡ºäº†LightReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¼ºå¤§ä¸“å®¶æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸šä½™æ¨¡å‹ï¼ˆSLMï¼‰ä¹‹é—´è¡Œä¸ºå·®å¼‚çš„æ–°å‹æ¡†æ¶ã€‚LightReasoneråˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰é‡‡æ ·é˜¶æ®µï¼Œå®ƒå®šä½å…³é”®çš„æ¨ç†æ—¶åˆ»ï¼Œå¹¶é€šè¿‡ä¸“å®¶ä¸ä¸šä½™çˆ±å¥½è€…çš„å¯¹æ¯”æ¥æ„å»ºæ•æ‰ä¸“å®¶ä¼˜åŠ¿çš„ç›‘ç£ç¤ºä¾‹ï¼›2)å¾®è°ƒé˜¶æ®µï¼Œä½¿ä¸“å®¶æ¨¡å‹ä¸è¿™äº›è’¸é¦ç¤ºä¾‹å¯¹é½ï¼Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLightReasoneråœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾90%çš„æ—¶é—´æ¶ˆè€—ã€80%çš„é‡‡æ ·é—®é¢˜å’Œ99%çš„è°ƒä¼˜æ ‡è®°ä½¿ç”¨é‡ï¼Œè€Œä¸”è¿™ä¸€åˆ‡éƒ½ä¸ä¾èµ–äºçœŸå®æ ‡ç­¾ã€‚é€šè¿‡å°†è¾ƒå¼±çš„å°å‹è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºæœ‰æ•ˆçš„æ•™å­¦ä¿¡å·ï¼ŒLightReasoneræä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”èµ„æºé«˜æ•ˆçš„æ–¹æ³•æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HKUDS/LightReasoner%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HKUDS/LightReasonerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07962v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å®ç°ã€‚ç„¶è€Œï¼ŒSFTèµ„æºå¯†é›†ï¼Œä¾èµ–å¤§å‹æ•°æ®é›†ã€æ‹’ç»é‡‡æ ·æ¼”ç¤ºå’Œç»Ÿä¸€ä¼˜åŒ–æ‰€æœ‰æ ‡è®°ç¬¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç›¸åçš„æƒ³æ³•ï¼šæ˜¯å¦å¯ä»¥ç”±å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ¥æ•™å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ­ç¤ºåæ˜ å…¶ç‹¬ç‰¹ä¼˜åŠ¿çš„æ¨ç†æ—¶åˆ»ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LightReasoneræ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¼ºä¸“å®¶æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼±ä¸šä½™æ¨¡å‹ï¼ˆSLMï¼‰ä¹‹é—´çš„è¡Œä¸ºå·®å¼‚ã€‚LightReasoneråˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä¸€æ˜¯é‡‡æ ·é˜¶æ®µï¼Œç¡®å®šå…³é”®æ¨ç†æ—¶åˆ»å¹¶é€šè¿‡ä¸“å®¶ä¸ä¸šä½™å¯¹æ¯”æ„å»ºæ•æ‰ä¸“å®¶ä¼˜åŠ¿çš„ç›‘ç£ç¤ºä¾‹ï¼›äºŒæ˜¯å¾®è°ƒé˜¶æ®µï¼Œç”¨è¿™äº›æç‚¼çš„ç¤ºä¾‹å¯¹é½ä¸“å®¶æ¨¡å‹ï¼Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚LightReasoneråœ¨ä¸ƒä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œæœ€é«˜è¾¾28.1%ï¼ŒåŒæ—¶å‡å°‘äº†æ—¶é—´æ¶ˆè€—ã€é‡‡æ ·é—®é¢˜å’Œå¾®è°ƒæ ‡è®°çš„ä½¿ç”¨é‡ï¼Œä¸”æ— éœ€ä¾èµ–çœŸå®æ ‡ç­¾ã€‚é€šè¿‡å°†è¾ƒå¼±çš„å°å‹è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºæœ‰æ•ˆçš„æ•™å­¦ä¿¡å·ï¼ŒLightReasonerä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†å¯ä¼¸ç¼©å’Œèµ„æºé«˜æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»éœ€èµ„æºå¯†é›†å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å¯èƒ½é€šè¿‡æ­ç¤ºé«˜ä»·å€¼æ¨ç†æ—¶åˆ»æ¥æ•™å¯¼LLMï¼Œåæ˜ å…¶ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</li>
<li>LightReasoneræ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨ä¸“å®¶æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸šä½™æ¨¡å‹ï¼ˆSLMï¼‰ä¹‹é—´çš„è¡Œä¸ºå·®å¼‚ã€‚</li>
<li>LightReasoneråŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé‡‡æ ·é˜¶æ®µå’Œå¾®è°ƒé˜¶æ®µã€‚</li>
<li>é‡‡æ ·é˜¶æ®µç¡®å®šå…³é”®æ¨ç†æ—¶åˆ»å¹¶æ„å»ºæ•æ‰ä¸“å®¶ä¼˜åŠ¿çš„ç›‘ç£ç¤ºä¾‹ã€‚</li>
<li>å¾®è°ƒé˜¶æ®µä½¿ç”¨æç‚¼çš„ç¤ºä¾‹å¯¹é½ä¸“å®¶æ¨¡å‹ï¼Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LightReasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æé«˜çš„å‡†ç¡®æ€§å’Œèµ„æºæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c407ba378a65a35f8df1e7cd1251d667~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145146&auth_key=1760145146-0-0-9cc195be75c1ef4bafc6373208314b58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3ca17f92b44ef005866088d301b63bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145154&auth_key=1760145154-0-0-0bc1bf9874697a70129b69a0e6d36b37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d190eec7b89314583b9adfe4a1d1efe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145160&auth_key=1760145160-0-0-5e026b39ba2b6145ee758898cb974b8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3fe49e9abaee7fbce4fc5ad7cf9eed4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145167&auth_key=1760145167-0-0-a078dbf792e0db03013b36687c3676e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a13d8d1b8e1a5f4ed519f3c6388d855~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145173&auth_key=1760145173-0-0-bfdfc1256b44ccf8b35a23cd88d98fdc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-a7ef7465fa414c8bad36ed951b897930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760145180&auth_key=1760145180-0-0-1a40e4df6f93262ee4e95af44576d06e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  NaViL Rethinking Scaling Properties of Native Multimodal Large Language   Models under Data Constraints
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ac567ddf04807c810a888cfc8183129f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083871&auth_key=1760083871-0-0-c599fb4d7d75497fab9f282ae0c30f36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  SHADES -- $^{22}$Ne($Î±$,n)$^{25}$Mg reaction rate in the Gamow   window
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
