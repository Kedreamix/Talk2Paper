<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-29  Teaching Audio Models to Reason A Unified Framework for Source- and   Layer-wise Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    58 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-29-更新"><a href="#2025-09-29-更新" class="headerlink" title="2025-09-29 更新"></a>2025-09-29 更新</h1><h2 id="Teaching-Audio-Models-to-Reason-A-Unified-Framework-for-Source-and-Layer-wise-Distillation"><a href="#Teaching-Audio-Models-to-Reason-A-Unified-Framework-for-Source-and-Layer-wise-Distillation" class="headerlink" title="Teaching Audio Models to Reason: A Unified Framework for Source- and   Layer-wise Distillation"></a>Teaching Audio Models to Reason: A Unified Framework for Source- and   Layer-wise Distillation</h2><p><strong>Authors:Runyan Yang, Yuke Si, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang</strong></p>
<p>While large audio language models excel at tasks like ASR and emotion recognition, they still struggle with complex reasoning due to the modality gap between audio and text as well as the lack of structured intermediate supervision. To address this, we propose a unified knowledge distillation framework to transfer reasoning capabilities from a high-capacity textual teacher model to a student audio models while preserving its acoustic competence. Our method introduces two key dimensions: source-wise distillation, which leverages both textual and acoustic teachers to provide complementary modality-specific supervision; and layer-wise distillation, which aligns teacher signals with appropriate student layers to improve transfer efficiency. This dual-dimensional strategy enables fine-grained control over the distillation process, effectively bridging the gap between symbolic reasoning and speech representations. Experimental results show significant improvements in audio reasoning performance, demonstrating the effectiveness of our framework as a reasoning transfer solution for audio modeling. </p>
<blockquote>
<p>虽然大型音频语言模型在语音识别和情感识别等任务上表现出色，但由于音频和文本之间的模态差距以及缺乏结构化中间监督，它们在复杂推理方面仍面临挑战。为了解决这一问题，我们提出了一个统一的知识蒸馏框架，以从高容量的文本教师模型向学生音频模型转移推理能力，同时保留其声学能力。我们的方法引入了两个关键维度：源级蒸馏，利用文本和声音教师提供补充的模态特定监督；和层级蒸馏，将教师信号与适当的学生层对齐，以提高传输效率。这种双重维度的策略可以对蒸馏过程进行精细控制，有效地弥合了符号推理和语音表示之间的差距。实验结果表明，在音频推理性能方面取得了显著改进，证明了我们框架作为音频建模推理解决方案的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18579v1">PDF</a> 5 pages; submitted to ICASSP 2026</p>
<p><strong>总结</strong></p>
<p>该文针对大型音频语言模型在处理复杂的推理任务时面临的挑战，提出了一种统一的知识蒸馏框架。该框架旨在从高容量的文本教师模型向学生音频模型转移推理能力，同时保留其声学能力。方法包括两个关键维度：源级蒸馏和层级蒸馏。源级蒸馏利用文本和声音教师提供模态特定的监督，而层级蒸馏则使教师信号与学生层对齐以提高转移效率。这种双重维度的策略对蒸馏过程进行了精细控制，有效地缩小了符号推理与语音表示之间的差距。实验结果表明，该框架在音频推理性能上取得了显著提高，证明了其作为音频模型的推理转移解决方案的有效性。</p>
<p><strong>要点</strong></p>
<ol>
<li>大型音频语言模型在复杂推理任务上仍有挑战，原因在于音频和文本之间的模态差距以及缺乏结构化的中间监督。</li>
<li>提出了一种统一的知识蒸馏框架，旨在从文本教师模型向音频学生模型转移推理能力。</li>
<li>框架包含两个关键维度：源级蒸馏和层级蒸馏，分别利用文本和声音教师提供模态特定的监督，以及提高教师信号与学生层的对齐效率。</li>
<li>双重维度的策略对蒸馏过程进行了精细控制，有效缩小了符号推理与语音表示之间的差距。</li>
<li>实验结果证明了该框架在音频推理性能上的显著提高。</li>
<li>框架的出现解决了音频模型在复杂推理任务上的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18579v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18579v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18579v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HarmoniFuse-A-Component-Selective-and-Prompt-Adaptive-Framework-for-Multi-Task-Speech-Language-Modeling"><a href="#HarmoniFuse-A-Component-Selective-and-Prompt-Adaptive-Framework-for-Multi-Task-Speech-Language-Modeling" class="headerlink" title="HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for   Multi-Task Speech Language Modeling"></a>HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for   Multi-Task Speech Language Modeling</h2><p><strong>Authors:Yuke Si, Runyan Yang, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang</strong></p>
<p>Recent advances in large language models have facilitated the development of unified speech language models (SLMs) capable of supporting multiple speech tasks within a shared architecture. However, tasks such as automatic speech recognition (ASR) and speech emotion recognition (SER) rely on distinct types of information: ASR primarily depends on linguistic content, whereas SER requires the integration of both linguistic and paralinguistic cues. Existing multitask SLMs typically adopt naive parameter sharing or prompt-based conditioning without explicitly modeling the differences in information composition required by each task. Such designs risk task interference and performance degradation, especially under limited data conditions. To address these limitations, we propose HarmoniFuse, a component-selective and prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse is designed to harmonize heterogeneous task demands by selecting and fusing task-relevant components of speech representations. Specifically, it integrates a gated speech encoder to extract task-specific acoustic features and a prompt-adaptive dynamic fusion module to aggregate transformer layers based on task characteristics. In addition, a batch-interleaved training strategy enables leveraging separate ASR and SER datasets without requiring joint annotation. Experimental results demonstrate that HarmoniFuse improves both ASR and SER performance, offering a scalable and robust solution for multitask speech understanding under realistic data constraints. </p>
<blockquote>
<p>近期大型语言模型的进步促进了统一语音语言模型（SLM）的发展，这种模型能够在共享架构内支持多种语音任务。然而，自动语音识别（ASR）和语音情感识别（SER）等任务依赖于不同类型的信息：ASR主要依赖于语言内容，而SER则需要整合语言和副语言线索。现有的多任务SLM通常采用简单的参数共享或基于提示的条件设置，而没有明确建模每个任务所需的信息组成的差异。这种设计存在任务干扰和性能下降的风险，特别是在数据有限的情况下。为了解决这些局限性，我们提出了HarmoniFuse，这是一个用于多任务语音语言建模的组件选择和提示自适应框架。HarmoniFuse旨在通过选择和融合与任务相关的语音表示组件来协调异质的任务需求。具体来说，它集成了门控语音编码器以提取特定于任务的声学特征，以及一个提示自适应动态融合模块，该模块基于任务特性聚合变压器层。此外，批内交错训练策略使得我们能够利用单独的ASR和SER数据集，而无需联合注释。实验结果表明，HarmoniFuse提高了ASR和SER的性能，为现实数据约束下的多任务语音理解提供了可扩展且稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18570v1">PDF</a> 5 pages; submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型的新进展推动了统一语音语言模型（SLM）的发展，能够在一个共享架构内支持多个语音任务。针对自动语音识别（ASR）和语音情感识别（SER）等任务，提出了HarmoniFuse框架，该框架通过选择语音表示的任务相关组件并进行融合，来协调不同任务的需求。实验结果表明，HarmoniFuse在ASR和SER的性能上均有提升，为现实数据约束下的多任务语音理解提供了可扩展和稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的进步推动了统一语音语言模型（SLM）的发展，能支持多种语音任务。</li>
<li>ASR主要依赖语言内容，而SER需要整合语言和副语言线索。</li>
<li>现有多任务SLM通常采用简单的参数共享或基于提示的条件设置，没有显式地建模每个任务所需的信息组成差异，这可能导致任务干扰和性能下降。</li>
<li>HarmoniFuse框架通过选择并融合语音表示的任务相关组件，协调不同任务的需求。</li>
<li>HarmoniFuse设计了门控语音编码器和提示自适应动态融合模块，分别用于提取任务特定的声音特征和根据任务特性聚合变压器层。</li>
<li>HarmoniFuse采用批内交错训练策略，能够利用单独的ASR和SER数据集，无需联合注释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18570">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18570v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18570v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18570v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CPCLDETECTOR-Knowledge-Enhancement-and-Alignment-Selection-for-Chinese-Patronizing-and-Condescending-Language-Detection"><a href="#CPCLDETECTOR-Knowledge-Enhancement-and-Alignment-Selection-for-Chinese-Patronizing-and-Condescending-Language-Detection" class="headerlink" title="CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese   Patronizing and Condescending Language Detection"></a>CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese   Patronizing and Condescending Language Detection</h2><p><strong>Authors:Jiaxun Yang, Yifei Han, Long Zhang, Yujie Liu, Bin Li, Bo Gao, Yangfan He, Kejia Zhan</strong></p>
<p>Chinese Patronizing and Condescending Language (CPCL) is an implicitly discriminatory toxic speech targeting vulnerable groups on Chinese video platforms. The existing dataset lacks user comments, which are a direct reflection of video content. This undermines the model’s understanding of video content and results in the failure to detect some CPLC videos. To make up for this loss, this research reconstructs a new dataset PCLMMPLUS that includes 103k comment entries and expands the dataset size. We also propose the CPCLDetector model with alignment selection and knowledge-enhanced comment content modules. Extensive experiments show the proposed CPCLDetector outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS . CPLC videos are detected more accurately, supporting content governance and protecting vulnerable groups. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/jiaxunyang256/PCLD">https://github.com/jiaxunyang256/PCLD</a>. </p>
<blockquote>
<p>中文霸权与居高临下的语言（CPCL）是一种针对中文视频平台上脆弱群体的隐性歧视性有毒言论。现有的数据集缺少用户评论，这些评论是视频内容的直接反映。这削弱了模型对视频内容的理解，导致无法检测到一些CPCL视频。为了弥补这一损失，本研究重建了一个新的数据集PCLMMPLUS，其中包括10.3万条评论条目，并扩大了数据集规模。我们还提出了带有对齐选择和知识增强评论内容的CPCLDetector模型。大量实验表明，提出的CPCLDetector在PCLMM上优于SOTA，在PCLMMPLUS上表现更佳。CPLC视频的检测更为准确，有助于内容治理和保护脆弱群体。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/jiaxunyang256/PCLD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jiaxunyang256/PCLD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18562v2">PDF</a> Submitted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>面向中文视频平台上的脆弱群体的歧视性语言——中国居高临下式与轻蔑性语言（CPCL）是一种隐性歧视的有毒言论。现有数据集缺乏用户评论，无法直接反映视频内容，削弱了模型对视频内容的理解，导致部分CPCL视频检测失败。本研究构建了新的PCLMMPLUS数据集，包含10.3万条评论条目，扩大了数据集规模。同时，提出了CPCLDetector模型，包括对齐选择和知识增强评论内容模块。实验表明，CPCLDetector在PCLMM上优于现有技术，在PCLMMPLUS上性能更高，更准确地检测出CPLC视频，为内容治理和保护脆弱群体提供支持。代码和数据集可访问于：<a target="_blank" rel="noopener" href="https://github.com/jiaxunyang256/PCLD%E3%80%82">https://github.com/jiaxunyang256/PCLD。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>中国居高临下式与轻蔑性语言（CPCL）是针对中文视频平台脆弱群体的隐性歧视语言。</li>
<li>现有数据集缺乏用户评论，无法全面反映视频内容，影响模型对CPCL视频的检测效果。</li>
<li>研究构建了新的PCLMMPLUS数据集，包括大量用户评论，以扩大数据集规模并改进模型检测效果。</li>
<li>提出了CPCLDetector模型，包含对齐选择和知识增强评论内容模块，以提高对CPCL视频的检测准确性。</li>
<li>实验表明，CPCLDetector在PCLMM和PCLMMPLUS数据集上的性能优于现有技术。</li>
<li>CPCLDetector能更准确地检测出CPLC视频，为内容治理和保护脆弱群体提供支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18562v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18562v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18562v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18562v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18562v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18562v2/page_3_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MNV-17-A-High-Quality-Performative-Mandarin-Dataset-for-Nonverbal-Vocalization-Recognition-in-Speech"><a href="#MNV-17-A-High-Quality-Performative-Mandarin-Dataset-for-Nonverbal-Vocalization-Recognition-in-Speech" class="headerlink" title="MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal   Vocalization Recognition in Speech"></a>MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal   Vocalization Recognition in Speech</h2><p><strong>Authors:Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu</strong></p>
<p>Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing lexical content, but largely fail to recognize nonverbal vocalizations (NVs) embedded in speech, such as sighs, laughs, and coughs. This capability is important for a comprehensive understanding of human communication, as NVs convey crucial emotional and intentional cues. Progress in NV-aware ASR has been hindered by the lack of high-quality, well-annotated datasets. To address this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech dataset. Unlike most existing corpora that rely on model-based detection, MNV-17’s performative nature ensures high-fidelity, clearly articulated NV instances. To the best of our knowledge, MNV-17 provides the most extensive set of nonverbal vocalization categories, comprising 17 distinct and well-balanced classes of common NVs. We benchmarked MNV-17 on four mainstream ASR architectures, evaluating their joint performance on semantic transcription and NV classification. The dataset and the pretrained model checkpoints will be made publicly available to facilitate future research in expressive ASR. </p>
<blockquote>
<p>主流自动语音识别（ASR）系统在转录词汇内容方面表现出色，但很大程度上无法识别嵌入语音中的非言语发声（NVs），如叹息、笑声和咳嗽。对于全面理解人类交流而言，这种能力很重要，因为非言语发声传递了重要的情感和意图线索。缺乏高质量、良好注释的数据集阻碍了NV感知ASR的进展。为了弥补这一差距，我们引入了MNV-17，这是一个7.55小时的表演性普通话语音数据集。与大多数现有语料库基于模型检测不同，MNV-17的表演性质确保了高保真、表达清晰的NV实例。据我们所知，MNV-17提供了最广泛的非言语发声类别集合，包含17个独特且平衡良好的常见NV类别。我们在四种主流ASR架构上评估了MNV-17，评估它们在语义转录和NV分类上的联合性能。数据集和预训练模型检查点将公开发布，以促进未来在表达性ASR方面的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18196v2">PDF</a> Official dataset available at:   <a target="_blank" rel="noopener" href="https://github.com/yongaifadian1/MNV-17">https://github.com/yongaifadian1/MNV-17</a>. Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了主流自动语音识别（ASR）系统在处理非语言声音（NVs）方面的不足，如叹息、笑声和咳嗽等。为了解决这个问题，并推动非语言声音感知的ASR研究，本文引入了一个名为MNV-17的中文语音数据集。该数据集包含多种非语言声音类别，具有表演性特点，确保了高保真、清晰的非语言声音实例。作者对四种主流ASR架构进行了基准测试，评估它们在语义转录和非语言声音分类上的联合性能。该数据集和预训练模型检查点将公开发布，以促进未来在非语言声音感知ASR领域的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>主流ASR系统在转录词汇内容方面表现出色，但在识别嵌入在语音中的非语言声音（NVs）方面存在不足。</li>
<li>非语言声音在全面理解人类沟通中很重要，因为它们传递了关键的情感和意图线索。</li>
<li>MNV-17是一个新的中文语音数据集，旨在解决现有研究中缺乏高质量、良好注释的数据集的问题。</li>
<li>MNV-17数据集具有表演性特点，确保非语言声音实例的高保真和清晰性。</li>
<li>MNV-17包含了最广泛的非语言声音类别集，包括17种常见且平衡良好的类别。</li>
<li>作者使用四种主流ASR架构对MNV-17进行了基准测试，评估其在语义转录和非语言声音分类上的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18196v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18196v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18196v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18196v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18196v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.18196v2/page_3_2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LibriTTS-VI-A-Public-Corpus-and-Novel-Methods-for-Efficient-Voice-Impression-Control"><a href="#LibriTTS-VI-A-Public-Corpus-and-Novel-Methods-for-Efficient-Voice-Impression-Control" class="headerlink" title="LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice   Impression Control"></a>LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice   Impression Control</h2><p><strong>Authors:Junki Ohmura, Yuki Ito, Emiru Tsunoo, Toshiyuki Sekiya, Toshiyuki Kumakura</strong></p>
<p>Fine-grained control over voice impressions (e.g., making a voice brighter or calmer) is a key frontier for creating more controllable text-to-speech. However, this nascent field faces two key challenges. The first is the problem of impression leakage, where the synthesized voice is undesirably influenced by the speaker’s reference audio, rather than the separately specified target impression, and the second is the lack of a public, annotated corpus. To mitigate impression leakage, we propose two methods: 1) a training strategy that separately uses an utterance for speaker identity and another utterance of the same speaker for target impression, and 2) a novel reference-free model that generates a speaker embedding solely from the target impression, achieving the benefits of improved robustness against the leakage and the convenience of reference-free generation. Objective and subjective evaluations demonstrate a significant improvement in controllability. Our best method reduced the mean squared error of 11-dimensional voice impression vectors from 0.61 to 0.41 objectively and from 1.15 to 0.92 subjectively, while maintaining high fidelity. To foster reproducible research, we introduce LibriTTS-VI, the first public voice impression dataset released with clear annotation standards, built upon the LibriTTS-R corpus. </p>
<blockquote>
<p>对语音印象进行精细控制（例如，使声音更明亮或更平静）是创建更多可控文本到语音的关键前沿领域。然而，这个新兴领域面临两个主要挑战。第一个问题是印象泄露的问题，即合成声音受到说话者的参考音频的不必要影响，而不是单独指定的目标印象。第二个挑战是缺乏公共注释语料库。为了减轻印象泄露的问题，我们提出了两种方法：1) 一种训练策略，它分别使用说话人身份的语句和同一说话人的另一个语句作为目标印象；2) 一种新型无参考模型，该模型仅从目标印象中生成说话人嵌入，实现了对抗泄露的鲁棒性和无参考生成的便利性。客观和主观评估表明，可控性有了显著改进。我们的最佳方法将11维语音印象向量的均方误差从客观上的0.61降低到0.41，从主观上的1.15降低到0.92，同时保持了高保真度。为了推动可重复的研究，我们基于LibriTTS-R语料库，介绍了具有清晰注释标准的首个公共语音印象数据集LibriTTS-VI。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15626v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong><br>语音印象的精细控制（例如，使声音更明亮或更平静）是创建更可控的文本到语音转换的关键前沿领域。然而，这个新兴领域面临两个主要挑战：一是印象泄露问题，合成声音受到说话者的参考音频的不必要影响，而非单独指定的目标印象；二是缺乏公共、注释的语料库。为缓解印象泄露问题，我们提出了两种办法：一是分别使用说话者身份和目的印象的发音训练策略；二是全新无需参考模型的说话者嵌入生成方式，仅从目标印象中生成。客观和主观评估显示，控制性显著提高。我们的最佳方法将11维语音印象向量的均方误差从客观上的0.61降至0.41，主观上从1.15降至0.92，同时保持高保真度。为支持可重复研究，我们基于LibriTTS-R语料库，推出了带有清晰注释标准的首个公共语音印象数据集LibriTTS-VI。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音印象的精细控制是文本到语音转换领域的关键前沿。</li>
<li>印象泄露和缺乏公共数据集是该领域的主要挑战。</li>
<li>提出了两种解决印象泄露问题的方法：分别使用说话者身份和目的印象的发音训练策略，以及全新的无需参考模型的说话者嵌入生成方式。</li>
<li>该方法在控制性和语音质量上均有显著提高。</li>
<li>最佳方法的均方误差显著降低，同时保持高保真度。</li>
<li>推出了首个公共语音印象数据集LibriTTS-VI，带有清晰注释标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15626v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15626v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15626v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15626v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR"><a href="#Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR" class="headerlink" title="Frustratingly Easy Data Augmentation for Low-Resource ASR"></a>Frustratingly Easy Data Augmentation for Low-Resource ASR</h2><p><strong>Authors:Katsumi Ibaraki, David Chiang</strong></p>
<p>This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel text–using gloss-based replacement, random replacement, or an LLM-based approach–and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability. </p>
<blockquote>
<p>本文介绍了三种针对低资源自动语音识别（ASR）的自主数据增强方法。我们的技术首先使用基于术语的替换、随机替换或基于大型语言模型（LLM）的方法生成新文本，然后将文本转换为语音（TTS）以生成合成音频。我们将这些方法仅应用于原始标注数据，应用于四种资源极为有限的语言（瓦特龙戈语、纳斯塔语、谢内克赫恩布里亚特语和卡卡贝语）。在原始音频和生成的合成数据组合上微调预训练的Wav2Vec2-XLSR-53模型，取得了显著的性能提升，其中纳斯塔语的绝对字词错误率（WER）降低了14.3%。这些方法在四种低资源语言中都证明了其有效性，并显示出对高资源语言如英语的实用性，证明了其广泛的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15373v2">PDF</a> 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026</p>
<p><strong>Summary</strong><br>本文介绍了三种针对低资源自动语音识别（ASR）的自我完善数据增强方法。这些方法通过生成新文本（使用基于光泽的替换、随机替换或大型语言模型（LLM）方法），然后应用文本转语音（TTS）技术生成合成音频。将这些仅利用原始注释数据的方法应用于四种资源极度有限的语言（Vatlongos、Nashta、Shinekhen Buryat和Kakabe）。通过对预训练的Wav2Vec2-XLSR-53模型进行微调，结合原始音频和生成的合成数据，取得了显著的性能提升，其中Nashta的绝对词错误率（WER）降低了14.3%。这些方法在四种低资源语言中都表现出有效性，并证明在高资源语言如英语中也有实用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文介绍了三种自我完善的数据增强方法，这些方法专门针对低资源的自动语音识别（ASR）。</li>
<li>方法包括基于光泽的替换、随机替换或使用大型语言模型（LLM）生成新文本。</li>
<li>通过文本转语音（TTS）技术，将生成的文本转化为合成音频。</li>
<li>在四种资源有限的语言中进行了实验，并结合了原始音频和合成数据对预训练模型进行了微调。</li>
<li>实验结果显示，这种方法显著提高了性能，其中Nashta语言的词错误率降低了14.3%。</li>
<li>这些方法不仅适用于低资源语言，也对高资源语言如英语有实用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15373v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15373v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15373v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15373v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15373v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Speech-Language-Models-for-Under-Represented-Languages-Insights-from-Wolof"><a href="#Speech-Language-Models-for-Under-Represented-Languages-Insights-from-Wolof" class="headerlink" title="Speech Language Models for Under-Represented Languages: Insights from   Wolof"></a>Speech Language Models for Under-Represented Languages: Insights from   Wolof</h2><p><strong>Authors:Yaya Sy, Dioula Doucouré, Christophe Cerisara, Irina Illina</strong></p>
<p>We present our journey in training a speech language model for Wolof, an underrepresented language spoken in West Africa, and share key insights. We first emphasize the importance of collecting large-scale, spontaneous, high-quality unsupervised speech data, and show that continued pretraining HuBERT on this dataset outperforms both the base model and African-centric models on ASR. We then integrate this speech encoder into a Wolof LLM to train the first Speech LLM for this language, extending its capabilities to tasks such as speech translation. Furthermore, we explore training the Speech LLM to perform multi-step Chain-of-Thought before transcribing or translating. Our results show that the Speech LLM not only improves speech recognition but also performs well in speech translation. The models and the code will be openly shared. </p>
<blockquote>
<p>我们介绍了在西非语少数民族语言之一的沃洛夫语中训练语音语言模型的旅程，并分享了关键见解。我们首先强调收集大规模、自发性的高质量无监督语音数据的重要性，并展示了在持续使用该数据集对HuBERT进行预训练的效果，其在语音识别方面超越了基础模型和面向非洲的模型。然后，我们将此语音编码器集成到沃洛夫语LLM中，以训练该语言的第一个语音LLM，并将其功能扩展到语音翻译等任务。此外，我们探索训练语音LLM执行多步骤的Chain-of-Thought后进行转录或翻译。我们的结果表明，语音LLM不仅提高了语音识别能力，而且在语音翻译方面也表现出色。模型和代码将公开共享。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15362v2">PDF</a> </p>
<p><strong>总结</strong><br>    本文介绍了对西非地区的沃尔夫语言进行语言模型训练的过程，并分享了关键见解。文章强调收集大规模、自发、高质量的无监督语音数据的重要性，展示在此基础上继续进行HuBERT预训练的优势。接着，将语音编码器集成到沃尔夫语言的大型语言模型中，扩展其语音翻译等功能。此外，探索了让语言模型进行多步骤Chain-of-Thought训练，以提高其语音识别和翻译性能。模型及代码将公开共享。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>收集大规模、自发、高质量的无监督语音数据对于训练语言模型至关重要。</li>
<li>继续在特定数据集上进行HuBERT预训练，可提高语音识别性能，超越基础模型和非洲中心模型。</li>
<li>将语音编码器集成到沃尔夫的大型语言模型中，实现了该语言的首个语音语言模型，扩展了其语音翻译等功能。</li>
<li>训练语言模型进行多步骤的Chain-of-Thought，提升了语音识别和翻译的性能。</li>
<li>这种语音语言模型不仅提高了语音识别率，而且在语音翻译方面也表现出色。</li>
<li>模型和代码将公开共享，有助于促进对该语言和其它非洲语言的进一步研究。</li>
<li>此项目展示了在对待代表性较低的语言（如沃尔夫语）时的技术进步和重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15362">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15362v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15362v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15362v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.15362v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Canary-1B-v2-Parakeet-TDT-0-6B-v3-Efficient-and-High-Performance-Models-for-Multilingual-ASR-and-AST"><a href="#Canary-1B-v2-Parakeet-TDT-0-6B-v3-Efficient-and-High-Performance-Models-for-Multilingual-ASR-and-AST" class="headerlink" title="Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST"></a>Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST</h2><p><strong>Authors:Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nikolay Karpov, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>This report introduces Canary-1B-v2, a fast, robust multilingual model for Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built with a FastConformer encoder and Transformer decoder, it supports 25 languages primarily European. The model was trained on 1.7M hours of total data samples, including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce hallucinations for ASR and AST. We describe its two-stage pre-training and fine-tuning process with dynamic data balancing, as well as experiments with an nGPT encoder. Results show nGPT scales well with massive data, while FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2 outperforms Whisper-large-v3 on English ASR while being 10x faster, and delivers competitive multilingual ASR and AST performance against larger models like Seamless-M4T-v2-large and LLM-based systems. We also release Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the same 25 languages with just 600M parameters. </p>
<blockquote>
<p>本报告介绍了Canary-1B-v2，这是一个快速、稳健的多语言模型，用于自动语音识别（ASR）和语音到文本的翻译（AST）。该模型采用FastConformer编码器和Transformer解码器构建，主要支持25种欧洲语言。该模型在170万小时的数据样本上进行训练，包括Granary和NeMo ASR Set 3.0，并添加了非语音音频，以减少ASR和AST的幻觉。我们描述了其两阶段预训练和微调过程，采用动态数据平衡，以及使用nGPT编码器的实验。结果表明，nGPT在大量数据中表现良好，而FastConformer在微调后表现出色。对于时间戳，Canary-1B-v2使用NeMo强制对齐器（NFA）和辅助CTC模型，为ASR和AST提供可靠的时间戳。评估表明，Canary-1B-v2在英语ASR方面的表现优于Whisper-large-v3，而且速度更快（快10倍），并在多语言ASR和AST方面与更大的模型（如无缝M4T-v2大型模型和基于LLM的系统）表现出竞争力。我们还发布了Parakeet-TDT-0.6B-v3，它是v2的继任者，提供相同的25种语言的多语言ASR，仅使用6亿个参数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14128v2">PDF</a> Mini Version of it Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>这份报告介绍了Canary-1B-v2模型，这是一个快速、稳健的多语言自动语音识别（ASR）和语音到文本翻译（AST）模型。该模型采用FastConformer编码器和Transformer解码器，支持25种主要欧洲语言。模型在170万小时的数据样本上进行训练，包括Granary和NeMo ASR Set 3.0，并添加了非语音音频以减少ASR和AST的幻觉。描述了其两阶段预训练和微调过程以及动态数据平衡技术，并与nGPT编码器的实验进行了比较。实验结果显示，FastConformer在微调后表现出色，而nGPT在大规模数据上表现良好。Canary-1B-v2使用NeMo强制对齐器（NFA）提供可靠的分段级时间戳。评估显示，Canary-1B-v2在英语ASR上的表现优于Whisper-large-v3，速度更快，并且在多语言ASR和AST性能上与其他大型模型如无缝M4T-v2大型和基于LLM的系统具有竞争力。此外，还发布了Parakeet-TDT-0.6B-v3版本，具有相同的支持语言数，但参数更少，只有6亿个参数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Canary-1B-v2是一个快速、稳健的多语言自动语音识别（ASR）和语音到文本翻译（AST）模型。</li>
<li>模型采用FastConformer编码器和Transformer解码器结构。</li>
<li>支持包括欧洲在内的25种语言。</li>
<li>模型训练在大量数据样本上进行，包含非语音音频以优化ASR和AST性能。</li>
<li>模型采用两阶段预训练和微调过程及动态数据平衡技术实现高性能。FastConformer在微调后表现出最佳性能。同时验证了nGPT编码器在处理大规模数据时的优势。</li>
<li>Canary-1B-v2使用NeMo强制对齐器提供可靠的时间戳信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.14128v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.14128v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.14128v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.14128v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.14128v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Robust-Audio-Visual-Target-Speaker-Extraction-with-Emotion-Aware-Multiple-Enrollment-Fusion"><a href="#Robust-Audio-Visual-Target-Speaker-Extraction-with-Emotion-Aware-Multiple-Enrollment-Fusion" class="headerlink" title="Robust Audio-Visual Target Speaker Extraction with Emotion-Aware   Multiple Enrollment Fusion"></a>Robust Audio-Visual Target Speaker Extraction with Emotion-Aware   Multiple Enrollment Fusion</h2><p><strong>Authors:Zhan Jin, Bang Zeng, Peijun Yang, Jiarong Du, Juan Liu, Ming Li</strong></p>
<p>Target Speaker Extraction (TSE) is a critical challenge in cocktail party scenarios. While leveraging multiple modalities, such as voice, lip, face, and expression embeddings, can enhance performance, real-world applications often suffer from intermittent modality dropout. This paper presents a comprehensive study on the interactions and robustness of various multimodal fusion strategies under varying degrees of modality dropout. We build upon a state-of-the-art audio-visual speech enhancement system and integrate four distinct speaker identity cues: lip embeddings for synchronized contextual information, a voice speaker embedding extracted via cross-attention for acoustic consistency, a static face embedding for speaker identity, and a novel dynamic expression embedding for frame-wise emotional features. We systematically evaluate different combinations of these modalities under two key training regimes: zero dropout and 80% modality dropout. Extensive experiments demonstrate that while a full multimodal ensemble achieves optimal performance under ideal (zero dropout) conditions, its effectiveness diminishes significantly when test-time dropout occurs without prior exposure during training. Crucially, we show that training with a high (80%) modality dropout rate dramatically enhances model robustness, enabling the system to maintain superior performance even under severe test-time missing modalities. Our findings highlight that voice embeddings exhibit consistent robustness, while the proposed expression embedding provides valuable complementary information. This work underscores the importance of training strategies that account for real-world imperfection, moving beyond pure performance maximization to achieve practical reliability in multimodal speech enhancement systems. </p>
<blockquote>
<p>目标说话人提取（TSE）是鸡尾酒会场景中的一个关键挑战。虽然利用多种模态（如语音、嘴唇、面部和表情嵌入）可以提高性能，但实际应用中经常受到间歇性模态中断的影响。本文对不同模态融合策略在不同模态中断程度下的交互和稳健性进行了全面研究。我们基于先进的视听语音增强系统，集成了四种不同的说话人身份线索：用于同步上下文信息的嘴唇嵌入、通过交叉注意力提取的语音说话人嵌入以用于声音一致性、用于说话人身份的静态面部嵌入、以及用于帧级情感特征的新型动态表情嵌入。我们系统地评估了这两种关键训练制度下的不同模态组合：零中断和80%的模态中断。大量实验表明，虽然在理想条件下（零中断），全模态集合达到最佳性能，但当测试时发生中断且训练期间未暴露于中断时，其有效性会显著降低。关键的是，我们表明，以高（80%）的模态中断率进行训练可以极大地提高模型的稳健性，使系统在测试时即使在严重缺失模态的情况下也能保持卓越的性能。我们的研究结果表明，语音嵌入表现出持续的稳健性，而所提出的表情嵌入提供了有价值的补充信息。这项工作强调了训练策略的重要性，这些策略要考虑到现实世界的缺陷，超越单纯的性能最大化，以实现多模态语音增强系统的实际可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12583v2">PDF</a> </p>
<p><strong>摘要</strong><br>在目标语音提取（TSE）方面，本研究对现实场景中的多变表现提出了深度分析。利用多重模式，例如声音、嘴唇、脸部以及表情嵌入可以增强系统性能。本研究综合分析了在不同模态脱落条件下，多种模态融合策略的互动及稳健性。基于对顶尖视听语音增强系统的研究，本研究融入了四个主要的说话人身份线索，包括同步语境信息的嘴唇嵌入、用于声音一致性的跨注意力语音嵌入、用于身份识别的静态面部嵌入以及用于情绪特征的新动态表情嵌入。实验系统地评估了在两种主要训练方案下不同模态的组合：零脱落率和百分之八十的模态脱落率。大量实验显示，全模态集合在理想条件下（零脱落率）表现最优，但在测试时若发生未受训练的模态脱落现象，其有效性会显著下降。关键的是，训练时采用高模态脱落率（百分之八十）可极大提高模型的稳健性，使系统在测试时即使面临严重缺失模态也能保持卓越性能。本研究发现语音嵌入具有一贯的稳健性，而提出的表现嵌入提供了宝贵的辅助信息。这项研究强调在设计实用可靠的模态语音增强系统时，需超越纯粹的性能最大化考虑现实世界的不完善之处并制定有效的训练策略。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>目标语音提取（TSE）在鸡尾酒会场景中是一个挑战。</li>
<li>多模式融合（如声音、嘴唇、脸部和表情嵌入）能提高系统性能。</li>
<li>研究评估了不同模态组合在不同训练环境下的表现，包括零脱落和百分之八十的模态脱落。</li>
<li>在理想条件下，全模态集合表现最优，但在测试时模态脱落会降低其有效性。</li>
<li>高模态脱落率训练提高模型稳健性，使系统在面对严重缺失模态时仍能保持性能。</li>
<li>语音嵌入展现出稳健性，而表情嵌入提供了有价值的辅助信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.12583v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.12583v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.12583v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.12583v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2509.12583v2/page_3_2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild"><a href="#Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild" class="headerlink" title="Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild"></a>Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild</h2><p><strong>Authors:Jing-Tong Tzeng, Bo-Hao Su, Ya-Tse Wu, Hsing-Hang Chou, Chi-Chun Lee</strong></p>
<p>In this study, we revisit key training strategies in machine learning often overlooked in favor of deeper architectures. Specifically, we explore balancing strategies, activation functions, and fine-tuning techniques to enhance speech emotion recognition (SER) in naturalistic conditions. Our findings show that simple modifications improve generalization with minimal architectural changes. Our multi-modal fusion model, integrating these optimizations, achieves a valence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute Regression. Notably, fine-tuning RoBERTa and WavLM separately in a single-modality setting, followed by feature fusion without training the backbone extractor, yields the highest valence performance. Additionally, focal loss and activation functions significantly enhance performance without increasing complexity. These results suggest that refining core components, rather than deepening models, leads to more robust SER in-the-wild. </p>
<blockquote>
<p>在这项研究中，我们重新审视了机器学习中的关键训练策略，这些策略通常被更深的架构所忽视。具体来说，我们探索了平衡策略、激活函数和微调技术，以提高自然条件下的语音情绪识别（SER）。我们的研究结果表明，简单的修改可以在极小的架构变动下提高泛化能力。我们融合多种模式的多模态融合模型，通过整合这些优化措施，在任务2：情感属性回归中取得了最高价态评分（CCC值为0.6953）。值得注意的是，在单模态环境中分别对RoBERTa和WavLM进行微调，然后进行特征融合而不训练骨架提取器，可以获得最高的价态性能。此外，焦点损失和激活函数在不增加复杂性的情况下显著提高了性能。这些结果表明，优化核心组件而不是深化模型，可以在野外实现更稳健的SER。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07282v2">PDF</a> Proceedings of Interspeech 2025</p>
<p><strong>Summary</strong><br>     本研究重新探讨了机器学习中的关键训练策略，这些策略通常被更深的架构所忽视。研究探索了平衡策略、激活函数和微调技术，以改善自然条件下的语音情感识别（SER）。研究发现，简单的修改可以改进泛化能力，且只需进行少量的架构更改。多模态融合模型整合这些优化后，在任务2：情感属性回归中取得了0.6953的效价CCC，为最佳效价得分。特别是，分别在单模态设置中微调RoBERTa和WavLM，然后进行特征融合而不训练主干提取器，获得了最高的效价性能。此外，焦点损失和激活函数显著提高性能，且不增加复杂性。结果暗示，改进核心组件，而非深化模型，是实现更稳健的野生SER的关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究重新考虑了常被忽视的训练策略，如平衡策略、激活函数和微调技术。</li>
<li>这些策略被应用于改善自然条件下的语音情感识别（SER）。</li>
<li>简单的修改可以显著提高模型的泛化能力，且不需要复杂的架构变动。</li>
<li>多模态融合模型取得最佳效价得分，在情感属性回归任务中表现优异。</li>
<li>单独微调RoBERTa和WavLM，再结合特征融合，获得最高效价性能。</li>
<li>焦点损失和激活函数能显著提高性能，同时不增加模型复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2508.07282v2/page_3_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Interpretable-Embeddings-of-Speech-Enhance-and-Explain-Brain-Encoding-Performance-of-Audio-Models"><a href="#Interpretable-Embeddings-of-Speech-Enhance-and-Explain-Brain-Encoding-Performance-of-Audio-Models" class="headerlink" title="Interpretable Embeddings of Speech Enhance and Explain Brain Encoding   Performance of Audio Models"></a>Interpretable Embeddings of Speech Enhance and Explain Brain Encoding   Performance of Audio Models</h2><p><strong>Authors:Riki Shimizu, Richard J. Antonello, Chandan Singh, Nima Mesgarani</strong></p>
<p>Speech foundation models (SFMs) are increasingly hailed as powerful computational models of human speech perception. However, since their representations are inherently black-box, it remains unclear what drives their alignment with brain responses. To remedy this, we built linear encoding models from six interpretable feature families: mel-spectrogram, Gabor filter bank features, speech presence, phonetic, syntactic, and semantic features, and contextualized embeddings from three state-of-the-art SFMs (Whisper, HuBERT, WavLM), quantifying electrocorticography (ECoG) response variance shared between feature classes. Variance-partitioning analyses revealed several key insights: First, the SFMs’ alignment with the brain can be mostly explained by their ability to learn and encode simple interpretable speech features. Second, SFMs exhibit a systematic trade-off between encoding of brain-relevant low-level and high-level features across layers. Finally, our results show that SFMs learn brain-relevant semantics which cannot be explained by lower-level speech features, with this capacity increasing with model size and context length. Together, our findings suggest a principled approach to build more interpretable, accurate, and efficient encoding models of the brain by augmenting SFM embeddings with interpretable features. </p>
<blockquote>
<p>语音基础模型（SFMs）被越来越多地誉为人类语音感知的强大计算模型。然而，由于它们的表示本质上是黑箱，尚不清楚是什么驱动它们与大脑反应的匹配。为了弥补这一缺陷，我们从六个可解释的特征家族中构建了线性编码模型：梅尔频谱图、Gabor滤波器组特征、语音存在特征、语音特征、句法特征和语义特征，以及来自三种最先进的SFM（Whisper、HuBERT、WavLM）的上下文嵌入，量化电皮质图（ECoG）在特征类别之间共享的反应方差。方差分配分析揭示了几个关键见解：首先，SFM与大脑的匹配度主要可以解释为它们学习和编码简单可解释的语音特征的能力。其次，SFM在编码与大脑相关的高级和低级特征时，各层级间存在系统性权衡。最后，我们的结果表明，SFM学习到的与大脑相关的语义无法仅通过低级语音特征来解释，且这种能力随着模型大小和上下文长度的增加而增强。总之，我们的研究结果表明，通过增强SFM嵌入与可解释特征，可以建立更具原则性、更准确和高效的脑编码模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16080v2">PDF</a> 19 pages, 5 figures</p>
<p><strong>Summary</strong>：</p>
<p>语音基础模型（SFMs）被赞誉为人类语音感知的强大计算模型。然而，由于其表示方式本质上是黑盒，尚不清楚是什么驱动其与大脑响应的对齐。为解决这一问题，本文通过线性编码模型，从六个可解释特征家族出发，以及来自三种最新SFMs的语境化嵌入，量化了与大脑响应的共享电皮质响应方差。分析揭示了关键见解：首先，SFMs与大脑的对齐主要源于其学习和编码简单可解释语音特征的能力；其次，SFMs在编码大脑相关低级别和高级别特征时存在系统性权衡；最后，SFMs学习到与大脑相关的语义信息，无法由低级语音特征解释，且这种能力随模型大小和语境长度增加而增强。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>SFMs与大脑的对接主要源于其编码简单可解释语音特征的能力。</li>
<li>SFMs在编码大脑相关低级别和高级别特征时存在权衡。</li>
<li>SFMs能学习到与大脑相关的语义信息，这一能力随模型大小和语境长度增加而增强。</li>
<li>电皮质响应方差分析能有效评估SFMs与大脑响应的对齐程度。</li>
<li>SFMs的嵌入可以通过增加可解释特征来增强对大脑的编码模型的解读性、准确性和效率。</li>
<li>不同特征家族对SFMs与大脑对齐的贡献程度不同，需要针对性地进行特征选择和优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2507.16080v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2507.16080v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2507.16080v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation"><a href="#Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation" class="headerlink" title="Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation"></a>Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation</h2><p><strong>Authors:Jialong Mai, Xiaofen Xing, Yawei Li, Weidong Chen, Zhipeng Li, Jingyuan Xing, Xiangmin Xu</strong></p>
<p>Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively “memorize” the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance. </p>
<blockquote>
<p>最近的研究集中在将语音大语言模型（SLLM）应用于语音情感识别（SER）中。然而，语音模态中固有的高帧率严重限制了SLLM的信号处理和理解能力。例如，一个具有4K上下文窗口的SLLM在50Hz的特征采样率下只能处理80秒的音频，然后就达到其容量极限。SLLM中使用的输入令牌压缩方法忽略了情感在多次对话回合中的连续性和惯性。本文提出了一种带有上下文语义和句子级情感编码的动态参数内存（DPM）机制，能够在SLLM中的有限上下文窗口中处理无限长度的音频。具体来说，DPM在推理过程中逐步将句子级信息和情感编码到一个临时的LoRA模块中，以有效地“记住”上下文信息。我们以情感SLLM作为主干进行训练，并将DPM纳入推理过程中进行对话情感识别（ERC）。在IEMOCAP数据集上的实验结果表明，在处理长音频序列时，DPM能够显著提高SLLM的情感识别能力，达到了最先进的表现水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09076v2">PDF</a> submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>近期研究将语音大语言模型（SLLM）应用于语音情感识别（SER）的提升。然而，语音模态的高帧率严重限制了SLLM的信号处理和理解能力。本文提出一种动态参数记忆（DPM）机制，结合上下文语义和句子级情感编码，在有限的语境窗口内实现无限长度音频的处理。DPM将句子级信息和情感逐步编码到临时LoRA模块中，有效“记忆”上下文信息。在IEMOCAP数据集上的实验结果表明，DPM在处理长音频序列时显著提高了SLLM的情感识别能力，达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音大语言模型（SLLM）在语音情感识别（SER）中的应用受到高帧率限制。</li>
<li>现有输入令牌压缩方法忽略了情感在多个对话回合中的连续性和惯性。</li>
<li>动态参数记忆（DPM）机制结合了上下文语义和句子级情感编码。</li>
<li>DPM通过逐步编码句子级信息和情感到临时LoRA模块中，有效“记忆”上下文。</li>
<li>DPM提高了在处理长音频序列时的情感识别能力。</li>
<li>在IEMOCAP数据集上的实验结果表明DPM性能达到先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09076">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2507.09076v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2507.09076v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2507.09076v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAARMA-Class-Augmentation-with-Adversarial-Mixup-Regularization"><a href="#CAARMA-Class-Augmentation-with-Adversarial-Mixup-Regularization" class="headerlink" title="CAARMA: Class Augmentation with Adversarial Mixup Regularization"></a>CAARMA: Class Augmentation with Adversarial Mixup Regularization</h2><p><strong>Authors:Massa Baali, Xiang Li, Hao Chen, Syed Abdul Hannan, Rita Singh, Bhiksha Raj</strong></p>
<p>Speaker verification is a typical zero-shot learning task, where inference of unseen classes is performed by comparing embeddings of test instances to known examples. The models performing inference must hence naturally generate embeddings that cluster same-class instances compactly, while maintaining separation across classes. In order to learn to do so, they are typically trained on a large number of classes (speakers), often using specialized losses. However real-world speaker datasets often lack the class diversity needed to effectively learn this in a generalizable manner. We introduce CAARMA, a class augmentation framework that addresses this problem by generating synthetic classes through data mixing in the embedding space, expanding the number of training classes. To ensure the authenticity of the synthetic classes we adopt a novel adversarial refinement mechanism that minimizes categorical distinctions between synthetic and real classes. We evaluate CAARMA on multiple speaker verification tasks, as well as other representative zero-shot comparison-based speech analysis tasks and obtain consistent improvements: our framework demonstrates a significant improvement of 8% over all baseline models. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/massabaali7/CAARMA/">https://github.com/massabaali7/CAARMA/</a> </p>
<blockquote>
<p>说话人验证是一个典型的零样本学习任务，通过比较测试实例的嵌入和已知示例来进行未见类的推理。因此，执行推理的模型必须自然地生成嵌入，将同类实例紧密聚类，同时保持各类之间的分离。为了学会这样做，它们通常会在大量类别（说话人）上进行训练，常常使用专门的损失函数。然而，现实世界的说话人数据集往往缺乏以通用方式有效学习所需的类别多样性。我们引入了CAARMA，这是一种类扩展框架，通过嵌入空间中的数据混合生成合成类来解决这个问题，扩大了训练类的数量。为了确保合成类的真实性，我们采用了一种新型对抗性改进机制，最小化合成类和真实类之间的类别差异。我们在多个说话人验证任务以及其他具有代表性的基于零样本比较的语音分析任务上评估了CAARMA，并获得了持续性的改进：我们的框架在所有基准模型上实现了8%的显著改进。代码可在以下网址找到：[<a target="_blank" rel="noopener" href="https://github.com/massabaali7/CAARMA/]">https://github.com/massabaali7/CAARMA/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16718v3">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong>：<br>文本介绍了发言人验证中的零样本学习问题。为了提高模型的泛化能力，提出一个名为CAARMA的类增强框架，通过嵌入空间中的数据混合生成合成类来扩展训练类的数量。采用对抗性精炼机制确保合成类的真实性，缩小与真实类的分类差异。在多个发言人验证任务以及其他基于零样本比较的语音分析任务上进行了评估，相较于基线模型，CAARMA框架显示出一致的改进，提升幅度达8%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>发言人验证是零样本学习任务的典型代表，需要通过比较测试实例的嵌入与已知示例来进行未见类的推断。</li>
<li>模型需要在训练过程中生成紧凑的同类嵌入，同时保持类间的分离。</li>
<li>现实世界的发言人数据集往往缺乏类多样性，难以有效学习泛化能力。</li>
<li>引入CAARMA框架，通过嵌入空间中的数据混合生成合成类，以扩展训练类的数量。</li>
<li>采纳新颖的对抗性精炼机制确保合成类的真实性。</li>
<li>在多个发言人验证任务以及其他基于零样本比较的语音分析任务上评估CAARMA，显示出显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16718">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2503.16718v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2503.16718v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2503.16718v3/page_4_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration"><a href="#LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration" class="headerlink" title="LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration"></a>LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration</h2><p><strong>Authors:Sangmin Lee, Woo-Jin Chung, Hong-Goo Kang</strong></p>
<p>Building a universal multilingual automatic speech recognition (ASR) model that performs equitably across languages has long been a challenge due to its inherent difficulties. To address this task we introduce a Language-Agnostic Multilingual ASR pipeline through orthography Unification and language-specific Transliteration (LAMA-UT). LAMA-UT operates without any language-specific modules while matching the performance of state-of-the-art models trained on a minimal amount of data. Our pipeline consists of two key steps. First, we utilize a universal transcription generator to unify orthographic features into Romanized form and capture common phonetic characteristics across diverse languages. Second, we utilize a universal converter to transform these universal transcriptions into language-specific ones. In experiments, we demonstrate the effectiveness of our proposed method leveraging universal transcriptions for massively multilingual ASR. Our pipeline achieves a relative error reduction rate of 45% when compared to Whisper and performs comparably to MMS, despite being trained on only 0.1% of Whisper’s training data. Furthermore, our pipeline does not rely on any language-specific modules. However, it performs on par with zero-shot ASR approaches which utilize additional language-specific lexicons and language models. We expect this framework to serve as a cornerstone for flexible multilingual ASR systems that are generalizable even to unseen languages. </p>
<blockquote>
<p>构建一种能在各种语言中表现均衡的通用多语种自动语音识别（ASR）模型一直是一个挑战，因为其固有的难度。为了解决这一任务，我们引入了通过正字法统一和语言特定转写（LAMA-UT）的语言无关多语种ASR管道。LAMA-UT在没有任何语言特定模块的情况下运行，同时匹配在少量数据上训练的最新模型的性能。我们的管道包括两个关键步骤。首先，我们利用通用转录生成器将正字特征统一为罗马化形式，并捕捉不同语言的共同语音特征。其次，我们利用通用转换器将这些通用转录转化为特定语言的转录。在实验中，我们证明了利用通用转录进行大规模多语种ASR的方法的有效性。与whisper相比，我们的管道在相对误差减少率方面实现了45%的降低，并且在仅使用whisper 0.1%的训练数据的情况下，其性能与MMS相当。此外，我们的管道不依赖于任何语言特定的模块，但其性能与零射击ASR方法相当，后者利用额外的语言特定词汇和语言模型。我们希望这个框架能成为灵活的多语种ASR系统的基石，甚至可以对未见过的语言进行推广。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15299v3">PDF</a> Accepted to AAAI 2025 (Oral Presentation)</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种语言无关的多语种自动语音识别（ASR）管道，通过正字法统一和语言特定转写（LAMA-UT）来解决多语种ASR的难题。该管道无需任何特定语言的模块，同时在少量数据上达到了最先进的性能。它通过两个关键步骤实现：首先利用通用转录生成器将正字法特征统一转换为罗马化形式，捕捉不同语言的共同语音特征；其次利用通用转换器将这些通用转录转换为特定语言的转录。实验表明，该方法在大量多语种ASR中利用通用转录非常有效，与whisper相比实现了相对误差降低率45%，并且与MMS表现相当，尽管只使用了whisper 0.1%的训练数据。预期该框架将为灵活的多语种ASR系统提供服务，这些系统可以推广到未见过的语言。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新的多语种自动语音识别（ASR）管道LAMA-UT，解决了多语种ASR的难题。</li>
<li>该管道无需任何特定语言的模块，适应多种语言环境下的识别任务。</li>
<li>LAMA-UT包含两个关键步骤：通用转录生成器和通用转换器。</li>
<li>实验显示，LAMA-UT在少量数据上达到了最先进的性能，与whisper相比实现了相对误差降低率45%。</li>
<li>LAMA-UT与MMS表现相当，尽管只使用了whisper 0.1%的训练数据。</li>
<li>LAMA-UT框架可推广到未见过的语言，具有灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15299">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2412.15299v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Application-of-Audio-Fingerprinting-Techniques-for-Real-Time-Scalable-Speech-Retrieval-and-Speech-Clusterization"><a href="#Application-of-Audio-Fingerprinting-Techniques-for-Real-Time-Scalable-Speech-Retrieval-and-Speech-Clusterization" class="headerlink" title="Application of Audio Fingerprinting Techniques for Real-Time Scalable   Speech Retrieval and Speech Clusterization"></a>Application of Audio Fingerprinting Techniques for Real-Time Scalable   Speech Retrieval and Speech Clusterization</h2><p><strong>Authors:Kemal Altwlkany, Sead Delalić, Adis Alihodžić, Elmedin Selmanović, Damir Hasić</strong></p>
<p>Audio fingerprinting techniques have seen great advances in recent years, enabling accurate and fast audio retrieval even in conditions when the queried audio sample has been highly deteriorated or recorded in noisy conditions. Expectedly, most of the existing work is centered around music, with popular music identification services such as Apple’s Shazam or Google’s Now Playing designed for individual audio recognition on mobile devices. However, the spectral content of speech differs from that of music, necessitating modifications to current audio fingerprinting approaches. This paper offers fresh insights into adapting existing techniques to address the specialized challenge of speech retrieval in telecommunications and cloud communications platforms. The focus is on achieving rapid and accurate audio retrieval in batch processing instead of facilitating single requests, typically on a centralized server. Moreover, the paper demonstrates how this approach can be utilized to support audio clustering based on speech transcripts without undergoing actual speech-to-text conversion. This optimization enables significantly faster processing without the need for GPU computing, a requirement for real-time operation that is typically associated with state-of-the-art speech-to-text tools. </p>
<blockquote>
<p>音频指纹技术近年来取得了巨大的进步，即使在查询的音频样本高度退化或嘈杂环境下录制的情况下，也能实现准确快速的音频检索。不出所料，现有工作大多围绕音乐展开，苹果公司的Shazam或谷歌的Now Playing等流行音乐识别服务就是为了在移动设备上实现单个音频识别而设计的。然而，语音的频谱内容与音乐不同，需要对当前的音频指纹方法进行修改。本文旨在为电信和云通信平台上的语音检索这一特殊挑战提供适应现有技术的全新见解。重点是在批处理中实现快速准确的音频检索，而不是为单个请求提供便利，通常在集中式服务器上实现。此外，本文还展示了如何利用这种方法在无需实际进行语音到文本的转换的情况下，根据语音转录来支持音频聚类。这一优化在不使用GPU计算的情况下实现了显著更快的处理速度，而GPU计算通常是与最先进的语音到文本工具相关的实时操作要求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21876v2">PDF</a> Proceedings of the International Convention MIPRO</p>
<p><strong>Summary</strong></p>
<p>近期音频指纹技术在音频检索方面取得了重大进展，即使在音频样本高度失真或噪声环境下也能实现准确快速的音频检索。虽然大多数现有工作都集中在音乐领域，但本文探讨了适应现有技术来解决电信和云通信平台语音检索的特殊挑战。重点在于实现批量处理的快速准确的音频检索，而不是支持单个请求。此外，该方法还可以用于基于语音文本的音频聚类，无需实际进行语音到文本的转换，从而优化处理速度，无需使用通常与最先进的语音到文本工具相关的GPU计算。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频指纹技术近年来在音频检索方面取得了重大进展，实现了在恶劣条件下的快速准确检索。</li>
<li>现有工作主要集中在音乐领域，但语音检索在电信和云通信平台上有特殊挑战。</li>
<li>论文重点研究批量处理的快速准确的音频检索。</li>
<li>论文提出的方法可以支持基于语音文本的音频聚类，无需实际进行语音到文本的转换。</li>
<li>这种新方法优化了处理速度，避免了使用GPU计算的必要性。</li>
<li>该方法有望提高电信和云通信平台的音频处理效率。</li>
<li>此研究为音频指纹技术在语音处理领域的应用提供了新的视角和解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21876">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2410.21876v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2410.21876v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Speech/2410.21876v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/3DGS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_3DGS/2506.09534v2/page_1_0.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-29  Gaussian Herding across Pens An Optimal Transport Perspective on Global   Gaussian Reduction for 3DGS
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_I2I Translation/2505.17912v3/page_0_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-09-29  OrthoLoC UAV 6-DoF Localization and Calibration Using Orthographic   Geodata
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
