<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Teaching Audio Models to Reason A Unified Framework for Source- and   Layer-wise Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1b1b3cfecfb0cefa49c8825dd18834d1')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-29-æ›´æ–°"><a href="#2025-09-29-æ›´æ–°" class="headerlink" title="2025-09-29 æ›´æ–°"></a>2025-09-29 æ›´æ–°</h1><h2 id="Teaching-Audio-Models-to-Reason-A-Unified-Framework-for-Source-and-Layer-wise-Distillation"><a href="#Teaching-Audio-Models-to-Reason-A-Unified-Framework-for-Source-and-Layer-wise-Distillation" class="headerlink" title="Teaching Audio Models to Reason: A Unified Framework for Source- and   Layer-wise Distillation"></a>Teaching Audio Models to Reason: A Unified Framework for Source- and   Layer-wise Distillation</h2><p><strong>Authors:Runyan Yang, Yuke Si, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang</strong></p>
<p>While large audio language models excel at tasks like ASR and emotion recognition, they still struggle with complex reasoning due to the modality gap between audio and text as well as the lack of structured intermediate supervision. To address this, we propose a unified knowledge distillation framework to transfer reasoning capabilities from a high-capacity textual teacher model to a student audio models while preserving its acoustic competence. Our method introduces two key dimensions: source-wise distillation, which leverages both textual and acoustic teachers to provide complementary modality-specific supervision; and layer-wise distillation, which aligns teacher signals with appropriate student layers to improve transfer efficiency. This dual-dimensional strategy enables fine-grained control over the distillation process, effectively bridging the gap between symbolic reasoning and speech representations. Experimental results show significant improvements in audio reasoning performance, demonstrating the effectiveness of our framework as a reasoning transfer solution for audio modeling. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿè¯†åˆ«ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºéŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å·®è·ä»¥åŠç¼ºä¹ç»“æ„åŒ–ä¸­é—´ç›‘ç£ï¼Œå®ƒä»¬åœ¨å¤æ‚æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œä»¥ä»é«˜å®¹é‡çš„æ–‡æœ¬æ•™å¸ˆæ¨¡å‹å‘å­¦ç”ŸéŸ³é¢‘æ¨¡å‹è½¬ç§»æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶å£°å­¦èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼šæºçº§è’¸é¦ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œå£°éŸ³æ•™å¸ˆæä¾›è¡¥å……çš„æ¨¡æ€ç‰¹å®šç›‘ç£ï¼›å’Œå±‚çº§è’¸é¦ï¼Œå°†æ•™å¸ˆä¿¡å·ä¸é€‚å½“çš„å­¦ç”Ÿå±‚å¯¹é½ï¼Œä»¥æé«˜ä¼ è¾“æ•ˆç‡ã€‚è¿™ç§åŒé‡ç»´åº¦çš„ç­–ç•¥å¯ä»¥å¯¹è’¸é¦è¿‡ç¨‹è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†ç¬¦å·æ¨ç†å’Œè¯­éŸ³è¡¨ç¤ºä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨éŸ³é¢‘æ¨ç†æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶ä½œä¸ºéŸ³é¢‘å»ºæ¨¡æ¨ç†è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18579v1">PDF</a> 5 pages; submitted to ICASSP 2026</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä»é«˜å®¹é‡çš„æ–‡æœ¬æ•™å¸ˆæ¨¡å‹å‘å­¦ç”ŸéŸ³é¢‘æ¨¡å‹è½¬ç§»æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶å£°å­¦èƒ½åŠ›ã€‚æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼šæºçº§è’¸é¦å’Œå±‚çº§è’¸é¦ã€‚æºçº§è’¸é¦åˆ©ç”¨æ–‡æœ¬å’Œå£°éŸ³æ•™å¸ˆæä¾›æ¨¡æ€ç‰¹å®šçš„ç›‘ç£ï¼Œè€Œå±‚çº§è’¸é¦åˆ™ä½¿æ•™å¸ˆä¿¡å·ä¸å­¦ç”Ÿå±‚å¯¹é½ä»¥æé«˜è½¬ç§»æ•ˆç‡ã€‚è¿™ç§åŒé‡ç»´åº¦çš„ç­–ç•¥å¯¹è’¸é¦è¿‡ç¨‹è¿›è¡Œäº†ç²¾ç»†æ§åˆ¶ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†ç¬¦å·æ¨ç†ä¸è¯­éŸ³è¡¨ç¤ºä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨éŸ³é¢‘æ¨ç†æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æé«˜ï¼Œè¯æ˜äº†å…¶ä½œä¸ºéŸ³é¢‘æ¨¡å‹çš„æ¨ç†è½¬ç§»è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šä»æœ‰æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºéŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å·®è·ä»¥åŠç¼ºä¹ç»“æ„åŒ–çš„ä¸­é—´ç›‘ç£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨ä»æ–‡æœ¬æ•™å¸ˆæ¨¡å‹å‘éŸ³é¢‘å­¦ç”Ÿæ¨¡å‹è½¬ç§»æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼šæºçº§è’¸é¦å’Œå±‚çº§è’¸é¦ï¼Œåˆ†åˆ«åˆ©ç”¨æ–‡æœ¬å’Œå£°éŸ³æ•™å¸ˆæä¾›æ¨¡æ€ç‰¹å®šçš„ç›‘ç£ï¼Œä»¥åŠæé«˜æ•™å¸ˆä¿¡å·ä¸å­¦ç”Ÿå±‚çš„å¯¹é½æ•ˆç‡ã€‚</li>
<li>åŒé‡ç»´åº¦çš„ç­–ç•¥å¯¹è’¸é¦è¿‡ç¨‹è¿›è¡Œäº†ç²¾ç»†æ§åˆ¶ï¼Œæœ‰æ•ˆç¼©å°äº†ç¬¦å·æ¨ç†ä¸è¯­éŸ³è¡¨ç¤ºä¹‹é—´çš„å·®è·ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨éŸ³é¢‘æ¨ç†æ€§èƒ½ä¸Šçš„æ˜¾è‘—æé«˜ã€‚</li>
<li>æ¡†æ¶çš„å‡ºç°è§£å†³äº†éŸ³é¢‘æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85080aab72260eae4fae5b0f39826af7" align="middle">
<img src="https://picx.zhimg.com/v2-3c20e3f92736badde6f278b8f43648af" align="middle">
<img src="https://picx.zhimg.com/v2-18eb999e0bb25074ff0eebfa55d38501" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HarmoniFuse-A-Component-Selective-and-Prompt-Adaptive-Framework-for-Multi-Task-Speech-Language-Modeling"><a href="#HarmoniFuse-A-Component-Selective-and-Prompt-Adaptive-Framework-for-Multi-Task-Speech-Language-Modeling" class="headerlink" title="HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for   Multi-Task Speech Language Modeling"></a>HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for   Multi-Task Speech Language Modeling</h2><p><strong>Authors:Yuke Si, Runyan Yang, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang</strong></p>
<p>Recent advances in large language models have facilitated the development of unified speech language models (SLMs) capable of supporting multiple speech tasks within a shared architecture. However, tasks such as automatic speech recognition (ASR) and speech emotion recognition (SER) rely on distinct types of information: ASR primarily depends on linguistic content, whereas SER requires the integration of both linguistic and paralinguistic cues. Existing multitask SLMs typically adopt naive parameter sharing or prompt-based conditioning without explicitly modeling the differences in information composition required by each task. Such designs risk task interference and performance degradation, especially under limited data conditions. To address these limitations, we propose HarmoniFuse, a component-selective and prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse is designed to harmonize heterogeneous task demands by selecting and fusing task-relevant components of speech representations. Specifically, it integrates a gated speech encoder to extract task-specific acoustic features and a prompt-adaptive dynamic fusion module to aggregate transformer layers based on task characteristics. In addition, a batch-interleaved training strategy enables leveraging separate ASR and SER datasets without requiring joint annotation. Experimental results demonstrate that HarmoniFuse improves both ASR and SER performance, offering a scalable and robust solution for multitask speech understanding under realistic data constraints. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¿ƒè¿›äº†ç»Ÿä¸€è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å‘å±•ï¼Œè¿™ç§æ¨¡å‹èƒ½å¤Ÿåœ¨å…±äº«æ¶æ„å†…æ”¯æŒå¤šç§è¯­éŸ³ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç­‰ä»»åŠ¡ä¾èµ–äºä¸åŒç±»å‹çš„ä¿¡æ¯ï¼šASRä¸»è¦ä¾èµ–äºè¯­è¨€å†…å®¹ï¼Œè€ŒSERåˆ™éœ€è¦æ•´åˆè¯­è¨€å’Œå‰¯è¯­è¨€çº¿ç´¢ã€‚ç°æœ‰çš„å¤šä»»åŠ¡SLMé€šå¸¸é‡‡ç”¨ç®€å•çš„å‚æ•°å…±äº«æˆ–åŸºäºæç¤ºçš„æ¡ä»¶è®¾ç½®ï¼Œè€Œæ²¡æœ‰æ˜ç¡®å»ºæ¨¡æ¯ä¸ªä»»åŠ¡æ‰€éœ€çš„ä¿¡æ¯ç»„æˆçš„å·®å¼‚ã€‚è¿™ç§è®¾è®¡å­˜åœ¨ä»»åŠ¡å¹²æ‰°å’Œæ€§èƒ½ä¸‹é™çš„é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HarmoniFuseï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šä»»åŠ¡è¯­éŸ³è¯­è¨€å»ºæ¨¡çš„ç»„ä»¶é€‰æ‹©å’Œæç¤ºè‡ªé€‚åº”æ¡†æ¶ã€‚HarmoniFuseæ—¨åœ¨é€šè¿‡é€‰æ‹©å’Œèåˆä¸ä»»åŠ¡ç›¸å…³çš„è¯­éŸ³è¡¨ç¤ºç»„ä»¶æ¥åè°ƒå¼‚è´¨çš„ä»»åŠ¡éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé›†æˆäº†é—¨æ§è¯­éŸ³ç¼–ç å™¨ä»¥æå–ç‰¹å®šäºä»»åŠ¡çš„å£°å­¦ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªæç¤ºè‡ªé€‚åº”åŠ¨æ€èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—åŸºäºä»»åŠ¡ç‰¹æ€§èšåˆå˜å‹å™¨å±‚ã€‚æ­¤å¤–ï¼Œæ‰¹å†…äº¤é”™è®­ç»ƒç­–ç•¥ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å•ç‹¬çš„ASRå’ŒSERæ•°æ®é›†ï¼Œè€Œæ— éœ€è”åˆæ³¨é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHarmoniFuseæé«˜äº†ASRå’ŒSERçš„æ€§èƒ½ï¼Œä¸ºç°å®æ•°æ®çº¦æŸä¸‹çš„å¤šä»»åŠ¡è¯­éŸ³ç†è§£æä¾›äº†å¯æ‰©å±•ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18570v1">PDF</a> 5 pages; submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°è¿›å±•æ¨åŠ¨äº†ç»Ÿä¸€è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å‘å±•ï¼Œèƒ½å¤Ÿåœ¨ä¸€ä¸ªå…±äº«æ¶æ„å†…æ”¯æŒå¤šä¸ªè¯­éŸ³ä»»åŠ¡ã€‚é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç­‰ä»»åŠ¡ï¼Œæå‡ºäº†HarmoniFuseæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‰æ‹©è¯­éŸ³è¡¨ç¤ºçš„ä»»åŠ¡ç›¸å…³ç»„ä»¶å¹¶è¿›è¡Œèåˆï¼Œæ¥åè°ƒä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHarmoniFuseåœ¨ASRå’ŒSERçš„æ€§èƒ½ä¸Šå‡æœ‰æå‡ï¼Œä¸ºç°å®æ•°æ®çº¦æŸä¸‹çš„å¤šä»»åŠ¡è¯­éŸ³ç†è§£æä¾›äº†å¯æ‰©å±•å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†ç»Ÿä¸€è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å‘å±•ï¼Œèƒ½æ”¯æŒå¤šç§è¯­éŸ³ä»»åŠ¡ã€‚</li>
<li>ASRä¸»è¦ä¾èµ–è¯­è¨€å†…å®¹ï¼Œè€ŒSERéœ€è¦æ•´åˆè¯­è¨€å’Œå‰¯è¯­è¨€çº¿ç´¢ã€‚</li>
<li>ç°æœ‰å¤šä»»åŠ¡SLMé€šå¸¸é‡‡ç”¨ç®€å•çš„å‚æ•°å…±äº«æˆ–åŸºäºæç¤ºçš„æ¡ä»¶è®¾ç½®ï¼Œæ²¡æœ‰æ˜¾å¼åœ°å»ºæ¨¡æ¯ä¸ªä»»åŠ¡æ‰€éœ€çš„ä¿¡æ¯ç»„æˆå·®å¼‚ï¼Œè¿™å¯èƒ½å¯¼è‡´ä»»åŠ¡å¹²æ‰°å’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>HarmoniFuseæ¡†æ¶é€šè¿‡é€‰æ‹©å¹¶èåˆè¯­éŸ³è¡¨ç¤ºçš„ä»»åŠ¡ç›¸å…³ç»„ä»¶ï¼Œåè°ƒä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚</li>
<li>HarmoniFuseè®¾è®¡äº†é—¨æ§è¯­éŸ³ç¼–ç å™¨å’Œæç¤ºè‡ªé€‚åº”åŠ¨æ€èåˆæ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºæå–ä»»åŠ¡ç‰¹å®šçš„å£°éŸ³ç‰¹å¾å’Œæ ¹æ®ä»»åŠ¡ç‰¹æ€§èšåˆå˜å‹å™¨å±‚ã€‚</li>
<li>HarmoniFuseé‡‡ç”¨æ‰¹å†…äº¤é”™è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿåˆ©ç”¨å•ç‹¬çš„ASRå’ŒSERæ•°æ®é›†ï¼Œæ— éœ€è”åˆæ³¨é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7191f4c806a476191c09c6a1bceb82e0" align="middle">
<img src="https://picx.zhimg.com/v2-12da6b32b3fb0f91491fea303fa68a23" align="middle">
<img src="https://picx.zhimg.com/v2-5442f6d5f2effd6ac24adc5d1cbda40d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CPCLDETECTOR-Knowledge-Enhancement-and-Alignment-Selection-for-Chinese-Patronizing-and-Condescending-Language-Detection"><a href="#CPCLDETECTOR-Knowledge-Enhancement-and-Alignment-Selection-for-Chinese-Patronizing-and-Condescending-Language-Detection" class="headerlink" title="CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese   Patronizing and Condescending Language Detection"></a>CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese   Patronizing and Condescending Language Detection</h2><p><strong>Authors:Jiaxun Yang, Yifei Han, Long Zhang, Yujie Liu, Bin Li, Bo Gao, Yangfan He, Kejia Zhan</strong></p>
<p>Chinese Patronizing and Condescending Language (CPCL) is an implicitly discriminatory toxic speech targeting vulnerable groups on Chinese video platforms. The existing dataset lacks user comments, which are a direct reflection of video content. This undermines the modelâ€™s understanding of video content and results in the failure to detect some CPLC videos. To make up for this loss, this research reconstructs a new dataset PCLMMPLUS that includes 103k comment entries and expands the dataset size. We also propose the CPCLDetector model with alignment selection and knowledge-enhanced comment content modules. Extensive experiments show the proposed CPCLDetector outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS . CPLC videos are detected more accurately, supporting content governance and protecting vulnerable groups. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/jiaxunyang256/PCLD">https://github.com/jiaxunyang256/PCLD</a>. </p>
<blockquote>
<p>ä¸­æ–‡éœ¸æƒä¸å±…é«˜ä¸´ä¸‹çš„è¯­è¨€ï¼ˆCPCLï¼‰æ˜¯ä¸€ç§é’ˆå¯¹ä¸­æ–‡è§†é¢‘å¹³å°ä¸Šè„†å¼±ç¾¤ä½“çš„éšæ€§æ­§è§†æ€§æœ‰æ¯’è¨€è®ºã€‚ç°æœ‰çš„æ•°æ®é›†ç¼ºå°‘ç”¨æˆ·è¯„è®ºï¼Œè¿™äº›è¯„è®ºæ˜¯è§†é¢‘å†…å®¹çš„ç›´æ¥åæ˜ ã€‚è¿™å‰Šå¼±äº†æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„ç†è§£ï¼Œå¯¼è‡´æ— æ³•æ£€æµ‹åˆ°ä¸€äº›CPCLè§†é¢‘ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€æŸå¤±ï¼Œæœ¬ç ”ç©¶é‡å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†PCLMMPLUSï¼Œå…¶ä¸­åŒ…æ‹¬10.3ä¸‡æ¡è¯„è®ºæ¡ç›®ï¼Œå¹¶æ‰©å¤§äº†æ•°æ®é›†è§„æ¨¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å¸¦æœ‰å¯¹é½é€‰æ‹©å’ŒçŸ¥è¯†å¢å¼ºè¯„è®ºå†…å®¹çš„CPCLDetectoræ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„CPCLDetectoråœ¨PCLMMä¸Šä¼˜äºSOTAï¼Œåœ¨PCLMMPLUSä¸Šè¡¨ç°æ›´ä½³ã€‚CPLCè§†é¢‘çš„æ£€æµ‹æ›´ä¸ºå‡†ç¡®ï¼Œæœ‰åŠ©äºå†…å®¹æ²»ç†å’Œä¿æŠ¤è„†å¼±ç¾¤ä½“ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiaxunyang256/PCLD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jiaxunyang256/PCLDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18562v2">PDF</a> Submitted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>é¢å‘ä¸­æ–‡è§†é¢‘å¹³å°ä¸Šçš„è„†å¼±ç¾¤ä½“çš„æ­§è§†æ€§è¯­è¨€â€”â€”ä¸­å›½å±…é«˜ä¸´ä¸‹å¼ä¸è½»è”‘æ€§è¯­è¨€ï¼ˆCPCLï¼‰æ˜¯ä¸€ç§éšæ€§æ­§è§†çš„æœ‰æ¯’è¨€è®ºã€‚ç°æœ‰æ•°æ®é›†ç¼ºä¹ç”¨æˆ·è¯„è®ºï¼Œæ— æ³•ç›´æ¥åæ˜ è§†é¢‘å†…å®¹ï¼Œå‰Šå¼±äº†æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„ç†è§£ï¼Œå¯¼è‡´éƒ¨åˆ†CPCLè§†é¢‘æ£€æµ‹å¤±è´¥ã€‚æœ¬ç ”ç©¶æ„å»ºäº†æ–°çš„PCLMMPLUSæ•°æ®é›†ï¼ŒåŒ…å«10.3ä¸‡æ¡è¯„è®ºæ¡ç›®ï¼Œæ‰©å¤§äº†æ•°æ®é›†è§„æ¨¡ã€‚åŒæ—¶ï¼Œæå‡ºäº†CPCLDetectoræ¨¡å‹ï¼ŒåŒ…æ‹¬å¯¹é½é€‰æ‹©å’ŒçŸ¥è¯†å¢å¼ºè¯„è®ºå†…å®¹æ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼ŒCPCLDetectoråœ¨PCLMMä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨PCLMMPLUSä¸Šæ€§èƒ½æ›´é«˜ï¼Œæ›´å‡†ç¡®åœ°æ£€æµ‹å‡ºCPLCè§†é¢‘ï¼Œä¸ºå†…å®¹æ²»ç†å’Œä¿æŠ¤è„†å¼±ç¾¤ä½“æä¾›æ”¯æŒã€‚ä»£ç å’Œæ•°æ®é›†å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/jiaxunyang256/PCLD%E3%80%82">https://github.com/jiaxunyang256/PCLDã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­å›½å±…é«˜ä¸´ä¸‹å¼ä¸è½»è”‘æ€§è¯­è¨€ï¼ˆCPCLï¼‰æ˜¯é’ˆå¯¹ä¸­æ–‡è§†é¢‘å¹³å°è„†å¼±ç¾¤ä½“çš„éšæ€§æ­§è§†è¯­è¨€ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ç¼ºä¹ç”¨æˆ·è¯„è®ºï¼Œæ— æ³•å…¨é¢åæ˜ è§†é¢‘å†…å®¹ï¼Œå½±å“æ¨¡å‹å¯¹CPCLè§†é¢‘çš„æ£€æµ‹æ•ˆæœã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†æ–°çš„PCLMMPLUSæ•°æ®é›†ï¼ŒåŒ…æ‹¬å¤§é‡ç”¨æˆ·è¯„è®ºï¼Œä»¥æ‰©å¤§æ•°æ®é›†è§„æ¨¡å¹¶æ”¹è¿›æ¨¡å‹æ£€æµ‹æ•ˆæœã€‚</li>
<li>æå‡ºäº†CPCLDetectoræ¨¡å‹ï¼ŒåŒ…å«å¯¹é½é€‰æ‹©å’ŒçŸ¥è¯†å¢å¼ºè¯„è®ºå†…å®¹æ¨¡å—ï¼Œä»¥æé«˜å¯¹CPCLè§†é¢‘çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCPCLDetectoråœ¨PCLMMå’ŒPCLMMPLUSæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>CPCLDetectorèƒ½æ›´å‡†ç¡®åœ°æ£€æµ‹å‡ºCPLCè§†é¢‘ï¼Œä¸ºå†…å®¹æ²»ç†å’Œä¿æŠ¤è„†å¼±ç¾¤ä½“æä¾›æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2fa33de51e496f508e5fcd722a31f83" align="middle">
<img src="https://picx.zhimg.com/v2-2ca93549c78d829941de366bb3e27c79" align="middle">
<img src="https://picx.zhimg.com/v2-30391bbeb0de7436bf56f2bcbeacd3b1" align="middle">
<img src="https://picx.zhimg.com/v2-ec8481fadb81de158875ad7edb127f14" align="middle">
<img src="https://picx.zhimg.com/v2-4a71989bdf2924e3386b37577e6de176" align="middle">
<img src="https://picx.zhimg.com/v2-e1bc558f792bb85e0defa3ced306e8f2" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MNV-17-A-High-Quality-Performative-Mandarin-Dataset-for-Nonverbal-Vocalization-Recognition-in-Speech"><a href="#MNV-17-A-High-Quality-Performative-Mandarin-Dataset-for-Nonverbal-Vocalization-Recognition-in-Speech" class="headerlink" title="MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal   Vocalization Recognition in Speech"></a>MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal   Vocalization Recognition in Speech</h2><p><strong>Authors:Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu</strong></p>
<p>Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing lexical content, but largely fail to recognize nonverbal vocalizations (NVs) embedded in speech, such as sighs, laughs, and coughs. This capability is important for a comprehensive understanding of human communication, as NVs convey crucial emotional and intentional cues. Progress in NV-aware ASR has been hindered by the lack of high-quality, well-annotated datasets. To address this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech dataset. Unlike most existing corpora that rely on model-based detection, MNV-17â€™s performative nature ensures high-fidelity, clearly articulated NV instances. To the best of our knowledge, MNV-17 provides the most extensive set of nonverbal vocalization categories, comprising 17 distinct and well-balanced classes of common NVs. We benchmarked MNV-17 on four mainstream ASR architectures, evaluating their joint performance on semantic transcription and NV classification. The dataset and the pretrained model checkpoints will be made publicly available to facilitate future research in expressive ASR. </p>
<blockquote>
<p>ä¸»æµè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨è½¬å½•è¯æ±‡å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¾ˆå¤§ç¨‹åº¦ä¸Šæ— æ³•è¯†åˆ«åµŒå…¥è¯­éŸ³ä¸­çš„éè¨€è¯­å‘å£°ï¼ˆNVsï¼‰ï¼Œå¦‚å¹æ¯ã€ç¬‘å£°å’Œå’³å—½ã€‚å¯¹äºå…¨é¢ç†è§£äººç±»äº¤æµè€Œè¨€ï¼Œè¿™ç§èƒ½åŠ›å¾ˆé‡è¦ï¼Œå› ä¸ºéè¨€è¯­å‘å£°ä¼ é€’äº†é‡è¦çš„æƒ…æ„Ÿå’Œæ„å›¾çº¿ç´¢ã€‚ç¼ºä¹é«˜è´¨é‡ã€è‰¯å¥½æ³¨é‡Šçš„æ•°æ®é›†é˜»ç¢äº†NVæ„ŸçŸ¥ASRçš„è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MNV-17ï¼Œè¿™æ˜¯ä¸€ä¸ª7.55å°æ—¶çš„è¡¨æ¼”æ€§æ™®é€šè¯è¯­éŸ³æ•°æ®é›†ã€‚ä¸å¤§å¤šæ•°ç°æœ‰è¯­æ–™åº“åŸºäºæ¨¡å‹æ£€æµ‹ä¸åŒï¼ŒMNV-17çš„è¡¨æ¼”æ€§è´¨ç¡®ä¿äº†é«˜ä¿çœŸã€è¡¨è¾¾æ¸…æ™°çš„NVå®ä¾‹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMNV-17æä¾›äº†æœ€å¹¿æ³›çš„éè¨€è¯­å‘å£°ç±»åˆ«é›†åˆï¼ŒåŒ…å«17ä¸ªç‹¬ç‰¹ä¸”å¹³è¡¡è‰¯å¥½çš„å¸¸è§NVç±»åˆ«ã€‚æˆ‘ä»¬åœ¨å››ç§ä¸»æµASRæ¶æ„ä¸Šè¯„ä¼°äº†MNV-17ï¼Œè¯„ä¼°å®ƒä»¬åœ¨è¯­ä¹‰è½¬å½•å’ŒNVåˆ†ç±»ä¸Šçš„è”åˆæ€§èƒ½ã€‚æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨è¡¨è¾¾æ€§ASRæ–¹é¢çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18196v2">PDF</a> Official dataset available at:   <a target="_blank" rel="noopener" href="https://github.com/yongaifadian1/MNV-17">https://github.com/yongaifadian1/MNV-17</a>. Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸»æµè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å¤„ç†éè¯­è¨€å£°éŸ³ï¼ˆNVsï¼‰æ–¹é¢çš„ä¸è¶³ï¼Œå¦‚å¹æ¯ã€ç¬‘å£°å’Œå’³å—½ç­‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æ¨åŠ¨éè¯­è¨€å£°éŸ³æ„ŸçŸ¥çš„ASRç ”ç©¶ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåä¸ºMNV-17çš„ä¸­æ–‡è¯­éŸ³æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šç§éè¯­è¨€å£°éŸ³ç±»åˆ«ï¼Œå…·æœ‰è¡¨æ¼”æ€§ç‰¹ç‚¹ï¼Œç¡®ä¿äº†é«˜ä¿çœŸã€æ¸…æ™°çš„éè¯­è¨€å£°éŸ³å®ä¾‹ã€‚ä½œè€…å¯¹å››ç§ä¸»æµASRæ¶æ„è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å®ƒä»¬åœ¨è¯­ä¹‰è½¬å½•å’Œéè¯­è¨€å£°éŸ³åˆ†ç±»ä¸Šçš„è”åˆæ€§èƒ½ã€‚è¯¥æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨éè¯­è¨€å£°éŸ³æ„ŸçŸ¥ASRé¢†åŸŸçš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»æµASRç³»ç»Ÿåœ¨è½¬å½•è¯æ±‡å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¯†åˆ«åµŒå…¥åœ¨è¯­éŸ³ä¸­çš„éè¯­è¨€å£°éŸ³ï¼ˆNVsï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>éè¯­è¨€å£°éŸ³åœ¨å…¨é¢ç†è§£äººç±»æ²Ÿé€šä¸­å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä¼ é€’äº†å…³é”®çš„æƒ…æ„Ÿå’Œæ„å›¾çº¿ç´¢ã€‚</li>
<li>MNV-17æ˜¯ä¸€ä¸ªæ–°çš„ä¸­æ–‡è¯­éŸ³æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¸­ç¼ºä¹é«˜è´¨é‡ã€è‰¯å¥½æ³¨é‡Šçš„æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>MNV-17æ•°æ®é›†å…·æœ‰è¡¨æ¼”æ€§ç‰¹ç‚¹ï¼Œç¡®ä¿éè¯­è¨€å£°éŸ³å®ä¾‹çš„é«˜ä¿çœŸå’Œæ¸…æ™°æ€§ã€‚</li>
<li>MNV-17åŒ…å«äº†æœ€å¹¿æ³›çš„éè¯­è¨€å£°éŸ³ç±»åˆ«é›†ï¼ŒåŒ…æ‹¬17ç§å¸¸è§ä¸”å¹³è¡¡è‰¯å¥½çš„ç±»åˆ«ã€‚</li>
<li>ä½œè€…ä½¿ç”¨å››ç§ä¸»æµASRæ¶æ„å¯¹MNV-17è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å…¶åœ¨è¯­ä¹‰è½¬å½•å’Œéè¯­è¨€å£°éŸ³åˆ†ç±»ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-802b07ab6617925f2f27c676b2559c53" align="middle">
<img src="https://picx.zhimg.com/v2-30404053beba98e799fc6122a606e120" align="middle">
<img src="https://picx.zhimg.com/v2-8c16404c48cdf16dbb6372a150e1070d" align="middle">
<img src="https://picx.zhimg.com/v2-c753b7c77e1c92ef7f7b1ae9e95415b2" align="middle">
<img src="https://picx.zhimg.com/v2-c491a060860ff9ebfb053d383fc3e5fd" align="middle">
<img src="https://picx.zhimg.com/v2-2d8eea7d1f9ec4655057264fc7c929e7" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LibriTTS-VI-A-Public-Corpus-and-Novel-Methods-for-Efficient-Voice-Impression-Control"><a href="#LibriTTS-VI-A-Public-Corpus-and-Novel-Methods-for-Efficient-Voice-Impression-Control" class="headerlink" title="LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice   Impression Control"></a>LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice   Impression Control</h2><p><strong>Authors:Junki Ohmura, Yuki Ito, Emiru Tsunoo, Toshiyuki Sekiya, Toshiyuki Kumakura</strong></p>
<p>Fine-grained control over voice impressions (e.g., making a voice brighter or calmer) is a key frontier for creating more controllable text-to-speech. However, this nascent field faces two key challenges. The first is the problem of impression leakage, where the synthesized voice is undesirably influenced by the speakerâ€™s reference audio, rather than the separately specified target impression, and the second is the lack of a public, annotated corpus. To mitigate impression leakage, we propose two methods: 1) a training strategy that separately uses an utterance for speaker identity and another utterance of the same speaker for target impression, and 2) a novel reference-free model that generates a speaker embedding solely from the target impression, achieving the benefits of improved robustness against the leakage and the convenience of reference-free generation. Objective and subjective evaluations demonstrate a significant improvement in controllability. Our best method reduced the mean squared error of 11-dimensional voice impression vectors from 0.61 to 0.41 objectively and from 1.15 to 0.92 subjectively, while maintaining high fidelity. To foster reproducible research, we introduce LibriTTS-VI, the first public voice impression dataset released with clear annotation standards, built upon the LibriTTS-R corpus. </p>
<blockquote>
<p>å¯¹è¯­éŸ³å°è±¡è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼ˆä¾‹å¦‚ï¼Œä½¿å£°éŸ³æ›´æ˜äº®æˆ–æ›´å¹³é™ï¼‰æ˜¯åˆ›å»ºæ›´å¤šå¯æ§æ–‡æœ¬åˆ°è¯­éŸ³çš„å…³é”®å‰æ²¿é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ–°å…´é¢†åŸŸé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å°è±¡æ³„éœ²çš„é—®é¢˜ï¼Œå³åˆæˆå£°éŸ³å—åˆ°è¯´è¯è€…çš„å‚è€ƒéŸ³é¢‘çš„ä¸å¿…è¦å½±å“ï¼Œè€Œä¸æ˜¯å•ç‹¬æŒ‡å®šçš„ç›®æ ‡å°è±¡ã€‚ç¬¬äºŒä¸ªæŒ‘æˆ˜æ˜¯ç¼ºä¹å…¬å…±æ³¨é‡Šè¯­æ–™åº“ã€‚ä¸ºäº†å‡è½»å°è±¡æ³„éœ²çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼š1) ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œå®ƒåˆ†åˆ«ä½¿ç”¨è¯´è¯äººèº«ä»½çš„è¯­å¥å’ŒåŒä¸€è¯´è¯äººçš„å¦ä¸€ä¸ªè¯­å¥ä½œä¸ºç›®æ ‡å°è±¡ï¼›2) ä¸€ç§æ–°å‹æ— å‚è€ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…ä»ç›®æ ‡å°è±¡ä¸­ç”Ÿæˆè¯´è¯äººåµŒå…¥ï¼Œå®ç°äº†å¯¹æŠ—æ³„éœ²çš„é²æ£’æ€§å’Œæ— å‚è€ƒç”Ÿæˆçš„ä¾¿åˆ©æ€§ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼Œå¯æ§æ€§æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„æœ€ä½³æ–¹æ³•å°†11ç»´è¯­éŸ³å°è±¡å‘é‡çš„å‡æ–¹è¯¯å·®ä»å®¢è§‚ä¸Šçš„0.61é™ä½åˆ°0.41ï¼Œä»ä¸»è§‚ä¸Šçš„1.15é™ä½åˆ°0.92ï¼ŒåŒæ—¶ä¿æŒäº†é«˜ä¿çœŸåº¦ã€‚ä¸ºäº†æ¨åŠ¨å¯é‡å¤çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åŸºäºLibriTTS-Rè¯­æ–™åº“ï¼Œä»‹ç»äº†å…·æœ‰æ¸…æ™°æ³¨é‡Šæ ‡å‡†çš„é¦–ä¸ªå…¬å…±è¯­éŸ³å°è±¡æ•°æ®é›†LibriTTS-VIã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15626v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong><br>è¯­éŸ³å°è±¡çš„ç²¾ç»†æ§åˆ¶ï¼ˆä¾‹å¦‚ï¼Œä½¿å£°éŸ³æ›´æ˜äº®æˆ–æ›´å¹³é™ï¼‰æ˜¯åˆ›å»ºæ›´å¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢çš„å…³é”®å‰æ²¿é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ–°å…´é¢†åŸŸé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯å°è±¡æ³„éœ²é—®é¢˜ï¼Œåˆæˆå£°éŸ³å—åˆ°è¯´è¯è€…çš„å‚è€ƒéŸ³é¢‘çš„ä¸å¿…è¦å½±å“ï¼Œè€Œéå•ç‹¬æŒ‡å®šçš„ç›®æ ‡å°è±¡ï¼›äºŒæ˜¯ç¼ºä¹å…¬å…±ã€æ³¨é‡Šçš„è¯­æ–™åº“ã€‚ä¸ºç¼“è§£å°è±¡æ³„éœ²é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŠæ³•ï¼šä¸€æ˜¯åˆ†åˆ«ä½¿ç”¨è¯´è¯è€…èº«ä»½å’Œç›®çš„å°è±¡çš„å‘éŸ³è®­ç»ƒç­–ç•¥ï¼›äºŒæ˜¯å…¨æ–°æ— éœ€å‚è€ƒæ¨¡å‹çš„è¯´è¯è€…åµŒå…¥ç”Ÿæˆæ–¹å¼ï¼Œä»…ä»ç›®æ ‡å°è±¡ä¸­ç”Ÿæˆã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°æ˜¾ç¤ºï¼Œæ§åˆ¶æ€§æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„æœ€ä½³æ–¹æ³•å°†11ç»´è¯­éŸ³å°è±¡å‘é‡çš„å‡æ–¹è¯¯å·®ä»å®¢è§‚ä¸Šçš„0.61é™è‡³0.41ï¼Œä¸»è§‚ä¸Šä»1.15é™è‡³0.92ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚ä¸ºæ”¯æŒå¯é‡å¤ç ”ç©¶ï¼Œæˆ‘ä»¬åŸºäºLibriTTS-Rè¯­æ–™åº“ï¼Œæ¨å‡ºäº†å¸¦æœ‰æ¸…æ™°æ³¨é‡Šæ ‡å‡†çš„é¦–ä¸ªå…¬å…±è¯­éŸ³å°è±¡æ•°æ®é›†LibriTTS-VIã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å°è±¡çš„ç²¾ç»†æ§åˆ¶æ˜¯æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢é¢†åŸŸçš„å…³é”®å‰æ²¿ã€‚</li>
<li>å°è±¡æ³„éœ²å’Œç¼ºä¹å…¬å…±æ•°æ®é›†æ˜¯è¯¥é¢†åŸŸçš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§è§£å†³å°è±¡æ³„éœ²é—®é¢˜çš„æ–¹æ³•ï¼šåˆ†åˆ«ä½¿ç”¨è¯´è¯è€…èº«ä»½å’Œç›®çš„å°è±¡çš„å‘éŸ³è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠå…¨æ–°çš„æ— éœ€å‚è€ƒæ¨¡å‹çš„è¯´è¯è€…åµŒå…¥ç”Ÿæˆæ–¹å¼ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ§åˆ¶æ€§å’Œè¯­éŸ³è´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æœ€ä½³æ–¹æ³•çš„å‡æ–¹è¯¯å·®æ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚</li>
<li>æ¨å‡ºäº†é¦–ä¸ªå…¬å…±è¯­éŸ³å°è±¡æ•°æ®é›†LibriTTS-VIï¼Œå¸¦æœ‰æ¸…æ™°æ³¨é‡Šæ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-814fc6ac3f0515bf09a505a16f78f9c1" align="middle">
<img src="https://picx.zhimg.com/v2-cfbd9b77765db8a8101fe91541a87bb6" align="middle">
<img src="https://picx.zhimg.com/v2-d1fa2813308d4443c49fe219eae3257b" align="middle">
<img src="https://picx.zhimg.com/v2-e65574200a3c5d883b1822ccaacec87a" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR"><a href="#Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR" class="headerlink" title="Frustratingly Easy Data Augmentation for Low-Resource ASR"></a>Frustratingly Easy Data Augmentation for Low-Resource ASR</h2><p><strong>Authors:Katsumi Ibaraki, David Chiang</strong></p>
<p>This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel textâ€“using gloss-based replacement, random replacement, or an LLM-based approachâ€“and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§é’ˆå¯¹ä½èµ„æºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è‡ªä¸»æ•°æ®å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æŠ€æœ¯é¦–å…ˆä½¿ç”¨åŸºäºæœ¯è¯­çš„æ›¿æ¢ã€éšæœºæ›¿æ¢æˆ–åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ç”Ÿæˆæ–°æ–‡æœ¬ï¼Œç„¶åå°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼ˆTTSï¼‰ä»¥ç”ŸæˆåˆæˆéŸ³é¢‘ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•ä»…åº”ç”¨äºåŸå§‹æ ‡æ³¨æ•°æ®ï¼Œåº”ç”¨äºå››ç§èµ„æºæä¸ºæœ‰é™çš„è¯­è¨€ï¼ˆç“¦ç‰¹é¾™æˆˆè¯­ã€çº³æ–¯å¡”è¯­ã€è°¢å†…å…‹èµ«æ©å¸ƒé‡Œäºšç‰¹è¯­å’Œå¡å¡è´è¯­ï¼‰ã€‚åœ¨åŸå§‹éŸ³é¢‘å’Œç”Ÿæˆçš„åˆæˆæ•°æ®ç»„åˆä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„Wav2Vec2-XLSR-53æ¨¡å‹ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­çº³æ–¯å¡”è¯­çš„ç»å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†14.3%ã€‚è¿™äº›æ–¹æ³•åœ¨å››ç§ä½èµ„æºè¯­è¨€ä¸­éƒ½è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¯¹é«˜èµ„æºè¯­è¨€å¦‚è‹±è¯­çš„å®ç”¨æ€§ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15373v2">PDF</a> 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§é’ˆå¯¹ä½èµ„æºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è‡ªæˆ‘å®Œå–„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•é€šè¿‡ç”Ÿæˆæ–°æ–‡æœ¬ï¼ˆä½¿ç”¨åŸºäºå…‰æ³½çš„æ›¿æ¢ã€éšæœºæ›¿æ¢æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•ï¼‰ï¼Œç„¶ååº”ç”¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ç”ŸæˆåˆæˆéŸ³é¢‘ã€‚å°†è¿™äº›ä»…åˆ©ç”¨åŸå§‹æ³¨é‡Šæ•°æ®çš„æ–¹æ³•åº”ç”¨äºå››ç§èµ„æºæåº¦æœ‰é™çš„è¯­è¨€ï¼ˆVatlongosã€Nashtaã€Shinekhen Buryatå’ŒKakabeï¼‰ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒçš„Wav2Vec2-XLSR-53æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç»“åˆåŸå§‹éŸ³é¢‘å’Œç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­Nashtaçš„ç»å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†14.3%ã€‚è¿™äº›æ–¹æ³•åœ¨å››ç§ä½èµ„æºè¯­è¨€ä¸­éƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜åœ¨é«˜èµ„æºè¯­è¨€å¦‚è‹±è¯­ä¸­ä¹Ÿæœ‰å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§è‡ªæˆ‘å®Œå–„çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¸“é—¨é’ˆå¯¹ä½èµ„æºçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬åŸºäºå…‰æ³½çš„æ›¿æ¢ã€éšæœºæ›¿æ¢æˆ–ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ–°æ–‡æœ¬ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ï¼Œå°†ç”Ÿæˆçš„æ–‡æœ¬è½¬åŒ–ä¸ºåˆæˆéŸ³é¢‘ã€‚</li>
<li>åœ¨å››ç§èµ„æºæœ‰é™çš„è¯­è¨€ä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶ç»“åˆäº†åŸå§‹éŸ³é¢‘å’Œåˆæˆæ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå…¶ä¸­Nashtaè¯­è¨€çš„è¯é”™è¯¯ç‡é™ä½äº†14.3%ã€‚</li>
<li>è¿™äº›æ–¹æ³•ä¸ä»…é€‚ç”¨äºä½èµ„æºè¯­è¨€ï¼Œä¹Ÿå¯¹é«˜èµ„æºè¯­è¨€å¦‚è‹±è¯­æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e607b8f83468f4baea55d969f7d47e58" align="middle">
<img src="https://picx.zhimg.com/v2-d7d690eb562288305439ac1d0e6510cb" align="middle">
<img src="https://picx.zhimg.com/v2-67fab1ec14f6d32327c4039325b0d830" align="middle">
<img src="https://picx.zhimg.com/v2-450979640f185d77be41b3fea5f22364" align="middle">
<img src="https://picx.zhimg.com/v2-0030000340f7ffe547f2b27343c03b7a" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Speech-Language-Models-for-Under-Represented-Languages-Insights-from-Wolof"><a href="#Speech-Language-Models-for-Under-Represented-Languages-Insights-from-Wolof" class="headerlink" title="Speech Language Models for Under-Represented Languages: Insights from   Wolof"></a>Speech Language Models for Under-Represented Languages: Insights from   Wolof</h2><p><strong>Authors:Yaya Sy, Dioula DoucourÃ©, Christophe Cerisara, Irina Illina</strong></p>
<p>We present our journey in training a speech language model for Wolof, an underrepresented language spoken in West Africa, and share key insights. We first emphasize the importance of collecting large-scale, spontaneous, high-quality unsupervised speech data, and show that continued pretraining HuBERT on this dataset outperforms both the base model and African-centric models on ASR. We then integrate this speech encoder into a Wolof LLM to train the first Speech LLM for this language, extending its capabilities to tasks such as speech translation. Furthermore, we explore training the Speech LLM to perform multi-step Chain-of-Thought before transcribing or translating. Our results show that the Speech LLM not only improves speech recognition but also performs well in speech translation. The models and the code will be openly shared. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åœ¨è¥¿éè¯­å°‘æ•°æ°‘æ—è¯­è¨€ä¹‹ä¸€çš„æ²ƒæ´›å¤«è¯­ä¸­è®­ç»ƒè¯­éŸ³è¯­è¨€æ¨¡å‹çš„æ—…ç¨‹ï¼Œå¹¶åˆ†äº«äº†å…³é”®è§è§£ã€‚æˆ‘ä»¬é¦–å…ˆå¼ºè°ƒæ”¶é›†å¤§è§„æ¨¡ã€è‡ªå‘æ€§çš„é«˜è´¨é‡æ— ç›‘ç£è¯­éŸ³æ•°æ®çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨æŒç»­ä½¿ç”¨è¯¥æ•°æ®é›†å¯¹HuBERTè¿›è¡Œé¢„è®­ç»ƒçš„æ•ˆæœï¼Œå…¶åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¶…è¶Šäº†åŸºç¡€æ¨¡å‹å’Œé¢å‘éæ´²çš„æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ­¤è¯­éŸ³ç¼–ç å™¨é›†æˆåˆ°æ²ƒæ´›å¤«è¯­LLMä¸­ï¼Œä»¥è®­ç»ƒè¯¥è¯­è¨€çš„ç¬¬ä¸€ä¸ªè¯­éŸ³LLMï¼Œå¹¶å°†å…¶åŠŸèƒ½æ‰©å±•åˆ°è¯­éŸ³ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢è®­ç»ƒè¯­éŸ³LLMæ‰§è¡Œå¤šæ­¥éª¤çš„Chain-of-Thoughtåè¿›è¡Œè½¬å½•æˆ–ç¿»è¯‘ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯­éŸ³LLMä¸ä»…æé«˜äº†è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œè€Œä¸”åœ¨è¯­éŸ³ç¿»è¯‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚æ¨¡å‹å’Œä»£ç å°†å…¬å¼€å…±äº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15362v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡ä»‹ç»äº†å¯¹è¥¿éåœ°åŒºçš„æ²ƒå°”å¤«è¯­è¨€è¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ï¼Œå¹¶åˆ†äº«äº†å…³é”®è§è§£ã€‚æ–‡ç« å¼ºè°ƒæ”¶é›†å¤§è§„æ¨¡ã€è‡ªå‘ã€é«˜è´¨é‡çš„æ— ç›‘ç£è¯­éŸ³æ•°æ®çš„é‡è¦æ€§ï¼Œå±•ç¤ºåœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­è¿›è¡ŒHuBERTé¢„è®­ç»ƒçš„ä¼˜åŠ¿ã€‚æ¥ç€ï¼Œå°†è¯­éŸ³ç¼–ç å™¨é›†æˆåˆ°æ²ƒå°”å¤«è¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ‰©å±•å…¶è¯­éŸ³ç¿»è¯‘ç­‰åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œæ¢ç´¢äº†è®©è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ­¥éª¤Chain-of-Thoughtè®­ç»ƒï¼Œä»¥æé«˜å…¶è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ€§èƒ½ã€‚æ¨¡å‹åŠä»£ç å°†å…¬å¼€å…±äº«ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ”¶é›†å¤§è§„æ¨¡ã€è‡ªå‘ã€é«˜è´¨é‡çš„æ— ç›‘ç£è¯­éŸ³æ•°æ®å¯¹äºè®­ç»ƒè¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>ç»§ç»­åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡ŒHuBERTé¢„è®­ç»ƒï¼Œå¯æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ï¼Œè¶…è¶ŠåŸºç¡€æ¨¡å‹å’Œéæ´²ä¸­å¿ƒæ¨¡å‹ã€‚</li>
<li>å°†è¯­éŸ³ç¼–ç å™¨é›†æˆåˆ°æ²ƒå°”å¤«çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå®ç°äº†è¯¥è¯­è¨€çš„é¦–ä¸ªè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œæ‰©å±•äº†å…¶è¯­éŸ³ç¿»è¯‘ç­‰åŠŸèƒ½ã€‚</li>
<li>è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ­¥éª¤çš„Chain-of-Thoughtï¼Œæå‡äº†è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘çš„æ€§èƒ½ã€‚</li>
<li>è¿™ç§è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸ä»…æé«˜äº†è¯­éŸ³è¯†åˆ«ç‡ï¼Œè€Œä¸”åœ¨è¯­éŸ³ç¿»è¯‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹å’Œä»£ç å°†å…¬å¼€å…±äº«ï¼Œæœ‰åŠ©äºä¿ƒè¿›å¯¹è¯¥è¯­è¨€å’Œå…¶å®ƒéæ´²è¯­è¨€çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æ­¤é¡¹ç›®å±•ç¤ºäº†åœ¨å¯¹å¾…ä»£è¡¨æ€§è¾ƒä½çš„è¯­è¨€ï¼ˆå¦‚æ²ƒå°”å¤«è¯­ï¼‰æ—¶çš„æŠ€æœ¯è¿›æ­¥å’Œé‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15362">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4553fab167a987cabe24e0781fe4d4ea" align="middle">
<img src="https://picx.zhimg.com/v2-9794d7ac2d5645ddd34dee4bd011044e" align="middle">
<img src="https://picx.zhimg.com/v2-e048eae52e8ad2de34a402f0da130681" align="middle">
<img src="https://picx.zhimg.com/v2-b633957c47006d97870589938ce110df" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Canary-1B-v2-Parakeet-TDT-0-6B-v3-Efficient-and-High-Performance-Models-for-Multilingual-ASR-and-AST"><a href="#Canary-1B-v2-Parakeet-TDT-0-6B-v3-Efficient-and-High-Performance-Models-for-Multilingual-ASR-and-AST" class="headerlink" title="Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST"></a>Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST</h2><p><strong>Authors:Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nikolay Karpov, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>This report introduces Canary-1B-v2, a fast, robust multilingual model for Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built with a FastConformer encoder and Transformer decoder, it supports 25 languages primarily European. The model was trained on 1.7M hours of total data samples, including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce hallucinations for ASR and AST. We describe its two-stage pre-training and fine-tuning process with dynamic data balancing, as well as experiments with an nGPT encoder. Results show nGPT scales well with massive data, while FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2 outperforms Whisper-large-v3 on English ASR while being 10x faster, and delivers competitive multilingual ASR and AST performance against larger models like Seamless-M4T-v2-large and LLM-based systems. We also release Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the same 25 languages with just 600M parameters. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šä»‹ç»äº†Canary-1B-v2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿã€ç¨³å¥çš„å¤šè¯­è¨€æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘ï¼ˆASTï¼‰ã€‚è¯¥æ¨¡å‹é‡‡ç”¨FastConformerç¼–ç å™¨å’ŒTransformerè§£ç å™¨æ„å»ºï¼Œä¸»è¦æ”¯æŒ25ç§æ¬§æ´²è¯­è¨€ã€‚è¯¥æ¨¡å‹åœ¨170ä¸‡å°æ—¶çš„æ•°æ®æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬Granaryå’ŒNeMo ASR Set 3.0ï¼Œå¹¶æ·»åŠ äº†éè¯­éŸ³éŸ³é¢‘ï¼Œä»¥å‡å°‘ASRå’ŒASTçš„å¹»è§‰ã€‚æˆ‘ä»¬æè¿°äº†å…¶ä¸¤é˜¶æ®µé¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨åŠ¨æ€æ•°æ®å¹³è¡¡ï¼Œä»¥åŠä½¿ç”¨nGPTç¼–ç å™¨çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒnGPTåœ¨å¤§é‡æ•°æ®ä¸­è¡¨ç°è‰¯å¥½ï¼Œè€ŒFastConformeråœ¨å¾®è°ƒåè¡¨ç°å‡ºè‰²ã€‚å¯¹äºæ—¶é—´æˆ³ï¼ŒCanary-1B-v2ä½¿ç”¨NeMoå¼ºåˆ¶å¯¹é½å™¨ï¼ˆNFAï¼‰å’Œè¾…åŠ©CTCæ¨¡å‹ï¼Œä¸ºASRå’ŒASTæä¾›å¯é çš„æ—¶é—´æˆ³ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCanary-1B-v2åœ¨è‹±è¯­ASRæ–¹é¢çš„è¡¨ç°ä¼˜äºWhisper-large-v3ï¼Œè€Œä¸”é€Ÿåº¦æ›´å¿«ï¼ˆå¿«10å€ï¼‰ï¼Œå¹¶åœ¨å¤šè¯­è¨€ASRå’ŒASTæ–¹é¢ä¸æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚æ— ç¼M4T-v2å¤§å‹æ¨¡å‹å’ŒåŸºäºLLMçš„ç³»ç»Ÿï¼‰è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†Parakeet-TDT-0.6B-v3ï¼Œå®ƒæ˜¯v2çš„ç»§ä»»è€…ï¼Œæä¾›ç›¸åŒçš„25ç§è¯­è¨€çš„å¤šè¯­è¨€ASRï¼Œä»…ä½¿ç”¨6äº¿ä¸ªå‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14128v2">PDF</a> Mini Version of it Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>è¿™ä»½æŠ¥å‘Šä»‹ç»äº†Canary-1B-v2æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿã€ç¨³å¥çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ï¼ˆASTï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨FastConformerç¼–ç å™¨å’ŒTransformerè§£ç å™¨ï¼Œæ”¯æŒ25ç§ä¸»è¦æ¬§æ´²è¯­è¨€ã€‚æ¨¡å‹åœ¨170ä¸‡å°æ—¶çš„æ•°æ®æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬Granaryå’ŒNeMo ASR Set 3.0ï¼Œå¹¶æ·»åŠ äº†éè¯­éŸ³éŸ³é¢‘ä»¥å‡å°‘ASRå’ŒASTçš„å¹»è§‰ã€‚æè¿°äº†å…¶ä¸¤é˜¶æ®µé¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä»¥åŠåŠ¨æ€æ•°æ®å¹³è¡¡æŠ€æœ¯ï¼Œå¹¶ä¸nGPTç¼–ç å™¨çš„å®éªŒè¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFastConformeråœ¨å¾®è°ƒåè¡¨ç°å‡ºè‰²ï¼Œè€ŒnGPTåœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚Canary-1B-v2ä½¿ç”¨NeMoå¼ºåˆ¶å¯¹é½å™¨ï¼ˆNFAï¼‰æä¾›å¯é çš„åˆ†æ®µçº§æ—¶é—´æˆ³ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒCanary-1B-v2åœ¨è‹±è¯­ASRä¸Šçš„è¡¨ç°ä¼˜äºWhisper-large-v3ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”åœ¨å¤šè¯­è¨€ASRå’ŒASTæ€§èƒ½ä¸Šä¸å…¶ä»–å¤§å‹æ¨¡å‹å¦‚æ— ç¼M4T-v2å¤§å‹å’ŒåŸºäºLLMçš„ç³»ç»Ÿå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†Parakeet-TDT-0.6B-v3ç‰ˆæœ¬ï¼Œå…·æœ‰ç›¸åŒçš„æ”¯æŒè¯­è¨€æ•°ï¼Œä½†å‚æ•°æ›´å°‘ï¼Œåªæœ‰6äº¿ä¸ªå‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Canary-1B-v2æ˜¯ä¸€ä¸ªå¿«é€Ÿã€ç¨³å¥çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ï¼ˆASTï¼‰æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨FastConformerç¼–ç å™¨å’ŒTransformerè§£ç å™¨ç»“æ„ã€‚</li>
<li>æ”¯æŒåŒ…æ‹¬æ¬§æ´²åœ¨å†…çš„25ç§è¯­è¨€ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒåœ¨å¤§é‡æ•°æ®æ ·æœ¬ä¸Šè¿›è¡Œï¼ŒåŒ…å«éè¯­éŸ³éŸ³é¢‘ä»¥ä¼˜åŒ–ASRå’ŒASTæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µé¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹åŠåŠ¨æ€æ•°æ®å¹³è¡¡æŠ€æœ¯å®ç°é«˜æ€§èƒ½ã€‚FastConformeråœ¨å¾®è°ƒåè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚åŒæ—¶éªŒè¯äº†nGPTç¼–ç å™¨åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶çš„ä¼˜åŠ¿ã€‚</li>
<li>Canary-1B-v2ä½¿ç”¨NeMoå¼ºåˆ¶å¯¹é½å™¨æä¾›å¯é çš„æ—¶é—´æˆ³ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08cc1ec1e9bba36d7be01e72fad1983e" align="middle">
<img src="https://picx.zhimg.com/v2-a55d162a0e222c0435419da41bb6e634" align="middle">
<img src="https://picx.zhimg.com/v2-4a426e045c0bce6216512b8543879efc" align="middle">
<img src="https://picx.zhimg.com/v2-a054d9455c0cbd0843a81d256ca4f058" align="middle">
<img src="https://picx.zhimg.com/v2-fb8c42524e1678a0add2ff471d285f34" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Robust-Audio-Visual-Target-Speaker-Extraction-with-Emotion-Aware-Multiple-Enrollment-Fusion"><a href="#Robust-Audio-Visual-Target-Speaker-Extraction-with-Emotion-Aware-Multiple-Enrollment-Fusion" class="headerlink" title="Robust Audio-Visual Target Speaker Extraction with Emotion-Aware   Multiple Enrollment Fusion"></a>Robust Audio-Visual Target Speaker Extraction with Emotion-Aware   Multiple Enrollment Fusion</h2><p><strong>Authors:Zhan Jin, Bang Zeng, Peijun Yang, Jiarong Du, Juan Liu, Ming Li</strong></p>
<p>Target Speaker Extraction (TSE) is a critical challenge in cocktail party scenarios. While leveraging multiple modalities, such as voice, lip, face, and expression embeddings, can enhance performance, real-world applications often suffer from intermittent modality dropout. This paper presents a comprehensive study on the interactions and robustness of various multimodal fusion strategies under varying degrees of modality dropout. We build upon a state-of-the-art audio-visual speech enhancement system and integrate four distinct speaker identity cues: lip embeddings for synchronized contextual information, a voice speaker embedding extracted via cross-attention for acoustic consistency, a static face embedding for speaker identity, and a novel dynamic expression embedding for frame-wise emotional features. We systematically evaluate different combinations of these modalities under two key training regimes: zero dropout and 80% modality dropout. Extensive experiments demonstrate that while a full multimodal ensemble achieves optimal performance under ideal (zero dropout) conditions, its effectiveness diminishes significantly when test-time dropout occurs without prior exposure during training. Crucially, we show that training with a high (80%) modality dropout rate dramatically enhances model robustness, enabling the system to maintain superior performance even under severe test-time missing modalities. Our findings highlight that voice embeddings exhibit consistent robustness, while the proposed expression embedding provides valuable complementary information. This work underscores the importance of training strategies that account for real-world imperfection, moving beyond pure performance maximization to achieve practical reliability in multimodal speech enhancement systems. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰æ˜¯é¸¡å°¾é…’ä¼šåœºæ™¯ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚è™½ç„¶åˆ©ç”¨å¤šç§æ¨¡æ€ï¼ˆå¦‚è¯­éŸ³ã€å˜´å”‡ã€é¢éƒ¨å’Œè¡¨æƒ…åµŒå…¥ï¼‰å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å®é™…åº”ç”¨ä¸­ç»å¸¸å—åˆ°é—´æ­‡æ€§æ¨¡æ€ä¸­æ–­çš„å½±å“ã€‚æœ¬æ–‡å¯¹ä¸åŒæ¨¡æ€èåˆç­–ç•¥åœ¨ä¸åŒæ¨¡æ€ä¸­æ–­ç¨‹åº¦ä¸‹çš„äº¤äº’å’Œç¨³å¥æ€§è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚æˆ‘ä»¬åŸºäºå…ˆè¿›çš„è§†å¬è¯­éŸ³å¢å¼ºç³»ç»Ÿï¼Œé›†æˆäº†å››ç§ä¸åŒçš„è¯´è¯äººèº«ä»½çº¿ç´¢ï¼šç”¨äºåŒæ­¥ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å˜´å”‡åµŒå…¥ã€é€šè¿‡äº¤å‰æ³¨æ„åŠ›æå–çš„è¯­éŸ³è¯´è¯äººåµŒå…¥ä»¥ç”¨äºå£°éŸ³ä¸€è‡´æ€§ã€ç”¨äºè¯´è¯äººèº«ä»½çš„é™æ€é¢éƒ¨åµŒå…¥ã€ä»¥åŠç”¨äºå¸§çº§æƒ…æ„Ÿç‰¹å¾çš„æ–°å‹åŠ¨æ€è¡¨æƒ…åµŒå…¥ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†è¿™ä¸¤ç§å…³é”®è®­ç»ƒåˆ¶åº¦ä¸‹çš„ä¸åŒæ¨¡æ€ç»„åˆï¼šé›¶ä¸­æ–­å’Œ80%çš„æ¨¡æ€ä¸­æ–­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè™½ç„¶åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼ˆé›¶ä¸­æ–­ï¼‰ï¼Œå…¨æ¨¡æ€é›†åˆè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä½†å½“æµ‹è¯•æ—¶å‘ç”Ÿä¸­æ–­ä¸”è®­ç»ƒæœŸé—´æœªæš´éœ²äºä¸­æ–­æ—¶ï¼Œå…¶æœ‰æ•ˆæ€§ä¼šæ˜¾è‘—é™ä½ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä»¥é«˜ï¼ˆ80%ï¼‰çš„æ¨¡æ€ä¸­æ–­ç‡è¿›è¡Œè®­ç»ƒå¯ä»¥æå¤§åœ°æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä½¿ç³»ç»Ÿåœ¨æµ‹è¯•æ—¶å³ä½¿åœ¨ä¸¥é‡ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯­éŸ³åµŒå…¥è¡¨ç°å‡ºæŒç»­çš„ç¨³å¥æ€§ï¼Œè€Œæ‰€æå‡ºçš„è¡¨æƒ…åµŒå…¥æä¾›äº†æœ‰ä»·å€¼çš„è¡¥å……ä¿¡æ¯ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†è®­ç»ƒç­–ç•¥çš„é‡è¦æ€§ï¼Œè¿™äº›ç­–ç•¥è¦è€ƒè™‘åˆ°ç°å®ä¸–ç•Œçš„ç¼ºé™·ï¼Œè¶…è¶Šå•çº¯çš„æ€§èƒ½æœ€å¤§åŒ–ï¼Œä»¥å®ç°å¤šæ¨¡æ€è¯­éŸ³å¢å¼ºç³»ç»Ÿçš„å®é™…å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12583v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åœ¨ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ–¹é¢ï¼Œæœ¬ç ”ç©¶å¯¹ç°å®åœºæ™¯ä¸­çš„å¤šå˜è¡¨ç°æå‡ºäº†æ·±åº¦åˆ†æã€‚åˆ©ç”¨å¤šé‡æ¨¡å¼ï¼Œä¾‹å¦‚å£°éŸ³ã€å˜´å”‡ã€è„¸éƒ¨ä»¥åŠè¡¨æƒ…åµŒå…¥å¯ä»¥å¢å¼ºç³»ç»Ÿæ€§èƒ½ã€‚æœ¬ç ”ç©¶ç»¼åˆåˆ†æäº†åœ¨ä¸åŒæ¨¡æ€è„±è½æ¡ä»¶ä¸‹ï¼Œå¤šç§æ¨¡æ€èåˆç­–ç•¥çš„äº’åŠ¨åŠç¨³å¥æ€§ã€‚åŸºäºå¯¹é¡¶å°–è§†å¬è¯­éŸ³å¢å¼ºç³»ç»Ÿçš„ç ”ç©¶ï¼Œæœ¬ç ”ç©¶èå…¥äº†å››ä¸ªä¸»è¦çš„è¯´è¯äººèº«ä»½çº¿ç´¢ï¼ŒåŒ…æ‹¬åŒæ­¥è¯­å¢ƒä¿¡æ¯çš„å˜´å”‡åµŒå…¥ã€ç”¨äºå£°éŸ³ä¸€è‡´æ€§çš„è·¨æ³¨æ„åŠ›è¯­éŸ³åµŒå…¥ã€ç”¨äºèº«ä»½è¯†åˆ«çš„é™æ€é¢éƒ¨åµŒå…¥ä»¥åŠç”¨äºæƒ…ç»ªç‰¹å¾çš„æ–°åŠ¨æ€è¡¨æƒ…åµŒå…¥ã€‚å®éªŒç³»ç»Ÿåœ°è¯„ä¼°äº†åœ¨ä¸¤ç§ä¸»è¦è®­ç»ƒæ–¹æ¡ˆä¸‹ä¸åŒæ¨¡æ€çš„ç»„åˆï¼šé›¶è„±è½ç‡å’Œç™¾åˆ†ä¹‹å…«åçš„æ¨¡æ€è„±è½ç‡ã€‚å¤§é‡å®éªŒæ˜¾ç¤ºï¼Œå…¨æ¨¡æ€é›†åˆåœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼ˆé›¶è„±è½ç‡ï¼‰è¡¨ç°æœ€ä¼˜ï¼Œä½†åœ¨æµ‹è¯•æ—¶è‹¥å‘ç”Ÿæœªå—è®­ç»ƒçš„æ¨¡æ€è„±è½ç°è±¡ï¼Œå…¶æœ‰æ•ˆæ€§ä¼šæ˜¾è‘—ä¸‹é™ã€‚å…³é”®çš„æ˜¯ï¼Œè®­ç»ƒæ—¶é‡‡ç”¨é«˜æ¨¡æ€è„±è½ç‡ï¼ˆç™¾åˆ†ä¹‹å…«åï¼‰å¯æå¤§æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä½¿ç³»ç»Ÿåœ¨æµ‹è¯•æ—¶å³ä½¿é¢ä¸´ä¸¥é‡ç¼ºå¤±æ¨¡æ€ä¹Ÿèƒ½ä¿æŒå“è¶Šæ€§èƒ½ã€‚æœ¬ç ”ç©¶å‘ç°è¯­éŸ³åµŒå…¥å…·æœ‰ä¸€è´¯çš„ç¨³å¥æ€§ï¼Œè€Œæå‡ºçš„è¡¨ç°åµŒå…¥æä¾›äº†å®è´µçš„è¾…åŠ©ä¿¡æ¯ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒåœ¨è®¾è®¡å®ç”¨å¯é çš„æ¨¡æ€è¯­éŸ³å¢å¼ºç³»ç»Ÿæ—¶ï¼Œéœ€è¶…è¶Šçº¯ç²¹çš„æ€§èƒ½æœ€å¤§åŒ–è€ƒè™‘ç°å®ä¸–ç•Œçš„ä¸å®Œå–„ä¹‹å¤„å¹¶åˆ¶å®šæœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰åœ¨é¸¡å°¾é…’ä¼šåœºæ™¯ä¸­æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡å¼èåˆï¼ˆå¦‚å£°éŸ³ã€å˜´å”‡ã€è„¸éƒ¨å’Œè¡¨æƒ…åµŒå…¥ï¼‰èƒ½æé«˜ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸åŒæ¨¡æ€ç»„åˆåœ¨ä¸åŒè®­ç»ƒç¯å¢ƒä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶è„±è½å’Œç™¾åˆ†ä¹‹å…«åçš„æ¨¡æ€è„±è½ã€‚</li>
<li>åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œå…¨æ¨¡æ€é›†åˆè¡¨ç°æœ€ä¼˜ï¼Œä½†åœ¨æµ‹è¯•æ—¶æ¨¡æ€è„±è½ä¼šé™ä½å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>é«˜æ¨¡æ€è„±è½ç‡è®­ç»ƒæé«˜æ¨¡å‹ç¨³å¥æ€§ï¼Œä½¿ç³»ç»Ÿåœ¨é¢å¯¹ä¸¥é‡ç¼ºå¤±æ¨¡æ€æ—¶ä»èƒ½ä¿æŒæ€§èƒ½ã€‚</li>
<li>è¯­éŸ³åµŒå…¥å±•ç°å‡ºç¨³å¥æ€§ï¼Œè€Œè¡¨æƒ…åµŒå…¥æä¾›äº†æœ‰ä»·å€¼çš„è¾…åŠ©ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d64a66da91317a6c60ea56b7e18fc608" align="middle">
<img src="https://picx.zhimg.com/v2-b81728f6289ea28eb854536c95609770" align="middle">
<img src="https://picx.zhimg.com/v2-219c9bbe718f9a1fd6cb35d3e3f70f71" align="middle">
<img src="https://picx.zhimg.com/v2-a05bd60ea81876a6b3a49cfd922ae42e" align="middle">
<img src="https://picx.zhimg.com/v2-74318f9f2a37da7c03f1a8da8c8ffb0d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild"><a href="#Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild" class="headerlink" title="Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild"></a>Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild</h2><p><strong>Authors:Jing-Tong Tzeng, Bo-Hao Su, Ya-Tse Wu, Hsing-Hang Chou, Chi-Chun Lee</strong></p>
<p>In this study, we revisit key training strategies in machine learning often overlooked in favor of deeper architectures. Specifically, we explore balancing strategies, activation functions, and fine-tuning techniques to enhance speech emotion recognition (SER) in naturalistic conditions. Our findings show that simple modifications improve generalization with minimal architectural changes. Our multi-modal fusion model, integrating these optimizations, achieves a valence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute Regression. Notably, fine-tuning RoBERTa and WavLM separately in a single-modality setting, followed by feature fusion without training the backbone extractor, yields the highest valence performance. Additionally, focal loss and activation functions significantly enhance performance without increasing complexity. These results suggest that refining core components, rather than deepening models, leads to more robust SER in-the-wild. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®è®­ç»ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥é€šå¸¸è¢«æ›´æ·±çš„æ¶æ„æ‰€å¿½è§†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¹³è¡¡ç­–ç•¥ã€æ¿€æ´»å‡½æ•°å’Œå¾®è°ƒæŠ€æœ¯ï¼Œä»¥æé«˜è‡ªç„¶æ¡ä»¶ä¸‹çš„è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç®€å•çš„ä¿®æ”¹å¯ä»¥åœ¨æå°çš„æ¶æ„å˜åŠ¨ä¸‹æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬èåˆå¤šç§æ¨¡å¼çš„å¤šæ¨¡æ€èåˆæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆè¿™äº›ä¼˜åŒ–æªæ–½ï¼Œåœ¨ä»»åŠ¡2ï¼šæƒ…æ„Ÿå±æ€§å›å½’ä¸­å–å¾—äº†æœ€é«˜ä»·æ€è¯„åˆ†ï¼ˆCCCå€¼ä¸º0.6953ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å•æ¨¡æ€ç¯å¢ƒä¸­åˆ†åˆ«å¯¹RoBERTaå’ŒWavLMè¿›è¡Œå¾®è°ƒï¼Œç„¶åè¿›è¡Œç‰¹å¾èåˆè€Œä¸è®­ç»ƒéª¨æ¶æå–å™¨ï¼Œå¯ä»¥è·å¾—æœ€é«˜çš„ä»·æ€æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç„¦ç‚¹æŸå¤±å’Œæ¿€æ´»å‡½æ•°åœ¨ä¸å¢åŠ å¤æ‚æ€§çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–æ ¸å¿ƒç»„ä»¶è€Œä¸æ˜¯æ·±åŒ–æ¨¡å‹ï¼Œå¯ä»¥åœ¨é‡å¤–å®ç°æ›´ç¨³å¥çš„SERã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07282v2">PDF</a> Proceedings of Interspeech 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡æ–°æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®è®­ç»ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥é€šå¸¸è¢«æ›´æ·±çš„æ¶æ„æ‰€å¿½è§†ã€‚ç ”ç©¶æ¢ç´¢äº†å¹³è¡¡ç­–ç•¥ã€æ¿€æ´»å‡½æ•°å’Œå¾®è°ƒæŠ€æœ¯ï¼Œä»¥æ”¹å–„è‡ªç„¶æ¡ä»¶ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•çš„ä¿®æ”¹å¯ä»¥æ”¹è¿›æ³›åŒ–èƒ½åŠ›ï¼Œä¸”åªéœ€è¿›è¡Œå°‘é‡çš„æ¶æ„æ›´æ”¹ã€‚å¤šæ¨¡æ€èåˆæ¨¡å‹æ•´åˆè¿™äº›ä¼˜åŒ–åï¼Œåœ¨ä»»åŠ¡2ï¼šæƒ…æ„Ÿå±æ€§å›å½’ä¸­å–å¾—äº†0.6953çš„æ•ˆä»·CCCï¼Œä¸ºæœ€ä½³æ•ˆä»·å¾—åˆ†ã€‚ç‰¹åˆ«æ˜¯ï¼Œåˆ†åˆ«åœ¨å•æ¨¡æ€è®¾ç½®ä¸­å¾®è°ƒRoBERTaå’ŒWavLMï¼Œç„¶åè¿›è¡Œç‰¹å¾èåˆè€Œä¸è®­ç»ƒä¸»å¹²æå–å™¨ï¼Œè·å¾—äº†æœ€é«˜çš„æ•ˆä»·æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç„¦ç‚¹æŸå¤±å’Œæ¿€æ´»å‡½æ•°æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä¸”ä¸å¢åŠ å¤æ‚æ€§ã€‚ç»“æœæš—ç¤ºï¼Œæ”¹è¿›æ ¸å¿ƒç»„ä»¶ï¼Œè€Œéæ·±åŒ–æ¨¡å‹ï¼Œæ˜¯å®ç°æ›´ç¨³å¥çš„é‡ç”ŸSERçš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡æ–°è€ƒè™‘äº†å¸¸è¢«å¿½è§†çš„è®­ç»ƒç­–ç•¥ï¼Œå¦‚å¹³è¡¡ç­–ç•¥ã€æ¿€æ´»å‡½æ•°å’Œå¾®è°ƒæŠ€æœ¯ã€‚</li>
<li>è¿™äº›ç­–ç•¥è¢«åº”ç”¨äºæ”¹å–„è‡ªç„¶æ¡ä»¶ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚</li>
<li>ç®€å•çš„ä¿®æ”¹å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”ä¸éœ€è¦å¤æ‚çš„æ¶æ„å˜åŠ¨ã€‚</li>
<li>å¤šæ¨¡æ€èåˆæ¨¡å‹å–å¾—æœ€ä½³æ•ˆä»·å¾—åˆ†ï¼Œåœ¨æƒ…æ„Ÿå±æ€§å›å½’ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å•ç‹¬å¾®è°ƒRoBERTaå’ŒWavLMï¼Œå†ç»“åˆç‰¹å¾èåˆï¼Œè·å¾—æœ€é«˜æ•ˆä»·æ€§èƒ½ã€‚</li>
<li>ç„¦ç‚¹æŸå¤±å’Œæ¿€æ´»å‡½æ•°èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¸å¢åŠ æ¨¡å‹å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9d9547d1d3c0f78249a7bdb101c5c3c" align="middle">
<img src="https://picx.zhimg.com/v2-86e564dad93eb7b1138d7a8641febf43" align="middle">
<img src="https://picx.zhimg.com/v2-a927b730c9a35b7019ff790fe690516d" align="middle">
<img src="https://picx.zhimg.com/v2-70228e842e6e19fc866c6abc4e4957b3" align="middle">
<img src="https://picx.zhimg.com/v2-8cc973384be02b8e338013d227cb13c8" align="middle">
<img src="https://picx.zhimg.com/v2-1b1b3cfecfb0cefa49c8825dd18834d1" align="middle">
<img src="https://picx.zhimg.com/v2-67cd18716a8f1ff65079856276f577c0" align="middle">
<img src="https://picx.zhimg.com/v2-be8ff450033c920c7bf8197a907aa424" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Interpretable-Embeddings-of-Speech-Enhance-and-Explain-Brain-Encoding-Performance-of-Audio-Models"><a href="#Interpretable-Embeddings-of-Speech-Enhance-and-Explain-Brain-Encoding-Performance-of-Audio-Models" class="headerlink" title="Interpretable Embeddings of Speech Enhance and Explain Brain Encoding   Performance of Audio Models"></a>Interpretable Embeddings of Speech Enhance and Explain Brain Encoding   Performance of Audio Models</h2><p><strong>Authors:Riki Shimizu, Richard J. Antonello, Chandan Singh, Nima Mesgarani</strong></p>
<p>Speech foundation models (SFMs) are increasingly hailed as powerful computational models of human speech perception. However, since their representations are inherently black-box, it remains unclear what drives their alignment with brain responses. To remedy this, we built linear encoding models from six interpretable feature families: mel-spectrogram, Gabor filter bank features, speech presence, phonetic, syntactic, and semantic features, and contextualized embeddings from three state-of-the-art SFMs (Whisper, HuBERT, WavLM), quantifying electrocorticography (ECoG) response variance shared between feature classes. Variance-partitioning analyses revealed several key insights: First, the SFMsâ€™ alignment with the brain can be mostly explained by their ability to learn and encode simple interpretable speech features. Second, SFMs exhibit a systematic trade-off between encoding of brain-relevant low-level and high-level features across layers. Finally, our results show that SFMs learn brain-relevant semantics which cannot be explained by lower-level speech features, with this capacity increasing with model size and context length. Together, our findings suggest a principled approach to build more interpretable, accurate, and efficient encoding models of the brain by augmenting SFM embeddings with interpretable features. </p>
<blockquote>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°èª‰ä¸ºäººç±»è¯­éŸ³æ„ŸçŸ¥çš„å¼ºå¤§è®¡ç®—æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå®ƒä»¬çš„è¡¨ç¤ºæœ¬è´¨ä¸Šæ˜¯é»‘ç®±ï¼Œå°šä¸æ¸…æ¥šæ˜¯ä»€ä¹ˆé©±åŠ¨å®ƒä»¬ä¸å¤§è„‘ååº”çš„åŒ¹é…ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬ä»å…­ä¸ªå¯è§£é‡Šçš„ç‰¹å¾å®¶æ—ä¸­æ„å»ºäº†çº¿æ€§ç¼–ç æ¨¡å‹ï¼šæ¢…å°”é¢‘è°±å›¾ã€Gaboræ»¤æ³¢å™¨ç»„ç‰¹å¾ã€è¯­éŸ³å­˜åœ¨ç‰¹å¾ã€è¯­éŸ³ç‰¹å¾ã€å¥æ³•ç‰¹å¾å’Œè¯­ä¹‰ç‰¹å¾ï¼Œä»¥åŠæ¥è‡ªä¸‰ç§æœ€å…ˆè¿›çš„SFMï¼ˆWhisperã€HuBERTã€WavLMï¼‰çš„ä¸Šä¸‹æ–‡åµŒå…¥ï¼Œé‡åŒ–ç”µçš®è´¨å›¾ï¼ˆECoGï¼‰åœ¨ç‰¹å¾ç±»åˆ«ä¹‹é—´å…±äº«çš„ååº”æ–¹å·®ã€‚æ–¹å·®åˆ†é…åˆ†ææ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼šé¦–å…ˆï¼ŒSFMä¸å¤§è„‘çš„åŒ¹é…åº¦ä¸»è¦å¯ä»¥è§£é‡Šä¸ºå®ƒä»¬å­¦ä¹ å’Œç¼–ç ç®€å•å¯è§£é‡Šçš„è¯­éŸ³ç‰¹å¾çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼ŒSFMåœ¨ç¼–ç ä¸å¤§è„‘ç›¸å…³çš„é«˜çº§å’Œä½çº§ç‰¹å¾æ—¶ï¼Œå„å±‚çº§é—´å­˜åœ¨ç³»ç»Ÿæ€§æƒè¡¡ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSFMå­¦ä¹ åˆ°çš„ä¸å¤§è„‘ç›¸å…³çš„è¯­ä¹‰æ— æ³•ä»…é€šè¿‡ä½çº§è¯­éŸ³ç‰¹å¾æ¥è§£é‡Šï¼Œä¸”è¿™ç§èƒ½åŠ›éšç€æ¨¡å‹å¤§å°å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ è€Œå¢å¼ºã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¢å¼ºSFMåµŒå…¥ä¸å¯è§£é‡Šç‰¹å¾ï¼Œå¯ä»¥å»ºç«‹æ›´å…·åŸåˆ™æ€§ã€æ›´å‡†ç¡®å’Œé«˜æ•ˆçš„è„‘ç¼–ç æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16080v2">PDF</a> 19 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰è¢«èµèª‰ä¸ºäººç±»è¯­éŸ³æ„ŸçŸ¥çš„å¼ºå¤§è®¡ç®—æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå…¶è¡¨ç¤ºæ–¹å¼æœ¬è´¨ä¸Šæ˜¯é»‘ç›’ï¼Œå°šä¸æ¸…æ¥šæ˜¯ä»€ä¹ˆé©±åŠ¨å…¶ä¸å¤§è„‘å“åº”çš„å¯¹é½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡çº¿æ€§ç¼–ç æ¨¡å‹ï¼Œä»å…­ä¸ªå¯è§£é‡Šç‰¹å¾å®¶æ—å‡ºå‘ï¼Œä»¥åŠæ¥è‡ªä¸‰ç§æœ€æ–°SFMsçš„è¯­å¢ƒåŒ–åµŒå…¥ï¼Œé‡åŒ–äº†ä¸å¤§è„‘å“åº”çš„å…±äº«ç”µçš®è´¨å“åº”æ–¹å·®ã€‚åˆ†ææ­ç¤ºäº†å…³é”®è§è§£ï¼šé¦–å…ˆï¼ŒSFMsä¸å¤§è„‘çš„å¯¹é½ä¸»è¦æºäºå…¶å­¦ä¹ å’Œç¼–ç ç®€å•å¯è§£é‡Šè¯­éŸ³ç‰¹å¾çš„èƒ½åŠ›ï¼›å…¶æ¬¡ï¼ŒSFMsåœ¨ç¼–ç å¤§è„‘ç›¸å…³ä½çº§åˆ«å’Œé«˜çº§åˆ«ç‰¹å¾æ—¶å­˜åœ¨ç³»ç»Ÿæ€§æƒè¡¡ï¼›æœ€åï¼ŒSFMså­¦ä¹ åˆ°ä¸å¤§è„‘ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ— æ³•ç”±ä½çº§è¯­éŸ³ç‰¹å¾è§£é‡Šï¼Œä¸”è¿™ç§èƒ½åŠ›éšæ¨¡å‹å¤§å°å’Œè¯­å¢ƒé•¿åº¦å¢åŠ è€Œå¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SFMsä¸å¤§è„‘çš„å¯¹æ¥ä¸»è¦æºäºå…¶ç¼–ç ç®€å•å¯è§£é‡Šè¯­éŸ³ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>SFMsåœ¨ç¼–ç å¤§è„‘ç›¸å…³ä½çº§åˆ«å’Œé«˜çº§åˆ«ç‰¹å¾æ—¶å­˜åœ¨æƒè¡¡ã€‚</li>
<li>SFMsèƒ½å­¦ä¹ åˆ°ä¸å¤§è„‘ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™ä¸€èƒ½åŠ›éšæ¨¡å‹å¤§å°å’Œè¯­å¢ƒé•¿åº¦å¢åŠ è€Œå¢å¼ºã€‚</li>
<li>ç”µçš®è´¨å“åº”æ–¹å·®åˆ†æèƒ½æœ‰æ•ˆè¯„ä¼°SFMsä¸å¤§è„‘å“åº”çš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>SFMsçš„åµŒå…¥å¯ä»¥é€šè¿‡å¢åŠ å¯è§£é‡Šç‰¹å¾æ¥å¢å¼ºå¯¹å¤§è„‘çš„ç¼–ç æ¨¡å‹çš„è§£è¯»æ€§ã€å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ä¸åŒç‰¹å¾å®¶æ—å¯¹SFMsä¸å¤§è„‘å¯¹é½çš„è´¡çŒ®ç¨‹åº¦ä¸åŒï¼Œéœ€è¦é’ˆå¯¹æ€§åœ°è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5784ea2800f4507f84f3b34c1bdf340" align="middle">
<img src="https://picx.zhimg.com/v2-c9e456c7e8786a8957465cb77b08a681" align="middle">
<img src="https://picx.zhimg.com/v2-74c852fbc58df56bbf4115393a36480f" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation"><a href="#Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation" class="headerlink" title="Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation"></a>Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation</h2><p><strong>Authors:Jialong Mai, Xiaofen Xing, Yawei Li, Weidong Chen, Zhipeng Li, Jingyuan Xing, Xiangmin Xu</strong></p>
<p>Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively â€œmemorizeâ€ the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨å°†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰åº”ç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­ã€‚ç„¶è€Œï¼Œè¯­éŸ³æ¨¡æ€ä¸­å›ºæœ‰çš„é«˜å¸§ç‡ä¸¥é‡é™åˆ¶äº†SLLMçš„ä¿¡å·å¤„ç†å’Œç†è§£èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…·æœ‰4Kä¸Šä¸‹æ–‡çª—å£çš„SLLMåœ¨50Hzçš„ç‰¹å¾é‡‡æ ·ç‡ä¸‹åªèƒ½å¤„ç†80ç§’çš„éŸ³é¢‘ï¼Œç„¶åå°±è¾¾åˆ°å…¶å®¹é‡æé™ã€‚SLLMä¸­ä½¿ç”¨çš„è¾“å…¥ä»¤ç‰Œå‹ç¼©æ–¹æ³•å¿½ç•¥äº†æƒ…æ„Ÿåœ¨å¤šæ¬¡å¯¹è¯å›åˆä¸­çš„è¿ç»­æ€§å’Œæƒ¯æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¸¦æœ‰ä¸Šä¸‹æ–‡è¯­ä¹‰å’Œå¥å­çº§æƒ…æ„Ÿç¼–ç çš„åŠ¨æ€å‚æ•°å†…å­˜ï¼ˆDPMï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨SLLMä¸­çš„æœ‰é™ä¸Šä¸‹æ–‡çª—å£ä¸­å¤„ç†æ— é™é•¿åº¦çš„éŸ³é¢‘ã€‚å…·ä½“æ¥è¯´ï¼ŒDPMåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€æ­¥å°†å¥å­çº§ä¿¡æ¯å’Œæƒ…æ„Ÿç¼–ç åˆ°ä¸€ä¸ªä¸´æ—¶çš„LoRAæ¨¡å—ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°â€œè®°ä½â€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬ä»¥æƒ…æ„ŸSLLMä½œä¸ºä¸»å¹²è¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†DPMçº³å…¥æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œå¯¹è¯æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶ï¼ŒDPMèƒ½å¤Ÿæ˜¾è‘—æé«˜SLLMçš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09076v2">PDF</a> submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶å°†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰åº”ç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„æå‡ã€‚ç„¶è€Œï¼Œè¯­éŸ³æ¨¡æ€çš„é«˜å¸§ç‡ä¸¥é‡é™åˆ¶äº†SLLMçš„ä¿¡å·å¤„ç†å’Œç†è§£èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŠ¨æ€å‚æ•°è®°å¿†ï¼ˆDPMï¼‰æœºåˆ¶ï¼Œç»“åˆä¸Šä¸‹æ–‡è¯­ä¹‰å’Œå¥å­çº§æƒ…æ„Ÿç¼–ç ï¼Œåœ¨æœ‰é™çš„è¯­å¢ƒçª—å£å†…å®ç°æ— é™é•¿åº¦éŸ³é¢‘çš„å¤„ç†ã€‚DPMå°†å¥å­çº§ä¿¡æ¯å’Œæƒ…æ„Ÿé€æ­¥ç¼–ç åˆ°ä¸´æ—¶LoRAæ¨¡å—ä¸­ï¼Œæœ‰æ•ˆâ€œè®°å¿†â€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDPMåœ¨å¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶æ˜¾è‘—æé«˜äº†SLLMçš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„åº”ç”¨å—åˆ°é«˜å¸§ç‡é™åˆ¶ã€‚</li>
<li>ç°æœ‰è¾“å…¥ä»¤ç‰Œå‹ç¼©æ–¹æ³•å¿½ç•¥äº†æƒ…æ„Ÿåœ¨å¤šä¸ªå¯¹è¯å›åˆä¸­çš„è¿ç»­æ€§å’Œæƒ¯æ€§ã€‚</li>
<li>åŠ¨æ€å‚æ•°è®°å¿†ï¼ˆDPMï¼‰æœºåˆ¶ç»“åˆäº†ä¸Šä¸‹æ–‡è¯­ä¹‰å’Œå¥å­çº§æƒ…æ„Ÿç¼–ç ã€‚</li>
<li>DPMé€šè¿‡é€æ­¥ç¼–ç å¥å­çº§ä¿¡æ¯å’Œæƒ…æ„Ÿåˆ°ä¸´æ—¶LoRAæ¨¡å—ä¸­ï¼Œæœ‰æ•ˆâ€œè®°å¿†â€ä¸Šä¸‹æ–‡ã€‚</li>
<li>DPMæé«˜äº†åœ¨å¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶çš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜DPMæ€§èƒ½è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9cd5a6e9a2938fe26254496f1646ee1" align="middle">
<img src="https://picx.zhimg.com/v2-9925bfd799d25959d30babe3b01c1b4d" align="middle">
<img src="https://picx.zhimg.com/v2-c90c16522637d6fb85ea5932488b9655" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAARMA-Class-Augmentation-with-Adversarial-Mixup-Regularization"><a href="#CAARMA-Class-Augmentation-with-Adversarial-Mixup-Regularization" class="headerlink" title="CAARMA: Class Augmentation with Adversarial Mixup Regularization"></a>CAARMA: Class Augmentation with Adversarial Mixup Regularization</h2><p><strong>Authors:Massa Baali, Xiang Li, Hao Chen, Syed Abdul Hannan, Rita Singh, Bhiksha Raj</strong></p>
<p>Speaker verification is a typical zero-shot learning task, where inference of unseen classes is performed by comparing embeddings of test instances to known examples. The models performing inference must hence naturally generate embeddings that cluster same-class instances compactly, while maintaining separation across classes. In order to learn to do so, they are typically trained on a large number of classes (speakers), often using specialized losses. However real-world speaker datasets often lack the class diversity needed to effectively learn this in a generalizable manner. We introduce CAARMA, a class augmentation framework that addresses this problem by generating synthetic classes through data mixing in the embedding space, expanding the number of training classes. To ensure the authenticity of the synthetic classes we adopt a novel adversarial refinement mechanism that minimizes categorical distinctions between synthetic and real classes. We evaluate CAARMA on multiple speaker verification tasks, as well as other representative zero-shot comparison-based speech analysis tasks and obtain consistent improvements: our framework demonstrates a significant improvement of 8% over all baseline models. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/massabaali7/CAARMA/">https://github.com/massabaali7/CAARMA/</a> </p>
<blockquote>
<p>è¯´è¯äººéªŒè¯æ˜¯ä¸€ä¸ªå…¸å‹çš„é›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼Œé€šè¿‡æ¯”è¾ƒæµ‹è¯•å®ä¾‹çš„åµŒå…¥å’Œå·²çŸ¥ç¤ºä¾‹æ¥è¿›è¡Œæœªè§ç±»çš„æ¨ç†ã€‚å› æ­¤ï¼Œæ‰§è¡Œæ¨ç†çš„æ¨¡å‹å¿…é¡»è‡ªç„¶åœ°ç”ŸæˆåµŒå…¥ï¼Œå°†åŒç±»å®ä¾‹ç´§å¯†èšç±»ï¼ŒåŒæ—¶ä¿æŒå„ç±»ä¹‹é—´çš„åˆ†ç¦»ã€‚ä¸ºäº†å­¦ä¼šè¿™æ ·åšï¼Œå®ƒä»¬é€šå¸¸ä¼šåœ¨å¤§é‡ç±»åˆ«ï¼ˆè¯´è¯äººï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¸¸å¸¸ä½¿ç”¨ä¸“é—¨çš„æŸå¤±å‡½æ•°ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„è¯´è¯äººæ•°æ®é›†å¾€å¾€ç¼ºä¹ä»¥é€šç”¨æ–¹å¼æœ‰æ•ˆå­¦ä¹ æ‰€éœ€çš„ç±»åˆ«å¤šæ ·æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†CAARMAï¼Œè¿™æ˜¯ä¸€ç§ç±»æ‰©å±•æ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ•°æ®æ··åˆç”Ÿæˆåˆæˆç±»æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‰©å¤§äº†è®­ç»ƒç±»çš„æ•°é‡ã€‚ä¸ºäº†ç¡®ä¿åˆæˆç±»çš„çœŸå®æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°å‹å¯¹æŠ—æ€§æ”¹è¿›æœºåˆ¶ï¼Œæœ€å°åŒ–åˆæˆç±»å’ŒçœŸå®ç±»ä¹‹é—´çš„ç±»åˆ«å·®å¼‚ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè¯´è¯äººéªŒè¯ä»»åŠ¡ä»¥åŠå…¶ä»–å…·æœ‰ä»£è¡¨æ€§çš„åŸºäºé›¶æ ·æœ¬æ¯”è¾ƒçš„è¯­éŸ³åˆ†æä»»åŠ¡ä¸Šè¯„ä¼°äº†CAARMAï¼Œå¹¶è·å¾—äº†æŒç»­æ€§çš„æ”¹è¿›ï¼šæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰åŸºå‡†æ¨¡å‹ä¸Šå®ç°äº†8%çš„æ˜¾è‘—æ”¹è¿›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://github.com/massabaali7/CAARMA/]">https://github.com/massabaali7/CAARMA/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16718v3">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong>ï¼š<br>æ–‡æœ¬ä»‹ç»äº†å‘è¨€äººéªŒè¯ä¸­çš„é›¶æ ·æœ¬å­¦ä¹ é—®é¢˜ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæå‡ºä¸€ä¸ªåä¸ºCAARMAçš„ç±»å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ•°æ®æ··åˆç”Ÿæˆåˆæˆç±»æ¥æ‰©å±•è®­ç»ƒç±»çš„æ•°é‡ã€‚é‡‡ç”¨å¯¹æŠ—æ€§ç²¾ç‚¼æœºåˆ¶ç¡®ä¿åˆæˆç±»çš„çœŸå®æ€§ï¼Œç¼©å°ä¸çœŸå®ç±»çš„åˆ†ç±»å·®å¼‚ã€‚åœ¨å¤šä¸ªå‘è¨€äººéªŒè¯ä»»åŠ¡ä»¥åŠå…¶ä»–åŸºäºé›¶æ ·æœ¬æ¯”è¾ƒçš„è¯­éŸ³åˆ†æä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼ŒCAARMAæ¡†æ¶æ˜¾ç¤ºå‡ºä¸€è‡´çš„æ”¹è¿›ï¼Œæå‡å¹…åº¦è¾¾8%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å‘è¨€äººéªŒè¯æ˜¯é›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡çš„å…¸å‹ä»£è¡¨ï¼Œéœ€è¦é€šè¿‡æ¯”è¾ƒæµ‹è¯•å®ä¾‹çš„åµŒå…¥ä¸å·²çŸ¥ç¤ºä¾‹æ¥è¿›è¡Œæœªè§ç±»çš„æ¨æ–­ã€‚</li>
<li>æ¨¡å‹éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆç´§å‡‘çš„åŒç±»åµŒå…¥ï¼ŒåŒæ—¶ä¿æŒç±»é—´çš„åˆ†ç¦»ã€‚</li>
<li>ç°å®ä¸–ç•Œçš„å‘è¨€äººæ•°æ®é›†å¾€å¾€ç¼ºä¹ç±»å¤šæ ·æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆå­¦ä¹ æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥CAARMAæ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ•°æ®æ··åˆç”Ÿæˆåˆæˆç±»ï¼Œä»¥æ‰©å±•è®­ç»ƒç±»çš„æ•°é‡ã€‚</li>
<li>é‡‡çº³æ–°é¢–çš„å¯¹æŠ—æ€§ç²¾ç‚¼æœºåˆ¶ç¡®ä¿åˆæˆç±»çš„çœŸå®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªå‘è¨€äººéªŒè¯ä»»åŠ¡ä»¥åŠå…¶ä»–åŸºäºé›¶æ ·æœ¬æ¯”è¾ƒçš„è¯­éŸ³åˆ†æä»»åŠ¡ä¸Šè¯„ä¼°CAARMAï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-adcd1e3454398bfe552d5c03c4f3a40e" align="middle">
<img src="https://picx.zhimg.com/v2-641bd2d7fddd01e11cfc37da1a8f3d55" align="middle">
<img src="https://picx.zhimg.com/v2-124cde84623e0166ea0f2df13af5efb9" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration"><a href="#LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration" class="headerlink" title="LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration"></a>LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration</h2><p><strong>Authors:Sangmin Lee, Woo-Jin Chung, Hong-Goo Kang</strong></p>
<p>Building a universal multilingual automatic speech recognition (ASR) model that performs equitably across languages has long been a challenge due to its inherent difficulties. To address this task we introduce a Language-Agnostic Multilingual ASR pipeline through orthography Unification and language-specific Transliteration (LAMA-UT). LAMA-UT operates without any language-specific modules while matching the performance of state-of-the-art models trained on a minimal amount of data. Our pipeline consists of two key steps. First, we utilize a universal transcription generator to unify orthographic features into Romanized form and capture common phonetic characteristics across diverse languages. Second, we utilize a universal converter to transform these universal transcriptions into language-specific ones. In experiments, we demonstrate the effectiveness of our proposed method leveraging universal transcriptions for massively multilingual ASR. Our pipeline achieves a relative error reduction rate of 45% when compared to Whisper and performs comparably to MMS, despite being trained on only 0.1% of Whisperâ€™s training data. Furthermore, our pipeline does not rely on any language-specific modules. However, it performs on par with zero-shot ASR approaches which utilize additional language-specific lexicons and language models. We expect this framework to serve as a cornerstone for flexible multilingual ASR systems that are generalizable even to unseen languages. </p>
<blockquote>
<p>æ„å»ºä¸€ç§èƒ½åœ¨å„ç§è¯­è¨€ä¸­è¡¨ç°å‡è¡¡çš„é€šç”¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå…¶å›ºæœ‰çš„éš¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šè¿‡æ­£å­—æ³•ç»Ÿä¸€å’Œè¯­è¨€ç‰¹å®šè½¬å†™ï¼ˆLAMA-UTï¼‰çš„è¯­è¨€æ— å…³å¤šè¯­ç§ASRç®¡é“ã€‚LAMA-UTåœ¨æ²¡æœ‰ä»»ä½•è¯­è¨€ç‰¹å®šæ¨¡å—çš„æƒ…å†µä¸‹è¿è¡Œï¼ŒåŒæ—¶åŒ¹é…åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç®¡é“åŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨é€šç”¨è½¬å½•ç”Ÿæˆå™¨å°†æ­£å­—ç‰¹å¾ç»Ÿä¸€ä¸ºç½—é©¬åŒ–å½¢å¼ï¼Œå¹¶æ•æ‰ä¸åŒè¯­è¨€çš„å…±åŒè¯­éŸ³ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨é€šç”¨è½¬æ¢å™¨å°†è¿™äº›é€šç”¨è½¬å½•è½¬åŒ–ä¸ºç‰¹å®šè¯­è¨€çš„è½¬å½•ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ©ç”¨é€šç”¨è½¬å½•è¿›è¡Œå¤§è§„æ¨¡å¤šè¯­ç§ASRçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸whisperç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç›¸å¯¹è¯¯å·®å‡å°‘ç‡æ–¹é¢å®ç°äº†45%çš„é™ä½ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨whisper 0.1%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¸MMSç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç®¡é“ä¸ä¾èµ–äºä»»ä½•è¯­è¨€ç‰¹å®šçš„æ¨¡å—ï¼Œä½†å…¶æ€§èƒ½ä¸é›¶å°„å‡»ASRæ–¹æ³•ç›¸å½“ï¼Œåè€…åˆ©ç”¨é¢å¤–çš„è¯­è¨€ç‰¹å®šè¯æ±‡å’Œè¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¡†æ¶èƒ½æˆä¸ºçµæ´»çš„å¤šè¯­ç§ASRç³»ç»Ÿçš„åŸºçŸ³ï¼Œç”šè‡³å¯ä»¥å¯¹æœªè§è¿‡çš„è¯­è¨€è¿›è¡Œæ¨å¹¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15299v3">PDF</a> Accepted to AAAI 2025 (Oral Presentation)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§è¯­è¨€æ— å…³çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç®¡é“ï¼Œé€šè¿‡æ­£å­—æ³•ç»Ÿä¸€å’Œè¯­è¨€ç‰¹å®šè½¬å†™ï¼ˆLAMA-UTï¼‰æ¥è§£å†³å¤šè¯­ç§ASRçš„éš¾é¢˜ã€‚è¯¥ç®¡é“æ— éœ€ä»»ä½•ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼ŒåŒæ—¶åœ¨å°‘é‡æ•°æ®ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®æ­¥éª¤å®ç°ï¼šé¦–å…ˆåˆ©ç”¨é€šç”¨è½¬å½•ç”Ÿæˆå™¨å°†æ­£å­—æ³•ç‰¹å¾ç»Ÿä¸€è½¬æ¢ä¸ºç½—é©¬åŒ–å½¢å¼ï¼Œæ•æ‰ä¸åŒè¯­è¨€çš„å…±åŒè¯­éŸ³ç‰¹å¾ï¼›å…¶æ¬¡åˆ©ç”¨é€šç”¨è½¬æ¢å™¨å°†è¿™äº›é€šç”¨è½¬å½•è½¬æ¢ä¸ºç‰¹å®šè¯­è¨€çš„è½¬å½•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§é‡å¤šè¯­ç§ASRä¸­åˆ©ç”¨é€šç”¨è½¬å½•éå¸¸æœ‰æ•ˆï¼Œä¸whisperç›¸æ¯”å®ç°äº†ç›¸å¯¹è¯¯å·®é™ä½ç‡45%ï¼Œå¹¶ä¸”ä¸MMSè¡¨ç°ç›¸å½“ï¼Œå°½ç®¡åªä½¿ç”¨äº†whisper 0.1%çš„è®­ç»ƒæ•°æ®ã€‚é¢„æœŸè¯¥æ¡†æ¶å°†ä¸ºçµæ´»çš„å¤šè¯­ç§ASRç³»ç»Ÿæä¾›æœåŠ¡ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯­è¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç®¡é“LAMA-UTï¼Œè§£å†³äº†å¤šè¯­ç§ASRçš„éš¾é¢˜ã€‚</li>
<li>è¯¥ç®¡é“æ— éœ€ä»»ä½•ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œé€‚åº”å¤šç§è¯­è¨€ç¯å¢ƒä¸‹çš„è¯†åˆ«ä»»åŠ¡ã€‚</li>
<li>LAMA-UTåŒ…å«ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šé€šç”¨è½¬å½•ç”Ÿæˆå™¨å’Œé€šç”¨è½¬æ¢å™¨ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒLAMA-UTåœ¨å°‘é‡æ•°æ®ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸whisperç›¸æ¯”å®ç°äº†ç›¸å¯¹è¯¯å·®é™ä½ç‡45%ã€‚</li>
<li>LAMA-UTä¸MMSè¡¨ç°ç›¸å½“ï¼Œå°½ç®¡åªä½¿ç”¨äº†whisper 0.1%çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>LAMA-UTæ¡†æ¶å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯­è¨€ï¼Œå…·æœ‰çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b4af20c6aa9256dc97f5939e0597f5b" align="middle">
<img src="https://picx.zhimg.com/v2-13f6665919362a43b62f117bf6042fc0" align="middle">
<img src="https://picx.zhimg.com/v2-3916b987a1187ae5bcec390dea002f3b" align="middle">
<img src="https://picx.zhimg.com/v2-1a789242d166d387373e24a3ebfab27f" align="middle">
<img src="https://picx.zhimg.com/v2-43aa212fda918dfce9a5ee1fad9d8806" align="middle">
<img src="https://picx.zhimg.com/v2-d9e1daa8c71227da7f9d4ff04987603f" align="middle">
<img src="https://picx.zhimg.com/v2-3da82cd00bea2a565229d9f1517eb953" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Application-of-Audio-Fingerprinting-Techniques-for-Real-Time-Scalable-Speech-Retrieval-and-Speech-Clusterization"><a href="#Application-of-Audio-Fingerprinting-Techniques-for-Real-Time-Scalable-Speech-Retrieval-and-Speech-Clusterization" class="headerlink" title="Application of Audio Fingerprinting Techniques for Real-Time Scalable   Speech Retrieval and Speech Clusterization"></a>Application of Audio Fingerprinting Techniques for Real-Time Scalable   Speech Retrieval and Speech Clusterization</h2><p><strong>Authors:Kemal Altwlkany, Sead DelaliÄ‡, Adis AlihodÅ¾iÄ‡, Elmedin SelmanoviÄ‡, Damir HasiÄ‡</strong></p>
<p>Audio fingerprinting techniques have seen great advances in recent years, enabling accurate and fast audio retrieval even in conditions when the queried audio sample has been highly deteriorated or recorded in noisy conditions. Expectedly, most of the existing work is centered around music, with popular music identification services such as Appleâ€™s Shazam or Googleâ€™s Now Playing designed for individual audio recognition on mobile devices. However, the spectral content of speech differs from that of music, necessitating modifications to current audio fingerprinting approaches. This paper offers fresh insights into adapting existing techniques to address the specialized challenge of speech retrieval in telecommunications and cloud communications platforms. The focus is on achieving rapid and accurate audio retrieval in batch processing instead of facilitating single requests, typically on a centralized server. Moreover, the paper demonstrates how this approach can be utilized to support audio clustering based on speech transcripts without undergoing actual speech-to-text conversion. This optimization enables significantly faster processing without the need for GPU computing, a requirement for real-time operation that is typically associated with state-of-the-art speech-to-text tools. </p>
<blockquote>
<p>éŸ³é¢‘æŒ‡çº¹æŠ€æœ¯è¿‘å¹´æ¥å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œå³ä½¿åœ¨æŸ¥è¯¢çš„éŸ³é¢‘æ ·æœ¬é«˜åº¦é€€åŒ–æˆ–å˜ˆæ‚ç¯å¢ƒä¸‹å½•åˆ¶çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å®ç°å‡†ç¡®å¿«é€Ÿçš„éŸ³é¢‘æ£€ç´¢ã€‚ä¸å‡ºæ‰€æ–™ï¼Œç°æœ‰å·¥ä½œå¤§å¤šå›´ç»•éŸ³ä¹å±•å¼€ï¼Œè‹¹æœå…¬å¸çš„Shazamæˆ–è°·æ­Œçš„Now Playingç­‰æµè¡ŒéŸ³ä¹è¯†åˆ«æœåŠ¡å°±æ˜¯ä¸ºäº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°å•ä¸ªéŸ³é¢‘è¯†åˆ«è€Œè®¾è®¡çš„ã€‚ç„¶è€Œï¼Œè¯­éŸ³çš„é¢‘è°±å†…å®¹ä¸éŸ³ä¹ä¸åŒï¼Œéœ€è¦å¯¹å½“å‰çš„éŸ³é¢‘æŒ‡çº¹æ–¹æ³•è¿›è¡Œä¿®æ”¹ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç”µä¿¡å’Œäº‘é€šä¿¡å¹³å°ä¸Šçš„è¯­éŸ³æ£€ç´¢è¿™ä¸€ç‰¹æ®ŠæŒ‘æˆ˜æä¾›é€‚åº”ç°æœ‰æŠ€æœ¯çš„å…¨æ–°è§è§£ã€‚é‡ç‚¹æ˜¯åœ¨æ‰¹å¤„ç†ä¸­å®ç°å¿«é€Ÿå‡†ç¡®çš„éŸ³é¢‘æ£€ç´¢ï¼Œè€Œä¸æ˜¯ä¸ºå•ä¸ªè¯·æ±‚æä¾›ä¾¿åˆ©ï¼Œé€šå¸¸åœ¨é›†ä¸­å¼æœåŠ¡å™¨ä¸Šå®ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™ç§æ–¹æ³•åœ¨æ— éœ€å®é™…è¿›è¡Œè¯­éŸ³åˆ°æ–‡æœ¬çš„è½¬æ¢çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®è¯­éŸ³è½¬å½•æ¥æ”¯æŒéŸ³é¢‘èšç±»ã€‚è¿™ä¸€ä¼˜åŒ–åœ¨ä¸ä½¿ç”¨GPUè®¡ç®—çš„æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—æ›´å¿«çš„å¤„ç†é€Ÿåº¦ï¼Œè€ŒGPUè®¡ç®—é€šå¸¸æ˜¯ä¸æœ€å…ˆè¿›çš„è¯­éŸ³åˆ°æ–‡æœ¬å·¥å…·ç›¸å…³çš„å®æ—¶æ“ä½œè¦æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21876v2">PDF</a> Proceedings of the International Convention MIPRO</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸéŸ³é¢‘æŒ‡çº¹æŠ€æœ¯åœ¨éŸ³é¢‘æ£€ç´¢æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå³ä½¿åœ¨éŸ³é¢‘æ ·æœ¬é«˜åº¦å¤±çœŸæˆ–å™ªå£°ç¯å¢ƒä¸‹ä¹Ÿèƒ½å®ç°å‡†ç¡®å¿«é€Ÿçš„éŸ³é¢‘æ£€ç´¢ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰å·¥ä½œéƒ½é›†ä¸­åœ¨éŸ³ä¹é¢†åŸŸï¼Œä½†æœ¬æ–‡æ¢è®¨äº†é€‚åº”ç°æœ‰æŠ€æœ¯æ¥è§£å†³ç”µä¿¡å’Œäº‘é€šä¿¡å¹³å°è¯­éŸ³æ£€ç´¢çš„ç‰¹æ®ŠæŒ‘æˆ˜ã€‚é‡ç‚¹åœ¨äºå®ç°æ‰¹é‡å¤„ç†çš„å¿«é€Ÿå‡†ç¡®çš„éŸ³é¢‘æ£€ç´¢ï¼Œè€Œä¸æ˜¯æ”¯æŒå•ä¸ªè¯·æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåŸºäºè¯­éŸ³æ–‡æœ¬çš„éŸ³é¢‘èšç±»ï¼Œæ— éœ€å®é™…è¿›è¡Œè¯­éŸ³åˆ°æ–‡æœ¬çš„è½¬æ¢ï¼Œä»è€Œä¼˜åŒ–å¤„ç†é€Ÿåº¦ï¼Œæ— éœ€ä½¿ç”¨é€šå¸¸ä¸æœ€å…ˆè¿›çš„è¯­éŸ³åˆ°æ–‡æœ¬å·¥å…·ç›¸å…³çš„GPUè®¡ç®—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘æŒ‡çº¹æŠ€æœ¯è¿‘å¹´æ¥åœ¨éŸ³é¢‘æ£€ç´¢æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå®ç°äº†åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„å¿«é€Ÿå‡†ç¡®æ£€ç´¢ã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨éŸ³ä¹é¢†åŸŸï¼Œä½†è¯­éŸ³æ£€ç´¢åœ¨ç”µä¿¡å’Œäº‘é€šä¿¡å¹³å°ä¸Šæœ‰ç‰¹æ®ŠæŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡é‡ç‚¹ç ”ç©¶æ‰¹é‡å¤„ç†çš„å¿«é€Ÿå‡†ç¡®çš„éŸ³é¢‘æ£€ç´¢ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥æ”¯æŒåŸºäºè¯­éŸ³æ–‡æœ¬çš„éŸ³é¢‘èšç±»ï¼Œæ— éœ€å®é™…è¿›è¡Œè¯­éŸ³åˆ°æ–‡æœ¬çš„è½¬æ¢ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•ä¼˜åŒ–äº†å¤„ç†é€Ÿåº¦ï¼Œé¿å…äº†ä½¿ç”¨GPUè®¡ç®—çš„å¿…è¦æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›æé«˜ç”µä¿¡å’Œäº‘é€šä¿¡å¹³å°çš„éŸ³é¢‘å¤„ç†æ•ˆç‡ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºéŸ³é¢‘æŒ‡çº¹æŠ€æœ¯åœ¨è¯­éŸ³å¤„ç†é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70571cc18cf570604c232006a3756f51" align="middle">
<img src="https://picx.zhimg.com/v2-c0e0668e8694b49bb559967ee7f7d8bb" align="middle">
<img src="https://picx.zhimg.com/v2-75a1b47f0b5b111e638903c90fb5653f" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4bcebee6dc782b0a6f8cb715c95b1e06" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Gaussian Herding across Pens An Optimal Transport Perspective on Global   Gaussian Reduction for 3DGS
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-876e312683adb582d0cc223c7e3f440b" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  OrthoLoC UAV 6-DoF Localization and Calibration Using Orthographic   Geodata
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
