<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-29  Diffusion Bridge Variational Inference for Deep Gaussian Processes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-6189bd830808381148085be7d6f20de0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058714&auth_key=1760058714-0-0-f696d7ef9bd43f7ba393c4cc84b62e4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-29-更新"><a href="#2025-09-29-更新" class="headerlink" title="2025-09-29 更新"></a>2025-09-29 更新</h1><h2 id="Diffusion-Bridge-Variational-Inference-for-Deep-Gaussian-Processes"><a href="#Diffusion-Bridge-Variational-Inference-for-Deep-Gaussian-Processes" class="headerlink" title="Diffusion Bridge Variational Inference for Deep Gaussian Processes"></a>Diffusion Bridge Variational Inference for Deep Gaussian Processes</h2><p><strong>Authors:Jian Xu, Qibin Zhao, John Paisley, Delu Zeng</strong></p>
<p>Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian modeling but pose substantial challenges for posterior inference, especially over inducing variables. Denoising diffusion variational inference (DDVI) addresses this by modeling the posterior as a time-reversed diffusion from a simple Gaussian prior. However, DDVI’s fixed unconditional starting distribution remains far from the complex true posterior, resulting in inefficient inference trajectories and slow convergence. In this work, we propose Diffusion Bridge Variational Inference (DBVI), a principled extension of DDVI that initiates the reverse diffusion from a learnable, data-dependent initial distribution. This initialization is parameterized via an amortized neural network and progressively adapted using gradients from the ELBO objective, reducing the posterior gap and improving sample efficiency. To enable scalable amortization, we design the network to operate on the inducing inputs, which serve as structured, low-dimensional summaries of the dataset and naturally align with the inducing variables’ shape. DBVI retains the mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We derive a tractable training objective under this formulation and implement DBVI for scalable inference in large-scale DGPs. Across regression, classification, and image reconstruction tasks, DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality. </p>
<blockquote>
<p>深度高斯过程（DGPs）能够实现表达性层次化的贝叶斯建模，但给后验推断带来了实质性的挑战，特别是在诱导变量方面。降噪扩散变分推断（DDVI）通过把后验建模为从简单高斯先验开始的时间反转扩散来解决这个问题。然而，DDVI的固定无条件起始分布与复杂的真实后验相差甚远，导致推理轨迹效率低下，收敛缓慢。在这项工作中，我们提出了扩散桥变分推断（DBVI），它是DDVI的一种原则性扩展，从可学习的、依赖于数据初始分布开始反向扩散。这个初始化是通过摊销神经网络进行参数化的，并且通过使用ELBO目标的梯度进行逐步适应，从而缩小了后验差距并提高了样本效率。为了实现可扩展的摊销，我们设计网络在诱导输入上运行，这些输入作为数据集的结构化、低维摘要，自然与诱导变量的形状对齐。DBVI保留了DDVI的数学优雅性，包括基于Girsanov的ELBOs和反向时间SDEs，同时通过Doob桥扩散过程重新解释先验。在这种形式下，我们推导出了一个可行的训练目标，并实现了DBVI用于在大规模DGP中进行可扩展推理。在回归、分类和图像重建任务中，DBVI在预测精度、收敛速度和后验质量方面始终优于DDVI和其他变分基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19078v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>深度高斯过程（DGPs）能够进行灵活的分层贝叶斯建模，但为后置推理带来了巨大挑战，尤其是在诱导变量方面。去噪扩散变分推理（DDVI）通过模拟从简单高斯先验开始的时间反转扩散过程来解决这个问题。然而，DDVI固定的无条件起始分布与复杂的真实后验分布相差甚远，导致推理轨迹效率低下，收敛速度慢。针对这一问题，本文提出了扩散桥变分推理（DBVI），这是一种对DDVI的理论扩展，它从可学习的、数据依赖的初始分布开始反向扩散。该初始化通过摊销神经网络进行参数化，并使用ELBO目标函数的梯度逐步适应，从而缩小后验差距并提高了样本效率。为了支持可扩展的摊销，我们设计了一个在诱导输入上运行的网络，这些输入作为数据集的结构化、低维摘要，自然地与诱导变量的形状对齐。DBVI保留了DDVI的数学优雅性，包括Girsanov-based ELBOs和反向时间SDEs，同时通过Doob桥扩散过程重新解释先验。我们在此公式下推导出了一个可行的训练目标，并实现了用于大规模DGPs中可扩展推理的DBVI。在回归、分类和图像重建任务中，DBVI在预测精度、收敛速度和后验质量方面均优于DDVI和其他变分基准测试。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>深度高斯过程（DGPs）在进行层次贝叶斯建模时面临后置推理的挑战，尤其是在诱导变量方面。</li>
<li>Denoising Diffusion Variational Inference (DDVI) 通过模拟从简单高斯先验开始的时间反转扩散过程来解决这个问题，但存在固定起始分布的问题。</li>
<li>提出的Diffusion Bridge Variational Inference (DBVI) 从可学习的数据依赖初始分布开始反向扩散，缩小了与复杂真实后验分布的差距。</li>
<li>DBVI通过摊销神经网络进行参数化初始化，并使用ELBO目标函数的梯度逐步适应，提高了样本效率和后验质量。</li>
<li>DBVI设计网络在诱导输入上运行，这些输入作为数据集的结构化、低维摘要。</li>
<li>DBVI保留了DDVI的数学优雅性，包括Girsanov-based ELBOs和反向时间SDEs，并通过Doob桥扩散过程重新解释先验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19078">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2ccbcbed87c0aa64041b3a5a095393f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058722&auth_key=1760058722-0-0-672def301002298498c837ee891e429e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a87220aab2b572f32501d1be9103c755~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058729&auth_key=1760058729-0-0-758773652f03dc7c724b949bc2af1c83&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiSSECT-Structuring-Transfer-Ready-Medical-Image-Representations-through-Discrete-Self-Supervision"><a href="#DiSSECT-Structuring-Transfer-Ready-Medical-Image-Representations-through-Discrete-Self-Supervision" class="headerlink" title="DiSSECT: Structuring Transfer-Ready Medical Image Representations   through Discrete Self-Supervision"></a>DiSSECT: Structuring Transfer-Ready Medical Image Representations   through Discrete Self-Supervision</h2><p><strong>Authors:Azad Singh, Deepak Mishra</strong></p>
<p>Self-supervised learning (SSL) has emerged as a powerful paradigm for medical image representation learning, particularly in settings with limited labeled data. However, existing SSL methods often rely on complex architectures, anatomy-specific priors, or heavily tuned augmentations, which limit their scalability and generalizability. More critically, these models are prone to shortcut learning, especially in modalities like chest X-rays, where anatomical similarity is high and pathology is subtle. In this work, we introduce DiSSECT – Discrete Self-Supervision for Efficient Clinical Transferable Representations, a framework that integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representational bottleneck. This constrains the model to learn repeatable, structure-aware features while suppressing view-specific or low-utility patterns, improving representation transfer across tasks and domains. DiSSECT achieves strong performance on both classification and segmentation tasks, requiring minimal or no fine-tuning, and shows particularly high label efficiency in low-label regimes. We validate DiSSECT across multiple public medical imaging datasets, demonstrating its robustness and generalizability compared to existing state-of-the-art approaches. </p>
<blockquote>
<p>自监督学习（SSL）已成为医学图像表示学习的一种强大范式，特别是在标签数据有限的情况下。然而，现有的SSL方法往往依赖于复杂的架构、特定的解剖先验知识或经过高度调整的数据增强，这限制了其可扩展性和通用性。更重要的是，这些模型容易陷入捷径学习，特别是在胸部X射线等模态中，由于解剖结构相似性高、病理表现微妙更是如此。在这项工作中，我们引入了DiSSECT——离散自监督高效临床可转移表示框架，它将多尺度矢量量化集成到SSL管道中，以施加离散表示瓶颈。这约束模型学习可重复、结构感知的特征，同时抑制特定视图或低效用模式，改进跨任务和领域的表示转移。DiSSECT在分类和分割任务上均表现出强大的性能，几乎不需要或根本不需要微调，并且在低标签状态下显示出极高的标签效率。我们在多个公共医学成像数据集上验证了DiSSECT，与现有最先进的方法相比，证明了其稳健性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18765v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对医学图像表示学习的自监督学习（SSL）方法的局限性，如复杂性高、依赖于特定解剖结构先验信息和需要大量调优的数据增强。针对这些问题，提出了一个新的SSL框架DiSSECT，通过将多尺度矢量量化整合到SSL管道中，以施加离散表示瓶颈，从而学习可重复的结构感知特征并抑制特定视图或低效用模式，提高跨任务和领域的表示转移能力。DiSSECT在分类和分割任务上表现出强大的性能，可以在低标签情况下实现高标签效率。通过跨多个公共医学成像数据集验证，展示了其在与现有最先进的策略相比具有稳健性和通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习（SSL）在医学图像表示学习中具有广泛的应用前景，尤其在有限标签数据的情况下。</li>
<li>现有的SSL方法存在复杂性高、依赖于特定解剖结构先验和需要大量调优的数据增强等问题。</li>
<li>DiSSECT框架通过整合多尺度矢量量化到SSL管道中，以施加离散表示瓶颈，从而提高模型的性能。</li>
<li>DiSSECT框架能够学习可重复的结构感知特征并抑制特定视图或低效用模式，提高跨任务和领域的表示转移能力。</li>
<li>DiSSECT在分类和分割任务上表现出强大的性能，且在高标签效率方面具有优势。</li>
<li>DiSSECT框架在不同的公共医学成像数据集上验证过，表现出了其稳健性和通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18765">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e05332056a4cc564241594ccb3f0000b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058737&auth_key=1760058737-0-0-bcc6725dbcccb79271b2b849678ba02c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f14730e54f715f65f18ca800e83c0a4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058745&auth_key=1760058745-0-0-9fad2c0ea5e507ec19cde52832bf432b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dadc6157d54dce2a363f094520422d69~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058752&auth_key=1760058752-0-0-ac0a4f071652fe458947bda75304a188&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-685176cfd3e8100f48131ec9c3592511~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058759&auth_key=1760058759-0-0-b996e9bb91336752667219f66d8310b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9af4a1744bdbccdeaf66e2513296da8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058766&auth_key=1760058766-0-0-94b25eb356ea540c0180cd0e818fce4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ce0a511e7a9c3f7f1dc19ce3dceccdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058773&auth_key=1760058773-0-0-a468d270fd36476f7e2bf8b9786cd777&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OraPO-Oracle-educated-Reinforcement-Learning-for-Data-efficient-and-Factual-Radiology-Report-Generation"><a href="#OraPO-Oracle-educated-Reinforcement-Learning-for-Data-efficient-and-Factual-Radiology-Report-Generation" class="headerlink" title="OraPO: Oracle-educated Reinforcement Learning for Data-efficient and   Factual Radiology Report Generation"></a>OraPO: Oracle-educated Reinforcement Learning for Data-efficient and   Factual Radiology Report Generation</h2><p><strong>Authors:Zhuoxiao Chen, Hongyang Yu, Ying Xu, Yadan Luo, Long Duong, Yuan-Fang Li</strong></p>
<p>Radiology report generation (RRG) aims to automatically produce clinically faithful reports from chest X-ray images. Prevailing work typically follows a scale-driven paradigm, by multi-stage training over large paired corpora and oversized backbones, making pipelines highly data- and compute-intensive. In this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables single-stage, RL-only training by converting failed GRPO explorations on rare or difficult studies into direct preference supervision via a lightweight oracle step. FactS grounds learning in diagnostic evidence by extracting atomic clinical facts and checking entailment against ground-truth labels, yielding dense, interpretable sentence-level rewards. Together, OraPO and FactS create a compact and powerful framework that significantly improves learning efficiency on clinically challenging cases, setting the new SOTA performance on the CheXpert Plus dataset (0.341 in F1) with 2–3 orders of magnitude less training data using a small base VLM on modest hardware. </p>
<blockquote>
<p>放射学报告生成（RRG）旨在从胸部X射线图像中自动产生临床真实的报告。现有的工作通常遵循规模驱动的方法，通过大规模配对语料库和过大的骨干网络的分阶段训练，使得管道高度依赖数据和计算资源。在本文中，我们提出了使用基于Oracle教育的GRPO（OraPO）和基于FactScore的奖励（FactS）在有限预算下解决RRG任务。OraPO通过轻量级的Oracle步骤将失败的GRPO探索转换为对罕见或困难研究的直接偏好监督，从而实现了单阶段仅RL训练。FactS通过提取原子临床事实并检查与真实标签的蕴涵关系，使学习基于诊断证据，从而产生密集、可解释的句子级奖励。奥拉普和法斯特一起，形成了一个紧凑而强大的框架，该框架在临床挑战案例中显著提高学习效率，在CheXpert Plus数据集上设定了新的最佳性能（F1得分为0.341），使用小型基础VLM在适度硬件上减少了2-3个数量级的训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Oracle-educated GRPO（OraPO）结合FactScore奖励机制的方法，旨在以有限的预算自动产生临床可靠的胸部X光图像报告。该方法通过单一阶段的强化学习训练，将失败的探索转化为直接的偏好监督，提高了学习效率，特别是在临床挑战案例中表现优异。在CheXpert Plus数据集上取得了最新的最佳性能（F1得分为0.341），并使用较小的基本VLM在适度硬件上实现了2-3个数量级的更少训练数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文旨在解决放射学报告生成（RRG）任务，即自动从胸部X光图像中产生临床可靠的报告。</li>
<li>现有方法通常采用多阶段大规模训练和大规模模型，导致数据和计算成本高昂。</li>
<li>论文提出了Oracle-educated GRPO（OraPO）方法，通过单一阶段的强化学习训练，提高了学习效率。</li>
<li>OraPO通过将失败的探索转化为直接的偏好监督，使得模型在困难案例上的表现得到显著提升。</li>
<li>论文引入了FactScore奖励机制，通过提取原子临床事实并检查其与真实标签的蕴含关系，为学习提供密集、可解释的句子级奖励。</li>
<li>OraPO和FactS结合形成了一个紧凑且强大的框架，在CheXpert Plus数据集上实现了卓越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-de46d64c2f76dd34468a293fef11e4cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058781&auth_key=1760058781-0-0-e1442641d65dd3393eaec121e64a179f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4167c110d71e6f48a84cf36057571bd5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058789&auth_key=1760058789-0-0-67307a684c26a15c291a3413367a6165&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca87693554fc1e65f95bc18b3b1ba48a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058796&auth_key=1760058796-0-0-a825462c6b9ad324a954547e044c2223&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MK-UNet-Multi-kernel-Lightweight-CNN-for-Medical-Image-Segmentation"><a href="#MK-UNet-Multi-kernel-Lightweight-CNN-for-Medical-Image-Segmentation" class="headerlink" title="MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation"></a>MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation</h2><p><strong>Authors:Md Mostafijur Rahman, Radu Marculescu</strong></p>
<p>In this paper, we introduce MK-UNet, a paradigm shift towards ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution block (MKDC) we design to adeptly process images through multiple kernels, while capturing complex multi-resolution spatial relationships. MK-UNet also emphasizes the images salient features through sophisticated attention mechanisms, including channel, spatial, and grouped gated attention. Our MK-UNet network, with a modest computational footprint of only 0.316M parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but also significantly improved segmentation solution that provides higher accuracy over state-of-the-art (SOTA) methods across six binary medical imaging benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively. Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation performance, improving the DICE score up to 6.7% margins while operating with 4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with much lower computational resources. This leap in performance, coupled with drastic computational gains, positions MK-UNet as an unparalleled solution for real-time, high-fidelity medical diagnostics in resource-limited settings, such as point-of-care devices. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/SLDGroup/MK-UNet">https://github.com/SLDGroup/MK-UNet</a>. </p>
<blockquote>
<p>本文介绍了MK-UNet，这是一种超轻量级多核U型CNN的范式转变，专门用于医学图像分割。MK-UNet的核心是我们设计的多核深度卷积块（MKDC），它能够灵活地处理图像，通过多个内核捕获复杂的跨分辨率空间关系。MK-UNet还通过复杂的注意力机制强调图像的显著特征，包括通道、空间和分组门控注意力。我们的MK-UNet网络仅有0.316M的参数和0.314G的FLOPs计算开销，不仅非常轻量级，而且在六个二元医学成像基准测试中提供了更高的精度，超过了最先进的方法。具体来说，MK-UNet在DICE得分上优于TransUNet，同时使用参数和FLOPs的近333倍和123倍更少。与UNeXt相比，MK-UNet展现出更优越的分割性能，DICE得分提高了高达6.7%，同时使用参数更少，为4.7倍。我们的MK-UNet还优于其他最新的轻量级网络，如MedT、CMUNeXt、EGE-UNet和Rolling-UNet，同时计算资源更低。这种性能的提升以及计算资源的显著节省，使MK-UNet成为资源受限环境中实时高保真医学诊断的无与伦比的选择，如便携式医疗设备。我们的实现可在<a target="_blank" rel="noopener" href="https://github.com/SLDGroup/MK-UNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SLDGroup/MK-UNet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18493v1">PDF</a> 11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD</p>
<p><strong>Summary</strong><br>    本文介绍了MK-UNet，一种面向医疗图像分割的超轻量级多核U形CNN。MK-UNet采用多核深度卷积块（MKDC）处理图像，同时捕捉复杂的多分辨率空间关系，并通过通道、空间和分组门控注意力机制强调图像的重要特征。MK-UNet具有较小的计算开销，仅0.316M参数和0.314G FLOPs，但在六个二进制医疗成像基准测试中实现了较高的准确性，优于现有方法。MK-UNet在DICE评分上优于TransUNet和UNeXt，同时需要更少的参数和FLOPs。此外，MK-UNet还优于其他轻量级网络，如MedT、CMUNeXt、EGE-UNet和Rolling-UNet。因此，MK-UNet被认为是资源受限环境中实时高保真医疗诊断的绝佳解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MK-UNet是一种面向医疗图像分割的超轻量级多核U形CNN。</li>
<li>MK-UNet采用多核深度卷积块（MKDC）处理图像，捕捉复杂的多分辨率空间关系。</li>
<li>MK-UNet通过通道、空间和分组门控注意力机制强调图像的重要特征。</li>
<li>MK-UNet在六个二进制医疗成像基准测试中实现了较高的准确性，优于现有方法。</li>
<li>MK-UNet在DICE评分上优于其他网络，如TransUNet和UNeXt，同时需要更少的参数和FLOPs。</li>
<li>MK-UNet的计算开销较小，适合在资源受限的环境中进行实时高保真医疗诊断。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ed59fd70022ddddde0d1f01ac4f6b11a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058803&auth_key=1760058803-0-0-fcc475e6b943807cb259f3176cd5df89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a84aceb0ff9bc71d8c2578fe98033ac5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058811&auth_key=1760058811-0-0-9bd5058d1d0ddaef31dd12ddb4414d5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bff185d9477a0d8b04b37906146a5d0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058817&auth_key=1760058817-0-0-0ba8b1c8bd573f263a0cd512b5b47869&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09becc58db39b4edc32f4b647cb4179b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058824&auth_key=1760058824-0-0-ab4abd4c94f45408197bedc9465c69c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Contrastive-Multimodal-Fusion-with-Improved-Modality-Dropout-for-Disease-Detection-and-Prediction"><a href="#Learning-Contrastive-Multimodal-Fusion-with-Improved-Modality-Dropout-for-Disease-Detection-and-Prediction" class="headerlink" title="Learning Contrastive Multimodal Fusion with Improved Modality Dropout   for Disease Detection and Prediction"></a>Learning Contrastive Multimodal Fusion with Improved Modality Dropout   for Disease Detection and Prediction</h2><p><strong>Authors:Yi Gu, Kuniaki Saito, Jiaxin Ma</strong></p>
<p>As medical diagnoses increasingly leverage multimodal data, machine learning models are expected to effectively fuse heterogeneous information while remaining robust to missing modalities. In this work, we propose a novel multimodal learning framework that integrates enhanced modalities dropout and contrastive learning to address real-world limitations such as modality imbalance and missingness. Our approach introduces learnable modality tokens for improving missingness-aware fusion of modalities and augments conventional unimodal contrastive objectives with fused multimodal representations. We validate our framework on large-scale clinical datasets for disease detection and prediction tasks, encompassing both visual and tabular modalities. Experimental results demonstrate that our method achieves state-of-the-art performance, particularly in challenging and practical scenarios where only a single modality is available. Furthermore, we show its adaptability through successful integration with a recent CT foundation model. Our findings highlight the effectiveness, efficiency, and generalizability of our approach for multimodal learning, offering a scalable, low-cost solution with significant potential for real-world clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/medical-modality-dropout">https://github.com/omron-sinicx/medical-modality-dropout</a>. </p>
<blockquote>
<p>随着医学诊断越来越多地利用多模态数据，机器学习模型需要有效地融合异构信息，同时对于缺失的模态也要保持稳健。在这项工作中，我们提出了一种新型的多模态学习框架，该框架集成了增强模态丢弃和对比学习，以解决现实世界中的局限性，如模态不平衡和缺失。我们的方法引入可学习的模态令牌，以改进缺失感知的模态融合，并用融合的多模态表示增强传统的单模态对比目标。我们在大规模临床数据集上验证了我们的框架，用于疾病检测和预测任务，涵盖视觉和表格模态。实验结果表明，我们的方法达到了最新技术水平，特别是在只有单一模态可用的具有挑战性和实际情景中表现尤其出色。此外，我们还展示了其与最新的CT基础模型的成功集成，证明了其适应性。我们的研究结果表明了该方法在多模态学习中的有效性、效率和泛化能力，提供了一种可扩展、成本低廉的解决方案，在现实世界临床应用中具有显著潜力。代码可在<a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/medical-modality-dropout%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/omron-sinicx/medical-modality-dropout上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18284v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的多模态学习框架，通过增强模态丢失和对比学习，有效融合异质信息，并应对模态不平衡和缺失等现实世界的挑战。该框架引入可学习的模态令牌，改进了缺失感知融合模态，并融合了传统的单模态对比目标。在大型临床数据集上进行疾病检测和预测任务验证，涵盖视觉和表格模态。实验结果表明，该方法在仅有单一模态的实用场景中表现优异。此外，它与最新的CT基础模型成功集成，展现出其适应性。该框架对于多模态学习的有效性、效率和泛化能力提供了显著的优势，是一种可扩展、低成本且具有实际应用前景的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种新型多模态学习框架，融合增强模态丢失和对比学习技术。</li>
<li>框架能够处理模态不平衡和缺失的现实挑战。</li>
<li>引入可学习的模态令牌，改进缺失感知融合模态技术。</li>
<li>融合了传统的单模态对比目标，形成融合的多模态表示。</li>
<li>在大型临床数据集上进行疾病检测和预测任务验证，表现优异。</li>
<li>框架成功集成最近的CT基础模型，展现出其适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18284">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6189bd830808381148085be7d6f20de0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058832&auth_key=1760058832-0-0-dcc4b61b6ecd7bbdacca7de7f3c3a329&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a07f98c368bd184cf840663eced19bbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058839&auth_key=1760058839-0-0-16eb76e2c9aa30153422a16c8f1ebf48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17dfa3ffa00f132b7bb8b15b71107366~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058846&auth_key=1760058846-0-0-df5c5036a65ee930351e42cb98b23ade&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Image-Classification-via-Synergistic-Learning-Pre-training"><a href="#Multimodal-Medical-Image-Classification-via-Synergistic-Learning-Pre-training" class="headerlink" title="Multimodal Medical Image Classification via Synergistic Learning   Pre-training"></a>Multimodal Medical Image Classification via Synergistic Learning   Pre-training</h2><p><strong>Authors:Qinghua Lin, Guang-Hai Liu, Zuoyong Li, Yang Li, Yuting Jiang, Xiang Wu</strong></p>
<p>Multimodal pathological images are usually in clinical diagnosis, but computer vision-based multimodal image-assisted diagnosis faces challenges with modality fusion, especially in the absence of expert-annotated data. To achieve the modality fusion in multimodal images with label scarcity, we propose a novel &#96;&#96;pretraining + fine-tuning” framework for multimodal semi-supervised medical image classification. Specifically, we propose a synergistic learning pretraining framework of consistency, reconstructive, and aligned learning. By treating one modality as an augmented sample of another modality, we implement a self-supervised learning pre-train, enhancing the baseline model’s feature representation capability. Then, we design a fine-tuning method for multimodal fusion. During the fine-tuning stage, we set different encoders to extract features from the original modalities and provide a multimodal fusion encoder for fusion modality. In addition, we propose a distribution shift method for multimodal fusion features, which alleviates the prediction uncertainty and overfitting risks caused by the lack of labeled samples. We conduct extensive experiments on the publicly available gastroscopy image datasets Kvasir and Kvasirv2. Quantitative and qualitative results demonstrate that the proposed method outperforms the current state-of-the-art classification methods. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/LQH89757/MICS">https://github.com/LQH89757/MICS</a>. </p>
<blockquote>
<p>多模态病理图像在临床诊断中通常很受欢迎，但基于计算机视觉的多模态图像辅助诊断在模态融合方面面临挑战，特别是在缺乏专家标注数据的情况下。为了实现多模态图像在标签稀缺情况下的模态融合，我们提出了一种新颖的“预训练+微调”框架，用于多模态半监督医学图像分类。具体来说，我们提出了一个协同学习的预训练框架，包括一致性、重建和对齐学习。通过将一种模态视为另一种模态的增强样本，我们实现了自监督学习预训练，提高了基线模型的特征表示能力。然后，我们设计了针对多模态融合的微调方法。在微调阶段，我们设置不同的编码器从原始模态中提取特征，并提供一个多模态融合编码器进行融合模态。此外，我们还提出了一种多模态融合特征分布转移方法，这减轻了由于缺少标记样本导致的预测不确定性和过拟合风险。我们在公开可用的胃镜图像数据集Kvasir和Kvasirv2上进行了大量实验。定量和定性结果表明，所提出的方法优于当前最先进的分类方法。代码将在以下网址发布：<a target="_blank" rel="noopener" href="https://github.com/LQH89757/MICS">https://github.com/LQH89757/MICS</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17492v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像多模态诊断面临标签稀缺和模态融合的挑战。为此，我们提出了一种新型的“预训练+微调”框架，实现半监督医学图像多模态分类。通过一致性、重建和对齐学习的预训练框架，提高模型的特征表示能力，并设计了一种针对模态融合的微调方法。在公开可用的胃镜图像数据集Kvasir和Kvasirv2上进行了实验验证，该方法优于当前最先进的分类方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态医学图像诊断面临标签稀缺和模态融合的挑战。</li>
<li>提出了一种新型的“预训练+微调”框架，用于半监督医学图像多模态分类。</li>
<li>通过预训练框架提高模型的特征表示能力，采用一致性、重建和对齐学习的方法。</li>
<li>设计了一种针对模态融合的微调方法，包括不同编码器提取特征、融合模态的编码器以及特征分布转换方法。</li>
<li>特征分布转换方法能减轻因缺乏标签样本导致的预测不确定性和过拟合风险。</li>
<li>在公开数据集Kvasir和Kvasirv2上进行了实验验证，证明了该方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c39d76aeca17ca043c0f8c2c67fe6a78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058854&auth_key=1760058854-0-0-6fe5a29c633f76600b6b9f077c47088c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-853f46e2f337756deaffa2eb56b7d644~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058861&auth_key=1760058861-0-0-72baa563ed1e16c5a0bd4f133ed4dbfd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23189dc0c6f43637ea0f48bb749b02ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058868&auth_key=1760058868-0-0-7bb72e43a2cb4840b080887c142e5e4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-faf309fa88fd6909710f3eb0a3072180~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058875&auth_key=1760058875-0-0-7ffb4eef16414c50bca42e809c32d84c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a887e7183afb0d87736d30b63d8e4e66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058882&auth_key=1760058882-0-0-35fb47aeb360bab0b00256f08d492eac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Implicit-Neural-Representations-of-Intramyocardial-Motion-and-Strain"><a href="#Implicit-Neural-Representations-of-Intramyocardial-Motion-and-Strain" class="headerlink" title="Implicit Neural Representations of Intramyocardial Motion and Strain"></a>Implicit Neural Representations of Intramyocardial Motion and Strain</h2><p><strong>Authors:Andrew Bell, Yan Kit Choi, Steffen E Petersen, Andrew King, Muhummad Sohaib Nazir, Alistair A Young</strong></p>
<p>Automatic quantification of intramyocardial motion and strain from tagging MRI remains an important but challenging task. We propose a method using implicit neural representations (INRs), conditioned on learned latent codes, to predict continuous left ventricular (LV) displacement – without requiring inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined error in global circumferential (2.86%) and radial (6.42%) strain compared to three deep learning baselines. In addition, our method is $\sim$380$\times$ faster than the most accurate baseline. These results highlight the suitability of INR-based models for accurate and scalable analysis of myocardial strain in large CMR datasets. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/andrewjackbell/Displacement-INR">https://github.com/andrewjackbell/Displacement-INR</a> </p>
<blockquote>
<p>心肌内运动的自动量化以及从标记MRI中获得的应变仍然是一项重要且具有挑战性的任务。我们提出了一种使用基于学习潜在代码的条件隐神经表示（INR）来预测左心室（LV）连续位移的方法，而无需在推理时间进行优化。我们在452例英国生物银行测试案例上对我们的方法进行了评估，与三个深度学习基线相比，我们的方法达到了最佳的跟踪精度（2.14毫米RMSE），并且在全球圆周应变（2.86%）和径向应变（6.42%）方面的组合误差最低。此外，我们的方法比最准确的基线快约380倍。这些结果突出了INR模型在大型CMR数据集中进行心肌应变准确和可伸缩分析的适用性。代码可在<a target="_blank" rel="noopener" href="https://github.com/andrewjackbell/Displacement-INR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/andrewjackbell/Displacement-INR找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09004v4">PDF</a> STACOM 2025 @ MICCAI</p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种使用隐神经表示（INR）的方法，通过学习潜在代码进行预测左心室位移，无需进行推理时间优化。该方法在测试数据集上表现出良好的跟踪精度和快速的计算速度，为大规模心脏磁共振数据集中心肌应变分析提供了准确且可量化的工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文中提出了一种基于隐神经表示（INR）的方法，用于自动量化心肌运动和应变。</li>
<li>该方法能够在不需要推理时间优化的情况下预测左心室位移。</li>
<li>在UK Biobank测试数据集上，该方法的跟踪精度达到最佳（RMSE为2.14mm）。</li>
<li>与其他深度学习模型相比，该方法在全球圆周和径向应变方面的综合误差最低。</li>
<li>该方法的计算速度比最准确的基线模型快约380倍。</li>
<li>实验结果证明了隐神经表示模型在心肌应变分析中的准确性和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4989985cdc59aa21dbadf0a74c745d81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058890&auth_key=1760058890-0-0-0cd3c7cf33ecd579a964387b9027d770&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df360f8497b51738373884d9168fe1ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058897&auth_key=1760058897-0-0-2215b5d6afce407c0aa97914d66cebfd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Clustering-methods-for-Categorical-Time-Series-and-Sequences-A-scoping-review"><a href="#Clustering-methods-for-Categorical-Time-Series-and-Sequences-A-scoping-review" class="headerlink" title="Clustering methods for Categorical Time Series and Sequences : A scoping   review"></a>Clustering methods for Categorical Time Series and Sequences : A scoping   review</h2><p><strong>Authors:Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, François Petit</strong></p>
<p>Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.   Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families – distance-based, feature-based, and model-based – and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.   Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( <a target="_blank" rel="noopener" href="https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/">https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/</a> )   Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.   Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases. </p>
<blockquote>
<p>目标：旨在为分类时间序列（CTS）的聚类方法提供概述。CTS是常见于流行病学、社会学、生物学和市场营销等领域的一种数据结构，并针对数据特性支持方法选择。方法：我们搜索了PubMed、Web of Science和Google Scholar，从创刊至2024年11月的文献，以识别和评估CTS的聚类技术。方法被分为三大类别——基于距离的、基于特征的、基于模型的，并评估了它们处理数据挑战的能力，例如可变序列长度、多元数据、连续时间、缺失数据、时间恒定协变量和大数据量。结果：在14607项研究中，我们纳入了124篇文章，描述了129种方法，涉及人工智能、社会科学和流行病学等领域。基于距离的方法，尤其是使用最佳匹配的方法最为普遍，共有56种方法。我们确定了28种基于模型的方法，在应对多元数据、连续时间和时间恒定协变量等复杂数据结构方面表现出较高的灵活性。我们还记录了45种基于特征的方法，这些方法在平均情况下更具可扩展性但灵活性较差。开发了一个可搜索的Web应用程序，以便根据数据集特性促进方法选择（ <a target="_blank" rel="noopener" href="https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/">https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/</a> ）。讨论：虽然基于距离的方法占主导地位，但基于模型的方法提供了最丰富的建模潜力，但扩展性较差。基于特征的方法更侧重于性能而非灵活性，对复杂数据结构的支持有限。结论：本综述突出了CTS聚类中的方法多样性和空白。提出的分类旨在指导研究人员为其特定用例选择合适的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07885v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文概述了针对类别时间序列（CTS）数据的聚类方法，该类数据广泛存在于流行病学、社会学、生物学和市场营销等领域。文章通过检索数据库和文献，对CTS聚类方法进行了分类和评估，包括距离基础、特征基础和模型基础三大类，并针对数据特性进行了方法选择。研究发现距离基础方法最为普遍，模型基础方法在处理复杂数据结构方面表现出较高灵活性，而特征基础方法则更侧重于性能。文章最后指出了聚类方法的多样性和现有差距，旨在为研究者选择合适的方法提供指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提供了类别时间序列（CTS）聚类方法的概述，旨在指导研究者根据特定用例选择合适的方法。</li>
<li>通过文献检索，对CTS聚类方法进行了距离基础、特征基础和模型基础三大类的分类。</li>
<li>距离基础方法最为普遍，但模型基础方法在处理复杂数据结构方面表现出较高灵活性。</li>
<li>特征基础方法更侧重于性能，但在处理复杂数据结构上有限制。</li>
<li>聚类方法的选择需考虑数据特性，如序列长度、多元数据、连续时间、缺失数据等。</li>
<li>文章指出当前聚类方法的多样性和现有差距，为未来的研究提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2c2d40c3ea500071a0dfb24dea097fbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058904&auth_key=1760058904-0-0-2e7262386e62c0544a88be759840fa10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d64417e50c09a7080e97a466091e8ba0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058911&auth_key=1760058911-0-0-8499f24bd299a0a3b35a057bbd021dde&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12"><a href="#Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12" class="headerlink" title="Robust Pan-Cancer Mitotic Figure Detection with YOLOv12"></a>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</h2><p><strong>Authors:Raphaël Bourgade, Guillaume Balezo, Thomas Walter</strong></p>
<p>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data. </p>
<blockquote>
<p>有丝分裂图像在肿瘤病理学中代表了关键的组织学预后特征，为理解肿瘤的侵袭性和增殖提供了重要依据。然而，它们的识别仍然具有挑战性，即使是经验丰富的病理学家之间也存在显著的观察者间变异。为了解决这一问题，有丝分裂域泛化（MIDOG）2025挑战赛是国际竞赛的第三版，旨在开发稳健的有丝分裂检测算法。在本文中，我们提出了一种基于最新YOLOv12目标检测架构的有丝分裂图像检测方法。我们的方法在初步测试集（仅热点）上达到了0.801的F1分数，并在最终测试排行榜上以0.7216的F1分数排名第二，且无需依赖外部数据，即可在复杂且异质的整张幻灯片区域进行检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02593v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于最新YOLOv12目标检测架构的细胞分裂图像检测方法，该方法是国际第三次细胞分裂检测挑战赛的重要成果之一。本文方法可以在复杂的全滑区域中准确检测细胞分裂图像，且无需依赖外部数据，在初步测试集上取得了F1分数为0.801的好成绩，并在最终测试中排名第二，展现了其在复杂环境下的稳健性和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>细胞分裂图像检测是肿瘤病理学中的关键特征，对肿瘤侵袭性和增殖性的评估至关重要。</li>
<li>细胞分裂检测面临观察者间差异大的挑战，国际上的MIDOG挑战赛旨在开发稳健的细胞分裂检测算法。</li>
<li>本文提出了一种基于YOLOv12架构的细胞分裂图像检测方法，初步测试成绩优异。</li>
<li>该方法能在复杂的全滑区域中准确检测细胞分裂图像，展示了其在复杂环境下的稳健性和可靠性。</li>
<li>该方法在最终测试中排名第二，表明其性能优异且具有潜力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02593">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-08ed8a7559fa45d662d81ab8eac00c52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058919&auth_key=1760058919-0-0-59277937d61056146672dad0324846a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-761c48e32872be873d51812cbc41c716~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058927&auth_key=1760058927-0-0-9e504eb5e869bd7a0d593c79728e9c00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09c94b7a55797b71b9b435ed72460743~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058934&auth_key=1760058934-0-0-43ed6b4a24152fb1f2e3c27f2447d4de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-and-Multi-centric-Head-and-Neck-Cancer-Dataset-for-Segmentation-Diagnosis-and-Outcome-Prediction"><a href="#A-Multimodal-and-Multi-centric-Head-and-Neck-Cancer-Dataset-for-Segmentation-Diagnosis-and-Outcome-Prediction" class="headerlink" title="A Multimodal and Multi-centric Head and Neck Cancer Dataset for   Segmentation, Diagnosis and Outcome Prediction"></a>A Multimodal and Multi-centric Head and Neck Cancer Dataset for   Segmentation, Diagnosis and Outcome Prediction</h2><p><strong>Authors:Numan Saeed, Salma Hassan, Shahad Hardan, Ahmed Aly, Darya Taratynova, Umair Nawaz, Ufaq Khan, Muhammad Ridzuan, Vincent Andrearczyk, Adrien Depeursinge, Yutong Xie, Thomas Eugene, Raphaël Metz, Mélanie Dore, Gregory Delpon, Vijay Ram Kumar Papineni, Kareem Wahid, Cem Dede, Alaa Mohamed Shawky Ali, Carlos Sjogreen, Mohamed Naser, Clifton D. Fuller, Valentin Oreiller, Mario Jreige, John O. Prior, Catherine Cheze Le Rest, Olena Tankyevych, Pierre Decazes, Su Ruan, Stephanie Tanadini-Lang, Martin Vallières, Hesham Elhalawani, Ronan Abgral, Romain Floch, Kevin Kerleguer, Ulrike Schick, Maelle Mauguen, David Bourhis, Jean-Christophe Leclere, Amandine Sambourg, Arman Rahmim, Mathieu Hatt, Mohammad Yaqub</strong></p>
<p>We present a publicly available multimodal dataset for head and neck cancer research, comprising 1123 annotated Positron Emission Tomography&#x2F;Computed Tomography (PET&#x2F;CT) studies from patients with histologically confirmed disease, acquired from 10 international medical centers. All studies contain co-registered PET&#x2F;CT scans with varying acquisition protocols, reflecting real-world clinical diversity from a long-term, multi-institution retrospective collection. Primary gross tumor volumes (GTVp) and involved lymph nodes (GTVn) were manually segmented by experienced radiation oncologists and radiologists following established guidelines. We provide anonymized NifTi files, expert-annotated segmentation masks, comprehensive clinical metadata, and radiotherapy dose distributions for a patient subset. The metadata include TNM staging, HPV status, demographics, long-term follow-up outcomes, survival times, censoring indicators, and treatment information. To demonstrate its utility, we benchmark three key clinical tasks: automated tumor segmentation, recurrence-free survival prediction, and HPV status classification, using state-of-the-art deep learning models like UNet, SegResNet, and multimodal prognostic frameworks. </p>
<blockquote>
<p>我们提供了一个公开可用的头颈部癌症研究多模式数据集，包含从国际上10个医疗中心采集的经组织病理学确诊疾病的患者的PET&#x2F;CT研究注解共1123项。所有研究都包含使用不同采集协议的共注册PET&#x2F;CT扫描，反映长期多机构回顾性收集的来自真实世界的临床多样性。原发肿瘤体积（GTVp）和受累淋巴结（GTVn）由经验丰富的放射肿瘤学家和放射科医生按照既定指南进行手动分割。我们提供匿名化的NifTi文件、专家注释的分割掩膜、全面的临床元数据以及患者子集的放射治疗剂量分布。元数据包括TNM分期、HPV状态、人口统计学信息、长期随访结果、生存时间、审查指标和治疗信息。为了证明其实用性，我们对三个关键的临床任务进行了基准测试：使用UNet、SegResNet等最新深度学习模型和多种模式预后框架进行自动肿瘤分割、无复发生存预测和HPV状态分类。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00367v3">PDF</a> 10 pages, 5 figures. Numan Saeed is the corresponding author. Numan   Saeed, Salma Hassan and Shahad Hardan contributed equally to this work.   Project page: <a target="_blank" rel="noopener" href="https://hecktor25.grand-challenge.org/">https://hecktor25.grand-challenge.org/</a></p>
<p><strong>Summary</strong></p>
<p>这是一个关于头颈癌研究的多模式数据集介绍，包含来自十个国际医疗中心的1123份经过注释的PET&#x2F;CT研究。数据集中包含了手动分割的主要肿瘤体积和涉及的淋巴结，以及包括TNM分期、HPV状态、人口统计学信息、长期随访结果等在内的综合临床元数据。此外，还提供了用于患者子集的匿名NifTi文件、专家注释的分割掩模、放射治疗剂量分布图。数据集可用于自动化肿瘤分割、无复发生存预测和HPV状态分类等任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该数据集是一个公开可用的多模式数据集，用于头颈癌研究，涵盖了国际十个医疗中心的PET&#x2F;CT研究数据。</li>
<li>数据集中包含了手动分割的主要肿瘤体积和涉及的淋巴结。</li>
<li>数据集包含了丰富的临床元数据，如TNM分期、HPV状态等。</li>
<li>数据集提供了匿名化的NifTi文件、专家注释的分割掩模和放射治疗剂量分布图。</li>
<li>数据集可用于自动化肿瘤分割任务。</li>
<li>数据集可用于无复发生存预测任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-53a4613a106291df423068d41479c866~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058942&auth_key=1760058942-0-0-7a6f36eafba36a191267233811f93270&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a3c62176d001aaee56989b07310c820~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058950&auth_key=1760058950-0-0-e9ff27c36416981e49b35bd8e339d4e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93bcd2db7606ea6c5a5c1670cb6ee4df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058958&auth_key=1760058958-0-0-4fb41d69bf31bae823ae2e273669afe2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-386283296385cc76b0bb5beca5f5b4aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058965&auth_key=1760058965-0-0-658c441cd41aa455552a0eec7cf49f0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1045739675af2de135264be2b7351a50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058972&auth_key=1760058972-0-0-3955bfb0ed73ea8a5fe6b7d59b2fecab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c6ef97489a5e841a0334a8486915b04~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058979&auth_key=1760058979-0-0-1001552e1687747e44d3bd0afb82c561&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Deep-Learning-for-Phyllodes-Tumor-Classification-from-Ultrasound-and-Clinical-Data"><a href="#Multimodal-Deep-Learning-for-Phyllodes-Tumor-Classification-from-Ultrasound-and-Clinical-Data" class="headerlink" title="Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data"></a>Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data</h2><p><strong>Authors:Farhan Fuad Abir, Abigail Elliott Daly, Kyle Anderman, Tolga Ozmen, Laura J. Brattain</strong></p>
<p>Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are difficult to classify preoperatively due to their radiological similarity to benign fibroadenomas. This often leads to unnecessary surgical excisions. To address this, we propose a multimodal deep learning framework that integrates breast ultrasound (BUS) images with structured clinical data to improve diagnostic accuracy. We developed a dual-branch neural network that extracts and fuses features from ultrasound images and patient metadata from 81 subjects with confirmed PTs. Class-aware sampling and subject-stratified 5-fold cross-validation were applied to prevent class imbalance and data leakage. The results show that our proposed multimodal method outperforms unimodal baselines in classifying benign versus borderline&#x2F;malignant PTs. Among six image encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and 0.7294, respectively. This study demonstrates the potential of multimodal AI to serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and improving clinical decision-making in breast tumor management. </p>
<blockquote>
<p>叶状肿瘤（PTs）是一种罕见的纤维上皮性乳腺病变，由于其与良性纤维腺瘤的放射学相似性，术前难以分类，这常常导致不必要的手术切除。为了解决这一问题，我们提出了一种多模式深度学习框架，该框架结合了乳腺超声（BUS）图像和结构化临床数据，以提高诊断准确性。我们开发了一个双分支神经网络，从超声图像和来自81名已确诊PTs患者的患者元数据中提取并融合特征。应用类别感知采样和受试者分层5倍交叉验证，以防止类别不平衡和数据泄露。结果表明，我们提出的多模式方法在分类良性与边界性或恶性PTs时优于单模式基线方法。在六种图像编码器中，ConvNeXt和ResNet18在多模式设置中表现最佳，AUC-ROC得分分别为0.9427和0.9349，F1得分分别为0.6720和0.7294。本研究展示了多模式人工智能作为非侵入性诊断工具的潜力，可以减少不必要的活检，提高乳腺肿瘤管理的临床决策水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00213v2">PDF</a> IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS   BSN 2025)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种多模态深度学习框架，该框架结合了乳腺超声图像和结构化临床数据，旨在提高叶状肿瘤的诊断准确性。通过开发一个双分支神经网络，该网络从超声图像和患者元数据中提取特征，并从81名确诊为叶状肿瘤的受试者中获取数据。该研究采用类感知采样和分层5倍交叉验证，以避免类别不平衡和数据泄露问题。结果显示，所提出的多模态方法在分类良性与边界性或恶性叶状肿瘤方面优于单模态基线。在多种图像编码器中，ConvNeXt和ResNet18在多模态设置中表现最佳，AUC-ROC得分分别为0.9427和0.9349，F1分数分别为0.6720和0.7294。该研究证明了多模态人工智能作为非侵入性诊断工具的潜力，有望降低不必要的活检率，提高乳腺肿瘤管理的临床决策水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种多模态深度学习框架，旨在提高叶状肿瘤（PTs）的诊断准确性。</li>
<li>通过结合乳腺超声图像和结构化临床数据，该框架能够更准确地分类PTs。</li>
<li>研究采用双分支神经网络，能够从超声图像和患者元数据中提取特征。</li>
<li>实验结果展示了所提出的多模态方法优于单模态基线方法在分类PTs方面的表现。</li>
<li>在多种图像编码器中，ConvNeXt和ResNet18表现出最佳性能。</li>
<li>多模态人工智能的应用有望降低不必要的活检率，提高临床决策水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-75b1168716067cd0415107dcacbb12eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058986&auth_key=1760058986-0-0-00f1b99cdd9671050f221291ba19582e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f6f98aad44ca8657d3155a7dbcf2fb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058993&auth_key=1760058993-0-0-ca9f039054f73ac86ef3f202b26602b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc994803e7bf11907eeb6ae4f904a24a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058999&auth_key=1760058999-0-0-399717b399d05c2509b8a2a2a56ecf0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f19872b5568a29c5f2ff8b9fa79261cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059006&auth_key=1760059006-0-0-7ce4c03f98bc13dcc525783664bb0e33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-080aa3b5008d9ae20dca9fcb207b15a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059012&auth_key=1760059012-0-0-f62d9a18749a72cb4a1ec522a2e03903&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e9ce53279e53895dbc66f8c719d4f0ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059019&auth_key=1760059019-0-0-c3baf0f5c0fd4fe31004118e3bd1c5ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cross-Cancer-Knowledge-Transfer-in-WSI-based-Prognosis-Prediction"><a href="#Cross-Cancer-Knowledge-Transfer-in-WSI-based-Prognosis-Prediction" class="headerlink" title="Cross-Cancer Knowledge Transfer in WSI-based Prognosis Prediction"></a>Cross-Cancer Knowledge Transfer in WSI-based Prognosis Prediction</h2><p><strong>Authors:Pei Liu, Luping Ji, Jiaxiang Gou, Xiangxiang Zeng</strong></p>
<p>Whole-Slide Image (WSI) is an important tool for estimating cancer prognosis. Current studies generally follow a conventional cancer-specific paradigm where one cancer corresponds to one model. However, it naturally struggles to scale to rare tumors and cannot utilize the knowledge of other cancers. Although a multi-task learning-like framework has been studied recently, it usually has high demands on computational resources and needs considerable costs in iterative training on ultra-large multi-cancer WSI datasets. To this end, this paper makes a paradigm shift to knowledge transfer and presents the first preliminary yet systematic study on cross-cancer prognosis knowledge transfer in WSIs, called CROPKT. It has three major parts: (i) we curate a large dataset (UNI2-h-DSS) with 26 cancers and use it to measure the transferability of WSI-based prognostic knowledge across different cancers (including rare tumors); (ii) beyond a simple evaluation merely for benchmark, we design a range of experiments to gain deeper insights into the underlying mechanism of transferability; (iii) we further show the utility of cross-cancer knowledge transfer, by proposing a routing-based baseline approach (ROUPKT) that could often efficiently utilize the knowledge transferred from off-the-shelf models of other cancers. We hope CROPKT could serve as an inception and lay the foundation for this nascent paradigm, i.e., WSI-based prognosis prediction with cross-cancer knowledge transfer. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/liupei101/CROPKT">https://github.com/liupei101/CROPKT</a>. </p>
<blockquote>
<p>全切片图像（WSI）是评估癌症预后的重要工具。当前的研究通常遵循一种传统的特定癌症模式，即一种癌症对应一个模型。然而，这种模式在应对罕见肿瘤时自然会遇到困难，并且无法利用其他癌症的知识。尽管最近已经研究了类似多任务学习的框架，但它通常对计算资源有很高的要求，并且需要在超大型多癌症WSI数据集上进行迭代训练，成本相当高。为此，本文进行了范式转变，转向知识转移，并对WSI中的跨癌症预后知识转移进行了首次初步但系统的研究，称为CROPKT。它主要包括三个部分：（i）我们整理了一个包含26种癌症的大型数据集（UNI2-h-DSS），并用来衡量不同癌症（包括罕见肿瘤）之间基于WSI的预后知识的可转移性；（ii）不仅为了基准测试而进行简单评估，我们还设计了一系列实验，以深入了解可转移性的内在机制；（iii）我们进一步展示了跨癌症知识转移的实用性，通过提出一种基于路由的基线方法（ROUPKT），该方法可以高效地利用其他癌症的现成模型所传递的知识。我们希望CROPKT能够作为一个开端，为这个新兴范式奠定基础，即基于WSI的具有跨癌症知识转移的预后预测。我们的源代码可在[<a target="_blank" rel="noopener" href="https://github.com/liupei101/CROPKT%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/liupei101/CROPKT找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13482v2">PDF</a> 20 pages (11 figures and 6 tables)</p>
<p><strong>Summary</strong><br>    该研究突破了传统的癌症特异性模式限制，首次系统性研究了跨癌预后知识转移在WSI中的应用，命名为CROPKT。研究内容包括：建立包含26种癌症的大型数据集UNI2-h-DSS，衡量WSI预后知识在不同癌症间的可转移性；设计一系列实验深入了解可转移性的内在机制；提出基于路由的基线方法ROUPKT，展示跨癌知识转移的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究突破了传统癌症特异性模式的限制，建立了跨癌预后知识转移的新模式（CROPKT）。</li>
<li>建立了包含26种癌症的大型数据集UNI2-h-DSS，用于衡量WSI预后知识在不同癌症间的可转移性。</li>
<li>通过一系列实验深入了解可转移性的内在机制。</li>
<li>提出了基于路由的基线方法ROUPKT，能够高效利用其他癌症的已训练模型进行知识转移。</li>
<li>CROPKT为WSI为基础的预后预测提供了新的研究视角，即跨癌知识转移。</li>
<li>该研究提高了处理罕见肿瘤的能力，并能利用其他癌症的知识。</li>
<li>研究的源代码已公开，便于后续研究使用与参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0eaafa722989e934ddad61cfa9e010cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059026&auth_key=1760059026-0-0-c8e60cc954247e34c505fb09f44c310e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91a2e3bdf832da061cfecac414b8d5c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059034&auth_key=1760059034-0-0-8275b629b7cb04ce11c3fb803fa56c34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a397f857b578111c73e5923e759229a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059041&auth_key=1760059041-0-0-41c8b37bf0b9f35b91ac9c62e10f3d1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14320e2ce44ddf7c0d1309e987997803~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059048&auth_key=1760059048-0-0-c6379b5f9a2dfecbf6e25aa715e08392&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f930fd0eb6762b268e3c1439b5b2c74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059056&auth_key=1760059056-0-0-1d2da794f026e3f99bac3b202e16a9d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding"></a>RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding</h2><p><strong>Authors:Tianchen Fang, Guiru Liu</strong></p>
<p>Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image-text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding. </p>
<blockquote>
<p>医学图像理解在实现自动化诊断和基于数据的临床决策支持中起着至关重要的作用。然而，其进展受到两个主要挑战的限制：高质量标注医学数据的有限可用性，以及对全局图像特征的过度依赖，这往往导致忽略细微但临床上重要的病理区域。为了解决这些问题，我们引入了RegionMed-CLIP，这是一个区域感知的多模式对比学习框架，它显式地结合了局部病理信号和整体语义表示。我们的方法的核心是一个创新的兴趣区域（ROI）处理器，它自适应地集成精细的局部特征与全局上下文，辅以一种增强分层多模式对齐的渐进式训练策略。为了实现大规模的区域级别表示学习，我们构建了MedRegion-500k，这是一个具有广泛区域注释和多层临床描述的综合医学图像-文本语料库。在图像-文本检索、零样本分类和视觉问答任务上的大量实验表明，RegionMed-CLIP始终大幅超越了最先进的视觉语言模型。我们的结果强调了区域感知对比预训练的关键重要性，并将RegionMed-CLIP定位为推动多模式医学图像理解发展的稳健基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05244v2">PDF</a> Upon further review, we identified that our dataset requires   optimization to ensure research reliability and accuracy. Additionally,   considering the target journal’s latest submission policies, we believe   comprehensive manuscript revisions are necessary</p>
<p><strong>Summary</strong><br>     医学图像理解在自动化诊断和数据驱动的临床决策支持中起着关键作用，但面临高质量标注医学数据有限和过度依赖全局图像特征两大挑战。为解决这个问题，我们提出了RegionMed-CLIP，一个结合局部病理信号和整体语义表示的区域感知多模态对比学习框架。其核心是创新的感兴趣区域（ROI）处理器，能自适应地集成细粒度区域特征与全局上下文，辅以渐进式训练策略，提高分层多模态对齐。为支持大规模区域级表示学习，我们构建了MedRegion-500k，一个包含丰富区域标注和多层临床描述的大规模医学图像文本语料库。实验表明，RegionMed-CLIP在图像文本检索、零样本分类和视觉问答任务上均显著超越现有视觉语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>医学图像理解在自动化诊断与临床决策支持中起关键作用。</li>
<li>两大挑战：高质量标注医学数据有限和过度依赖全局图像特征。</li>
<li>RegionMed-CLIP框架结合局部病理信号和整体语义表示。</li>
<li>ROI处理器自适应集成细粒度区域特征与全局上下文。</li>
<li>渐进式训练策略提高分层多模态对齐。</li>
<li>MedRegion-500k医学图像文本语料库支持大规模区域级表示学习。</li>
<li>RegionMed-CLIP在多项任务上表现超越现有视觉语言模型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4409389a475b3efc3467f6322ecaf107~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059063&auth_key=1760059063-0-0-4dc131135a8a1214c92cdef57c95d35e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0038387d1731557539231913673eb74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059071&auth_key=1760059071-0-0-4462df93f8d6a0d8438eceddd3cfb0e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b048fbad3f03f4ac0c39114b067eb10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059078&auth_key=1760059078-0-0-520b0d6b2a5df4f8aa95fa342864ee02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e98bdf7cef64cadde7f2ee8c5e2dc8cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059085&auth_key=1760059085-0-0-e2da65670a2326c875c5dbce894c2447&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a93e3776597773699674e91737962a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059092&auth_key=1760059092-0-0-231fb62e238c258026cb01260764bab4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RIS-LAD-A-Benchmark-and-Model-for-Referring-Low-Altitude-Drone-Image-Segmentation"><a href="#RIS-LAD-A-Benchmark-and-Model-for-Referring-Low-Altitude-Drone-Image-Segmentation" class="headerlink" title="RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image   Segmentation"></a>RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image   Segmentation</h2><p><strong>Authors:Kai Ye, YingShi Luan, Zhudi Chen, Guangyue Meng, Pingyang Dai, Liujuan Cao</strong></p>
<p>Referring Image Segmentation (RIS), which aims to segment specific objects based on natural language descriptions, plays an essential role in vision-language understanding. Despite its progress in remote sensing applications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored. Existing datasets and methods are typically designed for high-altitude and static-view imagery. They struggle to handle the unique characteristics of LAD views, such as diverse viewpoints and high object density. To fill this gap, we present RIS-LAD, the first fine-grained RIS benchmark tailored for LAD scenarios. This dataset comprises 13,871 carefully annotated image-text-mask triplets collected from realistic drone footage, with a focus on small, cluttered, and multi-viewpoint scenes. It highlights new challenges absent in previous benchmarks, such as category drift caused by tiny objects and object drift under crowded same-class objects. To tackle these issues, we propose the Semantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly injecting all linguistic features, SAARN decomposes and routes semantic information to different stages of the network. Specifically, the Category-Dominated Linguistic Enhancement (CDLE) aligns visual features with object categories during early encoding, while the Adaptive Reasoning Fusion Module (ARFM) dynamically selects semantic cues across scales to improve reasoning in complex scenes. The experimental evaluation reveals that RIS-LAD presents substantial challenges to state-of-the-art RIS algorithms, and also demonstrates the effectiveness of our proposed model in addressing these challenges. The dataset and code will be publicly released soon at: <a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/">https://github.com/AHideoKuzeA/RIS-LAD/</a>. </p>
<blockquote>
<p>基于自然语言描述的目标图像分割（RIS）在视觉语言理解中扮演着至关重要的角色，其旨在根据自然语言描述来分割特定对象。尽管其在遥感应用方面取得了进展，但低空无人机（LAD）场景中的RIS仍然鲜有研究。现有的数据集和方法通常针对高空和静态视图图像设计，它们难以处理LAD视图的独特特征，例如多样的视角和高密度的目标。为了填补这一空白，我们推出了RIS-LAD，这是针对LAD场景定制的首个精细的RIS基准数据集。该数据集包含从真实的无人机视频中收集的13,871个经过仔细注释的图像-文本-掩膜三元组，专注于小、杂乱和多视角的场景。它突出了以前基准测试中不存在的新的挑战，例如由于微小物体引起的类别漂移和在拥挤的同类物体下物体的漂移。为了解决这些问题，我们提出了语义感知自适应推理网络（SAARN）。SAARN不是统一地注入所有语言特征，而是分解和将语义信息路由到网络的不同阶段。具体来说，类别主导的语言增强（CDLE）在早期编码时将视觉特征与对象类别对齐，而自适应推理融合模块（ARFM）则动态选择跨尺度的语义线索，以改善复杂场景中的推理。实验评估表明，RIS-LAD给现有的RIS算法带来了实质性的挑战，同时也证明了我们提出的模型在应对这些挑战时的有效性。数据集和代码将很快在<a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/AHideoKuzeA/RIS-LAD/公开发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20920v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对低空无人机（LAD）场景的参照图像分割（RIS）研究存在空白。现有数据集和方法主要面向高空和静态图像，难以处理LAD视角的独特特征，如不同视角和高物体密度。为此，我们推出RIS-LAD，首个针对LAD场景的精细RIS基准数据集。该数据集包含从真实无人机影像中收集的13,871个精心标注的图像-文本-掩膜三元组，重点关注小、杂乱、多视角场景。我们提出语义感知自适应推理网络（SAARN）来解决新问题，如因小物体引起的类别漂移和在拥挤的同类物体中的对象漂移。实验评估显示，RIS-LAD为当前RIS算法带来重大挑战，同时验证了我们的模型在应对这些挑战时的有效性。数据集和代码将在 <a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/">https://github.com/AHideoKuzeA/RIS-LAD/</a> 公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低空无人机（LAD）场景的参照图像分割（RIS）研究尚待深入探索。</li>
<li>现有数据集和方法主要面向高空和静态图像，难以适应LAD视角的多样性。</li>
<li>推出RIS-LAD数据集，专为LAD场景设计，包含从真实无人机影像中精心标注的图像-文本-掩膜三元组。</li>
<li>数据集重点关注小、杂乱、多视角场景的分割问题。</li>
<li>提出语义感知自适应推理网络（SAARN）来解决类别漂移和对象漂移等问题。</li>
<li>实验评估显示，RIS-LAD对现有RIS算法构成重大挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e17ea3009e8ef64736ad4532d04cac83~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059100&auth_key=1760059100-0-0-52f860449edcdd4fc674842386bc4740&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1dcbd3aab3d07317e29a2eaa2ffe30b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059107&auth_key=1760059107-0-0-3bd6c2d92821d2c0aaca5412e5f7cb9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83adebd4854a35e929fc72b3c0b26601~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059114&auth_key=1760059114-0-0-2a559031f070a2769224ff129a25b76a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a0ca05b1720be445a7cbef0daeba518~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059121&auth_key=1760059121-0-0-d2dc72bada3226d54eba9c4e787d04df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-851c9efd0de3717aba74af02c499b959~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059127&auth_key=1760059127-0-0-8a12208bc14194dd700401ad9963c257&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee9a05bee21711f329b1996ce4be8152~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059134&auth_key=1760059134-0-0-4e45eb5887fff9bd624928c6d31d18f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27fb6ccd2928c28bd999a6ed44658101~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059142&auth_key=1760059142-0-0-2abc886e225140ae3daf541a32c9f6fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a1228b499ece1d667546645fc83d824b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059148&auth_key=1760059148-0-0-29f5792ba0892ff7e5ff5c90f98faa1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2290cdd0001759f7a29e91c7539076af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059155&auth_key=1760059155-0-0-18b5ed9e599e3c011151587b5d1e113c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DCFFSNet-Deep-Connectivity-Feature-Fusion-Separation-Network-for-Medical-Image-Segmentation"><a href="#DCFFSNet-Deep-Connectivity-Feature-Fusion-Separation-Network-for-Medical-Image-Segmentation" class="headerlink" title="DCFFSNet: Deep Connectivity Feature Fusion Separation Network for   Medical Image Segmentation"></a>DCFFSNet: Deep Connectivity Feature Fusion Separation Network for   Medical Image Segmentation</h2><p><strong>Authors:Mingda Zhang, Xun Ye, Ruixiang Tang, Haiyan Ding</strong></p>
<p>Medical image segmentation leverages topological connectivity theory to enhance edge precision and regional consistency. However, existing deep networks integrating connectivity often forcibly inject it as an additional feature module, resulting in coupled feature spaces with no standardized mechanism to quantify different feature strengths. To address these issues, we propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It introduces an innovative feature space decoupling strategy. This strategy quantifies the relative strength between connectivity features and other features. It then builds a deep connectivity feature fusion-separation architecture. This architecture dynamically balances multi-scale feature expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by 1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice) and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU). The results demonstrate that DCFFSNet exceeds existing mainstream methods across all metrics. It effectively resolves segmentation fragmentation and achieves smooth edge transitions. This significantly enhances clinical usability. </p>
<blockquote>
<p>医学图像分割利用拓扑连接理论来提高边缘精度和区域一致性。然而，现有的深度网络在集成连接性时通常强制将其注入作为附加特征模块，导致特征空间耦合，没有标准化的机制来量化不同特征的强度。为了解决这些问题，我们提出了DCFFSNet（双连接特征融合分离网络）。它引入了一种创新的特征空间解耦策略。该策略量化连接特征与其他特征之间的相对强度。然后，它建立了一个深度连接特征融合-分离架构。该架构动态平衡了多尺度特征表达。我们在ISIC2018、DSB2018和MoNuSeg数据集上进行了实验。在ISIC2018上，DCFFSNet在狄克系数和交并比方面分别比排名第二的最佳模型CMUNet高出1.3%和1.2%。在DSB2018上，它比TransUNet高出0.7%（狄克系数）和0.9%（交并比）。在MoNuSeg上，它比CSCAUNet高出0.8%（狄克系数）和0.9%（交并比）。结果表明，DCFFSNet在所有指标上都超过了现有的主流方法。它有效地解决了分割碎片问题，实现了平滑的边缘过渡。这显著提高了临床可用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18407v2">PDF</a> 16 pages , 11 figures</p>
<p><strong>Summary</strong><br>    医学图像分割利用拓扑连通性理论提高边缘精度和区域一致性。针对现有深度网络集成连通性时强行注入作为附加特征模块的问题，提出DCFFSNet（双连通特征融合分离网络）。引入特征空间解耦策略，量化连通性特征与其他特征的相对强度，建立深度连通特征融合分离架构，动态平衡多尺度特征表达。在ISIC2018、DSB2018和MoNuSeg数据集上的实验表明，DCFFSNet在各项指标上均超过主流方法，有效解决分割碎片化问题，实现平滑边缘过渡，显著提高临床实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割利用拓扑连通性理论提高边缘和区域一致性。</li>
<li>现有网络集成连通性存在特征空间耦合问题，缺乏量化不同特征强度的标准化机制。</li>
<li>DCFFSNet提出特征空间解耦策略，建立深度连通特征融合分离架构。</li>
<li>DCFFSNet在多个数据集上表现优异，超过主流方法。</li>
<li>DCFFSNet有效解决分割碎片化问题。</li>
<li>DCFFSNet实现平滑边缘过渡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18407">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bef15d846edf08a37fa1b33c8ccb89fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059162&auth_key=1760059162-0-0-eb1097d507e45613eed4b69f2453f5b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10c5c0a9b7d158d285f323debc8b7bc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059170&auth_key=1760059170-0-0-79c37c3b7c5491cf8b503607254aeba4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f434c2f03c4424da3a5a5faa536854d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059176&auth_key=1760059176-0-0-c32070af01d638cd1ca77d29c1ac18ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a16034ef4befa22ec49f66c59e5c509d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059183&auth_key=1760059183-0-0-eb68fdd49e7bbeb63588b50e4f4e0bb0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88bd4cfef6e3dd3ab100fa1e3b6c09bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059189&auth_key=1760059189-0-0-192b3c5e65c3820085b5a4957ced946b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation"><a href="#GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation" class="headerlink" title="GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation"></a>GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation</h2><p><strong>Authors:Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe</strong></p>
<p>Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at <a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix">https://github.com/hugocarlesso/GeMix</a> to foster reproducibility and further research. </p>
<blockquote>
<p>Mixup已成为图像分类中流行的数据增强策略，但其简单的像素级插值经常生成不真实的图像，这可能会阻碍学习，特别是在高风险的医学应用中。我们提出了GeMix，这是一个两阶段的框架，它用基于类别条件生成对抗网络（GANs）的学习感知插值替换了启发式混合方法。首先，我们在目标数据集上训练StyleGAN2-ADA生成器。在数据增强过程中，我们从偏向不同类别的Dirichlet先验中采样两个标签向量，并通过Beta分布系数将它们混合。然后，我们将生成器设置为该软标签的条件，合成沿连续类别流形的一致图像。我们在大规模的COVIDx-CT-3数据集上使用三种主干网络（ResNet-50、ResNet-101、EfficientNet-B0）对GeMix进行基准测试。当与真实数据结合时，我们的方法在所有主干网络上提高了宏观F1分数，降低了COVID-19检测的误报率。因此，GeMix可作为像素空间中Mixup的替代方案，提供更强的正则化和更高的语义保真度，而不会破坏现有的训练管道。我们已在<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/hugocarlesso/GeMix公开发布我们的代码，以促进可重复性和进一步研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15577v2">PDF</a> Accepted at CBMI 2025</p>
<p><strong>摘要</strong></p>
<pre><code>本文提出了一个名为GeMix的两阶段框架，用于图像分类中的增强策略。传统的混合策略使用像素级的插值产生不真实的图像，特别是在高风险的医学应用中可能阻碍学习。GeMix使用基于类条件生成对抗网络（GANs）的标签感知插值替代启发式混合方法。通过目标数据集训练StyleGAN2-ADA生成器，并使用偏向不同类的Dirichlet先验采样两个标签向量，通过Beta分布系数进行混合。然后将此软标签应用于生成器，合成沿着连续类流形分布视觉上连贯的图像。在大型COVIDx-CT-3数据集上进行的基准测试显示，与真实数据结合使用时，GeMix相对于传统混合方法提高了所有背骨网络的宏观F1分数，并降低了COVID-19检测的误报率。因此，GeMix可以作为像素空间混合的替代品，提供更强大的正则化和更高的语义保真度，而不会破坏现有的训练管道。我们公开发布了代码以促进可重复性和进一步研究。

**关键见解**

1. 传统混合策略常常产生不真实的图像，尤其是在医学应用中可能阻碍学习。
2. GeMix是一个基于类条件GANs的两阶段框架，通过标签感知插值替换启发式混合方法。
3. GeMix使用StyleGAN2-ADA生成器合成视觉上连贯的图像，这些图像沿着连续类流形分布。
4. 在大型COVIDx-CT-3数据集上进行的基准测试表明，GeMix提高了宏观F1分数并降低了COVID-19检测的误报率。
5. GeMix作为一种替代像素空间混合的方法，提供了更强的正则化和更高的语义保真度。
6. GeMix不会破坏现有的训练管道，可以公开发布的源代码促进可重复性和进一步研究。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-030bf440ac55c96263e57bc08421900a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059197&auth_key=1760059197-0-0-282ea6189dfa1f6cf3574372dcb5b0db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d29c3256c94661b424630d8f2a1417a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059204&auth_key=1760059204-0-0-4edc8e011174f7e7afc01f7b8bac10ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdc7ffdf5f932180a83612b9c0ec803a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059211&auth_key=1760059211-0-0-058600661bb7388d344721f7068cc293&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92a2067546ec4c4a0b1c309aa86006d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059218&auth_key=1760059218-0-0-141d1cee860c9ff63d71bdc2576a3ff5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Interpretability-Aware-Pruning-for-Efficient-Medical-Image-Analysis"><a href="#Interpretability-Aware-Pruning-for-Efficient-Medical-Image-Analysis" class="headerlink" title="Interpretability-Aware Pruning for Efficient Medical Image Analysis"></a>Interpretability-Aware Pruning for Efficient Medical Image Analysis</h2><p><strong>Authors:Nikita Malik, Pratinav Seth, Neeraj Kumar Singh, Chintan Chitroda, Vinay Kumar Sankarapu</strong></p>
<p>Deep learning has driven significant advances in medical image analysis, yet its adoption in clinical practice remains constrained by the large size and lack of transparency in modern models. Advances in interpretability techniques such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated Gradients make it possible to assess the contribution of individual components within neural networks trained on medical imaging tasks. In this work, we introduce an interpretability-guided pruning framework that reduces model complexity while preserving both predictive performance and transparency. By selectively retaining only the most relevant parts of each layer, our method enables targeted compression that maintains clinically meaningful representations. Experiments across multiple medical image classification benchmarks demonstrate that this approach achieves high compression rates with minimal loss in accuracy, paving the way for lightweight, interpretable models suited for real-world deployment in healthcare settings. </p>
<blockquote>
<p>深度学习已在医学图像分析方面取得显著进展，但在临床实践中应用仍受到现代模型体积庞大和不透明的限制。诸如DL-Backtrace、逐层相关性传播和集成梯度等解释技术的进展，使得评估经过医学成像任务训练的神经网络中各个组件的贡献成为可能。在这项工作中，我们引入了一个以解释性为指导的修剪框架，该框架能够在保持预测性能和透明度的同时，降低模型的复杂性。通过有选择地保留每层最相关的部分，我们的方法能够实现有针对性的压缩，同时保持对临床有意义的表示。在多个医学图像分类基准测试上的实验表明，该方法实现了较高的压缩率，并且精度损失较小，这为在医疗环境中部署轻便、可解释性强的模型铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08330v2">PDF</a> Accepted at The 1st MICCAI Workshop on Efficient Medical AI 2025</p>
<p><strong>Summary</strong></p>
<p>深度学习在医学图像分析领域取得了显著进展，但仍存在模型体积大、透明度不足等问题，限制了其在临床实践中的应用。采用DL-Backtrace、逐层相关性传播和集成梯度等解释性技术，可以评估神经网络中对医学成像任务训练各组成部分的作用。本研究引入了一种以解释性为指导的剪枝框架，旨在降低模型复杂度，同时保留预测性能和透明度。通过选择性保留每层最相关的部分，我们的方法能够实现有针对性的压缩，保持临床意义的表示。在多医学图像分类基准测试上的实验表明，该方法实现了高压缩率且准确性损失极小，为适合在医疗环境中实际部署的轻量级、可解释性模型铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在医学图像分析上取得显著进展，但实际应用中面临模型体积大、透明度不足的问题。</li>
<li>采用解释性技术（如DL-Backtrace、逐层相关性传播和集成梯度）以评估神经网络中各组成部分的作用。</li>
<li>引入了一种以解释性为指导的剪枝框架，旨在降低模型复杂度同时保留预测性能和透明度。</li>
<li>通过选择性保留神经网络层中最相关的部分，实现有针对性的模型压缩。</li>
<li>方法能够保持临床意义的表示，使医学图像分析更加透明和可解释。</li>
<li>实验证明，该方法在多个医学图像分类基准测试上实现了高压缩率和较小的准确性损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08330">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5b32d2387c6600cb81d2f099f01685e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059225&auth_key=1760059225-0-0-8ee5ddc0f8f3674d3a3dae53c0cf7f55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6771a99ec8ed1864c514585c714f9db1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059232&auth_key=1760059232-0-0-3d9318075fbf56038d2b8e470d9cc065&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5184e0306ed5daccb3477c130a268128~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059239&auth_key=1760059239-0-0-cde1427b475d91cabecfb3b3cc7d7da8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Information-Pursuit-for-Interpretable-and-Reliable-Medical-Image-Analysis"><a href="#Uncertainty-Aware-Information-Pursuit-for-Interpretable-and-Reliable-Medical-Image-Analysis" class="headerlink" title="Uncertainty-Aware Information Pursuit for Interpretable and Reliable   Medical Image Analysis"></a>Uncertainty-Aware Information Pursuit for Interpretable and Reliable   Medical Image Analysis</h2><p><strong>Authors:Md Nahiduzzaman, Steven Korevaar, Zongyuan Ge, Feng Xia, Alireza Bab-Hadiashar, Ruwan Tennakoon</strong></p>
<p>To be adopted in safety-critical domains like medical image analysis, AI systems must provide human-interpretable decisions. Variational Information Pursuit (V-IP) offers an interpretable-by-design framework by sequentially querying input images for human-understandable concepts, using their presence or absence to make predictions. However, existing V-IP methods overlook sample-specific uncertainty in concept predictions, which can arise from ambiguous features or model limitations, leading to suboptimal query selection and reduced robustness. In this paper, we propose an interpretable and uncertainty-aware framework for medical imaging that addresses these limitations by accounting for upstream uncertainties in concept-based, interpretable-by-design models. Specifically, we introduce two uncertainty-aware models, EUAV-IP and IUAV-IP, that integrate uncertainty estimates into the V-IP querying process to prioritize more reliable concepts per sample. EUAV-IP skips uncertain concepts via masking, while IUAV-IP incorporates uncertainty into query selection implicitly for more informed and clinically aligned decisions. Our approach allows models to make reliable decisions based on a subset of concepts tailored to each individual sample, without human intervention, while maintaining overall interpretability. We evaluate our methods on five medical imaging datasets across four modalities: dermoscopy, X-ray, ultrasound, and blood cell imaging. The proposed IUAV-IP model achieves state-of-the-art accuracy among interpretable-by-design approaches on four of the five datasets, and generates more concise explanations by selecting fewer yet more informative concepts. These advances enable more reliable and clinically meaningful outcomes, enhancing model trustworthiness and supporting safer AI deployment in healthcare. </p>
<blockquote>
<p>在医疗图像分析等安全关键领域，人工智能系统必须提供人类可解释的决策。变分信息追求（V-IP）提供了一个通过设计即可解释框架，通过顺序查询输入图像以获取人类可以理解的概念，并利用它们的存在与否来进行预测。然而，现有的V-IP方法忽略了概念预测中的样本特定不确定性，这种不确定性可能来源于特征模糊或模型局限性，从而导致查询选择不佳和稳健性降低。在本文中，我们提出了一个用于医学影像的可解释和不确定性感知框架，该框架通过考虑基于概念的、通过设计即可解释的模型中的上游不确定性来解决这些限制。具体来说，我们引入了两种不确定性感知模型，即EUAV-IP和IUAV-IP，它们将不确定性估计整合到V-IP查询过程中，以优先考虑每个样本的更可靠概念。EUAV-IP通过屏蔽跳过不确定概念，而IUAV-IP将不确定性隐式地纳入查询选择中，以做出更明智且与临床相符的决策。我们的方法允许模型基于针对每个个体样本量身定制的概念子集做出可靠决策，无需人工干预，同时保持整体可解释性。我们在五种医学影像数据集（包括四种模式：皮肤镜检查、X射线、超声波和血液细胞成像）上评估了我们的方法。所提出的IUAV-IP模型在四个数据集上实现了通过设计即可解释的先进准确性，并通过选择更少但更有信息量的概念来提供更简洁的解释。这些进展使结果更加可靠和具有临床意义，提高了模型的信任度，并支持更安全的人工智能在医疗保健中的部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16742v2">PDF</a> </p>
<p><strong>Summary</strong><br>在医学图像分析等重要领域应用AI系统时，需要注重提供人类可解释的决策。本研究提出了一种结合概念预测的不确定性感知框架，以解决现有变异信息追求（V-IP）方法在概念查询中的不足。本研究提出了两种不确定性感知模型EUAV-IP和IUAV-IP，将不确定性估计融入V-IP查询过程，优先处理每个样本更可靠的概念。其中，EUAV-IP通过屏蔽跳过不确定概念，而IUAV-IP则将不确定性纳入查询选择中，以做出更具针对性和临床对齐的决策。评估结果表明，该模型在五个医学图像数据集上的四个表现出优异准确性，生成的解释更简洁且具有更少而更具信息性的概念。此技术能更可靠和有意义地支持医疗健康领域的人工智能部署并提升模型的信赖度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AI系统在医学图像分析等领域需要提供人类可解释的决策。</li>
<li>变种信息追求（V-IP）框架通过查询输入图像中的可理解概念进行预测，但存在样本特定不确定性的忽略问题。</li>
<li>现有方法因为忽略了概念预测的不确定性可能导致查询选择不理想和鲁棒性降低。</li>
<li>研究者提出了一种不确定性感知框架，旨在解决上述问题，考虑了概念性设计中上游的不确定性问题。</li>
<li>EUAV-IP模型通过屏蔽不确定概念提高决策可靠性，而IUAV-IP模型将不确定性纳入查询选择中，以生成更精确且临床对齐的解释。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16742">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4321030797e6171dd94113133bf796d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059247&auth_key=1760059247-0-0-569a995bfac437745bc17cb93f73d133&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c46651c934f7793bb40a5d73d5d4fbcc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059254&auth_key=1760059254-0-0-a713f95a70e2852c40da14b97149ca37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Quad-Step-Approach-to-Uncertainty-Aware-Deep-Learning-for-Skin-Cancer-Classification"><a href="#A-Quad-Step-Approach-to-Uncertainty-Aware-Deep-Learning-for-Skin-Cancer-Classification" class="headerlink" title="A Quad-Step Approach to Uncertainty-Aware Deep Learning for Skin Cancer   Classification"></a>A Quad-Step Approach to Uncertainty-Aware Deep Learning for Skin Cancer   Classification</h2><p><strong>Authors:Hamzeh Asgharnezhad, Pegah Tabarisaadi, Abbas Khosravi, Roohallah Alizadehsani, U. Rajendra Acharya</strong></p>
<p>Accurate skin cancer diagnosis is vital for early treatment and improved patient outcomes. Deep learning (DL) models have shown promise in automating skin cancer classification, yet challenges remain due to data scarcity and limited uncertainty awareness. This study presents a comprehensive evaluation of DL-based skin lesion classification with transfer learning and uncertainty quantification (UQ) on the HAM10000 dataset. We benchmark several pre-trained feature extractors – including CLIP variants, ResNet50, DenseNet121, VGG16, and EfficientNet-V2-Large – combined with traditional classifiers such as SVM, XGBoost, and logistic regression. Multiple principal component analysis (PCA) settings (64, 128, 256, 512) are explored, with LAION CLIP ViT-H&#x2F;14 and ViT-L&#x2F;14 at PCA-256 achieving the strongest baseline results. In the UQ phase, Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte Carlo Dropout (EMCD) are applied and evaluated using uncertainty-aware metrics (UAcc, USen, USpe, UPre). Ensemble methods with PCA-256 provide the best balance between accuracy and reliability. Further improvements are obtained through feature fusion of top-performing extractors at PCA-256. Finally, we propose a feature-fusion based model trained with a predictive entropy (PE) loss function, which outperforms all prior configurations across both standard and uncertainty-aware evaluations, advancing trustworthy DL-based skin cancer diagnosis. </p>
<blockquote>
<p>精确的皮肤癌诊断对于早期治疗并改善患者的治疗效果至关重要。深度学习模型在自动进行皮肤癌分类方面展现出巨大潜力，但由于数据稀缺和缺乏不确定性意识，仍存在挑战。本研究对基于深度学习的皮肤病变分类进行了全面评估，采用迁移学习和不确定性量化（UQ）在HAM10000数据集上进行实验。我们基准测试了多种预训练特征提取器，包括CLIP变体、ResNet50、DenseNet121、VGG16和EfficientNet-V2-Large，结合传统分类器，如SVM、XGBoost和逻辑回归。探索了多个主成分分析（PCA）设置（64、128、256、512），在PCA-256设置下，LAION CLIP ViT-H&#x2F;14和ViT-L&#x2F;14取得了最强的基线结果。在不确定性量化阶段，应用了蒙特卡洛Dropout（MCD）、集成方法和集成蒙特卡洛Dropout（EMCD），并使用不确定性感知指标（UAcc、USen、USpe、UPre）进行评估。PCA-256的集成方法提供了准确性和可靠性之间的最佳平衡。通过融合表现最佳的提取器在PCA-256上的特征，获得了进一步的改进。最后，我们提出了一种基于特征融合的模型，该模型使用预测熵（PE）损失函数进行训练，在标准和不确定性感知评估中都优于所有先前的配置，从而推进了基于深度学习的可信皮肤癌诊断。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10302v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文研究了深度学习在皮肤癌诊断中的应用，特别是在皮肤病变分类方面的表现。文章使用了迁移学习和不确定性量化技术，并在HAM10000数据集上进行了评估。研究对比了多种预训练特征提取器的性能，包括CLIP变体、ResNet50、DenseNet121、VGG16和EfficientNet-V2-Large等，并结合传统分类器如SVM、XGBoost和逻辑回归。研究发现，LAION CLIP ViT-H&#x2F;14和ViT-L&#x2F;14在PCA-256设置下取得了最佳基线结果。在不确定性量化阶段，应用了Monte Carlo Dropout、Ensemble和Ensemble Monte Carlo Dropout等方法，并使用不确定性感知指标进行评估。融合特征的方法在提升准确性和可靠性方面表现最佳。最后，研究提出了一种基于特征融合的模型，该模型使用预测熵损失函数进行训练，在标准评估和不确定性感知评估中都表现出最佳性能，为可信赖的深度学习皮肤癌诊断提供了新的方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究评估了深度学习模型在皮肤癌诊断中的潜力，特别是在自动化皮肤病变分类方面的应用。</li>
<li>使用迁移学习和不确定性量化技术来提高模型的性能。</li>
<li>多种预训练特征提取器与传统分类器的结合，在皮肤病变分类中进行了对比研究。</li>
<li>LAION CLIP ViT模型在特定设置下取得了最佳基线结果。</li>
<li>Ensemble方法和特征融合有助于提高模型的准确性和可靠性。</li>
<li>提出了基于特征融合的模型，采用预测熵损失函数，在评估和不确定性感知评估中都表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10302">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-133dd204aff239821b45f47ae105e3d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059261&auth_key=1760059261-0-0-7c6c2d1e0627032834df1a80c2352155&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6df2f69962e1c70ff5fbe5db716a13c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059268&auth_key=1760059268-0-0-6c3d50271aa102e3e07e5d6e265273f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33882f121e5ea702e53adc27fdfdfdca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059275&auth_key=1760059275-0-0-989c4c05d12bbe4c94c913d3587daa8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45689764503f5facf60c1ee475107348~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059281&auth_key=1760059281-0-0-14819f5645b4ddfc91f26e9950ace57e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Image-Segmentation-and-Classification-of-E-waste-for-Training-Robots-for-Waste-Segregation"><a href="#Image-Segmentation-and-Classification-of-E-waste-for-Training-Robots-for-Waste-Segregation" class="headerlink" title="Image Segmentation and Classification of E-waste for Training Robots for   Waste Segregation"></a>Image Segmentation and Classification of E-waste for Training Robots for   Waste Segregation</h2><p><strong>Authors:Prakriti Tripathi</strong></p>
<p>Industry partners provided a problem statement that involves classifying electronic waste using machine learning models that will be used by pick-and-place robots for waste segregation. This was achieved by taking common electronic waste items, such as a mouse and charger, unsoldering them, and taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also trained and achieved 41 mAP. The model can be integrated with pick-and-place robots to perform segregation of e-waste. </p>
<blockquote>
<p>产业合作伙伴提供了一个问题陈述，该问题涉及使用机器学习模型对电子废物进行分类，这些模型将由拾取放置机器人用于废物分离。这是通过获取常见的电子废物项目（如鼠标和充电器），对其进行解焊，并拍照以创建自定义数据集来实现的。然后，使用最先进的YOLOv11模型进行训练和运行，以实时实现70 mAP。还训练了Mask-RCNN模型，达到了41 mAP。该模型可以集成到拾取放置机器人中，以执行电子废物的分离。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07122v2">PDF</a> 3 pages, 2 figures, submitted to 2025 5th International Conference on   AI-ML-Systems (AIMLSystems)</p>
<p><strong>Summary</strong><br>：本论文描述了如何利用机器学习模型对电子废物进行分类，采用合作伙伴提供的方案通过机器人进行废物的分类放置。通过对常见的电子废物如鼠标和充电器进行拆解并拍照创建自定义数据集，训练了先进的YOLOv11模型，实现了实时70 mAP的分类效果。同时，也训练了Mask-RCNN模型，取得了41 mAP的分类效果。该模型可集成到拾取放置机器人中，实现电子废物的分离。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>行业合作伙伴提出了一个涉及电子废物分类的问题，该问题将通过机器学习模型解决，并由拾取放置机器人用于废物分离。</li>
<li>通过拆解常见的电子废物物品并拍照创建自定义数据集来解决这个问题。</li>
<li>先进的YOLOv11模型被训练并用于实时分类电子废物，达到了70 mAP的效果。</li>
<li>Mask-RCNN模型也被训练用于电子废物分类，取得了41 mAP的分类效果。</li>
<li>训练出的模型可以集成到拾取放置机器人中，实现自动化的废物分离。</li>
<li>此方案对于处理电子废物具有实际应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-36ee894305b4cfc390880bcada05d9a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059289&auth_key=1760059289-0-0-6a1c8ff204406ffed3cd18f723050af1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07386b6685da2be24f843a85ca12ae5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059296&auth_key=1760059296-0-0-0185f2efc7381377340591f5aeefbe33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Talking Head Generation/2509.21465v1/page_3_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-30  Comprehend and Talk Text to Speech Synthesis via Dual Language Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4bcebee6dc782b0a6f8cb715c95b1e06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760058499&auth_key=1760058499-0-0-f2db49f17315282741887a5bf3ad095f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-29  Gaussian Herding across Pens An Optimal Transport Perspective on Global   Gaussian Reduction for 3DGS
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
