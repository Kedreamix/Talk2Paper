<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Diffusion Bridge Variational Inference for Deep Gaussian Processes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6189bd830808381148085be7d6f20de0')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-29-æ›´æ–°"><a href="#2025-09-29-æ›´æ–°" class="headerlink" title="2025-09-29 æ›´æ–°"></a>2025-09-29 æ›´æ–°</h1><h2 id="Diffusion-Bridge-Variational-Inference-for-Deep-Gaussian-Processes"><a href="#Diffusion-Bridge-Variational-Inference-for-Deep-Gaussian-Processes" class="headerlink" title="Diffusion Bridge Variational Inference for Deep Gaussian Processes"></a>Diffusion Bridge Variational Inference for Deep Gaussian Processes</h2><p><strong>Authors:Jian Xu, Qibin Zhao, John Paisley, Delu Zeng</strong></p>
<p>Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian modeling but pose substantial challenges for posterior inference, especially over inducing variables. Denoising diffusion variational inference (DDVI) addresses this by modeling the posterior as a time-reversed diffusion from a simple Gaussian prior. However, DDVIâ€™s fixed unconditional starting distribution remains far from the complex true posterior, resulting in inefficient inference trajectories and slow convergence. In this work, we propose Diffusion Bridge Variational Inference (DBVI), a principled extension of DDVI that initiates the reverse diffusion from a learnable, data-dependent initial distribution. This initialization is parameterized via an amortized neural network and progressively adapted using gradients from the ELBO objective, reducing the posterior gap and improving sample efficiency. To enable scalable amortization, we design the network to operate on the inducing inputs, which serve as structured, low-dimensional summaries of the dataset and naturally align with the inducing variablesâ€™ shape. DBVI retains the mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We derive a tractable training objective under this formulation and implement DBVI for scalable inference in large-scale DGPs. Across regression, classification, and image reconstruction tasks, DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality. </p>
<blockquote>
<p>æ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆDGPsï¼‰èƒ½å¤Ÿå®ç°è¡¨è¾¾æ€§å±‚æ¬¡åŒ–çš„è´å¶æ–¯å»ºæ¨¡ï¼Œä½†ç»™åéªŒæ¨æ–­å¸¦æ¥äº†å®è´¨æ€§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯±å¯¼å˜é‡æ–¹é¢ã€‚é™å™ªæ‰©æ•£å˜åˆ†æ¨æ–­ï¼ˆDDVIï¼‰é€šè¿‡æŠŠåéªŒå»ºæ¨¡ä¸ºä»ç®€å•é«˜æ–¯å…ˆéªŒå¼€å§‹çš„æ—¶é—´åè½¬æ‰©æ•£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼ŒDDVIçš„å›ºå®šæ— æ¡ä»¶èµ·å§‹åˆ†å¸ƒä¸å¤æ‚çš„çœŸå®åéªŒç›¸å·®ç”šè¿œï¼Œå¯¼è‡´æ¨ç†è½¨è¿¹æ•ˆç‡ä½ä¸‹ï¼Œæ”¶æ•›ç¼“æ…¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£æ¡¥å˜åˆ†æ¨æ–­ï¼ˆDBVIï¼‰ï¼Œå®ƒæ˜¯DDVIçš„ä¸€ç§åŸåˆ™æ€§æ‰©å±•ï¼Œä»å¯å­¦ä¹ çš„ã€ä¾èµ–äºæ•°æ®åˆå§‹åˆ†å¸ƒå¼€å§‹åå‘æ‰©æ•£ã€‚è¿™ä¸ªåˆå§‹åŒ–æ˜¯é€šè¿‡æ‘Šé”€ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–çš„ï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨ELBOç›®æ ‡çš„æ¢¯åº¦è¿›è¡Œé€æ­¥é€‚åº”ï¼Œä»è€Œç¼©å°äº†åéªŒå·®è·å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä¸ºäº†å®ç°å¯æ‰©å±•çš„æ‘Šé”€ï¼Œæˆ‘ä»¬è®¾è®¡ç½‘ç»œåœ¨è¯±å¯¼è¾“å…¥ä¸Šè¿è¡Œï¼Œè¿™äº›è¾“å…¥ä½œä¸ºæ•°æ®é›†çš„ç»“æ„åŒ–ã€ä½ç»´æ‘˜è¦ï¼Œè‡ªç„¶ä¸è¯±å¯¼å˜é‡çš„å½¢çŠ¶å¯¹é½ã€‚DBVIä¿ç•™äº†DDVIçš„æ•°å­¦ä¼˜é›…æ€§ï¼ŒåŒ…æ‹¬åŸºäºGirsanovçš„ELBOså’Œåå‘æ—¶é—´SDEsï¼ŒåŒæ—¶é€šè¿‡Doobæ¡¥æ‰©æ•£è¿‡ç¨‹é‡æ–°è§£é‡Šå…ˆéªŒã€‚åœ¨è¿™ç§å½¢å¼ä¸‹ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªå¯è¡Œçš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶å®ç°äº†DBVIç”¨äºåœ¨å¤§è§„æ¨¡DGPä¸­è¿›è¡Œå¯æ‰©å±•æ¨ç†ã€‚åœ¨å›å½’ã€åˆ†ç±»å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸­ï¼ŒDBVIåœ¨é¢„æµ‹ç²¾åº¦ã€æ”¶æ•›é€Ÿåº¦å’ŒåéªŒè´¨é‡æ–¹é¢å§‹ç»ˆä¼˜äºDDVIå’Œå…¶ä»–å˜åˆ†åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19078v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆDGPsï¼‰èƒ½å¤Ÿè¿›è¡Œçµæ´»çš„åˆ†å±‚è´å¶æ–¯å»ºæ¨¡ï¼Œä½†ä¸ºåç½®æ¨ç†å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯±å¯¼å˜é‡æ–¹é¢ã€‚å»å™ªæ‰©æ•£å˜åˆ†æ¨ç†ï¼ˆDDVIï¼‰é€šè¿‡æ¨¡æ‹Ÿä»ç®€å•é«˜æ–¯å…ˆéªŒå¼€å§‹çš„æ—¶é—´åè½¬æ‰©æ•£è¿‡ç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼ŒDDVIå›ºå®šçš„æ— æ¡ä»¶èµ·å§‹åˆ†å¸ƒä¸å¤æ‚çš„çœŸå®åéªŒåˆ†å¸ƒç›¸å·®ç”šè¿œï¼Œå¯¼è‡´æ¨ç†è½¨è¿¹æ•ˆç‡ä½ä¸‹ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ¡¥å˜åˆ†æ¨ç†ï¼ˆDBVIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯¹DDVIçš„ç†è®ºæ‰©å±•ï¼Œå®ƒä»å¯å­¦ä¹ çš„ã€æ•°æ®ä¾èµ–çš„åˆå§‹åˆ†å¸ƒå¼€å§‹åå‘æ‰©æ•£ã€‚è¯¥åˆå§‹åŒ–é€šè¿‡æ‘Šé”€ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶ä½¿ç”¨ELBOç›®æ ‡å‡½æ•°çš„æ¢¯åº¦é€æ­¥é€‚åº”ï¼Œä»è€Œç¼©å°åéªŒå·®è·å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒå¯æ‰©å±•çš„æ‘Šé”€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåœ¨è¯±å¯¼è¾“å…¥ä¸Šè¿è¡Œçš„ç½‘ç»œï¼Œè¿™äº›è¾“å…¥ä½œä¸ºæ•°æ®é›†çš„ç»“æ„åŒ–ã€ä½ç»´æ‘˜è¦ï¼Œè‡ªç„¶åœ°ä¸è¯±å¯¼å˜é‡çš„å½¢çŠ¶å¯¹é½ã€‚DBVIä¿ç•™äº†DDVIçš„æ•°å­¦ä¼˜é›…æ€§ï¼ŒåŒ…æ‹¬Girsanov-based ELBOså’Œåå‘æ—¶é—´SDEsï¼ŒåŒæ—¶é€šè¿‡Doobæ¡¥æ‰©æ•£è¿‡ç¨‹é‡æ–°è§£é‡Šå…ˆéªŒã€‚æˆ‘ä»¬åœ¨æ­¤å…¬å¼ä¸‹æ¨å¯¼å‡ºäº†ä¸€ä¸ªå¯è¡Œçš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶å®ç°äº†ç”¨äºå¤§è§„æ¨¡DGPsä¸­å¯æ‰©å±•æ¨ç†çš„DBVIã€‚åœ¨å›å½’ã€åˆ†ç±»å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸­ï¼ŒDBVIåœ¨é¢„æµ‹ç²¾åº¦ã€æ”¶æ•›é€Ÿåº¦å’ŒåéªŒè´¨é‡æ–¹é¢å‡ä¼˜äºDDVIå’Œå…¶ä»–å˜åˆ†åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆDGPsï¼‰åœ¨è¿›è¡Œå±‚æ¬¡è´å¶æ–¯å»ºæ¨¡æ—¶é¢ä¸´åç½®æ¨ç†çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯±å¯¼å˜é‡æ–¹é¢ã€‚</li>
<li>Denoising Diffusion Variational Inference (DDVI) é€šè¿‡æ¨¡æ‹Ÿä»ç®€å•é«˜æ–¯å…ˆéªŒå¼€å§‹çš„æ—¶é—´åè½¬æ‰©æ•£è¿‡ç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å­˜åœ¨å›ºå®šèµ·å§‹åˆ†å¸ƒçš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Diffusion Bridge Variational Inference (DBVI) ä»å¯å­¦ä¹ çš„æ•°æ®ä¾èµ–åˆå§‹åˆ†å¸ƒå¼€å§‹åå‘æ‰©æ•£ï¼Œç¼©å°äº†ä¸å¤æ‚çœŸå®åéªŒåˆ†å¸ƒçš„å·®è·ã€‚</li>
<li>DBVIé€šè¿‡æ‘Šé”€ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–åˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨ELBOç›®æ ‡å‡½æ•°çš„æ¢¯åº¦é€æ­¥é€‚åº”ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’ŒåéªŒè´¨é‡ã€‚</li>
<li>DBVIè®¾è®¡ç½‘ç»œåœ¨è¯±å¯¼è¾“å…¥ä¸Šè¿è¡Œï¼Œè¿™äº›è¾“å…¥ä½œä¸ºæ•°æ®é›†çš„ç»“æ„åŒ–ã€ä½ç»´æ‘˜è¦ã€‚</li>
<li>DBVIä¿ç•™äº†DDVIçš„æ•°å­¦ä¼˜é›…æ€§ï¼ŒåŒ…æ‹¬Girsanov-based ELBOså’Œåå‘æ—¶é—´SDEsï¼Œå¹¶é€šè¿‡Doobæ¡¥æ‰©æ•£è¿‡ç¨‹é‡æ–°è§£é‡Šå…ˆéªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ccbcbed87c0aa64041b3a5a095393f7" align="middle">
<img src="https://picx.zhimg.com/v2-a87220aab2b572f32501d1be9103c755" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiSSECT-Structuring-Transfer-Ready-Medical-Image-Representations-through-Discrete-Self-Supervision"><a href="#DiSSECT-Structuring-Transfer-Ready-Medical-Image-Representations-through-Discrete-Self-Supervision" class="headerlink" title="DiSSECT: Structuring Transfer-Ready Medical Image Representations   through Discrete Self-Supervision"></a>DiSSECT: Structuring Transfer-Ready Medical Image Representations   through Discrete Self-Supervision</h2><p><strong>Authors:Azad Singh, Deepak Mishra</strong></p>
<p>Self-supervised learning (SSL) has emerged as a powerful paradigm for medical image representation learning, particularly in settings with limited labeled data. However, existing SSL methods often rely on complex architectures, anatomy-specific priors, or heavily tuned augmentations, which limit their scalability and generalizability. More critically, these models are prone to shortcut learning, especially in modalities like chest X-rays, where anatomical similarity is high and pathology is subtle. In this work, we introduce DiSSECT â€“ Discrete Self-Supervision for Efficient Clinical Transferable Representations, a framework that integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representational bottleneck. This constrains the model to learn repeatable, structure-aware features while suppressing view-specific or low-utility patterns, improving representation transfer across tasks and domains. DiSSECT achieves strong performance on both classification and segmentation tasks, requiring minimal or no fine-tuning, and shows particularly high label efficiency in low-label regimes. We validate DiSSECT across multiple public medical imaging datasets, demonstrating its robustness and generalizability compared to existing state-of-the-art approaches. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å·²æˆä¸ºåŒ»å­¦å›¾åƒè¡¨ç¤ºå­¦ä¹ çš„ä¸€ç§å¼ºå¤§èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SSLæ–¹æ³•å¾€å¾€ä¾èµ–äºå¤æ‚çš„æ¶æ„ã€ç‰¹å®šçš„è§£å‰–å…ˆéªŒçŸ¥è¯†æˆ–ç»è¿‡é«˜åº¦è°ƒæ•´çš„æ•°æ®å¢å¼ºï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹å®¹æ˜“é™·å…¥æ·å¾„å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ¸éƒ¨Xå°„çº¿ç­‰æ¨¡æ€ä¸­ï¼Œç”±äºè§£å‰–ç»“æ„ç›¸ä¼¼æ€§é«˜ã€ç—…ç†è¡¨ç°å¾®å¦™æ›´æ˜¯å¦‚æ­¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiSSECTâ€”â€”ç¦»æ•£è‡ªç›‘ç£é«˜æ•ˆä¸´åºŠå¯è½¬ç§»è¡¨ç¤ºæ¡†æ¶ï¼Œå®ƒå°†å¤šå°ºåº¦çŸ¢é‡é‡åŒ–é›†æˆåˆ°SSLç®¡é“ä¸­ï¼Œä»¥æ–½åŠ ç¦»æ•£è¡¨ç¤ºç“¶é¢ˆã€‚è¿™çº¦æŸæ¨¡å‹å­¦ä¹ å¯é‡å¤ã€ç»“æ„æ„ŸçŸ¥çš„ç‰¹å¾ï¼ŒåŒæ—¶æŠ‘åˆ¶ç‰¹å®šè§†å›¾æˆ–ä½æ•ˆç”¨æ¨¡å¼ï¼Œæ”¹è¿›è·¨ä»»åŠ¡å’Œé¢†åŸŸçš„è¡¨ç¤ºè½¬ç§»ã€‚DiSSECTåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå‡ ä¹ä¸éœ€è¦æˆ–æ ¹æœ¬ä¸éœ€è¦å¾®è°ƒï¼Œå¹¶ä¸”åœ¨ä½æ ‡ç­¾çŠ¶æ€ä¸‹æ˜¾ç¤ºå‡ºæé«˜çš„æ ‡ç­¾æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…¬å…±åŒ»å­¦æˆåƒæ•°æ®é›†ä¸ŠéªŒè¯äº†DiSSECTï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18765v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒè¡¨ç¤ºå­¦ä¹ çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚å¤æ‚æ€§é«˜ã€ä¾èµ–äºç‰¹å®šè§£å‰–ç»“æ„å…ˆéªŒä¿¡æ¯å’Œéœ€è¦å¤§é‡è°ƒä¼˜çš„æ•°æ®å¢å¼ºã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„SSLæ¡†æ¶DiSSECTï¼Œé€šè¿‡å°†å¤šå°ºåº¦çŸ¢é‡é‡åŒ–æ•´åˆåˆ°SSLç®¡é“ä¸­ï¼Œä»¥æ–½åŠ ç¦»æ•£è¡¨ç¤ºç“¶é¢ˆï¼Œä»è€Œå­¦ä¹ å¯é‡å¤çš„ç»“æ„æ„ŸçŸ¥ç‰¹å¾å¹¶æŠ‘åˆ¶ç‰¹å®šè§†å›¾æˆ–ä½æ•ˆç”¨æ¨¡å¼ï¼Œæé«˜è·¨ä»»åŠ¡å’Œé¢†åŸŸçš„è¡¨ç¤ºè½¬ç§»èƒ½åŠ›ã€‚DiSSECTåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨ä½æ ‡ç­¾æƒ…å†µä¸‹å®ç°é«˜æ ‡ç­¾æ•ˆç‡ã€‚é€šè¿‡è·¨å¤šä¸ªå…¬å…±åŒ»å­¦æˆåƒæ•°æ®é›†éªŒè¯ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ç°æœ‰æœ€å…ˆè¿›çš„ç­–ç•¥ç›¸æ¯”å…·æœ‰ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨åŒ»å­¦å›¾åƒè¡¨ç¤ºå­¦ä¹ ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶åœ¨æœ‰é™æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰çš„SSLæ–¹æ³•å­˜åœ¨å¤æ‚æ€§é«˜ã€ä¾èµ–äºç‰¹å®šè§£å‰–ç»“æ„å…ˆéªŒå’Œéœ€è¦å¤§é‡è°ƒä¼˜çš„æ•°æ®å¢å¼ºç­‰é—®é¢˜ã€‚</li>
<li>DiSSECTæ¡†æ¶é€šè¿‡æ•´åˆå¤šå°ºåº¦çŸ¢é‡é‡åŒ–åˆ°SSLç®¡é“ä¸­ï¼Œä»¥æ–½åŠ ç¦»æ•£è¡¨ç¤ºç“¶é¢ˆï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>DiSSECTæ¡†æ¶èƒ½å¤Ÿå­¦ä¹ å¯é‡å¤çš„ç»“æ„æ„ŸçŸ¥ç‰¹å¾å¹¶æŠ‘åˆ¶ç‰¹å®šè§†å›¾æˆ–ä½æ•ˆç”¨æ¨¡å¼ï¼Œæé«˜è·¨ä»»åŠ¡å’Œé¢†åŸŸçš„è¡¨ç¤ºè½¬ç§»èƒ½åŠ›ã€‚</li>
<li>DiSSECTåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¸”åœ¨é«˜æ ‡ç­¾æ•ˆç‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>DiSSECTæ¡†æ¶åœ¨ä¸åŒçš„å…¬å…±åŒ»å­¦æˆåƒæ•°æ®é›†ä¸ŠéªŒè¯è¿‡ï¼Œè¡¨ç°å‡ºäº†å…¶ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e05332056a4cc564241594ccb3f0000b" align="middle">
<img src="https://picx.zhimg.com/v2-f14730e54f715f65f18ca800e83c0a4b" align="middle">
<img src="https://picx.zhimg.com/v2-dadc6157d54dce2a363f094520422d69" align="middle">
<img src="https://picx.zhimg.com/v2-685176cfd3e8100f48131ec9c3592511" align="middle">
<img src="https://picx.zhimg.com/v2-9af4a1744bdbccdeaf66e2513296da8b" align="middle">
<img src="https://picx.zhimg.com/v2-8ce0a511e7a9c3f7f1dc19ce3dceccdd" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OraPO-Oracle-educated-Reinforcement-Learning-for-Data-efficient-and-Factual-Radiology-Report-Generation"><a href="#OraPO-Oracle-educated-Reinforcement-Learning-for-Data-efficient-and-Factual-Radiology-Report-Generation" class="headerlink" title="OraPO: Oracle-educated Reinforcement Learning for Data-efficient and   Factual Radiology Report Generation"></a>OraPO: Oracle-educated Reinforcement Learning for Data-efficient and   Factual Radiology Report Generation</h2><p><strong>Authors:Zhuoxiao Chen, Hongyang Yu, Ying Xu, Yadan Luo, Long Duong, Yuan-Fang Li</strong></p>
<p>Radiology report generation (RRG) aims to automatically produce clinically faithful reports from chest X-ray images. Prevailing work typically follows a scale-driven paradigm, by multi-stage training over large paired corpora and oversized backbones, making pipelines highly data- and compute-intensive. In this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables single-stage, RL-only training by converting failed GRPO explorations on rare or difficult studies into direct preference supervision via a lightweight oracle step. FactS grounds learning in diagnostic evidence by extracting atomic clinical facts and checking entailment against ground-truth labels, yielding dense, interpretable sentence-level rewards. Together, OraPO and FactS create a compact and powerful framework that significantly improves learning efficiency on clinically challenging cases, setting the new SOTA performance on the CheXpert Plus dataset (0.341 in F1) with 2â€“3 orders of magnitude less training data using a small base VLM on modest hardware. </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ—¨åœ¨ä»èƒ¸éƒ¨Xå°„çº¿å›¾åƒä¸­è‡ªåŠ¨äº§ç”Ÿä¸´åºŠçœŸå®çš„æŠ¥å‘Šã€‚ç°æœ‰çš„å·¥ä½œé€šå¸¸éµå¾ªè§„æ¨¡é©±åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡å¤§è§„æ¨¡é…å¯¹è¯­æ–™åº“å’Œè¿‡å¤§çš„éª¨å¹²ç½‘ç»œçš„åˆ†é˜¶æ®µè®­ç»ƒï¼Œä½¿å¾—ç®¡é“é«˜åº¦ä¾èµ–æ•°æ®å’Œè®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨åŸºäºOracleæ•™è‚²çš„GRPOï¼ˆOraPOï¼‰å’ŒåŸºäºFactScoreçš„å¥–åŠ±ï¼ˆFactSï¼‰åœ¨æœ‰é™é¢„ç®—ä¸‹è§£å†³RRGä»»åŠ¡ã€‚OraPOé€šè¿‡è½»é‡çº§çš„Oracleæ­¥éª¤å°†å¤±è´¥çš„GRPOæ¢ç´¢è½¬æ¢ä¸ºå¯¹ç½•è§æˆ–å›°éš¾ç ”ç©¶çš„ç›´æ¥åå¥½ç›‘ç£ï¼Œä»è€Œå®ç°äº†å•é˜¶æ®µä»…RLè®­ç»ƒã€‚FactSé€šè¿‡æå–åŸå­ä¸´åºŠäº‹å®å¹¶æ£€æŸ¥ä¸çœŸå®æ ‡ç­¾çš„è•´æ¶µå…³ç³»ï¼Œä½¿å­¦ä¹ åŸºäºè¯Šæ–­è¯æ®ï¼Œä»è€Œäº§ç”Ÿå¯†é›†ã€å¯è§£é‡Šçš„å¥å­çº§å¥–åŠ±ã€‚å¥¥æ‹‰æ™®å’Œæ³•æ–¯ç‰¹ä¸€èµ·ï¼Œå½¢æˆäº†ä¸€ä¸ªç´§å‡‘è€Œå¼ºå¤§çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸´åºŠæŒ‘æˆ˜æ¡ˆä¾‹ä¸­æ˜¾è‘—æé«˜å­¦ä¹ æ•ˆç‡ï¼Œåœ¨CheXpert Plusæ•°æ®é›†ä¸Šè®¾å®šäº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼ˆF1å¾—åˆ†ä¸º0.341ï¼‰ï¼Œä½¿ç”¨å°å‹åŸºç¡€VLMåœ¨é€‚åº¦ç¡¬ä»¶ä¸Šå‡å°‘äº†2-3ä¸ªæ•°é‡çº§çš„è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºOracle-educated GRPOï¼ˆOraPOï¼‰ç»“åˆFactScoreå¥–åŠ±æœºåˆ¶çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä»¥æœ‰é™çš„é¢„ç®—è‡ªåŠ¨äº§ç”Ÿä¸´åºŠå¯é çš„èƒ¸éƒ¨Xå…‰å›¾åƒæŠ¥å‘Šã€‚è¯¥æ–¹æ³•é€šè¿‡å•ä¸€é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå°†å¤±è´¥çš„æ¢ç´¢è½¬åŒ–ä¸ºç›´æ¥çš„åå¥½ç›‘ç£ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠæŒ‘æˆ˜æ¡ˆä¾‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚åœ¨CheXpert Plusæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ï¼ˆF1å¾—åˆ†ä¸º0.341ï¼‰ï¼Œå¹¶ä½¿ç”¨è¾ƒå°çš„åŸºæœ¬VLMåœ¨é€‚åº¦ç¡¬ä»¶ä¸Šå®ç°äº†2-3ä¸ªæ•°é‡çº§çš„æ›´å°‘è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ—¨åœ¨è§£å†³æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰ä»»åŠ¡ï¼Œå³è‡ªåŠ¨ä»èƒ¸éƒ¨Xå…‰å›¾åƒä¸­äº§ç”Ÿä¸´åºŠå¯é çš„æŠ¥å‘Šã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¤šé˜¶æ®µå¤§è§„æ¨¡è®­ç»ƒå’Œå¤§è§„æ¨¡æ¨¡å‹ï¼Œå¯¼è‡´æ•°æ®å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†Oracle-educated GRPOï¼ˆOraPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡å•ä¸€é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>OraPOé€šè¿‡å°†å¤±è´¥çš„æ¢ç´¢è½¬åŒ–ä¸ºç›´æ¥çš„åå¥½ç›‘ç£ï¼Œä½¿å¾—æ¨¡å‹åœ¨å›°éš¾æ¡ˆä¾‹ä¸Šçš„è¡¨ç°å¾—åˆ°æ˜¾è‘—æå‡ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†FactScoreå¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡æå–åŸå­ä¸´åºŠäº‹å®å¹¶æ£€æŸ¥å…¶ä¸çœŸå®æ ‡ç­¾çš„è•´å«å…³ç³»ï¼Œä¸ºå­¦ä¹ æä¾›å¯†é›†ã€å¯è§£é‡Šçš„å¥å­çº§å¥–åŠ±ã€‚</li>
<li>OraPOå’ŒFactSç»“åˆå½¢æˆäº†ä¸€ä¸ªç´§å‡‘ä¸”å¼ºå¤§çš„æ¡†æ¶ï¼Œåœ¨CheXpert Plusæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de46d64c2f76dd34468a293fef11e4cc" align="middle">
<img src="https://picx.zhimg.com/v2-4167c110d71e6f48a84cf36057571bd5" align="middle">
<img src="https://picx.zhimg.com/v2-ca87693554fc1e65f95bc18b3b1ba48a" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MK-UNet-Multi-kernel-Lightweight-CNN-for-Medical-Image-Segmentation"><a href="#MK-UNet-Multi-kernel-Lightweight-CNN-for-Medical-Image-Segmentation" class="headerlink" title="MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation"></a>MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation</h2><p><strong>Authors:Md Mostafijur Rahman, Radu Marculescu</strong></p>
<p>In this paper, we introduce MK-UNet, a paradigm shift towards ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution block (MKDC) we design to adeptly process images through multiple kernels, while capturing complex multi-resolution spatial relationships. MK-UNet also emphasizes the images salient features through sophisticated attention mechanisms, including channel, spatial, and grouped gated attention. Our MK-UNet network, with a modest computational footprint of only 0.316M parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but also significantly improved segmentation solution that provides higher accuracy over state-of-the-art (SOTA) methods across six binary medical imaging benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively. Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation performance, improving the DICE score up to 6.7% margins while operating with 4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with much lower computational resources. This leap in performance, coupled with drastic computational gains, positions MK-UNet as an unparalleled solution for real-time, high-fidelity medical diagnostics in resource-limited settings, such as point-of-care devices. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/SLDGroup/MK-UNet">https://github.com/SLDGroup/MK-UNet</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MK-UNetï¼Œè¿™æ˜¯ä¸€ç§è¶…è½»é‡çº§å¤šæ ¸Uå‹CNNçš„èŒƒå¼è½¬å˜ï¼Œä¸“é—¨ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚MK-UNetçš„æ ¸å¿ƒæ˜¯æˆ‘ä»¬è®¾è®¡çš„å¤šæ ¸æ·±åº¦å·ç§¯å—ï¼ˆMKDCï¼‰ï¼Œå®ƒèƒ½å¤Ÿçµæ´»åœ°å¤„ç†å›¾åƒï¼Œé€šè¿‡å¤šä¸ªå†…æ ¸æ•è·å¤æ‚çš„è·¨åˆ†è¾¨ç‡ç©ºé—´å…³ç³»ã€‚MK-UNetè¿˜é€šè¿‡å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶å¼ºè°ƒå›¾åƒçš„æ˜¾è‘—ç‰¹å¾ï¼ŒåŒ…æ‹¬é€šé“ã€ç©ºé—´å’Œåˆ†ç»„é—¨æ§æ³¨æ„åŠ›ã€‚æˆ‘ä»¬çš„MK-UNetç½‘ç»œä»…æœ‰0.316Mçš„å‚æ•°å’Œ0.314Gçš„FLOPsè®¡ç®—å¼€é”€ï¼Œä¸ä»…éå¸¸è½»é‡çº§ï¼Œè€Œä¸”åœ¨å…­ä¸ªäºŒå…ƒåŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸­æä¾›äº†æ›´é«˜çš„ç²¾åº¦ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒMK-UNetåœ¨DICEå¾—åˆ†ä¸Šä¼˜äºTransUNetï¼ŒåŒæ—¶ä½¿ç”¨å‚æ•°å’ŒFLOPsçš„è¿‘333å€å’Œ123å€æ›´å°‘ã€‚ä¸UNeXtç›¸æ¯”ï¼ŒMK-UNetå±•ç°å‡ºæ›´ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼ŒDICEå¾—åˆ†æé«˜äº†é«˜è¾¾6.7%ï¼ŒåŒæ—¶ä½¿ç”¨å‚æ•°æ›´å°‘ï¼Œä¸º4.7å€ã€‚æˆ‘ä»¬çš„MK-UNetè¿˜ä¼˜äºå…¶ä»–æœ€æ–°çš„è½»é‡çº§ç½‘ç»œï¼Œå¦‚MedTã€CMUNeXtã€EGE-UNetå’ŒRolling-UNetï¼ŒåŒæ—¶è®¡ç®—èµ„æºæ›´ä½ã€‚è¿™ç§æ€§èƒ½çš„æå‡ä»¥åŠè®¡ç®—èµ„æºçš„æ˜¾è‘—èŠ‚çœï¼Œä½¿MK-UNetæˆä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å®æ—¶é«˜ä¿çœŸåŒ»å­¦è¯Šæ–­çš„æ— ä¸ä¼¦æ¯”çš„é€‰æ‹©ï¼Œå¦‚ä¾¿æºå¼åŒ»ç–—è®¾å¤‡ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SLDGroup/MK-UNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SLDGroup/MK-UNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18493v1">PDF</a> 11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†MK-UNetï¼Œä¸€ç§é¢å‘åŒ»ç–—å›¾åƒåˆ†å‰²çš„è¶…è½»é‡çº§å¤šæ ¸Uå½¢CNNã€‚MK-UNeté‡‡ç”¨å¤šæ ¸æ·±åº¦å·ç§¯å—ï¼ˆMKDCï¼‰å¤„ç†å›¾åƒï¼ŒåŒæ—¶æ•æ‰å¤æ‚çš„å¤šåˆ†è¾¨ç‡ç©ºé—´å…³ç³»ï¼Œå¹¶é€šè¿‡é€šé“ã€ç©ºé—´å’Œåˆ†ç»„é—¨æ§æ³¨æ„åŠ›æœºåˆ¶å¼ºè°ƒå›¾åƒçš„é‡è¦ç‰¹å¾ã€‚MK-UNetå…·æœ‰è¾ƒå°çš„è®¡ç®—å¼€é”€ï¼Œä»…0.316Må‚æ•°å’Œ0.314G FLOPsï¼Œä½†åœ¨å…­ä¸ªäºŒè¿›åˆ¶åŒ»ç–—æˆåƒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚MK-UNetåœ¨DICEè¯„åˆ†ä¸Šä¼˜äºTransUNetå’ŒUNeXtï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„å‚æ•°å’ŒFLOPsã€‚æ­¤å¤–ï¼ŒMK-UNetè¿˜ä¼˜äºå…¶ä»–è½»é‡çº§ç½‘ç»œï¼Œå¦‚MedTã€CMUNeXtã€EGE-UNetå’ŒRolling-UNetã€‚å› æ­¤ï¼ŒMK-UNetè¢«è®¤ä¸ºæ˜¯èµ„æºå—é™ç¯å¢ƒä¸­å®æ—¶é«˜ä¿çœŸåŒ»ç–—è¯Šæ–­çš„ç»ä½³è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MK-UNetæ˜¯ä¸€ç§é¢å‘åŒ»ç–—å›¾åƒåˆ†å‰²çš„è¶…è½»é‡çº§å¤šæ ¸Uå½¢CNNã€‚</li>
<li>MK-UNeté‡‡ç”¨å¤šæ ¸æ·±åº¦å·ç§¯å—ï¼ˆMKDCï¼‰å¤„ç†å›¾åƒï¼Œæ•æ‰å¤æ‚çš„å¤šåˆ†è¾¨ç‡ç©ºé—´å…³ç³»ã€‚</li>
<li>MK-UNeté€šè¿‡é€šé“ã€ç©ºé—´å’Œåˆ†ç»„é—¨æ§æ³¨æ„åŠ›æœºåˆ¶å¼ºè°ƒå›¾åƒçš„é‡è¦ç‰¹å¾ã€‚</li>
<li>MK-UNetåœ¨å…­ä¸ªäºŒè¿›åˆ¶åŒ»ç–—æˆåƒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>MK-UNetåœ¨DICEè¯„åˆ†ä¸Šä¼˜äºå…¶ä»–ç½‘ç»œï¼Œå¦‚TransUNetå’ŒUNeXtï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„å‚æ•°å’ŒFLOPsã€‚</li>
<li>MK-UNetçš„è®¡ç®—å¼€é”€è¾ƒå°ï¼Œé€‚åˆåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿›è¡Œå®æ—¶é«˜ä¿çœŸåŒ»ç–—è¯Šæ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed59fd70022ddddde0d1f01ac4f6b11a" align="middle">
<img src="https://picx.zhimg.com/v2-a84aceb0ff9bc71d8c2578fe98033ac5" align="middle">
<img src="https://picx.zhimg.com/v2-bff185d9477a0d8b04b37906146a5d0e" align="middle">
<img src="https://picx.zhimg.com/v2-09becc58db39b4edc32f4b647cb4179b" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Contrastive-Multimodal-Fusion-with-Improved-Modality-Dropout-for-Disease-Detection-and-Prediction"><a href="#Learning-Contrastive-Multimodal-Fusion-with-Improved-Modality-Dropout-for-Disease-Detection-and-Prediction" class="headerlink" title="Learning Contrastive Multimodal Fusion with Improved Modality Dropout   for Disease Detection and Prediction"></a>Learning Contrastive Multimodal Fusion with Improved Modality Dropout   for Disease Detection and Prediction</h2><p><strong>Authors:Yi Gu, Kuniaki Saito, Jiaxin Ma</strong></p>
<p>As medical diagnoses increasingly leverage multimodal data, machine learning models are expected to effectively fuse heterogeneous information while remaining robust to missing modalities. In this work, we propose a novel multimodal learning framework that integrates enhanced modalities dropout and contrastive learning to address real-world limitations such as modality imbalance and missingness. Our approach introduces learnable modality tokens for improving missingness-aware fusion of modalities and augments conventional unimodal contrastive objectives with fused multimodal representations. We validate our framework on large-scale clinical datasets for disease detection and prediction tasks, encompassing both visual and tabular modalities. Experimental results demonstrate that our method achieves state-of-the-art performance, particularly in challenging and practical scenarios where only a single modality is available. Furthermore, we show its adaptability through successful integration with a recent CT foundation model. Our findings highlight the effectiveness, efficiency, and generalizability of our approach for multimodal learning, offering a scalable, low-cost solution with significant potential for real-world clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/medical-modality-dropout">https://github.com/omron-sinicx/medical-modality-dropout</a>. </p>
<blockquote>
<p>éšç€åŒ»å­¦è¯Šæ–­è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦æœ‰æ•ˆåœ°èåˆå¼‚æ„ä¿¡æ¯ï¼ŒåŒæ—¶å¯¹äºç¼ºå¤±çš„æ¨¡æ€ä¹Ÿè¦ä¿æŒç¨³å¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¢å¼ºæ¨¡æ€ä¸¢å¼ƒå’Œå¯¹æ¯”å­¦ä¹ ï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„å±€é™æ€§ï¼Œå¦‚æ¨¡æ€ä¸å¹³è¡¡å’Œç¼ºå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥å¯å­¦ä¹ çš„æ¨¡æ€ä»¤ç‰Œï¼Œä»¥æ”¹è¿›ç¼ºå¤±æ„ŸçŸ¥çš„æ¨¡æ€èåˆï¼Œå¹¶ç”¨èåˆçš„å¤šæ¨¡æ€è¡¨ç¤ºå¢å¼ºä¼ ç»Ÿçš„å•æ¨¡æ€å¯¹æ¯”ç›®æ ‡ã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡ä¸´åºŠæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œç”¨äºç–¾ç—…æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ï¼Œæ¶µç›–è§†è§‰å’Œè¡¨æ ¼æ¨¡æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨åªæœ‰å•ä¸€æ¨¡æ€å¯ç”¨çš„å…·æœ‰æŒ‘æˆ˜æ€§å’Œå®é™…æƒ…æ™¯ä¸­è¡¨ç°å°¤å…¶å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å…¶ä¸æœ€æ–°çš„CTåŸºç¡€æ¨¡å‹çš„æˆåŠŸé›†æˆï¼Œè¯æ˜äº†å…¶é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ã€æˆæœ¬ä½å»‰çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/medical-modality-dropout%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/omron-sinicx/medical-modality-dropoutä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18284v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºæ¨¡æ€ä¸¢å¤±å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆèåˆå¼‚è´¨ä¿¡æ¯ï¼Œå¹¶åº”å¯¹æ¨¡æ€ä¸å¹³è¡¡å’Œç¼ºå¤±ç­‰ç°å®ä¸–ç•Œçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥å¯å­¦ä¹ çš„æ¨¡æ€ä»¤ç‰Œï¼Œæ”¹è¿›äº†ç¼ºå¤±æ„ŸçŸ¥èåˆæ¨¡æ€ï¼Œå¹¶èåˆäº†ä¼ ç»Ÿçš„å•æ¨¡æ€å¯¹æ¯”ç›®æ ‡ã€‚åœ¨å¤§å‹ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œç–¾ç—…æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡éªŒè¯ï¼Œæ¶µç›–è§†è§‰å’Œè¡¨æ ¼æ¨¡æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…æœ‰å•ä¸€æ¨¡æ€çš„å®ç”¨åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œå®ƒä¸æœ€æ–°çš„CTåŸºç¡€æ¨¡å‹æˆåŠŸé›†æˆï¼Œå±•ç°å‡ºå…¶é€‚åº”æ€§ã€‚è¯¥æ¡†æ¶å¯¹äºå¤šæ¨¡æ€å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæ˜¯ä¸€ç§å¯æ‰©å±•ã€ä½æˆæœ¬ä¸”å…·æœ‰å®é™…åº”ç”¨å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œèåˆå¢å¼ºæ¨¡æ€ä¸¢å¤±å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿå¤„ç†æ¨¡æ€ä¸å¹³è¡¡å’Œç¼ºå¤±çš„ç°å®æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ çš„æ¨¡æ€ä»¤ç‰Œï¼Œæ”¹è¿›ç¼ºå¤±æ„ŸçŸ¥èåˆæ¨¡æ€æŠ€æœ¯ã€‚</li>
<li>èåˆäº†ä¼ ç»Ÿçš„å•æ¨¡æ€å¯¹æ¯”ç›®æ ‡ï¼Œå½¢æˆèåˆçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤§å‹ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œç–¾ç—…æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡éªŒè¯ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¡†æ¶æˆåŠŸé›†æˆæœ€è¿‘çš„CTåŸºç¡€æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6189bd830808381148085be7d6f20de0" align="middle">
<img src="https://picx.zhimg.com/v2-a07f98c368bd184cf840663eced19bbe" align="middle">
<img src="https://picx.zhimg.com/v2-17dfa3ffa00f132b7bb8b15b71107366" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Image-Classification-via-Synergistic-Learning-Pre-training"><a href="#Multimodal-Medical-Image-Classification-via-Synergistic-Learning-Pre-training" class="headerlink" title="Multimodal Medical Image Classification via Synergistic Learning   Pre-training"></a>Multimodal Medical Image Classification via Synergistic Learning   Pre-training</h2><p><strong>Authors:Qinghua Lin, Guang-Hai Liu, Zuoyong Li, Yang Li, Yuting Jiang, Xiang Wu</strong></p>
<p>Multimodal pathological images are usually in clinical diagnosis, but computer vision-based multimodal image-assisted diagnosis faces challenges with modality fusion, especially in the absence of expert-annotated data. To achieve the modality fusion in multimodal images with label scarcity, we propose a novel &#96;&#96;pretraining + fine-tuningâ€ framework for multimodal semi-supervised medical image classification. Specifically, we propose a synergistic learning pretraining framework of consistency, reconstructive, and aligned learning. By treating one modality as an augmented sample of another modality, we implement a self-supervised learning pre-train, enhancing the baseline modelâ€™s feature representation capability. Then, we design a fine-tuning method for multimodal fusion. During the fine-tuning stage, we set different encoders to extract features from the original modalities and provide a multimodal fusion encoder for fusion modality. In addition, we propose a distribution shift method for multimodal fusion features, which alleviates the prediction uncertainty and overfitting risks caused by the lack of labeled samples. We conduct extensive experiments on the publicly available gastroscopy image datasets Kvasir and Kvasirv2. Quantitative and qualitative results demonstrate that the proposed method outperforms the current state-of-the-art classification methods. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/LQH89757/MICS">https://github.com/LQH89757/MICS</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç—…ç†å›¾åƒåœ¨ä¸´åºŠè¯Šæ–­ä¸­é€šå¸¸å¾ˆå—æ¬¢è¿ï¼Œä½†åŸºäºè®¡ç®—æœºè§†è§‰çš„å¤šæ¨¡æ€å›¾åƒè¾…åŠ©è¯Šæ–­åœ¨æ¨¡æ€èåˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹ä¸“å®¶æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†å®ç°å¤šæ¨¡æ€å›¾åƒåœ¨æ ‡ç­¾ç¨€ç¼ºæƒ…å†µä¸‹çš„æ¨¡æ€èåˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„â€œé¢„è®­ç»ƒ+å¾®è°ƒâ€æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªååŒå­¦ä¹ çš„é¢„è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€è‡´æ€§ã€é‡å»ºå’Œå¯¹é½å­¦ä¹ ã€‚é€šè¿‡å°†ä¸€ç§æ¨¡æ€è§†ä¸ºå¦ä¸€ç§æ¨¡æ€çš„å¢å¼ºæ ·æœ¬ï¼Œæˆ‘ä»¬å®ç°äº†è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒï¼Œæé«˜äº†åŸºçº¿æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†é’ˆå¯¹å¤šæ¨¡æ€èåˆçš„å¾®è°ƒæ–¹æ³•ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œæˆ‘ä»¬è®¾ç½®ä¸åŒçš„ç¼–ç å™¨ä»åŸå§‹æ¨¡æ€ä¸­æå–ç‰¹å¾ï¼Œå¹¶æä¾›ä¸€ä¸ªå¤šæ¨¡æ€èåˆç¼–ç å™¨è¿›è¡Œèåˆæ¨¡æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èåˆç‰¹å¾åˆ†å¸ƒè½¬ç§»æ–¹æ³•ï¼Œè¿™å‡è½»äº†ç”±äºç¼ºå°‘æ ‡è®°æ ·æœ¬å¯¼è‡´çš„é¢„æµ‹ä¸ç¡®å®šæ€§å’Œè¿‡æ‹Ÿåˆé£é™©ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„èƒƒé•œå›¾åƒæ•°æ®é›†Kvasirå’ŒKvasirv2ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åˆ†ç±»æ–¹æ³•ã€‚ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/LQH89757/MICS">https://github.com/LQH89757/MICS</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17492v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¤šæ¨¡æ€è¯Šæ–­é¢ä¸´æ ‡ç­¾ç¨€ç¼ºå’Œæ¨¡æ€èåˆçš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„â€œé¢„è®­ç»ƒ+å¾®è°ƒâ€æ¡†æ¶ï¼Œå®ç°åŠç›‘ç£åŒ»å­¦å›¾åƒå¤šæ¨¡æ€åˆ†ç±»ã€‚é€šè¿‡ä¸€è‡´æ€§ã€é‡å»ºå’Œå¯¹é½å­¦ä¹ çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæé«˜æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ¨¡æ€èåˆçš„å¾®è°ƒæ–¹æ³•ã€‚åœ¨å…¬å¼€å¯ç”¨çš„èƒƒé•œå›¾åƒæ•°æ®é›†Kvasirå’ŒKvasirv2ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åˆ†ç±»æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒè¯Šæ–­é¢ä¸´æ ‡ç­¾ç¨€ç¼ºå’Œæ¨¡æ€èåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„â€œé¢„è®­ç»ƒ+å¾®è°ƒâ€æ¡†æ¶ï¼Œç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒå¤šæ¨¡æ€åˆ†ç±»ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒæ¡†æ¶æé«˜æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œé‡‡ç”¨ä¸€è‡´æ€§ã€é‡å»ºå’Œå¯¹é½å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ¨¡æ€èåˆçš„å¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸åŒç¼–ç å™¨æå–ç‰¹å¾ã€èåˆæ¨¡æ€çš„ç¼–ç å™¨ä»¥åŠç‰¹å¾åˆ†å¸ƒè½¬æ¢æ–¹æ³•ã€‚</li>
<li>ç‰¹å¾åˆ†å¸ƒè½¬æ¢æ–¹æ³•èƒ½å‡è½»å› ç¼ºä¹æ ‡ç­¾æ ·æœ¬å¯¼è‡´çš„é¢„æµ‹ä¸ç¡®å®šæ€§å’Œè¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†Kvasirå’ŒKvasirv2ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c39d76aeca17ca043c0f8c2c67fe6a78" align="middle">
<img src="https://picx.zhimg.com/v2-853f46e2f337756deaffa2eb56b7d644" align="middle">
<img src="https://picx.zhimg.com/v2-23189dc0c6f43637ea0f48bb749b02ae" align="middle">
<img src="https://picx.zhimg.com/v2-faf309fa88fd6909710f3eb0a3072180" align="middle">
<img src="https://picx.zhimg.com/v2-a887e7183afb0d87736d30b63d8e4e66" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Implicit-Neural-Representations-of-Intramyocardial-Motion-and-Strain"><a href="#Implicit-Neural-Representations-of-Intramyocardial-Motion-and-Strain" class="headerlink" title="Implicit Neural Representations of Intramyocardial Motion and Strain"></a>Implicit Neural Representations of Intramyocardial Motion and Strain</h2><p><strong>Authors:Andrew Bell, Yan Kit Choi, Steffen E Petersen, Andrew King, Muhummad Sohaib Nazir, Alistair A Young</strong></p>
<p>Automatic quantification of intramyocardial motion and strain from tagging MRI remains an important but challenging task. We propose a method using implicit neural representations (INRs), conditioned on learned latent codes, to predict continuous left ventricular (LV) displacement â€“ without requiring inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined error in global circumferential (2.86%) and radial (6.42%) strain compared to three deep learning baselines. In addition, our method is $\sim$380$\times$ faster than the most accurate baseline. These results highlight the suitability of INR-based models for accurate and scalable analysis of myocardial strain in large CMR datasets. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/andrewjackbell/Displacement-INR">https://github.com/andrewjackbell/Displacement-INR</a> </p>
<blockquote>
<p>å¿ƒè‚Œå†…è¿åŠ¨çš„è‡ªåŠ¨é‡åŒ–ä»¥åŠä»æ ‡è®°MRIä¸­è·å¾—çš„åº”å˜ä»ç„¶æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨åŸºäºå­¦ä¹ æ½œåœ¨ä»£ç çš„æ¡ä»¶éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¥é¢„æµ‹å·¦å¿ƒå®¤ï¼ˆLVï¼‰è¿ç»­ä½ç§»çš„æ–¹æ³•ï¼Œè€Œæ— éœ€åœ¨æ¨ç†æ—¶é—´è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨452ä¾‹è‹±å›½ç”Ÿç‰©é“¶è¡Œæµ‹è¯•æ¡ˆä¾‹ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ä¸‰ä¸ªæ·±åº¦å­¦ä¹ åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€ä½³çš„è·Ÿè¸ªç²¾åº¦ï¼ˆ2.14æ¯«ç±³RMSEï¼‰ï¼Œå¹¶ä¸”åœ¨å…¨çƒåœ†å‘¨åº”å˜ï¼ˆ2.86%ï¼‰å’Œå¾„å‘åº”å˜ï¼ˆ6.42%ï¼‰æ–¹é¢çš„ç»„åˆè¯¯å·®æœ€ä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å‡†ç¡®çš„åŸºçº¿å¿«çº¦380å€ã€‚è¿™äº›ç»“æœçªå‡ºäº†INRæ¨¡å‹åœ¨å¤§å‹CMRæ•°æ®é›†ä¸­è¿›è¡Œå¿ƒè‚Œåº”å˜å‡†ç¡®å’Œå¯ä¼¸ç¼©åˆ†æçš„é€‚ç”¨æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/andrewjackbell/Displacement-INR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/andrewjackbell/Displacement-INRæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09004v4">PDF</a> STACOM 2025 @ MICCAI</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§ä½¿ç”¨éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æ½œåœ¨ä»£ç è¿›è¡Œé¢„æµ‹å·¦å¿ƒå®¤ä½ç§»ï¼Œæ— éœ€è¿›è¡Œæ¨ç†æ—¶é—´ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„è·Ÿè¸ªç²¾åº¦å’Œå¿«é€Ÿçš„è®¡ç®—é€Ÿåº¦ï¼Œä¸ºå¤§è§„æ¨¡å¿ƒè„ç£å…±æŒ¯æ•°æ®é›†ä¸­å¿ƒè‚Œåº”å˜åˆ†ææä¾›äº†å‡†ç¡®ä¸”å¯é‡åŒ–çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨é‡åŒ–å¿ƒè‚Œè¿åŠ¨å’Œåº”å˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸éœ€è¦æ¨ç†æ—¶é—´ä¼˜åŒ–çš„æƒ…å†µä¸‹é¢„æµ‹å·¦å¿ƒå®¤ä½ç§»ã€‚</li>
<li>åœ¨UK Biobankæµ‹è¯•æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„è·Ÿè¸ªç²¾åº¦è¾¾åˆ°æœ€ä½³ï¼ˆRMSEä¸º2.14mmï¼‰ã€‚</li>
<li>ä¸å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å…¨çƒåœ†å‘¨å’Œå¾„å‘åº”å˜æ–¹é¢çš„ç»¼åˆè¯¯å·®æœ€ä½ã€‚</li>
<li>è¯¥æ–¹æ³•çš„è®¡ç®—é€Ÿåº¦æ¯”æœ€å‡†ç¡®çš„åŸºçº¿æ¨¡å‹å¿«çº¦380å€ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†éšç¥ç»è¡¨ç¤ºæ¨¡å‹åœ¨å¿ƒè‚Œåº”å˜åˆ†æä¸­çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4989985cdc59aa21dbadf0a74c745d81" align="middle">
<img src="https://picx.zhimg.com/v2-df360f8497b51738373884d9168fe1ad" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Clustering-methods-for-Categorical-Time-Series-and-Sequences-A-scoping-review"><a href="#Clustering-methods-for-Categorical-Time-Series-and-Sequences-A-scoping-review" class="headerlink" title="Clustering methods for Categorical Time Series and Sequences : A scoping   review"></a>Clustering methods for Categorical Time Series and Sequences : A scoping   review</h2><p><strong>Authors:Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, FranÃ§ois Petit</strong></p>
<p>Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.   Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families â€“ distance-based, feature-based, and model-based â€“ and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.   Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( <a target="_blank" rel="noopener" href="https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/">https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/</a> )   Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.   Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases. </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ—¨åœ¨ä¸ºåˆ†ç±»æ—¶é—´åºåˆ—ï¼ˆCTSï¼‰çš„èšç±»æ–¹æ³•æä¾›æ¦‚è¿°ã€‚CTSæ˜¯å¸¸è§äºæµè¡Œç—…å­¦ã€ç¤¾ä¼šå­¦ã€ç”Ÿç‰©å­¦å’Œå¸‚åœºè¥é”€ç­‰é¢†åŸŸçš„ä¸€ç§æ•°æ®ç»“æ„ï¼Œå¹¶é’ˆå¯¹æ•°æ®ç‰¹æ€§æ”¯æŒæ–¹æ³•é€‰æ‹©ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æœç´¢äº†PubMedã€Web of Scienceå’ŒGoogle Scholarï¼Œä»åˆ›åˆŠè‡³2024å¹´11æœˆçš„æ–‡çŒ®ï¼Œä»¥è¯†åˆ«å’Œè¯„ä¼°CTSçš„èšç±»æŠ€æœ¯ã€‚æ–¹æ³•è¢«åˆ†ä¸ºä¸‰å¤§ç±»åˆ«â€”â€”åŸºäºè·ç¦»çš„ã€åŸºäºç‰¹å¾çš„ã€åŸºäºæ¨¡å‹çš„ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¤„ç†æ•°æ®æŒ‘æˆ˜çš„èƒ½åŠ›ï¼Œä¾‹å¦‚å¯å˜åºåˆ—é•¿åº¦ã€å¤šå…ƒæ•°æ®ã€è¿ç»­æ—¶é—´ã€ç¼ºå¤±æ•°æ®ã€æ—¶é—´æ’å®šåå˜é‡å’Œå¤§æ•°æ®é‡ã€‚ç»“æœï¼šåœ¨14607é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çº³å…¥äº†124ç¯‡æ–‡ç« ï¼Œæè¿°äº†129ç§æ–¹æ³•ï¼Œæ¶‰åŠäººå·¥æ™ºèƒ½ã€ç¤¾ä¼šç§‘å­¦å’Œæµè¡Œç—…å­¦ç­‰é¢†åŸŸã€‚åŸºäºè·ç¦»çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æœ€ä½³åŒ¹é…çš„æ–¹æ³•æœ€ä¸ºæ™®éï¼Œå…±æœ‰56ç§æ–¹æ³•ã€‚æˆ‘ä»¬ç¡®å®šäº†28ç§åŸºäºæ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨åº”å¯¹å¤šå…ƒæ•°æ®ã€è¿ç»­æ—¶é—´å’Œæ—¶é—´æ’å®šåå˜é‡ç­‰å¤æ‚æ•°æ®ç»“æ„æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è¿˜è®°å½•äº†45ç§åŸºäºç‰¹å¾çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨å¹³å‡æƒ…å†µä¸‹æ›´å…·å¯æ‰©å±•æ€§ä½†çµæ´»æ€§è¾ƒå·®ã€‚å¼€å‘äº†ä¸€ä¸ªå¯æœç´¢çš„Webåº”ç”¨ç¨‹åºï¼Œä»¥ä¾¿æ ¹æ®æ•°æ®é›†ç‰¹æ€§ä¿ƒè¿›æ–¹æ³•é€‰æ‹©ï¼ˆ <a target="_blank" rel="noopener" href="https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/">https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/</a> ï¼‰ã€‚è®¨è®ºï¼šè™½ç„¶åŸºäºè·ç¦»çš„æ–¹æ³•å ä¸»å¯¼åœ°ä½ï¼Œä½†åŸºäºæ¨¡å‹çš„æ–¹æ³•æä¾›äº†æœ€ä¸°å¯Œçš„å»ºæ¨¡æ½œåŠ›ï¼Œä½†æ‰©å±•æ€§è¾ƒå·®ã€‚åŸºäºç‰¹å¾çš„æ–¹æ³•æ›´ä¾§é‡äºæ€§èƒ½è€Œéçµæ´»æ€§ï¼Œå¯¹å¤æ‚æ•°æ®ç»“æ„çš„æ”¯æŒæœ‰é™ã€‚ç»“è®ºï¼šæœ¬ç»¼è¿°çªå‡ºäº†CTSèšç±»ä¸­çš„æ–¹æ³•å¤šæ ·æ€§å’Œç©ºç™½ã€‚æå‡ºçš„åˆ†ç±»æ—¨åœ¨æŒ‡å¯¼ç ”ç©¶äººå‘˜ä¸ºå…¶ç‰¹å®šç”¨ä¾‹é€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07885v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¦‚è¿°äº†é’ˆå¯¹ç±»åˆ«æ—¶é—´åºåˆ—ï¼ˆCTSï¼‰æ•°æ®çš„èšç±»æ–¹æ³•ï¼Œè¯¥ç±»æ•°æ®å¹¿æ³›å­˜åœ¨äºæµè¡Œç—…å­¦ã€ç¤¾ä¼šå­¦ã€ç”Ÿç‰©å­¦å’Œå¸‚åœºè¥é”€ç­‰é¢†åŸŸã€‚æ–‡ç« é€šè¿‡æ£€ç´¢æ•°æ®åº“å’Œæ–‡çŒ®ï¼Œå¯¹CTSèšç±»æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»å’Œè¯„ä¼°ï¼ŒåŒ…æ‹¬è·ç¦»åŸºç¡€ã€ç‰¹å¾åŸºç¡€å’Œæ¨¡å‹åŸºç¡€ä¸‰å¤§ç±»ï¼Œå¹¶é’ˆå¯¹æ•°æ®ç‰¹æ€§è¿›è¡Œäº†æ–¹æ³•é€‰æ‹©ã€‚ç ”ç©¶å‘ç°è·ç¦»åŸºç¡€æ–¹æ³•æœ€ä¸ºæ™®éï¼Œæ¨¡å‹åŸºç¡€æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®ç»“æ„æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çµæ´»æ€§ï¼Œè€Œç‰¹å¾åŸºç¡€æ–¹æ³•åˆ™æ›´ä¾§é‡äºæ€§èƒ½ã€‚æ–‡ç« æœ€åæŒ‡å‡ºäº†èšç±»æ–¹æ³•çš„å¤šæ ·æ€§å’Œç°æœ‰å·®è·ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…é€‰æ‹©åˆé€‚çš„æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æä¾›äº†ç±»åˆ«æ—¶é—´åºåˆ—ï¼ˆCTSï¼‰èšç±»æ–¹æ³•çš„æ¦‚è¿°ï¼Œæ—¨åœ¨æŒ‡å¯¼ç ”ç©¶è€…æ ¹æ®ç‰¹å®šç”¨ä¾‹é€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ–‡çŒ®æ£€ç´¢ï¼Œå¯¹CTSèšç±»æ–¹æ³•è¿›è¡Œäº†è·ç¦»åŸºç¡€ã€ç‰¹å¾åŸºç¡€å’Œæ¨¡å‹åŸºç¡€ä¸‰å¤§ç±»çš„åˆ†ç±»ã€‚</li>
<li>è·ç¦»åŸºç¡€æ–¹æ³•æœ€ä¸ºæ™®éï¼Œä½†æ¨¡å‹åŸºç¡€æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®ç»“æ„æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çµæ´»æ€§ã€‚</li>
<li>ç‰¹å¾åŸºç¡€æ–¹æ³•æ›´ä¾§é‡äºæ€§èƒ½ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°æ®ç»“æ„ä¸Šæœ‰é™åˆ¶ã€‚</li>
<li>èšç±»æ–¹æ³•çš„é€‰æ‹©éœ€è€ƒè™‘æ•°æ®ç‰¹æ€§ï¼Œå¦‚åºåˆ—é•¿åº¦ã€å¤šå…ƒæ•°æ®ã€è¿ç»­æ—¶é—´ã€ç¼ºå¤±æ•°æ®ç­‰ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºå½“å‰èšç±»æ–¹æ³•çš„å¤šæ ·æ€§å’Œç°æœ‰å·®è·ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c2d40c3ea500071a0dfb24dea097fbc" align="middle">
<img src="https://picx.zhimg.com/v2-d64417e50c09a7080e97a466091e8ba0" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12"><a href="#Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12" class="headerlink" title="Robust Pan-Cancer Mitotic Figure Detection with YOLOv12"></a>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</h2><p><strong>Authors:RaphaÃ«l Bourgade, Guillaume Balezo, Thomas Walter</strong></p>
<p>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data. </p>
<blockquote>
<p>æœ‰ä¸åˆ†è£‚å›¾åƒåœ¨è‚¿ç˜¤ç—…ç†å­¦ä¸­ä»£è¡¨äº†å…³é”®çš„ç»„ç»‡å­¦é¢„åç‰¹å¾ï¼Œä¸ºç†è§£è‚¿ç˜¤çš„ä¾µè¢­æ€§å’Œå¢æ®–æä¾›äº†é‡è¦ä¾æ®ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¯†åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå³ä½¿æ˜¯ç»éªŒä¸°å¯Œçš„ç—…ç†å­¦å®¶ä¹‹é—´ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ‰ä¸åˆ†è£‚åŸŸæ³›åŒ–ï¼ˆMIDOGï¼‰2025æŒ‘æˆ˜èµ›æ˜¯å›½é™…ç«èµ›çš„ç¬¬ä¸‰ç‰ˆï¼Œæ—¨åœ¨å¼€å‘ç¨³å¥çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ç®—æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°YOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„æœ‰ä¸åˆ†è£‚å›¾åƒæ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæ­¥æµ‹è¯•é›†ï¼ˆä»…çƒ­ç‚¹ï¼‰ä¸Šè¾¾åˆ°äº†0.801çš„F1åˆ†æ•°ï¼Œå¹¶åœ¨æœ€ç»ˆæµ‹è¯•æ’è¡Œæ¦œä¸Šä»¥0.7216çš„F1åˆ†æ•°æ’åç¬¬äºŒï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ï¼Œå³å¯åœ¨å¤æ‚ä¸”å¼‚è´¨çš„æ•´å¼ å¹»ç¯ç‰‡åŒºåŸŸè¿›è¡Œæ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02593v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæœ€æ–°YOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„ç»†èƒåˆ†è£‚å›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯å›½é™…ç¬¬ä¸‰æ¬¡ç»†èƒåˆ†è£‚æ£€æµ‹æŒ‘æˆ˜èµ›çš„é‡è¦æˆæœä¹‹ä¸€ã€‚æœ¬æ–‡æ–¹æ³•å¯ä»¥åœ¨å¤æ‚çš„å…¨æ»‘åŒºåŸŸä¸­å‡†ç¡®æ£€æµ‹ç»†èƒåˆ†è£‚å›¾åƒï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ï¼Œåœ¨åˆæ­¥æµ‹è¯•é›†ä¸Šå–å¾—äº†F1åˆ†æ•°ä¸º0.801çš„å¥½æˆç»©ï¼Œå¹¶åœ¨æœ€ç»ˆæµ‹è¯•ä¸­æ’åç¬¬äºŒï¼Œå±•ç°äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç»†èƒåˆ†è£‚å›¾åƒæ£€æµ‹æ˜¯è‚¿ç˜¤ç—…ç†å­¦ä¸­çš„å…³é”®ç‰¹å¾ï¼Œå¯¹è‚¿ç˜¤ä¾µè¢­æ€§å’Œå¢æ®–æ€§çš„è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç»†èƒåˆ†è£‚æ£€æµ‹é¢ä¸´è§‚å¯Ÿè€…é—´å·®å¼‚å¤§çš„æŒ‘æˆ˜ï¼Œå›½é™…ä¸Šçš„MIDOGæŒ‘æˆ˜èµ›æ—¨åœ¨å¼€å‘ç¨³å¥çš„ç»†èƒåˆ†è£‚æ£€æµ‹ç®—æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºYOLOv12æ¶æ„çš„ç»†èƒåˆ†è£‚å›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œåˆæ­¥æµ‹è¯•æˆç»©ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨å¤æ‚çš„å…¨æ»‘åŒºåŸŸä¸­å‡†ç¡®æ£€æµ‹ç»†èƒåˆ†è£‚å›¾åƒï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æœ€ç»ˆæµ‹è¯•ä¸­æ’åç¬¬äºŒï¼Œè¡¨æ˜å…¶æ€§èƒ½ä¼˜å¼‚ä¸”å…·æœ‰æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08ed8a7559fa45d662d81ab8eac00c52" align="middle">
<img src="https://picx.zhimg.com/v2-761c48e32872be873d51812cbc41c716" align="middle">
<img src="https://picx.zhimg.com/v2-09c94b7a55797b71b9b435ed72460743" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-and-Multi-centric-Head-and-Neck-Cancer-Dataset-for-Segmentation-Diagnosis-and-Outcome-Prediction"><a href="#A-Multimodal-and-Multi-centric-Head-and-Neck-Cancer-Dataset-for-Segmentation-Diagnosis-and-Outcome-Prediction" class="headerlink" title="A Multimodal and Multi-centric Head and Neck Cancer Dataset for   Segmentation, Diagnosis and Outcome Prediction"></a>A Multimodal and Multi-centric Head and Neck Cancer Dataset for   Segmentation, Diagnosis and Outcome Prediction</h2><p><strong>Authors:Numan Saeed, Salma Hassan, Shahad Hardan, Ahmed Aly, Darya Taratynova, Umair Nawaz, Ufaq Khan, Muhammad Ridzuan, Vincent Andrearczyk, Adrien Depeursinge, Yutong Xie, Thomas Eugene, RaphaÃ«l Metz, MÃ©lanie Dore, Gregory Delpon, Vijay Ram Kumar Papineni, Kareem Wahid, Cem Dede, Alaa Mohamed Shawky Ali, Carlos Sjogreen, Mohamed Naser, Clifton D. Fuller, Valentin Oreiller, Mario Jreige, John O. Prior, Catherine Cheze Le Rest, Olena Tankyevych, Pierre Decazes, Su Ruan, Stephanie Tanadini-Lang, Martin ValliÃ¨res, Hesham Elhalawani, Ronan Abgral, Romain Floch, Kevin Kerleguer, Ulrike Schick, Maelle Mauguen, David Bourhis, Jean-Christophe Leclere, Amandine Sambourg, Arman Rahmim, Mathieu Hatt, Mohammad Yaqub</strong></p>
<p>We present a publicly available multimodal dataset for head and neck cancer research, comprising 1123 annotated Positron Emission Tomography&#x2F;Computed Tomography (PET&#x2F;CT) studies from patients with histologically confirmed disease, acquired from 10 international medical centers. All studies contain co-registered PET&#x2F;CT scans with varying acquisition protocols, reflecting real-world clinical diversity from a long-term, multi-institution retrospective collection. Primary gross tumor volumes (GTVp) and involved lymph nodes (GTVn) were manually segmented by experienced radiation oncologists and radiologists following established guidelines. We provide anonymized NifTi files, expert-annotated segmentation masks, comprehensive clinical metadata, and radiotherapy dose distributions for a patient subset. The metadata include TNM staging, HPV status, demographics, long-term follow-up outcomes, survival times, censoring indicators, and treatment information. To demonstrate its utility, we benchmark three key clinical tasks: automated tumor segmentation, recurrence-free survival prediction, and HPV status classification, using state-of-the-art deep learning models like UNet, SegResNet, and multimodal prognostic frameworks. </p>
<blockquote>
<p>æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¤´é¢ˆéƒ¨ç™Œç—‡ç ”ç©¶å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«ä»å›½é™…ä¸Š10ä¸ªåŒ»ç–—ä¸­å¿ƒé‡‡é›†çš„ç»ç»„ç»‡ç—…ç†å­¦ç¡®è¯Šç–¾ç—…çš„æ‚£è€…çš„PET&#x2F;CTç ”ç©¶æ³¨è§£å…±1123é¡¹ã€‚æ‰€æœ‰ç ”ç©¶éƒ½åŒ…å«ä½¿ç”¨ä¸åŒé‡‡é›†åè®®çš„å…±æ³¨å†ŒPET&#x2F;CTæ‰«æï¼Œåæ˜ é•¿æœŸå¤šæœºæ„å›é¡¾æ€§æ”¶é›†çš„æ¥è‡ªçœŸå®ä¸–ç•Œçš„ä¸´åºŠå¤šæ ·æ€§ã€‚åŸå‘è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVpï¼‰å’Œå—ç´¯æ·‹å·´ç»“ï¼ˆGTVnï¼‰ç”±ç»éªŒä¸°å¯Œçš„æ”¾å°„è‚¿ç˜¤å­¦å®¶å’Œæ”¾å°„ç§‘åŒ»ç”ŸæŒ‰ç…§æ—¢å®šæŒ‡å—è¿›è¡Œæ‰‹åŠ¨åˆ†å‰²ã€‚æˆ‘ä»¬æä¾›åŒ¿ååŒ–çš„NifTiæ–‡ä»¶ã€ä¸“å®¶æ³¨é‡Šçš„åˆ†å‰²æ©è†œã€å…¨é¢çš„ä¸´åºŠå…ƒæ•°æ®ä»¥åŠæ‚£è€…å­é›†çš„æ”¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒã€‚å…ƒæ•°æ®åŒ…æ‹¬TNMåˆ†æœŸã€HPVçŠ¶æ€ã€äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ã€é•¿æœŸéšè®¿ç»“æœã€ç”Ÿå­˜æ—¶é—´ã€å®¡æŸ¥æŒ‡æ ‡å’Œæ²»ç–—ä¿¡æ¯ã€‚ä¸ºäº†è¯æ˜å…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¯¹ä¸‰ä¸ªå…³é”®çš„ä¸´åºŠä»»åŠ¡è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼šä½¿ç”¨UNetã€SegResNetç­‰æœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œå¤šç§æ¨¡å¼é¢„åæ¡†æ¶è¿›è¡Œè‡ªåŠ¨è‚¿ç˜¤åˆ†å‰²ã€æ— å¤å‘ç”Ÿå­˜é¢„æµ‹å’ŒHPVçŠ¶æ€åˆ†ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00367v3">PDF</a> 10 pages, 5 figures. Numan Saeed is the corresponding author. Numan   Saeed, Salma Hassan and Shahad Hardan contributed equally to this work.   Project page: <a target="_blank" rel="noopener" href="https://hecktor25.grand-challenge.org/">https://hecktor25.grand-challenge.org/</a></p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ä¸ªå…³äºå¤´é¢ˆç™Œç ”ç©¶çš„å¤šæ¨¡å¼æ•°æ®é›†ä»‹ç»ï¼ŒåŒ…å«æ¥è‡ªåä¸ªå›½é™…åŒ»ç–—ä¸­å¿ƒçš„1123ä»½ç»è¿‡æ³¨é‡Šçš„PET&#x2F;CTç ”ç©¶ã€‚æ•°æ®é›†ä¸­åŒ…å«äº†æ‰‹åŠ¨åˆ†å‰²çš„ä¸»è¦è‚¿ç˜¤ä½“ç§¯å’Œæ¶‰åŠçš„æ·‹å·´ç»“ï¼Œä»¥åŠåŒ…æ‹¬TNMåˆ†æœŸã€HPVçŠ¶æ€ã€äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ã€é•¿æœŸéšè®¿ç»“æœç­‰åœ¨å†…çš„ç»¼åˆä¸´åºŠå…ƒæ•°æ®ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ç”¨äºæ‚£è€…å­é›†çš„åŒ¿åNifTiæ–‡ä»¶ã€ä¸“å®¶æ³¨é‡Šçš„åˆ†å‰²æ©æ¨¡ã€æ”¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒå›¾ã€‚æ•°æ®é›†å¯ç”¨äºè‡ªåŠ¨åŒ–è‚¿ç˜¤åˆ†å‰²ã€æ— å¤å‘ç”Ÿå­˜é¢„æµ‹å’ŒHPVçŠ¶æ€åˆ†ç±»ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ•°æ®é›†æ˜¯ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç”¨äºå¤´é¢ˆç™Œç ”ç©¶ï¼Œæ¶µç›–äº†å›½é™…åä¸ªåŒ»ç–—ä¸­å¿ƒçš„PET&#x2F;CTç ”ç©¶æ•°æ®ã€‚</li>
<li>æ•°æ®é›†ä¸­åŒ…å«äº†æ‰‹åŠ¨åˆ†å‰²çš„ä¸»è¦è‚¿ç˜¤ä½“ç§¯å’Œæ¶‰åŠçš„æ·‹å·´ç»“ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº†ä¸°å¯Œçš„ä¸´åºŠå…ƒæ•°æ®ï¼Œå¦‚TNMåˆ†æœŸã€HPVçŠ¶æ€ç­‰ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†åŒ¿ååŒ–çš„NifTiæ–‡ä»¶ã€ä¸“å®¶æ³¨é‡Šçš„åˆ†å‰²æ©æ¨¡å’Œæ”¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒå›¾ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºè‡ªåŠ¨åŒ–è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºæ— å¤å‘ç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53a4613a106291df423068d41479c866" align="middle">
<img src="https://picx.zhimg.com/v2-5a3c62176d001aaee56989b07310c820" align="middle">
<img src="https://picx.zhimg.com/v2-93bcd2db7606ea6c5a5c1670cb6ee4df" align="middle">
<img src="https://picx.zhimg.com/v2-386283296385cc76b0bb5beca5f5b4aa" align="middle">
<img src="https://picx.zhimg.com/v2-1045739675af2de135264be2b7351a50" align="middle">
<img src="https://picx.zhimg.com/v2-5c6ef97489a5e841a0334a8486915b04" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Deep-Learning-for-Phyllodes-Tumor-Classification-from-Ultrasound-and-Clinical-Data"><a href="#Multimodal-Deep-Learning-for-Phyllodes-Tumor-Classification-from-Ultrasound-and-Clinical-Data" class="headerlink" title="Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data"></a>Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data</h2><p><strong>Authors:Farhan Fuad Abir, Abigail Elliott Daly, Kyle Anderman, Tolga Ozmen, Laura J. Brattain</strong></p>
<p>Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are difficult to classify preoperatively due to their radiological similarity to benign fibroadenomas. This often leads to unnecessary surgical excisions. To address this, we propose a multimodal deep learning framework that integrates breast ultrasound (BUS) images with structured clinical data to improve diagnostic accuracy. We developed a dual-branch neural network that extracts and fuses features from ultrasound images and patient metadata from 81 subjects with confirmed PTs. Class-aware sampling and subject-stratified 5-fold cross-validation were applied to prevent class imbalance and data leakage. The results show that our proposed multimodal method outperforms unimodal baselines in classifying benign versus borderline&#x2F;malignant PTs. Among six image encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and 0.7294, respectively. This study demonstrates the potential of multimodal AI to serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and improving clinical decision-making in breast tumor management. </p>
<blockquote>
<p>å¶çŠ¶è‚¿ç˜¤ï¼ˆPTsï¼‰æ˜¯ä¸€ç§ç½•è§çš„çº¤ç»´ä¸Šçš®æ€§ä¹³è…ºç—…å˜ï¼Œç”±äºå…¶ä¸è‰¯æ€§çº¤ç»´è…ºç˜¤çš„æ”¾å°„å­¦ç›¸ä¼¼æ€§ï¼Œæœ¯å‰éš¾ä»¥åˆ†ç±»ï¼Œè¿™å¸¸å¸¸å¯¼è‡´ä¸å¿…è¦çš„æ‰‹æœ¯åˆ‡é™¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¹³è…ºè¶…å£°ï¼ˆBUSï¼‰å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒåˆ†æ”¯ç¥ç»ç½‘ç»œï¼Œä»è¶…å£°å›¾åƒå’Œæ¥è‡ª81åå·²ç¡®è¯ŠPTsæ‚£è€…çš„æ‚£è€…å…ƒæ•°æ®ä¸­æå–å¹¶èåˆç‰¹å¾ã€‚åº”ç”¨ç±»åˆ«æ„ŸçŸ¥é‡‡æ ·å’Œå—è¯•è€…åˆ†å±‚5å€äº¤å‰éªŒè¯ï¼Œä»¥é˜²æ­¢ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®æ³„éœ²ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å¤šæ¨¡å¼æ–¹æ³•åœ¨åˆ†ç±»è‰¯æ€§ä¸è¾¹ç•Œæ€§æˆ–æ¶æ€§PTsæ—¶ä¼˜äºå•æ¨¡å¼åŸºçº¿æ–¹æ³•ã€‚åœ¨å…­ç§å›¾åƒç¼–ç å™¨ä¸­ï¼ŒConvNeXtå’ŒResNet18åœ¨å¤šæ¨¡å¼è®¾ç½®ä¸­è¡¨ç°æœ€ä½³ï¼ŒAUC-ROCå¾—åˆ†åˆ†åˆ«ä¸º0.9427å’Œ0.9349ï¼ŒF1å¾—åˆ†åˆ†åˆ«ä¸º0.6720å’Œ0.7294ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡å¼äººå·¥æ™ºèƒ½ä½œä¸ºéä¾µå…¥æ€§è¯Šæ–­å·¥å…·çš„æ½œåŠ›ï¼Œå¯ä»¥å‡å°‘ä¸å¿…è¦çš„æ´»æ£€ï¼Œæé«˜ä¹³è…ºè‚¿ç˜¤ç®¡ç†çš„ä¸´åºŠå†³ç­–æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00213v2">PDF</a> IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS   BSN 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¹³è…ºè¶…å£°å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œæ—¨åœ¨æé«˜å¶çŠ¶è‚¿ç˜¤çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚é€šè¿‡å¼€å‘ä¸€ä¸ªåŒåˆ†æ”¯ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œä»è¶…å£°å›¾åƒå’Œæ‚£è€…å…ƒæ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶ä»81åç¡®è¯Šä¸ºå¶çŠ¶è‚¿ç˜¤çš„å—è¯•è€…ä¸­è·å–æ•°æ®ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ç±»æ„ŸçŸ¥é‡‡æ ·å’Œåˆ†å±‚5å€äº¤å‰éªŒè¯ï¼Œä»¥é¿å…ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®æ³„éœ²é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„å¤šæ¨¡æ€æ–¹æ³•åœ¨åˆ†ç±»è‰¯æ€§ä¸è¾¹ç•Œæ€§æˆ–æ¶æ€§å¶çŠ¶è‚¿ç˜¤æ–¹é¢ä¼˜äºå•æ¨¡æ€åŸºçº¿ã€‚åœ¨å¤šç§å›¾åƒç¼–ç å™¨ä¸­ï¼ŒConvNeXtå’ŒResNet18åœ¨å¤šæ¨¡æ€è®¾ç½®ä¸­è¡¨ç°æœ€ä½³ï¼ŒAUC-ROCå¾—åˆ†åˆ†åˆ«ä¸º0.9427å’Œ0.9349ï¼ŒF1åˆ†æ•°åˆ†åˆ«ä¸º0.6720å’Œ0.7294ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ä½œä¸ºéä¾µå…¥æ€§è¯Šæ–­å·¥å…·çš„æ½œåŠ›ï¼Œæœ‰æœ›é™ä½ä¸å¿…è¦çš„æ´»æ£€ç‡ï¼Œæé«˜ä¹³è…ºè‚¿ç˜¤ç®¡ç†çš„ä¸´åºŠå†³ç­–æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¶çŠ¶è‚¿ç˜¤ï¼ˆPTsï¼‰çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆä¹³è…ºè¶…å£°å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ†ç±»PTsã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åŒåˆ†æ”¯ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿä»è¶…å£°å›¾åƒå’Œæ‚£è€…å…ƒæ•°æ®ä¸­æå–ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†æ‰€æå‡ºçš„å¤šæ¨¡æ€æ–¹æ³•ä¼˜äºå•æ¨¡æ€åŸºçº¿æ–¹æ³•åœ¨åˆ†ç±»PTsæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>åœ¨å¤šç§å›¾åƒç¼–ç å™¨ä¸­ï¼ŒConvNeXtå’ŒResNet18è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„åº”ç”¨æœ‰æœ›é™ä½ä¸å¿…è¦çš„æ´»æ£€ç‡ï¼Œæé«˜ä¸´åºŠå†³ç­–æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75b1168716067cd0415107dcacbb12eb" align="middle">
<img src="https://picx.zhimg.com/v2-9f6f98aad44ca8657d3155a7dbcf2fb0" align="middle">
<img src="https://picx.zhimg.com/v2-cc994803e7bf11907eeb6ae4f904a24a" align="middle">
<img src="https://picx.zhimg.com/v2-f19872b5568a29c5f2ff8b9fa79261cf" align="middle">
<img src="https://picx.zhimg.com/v2-080aa3b5008d9ae20dca9fcb207b15a5" align="middle">
<img src="https://picx.zhimg.com/v2-e9ce53279e53895dbc66f8c719d4f0ce" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cross-Cancer-Knowledge-Transfer-in-WSI-based-Prognosis-Prediction"><a href="#Cross-Cancer-Knowledge-Transfer-in-WSI-based-Prognosis-Prediction" class="headerlink" title="Cross-Cancer Knowledge Transfer in WSI-based Prognosis Prediction"></a>Cross-Cancer Knowledge Transfer in WSI-based Prognosis Prediction</h2><p><strong>Authors:Pei Liu, Luping Ji, Jiaxiang Gou, Xiangxiang Zeng</strong></p>
<p>Whole-Slide Image (WSI) is an important tool for estimating cancer prognosis. Current studies generally follow a conventional cancer-specific paradigm where one cancer corresponds to one model. However, it naturally struggles to scale to rare tumors and cannot utilize the knowledge of other cancers. Although a multi-task learning-like framework has been studied recently, it usually has high demands on computational resources and needs considerable costs in iterative training on ultra-large multi-cancer WSI datasets. To this end, this paper makes a paradigm shift to knowledge transfer and presents the first preliminary yet systematic study on cross-cancer prognosis knowledge transfer in WSIs, called CROPKT. It has three major parts: (i) we curate a large dataset (UNI2-h-DSS) with 26 cancers and use it to measure the transferability of WSI-based prognostic knowledge across different cancers (including rare tumors); (ii) beyond a simple evaluation merely for benchmark, we design a range of experiments to gain deeper insights into the underlying mechanism of transferability; (iii) we further show the utility of cross-cancer knowledge transfer, by proposing a routing-based baseline approach (ROUPKT) that could often efficiently utilize the knowledge transferred from off-the-shelf models of other cancers. We hope CROPKT could serve as an inception and lay the foundation for this nascent paradigm, i.e., WSI-based prognosis prediction with cross-cancer knowledge transfer. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/liupei101/CROPKT">https://github.com/liupei101/CROPKT</a>. </p>
<blockquote>
<p>å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æ˜¯è¯„ä¼°ç™Œç—‡é¢„åçš„é‡è¦å·¥å…·ã€‚å½“å‰çš„ç ”ç©¶é€šå¸¸éµå¾ªä¸€ç§ä¼ ç»Ÿçš„ç‰¹å®šç™Œç—‡æ¨¡å¼ï¼Œå³ä¸€ç§ç™Œç—‡å¯¹åº”ä¸€ä¸ªæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨¡å¼åœ¨åº”å¯¹ç½•è§è‚¿ç˜¤æ—¶è‡ªç„¶ä¼šé‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”æ— æ³•åˆ©ç”¨å…¶ä»–ç™Œç—‡çš„çŸ¥è¯†ã€‚å°½ç®¡æœ€è¿‘å·²ç»ç ”ç©¶äº†ç±»ä¼¼å¤šä»»åŠ¡å­¦ä¹ çš„æ¡†æ¶ï¼Œä½†å®ƒé€šå¸¸å¯¹è®¡ç®—èµ„æºæœ‰å¾ˆé«˜çš„è¦æ±‚ï¼Œå¹¶ä¸”éœ€è¦åœ¨è¶…å¤§å‹å¤šç™Œç—‡WSIæ•°æ®é›†ä¸Šè¿›è¡Œè¿­ä»£è®­ç»ƒï¼Œæˆæœ¬ç›¸å½“é«˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è¿›è¡Œäº†èŒƒå¼è½¬å˜ï¼Œè½¬å‘çŸ¥è¯†è½¬ç§»ï¼Œå¹¶å¯¹WSIä¸­çš„è·¨ç™Œç—‡é¢„åçŸ¥è¯†è½¬ç§»è¿›è¡Œäº†é¦–æ¬¡åˆæ­¥ä½†ç³»ç»Ÿçš„ç ”ç©¶ï¼Œç§°ä¸ºCROPKTã€‚å®ƒä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼šï¼ˆiï¼‰æˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«26ç§ç™Œç—‡çš„å¤§å‹æ•°æ®é›†ï¼ˆUNI2-h-DSSï¼‰ï¼Œå¹¶ç”¨æ¥è¡¡é‡ä¸åŒç™Œç—‡ï¼ˆåŒ…æ‹¬ç½•è§è‚¿ç˜¤ï¼‰ä¹‹é—´åŸºäºWSIçš„é¢„åçŸ¥è¯†çš„å¯è½¬ç§»æ€§ï¼›ï¼ˆiiï¼‰ä¸ä»…ä¸ºäº†åŸºå‡†æµ‹è¯•è€Œè¿›è¡Œç®€å•è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç³»åˆ—å®éªŒï¼Œä»¥æ·±å…¥äº†è§£å¯è½¬ç§»æ€§çš„å†…åœ¨æœºåˆ¶ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†è·¨ç™Œç—‡çŸ¥è¯†è½¬ç§»çš„å®ç”¨æ€§ï¼Œé€šè¿‡æå‡ºä¸€ç§åŸºäºè·¯ç”±çš„åŸºçº¿æ–¹æ³•ï¼ˆROUPKTï¼‰ï¼Œè¯¥æ–¹æ³•å¯ä»¥é«˜æ•ˆåœ°åˆ©ç”¨å…¶ä»–ç™Œç—‡çš„ç°æˆæ¨¡å‹æ‰€ä¼ é€’çš„çŸ¥è¯†ã€‚æˆ‘ä»¬å¸Œæœ›CROPKTèƒ½å¤Ÿä½œä¸ºä¸€ä¸ªå¼€ç«¯ï¼Œä¸ºè¿™ä¸ªæ–°å…´èŒƒå¼å¥ å®šåŸºç¡€ï¼Œå³åŸºäºWSIçš„å…·æœ‰è·¨ç™Œç—‡çŸ¥è¯†è½¬ç§»çš„é¢„åé¢„æµ‹ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/liupei101/CROPKT%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/liupei101/CROPKTæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13482v2">PDF</a> 20 pages (11 figures and 6 tables)</p>
<p><strong>Summary</strong><br>    è¯¥ç ”ç©¶çªç ´äº†ä¼ ç»Ÿçš„ç™Œç—‡ç‰¹å¼‚æ€§æ¨¡å¼é™åˆ¶ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶äº†è·¨ç™Œé¢„åçŸ¥è¯†è½¬ç§»åœ¨WSIä¸­çš„åº”ç”¨ï¼Œå‘½åä¸ºCROPKTã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬ï¼šå»ºç«‹åŒ…å«26ç§ç™Œç—‡çš„å¤§å‹æ•°æ®é›†UNI2-h-DSSï¼Œè¡¡é‡WSIé¢„åçŸ¥è¯†åœ¨ä¸åŒç™Œç—‡é—´çš„å¯è½¬ç§»æ€§ï¼›è®¾è®¡ä¸€ç³»åˆ—å®éªŒæ·±å…¥äº†è§£å¯è½¬ç§»æ€§çš„å†…åœ¨æœºåˆ¶ï¼›æå‡ºåŸºäºè·¯ç”±çš„åŸºçº¿æ–¹æ³•ROUPKTï¼Œå±•ç¤ºè·¨ç™ŒçŸ¥è¯†è½¬ç§»çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶çªç ´äº†ä¼ ç»Ÿç™Œç—‡ç‰¹å¼‚æ€§æ¨¡å¼çš„é™åˆ¶ï¼Œå»ºç«‹äº†è·¨ç™Œé¢„åçŸ¥è¯†è½¬ç§»çš„æ–°æ¨¡å¼ï¼ˆCROPKTï¼‰ã€‚</li>
<li>å»ºç«‹äº†åŒ…å«26ç§ç™Œç—‡çš„å¤§å‹æ•°æ®é›†UNI2-h-DSSï¼Œç”¨äºè¡¡é‡WSIé¢„åçŸ¥è¯†åœ¨ä¸åŒç™Œç—‡é—´çš„å¯è½¬ç§»æ€§ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—å®éªŒæ·±å…¥äº†è§£å¯è½¬ç§»æ€§çš„å†…åœ¨æœºåˆ¶ã€‚</li>
<li>æå‡ºäº†åŸºäºè·¯ç”±çš„åŸºçº¿æ–¹æ³•ROUPKTï¼Œèƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨å…¶ä»–ç™Œç—‡çš„å·²è®­ç»ƒæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚</li>
<li>CROPKTä¸ºWSIä¸ºåŸºç¡€çš„é¢„åé¢„æµ‹æä¾›äº†æ–°çš„ç ”ç©¶è§†è§’ï¼Œå³è·¨ç™ŒçŸ¥è¯†è½¬ç§»ã€‚</li>
<li>è¯¥ç ”ç©¶æé«˜äº†å¤„ç†ç½•è§è‚¿ç˜¤çš„èƒ½åŠ›ï¼Œå¹¶èƒ½åˆ©ç”¨å…¶ä»–ç™Œç—‡çš„çŸ¥è¯†ã€‚</li>
<li>ç ”ç©¶çš„æºä»£ç å·²å…¬å¼€ï¼Œä¾¿äºåç»­ç ”ç©¶ä½¿ç”¨ä¸å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0eaafa722989e934ddad61cfa9e010cc" align="middle">
<img src="https://picx.zhimg.com/v2-91a2e3bdf832da061cfecac414b8d5c5" align="middle">
<img src="https://picx.zhimg.com/v2-a397f857b578111c73e5923e759229a7" align="middle">
<img src="https://picx.zhimg.com/v2-14320e2ce44ddf7c0d1309e987997803" align="middle">
<img src="https://picx.zhimg.com/v2-3f930fd0eb6762b268e3c1439b5b2c74" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding"></a>RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding</h2><p><strong>Authors:Tianchen Fang, Guiru Liu</strong></p>
<p>Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image-text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒç†è§£åœ¨å®ç°è‡ªåŠ¨åŒ–è¯Šæ–­å’ŒåŸºäºæ•°æ®çš„ä¸´åºŠå†³ç­–æ”¯æŒä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå…¶è¿›å±•å—åˆ°ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜çš„é™åˆ¶ï¼šé«˜è´¨é‡æ ‡æ³¨åŒ»å­¦æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹å…¨å±€å›¾åƒç‰¹å¾çš„è¿‡åº¦ä¾èµ–ï¼Œè¿™å¾€å¾€å¯¼è‡´å¿½ç•¥ç»†å¾®ä½†ä¸´åºŠä¸Šé‡è¦çš„ç—…ç†åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RegionMed-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŒºåŸŸæ„ŸçŸ¥çš„å¤šæ¨¡å¼å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒæ˜¾å¼åœ°ç»“åˆäº†å±€éƒ¨ç—…ç†ä¿¡å·å’Œæ•´ä½“è¯­ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåˆ›æ–°çš„å…´è¶£åŒºåŸŸï¼ˆROIï¼‰å¤„ç†å™¨ï¼Œå®ƒè‡ªé€‚åº”åœ°é›†æˆç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè¾…ä»¥ä¸€ç§å¢å¼ºåˆ†å±‚å¤šæ¨¡å¼å¯¹é½çš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†å®ç°å¤§è§„æ¨¡çš„åŒºåŸŸçº§åˆ«è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬æ„å»ºäº†MedRegion-500kï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å¹¿æ³›åŒºåŸŸæ³¨é‡Šå’Œå¤šå±‚ä¸´åºŠæè¿°çš„ç»¼åˆåŒ»å­¦å›¾åƒ-æ–‡æœ¬è¯­æ–™åº“ã€‚åœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€é›¶æ ·æœ¬åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRegionMed-CLIPå§‹ç»ˆå¤§å¹…è¶…è¶Šäº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†åŒºåŸŸæ„ŸçŸ¥å¯¹æ¯”é¢„è®­ç»ƒçš„å…³é”®é‡è¦æ€§ï¼Œå¹¶å°†RegionMed-CLIPå®šä½ä¸ºæ¨åŠ¨å¤šæ¨¡å¼åŒ»å­¦å›¾åƒç†è§£å‘å±•çš„ç¨³å¥åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05244v2">PDF</a> Upon further review, we identified that our dataset requires   optimization to ensure research reliability and accuracy. Additionally,   considering the target journalâ€™s latest submission policies, we believe   comprehensive manuscript revisions are necessary</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒç†è§£åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­å’Œæ•°æ®é©±åŠ¨çš„ä¸´åºŠå†³ç­–æ”¯æŒä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†é¢ä¸´é«˜è´¨é‡æ ‡æ³¨åŒ»å­¦æ•°æ®æœ‰é™å’Œè¿‡åº¦ä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RegionMed-CLIPï¼Œä¸€ä¸ªç»“åˆå±€éƒ¨ç—…ç†ä¿¡å·å’Œæ•´ä½“è¯­ä¹‰è¡¨ç¤ºçš„åŒºåŸŸæ„ŸçŸ¥å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯åˆ›æ–°çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰å¤„ç†å™¨ï¼Œèƒ½è‡ªé€‚åº”åœ°é›†æˆç»†ç²’åº¦åŒºåŸŸç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè¾…ä»¥æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œæé«˜åˆ†å±‚å¤šæ¨¡æ€å¯¹é½ã€‚ä¸ºæ”¯æŒå¤§è§„æ¨¡åŒºåŸŸçº§è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬æ„å»ºäº†MedRegion-500kï¼Œä¸€ä¸ªåŒ…å«ä¸°å¯ŒåŒºåŸŸæ ‡æ³¨å’Œå¤šå±‚ä¸´åºŠæè¿°çš„å¤§è§„æ¨¡åŒ»å­¦å›¾åƒæ–‡æœ¬è¯­æ–™åº“ã€‚å®éªŒè¡¨æ˜ï¼ŒRegionMed-CLIPåœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ã€é›¶æ ·æœ¬åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå‡æ˜¾è‘—è¶…è¶Šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒç†è§£åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­ä¸ä¸´åºŠå†³ç­–æ”¯æŒä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ä¸¤å¤§æŒ‘æˆ˜ï¼šé«˜è´¨é‡æ ‡æ³¨åŒ»å­¦æ•°æ®æœ‰é™å’Œè¿‡åº¦ä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ã€‚</li>
<li>RegionMed-CLIPæ¡†æ¶ç»“åˆå±€éƒ¨ç—…ç†ä¿¡å·å’Œæ•´ä½“è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>ROIå¤„ç†å™¨è‡ªé€‚åº”é›†æˆç»†ç²’åº¦åŒºåŸŸç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ¸è¿›å¼è®­ç»ƒç­–ç•¥æé«˜åˆ†å±‚å¤šæ¨¡æ€å¯¹é½ã€‚</li>
<li>MedRegion-500kåŒ»å­¦å›¾åƒæ–‡æœ¬è¯­æ–™åº“æ”¯æŒå¤§è§„æ¨¡åŒºåŸŸçº§è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>RegionMed-CLIPåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°è¶…è¶Šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4409389a475b3efc3467f6322ecaf107" align="middle">
<img src="https://picx.zhimg.com/v2-e0038387d1731557539231913673eb74" align="middle">
<img src="https://picx.zhimg.com/v2-5b048fbad3f03f4ac0c39114b067eb10" align="middle">
<img src="https://picx.zhimg.com/v2-e98bdf7cef64cadde7f2ee8c5e2dc8cc" align="middle">
<img src="https://picx.zhimg.com/v2-a93e3776597773699674e91737962a70" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RIS-LAD-A-Benchmark-and-Model-for-Referring-Low-Altitude-Drone-Image-Segmentation"><a href="#RIS-LAD-A-Benchmark-and-Model-for-Referring-Low-Altitude-Drone-Image-Segmentation" class="headerlink" title="RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image   Segmentation"></a>RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image   Segmentation</h2><p><strong>Authors:Kai Ye, YingShi Luan, Zhudi Chen, Guangyue Meng, Pingyang Dai, Liujuan Cao</strong></p>
<p>Referring Image Segmentation (RIS), which aims to segment specific objects based on natural language descriptions, plays an essential role in vision-language understanding. Despite its progress in remote sensing applications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored. Existing datasets and methods are typically designed for high-altitude and static-view imagery. They struggle to handle the unique characteristics of LAD views, such as diverse viewpoints and high object density. To fill this gap, we present RIS-LAD, the first fine-grained RIS benchmark tailored for LAD scenarios. This dataset comprises 13,871 carefully annotated image-text-mask triplets collected from realistic drone footage, with a focus on small, cluttered, and multi-viewpoint scenes. It highlights new challenges absent in previous benchmarks, such as category drift caused by tiny objects and object drift under crowded same-class objects. To tackle these issues, we propose the Semantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly injecting all linguistic features, SAARN decomposes and routes semantic information to different stages of the network. Specifically, the Category-Dominated Linguistic Enhancement (CDLE) aligns visual features with object categories during early encoding, while the Adaptive Reasoning Fusion Module (ARFM) dynamically selects semantic cues across scales to improve reasoning in complex scenes. The experimental evaluation reveals that RIS-LAD presents substantial challenges to state-of-the-art RIS algorithms, and also demonstrates the effectiveness of our proposed model in addressing these challenges. The dataset and code will be publicly released soon at: <a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/">https://github.com/AHideoKuzeA/RIS-LAD/</a>. </p>
<blockquote>
<p>åŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„ç›®æ ‡å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå…¶æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°æ¥åˆ†å‰²ç‰¹å®šå¯¹è±¡ã€‚å°½ç®¡å…¶åœ¨é¥æ„Ÿåº”ç”¨æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä½ç©ºæ— äººæœºï¼ˆLADï¼‰åœºæ™¯ä¸­çš„RISä»ç„¶é²œæœ‰ç ”ç©¶ã€‚ç°æœ‰çš„æ•°æ®é›†å’Œæ–¹æ³•é€šå¸¸é’ˆå¯¹é«˜ç©ºå’Œé™æ€è§†å›¾å›¾åƒè®¾è®¡ï¼Œå®ƒä»¬éš¾ä»¥å¤„ç†LADè§†å›¾çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œä¾‹å¦‚å¤šæ ·çš„è§†è§’å’Œé«˜å¯†åº¦çš„ç›®æ ‡ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RIS-LADï¼Œè¿™æ˜¯é’ˆå¯¹LADåœºæ™¯å®šåˆ¶çš„é¦–ä¸ªç²¾ç»†çš„RISåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»çœŸå®çš„æ— äººæœºè§†é¢‘ä¸­æ”¶é›†çš„13,871ä¸ªç»è¿‡ä»”ç»†æ³¨é‡Šçš„å›¾åƒ-æ–‡æœ¬-æ©è†œä¸‰å…ƒç»„ï¼Œä¸“æ³¨äºå°ã€æ‚ä¹±å’Œå¤šè§†è§’çš„åœºæ™¯ã€‚å®ƒçªå‡ºäº†ä»¥å‰åŸºå‡†æµ‹è¯•ä¸­ä¸å­˜åœ¨çš„æ–°çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç”±äºå¾®å°ç‰©ä½“å¼•èµ·çš„ç±»åˆ«æ¼‚ç§»å’Œåœ¨æ‹¥æŒ¤çš„åŒç±»ç‰©ä½“ä¸‹ç‰©ä½“çš„æ¼‚ç§»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†ç½‘ç»œï¼ˆSAARNï¼‰ã€‚SAARNä¸æ˜¯ç»Ÿä¸€åœ°æ³¨å…¥æ‰€æœ‰è¯­è¨€ç‰¹å¾ï¼Œè€Œæ˜¯åˆ†è§£å’Œå°†è¯­ä¹‰ä¿¡æ¯è·¯ç”±åˆ°ç½‘ç»œçš„ä¸åŒé˜¶æ®µã€‚å…·ä½“æ¥è¯´ï¼Œç±»åˆ«ä¸»å¯¼çš„è¯­è¨€å¢å¼ºï¼ˆCDLEï¼‰åœ¨æ—©æœŸç¼–ç æ—¶å°†è§†è§‰ç‰¹å¾ä¸å¯¹è±¡ç±»åˆ«å¯¹é½ï¼Œè€Œè‡ªé€‚åº”æ¨ç†èåˆæ¨¡å—ï¼ˆARFMï¼‰åˆ™åŠ¨æ€é€‰æ‹©è·¨å°ºåº¦çš„è¯­ä¹‰çº¿ç´¢ï¼Œä»¥æ”¹å–„å¤æ‚åœºæ™¯ä¸­çš„æ¨ç†ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒRIS-LADç»™ç°æœ‰çš„RISç®—æ³•å¸¦æ¥äº†å®è´¨æ€§çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¹Ÿè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†å¾ˆå¿«åœ¨<a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/AHideoKuzeA/RIS-LAD/å…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20920v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä½ç©ºæ— äººæœºï¼ˆLADï¼‰åœºæ™¯çš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ç ”ç©¶å­˜åœ¨ç©ºç™½ã€‚ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦é¢å‘é«˜ç©ºå’Œé™æ€å›¾åƒï¼Œéš¾ä»¥å¤„ç†LADè§†è§’çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¦‚ä¸åŒè§†è§’å’Œé«˜ç‰©ä½“å¯†åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºRIS-LADï¼Œé¦–ä¸ªé’ˆå¯¹LADåœºæ™¯çš„ç²¾ç»†RISåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»çœŸå®æ— äººæœºå½±åƒä¸­æ”¶é›†çš„13,871ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å›¾åƒ-æ–‡æœ¬-æ©è†œä¸‰å…ƒç»„ï¼Œé‡ç‚¹å…³æ³¨å°ã€æ‚ä¹±ã€å¤šè§†è§’åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºè¯­ä¹‰æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†ç½‘ç»œï¼ˆSAARNï¼‰æ¥è§£å†³æ–°é—®é¢˜ï¼Œå¦‚å› å°ç‰©ä½“å¼•èµ·çš„ç±»åˆ«æ¼‚ç§»å’Œåœ¨æ‹¥æŒ¤çš„åŒç±»ç‰©ä½“ä¸­çš„å¯¹è±¡æ¼‚ç§»ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒRIS-LADä¸ºå½“å‰RISç®—æ³•å¸¦æ¥é‡å¤§æŒ‘æˆ˜ï¼ŒåŒæ—¶éªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨ <a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/">https://github.com/AHideoKuzeA/RIS-LAD/</a> å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç©ºæ— äººæœºï¼ˆLADï¼‰åœºæ™¯çš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ç ”ç©¶å°šå¾…æ·±å…¥æ¢ç´¢ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦é¢å‘é«˜ç©ºå’Œé™æ€å›¾åƒï¼Œéš¾ä»¥é€‚åº”LADè§†è§’çš„å¤šæ ·æ€§ã€‚</li>
<li>æ¨å‡ºRIS-LADæ•°æ®é›†ï¼Œä¸“ä¸ºLADåœºæ™¯è®¾è®¡ï¼ŒåŒ…å«ä»çœŸå®æ— äººæœºå½±åƒä¸­ç²¾å¿ƒæ ‡æ³¨çš„å›¾åƒ-æ–‡æœ¬-æ©è†œä¸‰å…ƒç»„ã€‚</li>
<li>æ•°æ®é›†é‡ç‚¹å…³æ³¨å°ã€æ‚ä¹±ã€å¤šè§†è§’åœºæ™¯çš„åˆ†å‰²é—®é¢˜ã€‚</li>
<li>æå‡ºè¯­ä¹‰æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†ç½‘ç»œï¼ˆSAARNï¼‰æ¥è§£å†³ç±»åˆ«æ¼‚ç§»å’Œå¯¹è±¡æ¼‚ç§»ç­‰é—®é¢˜ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒRIS-LADå¯¹ç°æœ‰RISç®—æ³•æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e17ea3009e8ef64736ad4532d04cac83" align="middle">
<img src="https://picx.zhimg.com/v2-e1dcbd3aab3d07317e29a2eaa2ffe30b" align="middle">
<img src="https://picx.zhimg.com/v2-83adebd4854a35e929fc72b3c0b26601" align="middle">
<img src="https://picx.zhimg.com/v2-0a0ca05b1720be445a7cbef0daeba518" align="middle">
<img src="https://picx.zhimg.com/v2-851c9efd0de3717aba74af02c499b959" align="middle">
<img src="https://picx.zhimg.com/v2-ee9a05bee21711f329b1996ce4be8152" align="middle">
<img src="https://picx.zhimg.com/v2-27fb6ccd2928c28bd999a6ed44658101" align="middle">
<img src="https://picx.zhimg.com/v2-a1228b499ece1d667546645fc83d824b" align="middle">
<img src="https://picx.zhimg.com/v2-2290cdd0001759f7a29e91c7539076af" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DCFFSNet-Deep-Connectivity-Feature-Fusion-Separation-Network-for-Medical-Image-Segmentation"><a href="#DCFFSNet-Deep-Connectivity-Feature-Fusion-Separation-Network-for-Medical-Image-Segmentation" class="headerlink" title="DCFFSNet: Deep Connectivity Feature Fusion Separation Network for   Medical Image Segmentation"></a>DCFFSNet: Deep Connectivity Feature Fusion Separation Network for   Medical Image Segmentation</h2><p><strong>Authors:Mingda Zhang, Xun Ye, Ruixiang Tang, Haiyan Ding</strong></p>
<p>Medical image segmentation leverages topological connectivity theory to enhance edge precision and regional consistency. However, existing deep networks integrating connectivity often forcibly inject it as an additional feature module, resulting in coupled feature spaces with no standardized mechanism to quantify different feature strengths. To address these issues, we propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It introduces an innovative feature space decoupling strategy. This strategy quantifies the relative strength between connectivity features and other features. It then builds a deep connectivity feature fusion-separation architecture. This architecture dynamically balances multi-scale feature expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by 1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice) and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU). The results demonstrate that DCFFSNet exceeds existing mainstream methods across all metrics. It effectively resolves segmentation fragmentation and achieves smooth edge transitions. This significantly enhances clinical usability. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åˆ©ç”¨æ‹“æ‰‘è¿æ¥ç†è®ºæ¥æé«˜è¾¹ç¼˜ç²¾åº¦å’ŒåŒºåŸŸä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ·±åº¦ç½‘ç»œåœ¨é›†æˆè¿æ¥æ€§æ—¶é€šå¸¸å¼ºåˆ¶å°†å…¶æ³¨å…¥ä½œä¸ºé™„åŠ ç‰¹å¾æ¨¡å—ï¼Œå¯¼è‡´ç‰¹å¾ç©ºé—´è€¦åˆï¼Œæ²¡æœ‰æ ‡å‡†åŒ–çš„æœºåˆ¶æ¥é‡åŒ–ä¸åŒç‰¹å¾çš„å¼ºåº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DCFFSNetï¼ˆåŒè¿æ¥ç‰¹å¾èåˆåˆ†ç¦»ç½‘ç»œï¼‰ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ç‰¹å¾ç©ºé—´è§£è€¦ç­–ç•¥ã€‚è¯¥ç­–ç•¥é‡åŒ–è¿æ¥ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾ä¹‹é—´çš„ç›¸å¯¹å¼ºåº¦ã€‚ç„¶åï¼Œå®ƒå»ºç«‹äº†ä¸€ä¸ªæ·±åº¦è¿æ¥ç‰¹å¾èåˆ-åˆ†ç¦»æ¶æ„ã€‚è¯¥æ¶æ„åŠ¨æ€å¹³è¡¡äº†å¤šå°ºåº¦ç‰¹å¾è¡¨è¾¾ã€‚æˆ‘ä»¬åœ¨ISIC2018ã€DSB2018å’ŒMoNuSegæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚åœ¨ISIC2018ä¸Šï¼ŒDCFFSNetåœ¨ç‹„å…‹ç³»æ•°å’Œäº¤å¹¶æ¯”æ–¹é¢åˆ†åˆ«æ¯”æ’åç¬¬äºŒçš„æœ€ä½³æ¨¡å‹CMUNeté«˜å‡º1.3%å’Œ1.2%ã€‚åœ¨DSB2018ä¸Šï¼Œå®ƒæ¯”TransUNeté«˜å‡º0.7%ï¼ˆç‹„å…‹ç³»æ•°ï¼‰å’Œ0.9%ï¼ˆäº¤å¹¶æ¯”ï¼‰ã€‚åœ¨MoNuSegä¸Šï¼Œå®ƒæ¯”CSCAUNeté«˜å‡º0.8%ï¼ˆç‹„å…‹ç³»æ•°ï¼‰å’Œ0.9%ï¼ˆäº¤å¹¶æ¯”ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼ŒDCFFSNetåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†ç°æœ‰çš„ä¸»æµæ–¹æ³•ã€‚å®ƒæœ‰æ•ˆåœ°è§£å†³äº†åˆ†å‰²ç¢ç‰‡é—®é¢˜ï¼Œå®ç°äº†å¹³æ»‘çš„è¾¹ç¼˜è¿‡æ¸¡ã€‚è¿™æ˜¾è‘—æé«˜äº†ä¸´åºŠå¯ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18407v2">PDF</a> 16 pages , 11 figures</p>
<p><strong>Summary</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²åˆ©ç”¨æ‹“æ‰‘è¿é€šæ€§ç†è®ºæé«˜è¾¹ç¼˜ç²¾åº¦å’ŒåŒºåŸŸä¸€è‡´æ€§ã€‚é’ˆå¯¹ç°æœ‰æ·±åº¦ç½‘ç»œé›†æˆè¿é€šæ€§æ—¶å¼ºè¡Œæ³¨å…¥ä½œä¸ºé™„åŠ ç‰¹å¾æ¨¡å—çš„é—®é¢˜ï¼Œæå‡ºDCFFSNetï¼ˆåŒè¿é€šç‰¹å¾èåˆåˆ†ç¦»ç½‘ç»œï¼‰ã€‚å¼•å…¥ç‰¹å¾ç©ºé—´è§£è€¦ç­–ç•¥ï¼Œé‡åŒ–è¿é€šæ€§ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾çš„ç›¸å¯¹å¼ºåº¦ï¼Œå»ºç«‹æ·±åº¦è¿é€šç‰¹å¾èåˆåˆ†ç¦»æ¶æ„ï¼ŒåŠ¨æ€å¹³è¡¡å¤šå°ºåº¦ç‰¹å¾è¡¨è¾¾ã€‚åœ¨ISIC2018ã€DSB2018å’ŒMoNuSegæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDCFFSNetåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¶…è¿‡ä¸»æµæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³åˆ†å‰²ç¢ç‰‡åŒ–é—®é¢˜ï¼Œå®ç°å¹³æ»‘è¾¹ç¼˜è¿‡æ¸¡ï¼Œæ˜¾è‘—æé«˜ä¸´åºŠå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åˆ©ç”¨æ‹“æ‰‘è¿é€šæ€§ç†è®ºæé«˜è¾¹ç¼˜å’ŒåŒºåŸŸä¸€è‡´æ€§ã€‚</li>
<li>ç°æœ‰ç½‘ç»œé›†æˆè¿é€šæ€§å­˜åœ¨ç‰¹å¾ç©ºé—´è€¦åˆé—®é¢˜ï¼Œç¼ºä¹é‡åŒ–ä¸åŒç‰¹å¾å¼ºåº¦çš„æ ‡å‡†åŒ–æœºåˆ¶ã€‚</li>
<li>DCFFSNetæå‡ºç‰¹å¾ç©ºé—´è§£è€¦ç­–ç•¥ï¼Œå»ºç«‹æ·±åº¦è¿é€šç‰¹å¾èåˆåˆ†ç¦»æ¶æ„ã€‚</li>
<li>DCFFSNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡ä¸»æµæ–¹æ³•ã€‚</li>
<li>DCFFSNetæœ‰æ•ˆè§£å†³åˆ†å‰²ç¢ç‰‡åŒ–é—®é¢˜ã€‚</li>
<li>DCFFSNetå®ç°å¹³æ»‘è¾¹ç¼˜è¿‡æ¸¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bef15d846edf08a37fa1b33c8ccb89fb" align="middle">
<img src="https://picx.zhimg.com/v2-10c5c0a9b7d158d285f323debc8b7bc4" align="middle">
<img src="https://picx.zhimg.com/v2-3f434c2f03c4424da3a5a5faa536854d" align="middle">
<img src="https://picx.zhimg.com/v2-a16034ef4befa22ec49f66c59e5c509d" align="middle">
<img src="https://picx.zhimg.com/v2-88bd4cfef6e3dd3ab100fa1e3b6c09bc" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation"><a href="#GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation" class="headerlink" title="GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation"></a>GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation</h2><p><strong>Authors:Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe</strong></p>
<p>Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at <a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix">https://github.com/hugocarlesso/GeMix</a> to foster reproducibility and further research. </p>
<blockquote>
<p>Mixupå·²æˆä¸ºå›¾åƒåˆ†ç±»ä¸­æµè¡Œçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä½†å…¶ç®€å•çš„åƒç´ çº§æ’å€¼ç»å¸¸ç”Ÿæˆä¸çœŸå®çš„å›¾åƒï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„åŒ»å­¦åº”ç”¨ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†GeMixï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œå®ƒç”¨åŸºäºç±»åˆ«æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å­¦ä¹ æ„ŸçŸ¥æ’å€¼æ›¿æ¢äº†å¯å‘å¼æ··åˆæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè®­ç»ƒStyleGAN2-ADAç”Ÿæˆå™¨ã€‚åœ¨æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»åå‘ä¸åŒç±»åˆ«çš„Dirichletå…ˆéªŒä¸­é‡‡æ ·ä¸¤ä¸ªæ ‡ç­¾å‘é‡ï¼Œå¹¶é€šè¿‡Betaåˆ†å¸ƒç³»æ•°å°†å®ƒä»¬æ··åˆã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç”Ÿæˆå™¨è®¾ç½®ä¸ºè¯¥è½¯æ ‡ç­¾çš„æ¡ä»¶ï¼Œåˆæˆæ²¿è¿ç»­ç±»åˆ«æµå½¢çš„ä¸€è‡´å›¾åƒã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡çš„COVIDx-CT-3æ•°æ®é›†ä¸Šä½¿ç”¨ä¸‰ç§ä¸»å¹²ç½‘ç»œï¼ˆResNet-50ã€ResNet-101ã€EfficientNet-B0ï¼‰å¯¹GeMixè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å½“ä¸çœŸå®æ•°æ®ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰ä¸»å¹²ç½‘ç»œä¸Šæé«˜äº†å®è§‚F1åˆ†æ•°ï¼Œé™ä½äº†COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚å› æ­¤ï¼ŒGeMixå¯ä½œä¸ºåƒç´ ç©ºé—´ä¸­Mixupçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæä¾›æ›´å¼ºçš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ï¼Œè€Œä¸ä¼šç ´åç°æœ‰çš„è®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/hugocarlesso/GeMixå…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15577v2">PDF</a> Accepted at CBMI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<pre><code>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGeMixçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå›¾åƒåˆ†ç±»ä¸­çš„å¢å¼ºç­–ç•¥ã€‚ä¼ ç»Ÿçš„æ··åˆç­–ç•¥ä½¿ç”¨åƒç´ çº§çš„æ’å€¼äº§ç”Ÿä¸çœŸå®çš„å›¾åƒï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„åŒ»å­¦åº”ç”¨ä¸­å¯èƒ½é˜»ç¢å­¦ä¹ ã€‚GeMixä½¿ç”¨åŸºäºç±»æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ ‡ç­¾æ„ŸçŸ¥æ’å€¼æ›¿ä»£å¯å‘å¼æ··åˆæ–¹æ³•ã€‚é€šè¿‡ç›®æ ‡æ•°æ®é›†è®­ç»ƒStyleGAN2-ADAç”Ÿæˆå™¨ï¼Œå¹¶ä½¿ç”¨åå‘ä¸åŒç±»çš„Dirichletå…ˆéªŒé‡‡æ ·ä¸¤ä¸ªæ ‡ç­¾å‘é‡ï¼Œé€šè¿‡Betaåˆ†å¸ƒç³»æ•°è¿›è¡Œæ··åˆã€‚ç„¶åå°†æ­¤è½¯æ ‡ç­¾åº”ç”¨äºç”Ÿæˆå™¨ï¼Œåˆæˆæ²¿ç€è¿ç»­ç±»æµå½¢åˆ†å¸ƒè§†è§‰ä¸Šè¿è´¯çš„å›¾åƒã€‚åœ¨å¤§å‹COVIDx-CT-3æ•°æ®é›†ä¸Šè¿›è¡Œçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œä¸çœŸå®æ•°æ®ç»“åˆä½¿ç”¨æ—¶ï¼ŒGeMixç›¸å¯¹äºä¼ ç»Ÿæ··åˆæ–¹æ³•æé«˜äº†æ‰€æœ‰èƒŒéª¨ç½‘ç»œçš„å®è§‚F1åˆ†æ•°ï¼Œå¹¶é™ä½äº†COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚å› æ­¤ï¼ŒGeMixå¯ä»¥ä½œä¸ºåƒç´ ç©ºé—´æ··åˆçš„æ›¿ä»£å“ï¼Œæä¾›æ›´å¼ºå¤§çš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ï¼Œè€Œä¸ä¼šç ´åç°æœ‰çš„è®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†ä»£ç ä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

**å…³é”®è§è§£**

1. ä¼ ç»Ÿæ··åˆç­–ç•¥å¸¸å¸¸äº§ç”Ÿä¸çœŸå®çš„å›¾åƒï¼Œå°¤å…¶æ˜¯åœ¨åŒ»å­¦åº”ç”¨ä¸­å¯èƒ½é˜»ç¢å­¦ä¹ ã€‚
2. GeMixæ˜¯ä¸€ä¸ªåŸºäºç±»æ¡ä»¶GANsçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾æ„ŸçŸ¥æ’å€¼æ›¿æ¢å¯å‘å¼æ··åˆæ–¹æ³•ã€‚
3. GeMixä½¿ç”¨StyleGAN2-ADAç”Ÿæˆå™¨åˆæˆè§†è§‰ä¸Šè¿è´¯çš„å›¾åƒï¼Œè¿™äº›å›¾åƒæ²¿ç€è¿ç»­ç±»æµå½¢åˆ†å¸ƒã€‚
4. åœ¨å¤§å‹COVIDx-CT-3æ•°æ®é›†ä¸Šè¿›è¡Œçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒGeMixæé«˜äº†å®è§‚F1åˆ†æ•°å¹¶é™ä½äº†COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚
5. GeMixä½œä¸ºä¸€ç§æ›¿ä»£åƒç´ ç©ºé—´æ··åˆçš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¼ºçš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚
6. GeMixä¸ä¼šç ´åç°æœ‰çš„è®­ç»ƒç®¡é“ï¼Œå¯ä»¥å…¬å¼€å‘å¸ƒçš„æºä»£ç ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-030bf440ac55c96263e57bc08421900a" align="middle">
<img src="https://picx.zhimg.com/v2-5d29c3256c94661b424630d8f2a1417a" align="middle">
<img src="https://picx.zhimg.com/v2-cdc7ffdf5f932180a83612b9c0ec803a" align="middle">
<img src="https://picx.zhimg.com/v2-92a2067546ec4c4a0b1c309aa86006d2" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Interpretability-Aware-Pruning-for-Efficient-Medical-Image-Analysis"><a href="#Interpretability-Aware-Pruning-for-Efficient-Medical-Image-Analysis" class="headerlink" title="Interpretability-Aware Pruning for Efficient Medical Image Analysis"></a>Interpretability-Aware Pruning for Efficient Medical Image Analysis</h2><p><strong>Authors:Nikita Malik, Pratinav Seth, Neeraj Kumar Singh, Chintan Chitroda, Vinay Kumar Sankarapu</strong></p>
<p>Deep learning has driven significant advances in medical image analysis, yet its adoption in clinical practice remains constrained by the large size and lack of transparency in modern models. Advances in interpretability techniques such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated Gradients make it possible to assess the contribution of individual components within neural networks trained on medical imaging tasks. In this work, we introduce an interpretability-guided pruning framework that reduces model complexity while preserving both predictive performance and transparency. By selectively retaining only the most relevant parts of each layer, our method enables targeted compression that maintains clinically meaningful representations. Experiments across multiple medical image classification benchmarks demonstrate that this approach achieves high compression rates with minimal loss in accuracy, paving the way for lightweight, interpretable models suited for real-world deployment in healthcare settings. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä¸´åºŠå®è·µä¸­åº”ç”¨ä»å—åˆ°ç°ä»£æ¨¡å‹ä½“ç§¯åºå¤§å’Œä¸é€æ˜çš„é™åˆ¶ã€‚è¯¸å¦‚DL-Backtraceã€é€å±‚ç›¸å…³æ€§ä¼ æ’­å’Œé›†æˆæ¢¯åº¦ç­‰è§£é‡ŠæŠ€æœ¯çš„è¿›å±•ï¼Œä½¿å¾—è¯„ä¼°ç»è¿‡åŒ»å­¦æˆåƒä»»åŠ¡è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸­å„ä¸ªç»„ä»¶çš„è´¡çŒ®æˆä¸ºå¯èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä»¥è§£é‡Šæ€§ä¸ºæŒ‡å¯¼çš„ä¿®å‰ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¿æŒé¢„æµ‹æ€§èƒ½å’Œé€æ˜åº¦çš„åŒæ—¶ï¼Œé™ä½æ¨¡å‹çš„å¤æ‚æ€§ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°ä¿ç•™æ¯å±‚æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æœ‰é’ˆå¯¹æ€§çš„å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒå¯¹ä¸´åºŠæœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„å‹ç¼©ç‡ï¼Œå¹¶ä¸”ç²¾åº¦æŸå¤±è¾ƒå°ï¼Œè¿™ä¸ºåœ¨åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²è½»ä¾¿ã€å¯è§£é‡Šæ€§å¼ºçš„æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08330v2">PDF</a> Accepted at The 1st MICCAI Workshop on Efficient Medical AI 2025</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ¨¡å‹ä½“ç§¯å¤§ã€é€æ˜åº¦ä¸è¶³ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚é‡‡ç”¨DL-Backtraceã€é€å±‚ç›¸å…³æ€§ä¼ æ’­å’Œé›†æˆæ¢¯åº¦ç­‰è§£é‡Šæ€§æŠ€æœ¯ï¼Œå¯ä»¥è¯„ä¼°ç¥ç»ç½‘ç»œä¸­å¯¹åŒ»å­¦æˆåƒä»»åŠ¡è®­ç»ƒå„ç»„æˆéƒ¨åˆ†çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ä»¥è§£é‡Šæ€§ä¸ºæŒ‡å¯¼çš„å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™é¢„æµ‹æ€§èƒ½å’Œé€æ˜åº¦ã€‚é€šè¿‡é€‰æ‹©æ€§ä¿ç•™æ¯å±‚æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æœ‰é’ˆå¯¹æ€§çš„å‹ç¼©ï¼Œä¿æŒä¸´åºŠæ„ä¹‰çš„è¡¨ç¤ºã€‚åœ¨å¤šåŒ»å­¦å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜å‹ç¼©ç‡ä¸”å‡†ç¡®æ€§æŸå¤±æå°ï¼Œä¸ºé€‚åˆåœ¨åŒ»ç–—ç¯å¢ƒä¸­å®é™…éƒ¨ç½²çš„è½»é‡çº§ã€å¯è§£é‡Šæ€§æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å®é™…åº”ç”¨ä¸­é¢ä¸´æ¨¡å‹ä½“ç§¯å¤§ã€é€æ˜åº¦ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨è§£é‡Šæ€§æŠ€æœ¯ï¼ˆå¦‚DL-Backtraceã€é€å±‚ç›¸å…³æ€§ä¼ æ’­å’Œé›†æˆæ¢¯åº¦ï¼‰ä»¥è¯„ä¼°ç¥ç»ç½‘ç»œä¸­å„ç»„æˆéƒ¨åˆ†çš„ä½œç”¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä»¥è§£é‡Šæ€§ä¸ºæŒ‡å¯¼çš„å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹å¤æ‚åº¦åŒæ—¶ä¿ç•™é¢„æµ‹æ€§èƒ½å’Œé€æ˜åº¦ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æ€§ä¿ç•™ç¥ç»ç½‘ç»œå±‚ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œå®ç°æœ‰é’ˆå¯¹æ€§çš„æ¨¡å‹å‹ç¼©ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿä¿æŒä¸´åºŠæ„ä¹‰çš„è¡¨ç¤ºï¼Œä½¿åŒ»å­¦å›¾åƒåˆ†ææ›´åŠ é€æ˜å’Œå¯è§£é‡Šã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜å‹ç¼©ç‡å’Œè¾ƒå°çš„å‡†ç¡®æ€§æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b32d2387c6600cb81d2f099f01685e8" align="middle">
<img src="https://picx.zhimg.com/v2-6771a99ec8ed1864c514585c714f9db1" align="middle">
<img src="https://picx.zhimg.com/v2-5184e0306ed5daccb3477c130a268128" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Information-Pursuit-for-Interpretable-and-Reliable-Medical-Image-Analysis"><a href="#Uncertainty-Aware-Information-Pursuit-for-Interpretable-and-Reliable-Medical-Image-Analysis" class="headerlink" title="Uncertainty-Aware Information Pursuit for Interpretable and Reliable   Medical Image Analysis"></a>Uncertainty-Aware Information Pursuit for Interpretable and Reliable   Medical Image Analysis</h2><p><strong>Authors:Md Nahiduzzaman, Steven Korevaar, Zongyuan Ge, Feng Xia, Alireza Bab-Hadiashar, Ruwan Tennakoon</strong></p>
<p>To be adopted in safety-critical domains like medical image analysis, AI systems must provide human-interpretable decisions. Variational Information Pursuit (V-IP) offers an interpretable-by-design framework by sequentially querying input images for human-understandable concepts, using their presence or absence to make predictions. However, existing V-IP methods overlook sample-specific uncertainty in concept predictions, which can arise from ambiguous features or model limitations, leading to suboptimal query selection and reduced robustness. In this paper, we propose an interpretable and uncertainty-aware framework for medical imaging that addresses these limitations by accounting for upstream uncertainties in concept-based, interpretable-by-design models. Specifically, we introduce two uncertainty-aware models, EUAV-IP and IUAV-IP, that integrate uncertainty estimates into the V-IP querying process to prioritize more reliable concepts per sample. EUAV-IP skips uncertain concepts via masking, while IUAV-IP incorporates uncertainty into query selection implicitly for more informed and clinically aligned decisions. Our approach allows models to make reliable decisions based on a subset of concepts tailored to each individual sample, without human intervention, while maintaining overall interpretability. We evaluate our methods on five medical imaging datasets across four modalities: dermoscopy, X-ray, ultrasound, and blood cell imaging. The proposed IUAV-IP model achieves state-of-the-art accuracy among interpretable-by-design approaches on four of the five datasets, and generates more concise explanations by selecting fewer yet more informative concepts. These advances enable more reliable and clinically meaningful outcomes, enhancing model trustworthiness and supporting safer AI deployment in healthcare. </p>
<blockquote>
<p>åœ¨åŒ»ç–—å›¾åƒåˆ†æç­‰å®‰å…¨å…³é”®é¢†åŸŸï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿå¿…é¡»æä¾›äººç±»å¯è§£é‡Šçš„å†³ç­–ã€‚å˜åˆ†ä¿¡æ¯è¿½æ±‚ï¼ˆV-IPï¼‰æä¾›äº†ä¸€ä¸ªé€šè¿‡è®¾è®¡å³å¯è§£é‡Šæ¡†æ¶ï¼Œé€šè¿‡é¡ºåºæŸ¥è¯¢è¾“å…¥å›¾åƒä»¥è·å–äººç±»å¯ä»¥ç†è§£çš„æ¦‚å¿µï¼Œå¹¶åˆ©ç”¨å®ƒä»¬çš„å­˜åœ¨ä¸å¦æ¥è¿›è¡Œé¢„æµ‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„V-IPæ–¹æ³•å¿½ç•¥äº†æ¦‚å¿µé¢„æµ‹ä¸­çš„æ ·æœ¬ç‰¹å®šä¸ç¡®å®šæ€§ï¼Œè¿™ç§ä¸ç¡®å®šæ€§å¯èƒ½æ¥æºäºç‰¹å¾æ¨¡ç³Šæˆ–æ¨¡å‹å±€é™æ€§ï¼Œä»è€Œå¯¼è‡´æŸ¥è¯¢é€‰æ‹©ä¸ä½³å’Œç¨³å¥æ€§é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºåŒ»å­¦å½±åƒçš„å¯è§£é‡Šå’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è€ƒè™‘åŸºäºæ¦‚å¿µçš„ã€é€šè¿‡è®¾è®¡å³å¯è§£é‡Šçš„æ¨¡å‹ä¸­çš„ä¸Šæ¸¸ä¸ç¡®å®šæ€§æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å‹ï¼Œå³EUAV-IPå’ŒIUAV-IPï¼Œå®ƒä»¬å°†ä¸ç¡®å®šæ€§ä¼°è®¡æ•´åˆåˆ°V-IPæŸ¥è¯¢è¿‡ç¨‹ä¸­ï¼Œä»¥ä¼˜å…ˆè€ƒè™‘æ¯ä¸ªæ ·æœ¬çš„æ›´å¯é æ¦‚å¿µã€‚EUAV-IPé€šè¿‡å±è”½è·³è¿‡ä¸ç¡®å®šæ¦‚å¿µï¼Œè€ŒIUAV-IPå°†ä¸ç¡®å®šæ€§éšå¼åœ°çº³å…¥æŸ¥è¯¢é€‰æ‹©ä¸­ï¼Œä»¥åšå‡ºæ›´æ˜æ™ºä¸”ä¸ä¸´åºŠç›¸ç¬¦çš„å†³ç­–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸æ¨¡å‹åŸºäºé’ˆå¯¹æ¯ä¸ªä¸ªä½“æ ·æœ¬é‡èº«å®šåˆ¶çš„æ¦‚å¿µå­é›†åšå‡ºå¯é å†³ç­–ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨äº”ç§åŒ»å­¦å½±åƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬å››ç§æ¨¡å¼ï¼šçš®è‚¤é•œæ£€æŸ¥ã€Xå°„çº¿ã€è¶…å£°æ³¢å’Œè¡€æ¶²ç»†èƒæˆåƒï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„IUAV-IPæ¨¡å‹åœ¨å››ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é€šè¿‡è®¾è®¡å³å¯è§£é‡Šçš„å…ˆè¿›å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ›´å°‘ä½†æ›´æœ‰ä¿¡æ¯é‡çš„æ¦‚å¿µæ¥æä¾›æ›´ç®€æ´çš„è§£é‡Šã€‚è¿™äº›è¿›å±•ä½¿ç»“æœæ›´åŠ å¯é å’Œå…·æœ‰ä¸´åºŠæ„ä¹‰ï¼Œæé«˜äº†æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œå¹¶æ”¯æŒæ›´å®‰å…¨çš„äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥ä¸­çš„éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16742v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨åŒ»å­¦å›¾åƒåˆ†æç­‰é‡è¦é¢†åŸŸåº”ç”¨AIç³»ç»Ÿæ—¶ï¼Œéœ€è¦æ³¨é‡æä¾›äººç±»å¯è§£é‡Šçš„å†³ç­–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ¦‚å¿µé¢„æµ‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰å˜å¼‚ä¿¡æ¯è¿½æ±‚ï¼ˆV-IPï¼‰æ–¹æ³•åœ¨æ¦‚å¿µæŸ¥è¯¢ä¸­çš„ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å‹EUAV-IPå’ŒIUAV-IPï¼Œå°†ä¸ç¡®å®šæ€§ä¼°è®¡èå…¥V-IPæŸ¥è¯¢è¿‡ç¨‹ï¼Œä¼˜å…ˆå¤„ç†æ¯ä¸ªæ ·æœ¬æ›´å¯é çš„æ¦‚å¿µã€‚å…¶ä¸­ï¼ŒEUAV-IPé€šè¿‡å±è”½è·³è¿‡ä¸ç¡®å®šæ¦‚å¿µï¼Œè€ŒIUAV-IPåˆ™å°†ä¸ç¡®å®šæ€§çº³å…¥æŸ¥è¯¢é€‰æ‹©ä¸­ï¼Œä»¥åšå‡ºæ›´å…·é’ˆå¯¹æ€§å’Œä¸´åºŠå¯¹é½çš„å†³ç­–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äº”ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å››ä¸ªè¡¨ç°å‡ºä¼˜å¼‚å‡†ç¡®æ€§ï¼Œç”Ÿæˆçš„è§£é‡Šæ›´ç®€æ´ä¸”å…·æœ‰æ›´å°‘è€Œæ›´å…·ä¿¡æ¯æ€§çš„æ¦‚å¿µã€‚æ­¤æŠ€æœ¯èƒ½æ›´å¯é å’Œæœ‰æ„ä¹‰åœ°æ”¯æŒåŒ»ç–—å¥åº·é¢†åŸŸçš„äººå·¥æ™ºèƒ½éƒ¨ç½²å¹¶æå‡æ¨¡å‹çš„ä¿¡èµ–åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIç³»ç»Ÿåœ¨åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸéœ€è¦æä¾›äººç±»å¯è§£é‡Šçš„å†³ç­–ã€‚</li>
<li>å˜ç§ä¿¡æ¯è¿½æ±‚ï¼ˆV-IPï¼‰æ¡†æ¶é€šè¿‡æŸ¥è¯¢è¾“å…¥å›¾åƒä¸­çš„å¯ç†è§£æ¦‚å¿µè¿›è¡Œé¢„æµ‹ï¼Œä½†å­˜åœ¨æ ·æœ¬ç‰¹å®šä¸ç¡®å®šæ€§çš„å¿½ç•¥é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› ä¸ºå¿½ç•¥äº†æ¦‚å¿µé¢„æµ‹çš„ä¸ç¡®å®šæ€§å¯èƒ½å¯¼è‡´æŸ¥è¯¢é€‰æ‹©ä¸ç†æƒ³å’Œé²æ£’æ€§é™ä½ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè€ƒè™‘äº†æ¦‚å¿µæ€§è®¾è®¡ä¸­ä¸Šæ¸¸çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>EUAV-IPæ¨¡å‹é€šè¿‡å±è”½ä¸ç¡®å®šæ¦‚å¿µæé«˜å†³ç­–å¯é æ€§ï¼Œè€ŒIUAV-IPæ¨¡å‹å°†ä¸ç¡®å®šæ€§çº³å…¥æŸ¥è¯¢é€‰æ‹©ä¸­ï¼Œä»¥ç”Ÿæˆæ›´ç²¾ç¡®ä¸”ä¸´åºŠå¯¹é½çš„è§£é‡Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4321030797e6171dd94113133bf796d6" align="middle">
<img src="https://picx.zhimg.com/v2-c46651c934f7793bb40a5d73d5d4fbcc" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Quad-Step-Approach-to-Uncertainty-Aware-Deep-Learning-for-Skin-Cancer-Classification"><a href="#A-Quad-Step-Approach-to-Uncertainty-Aware-Deep-Learning-for-Skin-Cancer-Classification" class="headerlink" title="A Quad-Step Approach to Uncertainty-Aware Deep Learning for Skin Cancer   Classification"></a>A Quad-Step Approach to Uncertainty-Aware Deep Learning for Skin Cancer   Classification</h2><p><strong>Authors:Hamzeh Asgharnezhad, Pegah Tabarisaadi, Abbas Khosravi, Roohallah Alizadehsani, U. Rajendra Acharya</strong></p>
<p>Accurate skin cancer diagnosis is vital for early treatment and improved patient outcomes. Deep learning (DL) models have shown promise in automating skin cancer classification, yet challenges remain due to data scarcity and limited uncertainty awareness. This study presents a comprehensive evaluation of DL-based skin lesion classification with transfer learning and uncertainty quantification (UQ) on the HAM10000 dataset. We benchmark several pre-trained feature extractors â€“ including CLIP variants, ResNet50, DenseNet121, VGG16, and EfficientNet-V2-Large â€“ combined with traditional classifiers such as SVM, XGBoost, and logistic regression. Multiple principal component analysis (PCA) settings (64, 128, 256, 512) are explored, with LAION CLIP ViT-H&#x2F;14 and ViT-L&#x2F;14 at PCA-256 achieving the strongest baseline results. In the UQ phase, Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte Carlo Dropout (EMCD) are applied and evaluated using uncertainty-aware metrics (UAcc, USen, USpe, UPre). Ensemble methods with PCA-256 provide the best balance between accuracy and reliability. Further improvements are obtained through feature fusion of top-performing extractors at PCA-256. Finally, we propose a feature-fusion based model trained with a predictive entropy (PE) loss function, which outperforms all prior configurations across both standard and uncertainty-aware evaluations, advancing trustworthy DL-based skin cancer diagnosis. </p>
<blockquote>
<p>ç²¾ç¡®çš„çš®è‚¤ç™Œè¯Šæ–­å¯¹äºæ—©æœŸæ²»ç–—å¹¶æ”¹å–„æ‚£è€…çš„æ²»ç–—æ•ˆæœè‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è‡ªåŠ¨è¿›è¡Œçš®è‚¤ç™Œåˆ†ç±»æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºå’Œç¼ºä¹ä¸ç¡®å®šæ€§æ„è¯†ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„çš®è‚¤ç—…å˜åˆ†ç±»è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œé‡‡ç”¨è¿ç§»å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰åœ¨HAM10000æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚æˆ‘ä»¬åŸºå‡†æµ‹è¯•äº†å¤šç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼ŒåŒ…æ‹¬CLIPå˜ä½“ã€ResNet50ã€DenseNet121ã€VGG16å’ŒEfficientNet-V2-Largeï¼Œç»“åˆä¼ ç»Ÿåˆ†ç±»å™¨ï¼Œå¦‚SVMã€XGBoostå’Œé€»è¾‘å›å½’ã€‚æ¢ç´¢äº†å¤šä¸ªä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è®¾ç½®ï¼ˆ64ã€128ã€256ã€512ï¼‰ï¼Œåœ¨PCA-256è®¾ç½®ä¸‹ï¼ŒLAION CLIP ViT-H&#x2F;14å’ŒViT-L&#x2F;14å–å¾—äº†æœ€å¼ºçš„åŸºçº¿ç»“æœã€‚åœ¨ä¸ç¡®å®šæ€§é‡åŒ–é˜¶æ®µï¼Œåº”ç”¨äº†è’™ç‰¹å¡æ´›Dropoutï¼ˆMCDï¼‰ã€é›†æˆæ–¹æ³•å’Œé›†æˆè’™ç‰¹å¡æ´›Dropoutï¼ˆEMCDï¼‰ï¼Œå¹¶ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡ï¼ˆUAccã€USenã€USpeã€UPreï¼‰è¿›è¡Œè¯„ä¼°ã€‚PCA-256çš„é›†æˆæ–¹æ³•æä¾›äº†å‡†ç¡®æ€§å’Œå¯é æ€§ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡èåˆè¡¨ç°æœ€ä½³çš„æå–å™¨åœ¨PCA-256ä¸Šçš„ç‰¹å¾ï¼Œè·å¾—äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾èåˆçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨é¢„æµ‹ç†µï¼ˆPEï¼‰æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ ‡å‡†å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯„ä¼°ä¸­éƒ½ä¼˜äºæ‰€æœ‰å…ˆå‰çš„é…ç½®ï¼Œä»è€Œæ¨è¿›äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å¯ä¿¡çš®è‚¤ç™Œè¯Šæ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10302v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ·±åº¦å­¦ä¹ åœ¨çš®è‚¤ç™Œè¯Šæ–­ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨çš®è‚¤ç—…å˜åˆ†ç±»æ–¹é¢çš„è¡¨ç°ã€‚æ–‡ç« ä½¿ç”¨äº†è¿ç§»å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ï¼Œå¹¶åœ¨HAM10000æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å¯¹æ¯”äº†å¤šç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬CLIPå˜ä½“ã€ResNet50ã€DenseNet121ã€VGG16å’ŒEfficientNet-V2-Largeç­‰ï¼Œå¹¶ç»“åˆä¼ ç»Ÿåˆ†ç±»å™¨å¦‚SVMã€XGBoostå’Œé€»è¾‘å›å½’ã€‚ç ”ç©¶å‘ç°ï¼ŒLAION CLIP ViT-H&#x2F;14å’ŒViT-L&#x2F;14åœ¨PCA-256è®¾ç½®ä¸‹å–å¾—äº†æœ€ä½³åŸºçº¿ç»“æœã€‚åœ¨ä¸ç¡®å®šæ€§é‡åŒ–é˜¶æ®µï¼Œåº”ç”¨äº†Monte Carlo Dropoutã€Ensembleå’ŒEnsemble Monte Carlo Dropoutç­‰æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚èåˆç‰¹å¾çš„æ–¹æ³•åœ¨æå‡å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾èåˆçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨é¢„æµ‹ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ ‡å‡†è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯„ä¼°ä¸­éƒ½è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¸ºå¯ä¿¡èµ–çš„æ·±åº¦å­¦ä¹ çš®è‚¤ç™Œè¯Šæ–­æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶è¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çš®è‚¤ç™Œè¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨åŒ–çš®è‚¤ç—…å˜åˆ†ç±»æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>ä½¿ç”¨è¿ç§»å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¤šç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨ä¸ä¼ ç»Ÿåˆ†ç±»å™¨çš„ç»“åˆï¼Œåœ¨çš®è‚¤ç—…å˜åˆ†ç±»ä¸­è¿›è¡Œäº†å¯¹æ¯”ç ”ç©¶ã€‚</li>
<li>LAION CLIP ViTæ¨¡å‹åœ¨ç‰¹å®šè®¾ç½®ä¸‹å–å¾—äº†æœ€ä½³åŸºçº¿ç»“æœã€‚</li>
<li>Ensembleæ–¹æ³•å’Œç‰¹å¾èåˆæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºç‰¹å¾èåˆçš„æ¨¡å‹ï¼Œé‡‡ç”¨é¢„æµ‹ç†µæŸå¤±å‡½æ•°ï¼Œåœ¨è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯„ä¼°ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-133dd204aff239821b45f47ae105e3d2" align="middle">
<img src="https://picx.zhimg.com/v2-6df2f69962e1c70ff5fbe5db716a13c8" align="middle">
<img src="https://picx.zhimg.com/v2-33882f121e5ea702e53adc27fdfdfdca" align="middle">
<img src="https://picx.zhimg.com/v2-45689764503f5facf60c1ee475107348" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Image-Segmentation-and-Classification-of-E-waste-for-Training-Robots-for-Waste-Segregation"><a href="#Image-Segmentation-and-Classification-of-E-waste-for-Training-Robots-for-Waste-Segregation" class="headerlink" title="Image Segmentation and Classification of E-waste for Training Robots for   Waste Segregation"></a>Image Segmentation and Classification of E-waste for Training Robots for   Waste Segregation</h2><p><strong>Authors:Prakriti Tripathi</strong></p>
<p>Industry partners provided a problem statement that involves classifying electronic waste using machine learning models that will be used by pick-and-place robots for waste segregation. This was achieved by taking common electronic waste items, such as a mouse and charger, unsoldering them, and taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also trained and achieved 41 mAP. The model can be integrated with pick-and-place robots to perform segregation of e-waste. </p>
<blockquote>
<p>äº§ä¸šåˆä½œä¼™ä¼´æä¾›äº†ä¸€ä¸ªé—®é¢˜é™ˆè¿°ï¼Œè¯¥é—®é¢˜æ¶‰åŠä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹ç”µå­åºŸç‰©è¿›è¡Œåˆ†ç±»ï¼Œè¿™äº›æ¨¡å‹å°†ç”±æ‹¾å–æ”¾ç½®æœºå™¨äººç”¨äºåºŸç‰©åˆ†ç¦»ã€‚è¿™æ˜¯é€šè¿‡è·å–å¸¸è§çš„ç”µå­åºŸç‰©é¡¹ç›®ï¼ˆå¦‚é¼ æ ‡å’Œå……ç”µå™¨ï¼‰ï¼Œå¯¹å…¶è¿›è¡Œè§£ç„Šï¼Œå¹¶æ‹ç…§ä»¥åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†æ¥å®ç°çš„ã€‚ç„¶åï¼Œä½¿ç”¨æœ€å…ˆè¿›çš„YOLOv11æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¿è¡Œï¼Œä»¥å®æ—¶å®ç°70 mAPã€‚è¿˜è®­ç»ƒäº†Mask-RCNNæ¨¡å‹ï¼Œè¾¾åˆ°äº†41 mAPã€‚è¯¥æ¨¡å‹å¯ä»¥é›†æˆåˆ°æ‹¾å–æ”¾ç½®æœºå™¨äººä¸­ï¼Œä»¥æ‰§è¡Œç”µå­åºŸç‰©çš„åˆ†ç¦»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07122v2">PDF</a> 3 pages, 2 figures, submitted to 2025 5th International Conference on   AI-ML-Systems (AIMLSystems)</p>
<p><strong>Summary</strong><br>ï¼šæœ¬è®ºæ–‡æè¿°äº†å¦‚ä½•åˆ©ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹ç”µå­åºŸç‰©è¿›è¡Œåˆ†ç±»ï¼Œé‡‡ç”¨åˆä½œä¼™ä¼´æä¾›çš„æ–¹æ¡ˆé€šè¿‡æœºå™¨äººè¿›è¡ŒåºŸç‰©çš„åˆ†ç±»æ”¾ç½®ã€‚é€šè¿‡å¯¹å¸¸è§çš„ç”µå­åºŸç‰©å¦‚é¼ æ ‡å’Œå……ç”µå™¨è¿›è¡Œæ‹†è§£å¹¶æ‹ç…§åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œè®­ç»ƒäº†å…ˆè¿›çš„YOLOv11æ¨¡å‹ï¼Œå®ç°äº†å®æ—¶70 mAPçš„åˆ†ç±»æ•ˆæœã€‚åŒæ—¶ï¼Œä¹Ÿè®­ç»ƒäº†Mask-RCNNæ¨¡å‹ï¼Œå–å¾—äº†41 mAPçš„åˆ†ç±»æ•ˆæœã€‚è¯¥æ¨¡å‹å¯é›†æˆåˆ°æ‹¾å–æ”¾ç½®æœºå™¨äººä¸­ï¼Œå®ç°ç”µå­åºŸç‰©çš„åˆ†ç¦»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡Œä¸šåˆä½œä¼™ä¼´æå‡ºäº†ä¸€ä¸ªæ¶‰åŠç”µå­åºŸç‰©åˆ†ç±»çš„é—®é¢˜ï¼Œè¯¥é—®é¢˜å°†é€šè¿‡æœºå™¨å­¦ä¹ æ¨¡å‹è§£å†³ï¼Œå¹¶ç”±æ‹¾å–æ”¾ç½®æœºå™¨äººç”¨äºåºŸç‰©åˆ†ç¦»ã€‚</li>
<li>é€šè¿‡æ‹†è§£å¸¸è§çš„ç”µå­åºŸç‰©ç‰©å“å¹¶æ‹ç…§åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>å…ˆè¿›çš„YOLOv11æ¨¡å‹è¢«è®­ç»ƒå¹¶ç”¨äºå®æ—¶åˆ†ç±»ç”µå­åºŸç‰©ï¼Œè¾¾åˆ°äº†70 mAPçš„æ•ˆæœã€‚</li>
<li>Mask-RCNNæ¨¡å‹ä¹Ÿè¢«è®­ç»ƒç”¨äºç”µå­åºŸç‰©åˆ†ç±»ï¼Œå–å¾—äº†41 mAPçš„åˆ†ç±»æ•ˆæœã€‚</li>
<li>è®­ç»ƒå‡ºçš„æ¨¡å‹å¯ä»¥é›†æˆåˆ°æ‹¾å–æ”¾ç½®æœºå™¨äººä¸­ï¼Œå®ç°è‡ªåŠ¨åŒ–çš„åºŸç‰©åˆ†ç¦»ã€‚</li>
<li>æ­¤æ–¹æ¡ˆå¯¹äºå¤„ç†ç”µå­åºŸç‰©å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36ee894305b4cfc390880bcada05d9a9" align="middle">
<img src="https://picx.zhimg.com/v2-07386b6685da2be24f843a85ca12ae5a" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1e99f3110d3840fcecdac10c291309b7" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  Comprehend and Talk Text to Speech Synthesis via Dual Language Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4bcebee6dc782b0a6f8cb715c95b1e06" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Gaussian Herding across Pens An Optimal Transport Perspective on Global   Gaussian Reduction for 3DGS
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
