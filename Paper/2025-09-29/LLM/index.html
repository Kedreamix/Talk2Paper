<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-09-29  Assessing Classical Machine Learning and Transformer-based Approaches   for Detecting AI-Generated Research Text">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.20690v3/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-29-更新"><a href="#2025-09-29-更新" class="headerlink" title="2025-09-29 更新"></a>2025-09-29 更新</h1><h2 id="Assessing-Classical-Machine-Learning-and-Transformer-based-Approaches-for-Detecting-AI-Generated-Research-Text"><a href="#Assessing-Classical-Machine-Learning-and-Transformer-based-Approaches-for-Detecting-AI-Generated-Research-Text" class="headerlink" title="Assessing Classical Machine Learning and Transformer-based Approaches   for Detecting AI-Generated Research Text"></a>Assessing Classical Machine Learning and Transformer-based Approaches   for Detecting AI-Generated Research Text</h2><p><strong>Authors:Sharanya Parimanoharan, Ruwan D. Nawarathna</strong></p>
<p>The rapid adoption of large language models (LLMs) such as ChatGPT has blurred the line between human and AI-generated texts, raising urgent questions about academic integrity, intellectual property, and the spread of misinformation. Thus, reliable AI-text detection is needed for fair assessment to safeguard human authenticity and cultivate trust in digital communication. In this study, we investigate how well current machine learning (ML) approaches can distinguish ChatGPT-3.5-generated texts from human-written texts employing a labeled data set of 250 pairs of abstracts from a wide range of research topics. We test and compare both classical (Logistic Regression armed with classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier, and LSTM-based N-gram models) ML detection techniques. As we aim to assess each model’s performance in detecting AI-generated research texts, we also aim to test whether an ensemble of these models can outperform any single detector. Results show DistilBERT achieves the overall best performance, while Logistic Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and BERT-N-gram approaches lag. The max voting ensemble of the three best models fails to surpass DistilBERT itself, highlighting the primacy of a single transformer-based representation over mere model diversity. By comprehensively assessing the strengths and weaknesses of these AI-text detection approaches, this work lays a foundation for more robust transformer frameworks with larger, richer datasets to keep pace with ever-improving generative AI models. </p>
<blockquote>
<p>随着ChatGPT等大型语言模型（LLM）的快速应用，人工智能生成文本与人类文本之间的界限变得模糊，引发了关于学术诚信、知识产权和误信息传播等的紧迫问题。因此，需要进行可靠的AI文本检测以进行公平评估，保障人类真实性和数字通信中的信任。在本研究中，我们调查了当前机器学习（ML）方法如何区分ChatGPT 3.5生成的文本和人类撰写的文本。我们使用一组包含广泛研究主题的250对摘要的标记数据集进行测试。我们测试和比较了经典（逻辑回归配备经典词袋、POS和TF-IDF特征）和基于Transformer的机器学习检测技术的性能表现。鉴于我们的目标旨在评估每个模型检测人工智能生成研究文本的性能表现，我们还测试了这些模型的组合是否能超越任何单一检测器。结果表明，DistilBERT取得了总体最佳性能表现，而逻辑回归和BERT定制则提供了稳固、平衡的替代方案；LSTM和BERT-N-gram方法表现滞后。这三个最佳模型的投票组合未能超越DistilBERT本身，这突显了单一基于Transformer的表示形式的重要性，而非单纯的模型多样性。通过全面评估这些AI文本检测方法的优缺点，本工作为使用更大、更丰富的数据集构建更强大的Transformer框架奠定了基础，以便与不断改进生成式AI模型保持同步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20375v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着ChatGPT等大语言模型（LLM）的快速采纳，AI生成文本与人类文本之间的界限变得模糊，对学术诚信、知识产权及误解传播等方面提出紧迫问题。因此，为了公正评估和保障人类真实性以及培养数字通信中的信任，可靠的AI文本检测至关重要。本研究旨在调查当前机器学习（ML）方法如何区分ChatGPT 3.5生成的文本和人类撰写的文本。我们利用一组包含广泛研究主题的250对摘要的标记数据集进行测试和比较。方法包括经典方法（如使用传统词袋模型、POS和TF-IDF特征的逻辑回归）和基于转换器的方法（如增强BERT的N元特征、蒸馏BERT、带有轻量级自定义分类器的BERT和基于LSTM的N元模型）。本研究旨在评估每种模型检测AI生成研究文本的性能，同时也测试了模型组合是否能超越单一检测器。结果显示，蒸馏BERT在总体性能上表现最佳，逻辑回归和自定义BERT提供了稳健且平衡的替代方案；LSTM和BERT的N元方法表现较弱。三个最佳模型的投票组合未能超越蒸馏BERT本身，突显出单一基于转换器的表示学习的重要性，而非单纯的模型多样性。本研究全面评估了这些AI文本检测方法的优缺点，为使用更大、更丰富数据集的更稳健的转换器框架奠定了基石，以跟上不断进步的生成式AI模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大语言模型（LLM）如ChatGPT的普及引发了关于学术诚信、知识产权和误解传播的关切。</li>
<li>当前机器学习技术在区分ChatGPT 3.5生成的文本和人类撰写的文本方面存在挑战。</li>
<li>研究采用多种机器学习技术进行比较，包括经典方法和基于转换器的技术。</li>
<li>在检测AI生成的文本方面，蒸馏BERT表现最佳，逻辑回归和自定义BERT为可靠替代方案。</li>
<li>基于LSTM和BERT的N元方法性能较弱，显示模型多样性的优势并非绝对。</li>
<li>模型组合尝试未能超越最佳单一模型表现，强调单一基于转换器的模型的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20375">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.20375v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.20375v1/page_2_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ButterflyQuant-Ultra-low-bit-LLM-Quantization-through-Learnable-Orthogonal-Butterfly-Transforms"><a href="#ButterflyQuant-Ultra-low-bit-LLM-Quantization-through-Learnable-Orthogonal-Butterfly-Transforms" class="headerlink" title="ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable   Orthogonal Butterfly Transforms"></a>ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable   Orthogonal Butterfly Transforms</h2><p><strong>Authors:Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang</strong></p>
<p>Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} &#x3D; \mathbf{Wx} &#x3D; (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transforms–Hadamard matrices achieving optimal worst-case coherence $\mu &#x3D; 1&#x2F;\sqrt{n}$–that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard’s discrete ${+1, -1}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms’ continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU–a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \href{<a target="_blank" rel="noopener" href="https://github.com/42Shawn/Butterflyquant-llm%7D%7BCodes%7D">https://github.com/42Shawn/Butterflyquant-llm}{Codes}</a> are available. </p>
<blockquote>
<p>大型语言模型需要大量的内存空间，严重限制了其在消费者硬件上的部署。量化通过降低数值精度来减少内存，但极端的2位量化由于激活值的异常值而遭受严重的性能损失。基于旋转的方法，如QuIP和QuaRot，通过对正交变换应用来计算不变性，以消除异常值，再进行量化：$\mathbf{y} &#x3D; \mathbf{Wx} &#x3D; (\mathbf{WQ}^T)(\mathbf{Qx})$，其中$\mathbf{Q}$为正交矩阵。然而，这些方法使用固定的变换——哈达玛矩阵，实现最优最坏的相干性$\mu &#x3D; 1&#x2F;\sqrt{n}$，无法适应特定的权重分布。我们发现不同的转换器层表现出不同的异常值模式，因此提倡采用分层自适应旋转而不是一刀切的方法。在这项工作中，我们提出了ButterflyQuant，它用可学习的蝴蝶变换来替代哈达玛旋转，由连续的吉文斯旋转角进行参数化。与哈达玛的离散${+1, -1}$条目不同，后者是不可微分的，因此禁止基于梯度的学习，蝴蝶变换的连续参数化可以在保持正交性的同时实现平滑优化。这种正交约束确保了异常值抑制的理论保证，同时实现了$O(n \log n)$的计算复杂性，只有$\frac{n \log n}{2}$个可学习参数。我们进一步对后变换激活值引入均匀性正则化，以促进更适合量化的平滑分布。学习只需要128个校准样本，并在单个GPU上几分钟内收敛——这是一次微不足道的成本。对于使用2位量化的LLaMA-2-7B，ButterflyQuant实现了15.4的困惑度，而QuIP为37.3。<a target="_blank" rel="noopener" href="https://github.com/42Shawn/Butterflyquant-llm">代码</a>可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09679v2">PDF</a> Replace discrete Hadamard transforms with continuous Butterfly   transforms to facilitate the learning of rotation matrices in LLM   quantization</p>
<p><strong>Summary</strong></p>
<p>大型语言模型需要大量内存，限制了其在消费者硬件上的部署。量化通过降低数值精度来减少内存使用，但极端2位量化会导致性能严重下降。旋转方法如QuIP和QuaRot应用正交变换消除异常值。本文提出ButterflyQuant，用可学习的蝴蝶变换代替Hadamard旋转，实现平滑优化并保障正交性，从而提高量化性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在消费者硬件上的部署受限于内存需求。</li>
<li>量化是降低内存使用的一种方法，但极端量化可能导致性能严重损失。</li>
<li>旋转方法如QuIP和QuaRot通过正交变换消除异常值。</li>
<li>ButterflyQuant使用可学习的蝴蝶变换，实现平滑优化并保障正交性。</li>
<li>ButterflyQuant方法实现了理论上的异常值抑制，同时具有O(n log n)的计算复杂度和有效的参数学习。</li>
<li>通过引入均匀性正则化，促进更平滑的分布适合量化。</li>
<li>ButterflyQuant方法的学习成本较低，只需要少量的校准样本，并在单个GPU上快速收敛。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.09679v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.09679v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.09679v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LM-Searcher-Cross-domain-Neural-Architecture-Search-with-LLMs-via-Unified-Numerical-Encoding"><a href="#LM-Searcher-Cross-domain-Neural-Architecture-Search-with-LLMs-via-Unified-Numerical-Encoding" class="headerlink" title="LM-Searcher: Cross-domain Neural Architecture Search with LLMs via   Unified Numerical Encoding"></a>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via   Unified Numerical Encoding</h2><p><strong>Authors:Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li</strong></p>
<p>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at <a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher">https://github.com/Ashone3/LM-Searcher</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的最新进展为解决复杂的优化问题，包括神经网络架构搜索（NAS）提供了新的途径。然而，现有的基于LLM的NAS方法严重依赖于提示工程和特定领域的调整，这限制了它们在各种任务中的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，这是一个新型框架，它利用LLM进行跨域神经网络架构优化，而无需进行广泛的特定领域适应。我们的方法的核心是NCode，这是一种用于神经网络架构的通用数值字符串表示，它实现了跨域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLM从候选池中选择高性能架构，使用基于新型基于剪枝的子空间采样策略生成的指令调整样本。我们整理的数据集涵盖了广泛的架构性能对，鼓励鲁棒和可迁移学习。综合实验表明，LM-Searcher在域内（例如，用于图像分类的CNN）和域外（例如，用于分割和生成的LoRA配置）任务中均取得了具有竞争力的表现，为灵活和通用的LLM基础架构搜索建立了新范式。数据集和模型将在<a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher%E5%B9%BF%E5%B8%83%E3%80%82">https://github.com/Ashone3/LM-Searcher发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05657v3">PDF</a> EMNLP 2025 Main</p>
<p><strong>Summary</strong></p>
<p>LLM的进步为求解复杂的优化问题提供了新的途径，包括神经网络架构搜索（NAS）。然而，现有的LLM驱动的NAS方法严重依赖于提示工程和特定领域的调整，这在实践中限制了其在不同任务中的实用性和可扩展性。本研究提出了一种新型的框架LM-Searcher，该框架利用LLMs进行跨域神经网络架构优化，无需大量的特定领域适应。其核心是NCode，一种用于神经网络架构的通用数值字符串表示，它实现了跨域架构编码和搜索。该研究将NAS问题重新表述为排序任务，并使用基于新型剪枝子空间采样策略生成的指令调整样本，训练LLMs从候选池中选择高性能架构。实验表明，LM-Searcher在域内（如用于图像分类的CNN）和域外（如用于分割和生成的LoRA配置）的任务中都取得了具有竞争力的表现，为灵活和通用的LLM基础架构搜索建立了新范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的进步为求解复杂的优化问题提供了新的途径，包括神经网络架构搜索（NAS）。</li>
<li>现有LLM驱动的NAS方法存在实践性和扩展性问题，因为它们严重依赖于提示工程和特定领域的调整。</li>
<li>LM-Searcher框架利用LLMs进行跨域神经网络架构优化，无需大量特定领域适应。</li>
<li>NCode是LM-Searcher的核心，作为一种通用数值字符串表示，用于跨域架构编码和搜索。</li>
<li>研究将NAS问题重新表述为排序任务，并使用指令调整样本训练LLMs选择高性能架构。</li>
<li>LM-Searcher在多种任务中都取得了具有竞争力的表现，包括域内和域外的任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.05657v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.05657v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.05657v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.05657v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2509.05657v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MMSI-Bench-A-Benchmark-for-Multi-Image-Spatial-Intelligence"><a href="#MMSI-Bench-A-Benchmark-for-Multi-Image-Spatial-Intelligence" class="headerlink" title="MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence"></a>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</h2><p><strong>Authors:Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang</strong></p>
<p>Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI’s o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench">https://runsenxu.com/projects/MMSI_Bench</a> . </p>
<blockquote>
<p>空间智能对于在复杂的物理世界中运行的多模态大型语言模型（MLLM）至关重要。然而，现有的基准测试仅探索单图像关系，因此无法评估现实世界部署所需的多图像空间推理能力。我们引入了MMSI-Bench，这是一个专门用于多图像空间智能的VQA基准测试。六位3D视觉研究人员花费了超过300小时的时间，从超过12万张图像中精心创作了1000个具有挑战性且毫无歧义的多项选择题，每个问题都配备了精心设计的干扰项和逐步推理过程。我们进行了广泛的实验，全面评估了34个开源和专有MLLM，观察到很大的差距：最强的开源模型准确率约为30%，OpenAI的o3推理模型达到40%，而人类得分为97%。这些结果强调了MMSI-Bench的挑战性，以及未来研究的巨大潜力。利用注释的推理过程，我们还提供了一个自动错误分析管道，诊断了四种主要的失败模式，包括（1）接地错误，（2）重叠匹配和场景重建错误，（3）情况转换推理错误，和（4）空间逻辑错误，为推进多图像空间智能提供了宝贵的见解。项目页面：<a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench%E3%80%82">https://runsenxu.com/projects/MMSI_Bench。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23764v2">PDF</a> 34 pages. A comprehensive, fully human-curated, multi-image-based   spatial intelligence benchmark with reasoning annotation for MLLMs. Project   page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench">https://runsenxu.com/projects/MMSI_Bench</a></p>
<p><strong>Summary</strong><br>     空间智能对于在复杂物理世界中运行的多模态大型语言模型（MLLMs）至关重要。然而，现有的基准测试仅探索单图像关系，无法评估多图像空间推理能力，与现实世界部署需求脱节。为此，我们推出MMSI-Bench，一个专注于多图像空间智能的VQA基准测试。该测试包含1000个具有挑战性的明确选择题，由六位3D视觉研究人员花费超过300小时精心创建，涵盖超过12万张图像，每张图像都配有精心设计的干扰项和逐步推理过程。我们对34个开源和专有MLLM进行了广泛实验和全面评估，发现了一个显著的差距：最强开源模型的准确率约为30%，OpenAI的o3推理模型达到40%，而人类得分率高达97%。该基准测试还提供自动错误分析管道，诊断了四种主要的失败模式，包括接地错误、重叠匹配和场景重建错误、情境转换推理错误和空间逻辑错误等，为推进多图像空间智能提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间智能对多模态大型语言模型在复杂物理世界中的性能至关重要。</li>
<li>现有基准测试无法充分评估多图像空间推理能力。</li>
<li>MMSI-Bench是一个新的VQA基准测试，专注于多图像空间智能，包含1000个具有挑战性的选择题。</li>
<li>MMSI-Bench评估了34个MLLM的性能，发现存在显著的性能差距。</li>
<li>最强开源模型准确率约为30%，OpenAI的o3推理模型达到40%，而人类得分率高达97%。</li>
<li>MMSI-Bench提供了自动错误分析管道，有助于诊断多图像空间智能的主要失败模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23764">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.23764v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.23764v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.23764v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.23764v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.23764v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MME-VideoOCR-Evaluating-OCR-Based-Capabilities-of-Multimodal-LLMs-in-Video-Scenarios"><a href="#MME-VideoOCR-Evaluating-OCR-Based-Capabilities-of-Multimodal-LLMs-in-Video-Scenarios" class="headerlink" title="MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in   Video Scenarios"></a>MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in   Video Scenarios</h2><p><strong>Authors:Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yushuo Guan, Zhang Zhang, Liang Wang, Haoxuan Li, Zhouchen Lin, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在静态图像的OCR（光学字符识别）中取得了相当的准确性。然而，由于运动模糊、时间变化以及视频内容固有的视觉效果等因素，它们在视频OCR中的有效性大大降低。为了为训练实用型MLLM提供更清晰的指导，我们引入了MME-VideoOCR基准测试，它涵盖了广泛的视频OCR应用场景。MME-VideoOCR包含10个任务类别，共计25个独立任务，涵盖44种不同场景。这些任务不仅涉及文本识别，还融入对视频内文本内容的更深理解和推理。该基准测试包含1464个视频，具有不同的分辨率、纵横比和持续时间，以及2000个精心策划、手动标注的问题答案对。我们在MME-VideoOCR上评估了18种最新MLLM，结果显示，即使表现最佳的模型（Gemini-2.5 Pro）准确率也只有73.7%。精细分析表明，虽然现有MLLM在相关文本包含在单个或少数几个帧的任务中表现出强大的性能，但在需要整体视频理解的任务中表现出有限的能力。这些局限性在需要时空推理、跨帧信息融合或抵抗语言先验偏见的场景中尤为明显。我们的研究还发现，高分辨率的视觉输入和充足的时间覆盖对于动态视频场景的可靠OCR至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21333v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>MLLMs在静态图像的OCR中表现出较高的准确性，但在视频OCR中的效果却大打折扣，面临运动模糊、时间变化和视频内容固有视觉效应的挑战。为给训练实用MLLMs提供更明确的指导，引入了MME-VideoOCR基准测试，涵盖广泛的视频OCR应用场景，包括10个任务类别、25个独立任务和44个多样化场景。该基准测试包含1464个不同分辨率、比例和时长的视频，以及2000对手动精心挑选和标注的问题答案对。对18种最先进的MLLMs的评估显示，即使是最好的模型（Gemini-2.5 Pro）准确率也只有73.7%。精细分析表明，现有MLLMs在处理单一或少数帧包含相关文本的任务时表现出强大性能，但在需要整体视频理解的任务中表现有限，尤其是在需要时空推理、跨帧信息整合或抵抗语言先验偏见的场景中。还发现高分辨率视觉输入和足够的时间覆盖对于动态视频场景的可靠OCR至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在视频OCR中的准确性较低，受到运动模糊、时间变化和视频内容固有视觉效应的影响。</li>
<li>引入MME-VideoOCR基准测试，为训练MLLMs提供清晰指导，涵盖多种视频OCR应用场景。</li>
<li>MME-VideoOCR包含多种任务，涉及视频中的文本识别、理解和推理。</li>
<li>评估显示，现有MLLMs在视频OCR方面存在局限性，即使最好的模型准确率也只有73.7%。</li>
<li>MLLMs在处理需要整体视频理解的任务时表现有限，特别是在需要时空推理和跨帧信息整合的场景中。</li>
<li>高分辨率视觉输入和足够的时间覆盖对动态视频场景的可靠OCR至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21333">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.21333v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.21333v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.21333v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.21333v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Unaligned-to-Aligned-Scaling-Multilingual-LLMs-with-Multi-Way-Parallel-Corpora"><a href="#From-Unaligned-to-Aligned-Scaling-Multilingual-LLMs-with-Multi-Way-Parallel-Corpora" class="headerlink" title="From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way   Parallel Corpora"></a>From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way   Parallel Corpora</h2><p><strong>Authors:Yingli Shen, Wen Lai, Shuo Wang, Ge Gao, Kangyang Luo, Alexander Fraser, Maosong Sun</strong></p>
<p>Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data. </p>
<blockquote>
<p>持续在大规模多语言数据上进行预训练和指导调整，已被证明在将大型语言模型（LLM）扩展到低资源语言时是有效的。然而，此类数据的未对齐性质限制了其有效捕捉跨语言语义的能力。相比之下，多向并行数据（其中相同的内容在多语言之间对齐）提供了更强的跨语言一致性，并提供了提高多语言性能的更大潜力。在本文中，我们介绍了一个基于TED演讲的大规模、高质量的多向并行语料库TED2025。该语料库涵盖113种语言，最多有50种语言并行对齐，确保广泛的多语言覆盖。使用该数据集，我们研究了如何利用多向并行数据来增强LLM的最佳实践，包括持续预训练、指令调整的策略，以及对关键影响因素的分析。在六个多语言基准测试上的实验表明，在多向并行数据上训练的模型始终优于在未对齐的多语言数据上训练的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14045v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模多语种数据的持续预训练与指令微调对于扩展大型语言模型（LLM）至低资源语言十分有效。然而，此类数据的未对齐特性限制了其捕捉跨语言语义的能力。相比之下，多向平行数据（在不同语言间具有相同内容的对齐）提供了更强的跨语言一致性，并有望改善多语言性能。本文介绍了一个基于TED演讲的大型、高质量多向平行语料库TED2025，该语料库涵盖113种语言，最多有50种语言并行对齐，确保了广泛的多语言覆盖。利用此数据集，我们探讨了如何利用多向平行数据来优化LLM的最佳实践，包括持续预训练策略、指令调整以及对关键影响因素的分析。在六个多语种基准测试上的实验表明，使用多向平行数据训练的模型始终优于使用未对齐的多语种数据训练的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模多语种数据的持续预训练和指令微调对于扩展LLM至低资源语言具有显著效果。</li>
<li>多向平行数据提供了更强的跨语言一致性，有助于改善多语言性能。</li>
<li>TED2025是一个大型、高质量的多向平行语料库，涵盖113种语言，为优化LLM提供了丰富的资源。</li>
<li>使用多向平行数据训练的模型性能优于使用未对齐的多语种数据训练的模型。</li>
<li>最佳实践包括利用多向平行数据的持续预训练策略、指令调整策略以及对关键影响因素的分析。</li>
<li>多语种基准测试表明，使用多向平行数据可以提高LLM的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.14045v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ambiguity-Resolution-in-Text-to-Structured-Data-Mapping"><a href="#Ambiguity-Resolution-in-Text-to-Structured-Data-Mapping" class="headerlink" title="Ambiguity Resolution in Text-to-Structured Data Mapping"></a>Ambiguity Resolution in Text-to-Structured Data Mapping</h2><p><strong>Authors:Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu</strong></p>
<p>Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods to ambiguity handling either rely on the ReACT framework to obtain correct mappings through trial and error, or on supervised fine-tuning to bias models toward specific tasks. In this paper, we adopt a different approach that characterizes representation differences of ambiguous text in the latent space and leverages these differences to identify ambiguity before mapping them to structured data. To detect sentence-level ambiguity, we focus on the relationship between ambiguous questions and their interpretations. Unlike distances calculated by dense embeddings, we introduce a new distance measure based on a path kernel over concepts. With this measurement, we identify patterns to distinguish ambiguous from unambiguous questions. Furthermore, we propose a method for improving LLM performance on ambiguous agentic tool calling through missing concept prediction. Both achieve state-of-the-art results. </p>
<blockquote>
<p>自然语言中的歧义是利用大型语言模型（LLM）实现从文本到结构化数据映射的显著障碍，这影响了文本映射到智能工具调用和文本到SQL查询等任务的性能。现有的处理歧义的方法要么依赖于ReACT框架通过试错获得正确的映射，要么依赖于有监督微调使模型偏向于特定任务。在本文中，我们采用了一种不同的方法，即在潜在空间中刻画模糊文本表示的差异，并利用这些差异在将其映射到结构化数据之前识别歧义。为了检测句子级别的歧义，我们关注模糊问题与其解释之间的关系。与密集嵌入计算的距离不同，我们引入了一种基于概念路径核的新距离度量。通过这种度量，我们能够识别模式来区分模糊和非模糊问题。此外，我们提出了一种通过预测缺失概念来提高LLM处理模糊智能工具调用性能的方法。两者均达到了目前最佳的水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11679v2">PDF</a> 17 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了在自然语言中的歧义对大型语言模型（LLM）实现文本到结构化数据映射的准确性的挑战。文章提出了一种新方法，通过识别文本表示中的差异来识别歧义，并引入了一种新的距离度量方法——概念路径核，以区分模糊和非模糊问题。此外，还提出了一种通过预测缺失概念来提高LLM在模糊代理工具调用方面的性能的方法。这些方法均达到了最新的性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自然语言中的歧义对大型语言模型实现文本到结构化数据映射的准确性构成了挑战。</li>
<li>现有方法主要依赖于ReACT框架进行歧义处理或监督微调来偏向特定任务。</li>
<li>文章提出了一种新方法，通过识别文本表示中的差异来识别歧义。</li>
<li>引入了一种新的距离度量方法——概念路径核，以区分模糊和非模糊问题。</li>
<li>提出了一种通过预测缺失概念来提高LLM在模糊代理工具调用方面的性能的方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.11679v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.11679v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2505.11679v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="In-Context-Edit-Enabling-Instructional-Image-Editing-with-In-Context-Generation-in-Large-Scale-Diffusion-Transformer"><a href="#In-Context-Edit-Enabling-Instructional-Image-Editing-with-In-Context-Generation-in-Large-Scale-Diffusion-Transformer" class="headerlink" title="In-Context Edit: Enabling Instructional Image Editing with In-Context   Generation in Large Scale Diffusion Transformer"></a>In-Context Edit: Enabling Instructional Image Editing with In-Context   Generation in Large Scale Diffusion Transformer</h2><p><strong>Authors:Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang</strong></p>
<p>Instruction-based image editing enables precise modifications via natural language prompts, but existing methods face a precision-efficiency tradeoff: fine-tuning demands massive datasets (&gt;10M) and computational resources, while training-free approaches suffer from weak instruction comprehension. We address this by proposing ICEdit, which leverages the inherent comprehension and generation abilities of large-scale Diffusion Transformers (DiTs) through three key innovations: (1) An in-context editing paradigm without architectural modifications; (2) Minimal parameter-efficient fine-tuning for quality improvement; (3) Early Filter Inference-Time Scaling, which uses VLMs to select high-quality noise samples for efficiency. Experiments show that ICEdit achieves state-of-the-art editing performance with only 0.1% of the training data and 1% trainable parameters compared to previous methods. Our approach establishes a new paradigm for balancing precision and efficiency in instructional image editing. Codes and demos can be found in <a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/">https://river-zhang.github.io/ICEdit-gh-pages/</a>. </p>
<blockquote>
<p>基于指令的图像编辑能够通过自然语言提示进行精确修改，但现有方法面临着精度与效率之间的权衡：微调需要大规模数据集（&gt; 10M）和计算资源，而无需训练的方法则存在指令理解较弱的问题。我们通过提出ICEdit来解决这一问题，它通过三个关键创新点利用大规模扩散变压器（DiTs）的固有理解和生成能力：（1）无需架构修改的内部编辑范式；（2）用于改进质量的参数效率极低的微调；（3）早期滤波推理时间缩放，使用VLMs选择高质量的噪声样本以提高效率。实验表明，与以前的方法相比，ICEdit仅使用0.1%的训练数据和1%的可训练参数就实现了最先进的编辑性能。我们的方法为平衡指令图像编辑中的精度和效率建立了新的范式。代码和演示可在<a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/%E6%89%BE%E5%88%B0%E3%80%82">https://river-zhang.github.io/ICEdit-gh-pages/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20690v3">PDF</a> Accepted by NeurIPS 2025, there will be future updates for camera   ready version. Code: <a target="_blank" rel="noopener" href="https://github.com/River-Zhang/ICEdit">https://github.com/River-Zhang/ICEdit</a></p>
<p><strong>Summary</strong><br>基于指令的图像编辑可以通过自然语言提示进行精确修改，但现有方法面临精确性与效率之间的权衡：精细调整需要大量数据集（&gt; 10M）和计算资源，而免培训方法则缺乏指令理解。我们提出ICEdit来解决这个问题，它通过三个关键创新点利用大型扩散变压器（DiTs）的固有理解和生成能力：（1）无需架构修改的在上下文中的编辑模式；（2）最小参数高效的微调以提高质量；（3）早期过滤推理时间缩放，使用VLMs选择高质量的噪声样本以提高效率。实验表明，ICEdit只需使用0.1%的训练数据和1%的可训练参数即可实现最先进的编辑性能。我们的方法为平衡指令图像编辑中的精确性和效率建立了新的范例。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICEdit解决了现有图像编辑方法的精确性与效率之间的权衡问题。</li>
<li>ICEdit通过三个关键创新点利用大型扩散变压器（DiTs）的固有理解和生成能力。</li>
<li>ICEdit实现了在上下文中的编辑模式，无需进行架构修改。</li>
<li>ICEdit通过最小参数高效的微调来提高图像编辑的质量。</li>
<li>ICEdit采用早期过滤推理时间缩放技术，通过选择高质量的噪声样本提高效率。</li>
<li>实验表明，ICEdit在仅使用少量训练数据和参数的情况下即可实现最先进的编辑性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.20690v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.20690v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.20690v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.20690v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.20690v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAIN-Mutual-Alignment-Is-Necessary-for-instruction-tuning"><a href="#MAIN-Mutual-Alignment-Is-Necessary-for-instruction-tuning" class="headerlink" title="MAIN: Mutual Alignment Is Necessary for instruction tuning"></a>MAIN: Mutual Alignment Is Necessary for instruction tuning</h2><p><strong>Authors:Fanyi Yang, Jianfeng Liu, Xin Zhang, Haoyu Liu, Xixin Cao, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang</strong></p>
<p>Instruction tuning has empowered large language models (LLMs) to achieve remarkable performance, yet its success heavily depends on the availability of large-scale, high-quality instruction-response pairs. To meet this demand, various methods have been developed to synthesize data at scale. However, current methods for scaling up data generation often overlook a crucial aspect: the alignment between instructions and responses. We hypothesize that the quality of instruction-response pairs is determined not by the individual quality of each component, but by the degree of mutual alignment. To address this, we propose a Mutual Alignment Framework (MAIN) which enforces coherence between instructions and responses through mutual constraints. We demonstrate that MAIN generalizes well across model architectures and sizes, achieving state-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse benchmarks. This work underscores the critical role of instruction-response alignment in enabling generalizable and high-quality instruction tuning for LLMs. All code is available from our repository. </p>
<blockquote>
<p>指令微调使大型语言模型（LLM）取得了显著的性能，但其成功在很大程度上取决于大规模高质量指令响应对的可用性。为了满足这一需求，已经开发了各种方法来大规模合成数据。然而，当前扩大数据生成规模的方法往往忽视了一个关键方面：指令与响应之间的对齐。我们假设指令响应对的质并不取决于每个组件的单独质量，而是取决于相互对齐的程度。为了解决这一问题，我们提出了相互对齐框架（MAIN），通过相互约束来加强指令和响应之间的连贯性。我们证明，MAIN在模型架构和规模方面具有很好的通用性，在LLaMA、Mistral和Qwen模型的各种基准测试中实现了最新性能。这项工作强调了指令响应对齐在使LLM实现通用和高质量的指令调整中的关键作用。所有代码均可在我们的存储库中获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12913v3">PDF</a> Accepted by EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>大规模指令微调使大型语言模型（LLM）取得了显著的性能提升，但成功很大程度上取决于大规模高质量指令响应对的可用性。为满足这一需求，已经开发了各种方法来实现数据的规模化合成。然而，现有方法往往忽视了指令与响应之间对齐的关键方面。本研究假设指令响应对的品质并非由单一成分的品质决定，而是由相互对齐的程度决定。为解决这一问题，我们提出了相互对齐框架（MAIN），通过相互约束实现指令与响应之间的连贯性。我们在不同的模型架构和规模上验证了MAIN的泛化能力，在LLaMA、Mistral和Qwen模型上的多种基准测试上取得了最先进的性能。这项工作强调了指令响应对齐在使LLM实现通用和高质感的指令调整中的关键作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令微调对LLM性能有重要影响。</li>
<li>大规模高质量指令响应对的可用性是关键。</li>
<li>现有数据合成方法往往忽视指令与响应之间的对齐。</li>
<li>指令响应对的品质取决于指令与响应的相互对齐程度。</li>
<li>提出相互对齐框架（MAIN）以实现指令与响应的连贯性。</li>
<li>MAIN框架在不同的模型架构和规模上具有良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.12913v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.12913v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.12913v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AttentionDrop-A-Novel-Regularization-Method-for-Transformer-Models"><a href="#AttentionDrop-A-Novel-Regularization-Method-for-Transformer-Models" class="headerlink" title="AttentionDrop: A Novel Regularization Method for Transformer Models"></a>AttentionDrop: A Novel Regularization Method for Transformer Models</h2><p><strong>Authors:Mirza Samad Ahmed Baig, Syeda Anshrah Gillani, Abdul Akbar Khan, Shahid Munir Shah, Muhammad Omer Khan</strong></p>
<p>Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech processing. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. In this research, a unified family of stochastic regularization techniques has been proposed, i.e. AttentionDrop with its three different variants, which operate directly on the self-attention distributions. Hard Attention Masking randomly zeroes out top-k attention logits per query to encourage diverse context utilization, Blurred Attention Smoothing applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions, and Consistency-Regularized AttentionDrop enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss. Results achieved in the study demonstrate that AttentionDrop consistently improves accuracy, calibration, and adversarial robustness over standard Dropout, DropConnect, and R-Drop baselines </p>
<blockquote>
<p>基于Transformer的架构在自然语言处理、计算机视觉和语音处理等任务中实现了最先进的性能。然而，它们巨大的容量往往会导致过拟合，尤其是在训练数据有限或嘈杂的情况下。在这项研究中，提出了一系列统一的随机正则化技术，即AttentionDrop及其三种不同变体。它们直接在自注意力分布上运行。硬注意力掩码随机将每个查询的前k个注意力对数置零，以鼓励多样化的上下文利用；模糊注意力平滑在注意力对数上应用动态高斯卷积，以扩散过于尖锐的分布；一致性正则化AttentionDrop通过基于KL的一致性损失，强制多个独立AttentionDrop扰动下的输出稳定性。研究结果表明，AttentionDrop在准确度、校准和对抗稳健性方面持续优于标准Dropout、DropConnect和R-Drop基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12088v2">PDF</a> 25 pages</p>
<p><strong>Summary</strong>：基于Transformer的架构在自然语言处理、计算机视觉和语音处理等任务中表现卓越，但其巨大的容量易导致过拟合。研究提出了一种统一的随机正则化技术——AttentionDrop及其三种变体，直接作用于自我注意分布。这三种技术包括硬注意力屏蔽、模糊注意力平滑和一致性正则化AttentionDrop。实验结果表明，AttentionDrop在准确性、校准和对抗稳健性方面均优于标准Dropout、DropConnect和R-Drop基线。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Transformer架构在多个任务中表现优秀，但存在过拟合问题。</li>
<li>研究提出了一种新的随机正则化技术——AttentionDrop，包括三种变体技术。</li>
<li>AttentionDrop直接作用于自我注意分布，旨在解决过拟合问题。</li>
<li>硬注意力屏蔽通过随机屏蔽注意力得分来鼓励多样化的上下文利用。</li>
<li>模糊注意力平滑通过动态高斯卷积来扩散过于集中的注意力分布。</li>
<li>一致性正则化AttentionDrop通过KL一致性损失确保输出在多次独立的AttentionDrop扰动下保持稳定。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.12088v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2504.12088v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding"><a href="#On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding" class="headerlink" title="On the Perception Bottleneck of VLMs for Chart Understanding"></a>On the Perception Bottleneck of VLMs for Chart Understanding</h2><p><strong>Authors:Junteng Liu, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, Junxian He</strong></p>
<p>Chart understanding requires models to effectively analyze and reason about numerical data, textual elements, and complex visual components. Our observations reveal that the perception capabilities of existing large vision-language models (LVLMs) constitute a critical bottleneck in this process. In this study, we delve into this perception bottleneck by decomposing it into two components: the vision encoder bottleneck, where the visual representation may fail to encapsulate the correct information, and the extraction bottleneck, where the language model struggles to extract the necessary information from the provided visual representations. Through comprehensive experiments, we find that (1) the information embedded within visual representations is substantially richer than what is typically captured by linear extractors, such as the widely used retrieval accuracy metric; (2) While instruction tuning effectively enhances the extraction capability of LVLMs, the vision encoder remains a critical bottleneck, demanding focused attention and improvement. Therefore, we further enhance the visual encoder to mitigate the vision encoder bottleneck under a contrastive learning framework. Empirical results demonstrate that our approach significantly mitigates the perception bottleneck and improves the ability of LVLMs to comprehend charts. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>. </p>
<blockquote>
<p>图表理解需要模型对数值数据、文本元素和复杂视觉成分进行有效的分析和推理。我们的观察发现，现有大型视觉语言模型（LVLMs）的感知能力构成这一过程中的关键瓶颈。在本研究中，我们通过将其分解为两个组件来深入研究这一感知瓶颈：视觉编码器瓶颈，其中视觉表示可能无法封装正确的信息；以及提取瓶颈，其中语言模型难以从提供的视觉表示中提取必要的信息。通过综合实验，我们发现（1）视觉表示中所嵌入的信息远比线性提取器（如广泛使用的检索准确率指标）所捕获的要丰富得多；（2）虽然指令调整有效地提高了LVLMs的提取能力，但视觉编码器仍然是一个关键的瓶颈，需要集中注意力和进行改进。因此，我们进一步增强了视觉编码器，以在对比学习框架下缓解视觉编码器瓶颈。经验结果表明，我们的方法显著缓解了感知瓶颈，提高了LVLMs理解图表的能力。代码已公开在<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18435v2">PDF</a> EMNLP 2025: Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>现有大型视觉语言模型在处理图表理解时存在视觉编码瓶颈和信息提取瓶颈。研究发现视觉表征所包含的信息丰富度超出传统线性提取器的捕捉能力，而指令微调虽然提高了模型的提取能力，但视觉编码器仍是关键瓶颈。为缓解这一问题，采用对比学习框架增强了视觉编码器。公开代码位于：[公开链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型在处理图表理解时面临视觉编码和信息提取两大瓶颈。</li>
<li>视觉表征包含的信息丰富度超出传统线性提取器的捕捉能力。</li>
<li>指令微调能提高模型的提取能力，但视觉编码器仍是关键改进点。</li>
<li>对比学习框架被用于增强视觉编码器，以缓解视觉编码瓶颈。</li>
<li>该研究提出了新的方法改善大型视觉语言模型对图表的理解能力。</li>
<li>研究成果公开可用，方便后续研究与应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18435">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.18435v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.18435v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.18435v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.18435v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.18435v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.18435v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition"><a href="#LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition" class="headerlink" title="LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?"></a>LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?</h2><p><strong>Authors:Bangyan Li, Wenxuan Huang, Zhenkun Gao, Yeqiang Wang, Yunhang Shen, Jingzhong Lin, Ling You, Yuxiang Shen, Shaohui Lin, Wanli Ouyang, Yuling Sun</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, we found that MLLMs cannot process effectively from fine-grained medical image data in the traditional Visual Question Answering (VQA) pipeline, as they do not exploit the captured features and available medical knowledge fully, results in MLLMs usually performing poorly in zero-shot medical disease recognition. Fortunately, this limitation does not indicate that MLLMs are fundamentally incapable of addressing fine-grained recognition tasks. From a feature representation perspective, MLLMs demonstrate considerable potential for tackling such challenging problems. Thus, to address this challenge, we propose LLaVA-RadZ, a simple yet effective framework for zero-shot medical disease recognition via utilizing the existing MLLM features. Specifically, we design an end-to-end training strategy, termed Decoding-Side Feature Alignment Training (DFAT) to take advantage of the characteristics of the MLLM decoder architecture and incorporate modality-specific tokens tailored for different modalities. Additionally, we introduce a Domain Knowledge Anchoring Module (DKAM) to exploit the intrinsic medical knowledge of large models, which mitigates the category semantic gap in image-text alignment. Extensive experiments demonstrate that our LLaVA-RadZ significantly outperforms traditional MLLMs in zero-shot disease recognition, achieving the comparable performance to the well-established and highly-optimized CLIP-based approaches. </p>
<blockquote>
<p>最近，多模态大型语言模型（MLLMs）在各种视觉语言任务中展示了出色的视觉理解和推理能力。然而，我们发现MLLMs在传统的视觉问答（VQA）管道中无法有效地处理精细的医学图像数据，因为它们没有充分利用捕获的特征和可用的医学知识，导致MLLMs在零样本医疗疾病识别中通常表现不佳。幸运的是，这一局限性并不意味着MLLMs从根本上无法解决精细识别任务。从特征表示的角度来看，MLLMs在解决这类难题方面显示出巨大的潜力。因此，为了应对这一挑战，我们提出了LLaVA-RadZ框架，这是一个利用现有MLLM特性进行零样本医疗疾病识别的简单有效的框架。具体来说，我们设计了一种端到端的训练策略，称为解码侧特征对齐训练（DFAT），以利用MLLM解码器的特性，并融入针对不同模态的特定标记。此外，我们引入了领域知识锚定模块（DKAM），以利用大型模型的内在医学知识，缩小图像文本对齐中的类别语义差距。大量实验表明，我们的LLaVA-RadZ在零样本疾病识别方面显著优于传统MLLMs，取得了与建立良好且高度优化的CLIP方法相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07487v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型多模态语言模型（MLLMs）在视觉理解及跨各种视觉语言任务上展现了出色能力。但在传统视觉问答（VQA）流程中处理精细医学图像数据时，它们无法有效发挥功能，导致在零样本医疗疾病识别中表现不佳。然而，MLLMs具有巨大潜力来解决这类问题。为此，我们提出LLaVA-RadZ框架，利用现有MLLM特性进行零样本医疗疾病识别。通过设计名为解码侧特征对齐训练（DFAT）的端到端训练策略并引入领域知识锚定模块（DKAM），我们的方法显著提高了MLLM在零样本疾病识别中的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在视觉理解和跨视觉语言任务中表现出色，但在处理精细医学图像数据时存在局限性。</li>
<li>MLLMs在零样本医疗疾病识别中表现不佳，但具有解决此类问题的潜力。</li>
<li>提出LLaVA-RadZ框架，旨在利用MLLM特性进行零样本医疗疾病识别。</li>
<li>DFAT是一种端到端的训练策略，旨在利用MLLM解码器架构的特性并结合不同模态的特定标记。</li>
<li>DKAM模块用于挖掘大型模型的内在医学知识，缩小图像文本对齐中的类别语义差距。</li>
<li>实验表明，LLaVA-RadZ在零样本疾病识别中显著优于传统MLLMs，并与经过良好训练的CLIP方法性能相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07487">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.07487v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.07487v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.07487v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Transformer-Model-for-Predicting-Chemical-Products-from-Generic-SMARTS-Templates-with-Data-Augmentation"><a href="#A-Transformer-Model-for-Predicting-Chemical-Products-from-Generic-SMARTS-Templates-with-Data-Augmentation" class="headerlink" title="A Transformer Model for Predicting Chemical Products from Generic SMARTS   Templates with Data Augmentation"></a>A Transformer Model for Predicting Chemical Products from Generic SMARTS   Templates with Data Augmentation</h2><p><strong>Authors:Derin Ozer, Sylvain Lamprier, Thomas Cauchy, Nicolas Gutowski, Benoit Da Mota</strong></p>
<p>The accurate prediction of chemical reaction outcomes is a major challenge in computational chemistry. Current models rely heavily on either highly specific reaction templates or template-free methods, both of which present limitations. To address these, this work proposes the Broad Reaction Set (BRS), a set featuring 20 generic reaction templates written in SMARTS, a pattern-based notation designed to describe substructures and reactivity. Additionally, we introduce ProPreT5, a T5-based model specifically adapted for chemistry and, to the best of our knowledge, the first language model capable of directly handling and applying SMARTS reaction templates. To further improve generalization, we propose the first augmentation strategy for SMARTS, which injects structural diversity at the pattern level. Trained on augmented templates, ProPreT5 demonstrates strong predictive performance and generalization to unseen reactions. Together, these contributions provide a novel and practical alternative to current methods, advancing the field of template-based reaction prediction. </p>
<blockquote>
<p>化学反应结果的准确预测是计算化学领域的一大挑战。当前模型严重依赖于特定的反应模板或无模板方法，二者都存在局限性。为解决这一问题，本研究提出了广谱反应集（BRS），这是一组以SMARTS编写的20个通用反应模板，SMARTS是一种基于模式的符号，用于描述子结构和反应活性。此外，我们引入了基于T5模型的ProPreT5，特别是针对化学进行适配的模型，据我们所知，它是第一个能够直接处理和应用SMARTS反应模板的语言模型。为进一步提高泛化能力，我们提出了SMARTS的首个增强策略，该策略在模式层面注入了结构多样性。在增强模板的训练下，ProPreT5显示出强大的预测性能和泛化到未见过的反应的能力。总之，这些贡献为当前方法提供了新颖实用的替代方案，推动了基于模板的反应预测领域的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05810v3">PDF</a> ICTAI 2025</p>
<p><strong>Summary</strong></p>
<p>该文针对计算化学中化学反应结果预测的挑战，提出了Broad Reaction Set（BRS）和ProPreT5模型。BRS包含20个通用反应模板，采用SMARTS描述子来描述子结构和反应性。ProPreT5是基于T5模型的改进版，能直接处理和应用SMARTS反应模板，并在最佳知识情况下为首个模型。为进一步提高通用性，文章还提出了SMARTS的增强策略，即在模式级别注入结构多样性。经过训练的ProPreT5模型对未见过的反应表现出强大的预测性能和良好的泛化能力，为当前方法提供了新颖实用的替代方案，推动了基于模板的反应预测领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出了Broad Reaction Set（BRS），包含20个通用反应模板，采用SMARTS描述子描述子结构和反应性。</li>
<li>引入了ProPreT5模型，它是首个能直接处理和应用SMARTS反应模板的语言模型。</li>
<li>为了提高模型的泛化能力，文章提出了SMARTS的增强策略，即在模式级别注入结构多样性。</li>
<li>ProPreT5模型在训练过程中表现出了强大的预测性能。</li>
<li>ProPreT5模型能够很好地泛化到未见过的反应。</li>
<li>与当前方法相比，提出的模型和策略为计算化学中的反应预测提供了新颖且实用的替代方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.05810v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.05810v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.05810v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.05810v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer"><a href="#Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer" class="headerlink" title="Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer"></a>Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer</h2><p><strong>Authors:Yujiao Yang, Jing Lian, Linhui Li</strong></p>
<p>Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. Conventional mixture-of-experts (MoE) architectures suffer from suboptimal coordination dynamics, where isolated expert operations expose the model to overfitting risks. Moreover, they have not been effectively extended to attention blocks, which limits further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes the transformer model into an equivalent group of experts and applies a hierarchical routing mechanism to allocate input subspaces to specialized experts. Our approach advances MoE design with four key innovations: (1) Constructing expert groups by partitioning non-MoE models into functionally equivalent specialists (2) Developing a hierarchical routing paradigm that integrates patch-wise data selection and expert selection strategies. (3) Extending the MoE design to attention blocks. (4) Proposing a hardware-optimized parallelization scheme that exploits batched matrix multiplications for efficient expert computation. The experiments demonstrate that our UoE model surpasses Full Attention, state-of-the-art MoEs and efficient transformers in several tasks across image and natural language domains. In language modeling tasks, UoE achieves an average reduction of 2.38 in perplexity compared to the best-performing MoE method with only 76% of its FLOPs. In the Long Range Arena benchmark, it demonstrates an average score at least 0.68% higher than all comparison models, with only 50% of the FLOPs of the best MoE method. In image classification, it yields an average accuracy improvement of 1.75% over the best model while maintaining comparable FLOPs. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE">https://github.com/YujiaoYang-work/UoE</a>. </p>
<blockquote>
<p>混合专家（MoE）在保持计算效率的同时提高了模型性能，非常适合大规模应用。传统的混合专家（MoE）架构存在协调动力不佳的问题，孤立的专家操作会使模型面临过拟合风险。此外，它们尚未有效地扩展到注意力块，这限制了进一步的效率改进。为了解决这些问题，我们提出了“联合专家”（UoE）方法，它将变压器模型分解为等效的专家组，并应用分层路由机制将输入子空间分配给专业专家。我们的方法以四个关键创新点推进MoE设计：（1）通过将非MoE模型划分为功能等效的专家来构建专家组；（2）开发了一种结合补丁级数据选择和专家选择策略的分层路由范式。（3）将MoE设计扩展到注意力块。（4）提出了一种硬件优化并行化方案，利用批量矩阵乘法进行高效的专家计算。实验表明，我们的UoE模型在图像和自然语言领域的多个任务中超越了全注意力模型、最新MoE和高效变压器。在语言建模任务中，UoE与性能最佳的MoE方法相比，平均降低了2.38个困惑度，同时仅使用其76%的浮点运算（FLOPs）。在Long Range Arena基准测试中，它的平均得分至少比所有对比模型高0.68%，同时仅使用最佳MoE方法的50%的浮点运算。在图像分类方面，与最佳模型相比，它的平均准确率提高了1.75%，同时保持了相当的浮点运算量。源代码可在<a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YujiaoYang-work/UoE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02495v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Mixture-of-Experts（MoE）在大型应用中的性能提升和计算效率优势，但存在协调动态不佳和无法有效扩展到注意力块的问题。为此，提出了Union-of-Experts（UoE）模型，通过分解专家组、应用分层路由机制、扩展到注意力块以及硬件优化并行化方案等四个关键创新点来改进MoE设计。实验证明，UoE模型在图像和自然语言领域的多个任务中超越了全注意力模型和当前MoE模型，实现了更高的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mixture-of-Experts (MoE) 提升模型性能并保持计算效率，适用于大规模应用。</li>
<li>传统MoE架构存在协调动态问题，可能导致模型过拟合风险。</li>
<li>Union-of-Experts (UoE) 通过构建专家组、开发分层路由机制来解决这些问题。</li>
<li>UoE将MoE设计扩展到注意力块，进一步提高效率。</li>
<li>UoE模型通过硬件优化并行化方案实现高效专家计算。</li>
<li>UoE模型在多个任务中表现超越全注意力模型和当前MoE模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02495">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.02495v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.02495v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2503.02495v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="JUREX-4E-Juridical-Expert-Annotated-Four-Element-Knowledge-Base-for-Legal-Reasoning"><a href="#JUREX-4E-Juridical-Expert-Annotated-Four-Element-Knowledge-Base-for-Legal-Reasoning" class="headerlink" title="JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for   Legal Reasoning"></a>JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for   Legal Reasoning</h2><p><strong>Authors:Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng</strong></p>
<p>In recent years, Large Language Models (LLMs) have been widely applied to legal tasks. To enhance their understanding of legal texts and improve reasoning accuracy, a promising approach is to incorporate legal theories. One of the most widely adopted theories is the Four-Element Theory (FET), which defines the crime constitution through four elements: Subject, Object, Subjective Aspect, and Objective Aspect. While recent work has explored prompting LLMs to follow FET, our evaluation demonstrates that LLM-generated four-elements are often incomplete and less representative, limiting their effectiveness in legal reasoning. To address these issues, we present JUREX-4E, an expert-annotated four-element knowledge base covering 155 criminal charges. The annotations follow a progressive hierarchical framework grounded in legal source validity and incorporate diverse interpretive methods to ensure precision and authority. We evaluate JUREX-4E on the Similar Charge Disambiguation task and apply it to Legal Case Retrieval. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. The dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/THUlawtech/JUREX">https://github.com/THUlawtech/JUREX</a> </p>
<blockquote>
<p>近年来，大型语言模型（LLM）已广泛应用于法律任务。为了增强它们对法律文本的理解并提高推理准确性，融入法律理论是一种前景看好的方法。其中采用最广泛的理论之一是四要素理论（FET），它通过四个要素定义犯罪构成：主体、客体、主观方面和客观方面。虽然近期的工作已经探索了引导LLM遵循FET，但我们的评估表明，LLM生成的四要素往往不完整且代表性不足，限制了它们在法律推理中的有效性。为了解决这些问题，我们推出了JUREX-4E，这是一个专家注释的四要素知识库，涵盖155项刑事指控。注释遵循基于法律源合法性的递进分层框架，并融入多种解释方法，以确保精确性和权威性。我们在类似罪名辨析任务上评估了JUREX-4E，并将其应用于法律案例检索。实验结果验证了JUREX-4E的高质量及其对下游法律任务的重大影响，突显其在推动法律人工智能应用方面的潜力。数据集和代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/THUlawtech/JUREX">https://github.com/THUlawtech/JUREX</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17166v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近年来，大型语言模型（LLM）在法律任务中得到了广泛应用。为增强LLM对法律文本的理解和提高推理准确性，融入法律理论成为一种有前途的方法。其中，被广泛采纳的理论之一是四要素理论（FET），它通过主体、客体、主观方面和客观方面四个要素来定义犯罪构成。尽管近期研究尝试引导LLM遵循FET，但评估显示LLM生成四要素往往不完整且代表性不足，限制了其在法律推理中的有效性。为解决这些问题，我们提出了JUREX-4E，这是一个包含155项刑事指控的专家注释四要素知识库。注释遵循基于法律源合法性的分层框架，并融入多种解释方法以确保精确性和权威性。我们在类似罪名辨识任务上评估了JUREX-4E，并应用于法律案例检索。实验结果验证了JUREX-4E的高质量及其对下游法律任务的显著影响，突显其在推动法律人工智能应用方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）已广泛应用于法律任务。</li>
<li>融入法律理论，如四要素理论（FET），可提升LLMs对法律文本的理解和推理准确性。</li>
<li>尽管LLMs尝试遵循FET，但在生成四要素时常常存在不完整和代表性不足的问题。</li>
<li>提出JUREX-4E：一个包含155项刑事指控的专家注释四要素知识库。</li>
<li>JUREX-4E的注释遵循基于法律源合法性的分层框架，并结合多种解释方法以确保精确性和权威性。</li>
<li>JUREX-4E在类似罪名辨识任务上表现出高质量，并成功应用于法律案例检索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.17166v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.17166v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.17166v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.17166v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Does-Reasoning-Introduce-Bias-A-Study-of-Social-Bias-Evaluation-and-Mitigation-in-LLM-Reasoning"><a href="#Does-Reasoning-Introduce-Bias-A-Study-of-Social-Bias-Evaluation-and-Mitigation-in-LLM-Reasoning" class="headerlink" title="Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and   Mitigation in LLM Reasoning"></a>Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and   Mitigation in LLM Reasoning</h2><p><strong>Authors:Xuyang Wu, Jinming Nian, Ting-Ruen Wei, Zhiqiang Tao, Hsin-Tai Wu, Yi Fang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled automatic generation of chain-of-thought (CoT) reasoning, leading to strong performance on tasks such as math and code. However, when reasoning steps reflect social stereotypes (e.g., those related to gender, race or age), they can reinforce harmful associations and lead to misleading conclusions. We present the first systematic evaluation of social bias within LLM-generated reasoning, focusing on reasoning language models (e.g., DeepSeek-R1, OpenAI o1) that natively produce reasoning chains as part of their answers. Using the BBQ dataset, we analyze both prediction accuracy and reasoning bias across a broad spectrum of models, including instruction-tuned and CoT-augmented variants of DeepSeek-R1 (8B&#x2F;32B), ChatGPT, and other open-source LLMs. We quantify how biased reasoning steps correlate with incorrect predictions and often lead to stereotype expression. To mitigate reasoning-induced bias, we propose Answer Distribution as Bias Proxy (ADBP), a lightweight mitigation method that detects bias by tracking how model predictions change across incremental reasoning steps. ADBP outperforms Stereotype-free Reasoning Pattern (SfRP) baseline in most cases, mitigating bias and improving the accuracy of LLM outputs. Evaluation and mitigation code is available at <a target="_blank" rel="noopener" href="https://github.com/elviswxy/LLM_reasoning_bias">https://github.com/elviswxy/LLM_reasoning_bias</a>. </p>
<blockquote>
<p>最近大型语言模型（LLM）的进步已经能够实现自动生成思维链（CoT）推理，这在数学和代码等任务上取得了出色的表现。然而，当推理步骤反映出社会刻板印象（例如与性别、种族或年龄相关的刻板印象）时，它们会强化有害的关联并导致误导性的结论。我们首次对LLM生成推理中的社会偏见进行了系统评估，重点关注能够原生地产出推理链作为答案一部分的推理语言模型（例如DeepSeek-R1、OpenAI o1等）。我们使用BBQ数据集分析了广泛模型群的预测精度和推理偏见，包括DeepSeek-R1（8B&#x2F;32B）的指令调整和CoT增强版本、ChatGPT以及其他开源LLM。我们量化了带有偏见的推理步骤如何与错误预测相关联，并经常导致刻板印象的表达。为了缓解推理导致的偏见，我们提出了答案分布偏见代理（ADBP），这是一种轻量级的缓解方法，通过跟踪模型预测在增量推理步骤中的变化来检测偏见。在大多数情况下，ADBP的表现优于无刻板推理模式（SfRP），能够缓解偏见并提高了LLM输出的准确性。评估和缓解方法的代码可在<a target="_blank" rel="noopener" href="https://github.com/elviswxy/LLM_reasoning_bias%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/elviswxy/LLM_reasoning_bias找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15361v3">PDF</a> EMNLP Findings</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的最新进展已实现了自动生成思维链（CoT）推理，从而在数学和代码等任务上表现出强大的性能。然而，当推理步骤反映社会刻板印象（如性别、种族或年龄）时，它们会强化有害的关联并导致误导性的结论。本文对LLM生成推理中的社会偏见进行了首次系统评估，重点关注能够作为答案一部分生成推理链的推理语言模型（如DeepSeek-R1、OpenAI o1等）。我们使用BBQ数据集分析了包括DeepSeek-R1（8B&#x2F;32B）的指令调优和CoT增强版本、ChatGPT以及其他开源LLM的预测精度和推理偏见。我们量化了有偏见的推理步骤与错误预测之间的关联，以及它们如何常常导致刻板印象的表达。为了缓解推理导致的偏见，我们提出了答案分布偏见代理（ADBP）这一轻量级缓解方法，它通过跟踪模型预测在增量推理步骤中的变化来检测偏见。在大多数情况下，ADBP的表现优于无刻板推理模式（SfRP）基准测试，缓解偏见并提高了LLM输出的准确性。评估与缓解代码可通过<a target="_blank" rel="noopener" href="https://github.com/elviswxy/LLM_reasoning_bias%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/elviswxy/LLM_reasoning_bias获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）能自动生成思维链（CoT）推理，但在涉及社会刻板印象的推理步骤中可能强化有害的关联。</li>
<li>使用BBQ数据集对多种LLM的预测精度和推理偏见进行了系统评估。</li>
<li>有偏见的推理步骤常与错误预测相关，并可能导致刻板印象的表达。</li>
<li>提出了答案分布偏见代理（ADBP）方法，用于检测并缓解LLM中的推理偏见。</li>
<li>ADBP在大多数情况下表现优于现有的无刻板推理模式（SfRP）方法。</li>
<li>通过跟踪模型预测在增量推理步骤中的变化，ADBP提高了LLM输出的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15361">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.15361v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.15361v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.15361v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.15361v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DeepResonance-Enhancing-Multimodal-Music-Understanding-via-Music-centric-Multi-way-Instruction-Tuning"><a href="#DeepResonance-Enhancing-Multimodal-Music-Understanding-via-Music-centric-Multi-way-Instruction-Tuning" class="headerlink" title="DeepResonance: Enhancing Multimodal Music Understanding via   Music-centric Multi-way Instruction Tuning"></a>DeepResonance: Enhancing Multimodal Music Understanding via   Music-centric Multi-way Instruction Tuning</h2><p><strong>Authors:Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji</strong></p>
<p>Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the model’s ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-LLM fusion Transformer to enhance modality fusion prior to input into text LLMs, tailoring for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We open-source the codes, models and datasets we constructed: github.com&#x2F;sony&#x2F;DeepResonance. </p>
<blockquote>
<p>音乐大型语言模型（LLM）的最新进展极大地提高了音乐理解任务的能力，这些任务涉及模型分析和解释各种音乐元素的能力。这些改进主要集中在整合音乐和文本输入上。然而，结合图像、视频和文本音乐特征等额外模态以增强音乐理解的潜力尚未被探索。为了填补这一空白，我们提出了DeepResonance，这是一种多模态音乐理解LLM，通过多向指令调整与多向对齐的音乐、文本、图像和视频数据进行微调。为此，我们构建了Music4way-MI2T、Music4way-MV2T和Music4way-Any2T三个数据集，这三个数据集是为了让DeepResonance能够整合视觉和文本音乐特征内容而设计的4向训练和评估数据集。我们还引入了多采样ImageBind嵌入和预LLM融合Transformer，以增强模态融合，然后输入文本LLM，为多角度指令调整量身定制。我们的模型在六个音乐理解任务上达到了最先进的性能，凸显了辅助模态的益处和DeepResonance的结构优势。我们公开了我们构建的源代码、模型和数据集：github.com&#x2F;sony&#x2F;DeepResonance。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12623v3">PDF</a> Accepted to EMNLP 2025 main conference</p>
<p><strong>Summary</strong><br>音乐大型语言模型（LLM）的最新进展显著提高了音乐理解任务的能力，包括分析和解释各种音乐元素。研究集中在整合音乐和文本输入，但融入图像、视频和文本音乐特征等额外模态的潜力尚未被探索。为此，提出DeepResonance多模态音乐理解LLM，通过多向指令调整与多向对齐的音乐、文本、图像和视频数据微调。为此构建了Music4way-MI2T、Music4way-MV2T和Music4way-Any2T三个4向训练与评估数据集，使DeepResonance能够融合视觉和文本音乐特征内容。还引入了多采样ImageBind嵌入和预LLM融合Transformer，以加强模态融合，然后输入文本LLM中，为多方指令调整量身定制。该模型在六项音乐理解任务上取得了最先进的性能，突显了辅助模态的益处和DeepResonance的结构优越性。我们公开了构建的源代码、模型和数据集：github.com&#x2F;sony&#x2F;DeepResonance。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音乐LLM最新进展提高了音乐理解任务的能力，包括分析和解释音乐元素。</li>
<li>研究集中在整合音乐和文本输入，但额外模态（如图像、视频和文本音乐特征）的潜力未被充分探索。</li>
<li>提出DeepResonance多模态音乐理解LLM，通过多向指令调整与多模态数据微调。</li>
<li>构建了三个4向训练与评估数据集，以融合视觉和文本音乐特征内容。</li>
<li>引入多采样ImageBind嵌入和预LLM融合Transformer以加强模态融合。</li>
<li>DeepResonance在多项音乐理解任务上取得最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12623">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.12623v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.12623v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.12623v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.12623v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.12623v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators"><a href="#Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators" class="headerlink" title="Flatten Graphs as Sequences: Transformers are Scalable Graph Generators"></a>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</h2><p><strong>Authors:Dexiong Chen, Markus Krimmel, Karsten Borgwardt</strong></p>
<p>We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/AutoGraph">https://github.com/BorgwardtLab/AutoGraph</a>. </p>
<blockquote>
<p>我们介绍了AutoGraph，这是一个利用仅解码器转换器进行属性图生成的可扩展自回归模型。通过可逆过程将图展平为随机令牌序列，AutoGraph能够建模图序列，而无需依赖计算成本高昂的附加节点特征，这与基于扩散的方法形成对比。这导致采样复杂性和序列长度与边的数量呈最优线性关系，对于大型稀疏图而言，它具有可扩展性和效率。AutoGraph的一个关键成功因素是它的序列前缀代表诱导子图，与语言建模中的句子直接相关。在合成和分子基准测试中，AutoGraph实现了最先进的性能，生成速度比领先的扩散模型快100倍，训练速度快3倍。它还支持子结构条件下的生成而无需微调，并显示出有希望的迁移性，在建立图形基础模型方面奠定了语言建模和图生成之间的桥梁。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/AutoGraph%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BorgwardtLab/AutoGraph找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02216v2">PDF</a> To appear at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>AutoGraph是一个利用解码器为主的Transformer进行属性图生成的规模化自回归模型。它通过可逆过程将图展平为随机令牌序列，无需依赖昂贵的节点特征计算，不同于基于扩散的方法。因此，其采样复杂性和序列长度与边的数量呈最优线性关系，适用于大规模稀疏图的建模。AutoGraph的关键成功因素在于其序列前缀表示诱导子图，与语言建模中的子句建立直接联系。实证显示，AutoGraph在合成和分子基准测试中达到业界领先水平，生成速度最快达领先扩散模型的100倍，训练速度最快达3倍。此外，它支持子结构条件下的生成无需微调，显示出良好的可迁移性，为图基础模型的建立奠定了语言建模和图生成之间的桥梁。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/AutoGraph%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BorgwardtLab/AutoGraph上找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoGraph是一个自回归模型，用于属性图生成。</li>
<li>通过将图展平为令牌序列，实现了图的序列建模。</li>
<li>该模型不需要依赖昂贵的节点特征计算。</li>
<li>采样复杂性和序列长度与边的数量呈线性关系，适合大规模稀疏图。</li>
<li>AutoGraph序列前缀代表诱导子图，与语言建模中的子句有直接联系。</li>
<li>在合成和分子基准测试中达到业界领先水平，生成和训练速度均优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02216">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.02216v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2502.02216v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Biology-Instructions-A-Dataset-and-Benchmark-for-Multi-Omics-Sequence-Understanding-Capability-of-Large-Language-Models"><a href="#Biology-Instructions-A-Dataset-and-Benchmark-for-Multi-Omics-Sequence-Understanding-Capability-of-Large-Language-Models" class="headerlink" title="Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence   Understanding Capability of Large Language Models"></a>Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence   Understanding Capability of Large Language Models</h2><p><strong>Authors:Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye</strong></p>
<p>Large language models (LLMs) have shown remarkable capabilities in general domains, but their application to multi-omics biology remains underexplored. To address this gap, we introduce Biology-Instructions, the first large-scale instruction-tuning dataset for multi-omics biological sequences, including DNA, RNA, proteins, and multi-molecules. This dataset bridges LLMs and complex biological sequence-related tasks, enhancing their versatility and reasoning while maintaining conversational fluency. We also highlight significant limitations of current state-of-the-art LLMs on multi-omics tasks without specialized training. To overcome this, we propose ChatMultiOmics, a strong baseline with a novel three-stage training pipeline, demonstrating superior biological understanding through Biology-Instructions. Both resources are publicly available, paving the way for better integration of LLMs in multi-omics analysis. The Biology-Instructions is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/hhnqqq/Biology-Instructions">https://github.com/hhnqqq/Biology-Instructions</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在通用领域表现出了显著的能力，但它们在多组学生物学中的应用仍然被探索得不够深入。为了弥补这一空白，我们引入了生物学指令数据集（Biology-Instructions），这是首个针对多组学生物序列的大规模指令调整数据集，包括DNA、RNA、蛋白质和多种分子。该数据集将LLM与复杂的生物序列相关任务联系起来，增强了它们的通用性和推理能力，同时保持了对话的流畅性。我们还强调了当前最先进的LLM在多组学任务上存在的重大局限性，没有经过专门的训练。为了克服这一问题，我们提出了ChatMultiOmics这一强大的基线模型，它采用了新型的三阶段训练管道，通过生物学指令展示了卓越的生物理解力。这两种资源均可公开访问，为多组学分析中LLM的更好集成铺平了道路。生物学指令公开可用的网址为：<a target="_blank" rel="noopener" href="https://github.com/hhnqqq/Biology-Instructions">https://github.com/hhnqqq/Biology-Instructions</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19191v2">PDF</a> EMNLP 2025 findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多领域表现卓越，但在多组学生物学领域的应用仍然有待探索。为解决此空白，我们推出生物学指令数据集，这是首个针对多组学生物序列的大型指令调优数据集，涵盖DNA、RNA、蛋白质和多分子。该数据集强化了LLM与复杂生物序列相关任务的联系，提高了其通用性和推理能力，同时保持对话流畅性。我们还指出了当前最先进LLM在多组学任务上的重要局限性，并提出通过ChatMultiOmics这一强大的基线模型和新型三阶段训练管道来克服这些局限性，通过生物学指令展示出色的生物学理解力。两个资源均公开可用，为多组学分析中LLM的更好集成铺平了道路。生物学指令数据集可在<a target="_blank" rel="noopener" href="https://github.com/hhnqqq/Biology-Instructions%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hhnqqq/Biology-Instructions获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在多组学生物学领域的应用尚待探索。</li>
<li>生物学指令数据集是首个针对多组学生物序列的大型指令调优数据集。</li>
<li>该数据集提高了LLM处理复杂生物序列任务的能力，并保持了对话的流畅性。</li>
<li>当前LLM在多组学任务上存在局限性。</li>
<li>ChatMultiOmics是一个强大的基线模型，通过新型三阶段训练管道提高LLM在生物学领域的理解力。</li>
<li>生物学指令数据集和ChatMultiOmics都为多组学分析中LLM的集成提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19191">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2412.19191v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2412.19191v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2412.19191v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2412.19191v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Bias-Similarity-Measurement-A-Black-Box-Audit-of-Fairness-Across-LLMs"><a href="#Bias-Similarity-Measurement-A-Black-Box-Audit-of-Fairness-Across-LLMs" class="headerlink" title="Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs"></a>Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs</h2><p><strong>Authors:Hyejun Jeong, Shiqing Ma, Amir Houmansadr</strong></p>
<p>Large Language Models (LLMs) reproduce social biases, yet prevailing evaluations score models in isolation, obscuring how biases persist across families and releases. We introduce Bias Similarity Measurement (BSM), which treats fairness as a relational property between models, unifying scalar, distributional, behavioral, and representational signals into a single similarity space. Evaluating 30 LLMs on 1M+ prompts, we find that instruction tuning primarily enforces abstention rather than altering internal representations; small models gain little accuracy and can become less fair under forced choice; and open-weight models can match or exceed proprietary systems. Family signatures diverge: Gemma favors refusal, LLaMA 3.1 approaches neutrality with fewer refusals, and converges toward abstention-heavy behavior overall. Counterintuitively, Gemma 3 Instruct matches GPT-4-level fairness at far lower cost, whereas Gemini’s heavy abstention suppresses utility. Beyond these findings, BSM offers an auditing workflow for procurement, regression testing, and lineage screening, and extends naturally to code and multilingual settings. Our results reframe fairness not as isolated scores but as comparative bias similarity, enabling systematic auditing of LLM ecosystems. Code available at <a target="_blank" rel="noopener" href="https://github.com/HyejunJeong/bias_llm">https://github.com/HyejunJeong/bias_llm</a>. </p>
<blockquote>
<p>大型语言模型（LLM）会复制社会偏见，然而现行的评估方法都是孤立地评估模型，这掩盖了偏见如何在不同家族和版本之间持续存在。我们引入了偏见相似性度量（BSM），它将公平性视为模型之间的关联属性，将标量、分布、行为和表示信号统一到一个单一的相似度空间中。我们对30个LLM进行了超过100万条提示的评估，发现指令微调主要强制执行回避而不是改变内部表示；小型模型在强制选择下的准确率几乎没有提高，而且可能会变得不那么公平；而开放式权重模型可以匹配甚至超过专有系统。不同家族的特性也有所不同：Gemma更倾向于拒绝，LLaMA 3.1趋向于中性但拒绝较少，总体而言趋向于避免过度的拒绝行为。有些反直觉的是，Gemma 3 Instruct能够以更低的成本达到GPT-4级别的公平性，而Gemini的过度回避行为抑制了实用性。除了这些发现之外，BSM还提供了采购、回归测试和谱系筛查的审计工作流程，并自然地扩展到代码和多语言环境中。我们的研究结果重新定义了公平性的概念，不是孤立的分数，而是比较偏见相似性，能够系统地审计LLM生态系统。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/HyejunJeong/bias_llm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HyejunJeong/bias_llm找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12010v4">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/HyejunJeong/bias_llm">https://github.com/HyejunJeong/bias_llm</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）存在社会偏见复制问题，但现有的评估方法通常孤立地评估模型，导致难以发现偏见如何在不同的家族和版本之间持续存在。本文提出了偏见相似性度量（BSM）方法，将公平性视为模型之间的关系属性，将标量、分布、行为和表示信号统一到一个相似性空间中。通过对30个LLM模型进行超过100万个提示进行评估，发现指令微调主要强制执行回避而非改变内部表示；小型模型的准确性提高不多，且在强制选择下可能变得更不公平；而开放权重模型可以达到或超过专有系统的水平。不同模型家族表现出不同的特点，如Gemma倾向于拒绝，LLaMA 3.1趋于中立且拒绝较少，整体表现出倾向于回避的行为。此外，BSM方法为采购、回归测试和谱系筛选提供了审计工作流程，并自然地扩展到代码和多语言环境中。本研究结果重新定义了公平性，将其视为比较性的偏见相似性，使LLM生态系统的系统性审计成为可能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）存在社会偏见复制问题。</li>
<li>现有的评估方法孤立地评估模型，导致难以发现偏见在不同家族和版本间的持续存在。</li>
<li>提出了偏见相似性度量（BSM）方法，将公平性视为模型间的关系属性。</li>
<li>指令微调主要强制执行回避而非改变内部表示。</li>
<li>不同LLM模型家族展现出不同的偏见特点。</li>
<li>BSM方法提供了审计工作流程，适用于采购、回归测试和谱系筛选。</li>
<li>本研究重新定义了公平性为比较性的偏见相似性，使LLM生态系统的系统性审计成为可能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12010">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2410.12010v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2410.12010v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2410.12010v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_LLM/2410.12010v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_Agent/2502.20073v3/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-29  Collab-Overcooked Benchmarking and Evaluating Large Language Models as   Collaborative Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-29\./crop_R1_Reasoning/2509.04027v2/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-29  Enrich-on-Graph Query-Graph Alignment for Complex Reasoning with LLM   Enriching
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
