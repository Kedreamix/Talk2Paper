<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Assessing Classical Machine Learning and Transformer-based Approaches   for Detecting AI-Generated Research Text">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2031e9b03c7e460b5852aa53525c223f')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-29-æ›´æ–°"><a href="#2025-09-29-æ›´æ–°" class="headerlink" title="2025-09-29 æ›´æ–°"></a>2025-09-29 æ›´æ–°</h1><h2 id="Assessing-Classical-Machine-Learning-and-Transformer-based-Approaches-for-Detecting-AI-Generated-Research-Text"><a href="#Assessing-Classical-Machine-Learning-and-Transformer-based-Approaches-for-Detecting-AI-Generated-Research-Text" class="headerlink" title="Assessing Classical Machine Learning and Transformer-based Approaches   for Detecting AI-Generated Research Text"></a>Assessing Classical Machine Learning and Transformer-based Approaches   for Detecting AI-Generated Research Text</h2><p><strong>Authors:Sharanya Parimanoharan, Ruwan D. Nawarathna</strong></p>
<p>The rapid adoption of large language models (LLMs) such as ChatGPT has blurred the line between human and AI-generated texts, raising urgent questions about academic integrity, intellectual property, and the spread of misinformation. Thus, reliable AI-text detection is needed for fair assessment to safeguard human authenticity and cultivate trust in digital communication. In this study, we investigate how well current machine learning (ML) approaches can distinguish ChatGPT-3.5-generated texts from human-written texts employing a labeled data set of 250 pairs of abstracts from a wide range of research topics. We test and compare both classical (Logistic Regression armed with classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier, and LSTM-based N-gram models) ML detection techniques. As we aim to assess each modelâ€™s performance in detecting AI-generated research texts, we also aim to test whether an ensemble of these models can outperform any single detector. Results show DistilBERT achieves the overall best performance, while Logistic Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and BERT-N-gram approaches lag. The max voting ensemble of the three best models fails to surpass DistilBERT itself, highlighting the primacy of a single transformer-based representation over mere model diversity. By comprehensively assessing the strengths and weaknesses of these AI-text detection approaches, this work lays a foundation for more robust transformer frameworks with larger, richer datasets to keep pace with ever-improving generative AI models. </p>
<blockquote>
<p>éšç€ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿåº”ç”¨ï¼Œäººå·¥æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬ä¸äººç±»æ–‡æœ¬ä¹‹é—´çš„ç•Œé™å˜å¾—æ¨¡ç³Šï¼Œå¼•å‘äº†å…³äºå­¦æœ¯è¯šä¿¡ã€çŸ¥è¯†äº§æƒå’Œè¯¯ä¿¡æ¯ä¼ æ’­ç­‰çš„ç´§è¿«é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦è¿›è¡Œå¯é çš„AIæ–‡æœ¬æ£€æµ‹ä»¥è¿›è¡Œå…¬å¹³è¯„ä¼°ï¼Œä¿éšœäººç±»çœŸå®æ€§å’Œæ•°å­—é€šä¿¡ä¸­çš„ä¿¡ä»»ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å½“å‰æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•å¦‚ä½•åŒºåˆ†ChatGPT 3.5ç”Ÿæˆçš„æ–‡æœ¬å’Œäººç±»æ’°å†™çš„æ–‡æœ¬ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç»„åŒ…å«å¹¿æ³›ç ”ç©¶ä¸»é¢˜çš„250å¯¹æ‘˜è¦çš„æ ‡è®°æ•°æ®é›†è¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬æµ‹è¯•å’Œæ¯”è¾ƒäº†ç»å…¸ï¼ˆé€»è¾‘å›å½’é…å¤‡ç»å…¸è¯è¢‹ã€POSå’ŒTF-IDFç‰¹å¾ï¼‰å’ŒåŸºäºTransformerçš„æœºå™¨å­¦ä¹ æ£€æµ‹æŠ€æœ¯çš„æ€§èƒ½è¡¨ç°ã€‚é‰´äºæˆ‘ä»¬çš„ç›®æ ‡æ—¨åœ¨è¯„ä¼°æ¯ä¸ªæ¨¡å‹æ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆç ”ç©¶æ–‡æœ¬çš„æ€§èƒ½è¡¨ç°ï¼Œæˆ‘ä»¬è¿˜æµ‹è¯•äº†è¿™äº›æ¨¡å‹çš„ç»„åˆæ˜¯å¦èƒ½è¶…è¶Šä»»ä½•å•ä¸€æ£€æµ‹å™¨ã€‚ç»“æœè¡¨æ˜ï¼ŒDistilBERTå–å¾—äº†æ€»ä½“æœ€ä½³æ€§èƒ½è¡¨ç°ï¼Œè€Œé€»è¾‘å›å½’å’ŒBERTå®šåˆ¶åˆ™æä¾›äº†ç¨³å›ºã€å¹³è¡¡çš„æ›¿ä»£æ–¹æ¡ˆï¼›LSTMå’ŒBERT-N-gramæ–¹æ³•è¡¨ç°æ»åã€‚è¿™ä¸‰ä¸ªæœ€ä½³æ¨¡å‹çš„æŠ•ç¥¨ç»„åˆæœªèƒ½è¶…è¶ŠDistilBERTæœ¬èº«ï¼Œè¿™çªæ˜¾äº†å•ä¸€åŸºäºTransformerçš„è¡¨ç¤ºå½¢å¼çš„é‡è¦æ€§ï¼Œè€Œéå•çº¯çš„æ¨¡å‹å¤šæ ·æ€§ã€‚é€šè¿‡å…¨é¢è¯„ä¼°è¿™äº›AIæ–‡æœ¬æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œæœ¬å·¥ä½œä¸ºä½¿ç”¨æ›´å¤§ã€æ›´ä¸°å¯Œçš„æ•°æ®é›†æ„å»ºæ›´å¼ºå¤§çš„Transformeræ¡†æ¶å¥ å®šäº†åŸºç¡€ï¼Œä»¥ä¾¿ä¸ä¸æ–­æ”¹è¿›ç”Ÿæˆå¼AIæ¨¡å‹ä¿æŒåŒæ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20375v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€ChatGPTç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿé‡‡çº³ï¼ŒAIç”Ÿæˆæ–‡æœ¬ä¸äººç±»æ–‡æœ¬ä¹‹é—´çš„ç•Œé™å˜å¾—æ¨¡ç³Šï¼Œå¯¹å­¦æœ¯è¯šä¿¡ã€çŸ¥è¯†äº§æƒåŠè¯¯è§£ä¼ æ’­ç­‰æ–¹é¢æå‡ºç´§è¿«é—®é¢˜ã€‚å› æ­¤ï¼Œä¸ºäº†å…¬æ­£è¯„ä¼°å’Œä¿éšœäººç±»çœŸå®æ€§ä»¥åŠåŸ¹å…»æ•°å­—é€šä¿¡ä¸­çš„ä¿¡ä»»ï¼Œå¯é çš„AIæ–‡æœ¬æ£€æµ‹è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥å½“å‰æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•å¦‚ä½•åŒºåˆ†ChatGPT 3.5ç”Ÿæˆçš„æ–‡æœ¬å’Œäººç±»æ’°å†™çš„æ–‡æœ¬ã€‚æˆ‘ä»¬åˆ©ç”¨ä¸€ç»„åŒ…å«å¹¿æ³›ç ”ç©¶ä¸»é¢˜çš„250å¯¹æ‘˜è¦çš„æ ‡è®°æ•°æ®é›†è¿›è¡Œæµ‹è¯•å’Œæ¯”è¾ƒã€‚æ–¹æ³•åŒ…æ‹¬ç»å…¸æ–¹æ³•ï¼ˆå¦‚ä½¿ç”¨ä¼ ç»Ÿè¯è¢‹æ¨¡å‹ã€POSå’ŒTF-IDFç‰¹å¾çš„é€»è¾‘å›å½’ï¼‰å’ŒåŸºäºè½¬æ¢å™¨çš„æ–¹æ³•ï¼ˆå¦‚å¢å¼ºBERTçš„Nå…ƒç‰¹å¾ã€è’¸é¦BERTã€å¸¦æœ‰è½»é‡çº§è‡ªå®šä¹‰åˆ†ç±»å™¨çš„BERTå’ŒåŸºäºLSTMçš„Nå…ƒæ¨¡å‹ï¼‰ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°æ¯ç§æ¨¡å‹æ£€æµ‹AIç”Ÿæˆç ”ç©¶æ–‡æœ¬çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿæµ‹è¯•äº†æ¨¡å‹ç»„åˆæ˜¯å¦èƒ½è¶…è¶Šå•ä¸€æ£€æµ‹å™¨ã€‚ç»“æœæ˜¾ç¤ºï¼Œè’¸é¦BERTåœ¨æ€»ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œé€»è¾‘å›å½’å’Œè‡ªå®šä¹‰BERTæä¾›äº†ç¨³å¥ä¸”å¹³è¡¡çš„æ›¿ä»£æ–¹æ¡ˆï¼›LSTMå’ŒBERTçš„Nå…ƒæ–¹æ³•è¡¨ç°è¾ƒå¼±ã€‚ä¸‰ä¸ªæœ€ä½³æ¨¡å‹çš„æŠ•ç¥¨ç»„åˆæœªèƒ½è¶…è¶Šè’¸é¦BERTæœ¬èº«ï¼Œçªæ˜¾å‡ºå•ä¸€åŸºäºè½¬æ¢å™¨çš„è¡¨ç¤ºå­¦ä¹ çš„é‡è¦æ€§ï¼Œè€Œéå•çº¯çš„æ¨¡å‹å¤šæ ·æ€§ã€‚æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†è¿™äº›AIæ–‡æœ¬æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºä½¿ç”¨æ›´å¤§ã€æ›´ä¸°å¯Œæ•°æ®é›†çš„æ›´ç¨³å¥çš„è½¬æ¢å™¨æ¡†æ¶å¥ å®šäº†åŸºçŸ³ï¼Œä»¥è·Ÿä¸Šä¸æ–­è¿›æ­¥çš„ç”Ÿæˆå¼AIæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTçš„æ™®åŠå¼•å‘äº†å…³äºå­¦æœ¯è¯šä¿¡ã€çŸ¥è¯†äº§æƒå’Œè¯¯è§£ä¼ æ’­çš„å…³åˆ‡ã€‚</li>
<li>å½“å‰æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨åŒºåˆ†ChatGPT 3.5ç”Ÿæˆçš„æ–‡æœ¬å’Œäººç±»æ’°å†™çš„æ–‡æœ¬æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬ç»å…¸æ–¹æ³•å’ŒåŸºäºè½¬æ¢å™¨çš„æŠ€æœ¯ã€‚</li>
<li>åœ¨æ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬æ–¹é¢ï¼Œè’¸é¦BERTè¡¨ç°æœ€ä½³ï¼Œé€»è¾‘å›å½’å’Œè‡ªå®šä¹‰BERTä¸ºå¯é æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åŸºäºLSTMå’ŒBERTçš„Nå…ƒæ–¹æ³•æ€§èƒ½è¾ƒå¼±ï¼Œæ˜¾ç¤ºæ¨¡å‹å¤šæ ·æ€§çš„ä¼˜åŠ¿å¹¶éç»å¯¹ã€‚</li>
<li>æ¨¡å‹ç»„åˆå°è¯•æœªèƒ½è¶…è¶Šæœ€ä½³å•ä¸€æ¨¡å‹è¡¨ç°ï¼Œå¼ºè°ƒå•ä¸€åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da4b7775c981c9ab91ec0a390c26ee6a" align="middle">
<img src="https://picx.zhimg.com/v2-fb4fdc45efd51d1af7a08098376aca65" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ButterflyQuant-Ultra-low-bit-LLM-Quantization-through-Learnable-Orthogonal-Butterfly-Transforms"><a href="#ButterflyQuant-Ultra-low-bit-LLM-Quantization-through-Learnable-Orthogonal-Butterfly-Transforms" class="headerlink" title="ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable   Orthogonal Butterfly Transforms"></a>ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable   Orthogonal Butterfly Transforms</h2><p><strong>Authors:Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang</strong></p>
<p>Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} &#x3D; \mathbf{Wx} &#x3D; (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transformsâ€“Hadamard matrices achieving optimal worst-case coherence $\mu &#x3D; 1&#x2F;\sqrt{n}$â€“that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamardâ€™s discrete ${+1, -1}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transformsâ€™ continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPUâ€“a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \href{<a target="_blank" rel="noopener" href="https://github.com/42Shawn/Butterflyquant-llm%7D%7BCodes%7D">https://github.com/42Shawn/Butterflyquant-llm}{Codes}</a> are available. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„å†…å­˜ç©ºé—´ï¼Œä¸¥é‡é™åˆ¶äº†å…¶åœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²ã€‚é‡åŒ–é€šè¿‡é™ä½æ•°å€¼ç²¾åº¦æ¥å‡å°‘å†…å­˜ï¼Œä½†æç«¯çš„2ä½é‡åŒ–ç”±äºæ¿€æ´»å€¼çš„å¼‚å¸¸å€¼è€Œé­å—ä¸¥é‡çš„æ€§èƒ½æŸå¤±ã€‚åŸºäºæ—‹è½¬çš„æ–¹æ³•ï¼Œå¦‚QuIPå’ŒQuaRotï¼Œé€šè¿‡å¯¹æ­£äº¤å˜æ¢åº”ç”¨æ¥è®¡ç®—ä¸å˜æ€§ï¼Œä»¥æ¶ˆé™¤å¼‚å¸¸å€¼ï¼Œå†è¿›è¡Œé‡åŒ–ï¼š$\mathbf{y} &#x3D; \mathbf{Wx} &#x3D; (\mathbf{WQ}^T)(\mathbf{Qx})$ï¼Œå…¶ä¸­$\mathbf{Q}$ä¸ºæ­£äº¤çŸ©é˜µã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨å›ºå®šçš„å˜æ¢â€”â€”å“ˆè¾¾ç›çŸ©é˜µï¼Œå®ç°æœ€ä¼˜æœ€åçš„ç›¸å¹²æ€§$\mu &#x3D; 1&#x2F;\sqrt{n}$ï¼Œæ— æ³•é€‚åº”ç‰¹å®šçš„æƒé‡åˆ†å¸ƒã€‚æˆ‘ä»¬å‘ç°ä¸åŒçš„è½¬æ¢å™¨å±‚è¡¨ç°å‡ºä¸åŒçš„å¼‚å¸¸å€¼æ¨¡å¼ï¼Œå› æ­¤æå€¡é‡‡ç”¨åˆ†å±‚è‡ªé€‚åº”æ—‹è½¬è€Œä¸æ˜¯ä¸€åˆ€åˆ‡çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ButterflyQuantï¼Œå®ƒç”¨å¯å­¦ä¹ çš„è´è¶å˜æ¢æ¥æ›¿ä»£å“ˆè¾¾ç›æ—‹è½¬ï¼Œç”±è¿ç»­çš„å‰æ–‡æ–¯æ—‹è½¬è§’è¿›è¡Œå‚æ•°åŒ–ã€‚ä¸å“ˆè¾¾ç›çš„ç¦»æ•£${+1, -1}$æ¡ç›®ä¸åŒï¼Œåè€…æ˜¯ä¸å¯å¾®åˆ†çš„ï¼Œå› æ­¤ç¦æ­¢åŸºäºæ¢¯åº¦çš„å­¦ä¹ ï¼Œè´è¶å˜æ¢çš„è¿ç»­å‚æ•°åŒ–å¯ä»¥åœ¨ä¿æŒæ­£äº¤æ€§çš„åŒæ—¶å®ç°å¹³æ»‘ä¼˜åŒ–ã€‚è¿™ç§æ­£äº¤çº¦æŸç¡®ä¿äº†å¼‚å¸¸å€¼æŠ‘åˆ¶çš„ç†è®ºä¿è¯ï¼ŒåŒæ—¶å®ç°äº†$O(n \log n)$çš„è®¡ç®—å¤æ‚æ€§ï¼Œåªæœ‰$\frac{n \log n}{2}$ä¸ªå¯å­¦ä¹ å‚æ•°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹åå˜æ¢æ¿€æ´»å€¼å¼•å…¥å‡åŒ€æ€§æ­£åˆ™åŒ–ï¼Œä»¥ä¿ƒè¿›æ›´é€‚åˆé‡åŒ–çš„å¹³æ»‘åˆ†å¸ƒã€‚å­¦ä¹ åªéœ€è¦128ä¸ªæ ¡å‡†æ ·æœ¬ï¼Œå¹¶åœ¨å•ä¸ªGPUä¸Šå‡ åˆ†é’Ÿå†…æ”¶æ•›â€”â€”è¿™æ˜¯ä¸€æ¬¡å¾®ä¸è¶³é“çš„æˆæœ¬ã€‚å¯¹äºä½¿ç”¨2ä½é‡åŒ–çš„LLaMA-2-7Bï¼ŒButterflyQuantå®ç°äº†15.4çš„å›°æƒ‘åº¦ï¼Œè€ŒQuIPä¸º37.3ã€‚<a target="_blank" rel="noopener" href="https://github.com/42Shawn/Butterflyquant-llm">ä»£ç </a>å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09679v2">PDF</a> Replace discrete Hadamard transforms with continuous Butterfly   transforms to facilitate the learning of rotation matrices in LLM   quantization</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡å†…å­˜ï¼Œé™åˆ¶äº†å…¶åœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²ã€‚é‡åŒ–é€šè¿‡é™ä½æ•°å€¼ç²¾åº¦æ¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†æç«¯2ä½é‡åŒ–ä¼šå¯¼è‡´æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚æ—‹è½¬æ–¹æ³•å¦‚QuIPå’ŒQuaRotåº”ç”¨æ­£äº¤å˜æ¢æ¶ˆé™¤å¼‚å¸¸å€¼ã€‚æœ¬æ–‡æå‡ºButterflyQuantï¼Œç”¨å¯å­¦ä¹ çš„è´è¶å˜æ¢ä»£æ›¿Hadamardæ—‹è½¬ï¼Œå®ç°å¹³æ»‘ä¼˜åŒ–å¹¶ä¿éšœæ­£äº¤æ€§ï¼Œä»è€Œæé«˜é‡åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²å—é™äºå†…å­˜éœ€æ±‚ã€‚</li>
<li>é‡åŒ–æ˜¯é™ä½å†…å­˜ä½¿ç”¨çš„ä¸€ç§æ–¹æ³•ï¼Œä½†æç«¯é‡åŒ–å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸¥é‡æŸå¤±ã€‚</li>
<li>æ—‹è½¬æ–¹æ³•å¦‚QuIPå’ŒQuaRoté€šè¿‡æ­£äº¤å˜æ¢æ¶ˆé™¤å¼‚å¸¸å€¼ã€‚</li>
<li>ButterflyQuantä½¿ç”¨å¯å­¦ä¹ çš„è´è¶å˜æ¢ï¼Œå®ç°å¹³æ»‘ä¼˜åŒ–å¹¶ä¿éšœæ­£äº¤æ€§ã€‚</li>
<li>ButterflyQuantæ–¹æ³•å®ç°äº†ç†è®ºä¸Šçš„å¼‚å¸¸å€¼æŠ‘åˆ¶ï¼ŒåŒæ—¶å…·æœ‰O(n log n)çš„è®¡ç®—å¤æ‚åº¦å’Œæœ‰æ•ˆçš„å‚æ•°å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¼•å…¥å‡åŒ€æ€§æ­£åˆ™åŒ–ï¼Œä¿ƒè¿›æ›´å¹³æ»‘çš„åˆ†å¸ƒé€‚åˆé‡åŒ–ã€‚</li>
<li>ButterflyQuantæ–¹æ³•çš„å­¦ä¹ æˆæœ¬è¾ƒä½ï¼Œåªéœ€è¦å°‘é‡çš„æ ¡å‡†æ ·æœ¬ï¼Œå¹¶åœ¨å•ä¸ªGPUä¸Šå¿«é€Ÿæ”¶æ•›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78bfd0e527f585131a74aa8db4509393" align="middle">
<img src="https://picx.zhimg.com/v2-c2cf34692e18e9a87e0d6eb390eae678" align="middle">
<img src="https://picx.zhimg.com/v2-618f8471c2c11fad1b99d7f14db5aacf" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LM-Searcher-Cross-domain-Neural-Architecture-Search-with-LLMs-via-Unified-Numerical-Encoding"><a href="#LM-Searcher-Cross-domain-Neural-Architecture-Search-with-LLMs-via-Unified-Numerical-Encoding" class="headerlink" title="LM-Searcher: Cross-domain Neural Architecture Search with LLMs via   Unified Numerical Encoding"></a>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via   Unified Numerical Encoding</h2><p><strong>Authors:Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li</strong></p>
<p>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at <a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher">https://github.com/Ashone3/LM-Searcher</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„NASæ–¹æ³•ä¸¥é‡ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œç‰¹å®šé¢†åŸŸçš„è°ƒæ•´ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LM-Searcherï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLMè¿›è¡Œè·¨åŸŸç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼Œè€Œæ— éœ€è¿›è¡Œå¹¿æ³›çš„ç‰¹å®šé¢†åŸŸé€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯NCodeï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¥ç»ç½‘ç»œæ¶æ„çš„é€šç”¨æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œå®ƒå®ç°äº†è·¨åŸŸæ¶æ„ç¼–ç å’Œæœç´¢ã€‚æˆ‘ä»¬è¿˜é‡æ–°å°†NASé—®é¢˜è¡¨è¿°ä¸ºæ’åºä»»åŠ¡ï¼Œè®­ç»ƒLLMä»å€™é€‰æ± ä¸­é€‰æ‹©é«˜æ€§èƒ½æ¶æ„ï¼Œä½¿ç”¨åŸºäºæ–°å‹åŸºäºå‰ªæçš„å­ç©ºé—´é‡‡æ ·ç­–ç•¥ç”Ÿæˆçš„æŒ‡ä»¤è°ƒæ•´æ ·æœ¬ã€‚æˆ‘ä»¬æ•´ç†çš„æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„æ¶æ„æ€§èƒ½å¯¹ï¼Œé¼“åŠ±é²æ£’å’Œå¯è¿ç§»å­¦ä¹ ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLM-Searcheråœ¨åŸŸå†…ï¼ˆä¾‹å¦‚ï¼Œç”¨äºå›¾åƒåˆ†ç±»çš„CNNï¼‰å’ŒåŸŸå¤–ï¼ˆä¾‹å¦‚ï¼Œç”¨äºåˆ†å‰²å’Œç”Ÿæˆçš„LoRAé…ç½®ï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸ºçµæ´»å’Œé€šç”¨çš„LLMåŸºç¡€æ¶æ„æœç´¢å»ºç«‹äº†æ–°èŒƒå¼ã€‚æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher%E5%B9%BF%E5%B8%83%E3%80%82">https://github.com/Ashone3/LM-Searcherå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05657v3">PDF</a> EMNLP 2025 Main</p>
<p><strong>Summary</strong></p>
<p>LLMçš„è¿›æ­¥ä¸ºæ±‚è§£å¤æ‚çš„ä¼˜åŒ–é—®é¢˜æä¾›äº†æ–°çš„é€”å¾„ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMé©±åŠ¨çš„NASæ–¹æ³•ä¸¥é‡ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œç‰¹å®šé¢†åŸŸçš„è°ƒæ•´ï¼Œè¿™åœ¨å®è·µä¸­é™åˆ¶äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¡†æ¶LM-Searcherï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMsè¿›è¡Œè·¨åŸŸç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼Œæ— éœ€å¤§é‡çš„ç‰¹å®šé¢†åŸŸé€‚åº”ã€‚å…¶æ ¸å¿ƒæ˜¯NCodeï¼Œä¸€ç§ç”¨äºç¥ç»ç½‘ç»œæ¶æ„çš„é€šç”¨æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œå®ƒå®ç°äº†è·¨åŸŸæ¶æ„ç¼–ç å’Œæœç´¢ã€‚è¯¥ç ”ç©¶å°†NASé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ’åºä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨åŸºäºæ–°å‹å‰ªæå­ç©ºé—´é‡‡æ ·ç­–ç•¥ç”Ÿæˆçš„æŒ‡ä»¤è°ƒæ•´æ ·æœ¬ï¼Œè®­ç»ƒLLMsä»å€™é€‰æ± ä¸­é€‰æ‹©é«˜æ€§èƒ½æ¶æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒLM-Searcheråœ¨åŸŸå†…ï¼ˆå¦‚ç”¨äºå›¾åƒåˆ†ç±»çš„CNNï¼‰å’ŒåŸŸå¤–ï¼ˆå¦‚ç”¨äºåˆ†å‰²å’Œç”Ÿæˆçš„LoRAé…ç½®ï¼‰çš„ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸ºçµæ´»å’Œé€šç”¨çš„LLMåŸºç¡€æ¶æ„æœç´¢å»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„è¿›æ­¥ä¸ºæ±‚è§£å¤æ‚çš„ä¼˜åŒ–é—®é¢˜æä¾›äº†æ–°çš„é€”å¾„ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰ã€‚</li>
<li>ç°æœ‰LLMé©±åŠ¨çš„NASæ–¹æ³•å­˜åœ¨å®è·µæ€§å’Œæ‰©å±•æ€§é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œç‰¹å®šé¢†åŸŸçš„è°ƒæ•´ã€‚</li>
<li>LM-Searcheræ¡†æ¶åˆ©ç”¨LLMsè¿›è¡Œè·¨åŸŸç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼Œæ— éœ€å¤§é‡ç‰¹å®šé¢†åŸŸé€‚åº”ã€‚</li>
<li>NCodeæ˜¯LM-Searcherçš„æ ¸å¿ƒï¼Œä½œä¸ºä¸€ç§é€šç”¨æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œç”¨äºè·¨åŸŸæ¶æ„ç¼–ç å’Œæœç´¢ã€‚</li>
<li>ç ”ç©¶å°†NASé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ’åºä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´æ ·æœ¬è®­ç»ƒLLMsé€‰æ‹©é«˜æ€§èƒ½æ¶æ„ã€‚</li>
<li>LM-Searcheråœ¨å¤šç§ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cc9b30a6fbd746dcefac62766c07449" align="middle">
<img src="https://picx.zhimg.com/v2-b39341d7f46f01a9c3115c77600f4431" align="middle">
<img src="https://picx.zhimg.com/v2-23b6f703aafc14f0d76b6cdb45221ef4" align="middle">
<img src="https://picx.zhimg.com/v2-aecf2d9a1dae5e35c333c08e8a8e4fff" align="middle">
<img src="https://picx.zhimg.com/v2-477a685c398bdac6555d4909c5613eb4" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MMSI-Bench-A-Benchmark-for-Multi-Image-Spatial-Intelligence"><a href="#MMSI-Bench-A-Benchmark-for-Multi-Image-Spatial-Intelligence" class="headerlink" title="MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence"></a>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</h2><p><strong>Authors:Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang</strong></p>
<p>Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAIâ€™s o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench">https://runsenxu.com/projects/MMSI_Bench</a> . </p>
<blockquote>
<p>ç©ºé—´æ™ºèƒ½å¯¹äºåœ¨å¤æ‚çš„ç‰©ç†ä¸–ç•Œä¸­è¿è¡Œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»…æ¢ç´¢å•å›¾åƒå…³ç³»ï¼Œå› æ­¤æ— æ³•è¯„ä¼°ç°å®ä¸–ç•Œéƒ¨ç½²æ‰€éœ€çš„å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†MMSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„VQAåŸºå‡†æµ‹è¯•ã€‚å…­ä½3Dè§†è§‰ç ”ç©¶äººå‘˜èŠ±è´¹äº†è¶…è¿‡300å°æ—¶çš„æ—¶é—´ï¼Œä»è¶…è¿‡12ä¸‡å¼ å›¾åƒä¸­ç²¾å¿ƒåˆ›ä½œäº†1000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”æ¯«æ— æ­§ä¹‰çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…å¤‡äº†ç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°é¡¹å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå…¨é¢è¯„ä¼°äº†34ä¸ªå¼€æºå’Œä¸“æœ‰MLLMï¼Œè§‚å¯Ÿåˆ°å¾ˆå¤§çš„å·®è·ï¼šæœ€å¼ºçš„å¼€æºæ¨¡å‹å‡†ç¡®ç‡çº¦ä¸º30%ï¼ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ï¼Œè€Œäººç±»å¾—åˆ†ä¸º97%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†MMSI-Benchçš„æŒ‘æˆ˜æ€§ï¼Œä»¥åŠæœªæ¥ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚åˆ©ç”¨æ³¨é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨é”™è¯¯åˆ†æç®¡é“ï¼Œè¯Šæ–­äº†å››ç§ä¸»è¦çš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ¥åœ°é”™è¯¯ï¼Œï¼ˆ2ï¼‰é‡å åŒ¹é…å’Œåœºæ™¯é‡å»ºé”™è¯¯ï¼Œï¼ˆ3ï¼‰æƒ…å†µè½¬æ¢æ¨ç†é”™è¯¯ï¼Œå’Œï¼ˆ4ï¼‰ç©ºé—´é€»è¾‘é”™è¯¯ï¼Œä¸ºæ¨è¿›å¤šå›¾åƒç©ºé—´æ™ºèƒ½æä¾›äº†å®è´µçš„è§è§£ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench%E3%80%82">https://runsenxu.com/projects/MMSI_Benchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23764v2">PDF</a> 34 pages. A comprehensive, fully human-curated, multi-image-based   spatial intelligence benchmark with reasoning annotation for MLLMs. Project   page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench">https://runsenxu.com/projects/MMSI_Bench</a></p>
<p><strong>Summary</strong><br>     ç©ºé—´æ™ºèƒ½å¯¹äºåœ¨å¤æ‚ç‰©ç†ä¸–ç•Œä¸­è¿è¡Œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»…æ¢ç´¢å•å›¾åƒå…³ç³»ï¼Œæ— æ³•è¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä¸ç°å®ä¸–ç•Œéƒ¨ç½²éœ€æ±‚è„±èŠ‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºMMSI-Benchï¼Œä¸€ä¸ªä¸“æ³¨äºå¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„VQAåŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•åŒ…å«1000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ˜ç¡®é€‰æ‹©é¢˜ï¼Œç”±å…­ä½3Dè§†è§‰ç ”ç©¶äººå‘˜èŠ±è´¹è¶…è¿‡300å°æ—¶ç²¾å¿ƒåˆ›å»ºï¼Œæ¶µç›–è¶…è¿‡12ä¸‡å¼ å›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½é…æœ‰ç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°é¡¹å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬å¯¹34ä¸ªå¼€æºå’Œä¸“æœ‰MLLMè¿›è¡Œäº†å¹¿æ³›å®éªŒå’Œå…¨é¢è¯„ä¼°ï¼Œå‘ç°äº†ä¸€ä¸ªæ˜¾è‘—çš„å·®è·ï¼šæœ€å¼ºå¼€æºæ¨¡å‹çš„å‡†ç¡®ç‡çº¦ä¸º30%ï¼ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ï¼Œè€Œäººç±»å¾—åˆ†ç‡é«˜è¾¾97%ã€‚è¯¥åŸºå‡†æµ‹è¯•è¿˜æä¾›è‡ªåŠ¨é”™è¯¯åˆ†æç®¡é“ï¼Œè¯Šæ–­äº†å››ç§ä¸»è¦çš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬æ¥åœ°é”™è¯¯ã€é‡å åŒ¹é…å’Œåœºæ™¯é‡å»ºé”™è¯¯ã€æƒ…å¢ƒè½¬æ¢æ¨ç†é”™è¯¯å’Œç©ºé—´é€»è¾‘é”™è¯¯ç­‰ï¼Œä¸ºæ¨è¿›å¤šå›¾åƒç©ºé—´æ™ºèƒ½æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´æ™ºèƒ½å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç‰©ç†ä¸–ç•Œä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MMSI-Benchæ˜¯ä¸€ä¸ªæ–°çš„VQAåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå¤šå›¾åƒç©ºé—´æ™ºèƒ½ï¼ŒåŒ…å«1000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é€‰æ‹©é¢˜ã€‚</li>
<li>MMSI-Benchè¯„ä¼°äº†34ä¸ªMLLMçš„æ€§èƒ½ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚</li>
<li>æœ€å¼ºå¼€æºæ¨¡å‹å‡†ç¡®ç‡çº¦ä¸º30%ï¼ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ï¼Œè€Œäººç±»å¾—åˆ†ç‡é«˜è¾¾97%ã€‚</li>
<li>MMSI-Benchæä¾›äº†è‡ªåŠ¨é”™è¯¯åˆ†æç®¡é“ï¼Œæœ‰åŠ©äºè¯Šæ–­å¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„ä¸»è¦å¤±è´¥æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06e07f134dd908bff89de992286a419e" align="middle">
<img src="https://picx.zhimg.com/v2-36a1871cc32ec9ab9dfa1d263ab84c53" align="middle">
<img src="https://picx.zhimg.com/v2-171781c148d843fd68f9f63208195069" align="middle">
<img src="https://picx.zhimg.com/v2-b5290571cd809a6d702137a414a28f6b" align="middle">
<img src="https://picx.zhimg.com/v2-5b59480c71bef8d505e38d1868879312" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MME-VideoOCR-Evaluating-OCR-Based-Capabilities-of-Multimodal-LLMs-in-Video-Scenarios"><a href="#MME-VideoOCR-Evaluating-OCR-Based-Capabilities-of-Multimodal-LLMs-in-Video-Scenarios" class="headerlink" title="MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in   Video Scenarios"></a>MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in   Video Scenarios</h2><p><strong>Authors:Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yushuo Guan, Zhang Zhang, Liang Wang, Haoxuan Li, Zhouchen Lin, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒçš„OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰ä¸­å–å¾—äº†ç›¸å½“çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºè¿åŠ¨æ¨¡ç³Šã€æ—¶é—´å˜åŒ–ä»¥åŠè§†é¢‘å†…å®¹å›ºæœ‰çš„è§†è§‰æ•ˆæœç­‰å› ç´ ï¼Œå®ƒä»¬åœ¨è§†é¢‘OCRä¸­çš„æœ‰æ•ˆæ€§å¤§å¤§é™ä½ã€‚ä¸ºäº†ä¸ºè®­ç»ƒå®ç”¨å‹MLLMæä¾›æ›´æ¸…æ™°çš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†MME-VideoOCRåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ¶µç›–äº†å¹¿æ³›çš„è§†é¢‘OCRåº”ç”¨åœºæ™¯ã€‚MME-VideoOCRåŒ…å«10ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œå…±è®¡25ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼Œæ¶µç›–44ç§ä¸åŒåœºæ™¯ã€‚è¿™äº›ä»»åŠ¡ä¸ä»…æ¶‰åŠæ–‡æœ¬è¯†åˆ«ï¼Œè¿˜èå…¥å¯¹è§†é¢‘å†…æ–‡æœ¬å†…å®¹çš„æ›´æ·±ç†è§£å’Œæ¨ç†ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«1464ä¸ªè§†é¢‘ï¼Œå…·æœ‰ä¸åŒçš„åˆ†è¾¨ç‡ã€çºµæ¨ªæ¯”å’ŒæŒç»­æ—¶é—´ï¼Œä»¥åŠ2000ä¸ªç²¾å¿ƒç­–åˆ’ã€æ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚æˆ‘ä»¬åœ¨MME-VideoOCRä¸Šè¯„ä¼°äº†18ç§æœ€æ–°MLLMï¼Œç»“æœæ˜¾ç¤ºï¼Œå³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆGemini-2.5 Proï¼‰å‡†ç¡®ç‡ä¹Ÿåªæœ‰73.7%ã€‚ç²¾ç»†åˆ†æè¡¨æ˜ï¼Œè™½ç„¶ç°æœ‰MLLMåœ¨ç›¸å…³æ–‡æœ¬åŒ…å«åœ¨å•ä¸ªæˆ–å°‘æ•°å‡ ä¸ªå¸§çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨éœ€è¦æ•´ä½“è§†é¢‘ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰é™çš„èƒ½åŠ›ã€‚è¿™äº›å±€é™æ€§åœ¨éœ€è¦æ—¶ç©ºæ¨ç†ã€è·¨å¸§ä¿¡æ¯èåˆæˆ–æŠµæŠ—è¯­è¨€å…ˆéªŒåè§çš„åœºæ™¯ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜å‘ç°ï¼Œé«˜åˆ†è¾¨ç‡çš„è§†è§‰è¾“å…¥å’Œå……è¶³çš„æ—¶é—´è¦†ç›–å¯¹äºåŠ¨æ€è§†é¢‘åœºæ™¯çš„å¯é OCRè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21333v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨é™æ€å›¾åƒçš„OCRä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨è§†é¢‘OCRä¸­çš„æ•ˆæœå´å¤§æ‰“æŠ˜æ‰£ï¼Œé¢ä¸´è¿åŠ¨æ¨¡ç³Šã€æ—¶é—´å˜åŒ–å’Œè§†é¢‘å†…å®¹å›ºæœ‰è§†è§‰æ•ˆåº”çš„æŒ‘æˆ˜ã€‚ä¸ºç»™è®­ç»ƒå®ç”¨MLLMsæä¾›æ›´æ˜ç¡®çš„æŒ‡å¯¼ï¼Œå¼•å…¥äº†MME-VideoOCRåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¹¿æ³›çš„è§†é¢‘OCRåº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬10ä¸ªä»»åŠ¡ç±»åˆ«ã€25ä¸ªç‹¬ç«‹ä»»åŠ¡å’Œ44ä¸ªå¤šæ ·åŒ–åœºæ™¯ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«1464ä¸ªä¸åŒåˆ†è¾¨ç‡ã€æ¯”ä¾‹å’Œæ—¶é•¿çš„è§†é¢‘ï¼Œä»¥åŠ2000å¯¹æ‰‹åŠ¨ç²¾å¿ƒæŒ‘é€‰å’Œæ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚å¯¹18ç§æœ€å…ˆè¿›çš„MLLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼ˆGemini-2.5 Proï¼‰å‡†ç¡®ç‡ä¹Ÿåªæœ‰73.7%ã€‚ç²¾ç»†åˆ†æè¡¨æ˜ï¼Œç°æœ‰MLLMsåœ¨å¤„ç†å•ä¸€æˆ–å°‘æ•°å¸§åŒ…å«ç›¸å…³æ–‡æœ¬çš„ä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†åœ¨éœ€è¦æ•´ä½“è§†é¢‘ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ—¶ç©ºæ¨ç†ã€è·¨å¸§ä¿¡æ¯æ•´åˆæˆ–æŠµæŠ—è¯­è¨€å…ˆéªŒåè§çš„åœºæ™¯ä¸­ã€‚è¿˜å‘ç°é«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥å’Œè¶³å¤Ÿçš„æ—¶é—´è¦†ç›–å¯¹äºåŠ¨æ€è§†é¢‘åœºæ™¯çš„å¯é OCRè‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§†é¢‘OCRä¸­çš„å‡†ç¡®æ€§è¾ƒä½ï¼Œå—åˆ°è¿åŠ¨æ¨¡ç³Šã€æ—¶é—´å˜åŒ–å’Œè§†é¢‘å†…å®¹å›ºæœ‰è§†è§‰æ•ˆåº”çš„å½±å“ã€‚</li>
<li>å¼•å…¥MME-VideoOCRåŸºå‡†æµ‹è¯•ï¼Œä¸ºè®­ç»ƒMLLMsæä¾›æ¸…æ™°æŒ‡å¯¼ï¼Œæ¶µç›–å¤šç§è§†é¢‘OCRåº”ç”¨åœºæ™¯ã€‚</li>
<li>MME-VideoOCRåŒ…å«å¤šç§ä»»åŠ¡ï¼Œæ¶‰åŠè§†é¢‘ä¸­çš„æ–‡æœ¬è¯†åˆ«ã€ç†è§£å’Œæ¨ç†ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰MLLMsåœ¨è§†é¢‘OCRæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå³ä½¿æœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä¹Ÿåªæœ‰73.7%ã€‚</li>
<li>MLLMsåœ¨å¤„ç†éœ€è¦æ•´ä½“è§†é¢‘ç†è§£çš„ä»»åŠ¡æ—¶è¡¨ç°æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ—¶ç©ºæ¨ç†å’Œè·¨å¸§ä¿¡æ¯æ•´åˆçš„åœºæ™¯ä¸­ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥å’Œè¶³å¤Ÿçš„æ—¶é—´è¦†ç›–å¯¹åŠ¨æ€è§†é¢‘åœºæ™¯çš„å¯é OCRè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3da034a40e2839994dec638506f3f8c0" align="middle">
<img src="https://picx.zhimg.com/v2-d103a2ce625d5bc8a5352704b692d5b0" align="middle">
<img src="https://picx.zhimg.com/v2-abb1d884468d010bc23ec0469dd9f7c2" align="middle">
<img src="https://picx.zhimg.com/v2-7f88118ebf2548f6a905d63654f3e647" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Unaligned-to-Aligned-Scaling-Multilingual-LLMs-with-Multi-Way-Parallel-Corpora"><a href="#From-Unaligned-to-Aligned-Scaling-Multilingual-LLMs-with-Multi-Way-Parallel-Corpora" class="headerlink" title="From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way   Parallel Corpora"></a>From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way   Parallel Corpora</h2><p><strong>Authors:Yingli Shen, Wen Lai, Shuo Wang, Ge Gao, Kangyang Luo, Alexander Fraser, Maosong Sun</strong></p>
<p>Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data. </p>
<blockquote>
<p>æŒç»­åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒå’ŒæŒ‡å¯¼è°ƒæ•´ï¼Œå·²è¢«è¯æ˜åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰©å±•åˆ°ä½èµ„æºè¯­è¨€æ—¶æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ•°æ®çš„æœªå¯¹é½æ€§è´¨é™åˆ¶äº†å…¶æœ‰æ•ˆæ•æ‰è·¨è¯­è¨€è¯­ä¹‰çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šå‘å¹¶è¡Œæ•°æ®ï¼ˆå…¶ä¸­ç›¸åŒçš„å†…å®¹åœ¨å¤šè¯­è¨€ä¹‹é—´å¯¹é½ï¼‰æä¾›äº†æ›´å¼ºçš„è·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œå¹¶æä¾›äº†æé«˜å¤šè¯­è¨€æ€§èƒ½çš„æ›´å¤§æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåŸºäºTEDæ¼”è®²çš„å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šå‘å¹¶è¡Œè¯­æ–™åº“TED2025ã€‚è¯¥è¯­æ–™åº“æ¶µç›–113ç§è¯­è¨€ï¼Œæœ€å¤šæœ‰50ç§è¯­è¨€å¹¶è¡Œå¯¹é½ï¼Œç¡®ä¿å¹¿æ³›çš„å¤šè¯­è¨€è¦†ç›–ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨å¤šå‘å¹¶è¡Œæ•°æ®æ¥å¢å¼ºLLMçš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´çš„ç­–ç•¥ï¼Œä»¥åŠå¯¹å…³é”®å½±å“å› ç´ çš„åˆ†æã€‚åœ¨å…­ä¸ªå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šå‘å¹¶è¡Œæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹å§‹ç»ˆä¼˜äºåœ¨æœªå¯¹é½çš„å¤šè¯­è¨€æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14045v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¤šè¯­ç§æ•°æ®çš„æŒç»­é¢„è®­ç»ƒä¸æŒ‡ä»¤å¾®è°ƒå¯¹äºæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³ä½èµ„æºè¯­è¨€ååˆ†æœ‰æ•ˆã€‚ç„¶è€Œï¼Œæ­¤ç±»æ•°æ®çš„æœªå¯¹é½ç‰¹æ€§é™åˆ¶äº†å…¶æ•æ‰è·¨è¯­è¨€è¯­ä¹‰çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šå‘å¹³è¡Œæ•°æ®ï¼ˆåœ¨ä¸åŒè¯­è¨€é—´å…·æœ‰ç›¸åŒå†…å®¹çš„å¯¹é½ï¼‰æä¾›äº†æ›´å¼ºçš„è·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œå¹¶æœ‰æœ›æ”¹å–„å¤šè¯­è¨€æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºTEDæ¼”è®²çš„å¤§å‹ã€é«˜è´¨é‡å¤šå‘å¹³è¡Œè¯­æ–™åº“TED2025ï¼Œè¯¥è¯­æ–™åº“æ¶µç›–113ç§è¯­è¨€ï¼Œæœ€å¤šæœ‰50ç§è¯­è¨€å¹¶è¡Œå¯¹é½ï¼Œç¡®ä¿äº†å¹¿æ³›çš„å¤šè¯­è¨€è¦†ç›–ã€‚åˆ©ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤šå‘å¹³è¡Œæ•°æ®æ¥ä¼˜åŒ–LLMçš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒç­–ç•¥ã€æŒ‡ä»¤è°ƒæ•´ä»¥åŠå¯¹å…³é”®å½±å“å› ç´ çš„åˆ†æã€‚åœ¨å…­ä¸ªå¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å¤šå‘å¹³è¡Œæ•°æ®è®­ç»ƒçš„æ¨¡å‹å§‹ç»ˆä¼˜äºä½¿ç”¨æœªå¯¹é½çš„å¤šè¯­ç§æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡å¤šè¯­ç§æ•°æ®çš„æŒç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒå¯¹äºæ‰©å±•LLMè‡³ä½èµ„æºè¯­è¨€å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
<li>å¤šå‘å¹³è¡Œæ•°æ®æä¾›äº†æ›´å¼ºçš„è·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œæœ‰åŠ©äºæ”¹å–„å¤šè¯­è¨€æ€§èƒ½ã€‚</li>
<li>TED2025æ˜¯ä¸€ä¸ªå¤§å‹ã€é«˜è´¨é‡çš„å¤šå‘å¹³è¡Œè¯­æ–™åº“ï¼Œæ¶µç›–113ç§è¯­è¨€ï¼Œä¸ºä¼˜åŒ–LLMæä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚</li>
<li>ä½¿ç”¨å¤šå‘å¹³è¡Œæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºä½¿ç”¨æœªå¯¹é½çš„å¤šè¯­ç§æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>æœ€ä½³å®è·µåŒ…æ‹¬åˆ©ç”¨å¤šå‘å¹³è¡Œæ•°æ®çš„æŒç»­é¢„è®­ç»ƒç­–ç•¥ã€æŒ‡ä»¤è°ƒæ•´ç­–ç•¥ä»¥åŠå¯¹å…³é”®å½±å“å› ç´ çš„åˆ†æã€‚</li>
<li>å¤šè¯­ç§åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œä½¿ç”¨å¤šå‘å¹³è¡Œæ•°æ®å¯ä»¥æé«˜LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a499afc3ca5b5daa145b94d93cec6ba0" align="middle">
<img src="https://picx.zhimg.com/v2-ca40d8cf55052eddbe6e8a561be7d995" align="middle">
<img src="https://picx.zhimg.com/v2-d1869d5463ee01f16367ad4be9946af6" align="middle">
<img src="https://picx.zhimg.com/v2-d989d17562d312ba8e983fd15e2f969e" align="middle">
<img src="https://picx.zhimg.com/v2-cb8268d51d55bdb80860dc3eadaf9660" align="middle">
<img src="https://picx.zhimg.com/v2-56c1c511441d8f2a6825cb7068a60ebd" align="middle">
<img src="https://picx.zhimg.com/v2-e8e286417f71b50ecdc5e6b6bba35529" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ambiguity-Resolution-in-Text-to-Structured-Data-Mapping"><a href="#Ambiguity-Resolution-in-Text-to-Structured-Data-Mapping" class="headerlink" title="Ambiguity Resolution in Text-to-Structured Data Mapping"></a>Ambiguity Resolution in Text-to-Structured Data Mapping</h2><p><strong>Authors:Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu</strong></p>
<p>Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods to ambiguity handling either rely on the ReACT framework to obtain correct mappings through trial and error, or on supervised fine-tuning to bias models toward specific tasks. In this paper, we adopt a different approach that characterizes representation differences of ambiguous text in the latent space and leverages these differences to identify ambiguity before mapping them to structured data. To detect sentence-level ambiguity, we focus on the relationship between ambiguous questions and their interpretations. Unlike distances calculated by dense embeddings, we introduce a new distance measure based on a path kernel over concepts. With this measurement, we identify patterns to distinguish ambiguous from unambiguous questions. Furthermore, we propose a method for improving LLM performance on ambiguous agentic tool calling through missing concept prediction. Both achieve state-of-the-art results. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€ä¸­çš„æ­§ä¹‰æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ä»æ–‡æœ¬åˆ°ç»“æ„åŒ–æ•°æ®æ˜ å°„çš„æ˜¾è‘—éšœç¢ï¼Œè¿™å½±å“äº†æ–‡æœ¬æ˜ å°„åˆ°æ™ºèƒ½å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬åˆ°SQLæŸ¥è¯¢ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰çš„å¤„ç†æ­§ä¹‰çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºReACTæ¡†æ¶é€šè¿‡è¯•é”™è·å¾—æ­£ç¡®çš„æ˜ å°„ï¼Œè¦ä¹ˆä¾èµ–äºæœ‰ç›‘ç£å¾®è°ƒä½¿æ¨¡å‹åå‘äºç‰¹å®šä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼Œå³åœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ»ç”»æ¨¡ç³Šæ–‡æœ¬è¡¨ç¤ºçš„å·®å¼‚ï¼Œå¹¶åˆ©ç”¨è¿™äº›å·®å¼‚åœ¨å°†å…¶æ˜ å°„åˆ°ç»“æ„åŒ–æ•°æ®ä¹‹å‰è¯†åˆ«æ­§ä¹‰ã€‚ä¸ºäº†æ£€æµ‹å¥å­çº§åˆ«çš„æ­§ä¹‰ï¼Œæˆ‘ä»¬å…³æ³¨æ¨¡ç³Šé—®é¢˜ä¸å…¶è§£é‡Šä¹‹é—´çš„å…³ç³»ã€‚ä¸å¯†é›†åµŒå…¥è®¡ç®—çš„è·ç¦»ä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ¦‚å¿µè·¯å¾„æ ¸çš„æ–°è·ç¦»åº¦é‡ã€‚é€šè¿‡è¿™ç§åº¦é‡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«æ¨¡å¼æ¥åŒºåˆ†æ¨¡ç³Šå’Œéæ¨¡ç³Šé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡é¢„æµ‹ç¼ºå¤±æ¦‚å¿µæ¥æé«˜LLMå¤„ç†æ¨¡ç³Šæ™ºèƒ½å·¥å…·è°ƒç”¨æ€§èƒ½çš„æ–¹æ³•ã€‚ä¸¤è€…å‡è¾¾åˆ°äº†ç›®å‰æœ€ä½³çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11679v2">PDF</a> 17 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨è‡ªç„¶è¯­è¨€ä¸­çš„æ­§ä¹‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°æ–‡æœ¬åˆ°ç»“æ„åŒ–æ•°æ®æ˜ å°„çš„å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«æ–‡æœ¬è¡¨ç¤ºä¸­çš„å·®å¼‚æ¥è¯†åˆ«æ­§ä¹‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„è·ç¦»åº¦é‡æ–¹æ³•â€”â€”æ¦‚å¿µè·¯å¾„æ ¸ï¼Œä»¥åŒºåˆ†æ¨¡ç³Šå’Œéæ¨¡ç³Šé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§é€šè¿‡é¢„æµ‹ç¼ºå¤±æ¦‚å¿µæ¥æé«˜LLMåœ¨æ¨¡ç³Šä»£ç†å·¥å…·è°ƒç”¨æ–¹é¢çš„æ€§èƒ½çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å‡è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªç„¶è¯­è¨€ä¸­çš„æ­§ä¹‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å®ç°æ–‡æœ¬åˆ°ç»“æ„åŒ–æ•°æ®æ˜ å°„çš„å‡†ç¡®æ€§æ„æˆäº†æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºReACTæ¡†æ¶è¿›è¡Œæ­§ä¹‰å¤„ç†æˆ–ç›‘ç£å¾®è°ƒæ¥åå‘ç‰¹å®šä»»åŠ¡ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«æ–‡æœ¬è¡¨ç¤ºä¸­çš„å·®å¼‚æ¥è¯†åˆ«æ­§ä¹‰ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è·ç¦»åº¦é‡æ–¹æ³•â€”â€”æ¦‚å¿µè·¯å¾„æ ¸ï¼Œä»¥åŒºåˆ†æ¨¡ç³Šå’Œéæ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡é¢„æµ‹ç¼ºå¤±æ¦‚å¿µæ¥æé«˜LLMåœ¨æ¨¡ç³Šä»£ç†å·¥å…·è°ƒç”¨æ–¹é¢çš„æ€§èƒ½çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fb9fa3d64d97fcfac0ce9cc491d8294" align="middle">
<img src="https://picx.zhimg.com/v2-db2f423b39ca9a2b5456db3138394684" align="middle">
<img src="https://picx.zhimg.com/v2-6e19e313ae441cc1d9348e2291a95f07" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="In-Context-Edit-Enabling-Instructional-Image-Editing-with-In-Context-Generation-in-Large-Scale-Diffusion-Transformer"><a href="#In-Context-Edit-Enabling-Instructional-Image-Editing-with-In-Context-Generation-in-Large-Scale-Diffusion-Transformer" class="headerlink" title="In-Context Edit: Enabling Instructional Image Editing with In-Context   Generation in Large Scale Diffusion Transformer"></a>In-Context Edit: Enabling Instructional Image Editing with In-Context   Generation in Large Scale Diffusion Transformer</h2><p><strong>Authors:Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang</strong></p>
<p>Instruction-based image editing enables precise modifications via natural language prompts, but existing methods face a precision-efficiency tradeoff: fine-tuning demands massive datasets (&gt;10M) and computational resources, while training-free approaches suffer from weak instruction comprehension. We address this by proposing ICEdit, which leverages the inherent comprehension and generation abilities of large-scale Diffusion Transformers (DiTs) through three key innovations: (1) An in-context editing paradigm without architectural modifications; (2) Minimal parameter-efficient fine-tuning for quality improvement; (3) Early Filter Inference-Time Scaling, which uses VLMs to select high-quality noise samples for efficiency. Experiments show that ICEdit achieves state-of-the-art editing performance with only 0.1% of the training data and 1% trainable parameters compared to previous methods. Our approach establishes a new paradigm for balancing precision and efficiency in instructional image editing. Codes and demos can be found in <a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/">https://river-zhang.github.io/ICEdit-gh-pages/</a>. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç²¾ç¡®ä¿®æ”¹ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç€ç²¾åº¦ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼šå¾®è°ƒéœ€è¦å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆ&gt; 10Mï¼‰å’Œè®¡ç®—èµ„æºï¼Œè€Œæ— éœ€è®­ç»ƒçš„æ–¹æ³•åˆ™å­˜åœ¨æŒ‡ä»¤ç†è§£è¾ƒå¼±çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºICEditæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹åˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰çš„å›ºæœ‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼šï¼ˆ1ï¼‰æ— éœ€æ¶æ„ä¿®æ”¹çš„å†…éƒ¨ç¼–è¾‘èŒƒå¼ï¼›ï¼ˆ2ï¼‰ç”¨äºæ”¹è¿›è´¨é‡çš„å‚æ•°æ•ˆç‡æä½çš„å¾®è°ƒï¼›ï¼ˆ3ï¼‰æ—©æœŸæ»¤æ³¢æ¨ç†æ—¶é—´ç¼©æ”¾ï¼Œä½¿ç”¨VLMsé€‰æ‹©é«˜è´¨é‡çš„å™ªå£°æ ·æœ¬ä»¥æé«˜æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒICEditä»…ä½¿ç”¨0.1%çš„è®­ç»ƒæ•°æ®å’Œ1%çš„å¯è®­ç»ƒå‚æ•°å°±å®ç°äº†æœ€å…ˆè¿›çš„ç¼–è¾‘æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºå¹³è¡¡æŒ‡ä»¤å›¾åƒç¼–è¾‘ä¸­çš„ç²¾åº¦å’Œæ•ˆç‡å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/%E6%89%BE%E5%88%B0%E3%80%82">https://river-zhang.github.io/ICEdit-gh-pages/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20690v3">PDF</a> Accepted by NeurIPS 2025, there will be future updates for camera   ready version. Code: <a target="_blank" rel="noopener" href="https://github.com/River-Zhang/ICEdit">https://github.com/River-Zhang/ICEdit</a></p>
<p><strong>Summary</strong><br>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç²¾ç¡®ä¿®æ”¹ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç²¾ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼šç²¾ç»†è°ƒæ•´éœ€è¦å¤§é‡æ•°æ®é›†ï¼ˆ&gt; 10Mï¼‰å’Œè®¡ç®—èµ„æºï¼Œè€Œå…åŸ¹è®­æ–¹æ³•åˆ™ç¼ºä¹æŒ‡ä»¤ç†è§£ã€‚æˆ‘ä»¬æå‡ºICEditæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹åˆ©ç”¨å¤§å‹æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰çš„å›ºæœ‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼šï¼ˆ1ï¼‰æ— éœ€æ¶æ„ä¿®æ”¹çš„åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç¼–è¾‘æ¨¡å¼ï¼›ï¼ˆ2ï¼‰æœ€å°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒä»¥æé«˜è´¨é‡ï¼›ï¼ˆ3ï¼‰æ—©æœŸè¿‡æ»¤æ¨ç†æ—¶é—´ç¼©æ”¾ï¼Œä½¿ç”¨VLMsé€‰æ‹©é«˜è´¨é‡çš„å™ªå£°æ ·æœ¬ä»¥æé«˜æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒICEditåªéœ€ä½¿ç”¨0.1%çš„è®­ç»ƒæ•°æ®å’Œ1%çš„å¯è®­ç»ƒå‚æ•°å³å¯å®ç°æœ€å…ˆè¿›çš„ç¼–è¾‘æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºå¹³è¡¡æŒ‡ä»¤å›¾åƒç¼–è¾‘ä¸­çš„ç²¾ç¡®æ€§å’Œæ•ˆç‡å»ºç«‹äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICEditè§£å†³äº†ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•çš„ç²¾ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ICEdité€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹åˆ©ç”¨å¤§å‹æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰çš„å›ºæœ‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ICEditå®ç°äº†åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç¼–è¾‘æ¨¡å¼ï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚</li>
<li>ICEdité€šè¿‡æœ€å°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ¥æé«˜å›¾åƒç¼–è¾‘çš„è´¨é‡ã€‚</li>
<li>ICEdité‡‡ç”¨æ—©æœŸè¿‡æ»¤æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯ï¼Œé€šè¿‡é€‰æ‹©é«˜è´¨é‡çš„å™ªå£°æ ·æœ¬æé«˜æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒICEditåœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®å’Œå‚æ•°çš„æƒ…å†µä¸‹å³å¯å®ç°æœ€å…ˆè¿›çš„ç¼–è¾‘æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0405b0556dd151e4203f3b00211de9f2" align="middle">
<img src="https://picx.zhimg.com/v2-3dbbc77eacf7784791530a96465d48df" align="middle">
<img src="https://picx.zhimg.com/v2-05b9fc627f41388c71c228a378e89d7f" align="middle">
<img src="https://picx.zhimg.com/v2-2031e9b03c7e460b5852aa53525c223f" align="middle">
<img src="https://picx.zhimg.com/v2-efeeb8004c9137be23aca7fca16d0d44" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAIN-Mutual-Alignment-Is-Necessary-for-instruction-tuning"><a href="#MAIN-Mutual-Alignment-Is-Necessary-for-instruction-tuning" class="headerlink" title="MAIN: Mutual Alignment Is Necessary for instruction tuning"></a>MAIN: Mutual Alignment Is Necessary for instruction tuning</h2><p><strong>Authors:Fanyi Yang, Jianfeng Liu, Xin Zhang, Haoyu Liu, Xixin Cao, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang</strong></p>
<p>Instruction tuning has empowered large language models (LLMs) to achieve remarkable performance, yet its success heavily depends on the availability of large-scale, high-quality instruction-response pairs. To meet this demand, various methods have been developed to synthesize data at scale. However, current methods for scaling up data generation often overlook a crucial aspect: the alignment between instructions and responses. We hypothesize that the quality of instruction-response pairs is determined not by the individual quality of each component, but by the degree of mutual alignment. To address this, we propose a Mutual Alignment Framework (MAIN) which enforces coherence between instructions and responses through mutual constraints. We demonstrate that MAIN generalizes well across model architectures and sizes, achieving state-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse benchmarks. This work underscores the critical role of instruction-response alignment in enabling generalizable and high-quality instruction tuning for LLMs. All code is available from our repository. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å…¶æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚ä¸ºäº†æ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œå·²ç»å¼€å‘äº†å„ç§æ–¹æ³•æ¥å¤§è§„æ¨¡åˆæˆæ•°æ®ã€‚ç„¶è€Œï¼Œå½“å‰æ‰©å¤§æ•°æ®ç”Ÿæˆè§„æ¨¡çš„æ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸€ä¸ªå…³é”®æ–¹é¢ï¼šæŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„å¯¹é½ã€‚æˆ‘ä»¬å‡è®¾æŒ‡ä»¤å“åº”å¯¹çš„è´¨å¹¶ä¸å–å†³äºæ¯ä¸ªç»„ä»¶çš„å•ç‹¬è´¨é‡ï¼Œè€Œæ˜¯å–å†³äºç›¸äº’å¯¹é½çš„ç¨‹åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸äº’å¯¹é½æ¡†æ¶ï¼ˆMAINï¼‰ï¼Œé€šè¿‡ç›¸äº’çº¦æŸæ¥åŠ å¼ºæŒ‡ä»¤å’Œå“åº”ä¹‹é—´çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒMAINåœ¨æ¨¡å‹æ¶æ„å’Œè§„æ¨¡æ–¹é¢å…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ï¼Œåœ¨LLaMAã€Mistralå’ŒQwenæ¨¡å‹çš„å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æŒ‡ä»¤å“åº”å¯¹é½åœ¨ä½¿LLMå®ç°é€šç”¨å’Œé«˜è´¨é‡çš„æŒ‡ä»¤è°ƒæ•´ä¸­çš„å…³é”®ä½œç”¨ã€‚æ‰€æœ‰ä»£ç å‡å¯åœ¨æˆ‘ä»¬çš„å­˜å‚¨åº“ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12913v3">PDF</a> Accepted by EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚ä¸ºæ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œå·²ç»å¼€å‘äº†å„ç§æ–¹æ³•æ¥å®ç°æ•°æ®çš„è§„æ¨¡åŒ–åˆæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†æŒ‡ä»¤ä¸å“åº”ä¹‹é—´å¯¹é½çš„å…³é”®æ–¹é¢ã€‚æœ¬ç ”ç©¶å‡è®¾æŒ‡ä»¤å“åº”å¯¹çš„å“è´¨å¹¶éç”±å•ä¸€æˆåˆ†çš„å“è´¨å†³å®šï¼Œè€Œæ˜¯ç”±ç›¸äº’å¯¹é½çš„ç¨‹åº¦å†³å®šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸äº’å¯¹é½æ¡†æ¶ï¼ˆMAINï¼‰ï¼Œé€šè¿‡ç›¸äº’çº¦æŸå®ç°æŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸ŠéªŒè¯äº†MAINçš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨LLaMAã€Mistralå’ŒQwenæ¨¡å‹ä¸Šçš„å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æŒ‡ä»¤å“åº”å¯¹é½åœ¨ä½¿LLMå®ç°é€šç”¨å’Œé«˜è´¨æ„Ÿçš„æŒ‡ä»¤è°ƒæ•´ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒå¯¹LLMæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>å¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§æ˜¯å…³é”®ã€‚</li>
<li>ç°æœ‰æ•°æ®åˆæˆæ–¹æ³•å¾€å¾€å¿½è§†æŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>æŒ‡ä»¤å“åº”å¯¹çš„å“è´¨å–å†³äºæŒ‡ä»¤ä¸å“åº”çš„ç›¸äº’å¯¹é½ç¨‹åº¦ã€‚</li>
<li>æå‡ºç›¸äº’å¯¹é½æ¡†æ¶ï¼ˆMAINï¼‰ä»¥å®ç°æŒ‡ä»¤ä¸å“åº”çš„è¿è´¯æ€§ã€‚</li>
<li>MAINæ¡†æ¶åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30f9b459ab8f9e86d20105e8a88ce13d" align="middle">
<img src="https://picx.zhimg.com/v2-27331bc7cb0863edb1a6a0c10dcc9e41" align="middle">
<img src="https://picx.zhimg.com/v2-31f25872c1dee6264918f67bbcbaca7c" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AttentionDrop-A-Novel-Regularization-Method-for-Transformer-Models"><a href="#AttentionDrop-A-Novel-Regularization-Method-for-Transformer-Models" class="headerlink" title="AttentionDrop: A Novel Regularization Method for Transformer Models"></a>AttentionDrop: A Novel Regularization Method for Transformer Models</h2><p><strong>Authors:Mirza Samad Ahmed Baig, Syeda Anshrah Gillani, Abdul Akbar Khan, Shahid Munir Shah, Muhammad Omer Khan</strong></p>
<p>Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech processing. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. In this research, a unified family of stochastic regularization techniques has been proposed, i.e. AttentionDrop with its three different variants, which operate directly on the self-attention distributions. Hard Attention Masking randomly zeroes out top-k attention logits per query to encourage diverse context utilization, Blurred Attention Smoothing applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions, and Consistency-Regularized AttentionDrop enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss. Results achieved in the study demonstrate that AttentionDrop consistently improves accuracy, calibration, and adversarial robustness over standard Dropout, DropConnect, and R-Drop baselines </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³å¤„ç†ç­‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„å®¹é‡å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™æˆ–å˜ˆæ‚çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ç³»åˆ—ç»Ÿä¸€çš„éšæœºæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå³AttentionDropåŠå…¶ä¸‰ç§ä¸åŒå˜ä½“ã€‚å®ƒä»¬ç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›åˆ†å¸ƒä¸Šè¿è¡Œã€‚ç¡¬æ³¨æ„åŠ›æ©ç éšæœºå°†æ¯ä¸ªæŸ¥è¯¢çš„å‰kä¸ªæ³¨æ„åŠ›å¯¹æ•°ç½®é›¶ï¼Œä»¥é¼“åŠ±å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡åˆ©ç”¨ï¼›æ¨¡ç³Šæ³¨æ„åŠ›å¹³æ»‘åœ¨æ³¨æ„åŠ›å¯¹æ•°ä¸Šåº”ç”¨åŠ¨æ€é«˜æ–¯å·ç§¯ï¼Œä»¥æ‰©æ•£è¿‡äºå°–é”çš„åˆ†å¸ƒï¼›ä¸€è‡´æ€§æ­£åˆ™åŒ–AttentionDropé€šè¿‡åŸºäºKLçš„ä¸€è‡´æ€§æŸå¤±ï¼Œå¼ºåˆ¶å¤šä¸ªç‹¬ç«‹AttentionDropæ‰°åŠ¨ä¸‹çš„è¾“å‡ºç¨³å®šæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAttentionDropåœ¨å‡†ç¡®åº¦ã€æ ¡å‡†å’Œå¯¹æŠ—ç¨³å¥æ€§æ–¹é¢æŒç»­ä¼˜äºæ ‡å‡†Dropoutã€DropConnectå’ŒR-DropåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12088v2">PDF</a> 25 pages</p>
<p><strong>Summary</strong>ï¼šåŸºäºTransformerçš„æ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³å¤„ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†å…¶å·¨å¤§çš„å®¹é‡æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„éšæœºæ­£åˆ™åŒ–æŠ€æœ¯â€”â€”AttentionDropåŠå…¶ä¸‰ç§å˜ä½“ï¼Œç›´æ¥ä½œç”¨äºè‡ªæˆ‘æ³¨æ„åˆ†å¸ƒã€‚è¿™ä¸‰ç§æŠ€æœ¯åŒ…æ‹¬ç¡¬æ³¨æ„åŠ›å±è”½ã€æ¨¡ç³Šæ³¨æ„åŠ›å¹³æ»‘å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–AttentionDropã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAttentionDropåœ¨å‡†ç¡®æ€§ã€æ ¡å‡†å’Œå¯¹æŠ—ç¨³å¥æ€§æ–¹é¢å‡ä¼˜äºæ ‡å‡†Dropoutã€DropConnectå’ŒR-DropåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformeræ¶æ„åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„éšæœºæ­£åˆ™åŒ–æŠ€æœ¯â€”â€”AttentionDropï¼ŒåŒ…æ‹¬ä¸‰ç§å˜ä½“æŠ€æœ¯ã€‚</li>
<li>AttentionDropç›´æ¥ä½œç”¨äºè‡ªæˆ‘æ³¨æ„åˆ†å¸ƒï¼Œæ—¨åœ¨è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>ç¡¬æ³¨æ„åŠ›å±è”½é€šè¿‡éšæœºå±è”½æ³¨æ„åŠ›å¾—åˆ†æ¥é¼“åŠ±å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡åˆ©ç”¨ã€‚</li>
<li>æ¨¡ç³Šæ³¨æ„åŠ›å¹³æ»‘é€šè¿‡åŠ¨æ€é«˜æ–¯å·ç§¯æ¥æ‰©æ•£è¿‡äºé›†ä¸­çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚</li>
<li>ä¸€è‡´æ€§æ­£åˆ™åŒ–AttentionDropé€šè¿‡KLä¸€è‡´æ€§æŸå¤±ç¡®ä¿è¾“å‡ºåœ¨å¤šæ¬¡ç‹¬ç«‹çš„AttentionDropæ‰°åŠ¨ä¸‹ä¿æŒç¨³å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6219f18a3173b5e0f040dc4eda0db44f" align="middle">
<img src="https://picx.zhimg.com/v2-80f83b96ea5c35edd74c5999c24c720c" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding"><a href="#On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding" class="headerlink" title="On the Perception Bottleneck of VLMs for Chart Understanding"></a>On the Perception Bottleneck of VLMs for Chart Understanding</h2><p><strong>Authors:Junteng Liu, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, Junxian He</strong></p>
<p>Chart understanding requires models to effectively analyze and reason about numerical data, textual elements, and complex visual components. Our observations reveal that the perception capabilities of existing large vision-language models (LVLMs) constitute a critical bottleneck in this process. In this study, we delve into this perception bottleneck by decomposing it into two components: the vision encoder bottleneck, where the visual representation may fail to encapsulate the correct information, and the extraction bottleneck, where the language model struggles to extract the necessary information from the provided visual representations. Through comprehensive experiments, we find that (1) the information embedded within visual representations is substantially richer than what is typically captured by linear extractors, such as the widely used retrieval accuracy metric; (2) While instruction tuning effectively enhances the extraction capability of LVLMs, the vision encoder remains a critical bottleneck, demanding focused attention and improvement. Therefore, we further enhance the visual encoder to mitigate the vision encoder bottleneck under a contrastive learning framework. Empirical results demonstrate that our approach significantly mitigates the perception bottleneck and improves the ability of LVLMs to comprehend charts. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>. </p>
<blockquote>
<p>å›¾è¡¨ç†è§£éœ€è¦æ¨¡å‹å¯¹æ•°å€¼æ•°æ®ã€æ–‡æœ¬å…ƒç´ å’Œå¤æ‚è§†è§‰æˆåˆ†è¿›è¡Œæœ‰æ•ˆçš„åˆ†æå’Œæ¨ç†ã€‚æˆ‘ä»¬çš„è§‚å¯Ÿå‘ç°ï¼Œç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›æ„æˆè¿™ä¸€è¿‡ç¨‹ä¸­çš„å…³é”®ç“¶é¢ˆã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªç»„ä»¶æ¥æ·±å…¥ç ”ç©¶è¿™ä¸€æ„ŸçŸ¥ç“¶é¢ˆï¼šè§†è§‰ç¼–ç å™¨ç“¶é¢ˆï¼Œå…¶ä¸­è§†è§‰è¡¨ç¤ºå¯èƒ½æ— æ³•å°è£…æ­£ç¡®çš„ä¿¡æ¯ï¼›ä»¥åŠæå–ç“¶é¢ˆï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹éš¾ä»¥ä»æä¾›çš„è§†è§‰è¡¨ç¤ºä¸­æå–å¿…è¦çš„ä¿¡æ¯ã€‚é€šè¿‡ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼ˆ1ï¼‰è§†è§‰è¡¨ç¤ºä¸­æ‰€åµŒå…¥çš„ä¿¡æ¯è¿œæ¯”çº¿æ€§æå–å™¨ï¼ˆå¦‚å¹¿æ³›ä½¿ç”¨çš„æ£€ç´¢å‡†ç¡®ç‡æŒ‡æ ‡ï¼‰æ‰€æ•è·çš„è¦ä¸°å¯Œå¾—å¤šï¼›ï¼ˆ2ï¼‰è™½ç„¶æŒ‡ä»¤è°ƒæ•´æœ‰æ•ˆåœ°æé«˜äº†LVLMsçš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œéœ€è¦é›†ä¸­æ³¨æ„åŠ›å’Œè¿›è¡Œæ”¹è¿›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ï¼Œä»¥åœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸‹ç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ç¼“è§£äº†æ„ŸçŸ¥ç“¶é¢ˆï¼Œæé«˜äº†LVLMsç†è§£å›¾è¡¨çš„èƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18435v2">PDF</a> EMNLP 2025: Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾è¡¨ç†è§£æ—¶å­˜åœ¨è§†è§‰ç¼–ç ç“¶é¢ˆå’Œä¿¡æ¯æå–ç“¶é¢ˆã€‚ç ”ç©¶å‘ç°è§†è§‰è¡¨å¾æ‰€åŒ…å«çš„ä¿¡æ¯ä¸°å¯Œåº¦è¶…å‡ºä¼ ç»Ÿçº¿æ€§æå–å™¨çš„æ•æ‰èƒ½åŠ›ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒè™½ç„¶æé«˜äº†æ¨¡å‹çš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ã€‚å…¬å¼€ä»£ç ä½äºï¼š[å…¬å¼€é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾è¡¨ç†è§£æ—¶é¢ä¸´è§†è§‰ç¼–ç å’Œä¿¡æ¯æå–ä¸¤å¤§ç“¶é¢ˆã€‚</li>
<li>è§†è§‰è¡¨å¾åŒ…å«çš„ä¿¡æ¯ä¸°å¯Œåº¦è¶…å‡ºä¼ ç»Ÿçº¿æ€§æå–å™¨çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒèƒ½æé«˜æ¨¡å‹çš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»æ˜¯å…³é”®æ”¹è¿›ç‚¹ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ¡†æ¶è¢«ç”¨äºå¢å¼ºè§†è§‰ç¼–ç å™¨ï¼Œä»¥ç¼“è§£è§†è§‰ç¼–ç ç“¶é¢ˆã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºäº†æ–°çš„æ–¹æ³•æ”¹å–„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å›¾è¡¨çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æˆæœå…¬å¼€å¯ç”¨ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶ä¸åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b4468e9272f3866d5ee8d5ecd71595c" align="middle">
<img src="https://picx.zhimg.com/v2-7228e0a246cc2a14b0235c874ae6f0f9" align="middle">
<img src="https://picx.zhimg.com/v2-e52846d8399e774728e18e91177aefa1" align="middle">
<img src="https://picx.zhimg.com/v2-1ad2d4fe3859fafca80f63e5c97b364a" align="middle">
<img src="https://picx.zhimg.com/v2-fa7d9c9a648fae3beb7e2279aa61f7bd" align="middle">
<img src="https://picx.zhimg.com/v2-cc9c941074f82a6cc6be40df6f598bcd" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition"><a href="#LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition" class="headerlink" title="LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?"></a>LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?</h2><p><strong>Authors:Bangyan Li, Wenxuan Huang, Zhenkun Gao, Yeqiang Wang, Yunhang Shen, Jingzhong Lin, Ling You, Yuxiang Shen, Shaohui Lin, Wanli Ouyang, Yuling Sun</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, we found that MLLMs cannot process effectively from fine-grained medical image data in the traditional Visual Question Answering (VQA) pipeline, as they do not exploit the captured features and available medical knowledge fully, results in MLLMs usually performing poorly in zero-shot medical disease recognition. Fortunately, this limitation does not indicate that MLLMs are fundamentally incapable of addressing fine-grained recognition tasks. From a feature representation perspective, MLLMs demonstrate considerable potential for tackling such challenging problems. Thus, to address this challenge, we propose LLaVA-RadZ, a simple yet effective framework for zero-shot medical disease recognition via utilizing the existing MLLM features. Specifically, we design an end-to-end training strategy, termed Decoding-Side Feature Alignment Training (DFAT) to take advantage of the characteristics of the MLLM decoder architecture and incorporate modality-specific tokens tailored for different modalities. Additionally, we introduce a Domain Knowledge Anchoring Module (DKAM) to exploit the intrinsic medical knowledge of large models, which mitigates the category semantic gap in image-text alignment. Extensive experiments demonstrate that our LLaVA-RadZ significantly outperforms traditional MLLMs in zero-shot disease recognition, achieving the comparable performance to the well-established and highly-optimized CLIP-based approaches. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å±•ç¤ºäº†å‡ºè‰²çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°MLLMsåœ¨ä¼ ç»Ÿçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç®¡é“ä¸­æ— æ³•æœ‰æ•ˆåœ°å¤„ç†ç²¾ç»†çš„åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å……åˆ†åˆ©ç”¨æ•è·çš„ç‰¹å¾å’Œå¯ç”¨çš„åŒ»å­¦çŸ¥è¯†ï¼Œå¯¼è‡´MLLMsåœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­é€šå¸¸è¡¨ç°ä¸ä½³ã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™ä¸€å±€é™æ€§å¹¶ä¸æ„å‘³ç€MLLMsä»æ ¹æœ¬ä¸Šæ— æ³•è§£å†³ç²¾ç»†è¯†åˆ«ä»»åŠ¡ã€‚ä»ç‰¹å¾è¡¨ç¤ºçš„è§’åº¦æ¥çœ‹ï¼ŒMLLMsåœ¨è§£å†³è¿™ç±»éš¾é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-RadZæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ç°æœ‰MLLMç‰¹æ€§è¿›è¡Œé›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«çš„ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºè§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒï¼ˆDFATï¼‰ï¼Œä»¥åˆ©ç”¨MLLMè§£ç å™¨çš„ç‰¹æ€§ï¼Œå¹¶èå…¥é’ˆå¯¹ä¸åŒæ¨¡æ€çš„ç‰¹å®šæ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼Œä»¥åˆ©ç”¨å¤§å‹æ¨¡å‹çš„å†…åœ¨åŒ»å­¦çŸ¥è¯†ï¼Œç¼©å°å›¾åƒæ–‡æœ¬å¯¹é½ä¸­çš„ç±»åˆ«è¯­ä¹‰å·®è·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LLaVA-RadZåœ¨é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸMLLMsï¼Œå–å¾—äº†ä¸å»ºç«‹è‰¯å¥½ä¸”é«˜åº¦ä¼˜åŒ–çš„CLIPæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07487v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£åŠè·¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå±•ç°äº†å‡ºè‰²èƒ½åŠ›ã€‚ä½†åœ¨ä¼ ç»Ÿè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æµç¨‹ä¸­å¤„ç†ç²¾ç»†åŒ»å­¦å›¾åƒæ•°æ®æ—¶ï¼Œå®ƒä»¬æ— æ³•æœ‰æ•ˆå‘æŒ¥åŠŸèƒ½ï¼Œå¯¼è‡´åœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­è¡¨ç°ä¸ä½³ã€‚ç„¶è€Œï¼ŒMLLMså…·æœ‰å·¨å¤§æ½œåŠ›æ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLLaVA-RadZæ¡†æ¶ï¼Œåˆ©ç”¨ç°æœ‰MLLMç‰¹æ€§è¿›è¡Œé›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ã€‚é€šè¿‡è®¾è®¡åä¸ºè§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒï¼ˆDFATï¼‰çš„ç«¯åˆ°ç«¯è®­ç»ƒç­–ç•¥å¹¶å¼•å…¥é¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMåœ¨é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«ä¸­çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§†è§‰ç†è§£å’Œè·¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç²¾ç»†åŒ»å­¦å›¾åƒæ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MLLMsåœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­è¡¨ç°ä¸ä½³ï¼Œä½†å…·æœ‰è§£å†³æ­¤ç±»é—®é¢˜çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºLLaVA-RadZæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨MLLMç‰¹æ€§è¿›è¡Œé›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ã€‚</li>
<li>DFATæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨åˆ©ç”¨MLLMè§£ç å™¨æ¶æ„çš„ç‰¹æ€§å¹¶ç»“åˆä¸åŒæ¨¡æ€çš„ç‰¹å®šæ ‡è®°ã€‚</li>
<li>DKAMæ¨¡å—ç”¨äºæŒ–æ˜å¤§å‹æ¨¡å‹çš„å†…åœ¨åŒ»å­¦çŸ¥è¯†ï¼Œç¼©å°å›¾åƒæ–‡æœ¬å¯¹é½ä¸­çš„ç±»åˆ«è¯­ä¹‰å·®è·ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLLaVA-RadZåœ¨é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸMLLMsï¼Œå¹¶ä¸ç»è¿‡è‰¯å¥½è®­ç»ƒçš„CLIPæ–¹æ³•æ€§èƒ½ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b9d97277ee0cb9a02e0f15ba3fd937e" align="middle">
<img src="https://picx.zhimg.com/v2-e2baac8642f1e902fcef43c6530079de" align="middle">
<img src="https://picx.zhimg.com/v2-39ed1522903d035d09885671c22c8a6b" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Transformer-Model-for-Predicting-Chemical-Products-from-Generic-SMARTS-Templates-with-Data-Augmentation"><a href="#A-Transformer-Model-for-Predicting-Chemical-Products-from-Generic-SMARTS-Templates-with-Data-Augmentation" class="headerlink" title="A Transformer Model for Predicting Chemical Products from Generic SMARTS   Templates with Data Augmentation"></a>A Transformer Model for Predicting Chemical Products from Generic SMARTS   Templates with Data Augmentation</h2><p><strong>Authors:Derin Ozer, Sylvain Lamprier, Thomas Cauchy, Nicolas Gutowski, Benoit Da Mota</strong></p>
<p>The accurate prediction of chemical reaction outcomes is a major challenge in computational chemistry. Current models rely heavily on either highly specific reaction templates or template-free methods, both of which present limitations. To address these, this work proposes the Broad Reaction Set (BRS), a set featuring 20 generic reaction templates written in SMARTS, a pattern-based notation designed to describe substructures and reactivity. Additionally, we introduce ProPreT5, a T5-based model specifically adapted for chemistry and, to the best of our knowledge, the first language model capable of directly handling and applying SMARTS reaction templates. To further improve generalization, we propose the first augmentation strategy for SMARTS, which injects structural diversity at the pattern level. Trained on augmented templates, ProPreT5 demonstrates strong predictive performance and generalization to unseen reactions. Together, these contributions provide a novel and practical alternative to current methods, advancing the field of template-based reaction prediction. </p>
<blockquote>
<p>åŒ–å­¦ååº”ç»“æœçš„å‡†ç¡®é¢„æµ‹æ˜¯è®¡ç®—åŒ–å­¦é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚å½“å‰æ¨¡å‹ä¸¥é‡ä¾èµ–äºç‰¹å®šçš„ååº”æ¨¡æ¿æˆ–æ— æ¨¡æ¿æ–¹æ³•ï¼ŒäºŒè€…éƒ½å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†å¹¿è°±ååº”é›†ï¼ˆBRSï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„ä»¥SMARTSç¼–å†™çš„20ä¸ªé€šç”¨ååº”æ¨¡æ¿ï¼ŒSMARTSæ˜¯ä¸€ç§åŸºäºæ¨¡å¼çš„ç¬¦å·ï¼Œç”¨äºæè¿°å­ç»“æ„å’Œååº”æ´»æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºT5æ¨¡å‹çš„ProPreT5ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŒ–å­¦è¿›è¡Œé€‚é…çš„æ¨¡å‹ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿç›´æ¥å¤„ç†å’Œåº”ç”¨SMARTSååº”æ¨¡æ¿çš„è¯­è¨€æ¨¡å‹ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†SMARTSçš„é¦–ä¸ªå¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ¨¡å¼å±‚é¢æ³¨å…¥äº†ç»“æ„å¤šæ ·æ€§ã€‚åœ¨å¢å¼ºæ¨¡æ¿çš„è®­ç»ƒä¸‹ï¼ŒProPreT5æ˜¾ç¤ºå‡ºå¼ºå¤§çš„é¢„æµ‹æ€§èƒ½å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ååº”çš„èƒ½åŠ›ã€‚æ€»ä¹‹ï¼Œè¿™äº›è´¡çŒ®ä¸ºå½“å‰æ–¹æ³•æä¾›äº†æ–°é¢–å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†åŸºäºæ¨¡æ¿çš„ååº”é¢„æµ‹é¢†åŸŸçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05810v3">PDF</a> ICTAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹è®¡ç®—åŒ–å­¦ä¸­åŒ–å­¦ååº”ç»“æœé¢„æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Broad Reaction Setï¼ˆBRSï¼‰å’ŒProPreT5æ¨¡å‹ã€‚BRSåŒ…å«20ä¸ªé€šç”¨ååº”æ¨¡æ¿ï¼Œé‡‡ç”¨SMARTSæè¿°å­æ¥æè¿°å­ç»“æ„å’Œååº”æ€§ã€‚ProPreT5æ˜¯åŸºäºT5æ¨¡å‹çš„æ”¹è¿›ç‰ˆï¼Œèƒ½ç›´æ¥å¤„ç†å’Œåº”ç”¨SMARTSååº”æ¨¡æ¿ï¼Œå¹¶åœ¨æœ€ä½³çŸ¥è¯†æƒ…å†µä¸‹ä¸ºé¦–ä¸ªæ¨¡å‹ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜é€šç”¨æ€§ï¼Œæ–‡ç« è¿˜æå‡ºäº†SMARTSçš„å¢å¼ºç­–ç•¥ï¼Œå³åœ¨æ¨¡å¼çº§åˆ«æ³¨å…¥ç»“æ„å¤šæ ·æ€§ã€‚ç»è¿‡è®­ç»ƒçš„ProPreT5æ¨¡å‹å¯¹æœªè§è¿‡çš„ååº”è¡¨ç°å‡ºå¼ºå¤§çš„é¢„æµ‹æ€§èƒ½å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå½“å‰æ–¹æ³•æä¾›äº†æ–°é¢–å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†åŸºäºæ¨¡æ¿çš„ååº”é¢„æµ‹é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†Broad Reaction Setï¼ˆBRSï¼‰ï¼ŒåŒ…å«20ä¸ªé€šç”¨ååº”æ¨¡æ¿ï¼Œé‡‡ç”¨SMARTSæè¿°å­æè¿°å­ç»“æ„å’Œååº”æ€§ã€‚</li>
<li>å¼•å…¥äº†ProPreT5æ¨¡å‹ï¼Œå®ƒæ˜¯é¦–ä¸ªèƒ½ç›´æ¥å¤„ç†å’Œåº”ç”¨SMARTSååº”æ¨¡æ¿çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ–‡ç« æå‡ºäº†SMARTSçš„å¢å¼ºç­–ç•¥ï¼Œå³åœ¨æ¨¡å¼çº§åˆ«æ³¨å…¥ç»“æ„å¤šæ ·æ€§ã€‚</li>
<li>ProPreT5æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>ProPreT5æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„ååº”ã€‚</li>
<li>ä¸å½“å‰æ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„æ¨¡å‹å’Œç­–ç•¥ä¸ºè®¡ç®—åŒ–å­¦ä¸­çš„ååº”é¢„æµ‹æä¾›äº†æ–°é¢–ä¸”å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-728185d621013515a1615b408371f72e" align="middle">
<img src="https://picx.zhimg.com/v2-f1f2e8319d467c294c5e383977d9ce43" align="middle">
<img src="https://picx.zhimg.com/v2-aa1427510db8b6cbf652d61227a97804" align="middle">
<img src="https://picx.zhimg.com/v2-90e11b01e4f40996394c8302170d1994" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer"><a href="#Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer" class="headerlink" title="Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer"></a>Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer</h2><p><strong>Authors:Yujiao Yang, Jing Lian, Linhui Li</strong></p>
<p>Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. Conventional mixture-of-experts (MoE) architectures suffer from suboptimal coordination dynamics, where isolated expert operations expose the model to overfitting risks. Moreover, they have not been effectively extended to attention blocks, which limits further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes the transformer model into an equivalent group of experts and applies a hierarchical routing mechanism to allocate input subspaces to specialized experts. Our approach advances MoE design with four key innovations: (1) Constructing expert groups by partitioning non-MoE models into functionally equivalent specialists (2) Developing a hierarchical routing paradigm that integrates patch-wise data selection and expert selection strategies. (3) Extending the MoE design to attention blocks. (4) Proposing a hardware-optimized parallelization scheme that exploits batched matrix multiplications for efficient expert computation. The experiments demonstrate that our UoE model surpasses Full Attention, state-of-the-art MoEs and efficient transformers in several tasks across image and natural language domains. In language modeling tasks, UoE achieves an average reduction of 2.38 in perplexity compared to the best-performing MoE method with only 76% of its FLOPs. In the Long Range Arena benchmark, it demonstrates an average score at least 0.68% higher than all comparison models, with only 50% of the FLOPs of the best MoE method. In image classification, it yields an average accuracy improvement of 1.75% over the best model while maintaining comparable FLOPs. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE">https://github.com/YujiaoYang-work/UoE</a>. </p>
<blockquote>
<p>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œéå¸¸é€‚åˆå¤§è§„æ¨¡åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å­˜åœ¨åè°ƒåŠ¨åŠ›ä¸ä½³çš„é—®é¢˜ï¼Œå­¤ç«‹çš„ä¸“å®¶æ“ä½œä¼šä½¿æ¨¡å‹é¢ä¸´è¿‡æ‹Ÿåˆé£é™©ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å°šæœªæœ‰æ•ˆåœ°æ‰©å±•åˆ°æ³¨æ„åŠ›å—ï¼Œè¿™é™åˆ¶äº†è¿›ä¸€æ­¥çš„æ•ˆç‡æ”¹è¿›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè”åˆä¸“å®¶â€ï¼ˆUoEï¼‰æ–¹æ³•ï¼Œå®ƒå°†å˜å‹å™¨æ¨¡å‹åˆ†è§£ä¸ºç­‰æ•ˆçš„ä¸“å®¶ç»„ï¼Œå¹¶åº”ç”¨åˆ†å±‚è·¯ç”±æœºåˆ¶å°†è¾“å…¥å­ç©ºé—´åˆ†é…ç»™ä¸“ä¸šä¸“å®¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥å››ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¨è¿›MoEè®¾è®¡ï¼šï¼ˆ1ï¼‰é€šè¿‡å°†éMoEæ¨¡å‹åˆ’åˆ†ä¸ºåŠŸèƒ½ç­‰æ•ˆçš„ä¸“å®¶æ¥æ„å»ºä¸“å®¶ç»„ï¼›ï¼ˆ2ï¼‰å¼€å‘äº†ä¸€ç§ç»“åˆè¡¥ä¸çº§æ•°æ®é€‰æ‹©å’Œä¸“å®¶é€‰æ‹©ç­–ç•¥çš„åˆ†å±‚è·¯ç”±èŒƒå¼ã€‚ï¼ˆ3ï¼‰å°†MoEè®¾è®¡æ‰©å±•åˆ°æ³¨æ„åŠ›å—ã€‚ï¼ˆ4ï¼‰æå‡ºäº†ä¸€ç§ç¡¬ä»¶ä¼˜åŒ–å¹¶è¡ŒåŒ–æ–¹æ¡ˆï¼Œåˆ©ç”¨æ‰¹é‡çŸ©é˜µä¹˜æ³•è¿›è¡Œé«˜æ•ˆçš„ä¸“å®¶è®¡ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„UoEæ¨¡å‹åœ¨å›¾åƒå’Œè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›æ¨¡å‹ã€æœ€æ–°MoEå’Œé«˜æ•ˆå˜å‹å™¨ã€‚åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ï¼ŒUoEä¸æ€§èƒ½æœ€ä½³çš„MoEæ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡é™ä½äº†2.38ä¸ªå›°æƒ‘åº¦ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¶76%çš„æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰ã€‚åœ¨Long Range ArenaåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒçš„å¹³å‡å¾—åˆ†è‡³å°‘æ¯”æ‰€æœ‰å¯¹æ¯”æ¨¡å‹é«˜0.68%ï¼ŒåŒæ—¶ä»…ä½¿ç”¨æœ€ä½³MoEæ–¹æ³•çš„50%çš„æµ®ç‚¹è¿ç®—ã€‚åœ¨å›¾åƒåˆ†ç±»æ–¹é¢ï¼Œä¸æœ€ä½³æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†1.75%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„æµ®ç‚¹è¿ç®—é‡ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YujiaoYang-work/UoEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02495v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Mixture-of-Expertsï¼ˆMoEï¼‰åœ¨å¤§å‹åº”ç”¨ä¸­çš„æ€§èƒ½æå‡å’Œè®¡ç®—æ•ˆç‡ä¼˜åŠ¿ï¼Œä½†å­˜åœ¨åè°ƒåŠ¨æ€ä¸ä½³å’Œæ— æ³•æœ‰æ•ˆæ‰©å±•åˆ°æ³¨æ„åŠ›å—çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Union-of-Expertsï¼ˆUoEï¼‰æ¨¡å‹ï¼Œé€šè¿‡åˆ†è§£ä¸“å®¶ç»„ã€åº”ç”¨åˆ†å±‚è·¯ç”±æœºåˆ¶ã€æ‰©å±•åˆ°æ³¨æ„åŠ›å—ä»¥åŠç¡¬ä»¶ä¼˜åŒ–å¹¶è¡ŒåŒ–æ–¹æ¡ˆç­‰å››ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¥æ”¹è¿›MoEè®¾è®¡ã€‚å®éªŒè¯æ˜ï¼ŒUoEæ¨¡å‹åœ¨å›¾åƒå’Œè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›æ¨¡å‹å’Œå½“å‰MoEæ¨¡å‹ï¼Œå®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mixture-of-Experts (MoE) æå‡æ¨¡å‹æ€§èƒ½å¹¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡åº”ç”¨ã€‚</li>
<li>ä¼ ç»ŸMoEæ¶æ„å­˜åœ¨åè°ƒåŠ¨æ€é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>Union-of-Experts (UoE) é€šè¿‡æ„å»ºä¸“å®¶ç»„ã€å¼€å‘åˆ†å±‚è·¯ç”±æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>UoEå°†MoEè®¾è®¡æ‰©å±•åˆ°æ³¨æ„åŠ›å—ï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡ã€‚</li>
<li>UoEæ¨¡å‹é€šè¿‡ç¡¬ä»¶ä¼˜åŒ–å¹¶è¡ŒåŒ–æ–¹æ¡ˆå®ç°é«˜æ•ˆä¸“å®¶è®¡ç®—ã€‚</li>
<li>UoEæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°è¶…è¶Šå…¨æ³¨æ„åŠ›æ¨¡å‹å’Œå½“å‰MoEæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee5788da6243472b1c1be9b1cb808f04" align="middle">
<img src="https://picx.zhimg.com/v2-3e7c734675c4dfc8d5dd688ae0bcb105" align="middle">
<img src="https://picx.zhimg.com/v2-9bf6b0756dd4f8becd79963bef04c226" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="JUREX-4E-Juridical-Expert-Annotated-Four-Element-Knowledge-Base-for-Legal-Reasoning"><a href="#JUREX-4E-Juridical-Expert-Annotated-Four-Element-Knowledge-Base-for-Legal-Reasoning" class="headerlink" title="JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for   Legal Reasoning"></a>JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for   Legal Reasoning</h2><p><strong>Authors:Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng</strong></p>
<p>In recent years, Large Language Models (LLMs) have been widely applied to legal tasks. To enhance their understanding of legal texts and improve reasoning accuracy, a promising approach is to incorporate legal theories. One of the most widely adopted theories is the Four-Element Theory (FET), which defines the crime constitution through four elements: Subject, Object, Subjective Aspect, and Objective Aspect. While recent work has explored prompting LLMs to follow FET, our evaluation demonstrates that LLM-generated four-elements are often incomplete and less representative, limiting their effectiveness in legal reasoning. To address these issues, we present JUREX-4E, an expert-annotated four-element knowledge base covering 155 criminal charges. The annotations follow a progressive hierarchical framework grounded in legal source validity and incorporate diverse interpretive methods to ensure precision and authority. We evaluate JUREX-4E on the Similar Charge Disambiguation task and apply it to Legal Case Retrieval. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. The dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/THUlawtech/JUREX">https://github.com/THUlawtech/JUREX</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºæ³•å¾‹ä»»åŠ¡ã€‚ä¸ºäº†å¢å¼ºå®ƒä»¬å¯¹æ³•å¾‹æ–‡æœ¬çš„ç†è§£å¹¶æé«˜æ¨ç†å‡†ç¡®æ€§ï¼Œèå…¥æ³•å¾‹ç†è®ºæ˜¯ä¸€ç§å‰æ™¯çœ‹å¥½çš„æ–¹æ³•ã€‚å…¶ä¸­é‡‡ç”¨æœ€å¹¿æ³›çš„ç†è®ºä¹‹ä¸€æ˜¯å››è¦ç´ ç†è®ºï¼ˆFETï¼‰ï¼Œå®ƒé€šè¿‡å››ä¸ªè¦ç´ å®šä¹‰çŠ¯ç½ªæ„æˆï¼šä¸»ä½“ã€å®¢ä½“ã€ä¸»è§‚æ–¹é¢å’Œå®¢è§‚æ–¹é¢ã€‚è™½ç„¶è¿‘æœŸçš„å·¥ä½œå·²ç»æ¢ç´¢äº†å¼•å¯¼LLMéµå¾ªFETï¼Œä½†æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„å››è¦ç´ å¾€å¾€ä¸å®Œæ•´ä¸”ä»£è¡¨æ€§ä¸è¶³ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ³•å¾‹æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†JUREX-4Eï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“å®¶æ³¨é‡Šçš„å››è¦ç´ çŸ¥è¯†åº“ï¼Œæ¶µç›–155é¡¹åˆ‘äº‹æŒ‡æ§ã€‚æ³¨é‡Šéµå¾ªåŸºäºæ³•å¾‹æºåˆæ³•æ€§çš„é€’è¿›åˆ†å±‚æ¡†æ¶ï¼Œå¹¶èå…¥å¤šç§è§£é‡Šæ–¹æ³•ï¼Œä»¥ç¡®ä¿ç²¾ç¡®æ€§å’Œæƒå¨æ€§ã€‚æˆ‘ä»¬åœ¨ç±»ä¼¼ç½ªåè¾¨æä»»åŠ¡ä¸Šè¯„ä¼°äº†JUREX-4Eï¼Œå¹¶å°†å…¶åº”ç”¨äºæ³•å¾‹æ¡ˆä¾‹æ£€ç´¢ã€‚å®éªŒç»“æœéªŒè¯äº†JUREX-4Eçš„é«˜è´¨é‡åŠå…¶å¯¹ä¸‹æ¸¸æ³•å¾‹ä»»åŠ¡çš„é‡å¤§å½±å“ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨æ³•å¾‹äººå·¥æ™ºèƒ½åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/THUlawtech/JUREX">https://github.com/THUlawtech/JUREX</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17166v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ä¸ºå¢å¼ºLLMå¯¹æ³•å¾‹æ–‡æœ¬çš„ç†è§£å’Œæé«˜æ¨ç†å‡†ç¡®æ€§ï¼Œèå…¥æ³•å¾‹ç†è®ºæˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚å…¶ä¸­ï¼Œè¢«å¹¿æ³›é‡‡çº³çš„ç†è®ºä¹‹ä¸€æ˜¯å››è¦ç´ ç†è®ºï¼ˆFETï¼‰ï¼Œå®ƒé€šè¿‡ä¸»ä½“ã€å®¢ä½“ã€ä¸»è§‚æ–¹é¢å’Œå®¢è§‚æ–¹é¢å››ä¸ªè¦ç´ æ¥å®šä¹‰çŠ¯ç½ªæ„æˆã€‚å°½ç®¡è¿‘æœŸç ”ç©¶å°è¯•å¼•å¯¼LLMéµå¾ªFETï¼Œä½†è¯„ä¼°æ˜¾ç¤ºLLMç”Ÿæˆå››è¦ç´ å¾€å¾€ä¸å®Œæ•´ä¸”ä»£è¡¨æ€§ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨æ³•å¾‹æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†JUREX-4Eï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«155é¡¹åˆ‘äº‹æŒ‡æ§çš„ä¸“å®¶æ³¨é‡Šå››è¦ç´ çŸ¥è¯†åº“ã€‚æ³¨é‡Šéµå¾ªåŸºäºæ³•å¾‹æºåˆæ³•æ€§çš„åˆ†å±‚æ¡†æ¶ï¼Œå¹¶èå…¥å¤šç§è§£é‡Šæ–¹æ³•ä»¥ç¡®ä¿ç²¾ç¡®æ€§å’Œæƒå¨æ€§ã€‚æˆ‘ä»¬åœ¨ç±»ä¼¼ç½ªåè¾¨è¯†ä»»åŠ¡ä¸Šè¯„ä¼°äº†JUREX-4Eï¼Œå¹¶åº”ç”¨äºæ³•å¾‹æ¡ˆä¾‹æ£€ç´¢ã€‚å®éªŒç»“æœéªŒè¯äº†JUREX-4Eçš„é«˜è´¨é‡åŠå…¶å¯¹ä¸‹æ¸¸æ³•å¾‹ä»»åŠ¡çš„æ˜¾è‘—å½±å“ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨æ³•å¾‹äººå·¥æ™ºèƒ½åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²å¹¿æ³›åº”ç”¨äºæ³•å¾‹ä»»åŠ¡ã€‚</li>
<li>èå…¥æ³•å¾‹ç†è®ºï¼Œå¦‚å››è¦ç´ ç†è®ºï¼ˆFETï¼‰ï¼Œå¯æå‡LLMså¯¹æ³•å¾‹æ–‡æœ¬çš„ç†è§£å’Œæ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>å°½ç®¡LLMså°è¯•éµå¾ªFETï¼Œä½†åœ¨ç”Ÿæˆå››è¦ç´ æ—¶å¸¸å¸¸å­˜åœ¨ä¸å®Œæ•´å’Œä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºJUREX-4Eï¼šä¸€ä¸ªåŒ…å«155é¡¹åˆ‘äº‹æŒ‡æ§çš„ä¸“å®¶æ³¨é‡Šå››è¦ç´ çŸ¥è¯†åº“ã€‚</li>
<li>JUREX-4Eçš„æ³¨é‡Šéµå¾ªåŸºäºæ³•å¾‹æºåˆæ³•æ€§çš„åˆ†å±‚æ¡†æ¶ï¼Œå¹¶ç»“åˆå¤šç§è§£é‡Šæ–¹æ³•ä»¥ç¡®ä¿ç²¾ç¡®æ€§å’Œæƒå¨æ€§ã€‚</li>
<li>JUREX-4Eåœ¨ç±»ä¼¼ç½ªåè¾¨è¯†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜è´¨é‡ï¼Œå¹¶æˆåŠŸåº”ç”¨äºæ³•å¾‹æ¡ˆä¾‹æ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-445a940822252249287bf527195070c3" align="middle">
<img src="https://picx.zhimg.com/v2-03f8d2417f9c5659250e6c5d3a489bfe" align="middle">
<img src="https://picx.zhimg.com/v2-060b97e56a2efa8cb5f296c590aaa4b9" align="middle">
<img src="https://picx.zhimg.com/v2-898e29040f632d77048071510c75aa9d" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Does-Reasoning-Introduce-Bias-A-Study-of-Social-Bias-Evaluation-and-Mitigation-in-LLM-Reasoning"><a href="#Does-Reasoning-Introduce-Bias-A-Study-of-Social-Bias-Evaluation-and-Mitigation-in-LLM-Reasoning" class="headerlink" title="Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and   Mitigation in LLM Reasoning"></a>Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and   Mitigation in LLM Reasoning</h2><p><strong>Authors:Xuyang Wu, Jinming Nian, Ting-Ruen Wei, Zhiqiang Tao, Hsin-Tai Wu, Yi Fang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled automatic generation of chain-of-thought (CoT) reasoning, leading to strong performance on tasks such as math and code. However, when reasoning steps reflect social stereotypes (e.g., those related to gender, race or age), they can reinforce harmful associations and lead to misleading conclusions. We present the first systematic evaluation of social bias within LLM-generated reasoning, focusing on reasoning language models (e.g., DeepSeek-R1, OpenAI o1) that natively produce reasoning chains as part of their answers. Using the BBQ dataset, we analyze both prediction accuracy and reasoning bias across a broad spectrum of models, including instruction-tuned and CoT-augmented variants of DeepSeek-R1 (8B&#x2F;32B), ChatGPT, and other open-source LLMs. We quantify how biased reasoning steps correlate with incorrect predictions and often lead to stereotype expression. To mitigate reasoning-induced bias, we propose Answer Distribution as Bias Proxy (ADBP), a lightweight mitigation method that detects bias by tracking how model predictions change across incremental reasoning steps. ADBP outperforms Stereotype-free Reasoning Pattern (SfRP) baseline in most cases, mitigating bias and improving the accuracy of LLM outputs. Evaluation and mitigation code is available at <a target="_blank" rel="noopener" href="https://github.com/elviswxy/LLM_reasoning_bias">https://github.com/elviswxy/LLM_reasoning_bias</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿå®ç°è‡ªåŠ¨ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿™åœ¨æ•°å­¦å’Œä»£ç ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå½“æ¨ç†æ­¥éª¤åæ˜ å‡ºç¤¾ä¼šåˆ»æ¿å°è±¡ï¼ˆä¾‹å¦‚ä¸æ€§åˆ«ã€ç§æ—æˆ–å¹´é¾„ç›¸å…³çš„åˆ»æ¿å°è±¡ï¼‰æ—¶ï¼Œå®ƒä»¬ä¼šå¼ºåŒ–æœ‰å®³çš„å…³è”å¹¶å¯¼è‡´è¯¯å¯¼æ€§çš„ç»“è®ºã€‚æˆ‘ä»¬é¦–æ¬¡å¯¹LLMç”Ÿæˆæ¨ç†ä¸­çš„ç¤¾ä¼šåè§è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨èƒ½å¤ŸåŸç”Ÿåœ°äº§å‡ºæ¨ç†é“¾ä½œä¸ºç­”æ¡ˆä¸€éƒ¨åˆ†çš„æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek-R1ã€OpenAI o1ç­‰ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨BBQæ•°æ®é›†åˆ†æäº†å¹¿æ³›æ¨¡å‹ç¾¤çš„é¢„æµ‹ç²¾åº¦å’Œæ¨ç†åè§ï¼ŒåŒ…æ‹¬DeepSeek-R1ï¼ˆ8B&#x2F;32Bï¼‰çš„æŒ‡ä»¤è°ƒæ•´å’ŒCoTå¢å¼ºç‰ˆæœ¬ã€ChatGPTä»¥åŠå…¶ä»–å¼€æºLLMã€‚æˆ‘ä»¬é‡åŒ–äº†å¸¦æœ‰åè§çš„æ¨ç†æ­¥éª¤å¦‚ä½•ä¸é”™è¯¯é¢„æµ‹ç›¸å…³è”ï¼Œå¹¶ç»å¸¸å¯¼è‡´åˆ»æ¿å°è±¡çš„è¡¨è¾¾ã€‚ä¸ºäº†ç¼“è§£æ¨ç†å¯¼è‡´çš„åè§ï¼Œæˆ‘ä»¬æå‡ºäº†ç­”æ¡ˆåˆ†å¸ƒåè§ä»£ç†ï¼ˆADBPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„ç¼“è§£æ–¹æ³•ï¼Œé€šè¿‡è·Ÿè¸ªæ¨¡å‹é¢„æµ‹åœ¨å¢é‡æ¨ç†æ­¥éª¤ä¸­çš„å˜åŒ–æ¥æ£€æµ‹åè§ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒADBPçš„è¡¨ç°ä¼˜äºæ— åˆ»æ¿æ¨ç†æ¨¡å¼ï¼ˆSfRPï¼‰ï¼Œèƒ½å¤Ÿç¼“è§£åè§å¹¶æé«˜äº†LLMè¾“å‡ºçš„å‡†ç¡®æ€§ã€‚è¯„ä¼°å’Œç¼“è§£æ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/elviswxy/LLM_reasoning_bias%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/elviswxy/LLM_reasoning_biasæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15361v3">PDF</a> EMNLP Findings</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²å®ç°äº†è‡ªåŠ¨ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œä»è€Œåœ¨æ•°å­¦å’Œä»£ç ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“æ¨ç†æ­¥éª¤åæ˜ ç¤¾ä¼šåˆ»æ¿å°è±¡ï¼ˆå¦‚æ€§åˆ«ã€ç§æ—æˆ–å¹´é¾„ï¼‰æ—¶ï¼Œå®ƒä»¬ä¼šå¼ºåŒ–æœ‰å®³çš„å…³è”å¹¶å¯¼è‡´è¯¯å¯¼æ€§çš„ç»“è®ºã€‚æœ¬æ–‡å¯¹LLMç”Ÿæˆæ¨ç†ä¸­çš„ç¤¾ä¼šåè§è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨èƒ½å¤Ÿä½œä¸ºç­”æ¡ˆä¸€éƒ¨åˆ†ç”Ÿæˆæ¨ç†é“¾çš„æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ã€OpenAI o1ç­‰ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨BBQæ•°æ®é›†åˆ†æäº†åŒ…æ‹¬DeepSeek-R1ï¼ˆ8B&#x2F;32Bï¼‰çš„æŒ‡ä»¤è°ƒä¼˜å’ŒCoTå¢å¼ºç‰ˆæœ¬ã€ChatGPTä»¥åŠå…¶ä»–å¼€æºLLMçš„é¢„æµ‹ç²¾åº¦å’Œæ¨ç†åè§ã€‚æˆ‘ä»¬é‡åŒ–äº†æœ‰åè§çš„æ¨ç†æ­¥éª¤ä¸é”™è¯¯é¢„æµ‹ä¹‹é—´çš„å…³è”ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å¸¸å¸¸å¯¼è‡´åˆ»æ¿å°è±¡çš„è¡¨è¾¾ã€‚ä¸ºäº†ç¼“è§£æ¨ç†å¯¼è‡´çš„åè§ï¼Œæˆ‘ä»¬æå‡ºäº†ç­”æ¡ˆåˆ†å¸ƒåè§ä»£ç†ï¼ˆADBPï¼‰è¿™ä¸€è½»é‡çº§ç¼“è§£æ–¹æ³•ï¼Œå®ƒé€šè¿‡è·Ÿè¸ªæ¨¡å‹é¢„æµ‹åœ¨å¢é‡æ¨ç†æ­¥éª¤ä¸­çš„å˜åŒ–æ¥æ£€æµ‹åè§ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒADBPçš„è¡¨ç°ä¼˜äºæ— åˆ»æ¿æ¨ç†æ¨¡å¼ï¼ˆSfRPï¼‰åŸºå‡†æµ‹è¯•ï¼Œç¼“è§£åè§å¹¶æé«˜äº†LLMè¾“å‡ºçš„å‡†ç¡®æ€§ã€‚è¯„ä¼°ä¸ç¼“è§£ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/elviswxy/LLM_reasoning_bias%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/elviswxy/LLM_reasoning_biasè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½è‡ªåŠ¨ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œä½†åœ¨æ¶‰åŠç¤¾ä¼šåˆ»æ¿å°è±¡çš„æ¨ç†æ­¥éª¤ä¸­å¯èƒ½å¼ºåŒ–æœ‰å®³çš„å…³è”ã€‚</li>
<li>ä½¿ç”¨BBQæ•°æ®é›†å¯¹å¤šç§LLMçš„é¢„æµ‹ç²¾åº¦å’Œæ¨ç†åè§è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>æœ‰åè§çš„æ¨ç†æ­¥éª¤å¸¸ä¸é”™è¯¯é¢„æµ‹ç›¸å…³ï¼Œå¹¶å¯èƒ½å¯¼è‡´åˆ»æ¿å°è±¡çš„è¡¨è¾¾ã€‚</li>
<li>æå‡ºäº†ç­”æ¡ˆåˆ†å¸ƒåè§ä»£ç†ï¼ˆADBPï¼‰æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å¹¶ç¼“è§£LLMä¸­çš„æ¨ç†åè§ã€‚</li>
<li>ADBPåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°ä¼˜äºç°æœ‰çš„æ— åˆ»æ¿æ¨ç†æ¨¡å¼ï¼ˆSfRPï¼‰æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è·Ÿè¸ªæ¨¡å‹é¢„æµ‹åœ¨å¢é‡æ¨ç†æ­¥éª¤ä¸­çš„å˜åŒ–ï¼ŒADBPæé«˜äº†LLMè¾“å‡ºçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b52328a0d663dcb52c71edb9fc5fedd" align="middle">
<img src="https://picx.zhimg.com/v2-6ab3f31da8b61e491606ed44cc37eec8" align="middle">
<img src="https://picx.zhimg.com/v2-31878d6aa876a122efed2c5cdd4c879f" align="middle">
<img src="https://picx.zhimg.com/v2-1e6ec32ab31ce55971f1ff4ef40aed73" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DeepResonance-Enhancing-Multimodal-Music-Understanding-via-Music-centric-Multi-way-Instruction-Tuning"><a href="#DeepResonance-Enhancing-Multimodal-Music-Understanding-via-Music-centric-Multi-way-Instruction-Tuning" class="headerlink" title="DeepResonance: Enhancing Multimodal Music Understanding via   Music-centric Multi-way Instruction Tuning"></a>DeepResonance: Enhancing Multimodal Music Understanding via   Music-centric Multi-way Instruction Tuning</h2><p><strong>Authors:Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji</strong></p>
<p>Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the modelâ€™s ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-LLM fusion Transformer to enhance modality fusion prior to input into text LLMs, tailoring for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We open-source the codes, models and datasets we constructed: github.com&#x2F;sony&#x2F;DeepResonance. </p>
<blockquote>
<p>éŸ³ä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†éŸ³ä¹ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠæ¨¡å‹åˆ†æå’Œè§£é‡Šå„ç§éŸ³ä¹å…ƒç´ çš„èƒ½åŠ›ã€‚è¿™äº›æ”¹è¿›ä¸»è¦é›†ä¸­åœ¨æ•´åˆéŸ³ä¹å’Œæ–‡æœ¬è¾“å…¥ä¸Šã€‚ç„¶è€Œï¼Œç»“åˆå›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾ç­‰é¢å¤–æ¨¡æ€ä»¥å¢å¼ºéŸ³ä¹ç†è§£çš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†DeepResonanceï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€éŸ³ä¹ç†è§£LLMï¼Œé€šè¿‡å¤šå‘æŒ‡ä»¤è°ƒæ•´ä¸å¤šå‘å¯¹é½çš„éŸ³ä¹ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†Music4way-MI2Tã€Music4way-MV2Tå’ŒMusic4way-Any2Tä¸‰ä¸ªæ•°æ®é›†ï¼Œè¿™ä¸‰ä¸ªæ•°æ®é›†æ˜¯ä¸ºäº†è®©DeepResonanceèƒ½å¤Ÿæ•´åˆè§†è§‰å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾å†…å®¹è€Œè®¾è®¡çš„4å‘è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šé‡‡æ ·ImageBindåµŒå…¥å’Œé¢„LLMèåˆTransformerï¼Œä»¥å¢å¼ºæ¨¡æ€èåˆï¼Œç„¶åè¾“å…¥æ–‡æœ¬LLMï¼Œä¸ºå¤šè§’åº¦æŒ‡ä»¤è°ƒæ•´é‡èº«å®šåˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å…­ä¸ªéŸ³ä¹ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†è¾…åŠ©æ¨¡æ€çš„ç›Šå¤„å’ŒDeepResonanceçš„ç»“æ„ä¼˜åŠ¿ã€‚æˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬æ„å»ºçš„æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ï¼šgithub.com&#x2F;sony&#x2F;DeepResonanceã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12623v3">PDF</a> Accepted to EMNLP 2025 main conference</p>
<p><strong>Summary</strong><br>éŸ³ä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†éŸ³ä¹ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åˆ†æå’Œè§£é‡Šå„ç§éŸ³ä¹å…ƒç´ ã€‚ç ”ç©¶é›†ä¸­åœ¨æ•´åˆéŸ³ä¹å’Œæ–‡æœ¬è¾“å…¥ï¼Œä½†èå…¥å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾ç­‰é¢å¤–æ¨¡æ€çš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæå‡ºDeepResonanceå¤šæ¨¡æ€éŸ³ä¹ç†è§£LLMï¼Œé€šè¿‡å¤šå‘æŒ‡ä»¤è°ƒæ•´ä¸å¤šå‘å¯¹é½çš„éŸ³ä¹ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ•°æ®å¾®è°ƒã€‚ä¸ºæ­¤æ„å»ºäº†Music4way-MI2Tã€Music4way-MV2Tå’ŒMusic4way-Any2Tä¸‰ä¸ª4å‘è®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†ï¼Œä½¿DeepResonanceèƒ½å¤Ÿèåˆè§†è§‰å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾å†…å®¹ã€‚è¿˜å¼•å…¥äº†å¤šé‡‡æ ·ImageBindåµŒå…¥å’Œé¢„LLMèåˆTransformerï¼Œä»¥åŠ å¼ºæ¨¡æ€èåˆï¼Œç„¶åè¾“å…¥æ–‡æœ¬LLMä¸­ï¼Œä¸ºå¤šæ–¹æŒ‡ä»¤è°ƒæ•´é‡èº«å®šåˆ¶ã€‚è¯¥æ¨¡å‹åœ¨å…­é¡¹éŸ³ä¹ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†è¾…åŠ©æ¨¡æ€çš„ç›Šå¤„å’ŒDeepResonanceçš„ç»“æ„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬å…¬å¼€äº†æ„å»ºçš„æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ï¼šgithub.com&#x2F;sony&#x2F;DeepResonanceã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³ä¹LLMæœ€æ–°è¿›å±•æé«˜äº†éŸ³ä¹ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åˆ†æå’Œè§£é‡ŠéŸ³ä¹å…ƒç´ ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨æ•´åˆéŸ³ä¹å’Œæ–‡æœ¬è¾“å…¥ï¼Œä½†é¢å¤–æ¨¡æ€ï¼ˆå¦‚å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾ï¼‰çš„æ½œåŠ›æœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºDeepResonanceå¤šæ¨¡æ€éŸ³ä¹ç†è§£LLMï¼Œé€šè¿‡å¤šå‘æŒ‡ä»¤è°ƒæ•´ä¸å¤šæ¨¡æ€æ•°æ®å¾®è°ƒã€‚</li>
<li>æ„å»ºäº†ä¸‰ä¸ª4å‘è®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†ï¼Œä»¥èåˆè§†è§‰å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾å†…å®¹ã€‚</li>
<li>å¼•å…¥å¤šé‡‡æ ·ImageBindåµŒå…¥å’Œé¢„LLMèåˆTransformerä»¥åŠ å¼ºæ¨¡æ€èåˆã€‚</li>
<li>DeepResonanceåœ¨å¤šé¡¹éŸ³ä¹ç†è§£ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4775337595e9c11d42804af7801f6ae8" align="middle">
<img src="https://picx.zhimg.com/v2-18b345fefa77cd31b1a2af403c8b40cd" align="middle">
<img src="https://picx.zhimg.com/v2-ea0b51967c2c111949d921ea37c7bd25" align="middle">
<img src="https://picx.zhimg.com/v2-bfe1f85d3135d516b1ad4f94b0d0ba9e" align="middle">
<img src="https://picx.zhimg.com/v2-586e8675bc485e131508b7bfe74015a6" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators"><a href="#Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators" class="headerlink" title="Flatten Graphs as Sequences: Transformers are Scalable Graph Generators"></a>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</h2><p><strong>Authors:Dexiong Chen, Markus Krimmel, Karsten Borgwardt</strong></p>
<p>We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/AutoGraph">https://github.com/BorgwardtLab/AutoGraph</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†AutoGraphï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ä»…è§£ç å™¨è½¬æ¢å™¨è¿›è¡Œå±æ€§å›¾ç”Ÿæˆçš„å¯æ‰©å±•è‡ªå›å½’æ¨¡å‹ã€‚é€šè¿‡å¯é€†è¿‡ç¨‹å°†å›¾å±•å¹³ä¸ºéšæœºä»¤ç‰Œåºåˆ—ï¼ŒAutoGraphèƒ½å¤Ÿå»ºæ¨¡å›¾åºåˆ—ï¼Œè€Œæ— éœ€ä¾èµ–è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é™„åŠ èŠ‚ç‚¹ç‰¹å¾ï¼Œè¿™ä¸åŸºäºæ‰©æ•£çš„æ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚è¿™å¯¼è‡´é‡‡æ ·å¤æ‚æ€§å’Œåºåˆ—é•¿åº¦ä¸è¾¹çš„æ•°é‡å‘ˆæœ€ä¼˜çº¿æ€§å…³ç³»ï¼Œå¯¹äºå¤§å‹ç¨€ç–å›¾è€Œè¨€ï¼Œå®ƒå…·æœ‰å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚AutoGraphçš„ä¸€ä¸ªå…³é”®æˆåŠŸå› ç´ æ˜¯å®ƒçš„åºåˆ—å‰ç¼€ä»£è¡¨è¯±å¯¼å­å›¾ï¼Œä¸è¯­è¨€å»ºæ¨¡ä¸­çš„å¥å­ç›´æ¥ç›¸å…³ã€‚åœ¨åˆæˆå’Œåˆ†å­åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAutoGraphå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”Ÿæˆé€Ÿåº¦æ¯”é¢†å…ˆçš„æ‰©æ•£æ¨¡å‹å¿«100å€ï¼Œè®­ç»ƒé€Ÿåº¦å¿«3å€ã€‚å®ƒè¿˜æ”¯æŒå­ç»“æ„æ¡ä»¶ä¸‹çš„ç”Ÿæˆè€Œæ— éœ€å¾®è°ƒï¼Œå¹¶æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„è¿ç§»æ€§ï¼Œåœ¨å»ºç«‹å›¾å½¢åŸºç¡€æ¨¡å‹æ–¹é¢å¥ å®šäº†è¯­è¨€å»ºæ¨¡å’Œå›¾ç”Ÿæˆä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/AutoGraph%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BorgwardtLab/AutoGraphæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02216v2">PDF</a> To appear at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>AutoGraphæ˜¯ä¸€ä¸ªåˆ©ç”¨è§£ç å™¨ä¸ºä¸»çš„Transformerè¿›è¡Œå±æ€§å›¾ç”Ÿæˆçš„è§„æ¨¡åŒ–è‡ªå›å½’æ¨¡å‹ã€‚å®ƒé€šè¿‡å¯é€†è¿‡ç¨‹å°†å›¾å±•å¹³ä¸ºéšæœºä»¤ç‰Œåºåˆ—ï¼Œæ— éœ€ä¾èµ–æ˜‚è´µçš„èŠ‚ç‚¹ç‰¹å¾è®¡ç®—ï¼Œä¸åŒäºåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œå…¶é‡‡æ ·å¤æ‚æ€§å’Œåºåˆ—é•¿åº¦ä¸è¾¹çš„æ•°é‡å‘ˆæœ€ä¼˜çº¿æ€§å…³ç³»ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡ç¨€ç–å›¾çš„å»ºæ¨¡ã€‚AutoGraphçš„å…³é”®æˆåŠŸå› ç´ åœ¨äºå…¶åºåˆ—å‰ç¼€è¡¨ç¤ºè¯±å¯¼å­å›¾ï¼Œä¸è¯­è¨€å»ºæ¨¡ä¸­çš„å­å¥å»ºç«‹ç›´æ¥è”ç³»ã€‚å®è¯æ˜¾ç¤ºï¼ŒAutoGraphåœ¨åˆæˆå’Œåˆ†å­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œç”Ÿæˆé€Ÿåº¦æœ€å¿«è¾¾é¢†å…ˆæ‰©æ•£æ¨¡å‹çš„100å€ï¼Œè®­ç»ƒé€Ÿåº¦æœ€å¿«è¾¾3å€ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒå­ç»“æ„æ¡ä»¶ä¸‹çš„ç”Ÿæˆæ— éœ€å¾®è°ƒï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å¯è¿ç§»æ€§ï¼Œä¸ºå›¾åŸºç¡€æ¨¡å‹çš„å»ºç«‹å¥ å®šäº†è¯­è¨€å»ºæ¨¡å’Œå›¾ç”Ÿæˆä¹‹é—´çš„æ¡¥æ¢ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/AutoGraph%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BorgwardtLab/AutoGraphä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoGraphæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºå±æ€§å›¾ç”Ÿæˆã€‚</li>
<li>é€šè¿‡å°†å›¾å±•å¹³ä¸ºä»¤ç‰Œåºåˆ—ï¼Œå®ç°äº†å›¾çš„åºåˆ—å»ºæ¨¡ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸éœ€è¦ä¾èµ–æ˜‚è´µçš„èŠ‚ç‚¹ç‰¹å¾è®¡ç®—ã€‚</li>
<li>é‡‡æ ·å¤æ‚æ€§å’Œåºåˆ—é•¿åº¦ä¸è¾¹çš„æ•°é‡å‘ˆçº¿æ€§å…³ç³»ï¼Œé€‚åˆå¤§è§„æ¨¡ç¨€ç–å›¾ã€‚</li>
<li>AutoGraphåºåˆ—å‰ç¼€ä»£è¡¨è¯±å¯¼å­å›¾ï¼Œä¸è¯­è¨€å»ºæ¨¡ä¸­çš„å­å¥æœ‰ç›´æ¥è”ç³»ã€‚</li>
<li>åœ¨åˆæˆå’Œåˆ†å­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œç”Ÿæˆå’Œè®­ç»ƒé€Ÿåº¦å‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff3cdfea513861a7ac4a4922b16a631c" align="middle">
<img src="https://picx.zhimg.com/v2-f90ebbb608cd2c69b728209f8567bbc9" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Biology-Instructions-A-Dataset-and-Benchmark-for-Multi-Omics-Sequence-Understanding-Capability-of-Large-Language-Models"><a href="#Biology-Instructions-A-Dataset-and-Benchmark-for-Multi-Omics-Sequence-Understanding-Capability-of-Large-Language-Models" class="headerlink" title="Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence   Understanding Capability of Large Language Models"></a>Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence   Understanding Capability of Large Language Models</h2><p><strong>Authors:Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye</strong></p>
<p>Large language models (LLMs) have shown remarkable capabilities in general domains, but their application to multi-omics biology remains underexplored. To address this gap, we introduce Biology-Instructions, the first large-scale instruction-tuning dataset for multi-omics biological sequences, including DNA, RNA, proteins, and multi-molecules. This dataset bridges LLMs and complex biological sequence-related tasks, enhancing their versatility and reasoning while maintaining conversational fluency. We also highlight significant limitations of current state-of-the-art LLMs on multi-omics tasks without specialized training. To overcome this, we propose ChatMultiOmics, a strong baseline with a novel three-stage training pipeline, demonstrating superior biological understanding through Biology-Instructions. Both resources are publicly available, paving the way for better integration of LLMs in multi-omics analysis. The Biology-Instructions is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/hhnqqq/Biology-Instructions">https://github.com/hhnqqq/Biology-Instructions</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤šç»„å­¦ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”Ÿç‰©å­¦æŒ‡ä»¤æ•°æ®é›†ï¼ˆBiology-Instructionsï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šç»„å­¦ç”Ÿç‰©åºåˆ—çš„å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ŒåŒ…æ‹¬DNAã€RNAã€è›‹ç™½è´¨å’Œå¤šç§åˆ†å­ã€‚è¯¥æ•°æ®é›†å°†LLMä¸å¤æ‚çš„ç”Ÿç‰©åºåˆ—ç›¸å…³ä»»åŠ¡è”ç³»èµ·æ¥ï¼Œå¢å¼ºäº†å®ƒä»¬çš„é€šç”¨æ€§å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹è¯çš„æµç•…æ€§ã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†å½“å‰æœ€å…ˆè¿›çš„LLMåœ¨å¤šç»„å­¦ä»»åŠ¡ä¸Šå­˜åœ¨çš„é‡å¤§å±€é™æ€§ï¼Œæ²¡æœ‰ç»è¿‡ä¸“é—¨çš„è®­ç»ƒã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ChatMultiOmicsè¿™ä¸€å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå®ƒé‡‡ç”¨äº†æ–°å‹çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ç”Ÿç‰©å­¦æŒ‡ä»¤å±•ç¤ºäº†å“è¶Šçš„ç”Ÿç‰©ç†è§£åŠ›ã€‚è¿™ä¸¤ç§èµ„æºå‡å¯å…¬å¼€è®¿é—®ï¼Œä¸ºå¤šç»„å­¦åˆ†æä¸­LLMçš„æ›´å¥½é›†æˆé“ºå¹³äº†é“è·¯ã€‚ç”Ÿç‰©å­¦æŒ‡ä»¤å…¬å¼€å¯ç”¨çš„ç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/hhnqqq/Biology-Instructions">https://github.com/hhnqqq/Biology-Instructions</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19191v2">PDF</a> EMNLP 2025 findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†åœ¨å¤šç»„å­¦ç”Ÿç‰©å­¦é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºç”Ÿç‰©å­¦æŒ‡ä»¤æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šç»„å­¦ç”Ÿç‰©åºåˆ—çš„å¤§å‹æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œæ¶µç›–DNAã€RNAã€è›‹ç™½è´¨å’Œå¤šåˆ†å­ã€‚è¯¥æ•°æ®é›†å¼ºåŒ–äº†LLMä¸å¤æ‚ç”Ÿç‰©åºåˆ—ç›¸å…³ä»»åŠ¡çš„è”ç³»ï¼Œæé«˜äº†å…¶é€šç”¨æ€§å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹è¯æµç•…æ€§ã€‚æˆ‘ä»¬è¿˜æŒ‡å‡ºäº†å½“å‰æœ€å…ˆè¿›LLMåœ¨å¤šç»„å­¦ä»»åŠ¡ä¸Šçš„é‡è¦å±€é™æ€§ï¼Œå¹¶æå‡ºé€šè¿‡ChatMultiOmicsè¿™ä¸€å¼ºå¤§çš„åŸºçº¿æ¨¡å‹å’Œæ–°å‹ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“æ¥å…‹æœè¿™äº›å±€é™æ€§ï¼Œé€šè¿‡ç”Ÿç‰©å­¦æŒ‡ä»¤å±•ç¤ºå‡ºè‰²çš„ç”Ÿç‰©å­¦ç†è§£åŠ›ã€‚ä¸¤ä¸ªèµ„æºå‡å…¬å¼€å¯ç”¨ï¼Œä¸ºå¤šç»„å­¦åˆ†æä¸­LLMçš„æ›´å¥½é›†æˆé“ºå¹³äº†é“è·¯ã€‚ç”Ÿç‰©å­¦æŒ‡ä»¤æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hhnqqq/Biology-Instructions%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hhnqqq/Biology-Instructionsè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç»„å­¦ç”Ÿç‰©å­¦é¢†åŸŸçš„åº”ç”¨å°šå¾…æ¢ç´¢ã€‚</li>
<li>ç”Ÿç‰©å­¦æŒ‡ä»¤æ•°æ®é›†æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šç»„å­¦ç”Ÿç‰©åºåˆ—çš„å¤§å‹æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†æé«˜äº†LLMå¤„ç†å¤æ‚ç”Ÿç‰©åºåˆ—ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶ä¿æŒäº†å¯¹è¯çš„æµç•…æ€§ã€‚</li>
<li>å½“å‰LLMåœ¨å¤šç»„å­¦ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ChatMultiOmicsæ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œé€šè¿‡æ–°å‹ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“æé«˜LLMåœ¨ç”Ÿç‰©å­¦é¢†åŸŸçš„ç†è§£åŠ›ã€‚</li>
<li>ç”Ÿç‰©å­¦æŒ‡ä»¤æ•°æ®é›†å’ŒChatMultiOmicséƒ½ä¸ºå¤šç»„å­¦åˆ†æä¸­LLMçš„é›†æˆæä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-faca66cd35ba9c5e222e7fe3348df459" align="middle">
<img src="https://picx.zhimg.com/v2-86ecd88b701a0c5e0170e4da5dc8275b" align="middle">
<img src="https://picx.zhimg.com/v2-39d14c5e2fbe764ac0a69c44ecbac731" align="middle">
<img src="https://picx.zhimg.com/v2-e882e92ac596b701d8c9c35dbefc8b67" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Bias-Similarity-Measurement-A-Black-Box-Audit-of-Fairness-Across-LLMs"><a href="#Bias-Similarity-Measurement-A-Black-Box-Audit-of-Fairness-Across-LLMs" class="headerlink" title="Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs"></a>Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs</h2><p><strong>Authors:Hyejun Jeong, Shiqing Ma, Amir Houmansadr</strong></p>
<p>Large Language Models (LLMs) reproduce social biases, yet prevailing evaluations score models in isolation, obscuring how biases persist across families and releases. We introduce Bias Similarity Measurement (BSM), which treats fairness as a relational property between models, unifying scalar, distributional, behavioral, and representational signals into a single similarity space. Evaluating 30 LLMs on 1M+ prompts, we find that instruction tuning primarily enforces abstention rather than altering internal representations; small models gain little accuracy and can become less fair under forced choice; and open-weight models can match or exceed proprietary systems. Family signatures diverge: Gemma favors refusal, LLaMA 3.1 approaches neutrality with fewer refusals, and converges toward abstention-heavy behavior overall. Counterintuitively, Gemma 3 Instruct matches GPT-4-level fairness at far lower cost, whereas Geminiâ€™s heavy abstention suppresses utility. Beyond these findings, BSM offers an auditing workflow for procurement, regression testing, and lineage screening, and extends naturally to code and multilingual settings. Our results reframe fairness not as isolated scores but as comparative bias similarity, enabling systematic auditing of LLM ecosystems. Code available at <a target="_blank" rel="noopener" href="https://github.com/HyejunJeong/bias_llm">https://github.com/HyejunJeong/bias_llm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šå¤åˆ¶ç¤¾ä¼šåè§ï¼Œç„¶è€Œç°è¡Œçš„è¯„ä¼°æ–¹æ³•éƒ½æ˜¯å­¤ç«‹åœ°è¯„ä¼°æ¨¡å‹ï¼Œè¿™æ©ç›–äº†åè§å¦‚ä½•åœ¨ä¸åŒå®¶æ—å’Œç‰ˆæœ¬ä¹‹é—´æŒç»­å­˜åœ¨ã€‚æˆ‘ä»¬å¼•å…¥äº†åè§ç›¸ä¼¼æ€§åº¦é‡ï¼ˆBSMï¼‰ï¼Œå®ƒå°†å…¬å¹³æ€§è§†ä¸ºæ¨¡å‹ä¹‹é—´çš„å…³è”å±æ€§ï¼Œå°†æ ‡é‡ã€åˆ†å¸ƒã€è¡Œä¸ºå’Œè¡¨ç¤ºä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„ç›¸ä¼¼åº¦ç©ºé—´ä¸­ã€‚æˆ‘ä»¬å¯¹30ä¸ªLLMè¿›è¡Œäº†è¶…è¿‡100ä¸‡æ¡æç¤ºçš„è¯„ä¼°ï¼Œå‘ç°æŒ‡ä»¤å¾®è°ƒä¸»è¦å¼ºåˆ¶æ‰§è¡Œå›é¿è€Œä¸æ˜¯æ”¹å˜å†…éƒ¨è¡¨ç¤ºï¼›å°å‹æ¨¡å‹åœ¨å¼ºåˆ¶é€‰æ‹©ä¸‹çš„å‡†ç¡®ç‡å‡ ä¹æ²¡æœ‰æé«˜ï¼Œè€Œä¸”å¯èƒ½ä¼šå˜å¾—ä¸é‚£ä¹ˆå…¬å¹³ï¼›è€Œå¼€æ”¾å¼æƒé‡æ¨¡å‹å¯ä»¥åŒ¹é…ç”šè‡³è¶…è¿‡ä¸“æœ‰ç³»ç»Ÿã€‚ä¸åŒå®¶æ—çš„ç‰¹æ€§ä¹Ÿæœ‰æ‰€ä¸åŒï¼šGemmaæ›´å€¾å‘äºæ‹’ç»ï¼ŒLLaMA 3.1è¶‹å‘äºä¸­æ€§ä½†æ‹’ç»è¾ƒå°‘ï¼Œæ€»ä½“è€Œè¨€è¶‹å‘äºé¿å…è¿‡åº¦çš„æ‹’ç»è¡Œä¸ºã€‚æœ‰äº›åç›´è§‰çš„æ˜¯ï¼ŒGemma 3 Instructèƒ½å¤Ÿä»¥æ›´ä½çš„æˆæœ¬è¾¾åˆ°GPT-4çº§åˆ«çš„å…¬å¹³æ€§ï¼Œè€ŒGeminiçš„è¿‡åº¦å›é¿è¡Œä¸ºæŠ‘åˆ¶äº†å®ç”¨æ€§ã€‚é™¤äº†è¿™äº›å‘ç°ä¹‹å¤–ï¼ŒBSMè¿˜æä¾›äº†é‡‡è´­ã€å›å½’æµ‹è¯•å’Œè°±ç³»ç­›æŸ¥çš„å®¡è®¡å·¥ä½œæµç¨‹ï¼Œå¹¶è‡ªç„¶åœ°æ‰©å±•åˆ°ä»£ç å’Œå¤šè¯­è¨€ç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœé‡æ–°å®šä¹‰äº†å…¬å¹³æ€§çš„æ¦‚å¿µï¼Œä¸æ˜¯å­¤ç«‹çš„åˆ†æ•°ï¼Œè€Œæ˜¯æ¯”è¾ƒåè§ç›¸ä¼¼æ€§ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°å®¡è®¡LLMç”Ÿæ€ç³»ç»Ÿã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HyejunJeong/bias_llm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HyejunJeong/bias_llmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12010v4">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/HyejunJeong/bias_llm">https://github.com/HyejunJeong/bias_llm</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç¤¾ä¼šåè§å¤åˆ¶é—®é¢˜ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•é€šå¸¸å­¤ç«‹åœ°è¯„ä¼°æ¨¡å‹ï¼Œå¯¼è‡´éš¾ä»¥å‘ç°åè§å¦‚ä½•åœ¨ä¸åŒçš„å®¶æ—å’Œç‰ˆæœ¬ä¹‹é—´æŒç»­å­˜åœ¨ã€‚æœ¬æ–‡æå‡ºäº†åè§ç›¸ä¼¼æ€§åº¦é‡ï¼ˆBSMï¼‰æ–¹æ³•ï¼Œå°†å…¬å¹³æ€§è§†ä¸ºæ¨¡å‹ä¹‹é—´çš„å…³ç³»å±æ€§ï¼Œå°†æ ‡é‡ã€åˆ†å¸ƒã€è¡Œä¸ºå’Œè¡¨ç¤ºä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ªç›¸ä¼¼æ€§ç©ºé—´ä¸­ã€‚é€šè¿‡å¯¹30ä¸ªLLMæ¨¡å‹è¿›è¡Œè¶…è¿‡100ä¸‡ä¸ªæç¤ºè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°æŒ‡ä»¤å¾®è°ƒä¸»è¦å¼ºåˆ¶æ‰§è¡Œå›é¿è€Œéæ”¹å˜å†…éƒ¨è¡¨ç¤ºï¼›å°å‹æ¨¡å‹çš„å‡†ç¡®æ€§æé«˜ä¸å¤šï¼Œä¸”åœ¨å¼ºåˆ¶é€‰æ‹©ä¸‹å¯èƒ½å˜å¾—æ›´ä¸å…¬å¹³ï¼›è€Œå¼€æ”¾æƒé‡æ¨¡å‹å¯ä»¥è¾¾åˆ°æˆ–è¶…è¿‡ä¸“æœ‰ç³»ç»Ÿçš„æ°´å¹³ã€‚ä¸åŒæ¨¡å‹å®¶æ—è¡¨ç°å‡ºä¸åŒçš„ç‰¹ç‚¹ï¼Œå¦‚Gemmaå€¾å‘äºæ‹’ç»ï¼ŒLLaMA 3.1è¶‹äºä¸­ç«‹ä¸”æ‹’ç»è¾ƒå°‘ï¼Œæ•´ä½“è¡¨ç°å‡ºå€¾å‘äºå›é¿çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒBSMæ–¹æ³•ä¸ºé‡‡è´­ã€å›å½’æµ‹è¯•å’Œè°±ç³»ç­›é€‰æä¾›äº†å®¡è®¡å·¥ä½œæµç¨‹ï¼Œå¹¶è‡ªç„¶åœ°æ‰©å±•åˆ°ä»£ç å’Œå¤šè¯­è¨€ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶ç»“æœé‡æ–°å®šä¹‰äº†å…¬å¹³æ€§ï¼Œå°†å…¶è§†ä¸ºæ¯”è¾ƒæ€§çš„åè§ç›¸ä¼¼æ€§ï¼Œä½¿LLMç”Ÿæ€ç³»ç»Ÿçš„ç³»ç»Ÿæ€§å®¡è®¡æˆä¸ºå¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç¤¾ä¼šåè§å¤åˆ¶é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­¤ç«‹åœ°è¯„ä¼°æ¨¡å‹ï¼Œå¯¼è‡´éš¾ä»¥å‘ç°åè§åœ¨ä¸åŒå®¶æ—å’Œç‰ˆæœ¬é—´çš„æŒç»­å­˜åœ¨ã€‚</li>
<li>æå‡ºäº†åè§ç›¸ä¼¼æ€§åº¦é‡ï¼ˆBSMï¼‰æ–¹æ³•ï¼Œå°†å…¬å¹³æ€§è§†ä¸ºæ¨¡å‹é—´çš„å…³ç³»å±æ€§ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒä¸»è¦å¼ºåˆ¶æ‰§è¡Œå›é¿è€Œéæ”¹å˜å†…éƒ¨è¡¨ç¤ºã€‚</li>
<li>ä¸åŒLLMæ¨¡å‹å®¶æ—å±•ç°å‡ºä¸åŒçš„åè§ç‰¹ç‚¹ã€‚</li>
<li>BSMæ–¹æ³•æä¾›äº†å®¡è®¡å·¥ä½œæµç¨‹ï¼Œé€‚ç”¨äºé‡‡è´­ã€å›å½’æµ‹è¯•å’Œè°±ç³»ç­›é€‰ã€‚</li>
<li>æœ¬ç ”ç©¶é‡æ–°å®šä¹‰äº†å…¬å¹³æ€§ä¸ºæ¯”è¾ƒæ€§çš„åè§ç›¸ä¼¼æ€§ï¼Œä½¿LLMç”Ÿæ€ç³»ç»Ÿçš„ç³»ç»Ÿæ€§å®¡è®¡æˆä¸ºå¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed42af4736700f6697ca0609f27ec592" align="middle">
<img src="https://picx.zhimg.com/v2-65acba25d7ce8e69fd1353dd66fe0474" align="middle">
<img src="https://picx.zhimg.com/v2-8edf0020eb4bdc77058f686aada6e8be" align="middle">
<img src="https://picx.zhimg.com/v2-e66006fd41f537144ead5d7ff5a746e1" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-960c312ae378e24931ae26d3bc3ccc05" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Collab-Overcooked Benchmarking and Evaluating Large Language Models as   Collaborative Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-29/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-65ba2d44c4c85be08dd38267db89b7fc" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-29  Enrich-on-Graph Query-Graph Alignment for Complex Reasoning with LLM   Enriching
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
