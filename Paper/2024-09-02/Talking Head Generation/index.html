<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-02  Mini-Omni Language Models Can Hear, Talk While Thinking in Streaming">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-09-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-09-02-更新"><a href="#2024-09-02-更新" class="headerlink" title="2024-09-02 更新"></a>2024-09-02 更新</h1><h2 id="Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming"><a href="#Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming" class="headerlink" title="Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"></a>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</h2><p><strong>Authors:Zhifei Xie, Changqiao Wu</strong></p>
<p>Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model’s language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method “Any Model Can Talk”. We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16725v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong><br>提出“Any Model Can Talk”训练方法，实现实时语音交互的Mini-Omni模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4o模型达到近人类自然流畅度，实现实时人机对话。</li>
<li>实时对话需要模型具备音频推理和流式生成输出能力。</li>
<li>现有学术模型依赖TTS系统，存在延迟问题。</li>
<li>Mini-Omni是音频基于的端到端对话模型，实现实时语音交互。</li>
<li>提出文本指令语音生成方法，并采用批并行策略提升性能。</li>
<li>保留模型语言能力，最小化性能退化。</li>
<li>引入VoiceAssistant-400K数据集，优化语音输出模型。</li>
<li>Mini-Omni是首个完全端到端、开源的实时语音交互模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题</strong>： Mini-Omni: 语言模型能听会说（基于流式处理的思考）</p>
</li>
<li><p><strong>作者</strong>： Zhifei Xie（谢智飞）和Changqiao Wu（吴昌桥）。</p>
</li>
<li><p><strong>所属机构</strong>： 第一作者Zhifei Xie来自清华大学。</p>
</li>
<li><p><strong>关键词</strong>： Mini-Omni模型，实时语音交互，流式处理，语言模型，语音合成，端对端模型。</p>
</li>
<li><p><strong>链接</strong>： 论文链接：暂未提供；GitHub代码链接：<a target="_blank" rel="noopener" href="https://github.com/gpt-omni/mini-omni">GitHub</a>（注意：如果无法访问或链接失效，请尝试其他可靠的学术资源平台）。</p>
</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li><strong>(1)</strong> 研究背景：随着语言模型技术的不断进步，与人类进行实时对话已成为可能。然而，当前的模型在直接处理音频模态进行推理和生成输出方面存在困难，这限制了它们在实时交互中的应用。文章在此背景下探讨了Mini-Omni模型的应用。</li>
<li><strong>(2)</strong> 过去的方法及问题：过去的语言模型在语音合成上通常依赖于文本到语音（TTS）系统，这导致了不理想的延迟。然而，这些方法并不适合实时的、端到端的语音交互。文章指出了现有方法的不足并提出了新的方法。</li>
<li><strong>(3)</strong> 研究方法：本研究提出了Mini-Omni模型，一个基于音频的端到端对话模型，能够实现实时语音交互。通过引入文本指导的语音生成方法和批并行推理策略来增强性能。同时，通过保留原始模型的语言能力，使其他工作能够建立实时交互能力。该研究还引入了VoiceAssistant-400K数据集来优化模型的语音输出。</li>
<li><strong>(4)</strong> 任务与性能：Mini-Omni模型在实时语音交互任务上表现出优异的性能。它是第一个完全端到端、开源的实时语音交互模型，为未来的研究提供了有价值的潜力。通过实验结果证明了模型的有效性和实用性。</li>
</ul>
</li>
</ol>
<p>希望这个总结符合您的要求！如有任何需要调整或补充的地方，请告诉我。<br>7. 方法：</p>
<pre><code>* (1) 研究背景分析：文章首先指出了当前语言模型技术在直接处理音频模态进行推理和生成输出方面的困难，限制了它们在实时交互中的应用。因此，文章基于这一背景，探讨了Mini-Omni模型的应用潜力。
* (2) 过去方法的回顾与问题：传统语言模型在语音合成上依赖于文本到语音（TTS）系统，这导致了不理想的延迟。然而，这些方法并不适合实时的、端到端的语音交互需求。文章指出了这一不足并寻求新的解决方案。
* (3) Mini-Omni模型的提出：为了克服现有方法的不足，研究团队提出了Mini-Omni模型，一个基于音频的端到端对话模型，能够实现实时语音交互。模型通过引入文本指导的语音生成方法和批并行推理策略来增强性能。此外，保留了原始模型的语言能力，使得其他功能能够在此基础上建立实时交互能力。为了更好地优化模型的语音输出，文章还引入了VoiceAssistant-400K数据集。
* (4) 实验设计与结果：文章通过一系列实验验证了Mini-Omni模型在实时语音交互任务上的性能。实验结果表明，该模型是第一个完全端到端、开源的实时语音交互模型，具有显著的有效性和实用性。此外，实验结果还为未来的研究提供了有价值的参考。
</code></pre>
<p>希望这个总结符合您的要求！如果您还有其他问题或需要进一步的解释，请随时告诉我。<br>8. Conclusion:</p>
<ul>
<li>(1) 工作的意义：该工作引入了一种名为Mini-Omni的多模态模型，具有直接的语音识别能力，推动了实时语音交互领域的技术发展。该研究在人机交互领域中具有重要的实用价值，并为其他相关研究提供了有价值的参考。</li>
<li>(2) 创新点、性能和工作量总结：<ul>
<li>创新点：文章提出了Mini-Omni模型，该模型基于音频的端到端对话模型实现实时语音交互，引入了文本指导的语音生成方法和批并行推理策略，保留了原始模型的语言能力，为其他功能建立实时交互能力提供了基础。此外，文章还引入了VoiceAssistant-400K数据集以优化模型的语音输出。</li>
<li>性能：Mini-Omni模型在实时语音交互任务上表现出优异的性能，是第一个完全端到端、开源的实时语音交互模型，具有显著的有效性和实用性。</li>
<li>工作量：文章的工作量大，涉及到模型的构建、实验设计、数据集的制作等多个方面的工作。同时，文章还提供了详细的实验过程和结果分析，为其他研究者提供了有价值的参考。</li>
</ul>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-71026de8fa830b36c55cac0303cdf935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0978d5710476765aa733dff4cc3c0839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ff6b6ea275704133ab69e2bf4053833.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f02a8ee9af043077b3169912bad47db0.jpg" align="middle">
</details>




<h2 id="SpeechCaps-Advancing-Instruction-Based-Universal-Speech-Models-with-Multi-Talker-Speaking-Style-Captioning"><a href="#SpeechCaps-Advancing-Instruction-Based-Universal-Speech-Models-with-Multi-Talker-Speaking-Style-Captioning" class="headerlink" title="SpeechCaps: Advancing Instruction-Based Universal Speech Models with   Multi-Talker Speaking Style Captioning"></a>SpeechCaps: Advancing Instruction-Based Universal Speech Models with   Multi-Talker Speaking Style Captioning</h2><p><strong>Authors:Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee</strong></p>
<p>Instruction-based speech processing is becoming popular. Studies show that training with multiple tasks boosts performance, but collecting diverse, large-scale tasks and datasets is expensive. Thus, it is highly desirable to design a fundamental task that benefits other downstream tasks. This paper introduces a multi-talker speaking style captioning task to enhance the understanding of speaker and prosodic information. We used large language models to generate descriptions for multi-talker speech. Then, we trained our model with pre-training on this captioning task followed by instruction tuning. Evaluation on Dynamic-SUPERB shows our model outperforming the baseline pre-trained only on single-talker tasks, particularly in speaker and emotion recognition. Additionally, tests on a multi-talker QA task reveal that current models struggle with attributes such as gender, pitch, and speaking rate. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/cyhuang-tw/speechcaps">https://github.com/cyhuang-tw/speechcaps</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13891v1">PDF</a> SynData4GenAI 2024</p>
<p><strong>Summary</strong><br>该文提出基于指令的多说话者语音处理任务，提升情感识别与风格理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令式语音处理研究兴起。</li>
<li>多任务训练提升模型性能。</li>
<li>设计基础任务以惠及下游任务。</li>
<li>提出多说话者风格字幕任务。</li>
<li>使用大型语言模型生成描述。</li>
<li>模型在动态-SUPERB测试中优于单说话者任务模型。</li>
<li>多说话者问答任务中模型在性别、音高和语速识别上表现不佳。</li>
<li>代码和数据集公开。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol start="8">
<li>结论：</li>
</ol>
<p>（1）xxx作品的意义在于xxx（此处需要根据文章内容填写具体的意义，例如：该作品展示了当代社会的矛盾与冲突，或是揭示了人性的复杂性与多样性等）。</p>
<p>（2）创新点：xxx（例如：文章在理论框架、研究方法或研究视角上的创新之处）。文章在性能方面的优势在于xxx（例如：研究结果显著提高了某一领域的性能或效率），但也存在一些局限性，如xxx（例如：研究未充分考虑其他影响因素或存在实验样本量较小等）。在工作量方面，文章呈现了xxx的工作量（例如：文章进行了大量的实证研究或数据分析，展现了作者深入和全面的研究态度），但也存在某些重复或冗余的工作内容。总体来说，该文章在某些方面表现出色，但也存在一些改进的空间。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8021415f823c5ce0acd5bb92d61e09b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e1c7406db684343030a6fdc9a395106.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5d9ab1e6a16acb6ef52191ed789cd35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5efff236d713d07c1290261d93c716a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg" align="middle">
</details>




<h2 id="TalkLoRA-Low-Rank-Adaptation-for-Speech-Driven-Animation"><a href="#TalkLoRA-Low-Rank-Adaptation-for-Speech-Driven-Animation" class="headerlink" title="TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation"></a>TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation</h2><p><strong>Authors:Jack Saunders, Vinay Namboodiri</strong></p>
<p>Speech-driven facial animation is important for many applications including TV, film, video games, telecommunication and AR&#x2F;VR. Recently, transformers have been shown to be extremely effective for this task. However, we identify two issues with the existing transformer-based models. Firstly, they are difficult to adapt to new personalised speaking styles and secondly, they are slow to run for long sentences due to the quadratic complexity of the transformer. We propose TalkLoRA to address both of these issues. TalkLoRA uses Low-Rank Adaptation to effectively and efficiently adapt to new speaking styles, even with limited data. It does this by training an adaptor with a small number of parameters for each subject. We also utilise a chunking strategy to reduce the complexity of the underlying transformer, allowing for long sentences at inference time. TalkLoRA can be applied to any transformer-based speech-driven animation method. We perform extensive experiments to show that TalkLoRA archives state-of-the-art style adaptation and that it allows for an order-of-complexity reduction in inference times without sacrificing quality. We also investigate and provide insights into the hyperparameter selection for LoRA fine-tuning of speech-driven facial animation models. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13714v1">PDF</a> </p>
<p><strong>Summary</strong><br>谈头生成中，TalkLoRA通过低秩适应和分块策略有效解决现有模型风格适应性和运行速度问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音驱动面部动画在多个领域应用广泛。</li>
<li>现有基于Transformer的模型难以适应新说话风格。</li>
<li>Transformer模型因二次方复杂度导致长句运行缓慢。</li>
<li>TalkLoRA利用低秩适应有效适应新说话风格。</li>
<li>TalkLoRA通过训练少量参数的适配器实现。</li>
<li>分块策略降低Transformer复杂度，实现长句推理。</li>
<li>TalkLoRA在风格适应性和推理时间上均有显著提升。</li>
<li>探讨了LoRA微调的超参数选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: TalkLoRA：基于低秩适应的语音驱动动画方法</p>
</li>
<li><p>Authors: Jack Saunders, Vinay P Namboodiri</p>
</li>
<li><p>Affiliation: </p>
<ul>
<li>Jack Saunders: 英国巴斯大学（University of Bath）；Deepreel Ltd公司（位于伦敦）。</li>
<li>Vinay P Namboodiri: 英国巴斯大学（University of Bath）。</li>
</ul>
</li>
<li><p>Keywords: 语音驱动动画、低秩适应、身份适应、效率优化、Transformer模型。</p>
</li>
<li><p>Urls: 论文链接：[论文链接]；代码链接：Github：[待补充（如果可用）]，否则填写“Github:None”。</p>
</li>
<li><p>Summary: </p>
<ul>
<li>(1) 研究背景：语音驱动的动画技术在电视、电影、视频游戏、电信和AR&#x2F;VR等领域有广泛应用。虽然基于Transformer的模型在此任务上表现卓越，但它们存在一些问题，如难以适应新的个性化说话风格和对于长句子的处理速度慢。本研究旨在解决这些问题。</li>
<li>(2) 过去的方法及问题：现有的基于Transformer的模型在适应新说话风格和处理速度方面存在挑战。它们难以有效地适应新的个性化说话风格，并且在处理长句子时速度较慢，这是由于Transformer的二次复杂度导致的。</li>
<li>(3) 研究方法：本研究提出了TalkLoRA方法，通过低秩适应技术有效地适应新说话风格，即使数据有限。该方法通过为每个主体训练一个小的适配器参数集来实现。同时，研究还采用了一种分块策略，以降低处理的复杂性。</li>
<li>(4) 任务与性能：TalkLoRA在语音驱动的动画任务上进行了测试，实现了对新的个性化说话风格的有效适应，并显著提高了处理长句子的速度。这些性能改进支持了该方法的目标。实验结果表明，TalkLoRA能够显著提高模型对新说话风格的适应能力，并显著减少处理时间，证明了其有效性。</li>
</ul>
</li>
</ol>
<p>希望以上总结符合您的要求。<br>7. 方法论：</p>
<p>该研究提出了一种基于低秩适应（Low-Rank Adaptation，LoRA）技术的语音驱动动画方法，TalkLoRA。其主要思想是针对现有的基于Transformer的语音驱动动画系统进行改进，提出一系列改进组件以适应新的个性化说话风格和提速推理过程。具体步骤包括：</p>
<pre><code>- (1) 研究背景与问题阐述：
    该研究首先指出语音驱动动画技术在电视、电影、视频游戏、电信和AR/VR等领域的广泛应用，以及现有基于Transformer的模型在适应新说话风格和处理速度方面的挑战。研究旨在解决这些问题。

- (2) 研究方法介绍：
    研究提出了TalkLoRA方法，通过低秩适应技术有效地适应新说话风格，即使数据有限。该方法通过为每个主体训练一个小的适配器参数集来实现。同时，研究还采用了一种分块策略，以降低处理的复杂性，提高推理速度。

- (3) 模型架构介绍：
    该方法的模型架构基于所使用的基线模型进行适应。对于实验中使用的情况，可以选择FaceFormer或Imitator作为基线模型。每个模型都由音频编码器、Transformer解码器和每帧解码器三个组件构成。音频编码器负责将音频特征提取出来，Transformer解码器考虑时间信息，运动解码器则负责从Transformer输出中生成顶点。

- (4) 低秩适配器（LoRA）的应用：
    为了将基线模型适应到新的主体，研究使用了低秩适配器（LoRA）。LoRA是一种参数高效的微调方法，通过向权重矩阵添加偏移量来适应模型。研究确定了哪些网络组件适合应用LoRA技术，并探讨了LoRA引入的参数如何平衡模型的表示能力与正则化之间的权衡。

- (5) 分块策略的应用：
    为了提高推理速度，研究采用了分块策略。通过将输入音频分成固定大小的块，并并行处理这些块，从而限制Transformer的上下文窗口。这种方法降低了模型的计算复杂性，提高了处理长句子的速度。

- (6) 实验实现细节：
    研究提供了实施TalkLoRA方法的详细实现细节，包括训练过程、损失函数权重、优化器选择、学习率设置、LoRA的秩和alpha值的选择等。实验结果表明，TalkLoRA能够显著提高模型对新说话风格的适应能力，并显著减少处理时间，证明了其有效性。
</code></pre>
<ol start="8">
<li>Conclusion:</li>
</ol>
<ul>
<li>(1) 这项工作的意义在于提出了一种基于低秩适应（LoRA）技术的语音驱动动画方法，TalkLoRA。该方法能够解决现有基于Transformer的模型在适应新说话风格和处理速度方面的挑战，具有重要的实际应用价值。</li>
<li>(2) 创新点：该研究通过引入低秩适配器（LoRA）技术，有效地提高了模型对新说话风格的适应能力，并采用了分块策略以提高推理速度。同时，该研究将LoRA技术应用于语音驱动动画任务，实现了对个性化说话风格的快速适应。</li>
<li>性能：实验结果表明，TalkLoRA在语音驱动的动画任务上实现了显著的性能改进，提高了模型对新说话风格的适应能力，并显著减少了处理时间。</li>
<li>工作量：该文章对TalkLoRA方法进行了详细的介绍和实验验证，包括方法论、模型架构、低秩适配器的应用、分块策略的应用以及实验实现细节等方面。工作量较大，但实验结果证明了方法的有效性。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3313994c278d325c8ef3fb44a5ba2d76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c2db76f55115f8dd725a17800048f2f.jpg" align="middle">
</details>




<h2 id="Empowering-Whisper-as-a-Joint-Multi-Talker-and-Target-Talker-Speech-Recognition-System"><a href="#Empowering-Whisper-as-a-Joint-Multi-Talker-and-Target-Talker-Speech-Recognition-System" class="headerlink" title="Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech   Recognition System"></a>Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech   Recognition System</h2><p><strong>Authors:Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng</strong></p>
<p>Multi-talker speech recognition and target-talker speech recognition, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously address both tasks. In this study, we propose a pioneering approach to empower Whisper, which is a speech foundation model, to tackle joint multi-talker and target-talker speech recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar separator into its encoder to separate mixed embedding for multiple talkers; (ii) a Target Talker Identifier is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment speech as a cue; (iii) soft prompt tuning for decoder is explored for better task adaptation. Our method outperforms previous methods on two- and three-talker LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09817v2">PDF</a> Accepted to INTERSPEECH 2024</p>
<p><strong>Summary</strong><br>提出了一种创新方法，使 Whisper 模型同时应对多说话者和目标说话者的语音识别任务。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>联合处理多说话者和目标说话者的语音识别挑战。</li>
<li>利用 Whisper 模型，结合 Sidecar 分隔器进行混合嵌入分离。</li>
<li>引入目标说话者识别器，仅需3秒语音即可识别。</li>
<li>探索解码器软提示调优以适应任务。</li>
<li>在 LibriMix 和 LibriSpeechMix 数据集上优于先前方法。</li>
<li>在 AishellMix 数据集上实现可接受的零样本性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 赋能whisper以联合处理多任务说话者和目标说话者的语音识别挑战</p>
</li>
<li><p>Authors: Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng</p>
</li>
<li><p>Affiliation: 中国香港大学 (The Chinese University of Hong Kong)</p>
</li>
<li><p>Keywords: 多说话人语音识别，目标说话人语音识别，提示微调，领域自适应</p>
</li>
<li><p>Urls: Paper链接：<a href="%E5%85%B7%E4%BD%93%E7%9A%84%E9%93%BE%E6%8E%A5%E5%9C%B0%E5%9D%80%E9%9C%80%E8%A6%81%E6%82%A8%E6%8F%90%E4%BE%9B">xxx</a>；Github代码链接：<a target="_blank" rel="noopener" href="https://github.com/LingweiMeng/Whisper-Sidecar">Github</a>（若不可用则填”None”）</p>
</li>
<li><p>Summary:</p>
<ul>
<li>(1)研究背景：多说话人和目标说话人的语音识别在多种场景下均具有重要意义，特别是在语音交互和信息检索等领域。然而，现有方法往往针对单一任务进行设计，缺乏同时处理两个任务的能力。</li>
</ul>
<p> -(2)过去的方法及其问题：早期的方法通常使用级联系统，通过语音分离模块将混合语音信号分离，然后输入到单说话人语音识别系统进行转录。然而，这些方法由于优化目标不匹配，通常需要联合训练来提高性能。最近，端到端的模型因其出色的性能而受到关注，但在训练多说话人端到端语音识别系统时，如何将预测与对应的目标标签关联起来以计算损失是一个主要挑战。尽管有一些方法如Permutation Invariant Training (PIT)、Heuristic Error Assignment Training (HEAT)和Serialized Output Training (SOT)等已经取得了一些成果，但它们通常需要从头开始训练或重新微调预训练模型，无法充分利用现有的单说话人语音识别模型的进步。</p>
<p> -(3)研究方法：针对上述问题，本文提出了一种基于Whisper模型的联合多说话人和目标说话人语音识别系统。首先，通过冻结Whisper模型的权重并引入Sidecar分离器到其编码器中以分离混合嵌入向量。其次，引入目标说话人识别器来实时识别目标说话人的嵌入流，仅需要三秒的注册语音作为提示。最后，采用软提示调整为解码器进行更好的任务适应。</p>
<p> -(4)任务与性能：本文方法在英文的LibriMix和LibriSpeechMix数据集以及Mandarin的AishellMix数据集上进行了实验验证。相较于之前的方法，本文方法在两项任务上都取得了领先性能。特别是在多说话人语音识别任务上，本文方法实现了令人满意的零样本性能。实验结果支持了该方法的有效性。</p>
</li>
<li><p>方法论：</p>
<ul>
<li><p>(1) 研究背景与问题：针对多说话人和目标说话人的语音识别问题，现有的方法往往针对单一任务设计，缺乏同时处理两个任务的能力。因此，本文提出一种基于Whisper模型的联合多说话人和目标说话人语音识别系统。</p>
</li>
<li><p>(2) 方法框架：首先，通过冻结Whisper模型的权重并引入Sidecar分离器到其编码器中分离混合嵌入向量。其次，引入目标说话人识别器实时识别目标说话人的嵌入流，仅需要三秒的注册语音作为提示。再次，采用软提示调整为解码器进行更好的任务适应。</p>
</li>
<li><p>(3) 关键技术：采用Whisper作为语音识别的基础模型，Sidecar分离器用于将混合嵌入向量分离，目标说话人识别器用于识别目标说话人的嵌入流，软提示调整用于适应多任务语音识别。</p>
</li>
<li><p>(4) 数据集与实验设置：在英文的LibriMix、LibriSpeechMix数据集和Mandarin的AishellMix数据集上进行实验验证。对数据集进行预处理，以适应模型输入的要求。</p>
</li>
<li><p>(5) 训练目标：采用Permutation Invariant Training（PIT）方法确定说话人顺序，解决标签模糊问题。最终的目标函数是PIT-ASR损失和对应TTI损失的加权和。</p>
</li>
<li><p>(6) 模型评估：通过对比实验，验证所提出方法在多项任务上的性能表现，并与其他先进方法进行比较。实验结果支持该方法的有效性。</p>
</li>
</ul>
</li>
<li><p>Conclusion:</p>
</li>
</ol>
<p>(1) 该工作的重要性：</p>
<p>该文章研究了多说话人和目标说话人的语音识别问题，提出了一种基于Whisper模型的联合多任务语音识别系统。这一研究对于提高语音交互和信息检索等领域的性能和用户体验具有重要意义。</p>
<p>(2) 创新性、性能和工作量评价：</p>
<ul>
<li>创新性：文章提出了一种新的基于Whisper模型的联合多说话人和目标说话人语音识别系统，通过引入Sidecar分离器和目标说话人识别器，实现了对混合语音信号的有效分离和识别。此外，采用软提示调整解码器，提高了系统的性能。</li>
<li>性能：文章在多个数据集上进行了实验验证，相较于之前的方法，所提出的方法在多说话人和目标说话人语音识别任务上都取得了领先性能。实验结果支持了该方法的有效性。</li>
<li>工作量：文章对问题的研究深入，方法新颖，实验设计合理，数据量大且处理得当，工作量较大。</li>
</ul>
<p>总体来说，该文章在解决多说话人和目标说话人的语音识别问题上取得了一定的进展，具有一定的创新性和实用性。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad0809bf1f2a0e13bfb58fed883c328f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4760bb1b1f83c77ff470a2676d9247aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba94c08ea3020d878a6417b75885d8b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbcd66af9e5a0c566946800bba17655.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-09-02/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-09-02/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-09-02/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3bb8211b03b171a8f4a7ce70802b43cd.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-09-02  ReconX Reconstruct Any Scene from Sparse Views with Video Diffusion   Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-09-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-08-28/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-040abb8d449461d49d65c3f779921419.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-08-28  TC-PDM Temporally Consistent Patch Diffusion Models for   Infrared-to-Visible Video Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-08-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4000.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
