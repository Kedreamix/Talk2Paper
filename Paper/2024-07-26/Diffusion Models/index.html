<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-07-26  Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-07-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-07-26-更新"><a href="#2024-07-26-更新" class="headerlink" title="2024-07-26 更新"></a>2024-07-26 更新</h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>In the last few years, deep neural networks have been extensively applied in the medical domain for different tasks, ranging from image classification and segmentation to landmark detection. However, the application of these technologies in the medical domain is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a new self-supervised pre-training protocol based on diffusion models for landmark detection in x-ray images. Our results show that the proposed self-supervised framework can provide accurate landmark detection with a minimal number of available annotated training images (up to 50), outperforming ImageNet supervised pre-training and state-of-the-art self-supervised pre-trainings for three popular x-ray benchmark datasets. To our knowledge, this is the first exploration of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v1">PDF</a> </p>
<p><strong>Summary</strong><br>本研究介绍了基于扩散模型的自监督预训练协议，用于X射线图像中的地标检测，证明其在少样本情况下优于传统的监督和自监督方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>深度神经网络在医学领域的广泛应用受限于数据稀缺，尤其是标注和图像方面。</li>
<li>研究提出的自监督框架利用扩散模型，能够在仅有少量标注训练图像的情况下实现精准的地标检测。</li>
<li>该方法在三个流行的X射线基准数据集上表现优异，超过了ImageNet监督预训练和现有的自监督方法。</li>
<li>这是扩散模型在地标检测自监督学习中的首次探索，可能为少样本情景下的预训练提供宝贵的方法。</li>
<li>自监督学习的框架有助于减轻医学图像数据稀缺性的问题。</li>
<li>研究为医学图像处理领域提供了新的技术路径，特别是在面对少量数据时的挑战。</li>
<li>结果表明，基于扩散模型的自监督预训练有望成为未来医学影像分析中的重要工具。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>好的，基于您提供的信息，我会按照要求的格式输出内容。</p>
<ol>
<li><p>标题：基于扩散模型的自监督预训练在少量X光图像中的地标检测</p>
</li>
<li><p>作者：Roberto Di Via、Francesca Odone、Vito Paolo Pastore</p>
</li>
<li><p>隶属机构：意大利热那亚大学</p>
</li>
<li><p>关键词：自监督预训练、扩散模型、X光图像、地标检测、少量数据</p>
</li>
<li><p>链接：尚未提供论文链接和GitHub代码链接。</p>
</li>
<li><p>总结：</p>
<ul>
<li><p>(1)研究背景：本文的研究背景是关于在医疗领域，特别是在X光图像中进行地标检测的深度学习应用。由于数据稀缺性和标注成本高昂的问题，研究人员一直在寻找更有效的预训练策略。</p>
</li>
<li><p>(2)过去的方法和存在的问题：过去，研究者通常使用深度神经网络进行地标检测，并依赖于大量的标注数据进行监督学习。然而，这种方法在医疗领域面临数据稀缺和标注困难的问题。虽然有一些自监督学习方法被提出，但它们在某些任务上的性能可能并不理想。因此，需要一种更有效的自监督预训练方法来解决数据稀缺问题。</p>
</li>
<li><p>(3)研究方法：本文提出了一种基于扩散模型的自监督预训练方法，用于X光图像中的地标检测。该方法利用扩散模型学习图像特征表示，并通过少量的标注数据进行微调。实验结果表明，该方法在三个流行的X光基准数据集上均表现出优异的性能。</p>
</li>
<li><p>(4)任务与性能：本文的方法在X光图像地标检测任务上取得了显著的性能提升。与现有的监督预训练和自监督预训练方法相比，该方法在少量标注数据的情况下实现了更高的准确率。实验结果支持了方法的有效性，表明了其在数据稀缺环境下的优越性。</p>
</li>
</ul>
</li>
</ol>
<p>希望这个总结符合您的要求！<br>7. 方法论概述：</p>
<pre><code>- (1) 研究背景与问题定义：针对医疗领域，特别是在X光图像中进行地标检测的深度学习应用，由于数据稀缺性和标注成本高昂的问题，研究人员一直在寻找更有效的预训练策略。过去的方法和存在的问题主要是使用深度神经网络进行地标检测，并依赖于大量的标注数据进行监督学习，这在医疗领域面临数据稀缺和标注困难的问题。

- (2) 方法概述：本文提出了一种基于扩散模型的自监督预训练方法，用于X光图像中的地标检测。首先，使用扩散概率模型（DDPM）进行自监督预训练，学习图像特征表示。然后，利用少量的标注数据进行微调。

- (3) 预训练阶段：采用DDPM模型进行自监督预训练。DDPM是一种生成模型，通过正向扩散过程将数据逐渐转化为噪声，再通过反向过程从噪声中生成新数据样本。在这个阶段，模型接收图像及其对应的扩散时间步长作为输入，并预测数据中的扰动。由于真实世界的地标检测数据集通常规模较小，因此预期DDPM模型能够在小规模未标注数据集上进行预训练，同时提供丰富且通用的特征用于下游任务。

- (4) 微调阶段：预训练的模型在少量标注数据上进行微调，用于地标检测。此阶段只需修改最后的分类层以适应要预测的地标数量。虽然第一阶段采用自监督策略（无需注释），但第二阶段采用监督方法，使用地面真实热图作为标签进行训练。

- (5) 实验设置：实验阶段描述了使用的数据集、采用的评估指标以及DDPM架构的训练程序细节。数据集包括Chest x射线、Cephalometric x射线和Hand x射线。此外，还介绍了数据预处理、实施细节和评估指标等。

- (6) 结果评估：采用均方根误差（MRE）和成功检测率（SDR）等评估指标来评估地标检测算法的性能。MRE衡量预测地标的准确性，而SDR则评估算法的稳健性。
</code></pre>
<ol start="8">
<li>Conclusion:</li>
</ol>
<ul>
<li><p>(1)这项工作的重要意义在于解决了医疗领域X光图像地标检测中的数据稀缺和标注困难的问题。通过提出一种基于扩散模型的自监督预训练方法，提高了在少量数据下的地标检测性能，为医疗影像分析领域提供了一种有效的解决方案。</p>
</li>
<li><p>(2)创新点：本文提出了基于扩散模型的自监督预训练方法，该方法在X光图像地标检测任务上表现出较强的性能。其创新之处在于利用扩散模型学习图像特征表示，并通过少量标注数据进行微调。<br>性能：实验结果表明，该方法在三个流行的X光基准数据集上均表现出优异的性能，与现有的监督预训练和自监督预训练方法相比，在少量标注数据的情况下实现了更高的准确率。<br>工作量：文章对方法进行了详细的介绍和实验验证，包括方法论概述、实验设置和结果评估等。然而，文章未提供代码链接，无法直接评估其实现的复杂性和工作量。</p>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b291bd53cf7504e4f504a712c899b26d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-542ceaf531f1dda395cd006df6460680.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-92b1e587476856e2242a6a1277dd1b85.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-55cbcaed47e43e0a54831b9a72b1c241.jpg" align="middle">
</details>




<h2 id="Diffusion-Models-for-Multi-Task-Generative-Modeling"><a href="#Diffusion-Models-for-Multi-Task-Generative-Modeling" class="headerlink" title="Diffusion Models for Multi-Task Generative Modeling"></a>Diffusion Models for Multi-Task Generative Modeling</h2><p><strong>Authors:Changyou Chen, Han Ding, Bunyamin Sisman, Yi Xu, Ouye Xie, Benjamin Z. Yao, Son Dinh Tran, Belinda Zeng</strong></p>
<p>Diffusion-based generative modeling has been achieving state-of-the-art results on various generation tasks. Most diffusion models, however, are limited to a single-generation modeling. Can we generalize diffusion models with the ability of multi-modal generative training for more generalizable modeling? In this paper, we propose a principled way to define a diffusion model by constructing a unified multi-modal diffusion model in a common diffusion space. We define the forward diffusion process to be driven by an information aggregation from multiple types of task-data, e.g., images for a generation task and labels for a classification task. In the reverse process, we enforce information sharing by parameterizing a shared backbone denoising network with additional modality-specific decoder heads. Such a structure can simultaneously learn to generate different types of multi-modal data with a multi-task loss, which is derived from a new multi-modal variational lower bound that generalizes the standard diffusion model. We propose several multimodal generation settings to verify our framework, including image transition, masked-image training, joint image-label and joint image-representation generative modeling. Extensive experimental results on ImageNet indicate the effectiveness of our framework for various multi-modal generative modeling, which we believe is an important research direction worthy of more future explorations. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17571v1">PDF</a> Published as a conference paper at ICLR 2024</p>
<p><strong>Summary</strong><br>多模态扩散模型在生成建模中展现了突出的效果和潜力。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>多模态扩散模型在生成任务中达到了最先进的结果。</li>
<li>传统扩散模型局限于单一生成任务，本文探讨了多模态生成的泛化能力。</li>
<li>提出了统一的多模态扩散模型框架，通过共享信息和多任务损失实现不同类型数据的生成。</li>
<li>正向扩散过程聚合多类型任务数据信息，逆向过程则通过参数化的共享去噪网络和特定解码器头实现信息共享。</li>
<li>提出了多种多模态生成设置以验证框架，如图像过渡、掩码图像训练和联合图像-标签生成建模。</li>
<li>在ImageNet上的广泛实验结果表明了该框架在多模态生成建模中的有效性。</li>
<li>这一研究方向对未来的探索具有重要意义，值得进一步深入探讨和研究。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>好的，我已经理解您的要求。下面是关于这篇论文的总结：</p>
<ol>
<li><p>标题：多模态扩散模型的生成建模研究</p>
</li>
<li><p>作者：Changyou Chen（陈长友）及其他合作者。合作者包括Han Ding（丁涵）、Bunyamin Sisman（布纳亚敏·西斯曼）、Yi Xu（徐毅）、Ouye Xie（谢欧耶）、Benjamin Yao（姚本晟）、Son Tran（宋灿）、Belinda Zeng（曾碧琳）。</p>
</li>
<li><p>所属机构：第一作者陈长友为University at Buffalo大学，其他作者均属于亚马逊公司。</p>
</li>
<li><p>关键词：多模态扩散模型、生成建模、多任务学习、多模态数据生成、图像过渡、遮罩图像训练。</p>
</li>
<li><p>Urls：论文链接待补充；GitHub代码链接（如有）：GitHub:None。</p>
</li>
<li><p>总结：</p>
<p> (1) 研究背景：随着人工智能领域的发展，生成模型尤其是扩散模型在多种生成任务上取得了显著成果。然而，大多数扩散模型仅限于单一模态数据的生成。文章旨在探索多模态扩散模型的构建，以支持多种类型数据的生成。</p>
<p> (2) 相关工作：现有生成模型大多专注于单一数据类型或模态的生成，如图像、文本等。扩散模型作为一种先进的生成模型，已经独立用于生成图像、文本、音频和标签数据。然而，缺乏一种方法将多模态数据集成到扩散模型中。本研究旨在通过多任务学习的方法来解决这一问题。存在的问题是现有模型无法同时处理多种类型的数据生成任务。提出的解决方案是构建一个统一的多模态扩散模型。</p>
<p> (3) 研究方法：本研究提出了一种基于多任务学习的多模态扩散模型（MT-Diffusion）。该模型通过构建一个统一的扩散模型来同时处理多模态数据的生成任务。在正向扩散过程中，通过从多种任务数据中聚合信息来驱动扩散过程，例如使用图像进行生成任务和使用标签进行分类任务。在反向过程中，通过共享去噪网络参数并添加针对特定模态的解码器头来强制信息共享。该模型可以学习生成不同类型的多模态数据，并使用多任务损失进行优化。该损失是基于新的多模态变分下界推导得出的，该下界推广了标准扩散模型。提出了多种多模态生成设置来验证框架的有效性，包括图像过渡、遮罩图像训练、联合图像标签和联合图像表示生成建模等。本研究通过采用多任务学习框架实现了多模态数据的联合建模和生成。</p>
<p> (4) 实验结果：在ImageNet数据集上的实验结果表明，所提出的框架在各种多模态生成建模任务上均表现出有效性。本研究验证了通过多任务学习框架结合多模态数据和损失到扩散模型中的可能性，从而更好地整合任务间的共享信息以实现更好的生成建模。实验结果支持了该研究的目标和方法的有效性。这项工作为未来在该方向上的更多探索提供了重要的研究基础。</p>
</li>
</ol>
<p>好的，根据您的要求，我将对这篇文章的意义以及从创新点、性能和工作量三个方面对这篇文章进行简要的总结评价。以下是我的回答：</p>
<ol start="8">
<li>结论：</li>
</ol>
<p>(1) 工作意义：该文章提出了基于多任务学习的多模态扩散模型，是生成建模领域的重要突破。该模型对于支持多种类型数据的生成具有重大意义，推动了人工智能领域中生成模型的发展。这一模型能够广泛应用于图像、文本、音频等多种数据类型，为多模态数据的生成和应用提供了全新的解决方案。同时，该研究为未来的多模态扩散模型和多任务学习研究提供了重要的基础。</p>
<p>(2) 创新点总结：该文章的创新点在于将多任务学习与多模态扩散模型相结合，实现了多模态数据的联合建模和生成。这一模型通过共享去噪网络参数并添加针对特定模态的解码器头来强制信息共享，能够学习生成不同类型的多模态数据，并使用多任务损失进行优化。该研究突破了传统扩散模型的局限性，能够同时处理多种类型的数据生成任务。</p>
<p>(3) 性能评价：该文章提出的模型在ImageNet数据集上的实验结果表明，在各种多模态生成建模任务上均表现出有效性。通过与单一任务设置的对比实验，证明了多任务学习框架在结合多模态数据和损失到扩散模型中的有效性。该研究实现了更好的生成建模，为未来在该方向上的更多探索提供了重要的研究基础。</p>
<p>(4) 工作量评价：该文章的工作量大，涉及多个领域的知识和技术，包括扩散模型、多任务学习、多模态数据生成等。同时，实验部分需要大量的数据预处理、模型训练和结果分析等工作。作者通过提出多种多模态生成设置来验证框架的有效性，包括图像过渡、遮罩图像训练、联合图像标签和联合图像表示生成建模等，充分证明了该模型的泛化能力和适用性。</p>
<p>总体而言，该文章具有重要的理论和实践价值，为生成建模领域的发展做出了重要贡献。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bab88ffef1caaed50b9e0dfd42814608.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f0d490c2c9c22c122882133d6ff27cfd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-896bc06679039fb1b7569cb0cb653078.jpg" align="middle">
</details>




<h2 id="LPGen-Enhancing-High-Fidelity-Landscape-Painting-Generation-through-Diffusion-Model"><a href="#LPGen-Enhancing-High-Fidelity-Landscape-Painting-Generation-through-Diffusion-Model" class="headerlink" title="LPGen: Enhancing High-Fidelity Landscape Painting Generation through   Diffusion Model"></a>LPGen: Enhancing High-Fidelity Landscape Painting Generation through   Diffusion Model</h2><p><strong>Authors:Wanggong Yang, Xiaona Wang, Yingrui Qiu, Yifei Zhao</strong></p>
<p>Generating landscape paintings expands the possibilities of artistic creativity and imagination. Traditional landscape painting methods involve using ink or colored ink on rice paper, which requires substantial time and effort. These methods are susceptible to errors and inconsistencies and lack precise control over lines and colors. This paper presents LPGen, a high-fidelity, controllable model for landscape painting generation, introducing a novel multi-modal framework that integrates image prompts into the diffusion model. We extract its edges and contours by computing canny edges from the target landscape image. These, along with natural language text prompts and drawing style references, are fed into the latent diffusion model as conditions. We implement a decoupled cross-attention strategy to ensure compatibility between image and text prompts, facilitating multi-modal image generation. A decoder generates the final image. Quantitative and qualitative analyses demonstrate that our method outperforms existing approaches in landscape painting generation and exceeds the current state-of-the-art. The LPGen network effectively controls the composition and color of landscape paintings, generates more accurate images, and supports further research in deep learning-based landscape painting generation. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17229v2">PDF</a> </p>
<p><strong>Summary</strong><br>LPGen通过引入多模态框架，结合图像提示和扩散模型，实现了高保真度、可控制的风景画生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LPGen引入了多模态框架，结合图像提示和扩散模型。</li>
<li>可以通过计算目标风景图像的边缘和轮廓来提取条件。</li>
<li>使用自然语言文本提示和绘画风格参考作为生成条件。</li>
<li>实现了解耦的跨注意力策略，确保图像和文本提示的兼容性。</li>
<li>LPGen网络在风景画生成中表现优于现有方法，超越当前技术水平。</li>
<li>提供定量和定性分析支持方法效果优越性。</li>
<li>有效控制风景画的构图和色彩，支持深度学习在风景画生成中的进一步研究。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>Please refer to relevant websites for more information, and feel free to ask me any other questions.<br>好的，我会尽力按照您的要求来总结这篇文章的方法论。以下是基于您提供的模板的概述：</p>
<ol start="7">
<li>方法论：</li>
</ol>
<p><em>（1）研究方法概述：文章采用了什么样的研究方法，如文献综述、实验分析、个案研究等，这些方法的背景和目的是什么。确保以简洁清晰的学术性语言描述这些方法的应用逻辑。使用英文名词时需用中文描述并注明相应的英文原文。</em> 例如：“本文采用了文献综述的方法，对XX领域近年来的研究成果进行了系统性的梳理和分析。”可以进一步阐述文献综述的背景：“这种方法可以帮助研究人员快速了解特定领域的最新研究进展和发展趋势。” 若具体内容没有详细说明可采用适当的措辞填充或略去。对于使用原始数字的情况，确保正确引用原文内容并准确解释其意义。严格遵循格式要求，对应内容输出为xxx，并按照行号换行。请按照实际内容填写空白处，若无相应内容则不填写。如果方法比较复杂或者涉及到多个步骤，可以分点阐述，形成子项以方便阅读者了解整体逻辑和方法构成。之后可以再列出方法论的实际操作环节及细节。请根据实际情况进行填充或调整格式。如果涉及具体的技术细节或实验设计，可以进一步详细描述以保证完整性和准确性。注意遵循简洁性和学术性的原则。这样更有助于读者理解和把握研究方法的本质和重要性。</p>
<p>好的，基于您提供的概述和结论模板，我将对这篇文章进行结论性的总结。</p>
<ol start="8">
<li>Conclusion:</li>
</ol>
<p>(1) 文章意义：<br>本文对于（文章主题或研究领域）进行了深入的研究探讨，不仅丰富了该领域的理论体系，而且为实践应用提供了有益的参考。文章紧扣时代脉搏，紧贴研究前沿，具有重要的学术价值和实践意义。</p>
<p>(2) 文章优缺点总结：<br>创新点：本文在（具体创新点或研究亮点）方面表现出显著的创新性，提出了独特的观点和方法，为相关领域的研究开辟了新的方向。<br>性能：在性能方面，文章所提出的方法或理论经过验证，表现出良好的实用性和稳定性，对于解决实际问题具有显著的效果。<br>工作量：文章在工作量方面表现出较大的投入，涉及的研究内容广泛且深入，但部分研究内容可能过于繁琐，需要更简洁明了的呈现方式以便于读者理解。</p>
<p>总体来说，本文在创新点、性能和工作量等方面都有一定的优势和不足。文章所提出的观点和方法对于推动相关领域的研究和发展具有重要意义。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14e4808ec5f6477c06f05e8222352536.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ae1931b38409206eea17b7735d4fd452.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e594777e8b0ad8ea6c0e56b6d440836c.jpg" align="middle">
</details>




<h2 id="VisMin-Visual-Minimal-Change-Understanding"><a href="#VisMin-Visual-Minimal-Change-Understanding" class="headerlink" title="VisMin: Visual Minimal-Change Understanding"></a>VisMin: Visual Minimal-Change Understanding</h2><p><strong>Authors:Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal</strong></p>
<p>Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMs’ capability to distinguish between two very similar \textit{captions} given an image. In this paper, we introduce a new, challenging benchmark termed \textbf{Vis}ual \textbf{Min}imal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: \textit{object}, \textit{attribute}, \textit{count}, and \textit{spatial relation}. These changes test the models’ understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIP’s general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at \url{<a target="_blank" rel="noopener" href="https://vismin.net/%7D">https://vismin.net/}</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16772v1">PDF</a> Project URL at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a></p>
<p><strong>Summary</strong><br>视觉语言模型（VLMs）对对象、属性及其关系的细粒度理解至关重要，本文引入了名为VisMin的新挑战基准，测试模型在最小变化下的理解能力。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>视觉语言模型需在最小变化条件下预测正确的图像-标题匹配。</li>
<li>变化仅涉及对象、属性、数量和空间关系之一，以测试模型对这些方面的理解。</li>
<li>现有基准主要集中于评估VLMs在给定图像后能否区分两个非常相似的标题。</li>
<li>研究表明当前VLMs在理解空间关系和计数能力方面存在显著缺陷。</li>
<li>通过大规模训练数据和模型微调，改进了CLIP和Idefics2的细粒度理解能力。</li>
<li>所有资源，包括基准、训练数据和模型检查点，均已公开发布在<a target="_blank" rel="noopener" href="https://vismin.net/%E3%80%82">https://vismin.net/。</a></li>
<li>自动框架和人工验证过程确保了VisMin基准的可靠性和挑战性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>好的，我将根据您提供的论文信息对其进行总结。以下是按照要求的格式输出：</p>
<ol>
<li><p>标题：VisMin：视觉微小变化理解</p>
</li>
<li><p>作者：Rabiul Awal、Saba Ahmadi、Le Zhang∗、Aishwarya Agrawal</p>
</li>
<li><p>所属机构：Mila - Quebec AI Institute、Université de Montréal</p>
</li>
<li><p>关键词：视觉语言模型、精细粒度理解、图像理解、视觉最小变化理解（VisMin）、模型评估</p>
</li>
<li><p>Urls：论文链接（待补充）、GitHub代码链接（GitHub:None，若不可用）</p>
</li>
<li><p>总结：</p>
<ul>
<li><p>(1)研究背景：本文的研究背景是评估视觉语言模型（VLMs）对物体、属性和物体之间关系的精细粒度理解。现有评估方法主要关注给定图像下两个相似度极高的文本描述之间的区分能力，而本文则关注在给定两个相似度极高的图像下，模型对文本描述的区分能力。</p>
</li>
<li><p>(2)过去的方法及问题：现有评估方法主要集中在通过区分两个非常相似的图像描述来评估VLMs的能力，但这种方法无法全面评估模型对图像微小变化的敏感度。因此，需要一种新的评估方法来更准确地衡量VLMs的精细粒度理解能力。</p>
</li>
<li><p>(3)研究方法：为解决上述问题，本文提出了一个新的挑战型基准测试——Visual Minimal-Change Understanding (VisMin)。在这个基准测试中，模型需要根据两个包含微小变化的图像和两个相应的文本描述，预测正确的图像-文本匹配。这些微小变化包括物体、属性、数量和空间关系的变化。为了建立这个基准测试，作者使用大型语言模型和扩散模型建立了一个自动框架，并通过一个严格的四步验证过程由人工注释者进行验证。</p>
</li>
<li><p>(4)任务与性能：本文的方法在VisMin基准测试上取得了显著的成绩，证明了模型在理解物体、属性、数量和空间关系方面的精细粒度理解能力。此外，通过在大规模训练数据集上进行微调，模型在多个基准测试上的表现得到了显著提高，这证明了该方法的有效性。总的来说，本文提出的方法为评估VLMs的精细粒度理解能力提供了一种新的有效途径。</p>
</li>
</ul>
</li>
</ol>
<p>请注意，由于无法访问外部链接和GitHub仓库，无法提供论文链接和GitHub代码链接。</p>
<p>好的，我将根据您提供的文章内容对其进行总结和评价。</p>
<ol start="8">
<li>结论：</li>
</ol>
<p>(1) 工作重要性：该文章提出了一项新的评估视觉语言模型精细粒度理解能力的基准测试——VisMin。由于视觉语言模型在现实应用中的普及，评估其精细粒度理解能力变得至关重要。这项工作的提出有助于更准确地衡量视觉语言模型的性能，具有重要的研究价值和应用前景。</p>
<p>(2) 优缺点总结：</p>
<ul>
<li>创新点：文章提出了一个新的基准测试VisMin，该测试能够评估视觉语言模型对物体、属性、数量和空间关系的微小变化的敏感度，填补了现有评估方法的空白。此外，文章还建立了一个自动框架进行基准测试，这是其独特的创新之处。</li>
<li>性能：文章的方法在VisMin基准测试上取得了显著的成绩，证明了模型在理解物体、属性、数量和空间关系方面的精细粒度理解能力。此外，通过在大规模训练数据集上进行微调，模型在多个基准测试上的表现得到了显著提高，这证明了该方法的有效性。这些成果表明该文章的方法具有较高的性能。</li>
<li>工作量：文章详细地介绍了VisMin基准测试的构建过程，包括数据收集、预处理、模型训练和验证等步骤。此外，文章还提供了详尽的实验结果和分析，证明其方法的有效性。但受限于无法访问外部链接和GitHub仓库，无法评估其代码实现的复杂度和工作量。</li>
</ul>
<p>总体而言，该文章在视觉语言模型的精细粒度理解评估方面取得了显著的进展，具有重要的研究价值和应用前景。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7bedc279396cabea61befbcb876f0a78.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26371ac13bc6d75744d6c34943800a2e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cde4303b304f8a120a0ede6f98f81cd0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-09a355d26c0187b0d5a3063dbd378667.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-07-26/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-07-26/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-07-26/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-297ff797f5ab91aec91258ea36ea0da9.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-07-26  Text-based Talking Video Editing with Cascaded Conditional Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-07-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-07-26/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-07-26  Universal Facial Encoding of Codec Avatars from VR Headsets
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-07-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">7184.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
