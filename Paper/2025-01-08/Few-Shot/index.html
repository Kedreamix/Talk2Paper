<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-08  RW-Net Enhancing Few-Shot Point Cloud Classification with a Wavelet   Transform Projection-based Network">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5b312baee0c90a02261e37c17fc5c519.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-08-更新"><a href="#2025-01-08-更新" class="headerlink" title="2025-01-08 更新"></a>2025-01-08 更新</h1><h2 id="RW-Net-Enhancing-Few-Shot-Point-Cloud-Classification-with-a-Wavelet-Transform-Projection-based-Network"><a href="#RW-Net-Enhancing-Few-Shot-Point-Cloud-Classification-with-a-Wavelet-Transform-Projection-based-Network" class="headerlink" title="RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet   Transform Projection-based Network"></a>RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet   Transform Projection-based Network</h2><p><strong>Authors:Haosheng Zhang, Hao Huang</strong></p>
<p>In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms. This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples. To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets. This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture. The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy. This process ensures the retention of essential information for effective decision-making, optimizing the model’s ability to learn from limited data. Complementing RDE, incorporating the wavelet transform further enhances the framework’s capability to generalize in low-data regimes. By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects. These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains. To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification. The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios. </p>
<blockquote>
<p>在3D对象分类领域，一个基本挑战在于解决标记数据稀缺的问题，这限制了传统数据密集型学习模式的应用。这种挑战在少量样本学习场景中尤为突出，该场景的目标是从极少的标注样本中实现稳健的泛化。为了克服这些限制，识别和利用3D对象的最突出和最具区分性的特征至关重要，从而提高了学习效率，并减少对大规模标注数据集的依赖。本研究引入了RW-Net，这是一个新型框架，通过整合速率失真解释（RDE）和小波变换，融入基于投影的先进3D对象分类架构，以解决上述挑战。所提出的方法利用RDE通过识别并保留最有信息的数据成分来提取关键特征，同时减少冗余。这一过程确保了保留用于有效决策的必要信息，优化模型从有限数据中学习的能力.</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03221v1">PDF</a> 11 pages, 5 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>在3D目标分类领域，标记数据的稀缺性是最大的挑战之一，这也限制了传统数据密集型学习模式的适用性。特别是在小样本学习中，目标是从极少量的标注样本中实现稳健的泛化。为了克服这些限制，需要识别并利用最显著和有鉴别力的三维物体的特征，以提高学习效率并减少对大规模标记数据集集的依赖。本研究介绍了RW-Net，这是一种新颖的框架，它通过整合率失真解释（RDE）和小波变换来解决上述挑战。所提出的利用RDE提取关键特征的方法确保了有效决策所需的必要信息的保留，优化了模型从有限数据中学习的能力。为了验证RW-Net的有效性，在ModelNet40、ModelNet40-C和ScanObjectNN三个数据集上进行了大量实验。结果表明，该方法实现了最先进的性能，在小样本学习场景中表现出卓越的泛化和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在处理稀缺的标注数据时，传统的数据密集型学习模式面临挑战。特别是在小样本学习中，实现稳健泛化的难度更大。</li>
<li>提出了一种新的框架RW-Net来解决上述问题，结合了率失真解释（RDE）和小波变换技术。</li>
<li>RDE技术用于提取关键特征，识别并保留最具有信息量的数据组件，同时减少冗余信息。这确保了有效决策所需的必要信息的保留。</li>
<li>小波变换增强了框架在低数据环境下的泛化能力。它通过强调输入数据的低频成分来捕捉三维物体的基本几何和结构属性。这些属性有助于减轻过拟合问题并提高在不同任务和领域中的模型稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ba498073ebfd3fa53d6b5634cd0c3ec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-993d8371fde8ed1e34681ed5617f46e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c19af3318b6ae23c4a6c2bddff0e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68d7136609548084c3dddda3c3638c2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed312ce19450517eab609d222e73e650.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f93be0b109b9ce0d1c2dc5efe0bccd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Semantic-Captioning-Benchmark-Dataset-and-Graph-Aware-Few-Shot-In-Context-Learning-for-SQL2Text"><a href="#Semantic-Captioning-Benchmark-Dataset-and-Graph-Aware-Few-Shot-In-Context-Learning-for-SQL2Text" class="headerlink" title="Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot   In-Context Learning for SQL2Text"></a>Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot   In-Context Learning for SQL2Text</h2><p><strong>Authors:Ali Al-Lawati, Jason Lucas, Prasenjit Mitra</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \url{<a target="_blank" rel="noopener" href="https://github.com/aliwister/ast-icl%7D">https://github.com/aliwister/ast-icl}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各种自然语言处理任务中表现出了显著的性能，包括将自然语言转换为正式代码表示的语义解析。然而，将代码转换为自然语言的逆向过程，即语义描述，受到的关注度较少。随着LLM被集成到代码生成、安全分析和教育目的的平台中，这项任务变得越来越重要。在本文中，我们关注SQL查询的标注（SQL2Text），以解决在LLM生成的代码存在潜在安全风险的时代，理解和解释SQL查询的迫切需求。我们通过对GPT-4o引入迭代ICL提示来重新利用Text2SQL数据集进行SQL2Text任务，生成多个附加话语，这增强了数据集在逆向任务中的稳健性。我们的实验使用基于不同样本选择方法进行的上下文内学习（ICL），侧重于更小、更计算高效的LLM。研究结果表明，利用SQL的固有图形属性进行ICL样本选择显著优于随机选择，BLEU得分提高了高达39%，并且比其他方法提供了更好的结果。数据集和代码已发布：\url{<a target="_blank" rel="noopener" href="https://github.com/aliwister/ast-icl%7D%E3%80%82">https://github.com/aliwister/ast-icl}。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03166v1">PDF</a> Accepted to COLING’25</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大语言模型在语义捕捉领域的应用，特别是在将代码转化为自然语言的任务中的重要性。针对SQL查询的语义捕捉（SQL2Text）问题，文章使用GPT-4o进行迭代ICL提示来生成额外的描述语句，增强数据集对反向任务的稳健性。实验采用基于不同样本选择方法的上下文学习（ICL），并强调更小、更高效的LLM的优势。利用SQL的内在图形属性进行ICL样本选择显著优于随机选择，BLEU得分提高了高达39%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLMs）在语义捕捉，特别是代码到自然语言的转化任务中扮演重要角色。</li>
<li>SQL查询的语义捕捉（SQL2Text）对于理解LLM生成的SQL查询至关重要，尤其在存在潜在安全风险的环境下。</li>
<li>使用GPT-4o进行迭代ICL提示，通过生成额外的描述语句来增强数据集。</li>
<li>采用基于不同样本选择方法的上下文学习（ICL）进行实验。</li>
<li>利用SQL的内在图形属性进行ICL样本选择显著提高性能，BLEU得分提升高达39%。</li>
<li>与随机选择和其它方法相比，利用SQL内在图形属性的ICL样本选择表现出更好的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-26fb357827978697f05a64db295a6a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99b3b8bfbaa07b56c9cd7ac29d35a5e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9160cd6db4dc6a9866e9a383d80a8947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa801f8289f43e48bb19dbe024fb5efe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Graph-based-Retrieval-Augmented-Generation-for-Dynamic-Few-shot-Text-Classification"><a href="#Graph-based-Retrieval-Augmented-Generation-for-Dynamic-Few-shot-Text-Classification" class="headerlink" title="Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text   Classification"></a>Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text   Classification</h2><p><strong>Authors:Yubo Wang, Haoyang Li, Fei Teng, Lei Chen</strong></p>
<p>Text classification is a fundamental task in natural language processing, pivotal to various applications such as query optimization, data integration, and schema matching. While neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and target labels frequently evolve based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to predict text labels. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. GORAG constructs and maintains an adaptive information graph by extracting side information across all target texts, rather than treating each input independently. It employs a weighted edge mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and accurate contextual information. </p>
<blockquote>
<p>文本分类是自然语言处理中的一项基本任务，在查询优化、数据集成和模式匹配等应用中发挥着关键作用。尽管基于神经网络（如CNN和BERT）的模型在文本分类方面表现出卓越的性能，但它们的有效性严重依赖于丰富的标记训练数据。这种依赖性使得这些模型在动态少样本文本分类中的效果较差，其中标记数据稀缺，目标标签经常根据应用需求而演变。最近，由于大型语言模型的广泛预训练和上下文理解，它们显示出巨大的潜力。当前的方法为大型语言模型提供文本输入、候选标签和额外的侧面信息（例如描述）来预测文本标签。然而，其有效性受到输入大小增加和通过侧面信息处理引入的噪声的阻碍。为了解决这些限制，我们提出了一种基于图的在线检索增强生成框架，名为GORAG，用于动态少样本文本分类。GORAG通过提取所有目标文本的侧面信息来构建和维护自适应信息图，而不是独立处理每个输入。它采用加权边机制来优先考虑提取信息的重要性和可靠性，并使用针对每个文本输入量身定制的最低成本生成树动态检索相关上下文。经验评估表明，GORAG通过提供更全面和准确的上文信息，在现有方法的基础上实现了性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于图的在线检索增强生成框架（GORAG）在动态少样本文本分类中的应用。针对神经网络模型在少样本情况下的不足，GORAG通过构建和维护一个自适应信息图，提取目标文本的所有侧信息，采用加权边机制，为每篇文本输入定制最小成本生成树，实现动态检索相关上下文，从而提高文本分类的准确性和全面性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本分类是自然语言处理中的基础任务，对于查询优化、数据集成和模式匹配等多种应用至关重要。</li>
<li>神经网络模型（如CNN和BERT）在文本分类中表现出卓越性能，但它们在少样本情况下效果较差。</li>
<li>大型语言模型（LLMs）在预训练和上下文理解方面具有潜力。</li>
<li>当前LLM方法面临输入大小增加和侧信息处理引入的噪声问题。</li>
<li>GORAG框架通过构建自适应信息图解决上述问题，提取目标文本的所有侧信息并进行管理。</li>
<li>GORAG采用加权边机制，优先处理重要且可靠的信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c4791f1b155d19f916156f7d5d74249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a400039d75843c9b340e17a8af4def8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d96f9b63cb7accfb902bb30b21ec2d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-082716739e410264600392b905a8d8e5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TARDiS-Text-Augmentation-for-Refining-Diversity-and-Separability"><a href="#TARDiS-Text-Augmentation-for-Refining-Diversity-and-Separability" class="headerlink" title="TARDiS : Text Augmentation for Refining Diversity and Separability"></a>TARDiS : Text Augmentation for Refining Diversity and Separability</h2><p><strong>Authors:Kyungmin Kim, SangHun Im, GiBaeg Kim, Heung-Seon Oh</strong></p>
<p>Text augmentation (TA) is a critical technique for text classification, especially in few-shot settings. This paper introduces a novel LLM-based TA method, TARDiS, to address challenges inherent in the generation and alignment stages of two-stage TA methods. For the generation stage, we propose two generation processes, SEG and CEG, incorporating multiple class-specific prompts to enhance diversity and separability. For the alignment stage, we introduce a class adaptation (CA) method to ensure that generated examples align with their target classes through verification and modification. Experimental results demonstrate TARDiS’s effectiveness, outperforming state-of-the-art LLM-based TA methods in various few-shot text classification tasks. An in-depth analysis confirms the detailed behaviors at each stage. </p>
<blockquote>
<p>文本增强（TA）是文本分类中的一项关键技术，尤其在少量样本的情况下尤为重要。本文介绍了一种基于大型语言模型（LLM）的新型TA方法TARDiS，以解决两阶段TA方法在生成和对齐阶段所面临的固有挑战。在生成阶段，我们提出了两种生成过程，即SEG和CEG，它们通过融入多个特定类别的提示来增强多样性和可分性。在对齐阶段，我们引入了一种类适应（CA）方法，以确保生成的例子通过验证和修改与其目标类别对齐。实验结果表明TARDiS的有效性，在各种少量文本分类任务中优于最先进的大型语言模型TA方法。深入分析证实了每一阶段的详细行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02739v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>文本扩充（TA）在文本分类中尤为关键，特别是在小样本场景中。本文提出了一种新型的大型语言模型基础TA方法TARDiS，旨在解决两阶段TA方法中的生成和校准阶段所面临的挑战。在生成阶段，我们提出了两种生成过程SEG和CEG，融入多种类别特定提示以强化多样性和可分离性。在校准阶段，我们引入类别适应（CA）方法确保生成示例与目标类别对齐，通过验证与修改来实现。实验结果证明TARDiS的有效性，在各种小样本文本分类任务中优于现有大型语言模型基础的TA方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TARDiS是一种基于大型语言模型（LLM）的文本扩充（TA）方法，旨在改进小样本文本分类。</li>
<li>TARDiS解决了传统两阶段文本扩充方法中的生成和校准阶段的挑战。</li>
<li>在生成阶段，TARDiS使用SEG和CEG两种生成过程，通过融入类别特定提示来提高多样性和可分离性。</li>
<li>在校准阶段，TARDiS通过类别适应（CA）方法确保生成的示例与目标类别对齐。</li>
<li>实验证明TARDiS在各种小样本文本分类任务中的性能优于现有方法。</li>
<li>TARDiS方法包含多个阶段，每个阶段都有详细的操作分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f21b7e75473f789bc0af73a01d3937f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f013ecdeb391664c5e128aa31d4be22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a2556a68656eda87da87e19b9baf3d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8147369fe87e6d9cb49a3bd8886a0a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-812c6631646cbb7fabded1ecb1a76eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811d5340a0fad8619ff67b5f37d25902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d063eb847b8988b081b997ed8e47fb76.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Holistic-Semantic-Representation-for-Navigational-Trajectory-Generation"><a href="#Holistic-Semantic-Representation-for-Navigational-Trajectory-Generation" class="headerlink" title="Holistic Semantic Representation for Navigational Trajectory Generation"></a>Holistic Semantic Representation for Navigational Trajectory Generation</h2><p><strong>Authors:Ji Cao, Tongya Zheng, Qinghong Guo, Yu Wang, Junshu Dai, Shunyu Liu, Jie Yang, Jie Song, Mingli Song</strong></p>
<p>Trajectory generation has garnered significant attention from researchers in the field of spatio-temporal analysis, as it can generate substantial synthesized human mobility trajectories that enhance user privacy and alleviate data scarcity. However, existing trajectory generation methods often focus on improving trajectory generation quality from a singular perspective, lacking a comprehensive semantic understanding across various scales. Consequently, we are inspired to develop a HOlistic SEmantic Representation (HOSER) framework for navigational trajectory generation. Given an origin-and-destination (OD) pair and the starting time point of a latent trajectory, we first propose a Road Network Encoder to expand the receptive field of road- and zone-level semantics. Second, we design a Multi-Granularity Trajectory Encoder to integrate the spatio-temporal semantics of the generated trajectory at both the point and trajectory levels. Finally, we employ a Destination-Oriented Navigator to seamlessly integrate destination-oriented guidance. Extensive experiments on three real-world datasets demonstrate that HOSER outperforms state-of-the-art baselines by a significant margin. Moreover, the model’s performance in few-shot learning and zero-shot learning scenarios further verifies the effectiveness of our holistic semantic representation. </p>
<blockquote>
<p>轨迹生成已引起时空分析领域研究人员的广泛关注，因为它可以生成大量合成的人类移动轨迹，增强用户隐私并缓解数据稀缺问题。然而，现有的轨迹生成方法往往从单一角度改进轨迹生成质量，缺乏跨不同尺度的全面语义理解。因此，我们受到启发，开发出用于导航轨迹生成的HOlistic SEmantic Representation（HOSER）框架。给定起点和终点对以及潜在轨迹的起始时间点，我们首先提出一种道路网络编码器，以扩大道路和区域级别的语义感受野。其次，我们设计了一种多粒度轨迹编码器，以整合生成轨迹的点和轨迹级别的时空语义。最后，我们采用目标导向导航器，无缝集成以目的地为导向的导航。在三个真实世界数据集上的大量实验表明，HOSER显著优于最新基线。此外，该模型在少样本学习和零样本学习场景中的性能进一步验证了我们的整体语义表示的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02737v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>轨迹生成受到时空分析领域研究者的关注，因为它能生成大量合成的人类移动轨迹，增强用户隐私并缓解数据稀缺问题。然而，现有轨迹生成方法往往从单一角度改进轨迹生成质量，缺乏跨不同尺度的全面语义理解。因此，我们提出一种整体的语义表示（HOSER）框架来进行导航轨迹生成。给定起点和终点对以及潜在轨迹的起始时间点，我们首先提出道路网络编码器来扩展道路和区域级别的语义感受野。其次，我们设计了一种多粒度轨迹编码器，以整合生成轨迹的点和轨迹级别的时空语义。最后，我们采用面向目的地的导航器无缝集成目的地导向的导航。在三个真实世界数据集上的广泛实验表明，HOSER显著优于最新基线。此外，该模型在少样本学习和零样本学习场景中的表现进一步验证了其全面的语义表示的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>轨迹生成是时空分析领域的重要研究方向，能生成合成的人类移动轨迹，增强用户隐私并缓解数据稀缺问题。</li>
<li>现有轨迹生成方法主要关注提高轨迹生成质量，但缺乏跨不同尺度的全面语义理解。</li>
<li>HOSER框架包含三个主要部分：道路网络编码器、多粒度轨迹编码器和面向目的地的导航器。</li>
<li>道路网络编码器扩大道路和区域级别的语义感受野。</li>
<li>多粒度轨迹编码器整合生成轨迹的点和轨迹级别的时空语义。</li>
<li>面向目的地的导航器无缝集成目的地导向的导航。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5bb98506b35cf3bb59b47de18d4ea712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b312baee0c90a02261e37c17fc5c519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24ca6454840c08c68ce4aa5865168bbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-589977983682146fe063ebdbe9fb5859.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aecd38687e7bdf493cb0e2ed03b3b86c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Noise-Tolerant-Hybrid-Prototypical-Learning-with-Noisy-Web-Data"><a href="#Noise-Tolerant-Hybrid-Prototypical-Learning-with-Noisy-Web-Data" class="headerlink" title="Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data"></a>Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data</h2><p><strong>Authors:Chao Liang, Linchao Zhu, Zongxin Yang, Wei Chen, Yi Yang</strong></p>
<p>We focus on the challenging problem of learning an unbiased classifier from a large number of potentially relevant but noisily labeled web images given only a few clean labeled images. This problem is particularly practical because it reduces the expensive annotation costs by utilizing freely accessible web images with noisy labels. Typically, prototypes are representative images or features used to classify or identify other images. However, in the few clean and many noisy scenarios, the class prototype can be severely biased due to the presence of irrelevant noisy images. The resulting prototypes are less compact and discriminative, as previous methods do not take into account the diverse range of images in the noisy web image collections. On the other hand, the relation modeling between noisy and clean images is not learned for the class prototype generation in an end-to-end manner, which results in a suboptimal class prototype. In this article, we introduce a similarity maximization loss named SimNoiPro. Our SimNoiPro first generates noise-tolerant hybrid prototypes composed of clean and noise-tolerant prototypes and then pulls them closer to each other. Our approach considers the diversity of noisy images by explicit division and overcomes the optimization discrepancy issue. This enables better relation modeling between clean and noisy images and helps extract judicious information from the noisy image set. The evaluation results on two extended few-shot classification benchmarks confirm that our SimNoiPro outperforms prior methods in measuring image relations and cleaning noisy data. </p>
<blockquote>
<p>我们关注从大量可能相关但带有噪声标签的网络图像中学习无偏见分类器的问题，仅使用少量干净标记的图像。这个问题特别实用，因为它通过利用带有噪声标签的可自由访问的网络图像来降低了昂贵的标注成本。然而，通常原型是用于分类或识别其他图像的代表图像或特征。但在少数清洁和大量噪声场景中，由于存在不相关的噪声图像，类原型可能会受到严重偏见。由此产生的原型不够紧凑和鉴别力不强，因为以前的方法没有考虑到噪声网络图像集合中图像的多样性范围。另一方面，类原型的生成并没有以端到端的方式学习噪声图像和干净图像之间的关系建模，这导致了次优的类原型。在本文中，我们引入了一种名为SimNoiPro的相似性最大化损失。我们的SimNoiPro首先生成由清洁和噪声容忍原型组成的噪声容忍混合原型，然后将它们彼此拉近。我们的方法通过显式划分考虑了噪声图像的多样性，并克服了优化差异问题。这能够实现对清洁和噪声图像之间关系的更好建模，并有助于从噪声图像集中提取明智的信息。在两个扩展的小样本分类基准上的评估结果证实，我们的SimNoiPro在衡量图像关系和清理噪声数据方面优于先前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02476v1">PDF</a> Accepted by TOMM 2024</p>
<p><strong>Summary</strong></p>
<p>本文关注从大量潜在相关但带有噪声标签的网络图像中学习无偏见分类器的问题，其中只有少数干净标签的图像。通过引入SimNoiPro相似性最大化损失，生成噪声容忍的混合原型，并将它们彼此拉近，以更好地模拟清洁和噪声图像之间的关系，并从噪声图像集中提取明智的信息。评价结果显示，SimNoiPro在衡量图像关系和清理噪声数据方面优于先前的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文关注从大量网络图像中学习无偏见分类器的问题，这些图像带有噪声标签，但仅有少数干净标签的图像。</li>
<li>引入SimNoiPro相似性最大化损失，用于处理噪声容忍和原型生成。</li>
<li>生成噪声容忍的混合原型，由干净和噪声容忍的原型组成。</li>
<li>将生成的原型彼此拉近，以更好地模拟清洁和噪声图像之间的关系。</li>
<li>方法考虑了噪声图像的多样性，并通过明确划分来克服优化差异问题。</li>
<li>能从噪声图像集中提取明智的信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02476v1/page_0_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ff66ce08ceb54f518bae00f93143bc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366d663e1147f3d5235e51645f835d37.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Generalization-Enhanced-Few-Shot-Object-Detection-in-Remote-Sensing"><a href="#Generalization-Enhanced-Few-Shot-Object-Detection-in-Remote-Sensing" class="headerlink" title="Generalization-Enhanced Few-Shot Object Detection in Remote Sensing"></a>Generalization-Enhanced Few-Shot Object Detection in Remote Sensing</h2><p><strong>Authors:Hui Lin, Nan Li, Pengjuan Yao, Kexin Dong, Yuhan Guo, Danfeng Hong, Ying Zhang, Congcong Wen</strong></p>
<p>Remote sensing object detection is particularly challenging due to the high resolution, multi-scale features, and diverse ground object characteristics inherent in satellite and UAV imagery. These challenges necessitate more advanced approaches for effective object detection in such environments. While deep learning methods have achieved remarkable success in remote sensing object detection, they typically rely on large amounts of labeled data. Acquiring sufficient labeled data, particularly for novel or rare objects, is both challenging and time-consuming in remote sensing scenarios, limiting the generalization capabilities of existing models. To address these challenges, few-shot learning (FSL) has emerged as a promising approach, aiming to enable models to learn new classes from limited labeled examples. Building on this concept, few-shot object detection (FSOD) specifically targets object detection challenges in data-limited conditions. However, the generalization capability of FSOD models, particularly in remote sensing, is often constrained by the complex and diverse characteristics of the objects present in such environments. In this paper, we propose the Generalization-Enhanced Few-Shot Object Detection (GE-FSOD) model to improve the generalization capability in remote sensing FSOD tasks. Our model introduces three key innovations: the Cross-Level Fusion Pyramid Attention Network (CFPAN) for enhanced multi-scale feature representation, the Multi-Stage Refinement Region Proposal Network (MRRPN) for more accurate region proposals, and the Generalized Classification Loss (GCL) for improved classification performance in few-shot scenarios. Extensive experiments on the DIOR and NWPU VHR-10 datasets show that our model achieves state-of-the-art performance for few-shot object detection in remote sensing. </p>
<blockquote>
<p>遥感目标检测因其固有的高分辨率、多尺度特征和地面目标特性多样性，在卫星和无人机图像中面临巨大挑战。这些挑战要求更先进的方法在这样的环境中进行有效的目标检测。虽然深度学习方法在遥感目标检测中取得了显著的成功，但它们通常依赖于大量的标记数据。在遥感场景中，获取足够的有标签数据，特别是对于新型或稀有目标，既具有挑战性又非常耗时，这限制了现有模型的泛化能力。为了解决这些挑战，小样本学习（FSL）作为一种有前景的方法应运而生，旨在使模型能够从有限的标记示例中学习新的类别。在此基础上，小样本目标检测（FSOD）专门解决数据有限条件下的目标检测挑战。然而，FSOD模型的泛化能力，特别是在遥感领域，往往受到此类环境中对象复杂和多样特性的制约。在本文中，我们提出了通用增强小样本目标检测（GE-FSOD）模型，以提高遥感FSOD任务的泛化能力。我们的模型引入了三个关键创新点：用于增强多尺度特征表示的跨级融合金字塔注意力网络（CFPAN）、用于更精确区域提议的多阶段细化区域提议网络（MRRPN）以及用于改进小样本场景中分类性能的广义分类损失（GCL）。在DIOR和NWPU VHR-10数据集上的广泛实验表明，我们的模型在遥感小样本目标检测方面达到了最新水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了遥感物体检测面临的挑战，如高分辨率、多尺度特征和地面物体特性的多样性。深度学习方法虽然取得了显著的成功，但依赖于大量标记数据。为解决获取足够标记数据的挑战，文中提出了基于泛化能力提升的少镜头目标检测模型（GE-FSOD）。该模型引入了三个关键创新点：跨层次融合金字塔注意力网络（CFPAN）用于增强多尺度特征表示，多阶段细化区域提议网络（MRRPN）用于更准确的区域提议，以及广义分类损失（GCL）用于提高少镜头场景中的分类性能。在DIOR和NWPU VHR-10数据集上的实验表明，该模型在遥感少镜头目标检测方面达到了最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>遥感物体检测面临高分辨率、多尺度特征和地面物体多样性等挑战。</li>
<li>深度学习方法在遥感物体检测中取得了显著成功，但依赖大量标记数据。</li>
<li>获取足够标记数据在遥感场景中具有挑战性和时间成本。</li>
<li>少镜头学习（FSL）旨在从有限标记示例中学习新类别，是解决上述挑战的有前途的方法。</li>
<li>本文提出了基于泛化能力提升的少镜头目标检测模型（GE-FSOD）。</li>
<li>GE-FSOD模型引入了三个关键创新点：CFPAN、MRRPN和GCL。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02474">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1649ee025e78126ba0c4fd31374651ad.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02474v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02474v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Advancing-Pancreatic-Cancer-Prediction-with-a-Next-Visit-Token-Prediction-Head-on-top-of-Med-BERT"><a href="#Advancing-Pancreatic-Cancer-Prediction-with-a-Next-Visit-Token-Prediction-Head-on-top-of-Med-BERT" class="headerlink" title="Advancing Pancreatic Cancer Prediction with a Next Visit Token   Prediction Head on top of Med-BERT"></a>Advancing Pancreatic Cancer Prediction with a Next Visit Token   Prediction Head on top of Med-BERT</h2><p><strong>Authors:Jianping He, Laila Rasmy, Degui Zhi, Cui Tao</strong></p>
<p>Background: Recently, numerous foundation models pretrained on extensive data have demonstrated efficacy in disease prediction using Electronic Health Records (EHRs). However, there remains some unanswered questions on how to best utilize such models especially with very small fine-tuning cohorts. Methods: We utilized Med-BERT, an EHR-specific foundation model, and reformulated the disease binary prediction task into a token prediction task and a next visit mask token prediction task to align with Med-BERT’s pretraining task format in order to improve the accuracy of pancreatic cancer (PaCa) prediction in both few-shot and fully supervised settings. Results: The reformulation of the task into a token prediction task, referred to as Med-BERT-Sum, demonstrates slightly superior performance in both few-shot scenarios and larger data samples. Furthermore, reformulating the prediction task as a Next Visit Mask Token Prediction task (Med-BERT-Mask) significantly outperforms the conventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to 7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These findings highlight that aligning the downstream task with Med-BERT’s pretraining objectives substantially enhances the model’s predictive capabilities, thereby improving its effectiveness in predicting both rare and common diseases. Conclusion: Reformatting disease prediction tasks to align with the pretraining of foundation models enhances prediction accuracy, leading to earlier detection and timely intervention. This approach improves treatment effectiveness, survival rates, and overall patient outcomes for PaCa and potentially other cancers. </p>
<blockquote>
<p>背景：最近，许多在大量数据上进行预训练的基石模型在利用电子健康记录（EHRs）进行疾病预测方面表现出了有效性。然而，关于如何最好在非常小的微调群体中使用这些模型的问题仍未得到解答。方法：我们使用了针对EHR的基石模型Med-BERT，并将疾病二分类预测任务重新制定为标记预测任务和下次就诊掩码标记预测任务，以与Med-BERT的预训练任务格式对齐，以提高胰腺癌（PaCa）在少量镜头和完全监督设置中的预测准确性。结果：将任务重新制定为标记预测任务（称为Med-BERT-Sum）在少量镜头场景和较大的数据样本中都表现出略微优越的性能。此外，将预测任务重新制定为下次就诊掩码标记预测任务（Med-BERT-Mask）比传统的二分类（BC）预测任务（Med-BERT-BC）在样本量从10到500的少量镜头场景中高出3%至7%。这些发现表明，将下游任务与Med-BERT的预训练目标对齐可以显著提高模型的预测能力，从而提高其在预测罕见疾病和常见疾病方面的有效性。结论：重新格式化疾病预测任务，以与基石模型的预训练对齐，可以提高预测准确性，从而实现早期检测和及时干预。这种方法提高了胰腺癌和其他潜在癌症的治疗效率、存活率和整体患者结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用Med-BERT这一针对电子健康记录（EHR）的预训练模型进行疾病预测的方法。通过改革任务形式，将疾病二分类预测任务转化为token预测任务和下次就诊mask token预测任务，提高了胰腺癌预测的准确性，特别是在小样数据场景下。实验结果表明，改革后的模型性能在少数样本和大样本下均表现更优，显示了对齐下游任务和预训练任务能提高模型的预测能力，特别是在预测罕见病方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>采用Med-BERT模型进行疾病预测具有有效性和潜力。</li>
<li>任务形式的改革，如token预测任务和下次就诊mask token预测任务，能提高胰腺癌预测的准确性。</li>
<li>在小样数据场景下，改革后的模型表现出更高的性能。</li>
<li>对齐下游任务和预训练任务能提高模型的预测能力。</li>
<li>该方法可能对预测罕见疾病和常见疾病都有效。</li>
<li>改革后的模型有助于提高胰腺癌及其他癌症的治疗效果和患者生存率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02044">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Training-Free-Point-Cloud-Recognition-Based-on-Geometric-and-Semantic-Information-Fusion"><a href="#Training-Free-Point-Cloud-Recognition-Based-on-Geometric-and-Semantic-Information-Fusion" class="headerlink" title="Training-Free Point Cloud Recognition Based on Geometric and Semantic   Information Fusion"></a>Training-Free Point Cloud Recognition Based on Geometric and Semantic   Information Fusion</h2><p><strong>Authors:Yan Chen, Di Huang, Zhichao Liao, Xi Cheng, Xinghui Li, Lone Zeng</strong></p>
<p>The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we are the first to propose a novel training-free method that integrates both geometric and semantic features. For the geometric branch, we adopt a non-parametric strategy to extract geometric features. In the semantic branch, we leverage a model aligned with text features to obtain semantic features. Additionally, we introduce the GFE module to complement the geometric information of point clouds and the MFF module to improve performance in few-shot settings. Experimental results demonstrate that our method outperforms existing state-of-the-art training-free approaches on mainstream benchmark datasets, including ModelNet and ScanObiectNN. </p>
<blockquote>
<p>采用无训练方法处理点云识别的趋势正越来越受欢迎，因为它能显著减少计算资源和时间成本。然而，现有方法通常仅限于提取几何或语义特征。为了解决这一局限性，我们首次提出了一种新型的无训练方法，该方法融合了几何和语义特征。在几何分支中，我们采用非参数策略来提取几何特征。在语义分支中，我们利用与文本特征对齐的模型来获取语义特征。此外，我们还引入了GFE模块来补充点云的几何信息，并引入了MFF模块来提高小样本场景下的性能。实验结果表明，我们的方法在主流基准数据集（包括ModelNet和ScanObiectNN）上的表现优于现有的最新无训练方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04760v4">PDF</a> </p>
<p><strong>Summary</strong><br>训练免费的方法在点云识别中越来越受欢迎，因为它能显著减少计算资源和时间成本。一种新型训练免费方法首次被提出，它整合了几何和语义特征，提高了点云识别的性能。采用非参数策略提取几何特征，利用与文本特征对齐的模型获取语义特征。此外，还引入了GFE模块来补充点云的几何信息，MFF模块提升在few-shot场景中的表现。实验结果证明，该方法在主流基准数据集ModelNet和ScanObiectNN上的表现优于现有的先进训练免费方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>训练免费方法在点云识别中逐渐普及，主要得益于其减少的计算资源和时间成本。</li>
<li>新型训练免费方法首次结合几何和语义特征进行点云识别。</li>
<li>采用非参数策略提取几何特征，利用与文本特征对齐的模型获取语义特征。</li>
<li>GFE模块的引入用于补充点云的几何信息。</li>
<li>MFF模块旨在提升在数据稀缺场景（few-shot）下的表现。</li>
<li>实验结果表明，该方法在主流基准数据集上的表现优于其他训练免费方法。</li>
<li>该方法具有潜力为点云识别领域带来革新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Distillation-Learning-Guided-by-Image-Reconstruction-for-One-Shot-Medical-Image-Segmentation"><a href="#Distillation-Learning-Guided-by-Image-Reconstruction-for-One-Shot-Medical-Image-Segmentation" class="headerlink" title="Distillation Learning Guided by Image Reconstruction for One-Shot   Medical Image Segmentation"></a>Distillation Learning Guided by Image Reconstruction for One-Shot   Medical Image Segmentation</h2><p><strong>Authors:Feng Zhou, Yanjie Zhou, Longjie Wang, Yun Peng, David E. Carlson, Liyun Tu</strong></p>
<p>Traditional one-shot medical image segmentation (MIS) methods use registration networks to propagate labels from a reference atlas or rely on comprehensive sampling strategies to generate synthetic labeled data for training. However, these methods often struggle with registration errors and low-quality synthetic images, leading to poor performance and generalization. To overcome this, we introduce a novel one-shot MIS framework based on knowledge distillation, which allows the network to directly ‘see’ real images through a distillation process guided by image reconstruction. It focuses on anatomical structures in a single labeled image and a few unlabeled ones. A registration-based data augmentation network creates realistic, labeled samples, while a feature distillation module helps the student network learn segmentation from these samples, guided by the teacher network. During inference, the streamlined student network accurately segments new images. Evaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen CT, and VerSe for vertebrae CT) show superior segmentation performance and generalization across different medical image datasets and modalities compared to leading methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NoviceFodder/OS-MedSeg">https://github.com/NoviceFodder/OS-MedSeg</a>. </p>
<blockquote>
<p>传统的单镜头医学图像分割（MIS）方法使用注册网络来传播来自参考图谱的标签，或者依赖于全面的采样策略来生成合成标记数据进行训练。然而，这些方法常常受到注册错误和低质量合成图像的影响，导致性能不佳和泛化能力差。为了克服这一问题，我们引入了一种基于知识蒸馏的新型单镜头MIS框架，该框架允许网络通过由图像重建引导的蒸馏过程直接“查看”真实图像。它专注于单个标记图像和少量未标记图像中的解剖结构。基于注册的数据增强网络创建逼真的标记样本，而特征蒸馏模块则帮助学生网络从这些样本中学习分割，并受教师网络的指导。在推理过程中，简化的学生网络能够准确地对新图像进行分割。在三个公共数据集（OASIS用于T1脑MRI、BCV用于腹部CT和VerSe用于椎体CT）上的评估显示，与领先的方法相比，我们的方法在医学图像数据集和不同模态的分割性能及泛化能力上具有优势。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/NoviceFodder/OS-MedSeg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NoviceFodder/OS-MedSeg上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.03616v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于知识蒸馏的新型单次医学图像分割（MIS）框架，解决了传统方法面临的注册误差和低质量合成图像问题。该框架通过图像重建引导蒸馏过程，使网络能够直接“看到”真实图像。它专注于单张标记图像和少量未标记图像中的解剖结构。注册式数据增强网络创建逼真的标记样本，特征蒸馏模块帮助学生网络从这些样本中学习分割，受教师网络的引导。在新图像上的评估表现出卓越的分割性能和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新型单次医学图像分割框架引入知识蒸馏技术，解决传统方法的注册误差和低质量合成图像问题。</li>
<li>框架允许网络通过蒸馏过程直接“看到”真实图像。</li>
<li>框架专注于单张标记图像和少量未标记图像中的解剖结构。</li>
<li>使用注册式数据增强网络创建逼真的标记样本。</li>
<li>特征蒸馏模块帮助学生网络从标记样本中学习分割，受教师网络的引导。</li>
<li>在三个公共数据集上的评估显示，该框架的分割性能和泛化能力优于领先方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.03616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Recent-Backdoor-Attacks-and-Defenses-in-Large-Language-Models"><a href="#A-Survey-of-Recent-Backdoor-Attacks-and-Defenses-in-Large-Language-Models" class="headerlink" title="A Survey of Recent Backdoor Attacks and Defenses in Large Language   Models"></a>A Survey of Recent Backdoor Attacks and Defenses in Large Language   Models</h2><p><strong>Authors:Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Xiaoyu Xu, Xiaobao Wu, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan</strong></p>
<p>Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms. </p>
<blockquote>
<p>大型语言模型（LLMs）弥合了人类语言理解与复杂问题求解之间的鸿沟，并在多个自然语言处理任务上达到了最先进的性能，特别是在小样本和零样本设置中。尽管LLM的效用已经得到了证明，但由于计算资源的限制，用户不得不使用开源语言模型或将整个训练过程外包给第三方平台。然而，研究表明，语言模型容易受到潜在的安全漏洞的影响，特别是在后门攻击中。后门攻击旨在通过毒害训练样本或模型权重来引入有针对性的漏洞，从而使攻击者能够通过恶意触发因素来操纵模型响应。虽然关于后门攻击的现有调查提供了全面的概述，但它们缺乏对针对LLM的后门攻击的深入研究。为了弥补这一差距并了解该领域的最新趋势，本文提出了一种关于LLM后门攻击的新视角，重点关注微调方法。具体来说，我们将后门攻击系统地分为三类：全参数微调、参数有效微调和无微调。基于大量的审查结果，我们还讨论了未来研究后门攻击的关键问题，如进一步探索不需要微调的攻击算法，或开发更隐蔽的攻击算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06852v5">PDF</a> Accepted in TMLR</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多个自然语言处理任务上实现了卓越的性能，特别是在小样例和零样例设置下。然而，由于计算资源限制，用户需要使用开源语言模型或将整个训练过程外包给第三方平台。此外，语言模型容易遭受安全漏洞攻击，特别是后门攻击。本文重点介绍了针对LLM的后门攻击的新视角，重点介绍了微调方法。我们将后门攻击系统地分为全参数微调、参数有效微调和无微调三类，并基于大量研究讨论了未来研究的关键问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在自然语言处理任务上表现卓越，尤其在小样例和零样例设置下。</li>
<li>由于计算资源限制，用户需依赖开源语言模型或第三方平台进行训练。</li>
<li>语言模型容易受到后门攻击等安全漏洞的影响。</li>
<li>后门攻击可以通过对训练样本或模型权重进行毒害，操纵模型响应。</li>
<li>现有后门攻击调查缺乏针对LLMs的深入研究。</li>
<li>本文对后门攻击针对LLMs进行了分类，包括全参数微调、参数有效微调和无微调三种类型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RETTA-Retrieval-Enhanced-Test-Time-Adaptation-for-Zero-Shot-Video-Captioning"><a href="#RETTA-Retrieval-Enhanced-Test-Time-Adaptation-for-Zero-Shot-Video-Captioning" class="headerlink" title="RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video   Captioning"></a>RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video   Captioning</h2><p><strong>Authors:Yunchuan Ma, Laiyun Qing, Guorong Li, Yuankai Qi, Amin Beheshti, Quan Z. Sheng, Qingming Huang</strong></p>
<p>Despite the significant progress of fully-supervised video captioning, zero-shot methods remain much less explored. In this paper, we propose a novel zero-shot video captioning framework named Retrieval-Enhanced Test-Time Adaptation (RETTA), which takes advantage of existing pretrained large-scale vision and language models to directly generate captions with test-time adaptation. Specifically, we bridge video and text using four key models: a general video-text retrieval model XCLIP, a general image-text matching model CLIP, a text alignment model AnglE, and a text generation model GPT-2, due to their source-code availability. The main challenge is how to enable the text generation model to be sufficiently aware of the content in a given video so as to generate corresponding captions. To address this problem, we propose using learnable tokens as a communication medium among these four frozen models GPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains these tokens with training data, we propose to learn these tokens with soft targets of the inference data under several carefully crafted loss functions, which enable the tokens to absorb video information catered for GPT-2. This procedure can be efficiently done in just a few iterations (we use 16 iterations in the experiments) and does not require ground truth data. Extensive experimental results on three widely used datasets, MSR-VTT, MSVD, and VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric CIDEr compared to several state-of-the-art zero-shot video captioning methods. </p>
<blockquote>
<p>尽管全监督视频描述取得了重大进展，但零样本方法仍然被研究得较少。在本文中，我们提出了一种新型的零样本视频描述框架，名为“基于检索的测试时间自适应（RETTA）”，它利用现有的预训练大规模视觉和语言模型，通过测试时间自适应直接生成描述。具体来说，我们使用四个关键模型来连接视频和文本：通用的视频文本检索模型XCLIP，通用的图像文本匹配模型CLIP，文本对齐模型AnglE，以及文本生成模型GPT-2，原因是这些模型有源代码可供使用。主要挑战在于如何让文本生成模型充分意识到给定视频的内容，从而生成相应的描述。为了解决这个问题，我们提出使用可学习的令牌作为这四个冻结模型GPT-2、XCLIP、CLIP和AnglE之间的通信媒介。不同于使用训练数据训练这些令牌的传统方式，我们提出使用精心设计的损失函数下的推理数据的软目标来学习这些令牌，使令牌能够吸收针对GPT-2的视频信息。这个过程只需几次迭代就能高效完成（我们在实验中使用16次迭代），并且不需要真实数据。在三个广泛使用的数据集MSR-VTT、MSVD和VATEX上的大量实验结果表明，与几种最先进的零样本视频描述方法相比，主要指标CIDEr的绝对改进率为5.1%~32.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07046v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Retrieval-Enhanced Test-Time Adaptation（RETTA）的零样本视频描述框架，利用现有的预训练大规模视觉和语言模型，通过测试时适应技术直接生成视频描述。该框架结合四种模型，包括视频文本检索模型XCLIP、图像文本匹配模型CLIP、文本对齐模型AnglE和文本生成模型GPT-2，通过可学习令牌作为沟通媒介，在几个精心设计的损失函数下，使令牌吸收视频信息并适应GPT-2，以生成相应的视频描述。这一流程在仅几次迭代中即可完成，无需真实数据。在三个广泛应用的数据集上进行了实验验证，与最先进的零样本视频描述方法相比，主要指标CIDEr有显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种名为RETTA的零样本视频描述框架，结合预训练的大型视觉和语言模型进行视频描述生成。</li>
<li>利用四种关键模型：XCLIP、CLIP、AnglE和GPT-2，通过可学习令牌作为媒介进行信息交互。</li>
<li>提出了使用软目标在测试时学习令牌的方法，通过精心设计的损失函数使令牌吸收视频信息并适应GPT-2。</li>
<li>该流程无需真实数据，可在仅几次迭代中完成。</li>
<li>在三个广泛应用的数据集上进行了实验验证，显示出显著的性能改进。</li>
<li>RETTA框架在零样本视频描述方面具有巨大的潜力，为未来的相关研究提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07046">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2405.07046v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2405.07046v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2405.07046v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8aa9463cad6600699429fa08e88254a4.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-01-08  Plasma-CycleGAN Plasma Biomarker-Guided MRI to PET Cross-modality   Translation Using Conditional CycleGAN
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ac39c89b14e8b743c89e1578ca0993f4.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-01-08  Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">8671.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
