<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  RW-Net Enhancing Few-Shot Point Cloud Classification with a Wavelet   Transform Projection-based Network">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5b312baee0c90a02261e37c17fc5c519.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-08-æ›´æ–°"><a href="#2025-01-08-æ›´æ–°" class="headerlink" title="2025-01-08 æ›´æ–°"></a>2025-01-08 æ›´æ–°</h1><h2 id="RW-Net-Enhancing-Few-Shot-Point-Cloud-Classification-with-a-Wavelet-Transform-Projection-based-Network"><a href="#RW-Net-Enhancing-Few-Shot-Point-Cloud-Classification-with-a-Wavelet-Transform-Projection-based-Network" class="headerlink" title="RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet   Transform Projection-based Network"></a>RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet   Transform Projection-based Network</h2><p><strong>Authors:Haosheng Zhang, Hao Huang</strong></p>
<p>In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms. This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples. To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets. This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture. The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy. This process ensures the retention of essential information for effective decision-making, optimizing the modelâ€™s ability to learn from limited data. Complementing RDE, incorporating the wavelet transform further enhances the frameworkâ€™s capability to generalize in low-data regimes. By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects. These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains. To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification. The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios. </p>
<blockquote>
<p>åœ¨3Då¯¹è±¡åˆ†ç±»é¢†åŸŸï¼Œä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜åœ¨äºè§£å†³æ ‡è®°æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†ä¼ ç»Ÿæ•°æ®å¯†é›†å‹å­¦ä¹ æ¨¡å¼çš„åº”ç”¨ã€‚è¿™ç§æŒ‘æˆ˜åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å°¤ä¸ºçªå‡ºï¼Œè¯¥åœºæ™¯çš„ç›®æ ‡æ˜¯ä»æå°‘çš„æ ‡æ³¨æ ·æœ¬ä¸­å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œè¯†åˆ«å’Œåˆ©ç”¨3Då¯¹è±¡çš„æœ€çªå‡ºå’Œæœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾è‡³å…³é‡è¦ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œå¹¶å‡å°‘å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†RW-Netï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆé€Ÿç‡å¤±çœŸè§£é‡Šï¼ˆRDEï¼‰å’Œå°æ³¢å˜æ¢ï¼Œèå…¥åŸºäºæŠ•å½±çš„å…ˆè¿›3Då¯¹è±¡åˆ†ç±»æ¶æ„ï¼Œä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨RDEé€šè¿‡è¯†åˆ«å¹¶ä¿ç•™æœ€æœ‰ä¿¡æ¯çš„æ•°æ®æˆåˆ†æ¥æå–å…³é”®ç‰¹å¾ï¼ŒåŒæ—¶å‡å°‘å†—ä½™ã€‚è¿™ä¸€è¿‡ç¨‹ç¡®ä¿äº†ä¿ç•™ç”¨äºæœ‰æ•ˆå†³ç­–çš„å¿…è¦ä¿¡æ¯ï¼Œä¼˜åŒ–æ¨¡å‹ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ çš„èƒ½åŠ›.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03221v1">PDF</a> 11 pages, 5 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨3Dç›®æ ‡åˆ†ç±»é¢†åŸŸï¼Œæ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯æœ€å¤§çš„æŒ‘æˆ˜ä¹‹ä¸€ï¼Œè¿™ä¹Ÿé™åˆ¶äº†ä¼ ç»Ÿæ•°æ®å¯†é›†å‹å­¦ä¹ æ¨¡å¼çš„é€‚ç”¨æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä¸­ï¼Œç›®æ ‡æ˜¯ä»æå°‘é‡çš„æ ‡æ³¨æ ·æœ¬ä¸­å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œéœ€è¦è¯†åˆ«å¹¶åˆ©ç”¨æœ€æ˜¾è‘—å’Œæœ‰é‰´åˆ«åŠ›çš„ä¸‰ç»´ç‰©ä½“çš„ç‰¹å¾ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å¹¶å‡å°‘å¯¹å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†é›†çš„ä¾èµ–ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†RW-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•´åˆç‡å¤±çœŸè§£é‡Šï¼ˆRDEï¼‰å’Œå°æ³¢å˜æ¢æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚æ‰€æå‡ºçš„åˆ©ç”¨RDEæå–å…³é”®ç‰¹å¾çš„æ–¹æ³•ç¡®ä¿äº†æœ‰æ•ˆå†³ç­–æ‰€éœ€çš„å¿…è¦ä¿¡æ¯çš„ä¿ç•™ï¼Œä¼˜åŒ–äº†æ¨¡å‹ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚ä¸ºäº†éªŒè¯RW-Netçš„æœ‰æ•ˆæ€§ï¼Œåœ¨ModelNet40ã€ModelNet40-Cå’ŒScanObjectNNä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤„ç†ç¨€ç¼ºçš„æ ‡æ³¨æ•°æ®æ—¶ï¼Œä¼ ç»Ÿçš„æ•°æ®å¯†é›†å‹å­¦ä¹ æ¨¡å¼é¢ä¸´æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä¸­ï¼Œå®ç°ç¨³å¥æ³›åŒ–çš„éš¾åº¦æ›´å¤§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶RW-Netæ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œç»“åˆäº†ç‡å¤±çœŸè§£é‡Šï¼ˆRDEï¼‰å’Œå°æ³¢å˜æ¢æŠ€æœ¯ã€‚</li>
<li>RDEæŠ€æœ¯ç”¨äºæå–å…³é”®ç‰¹å¾ï¼Œè¯†åˆ«å¹¶ä¿ç•™æœ€å…·æœ‰ä¿¡æ¯é‡çš„æ•°æ®ç»„ä»¶ï¼ŒåŒæ—¶å‡å°‘å†—ä½™ä¿¡æ¯ã€‚è¿™ç¡®ä¿äº†æœ‰æ•ˆå†³ç­–æ‰€éœ€çš„å¿…è¦ä¿¡æ¯çš„ä¿ç•™ã€‚</li>
<li>å°æ³¢å˜æ¢å¢å¼ºäº†æ¡†æ¶åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®ƒé€šè¿‡å¼ºè°ƒè¾“å…¥æ•°æ®çš„ä½é¢‘æˆåˆ†æ¥æ•æ‰ä¸‰ç»´ç‰©ä½“çš„åŸºæœ¬å‡ ä½•å’Œç»“æ„å±æ€§ã€‚è¿™äº›å±æ€§æœ‰åŠ©äºå‡è½»è¿‡æ‹Ÿåˆé—®é¢˜å¹¶æé«˜åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba498073ebfd3fa53d6b5634cd0c3ec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-993d8371fde8ed1e34681ed5617f46e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c19af3318b6ae23c4a6c2bddff0e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68d7136609548084c3dddda3c3638c2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed312ce19450517eab609d222e73e650.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f93be0b109b9ce0d1c2dc5efe0bccd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Semantic-Captioning-Benchmark-Dataset-and-Graph-Aware-Few-Shot-In-Context-Learning-for-SQL2Text"><a href="#Semantic-Captioning-Benchmark-Dataset-and-Graph-Aware-Few-Shot-In-Context-Learning-for-SQL2Text" class="headerlink" title="Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot   In-Context Learning for SQL2Text"></a>Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot   In-Context Learning for SQL2Text</h2><p><strong>Authors:Ali Al-Lawati, Jason Lucas, Prasenjit Mitra</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \url{<a target="_blank" rel="noopener" href="https://github.com/aliwister/ast-icl%7D">https://github.com/aliwister/ast-icl}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºæ­£å¼ä»£ç è¡¨ç¤ºçš„è¯­ä¹‰è§£æã€‚ç„¶è€Œï¼Œå°†ä»£ç è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€çš„é€†å‘è¿‡ç¨‹ï¼Œå³è¯­ä¹‰æè¿°ï¼Œå—åˆ°çš„å…³æ³¨åº¦è¾ƒå°‘ã€‚éšç€LLMè¢«é›†æˆåˆ°ä»£ç ç”Ÿæˆã€å®‰å…¨åˆ†æå’Œæ•™è‚²ç›®çš„çš„å¹³å°ä¸­ï¼Œè¿™é¡¹ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨SQLæŸ¥è¯¢çš„æ ‡æ³¨ï¼ˆSQL2Textï¼‰ï¼Œä»¥è§£å†³åœ¨LLMç”Ÿæˆçš„ä»£ç å­˜åœ¨æ½œåœ¨å®‰å…¨é£é™©çš„æ—¶ä»£ï¼Œç†è§£å’Œè§£é‡ŠSQLæŸ¥è¯¢çš„è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬é€šè¿‡å¯¹GPT-4oå¼•å…¥è¿­ä»£ICLæç¤ºæ¥é‡æ–°åˆ©ç”¨Text2SQLæ•°æ®é›†è¿›è¡ŒSQL2Textä»»åŠ¡ï¼Œç”Ÿæˆå¤šä¸ªé™„åŠ è¯è¯­ï¼Œè¿™å¢å¼ºäº†æ•°æ®é›†åœ¨é€†å‘ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å®éªŒä½¿ç”¨åŸºäºä¸åŒæ ·æœ¬é€‰æ‹©æ–¹æ³•è¿›è¡Œçš„ä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼ˆICLï¼‰ï¼Œä¾§é‡äºæ›´å°ã€æ›´è®¡ç®—é«˜æ•ˆçš„LLMã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨SQLçš„å›ºæœ‰å›¾å½¢å±æ€§è¿›è¡ŒICLæ ·æœ¬é€‰æ‹©æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼ŒBLEUå¾—åˆ†æé«˜äº†é«˜è¾¾39%ï¼Œå¹¶ä¸”æ¯”å…¶ä»–æ–¹æ³•æä¾›äº†æ›´å¥½çš„ç»“æœã€‚æ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒï¼š\url{<a target="_blank" rel="noopener" href="https://github.com/aliwister/ast-icl%7D%E3%80%82">https://github.com/aliwister/ast-icl}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03166v1">PDF</a> Accepted to COLINGâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰æ•æ‰é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†ä»£ç è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚é’ˆå¯¹SQLæŸ¥è¯¢çš„è¯­ä¹‰æ•æ‰ï¼ˆSQL2Textï¼‰é—®é¢˜ï¼Œæ–‡ç« ä½¿ç”¨GPT-4oè¿›è¡Œè¿­ä»£ICLæç¤ºæ¥ç”Ÿæˆé¢å¤–çš„æè¿°è¯­å¥ï¼Œå¢å¼ºæ•°æ®é›†å¯¹åå‘ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚å®éªŒé‡‡ç”¨åŸºäºä¸åŒæ ·æœ¬é€‰æ‹©æ–¹æ³•çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå¹¶å¼ºè°ƒæ›´å°ã€æ›´é«˜æ•ˆçš„LLMçš„ä¼˜åŠ¿ã€‚åˆ©ç”¨SQLçš„å†…åœ¨å›¾å½¢å±æ€§è¿›è¡ŒICLæ ·æœ¬é€‰æ‹©æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼ŒBLEUå¾—åˆ†æé«˜äº†é«˜è¾¾39%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­ä¹‰æ•æ‰ï¼Œç‰¹åˆ«æ˜¯ä»£ç åˆ°è‡ªç„¶è¯­è¨€çš„è½¬åŒ–ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>SQLæŸ¥è¯¢çš„è¯­ä¹‰æ•æ‰ï¼ˆSQL2Textï¼‰å¯¹äºç†è§£LLMç”Ÿæˆçš„SQLæŸ¥è¯¢è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨å­˜åœ¨æ½œåœ¨å®‰å…¨é£é™©çš„ç¯å¢ƒä¸‹ã€‚</li>
<li>ä½¿ç”¨GPT-4oè¿›è¡Œè¿­ä»£ICLæç¤ºï¼Œé€šè¿‡ç”Ÿæˆé¢å¤–çš„æè¿°è¯­å¥æ¥å¢å¼ºæ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨åŸºäºä¸åŒæ ·æœ¬é€‰æ‹©æ–¹æ³•çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è¿›è¡Œå®éªŒã€‚</li>
<li>åˆ©ç”¨SQLçš„å†…åœ¨å›¾å½¢å±æ€§è¿›è¡ŒICLæ ·æœ¬é€‰æ‹©æ˜¾è‘—æé«˜æ€§èƒ½ï¼ŒBLEUå¾—åˆ†æå‡é«˜è¾¾39%ã€‚</li>
<li>ä¸éšæœºé€‰æ‹©å’Œå…¶å®ƒæ–¹æ³•ç›¸æ¯”ï¼Œåˆ©ç”¨SQLå†…åœ¨å›¾å½¢å±æ€§çš„ICLæ ·æœ¬é€‰æ‹©è¡¨ç°å‡ºæ›´å¥½çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-26fb357827978697f05a64db295a6a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99b3b8bfbaa07b56c9cd7ac29d35a5e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9160cd6db4dc6a9866e9a383d80a8947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa801f8289f43e48bb19dbe024fb5efe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Graph-based-Retrieval-Augmented-Generation-for-Dynamic-Few-shot-Text-Classification"><a href="#Graph-based-Retrieval-Augmented-Generation-for-Dynamic-Few-shot-Text-Classification" class="headerlink" title="Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text   Classification"></a>Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text   Classification</h2><p><strong>Authors:Yubo Wang, Haoyang Li, Fei Teng, Lei Chen</strong></p>
<p>Text classification is a fundamental task in natural language processing, pivotal to various applications such as query optimization, data integration, and schema matching. While neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and target labels frequently evolve based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to predict text labels. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. GORAG constructs and maintains an adaptive information graph by extracting side information across all target texts, rather than treating each input independently. It employs a weighted edge mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and accurate contextual information. </p>
<blockquote>
<p>æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œåœ¨æŸ¥è¯¢ä¼˜åŒ–ã€æ•°æ®é›†æˆå’Œæ¨¡å¼åŒ¹é…ç­‰åº”ç”¨ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚å°½ç®¡åŸºäºç¥ç»ç½‘ç»œï¼ˆå¦‚CNNå’ŒBERTï¼‰çš„æ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºä¸°å¯Œçš„æ ‡è®°è®­ç»ƒæ•°æ®ã€‚è¿™ç§ä¾èµ–æ€§ä½¿å¾—è¿™äº›æ¨¡å‹åœ¨åŠ¨æ€å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä¸­çš„æ•ˆæœè¾ƒå·®ï¼Œå…¶ä¸­æ ‡è®°æ•°æ®ç¨€ç¼ºï¼Œç›®æ ‡æ ‡ç­¾ç»å¸¸æ ¹æ®åº”ç”¨éœ€æ±‚è€Œæ¼”å˜ã€‚æœ€è¿‘ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›é¢„è®­ç»ƒå’Œä¸Šä¸‹æ–‡ç†è§£ï¼Œå®ƒä»¬æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å½“å‰çš„æ–¹æ³•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›æ–‡æœ¬è¾“å…¥ã€å€™é€‰æ ‡ç­¾å’Œé¢å¤–çš„ä¾§é¢ä¿¡æ¯ï¼ˆä¾‹å¦‚æè¿°ï¼‰æ¥é¢„æµ‹æ–‡æœ¬æ ‡ç­¾ã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°è¾“å…¥å¤§å°å¢åŠ å’Œé€šè¿‡ä¾§é¢ä¿¡æ¯å¤„ç†å¼•å…¥çš„å™ªå£°çš„é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„åœ¨çº¿æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œåä¸ºGORAGï¼Œç”¨äºåŠ¨æ€å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»ã€‚GORAGé€šè¿‡æå–æ‰€æœ‰ç›®æ ‡æ–‡æœ¬çš„ä¾§é¢ä¿¡æ¯æ¥æ„å»ºå’Œç»´æŠ¤è‡ªé€‚åº”ä¿¡æ¯å›¾ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹å¤„ç†æ¯ä¸ªè¾“å…¥ã€‚å®ƒé‡‡ç”¨åŠ æƒè¾¹æœºåˆ¶æ¥ä¼˜å…ˆè€ƒè™‘æå–ä¿¡æ¯çš„é‡è¦æ€§å’Œå¯é æ€§ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹æ¯ä¸ªæ–‡æœ¬è¾“å…¥é‡èº«å®šåˆ¶çš„æœ€ä½æˆæœ¬ç”Ÿæˆæ ‘åŠ¨æ€æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGORAGé€šè¿‡æä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„ä¸Šæ–‡ä¿¡æ¯ï¼Œåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå›¾çš„åœ¨çº¿æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆGORAGï¼‰åœ¨åŠ¨æ€å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹çš„ä¸è¶³ï¼ŒGORAGé€šè¿‡æ„å»ºå’Œç»´æŠ¤ä¸€ä¸ªè‡ªé€‚åº”ä¿¡æ¯å›¾ï¼Œæå–ç›®æ ‡æ–‡æœ¬çš„æ‰€æœ‰ä¾§ä¿¡æ¯ï¼Œé‡‡ç”¨åŠ æƒè¾¹æœºåˆ¶ï¼Œä¸ºæ¯ç¯‡æ–‡æœ¬è¾“å…¥å®šåˆ¶æœ€å°æˆæœ¬ç”Ÿæˆæ ‘ï¼Œå®ç°åŠ¨æ€æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜æ–‡æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå¯¹äºæŸ¥è¯¢ä¼˜åŒ–ã€æ•°æ®é›†æˆå’Œæ¨¡å¼åŒ¹é…ç­‰å¤šç§åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚CNNå’ŒBERTï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹æ•ˆæœè¾ƒå·®ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„è®­ç»ƒå’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å½“å‰LLMæ–¹æ³•é¢ä¸´è¾“å…¥å¤§å°å¢åŠ å’Œä¾§ä¿¡æ¯å¤„ç†å¼•å…¥çš„å™ªå£°é—®é¢˜ã€‚</li>
<li>GORAGæ¡†æ¶é€šè¿‡æ„å»ºè‡ªé€‚åº”ä¿¡æ¯å›¾è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå–ç›®æ ‡æ–‡æœ¬çš„æ‰€æœ‰ä¾§ä¿¡æ¯å¹¶è¿›è¡Œç®¡ç†ã€‚</li>
<li>GORAGé‡‡ç”¨åŠ æƒè¾¹æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†é‡è¦ä¸”å¯é çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c4791f1b155d19f916156f7d5d74249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a400039d75843c9b340e17a8af4def8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d96f9b63cb7accfb902bb30b21ec2d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-082716739e410264600392b905a8d8e5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TARDiS-Text-Augmentation-for-Refining-Diversity-and-Separability"><a href="#TARDiS-Text-Augmentation-for-Refining-Diversity-and-Separability" class="headerlink" title="TARDiS : Text Augmentation for Refining Diversity and Separability"></a>TARDiS : Text Augmentation for Refining Diversity and Separability</h2><p><strong>Authors:Kyungmin Kim, SangHun Im, GiBaeg Kim, Heung-Seon Oh</strong></p>
<p>Text augmentation (TA) is a critical technique for text classification, especially in few-shot settings. This paper introduces a novel LLM-based TA method, TARDiS, to address challenges inherent in the generation and alignment stages of two-stage TA methods. For the generation stage, we propose two generation processes, SEG and CEG, incorporating multiple class-specific prompts to enhance diversity and separability. For the alignment stage, we introduce a class adaptation (CA) method to ensure that generated examples align with their target classes through verification and modification. Experimental results demonstrate TARDiSâ€™s effectiveness, outperforming state-of-the-art LLM-based TA methods in various few-shot text classification tasks. An in-depth analysis confirms the detailed behaviors at each stage. </p>
<blockquote>
<p>æ–‡æœ¬å¢å¼ºï¼ˆTAï¼‰æ˜¯æ–‡æœ¬åˆ†ç±»ä¸­çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ï¼Œå°¤å…¶åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹TAæ–¹æ³•TARDiSï¼Œä»¥è§£å†³ä¸¤é˜¶æ®µTAæ–¹æ³•åœ¨ç”Ÿæˆå’Œå¯¹é½é˜¶æ®µæ‰€é¢ä¸´çš„å›ºæœ‰æŒ‘æˆ˜ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”Ÿæˆè¿‡ç¨‹ï¼Œå³SEGå’ŒCEGï¼Œå®ƒä»¬é€šè¿‡èå…¥å¤šä¸ªç‰¹å®šç±»åˆ«çš„æç¤ºæ¥å¢å¼ºå¤šæ ·æ€§å’Œå¯åˆ†æ€§ã€‚åœ¨å¯¹é½é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç±»é€‚åº”ï¼ˆCAï¼‰æ–¹æ³•ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„ä¾‹å­é€šè¿‡éªŒè¯å’Œä¿®æ”¹ä¸å…¶ç›®æ ‡ç±»åˆ«å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜TARDiSçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å„ç§å°‘é‡æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹TAæ–¹æ³•ã€‚æ·±å…¥åˆ†æè¯å®äº†æ¯ä¸€é˜¶æ®µçš„è¯¦ç»†è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02739v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ‰©å……ï¼ˆTAï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä¸­å°¤ä¸ºå…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬åœºæ™¯ä¸­ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºç¡€TAæ–¹æ³•TARDiSï¼Œæ—¨åœ¨è§£å†³ä¸¤é˜¶æ®µTAæ–¹æ³•ä¸­çš„ç”Ÿæˆå’Œæ ¡å‡†é˜¶æ®µæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”Ÿæˆè¿‡ç¨‹SEGå’ŒCEGï¼Œèå…¥å¤šç§ç±»åˆ«ç‰¹å®šæç¤ºä»¥å¼ºåŒ–å¤šæ ·æ€§å’Œå¯åˆ†ç¦»æ€§ã€‚åœ¨æ ¡å‡†é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥ç±»åˆ«é€‚åº”ï¼ˆCAï¼‰æ–¹æ³•ç¡®ä¿ç”Ÿæˆç¤ºä¾‹ä¸ç›®æ ‡ç±»åˆ«å¯¹é½ï¼Œé€šè¿‡éªŒè¯ä¸ä¿®æ”¹æ¥å®ç°ã€‚å®éªŒç»“æœè¯æ˜TARDiSçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å„ç§å°æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åŸºç¡€çš„TAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TARDiSæ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬æ‰©å……ï¼ˆTAï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›å°æ ·æœ¬æ–‡æœ¬åˆ†ç±»ã€‚</li>
<li>TARDiSè§£å†³äº†ä¼ ç»Ÿä¸¤é˜¶æ®µæ–‡æœ¬æ‰©å……æ–¹æ³•ä¸­çš„ç”Ÿæˆå’Œæ ¡å‡†é˜¶æ®µçš„æŒ‘æˆ˜ã€‚</li>
<li>åœ¨ç”Ÿæˆé˜¶æ®µï¼ŒTARDiSä½¿ç”¨SEGå’ŒCEGä¸¤ç§ç”Ÿæˆè¿‡ç¨‹ï¼Œé€šè¿‡èå…¥ç±»åˆ«ç‰¹å®šæç¤ºæ¥æé«˜å¤šæ ·æ€§å’Œå¯åˆ†ç¦»æ€§ã€‚</li>
<li>åœ¨æ ¡å‡†é˜¶æ®µï¼ŒTARDiSé€šè¿‡ç±»åˆ«é€‚åº”ï¼ˆCAï¼‰æ–¹æ³•ç¡®ä¿ç”Ÿæˆçš„ç¤ºä¾‹ä¸ç›®æ ‡ç±»åˆ«å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜TARDiSåœ¨å„ç§å°æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>TARDiSæ–¹æ³•åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰è¯¦ç»†çš„æ“ä½œåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f21b7e75473f789bc0af73a01d3937f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f013ecdeb391664c5e128aa31d4be22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a2556a68656eda87da87e19b9baf3d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8147369fe87e6d9cb49a3bd8886a0a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-812c6631646cbb7fabded1ecb1a76eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811d5340a0fad8619ff67b5f37d25902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d063eb847b8988b081b997ed8e47fb76.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Holistic-Semantic-Representation-for-Navigational-Trajectory-Generation"><a href="#Holistic-Semantic-Representation-for-Navigational-Trajectory-Generation" class="headerlink" title="Holistic Semantic Representation for Navigational Trajectory Generation"></a>Holistic Semantic Representation for Navigational Trajectory Generation</h2><p><strong>Authors:Ji Cao, Tongya Zheng, Qinghong Guo, Yu Wang, Junshu Dai, Shunyu Liu, Jie Yang, Jie Song, Mingli Song</strong></p>
<p>Trajectory generation has garnered significant attention from researchers in the field of spatio-temporal analysis, as it can generate substantial synthesized human mobility trajectories that enhance user privacy and alleviate data scarcity. However, existing trajectory generation methods often focus on improving trajectory generation quality from a singular perspective, lacking a comprehensive semantic understanding across various scales. Consequently, we are inspired to develop a HOlistic SEmantic Representation (HOSER) framework for navigational trajectory generation. Given an origin-and-destination (OD) pair and the starting time point of a latent trajectory, we first propose a Road Network Encoder to expand the receptive field of road- and zone-level semantics. Second, we design a Multi-Granularity Trajectory Encoder to integrate the spatio-temporal semantics of the generated trajectory at both the point and trajectory levels. Finally, we employ a Destination-Oriented Navigator to seamlessly integrate destination-oriented guidance. Extensive experiments on three real-world datasets demonstrate that HOSER outperforms state-of-the-art baselines by a significant margin. Moreover, the modelâ€™s performance in few-shot learning and zero-shot learning scenarios further verifies the effectiveness of our holistic semantic representation. </p>
<blockquote>
<p>è½¨è¿¹ç”Ÿæˆå·²å¼•èµ·æ—¶ç©ºåˆ†æé¢†åŸŸç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”Ÿæˆå¤§é‡åˆæˆçš„äººç±»ç§»åŠ¨è½¨è¿¹ï¼Œå¢å¼ºç”¨æˆ·éšç§å¹¶ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è½¨è¿¹ç”Ÿæˆæ–¹æ³•å¾€å¾€ä»å•ä¸€è§’åº¦æ”¹è¿›è½¨è¿¹ç”Ÿæˆè´¨é‡ï¼Œç¼ºä¹è·¨ä¸åŒå°ºåº¦çš„å…¨é¢è¯­ä¹‰ç†è§£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å—åˆ°å¯å‘ï¼Œå¼€å‘å‡ºç”¨äºå¯¼èˆªè½¨è¿¹ç”Ÿæˆçš„HOlistic SEmantic Representationï¼ˆHOSERï¼‰æ¡†æ¶ã€‚ç»™å®šèµ·ç‚¹å’Œç»ˆç‚¹å¯¹ä»¥åŠæ½œåœ¨è½¨è¿¹çš„èµ·å§‹æ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ç§é“è·¯ç½‘ç»œç¼–ç å™¨ï¼Œä»¥æ‰©å¤§é“è·¯å’ŒåŒºåŸŸçº§åˆ«çš„è¯­ä¹‰æ„Ÿå—é‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šç²’åº¦è½¨è¿¹ç¼–ç å™¨ï¼Œä»¥æ•´åˆç”Ÿæˆè½¨è¿¹çš„ç‚¹å’Œè½¨è¿¹çº§åˆ«çš„æ—¶ç©ºè¯­ä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨ç›®æ ‡å¯¼å‘å¯¼èˆªå™¨ï¼Œæ— ç¼é›†æˆä»¥ç›®çš„åœ°ä¸ºå¯¼å‘çš„å¯¼èˆªã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHOSERæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„æ€§èƒ½è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ•´ä½“è¯­ä¹‰è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02737v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è½¨è¿¹ç”Ÿæˆå—åˆ°æ—¶ç©ºåˆ†æé¢†åŸŸç ”ç©¶è€…çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒèƒ½ç”Ÿæˆå¤§é‡åˆæˆçš„äººç±»ç§»åŠ¨è½¨è¿¹ï¼Œå¢å¼ºç”¨æˆ·éšç§å¹¶ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰è½¨è¿¹ç”Ÿæˆæ–¹æ³•å¾€å¾€ä»å•ä¸€è§’åº¦æ”¹è¿›è½¨è¿¹ç”Ÿæˆè´¨é‡ï¼Œç¼ºä¹è·¨ä¸åŒå°ºåº¦çš„å…¨é¢è¯­ä¹‰ç†è§£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ•´ä½“çš„è¯­ä¹‰è¡¨ç¤ºï¼ˆHOSERï¼‰æ¡†æ¶æ¥è¿›è¡Œå¯¼èˆªè½¨è¿¹ç”Ÿæˆã€‚ç»™å®šèµ·ç‚¹å’Œç»ˆç‚¹å¯¹ä»¥åŠæ½œåœ¨è½¨è¿¹çš„èµ·å§‹æ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºé“è·¯ç½‘ç»œç¼–ç å™¨æ¥æ‰©å±•é“è·¯å’ŒåŒºåŸŸçº§åˆ«çš„è¯­ä¹‰æ„Ÿå—é‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šç²’åº¦è½¨è¿¹ç¼–ç å™¨ï¼Œä»¥æ•´åˆç”Ÿæˆè½¨è¿¹çš„ç‚¹å’Œè½¨è¿¹çº§åˆ«çš„æ—¶ç©ºè¯­ä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨é¢å‘ç›®çš„åœ°çš„å¯¼èˆªå™¨æ— ç¼é›†æˆç›®çš„åœ°å¯¼å‘çš„å¯¼èˆªã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHOSERæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„è¡¨ç°è¿›ä¸€æ­¥éªŒè¯äº†å…¶å…¨é¢çš„è¯­ä¹‰è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¨è¿¹ç”Ÿæˆæ˜¯æ—¶ç©ºåˆ†æé¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œèƒ½ç”Ÿæˆåˆæˆçš„äººç±»ç§»åŠ¨è½¨è¿¹ï¼Œå¢å¼ºç”¨æˆ·éšç§å¹¶ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>ç°æœ‰è½¨è¿¹ç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨æé«˜è½¨è¿¹ç”Ÿæˆè´¨é‡ï¼Œä½†ç¼ºä¹è·¨ä¸åŒå°ºåº¦çš„å…¨é¢è¯­ä¹‰ç†è§£ã€‚</li>
<li>HOSERæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé“è·¯ç½‘ç»œç¼–ç å™¨ã€å¤šç²’åº¦è½¨è¿¹ç¼–ç å™¨å’Œé¢å‘ç›®çš„åœ°çš„å¯¼èˆªå™¨ã€‚</li>
<li>é“è·¯ç½‘ç»œç¼–ç å™¨æ‰©å¤§é“è·¯å’ŒåŒºåŸŸçº§åˆ«çš„è¯­ä¹‰æ„Ÿå—é‡ã€‚</li>
<li>å¤šç²’åº¦è½¨è¿¹ç¼–ç å™¨æ•´åˆç”Ÿæˆè½¨è¿¹çš„ç‚¹å’Œè½¨è¿¹çº§åˆ«çš„æ—¶ç©ºè¯­ä¹‰ã€‚</li>
<li>é¢å‘ç›®çš„åœ°çš„å¯¼èˆªå™¨æ— ç¼é›†æˆç›®çš„åœ°å¯¼å‘çš„å¯¼èˆªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5bb98506b35cf3bb59b47de18d4ea712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b312baee0c90a02261e37c17fc5c519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24ca6454840c08c68ce4aa5865168bbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-589977983682146fe063ebdbe9fb5859.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aecd38687e7bdf493cb0e2ed03b3b86c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Noise-Tolerant-Hybrid-Prototypical-Learning-with-Noisy-Web-Data"><a href="#Noise-Tolerant-Hybrid-Prototypical-Learning-with-Noisy-Web-Data" class="headerlink" title="Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data"></a>Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data</h2><p><strong>Authors:Chao Liang, Linchao Zhu, Zongxin Yang, Wei Chen, Yi Yang</strong></p>
<p>We focus on the challenging problem of learning an unbiased classifier from a large number of potentially relevant but noisily labeled web images given only a few clean labeled images. This problem is particularly practical because it reduces the expensive annotation costs by utilizing freely accessible web images with noisy labels. Typically, prototypes are representative images or features used to classify or identify other images. However, in the few clean and many noisy scenarios, the class prototype can be severely biased due to the presence of irrelevant noisy images. The resulting prototypes are less compact and discriminative, as previous methods do not take into account the diverse range of images in the noisy web image collections. On the other hand, the relation modeling between noisy and clean images is not learned for the class prototype generation in an end-to-end manner, which results in a suboptimal class prototype. In this article, we introduce a similarity maximization loss named SimNoiPro. Our SimNoiPro first generates noise-tolerant hybrid prototypes composed of clean and noise-tolerant prototypes and then pulls them closer to each other. Our approach considers the diversity of noisy images by explicit division and overcomes the optimization discrepancy issue. This enables better relation modeling between clean and noisy images and helps extract judicious information from the noisy image set. The evaluation results on two extended few-shot classification benchmarks confirm that our SimNoiPro outperforms prior methods in measuring image relations and cleaning noisy data. </p>
<blockquote>
<p>æˆ‘ä»¬å…³æ³¨ä»å¤§é‡å¯èƒ½ç›¸å…³ä½†å¸¦æœ‰å™ªå£°æ ‡ç­¾çš„ç½‘ç»œå›¾åƒä¸­å­¦ä¹ æ— åè§åˆ†ç±»å™¨çš„é—®é¢˜ï¼Œä»…ä½¿ç”¨å°‘é‡å¹²å‡€æ ‡è®°çš„å›¾åƒã€‚è¿™ä¸ªé—®é¢˜ç‰¹åˆ«å®ç”¨ï¼Œå› ä¸ºå®ƒé€šè¿‡åˆ©ç”¨å¸¦æœ‰å™ªå£°æ ‡ç­¾çš„å¯è‡ªç”±è®¿é—®çš„ç½‘ç»œå›¾åƒæ¥é™ä½äº†æ˜‚è´µçš„æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼Œé€šå¸¸åŸå‹æ˜¯ç”¨äºåˆ†ç±»æˆ–è¯†åˆ«å…¶ä»–å›¾åƒçš„ä»£è¡¨å›¾åƒæˆ–ç‰¹å¾ã€‚ä½†åœ¨å°‘æ•°æ¸…æ´å’Œå¤§é‡å™ªå£°åœºæ™¯ä¸­ï¼Œç”±äºå­˜åœ¨ä¸ç›¸å…³çš„å™ªå£°å›¾åƒï¼Œç±»åŸå‹å¯èƒ½ä¼šå—åˆ°ä¸¥é‡åè§ã€‚ç”±æ­¤äº§ç”Ÿçš„åŸå‹ä¸å¤Ÿç´§å‡‘å’Œé‰´åˆ«åŠ›ä¸å¼ºï¼Œå› ä¸ºä»¥å‰çš„æ–¹æ³•æ²¡æœ‰è€ƒè™‘åˆ°å™ªå£°ç½‘ç»œå›¾åƒé›†åˆä¸­å›¾åƒçš„å¤šæ ·æ€§èŒƒå›´ã€‚å¦ä¸€æ–¹é¢ï¼Œç±»åŸå‹çš„ç”Ÿæˆå¹¶æ²¡æœ‰ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ å™ªå£°å›¾åƒå’Œå¹²å‡€å›¾åƒä¹‹é—´çš„å…³ç³»å»ºæ¨¡ï¼Œè¿™å¯¼è‡´äº†æ¬¡ä¼˜çš„ç±»åŸå‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºSimNoiProçš„ç›¸ä¼¼æ€§æœ€å¤§åŒ–æŸå¤±ã€‚æˆ‘ä»¬çš„SimNoiProé¦–å…ˆç”Ÿæˆç”±æ¸…æ´å’Œå™ªå£°å®¹å¿åŸå‹ç»„æˆçš„å™ªå£°å®¹å¿æ··åˆåŸå‹ï¼Œç„¶åå°†å®ƒä»¬å½¼æ­¤æ‹‰è¿‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ˜¾å¼åˆ’åˆ†è€ƒè™‘äº†å™ªå£°å›¾åƒçš„å¤šæ ·æ€§ï¼Œå¹¶å…‹æœäº†ä¼˜åŒ–å·®å¼‚é—®é¢˜ã€‚è¿™èƒ½å¤Ÿå®ç°å¯¹æ¸…æ´å’Œå™ªå£°å›¾åƒä¹‹é—´å…³ç³»çš„æ›´å¥½å»ºæ¨¡ï¼Œå¹¶æœ‰åŠ©äºä»å™ªå£°å›¾åƒé›†ä¸­æå–æ˜æ™ºçš„ä¿¡æ¯ã€‚åœ¨ä¸¤ä¸ªæ‰©å±•çš„å°æ ·æœ¬åˆ†ç±»åŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœè¯å®ï¼Œæˆ‘ä»¬çš„SimNoiProåœ¨è¡¡é‡å›¾åƒå…³ç³»å’Œæ¸…ç†å™ªå£°æ•°æ®æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02476v1">PDF</a> Accepted by TOMM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨ä»å¤§é‡æ½œåœ¨ç›¸å…³ä½†å¸¦æœ‰å™ªå£°æ ‡ç­¾çš„ç½‘ç»œå›¾åƒä¸­å­¦ä¹ æ— åè§åˆ†ç±»å™¨çš„é—®é¢˜ï¼Œå…¶ä¸­åªæœ‰å°‘æ•°å¹²å‡€æ ‡ç­¾çš„å›¾åƒã€‚é€šè¿‡å¼•å…¥SimNoiProç›¸ä¼¼æ€§æœ€å¤§åŒ–æŸå¤±ï¼Œç”Ÿæˆå™ªå£°å®¹å¿çš„æ··åˆåŸå‹ï¼Œå¹¶å°†å®ƒä»¬å½¼æ­¤æ‹‰è¿‘ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿæ¸…æ´å’Œå™ªå£°å›¾åƒä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä»å™ªå£°å›¾åƒé›†ä¸­æå–æ˜æ™ºçš„ä¿¡æ¯ã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼ŒSimNoiProåœ¨è¡¡é‡å›¾åƒå…³ç³»å’Œæ¸…ç†å™ªå£°æ•°æ®æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å…³æ³¨ä»å¤§é‡ç½‘ç»œå›¾åƒä¸­å­¦ä¹ æ— åè§åˆ†ç±»å™¨çš„é—®é¢˜ï¼Œè¿™äº›å›¾åƒå¸¦æœ‰å™ªå£°æ ‡ç­¾ï¼Œä½†ä»…æœ‰å°‘æ•°å¹²å‡€æ ‡ç­¾çš„å›¾åƒã€‚</li>
<li>å¼•å…¥SimNoiProç›¸ä¼¼æ€§æœ€å¤§åŒ–æŸå¤±ï¼Œç”¨äºå¤„ç†å™ªå£°å®¹å¿å’ŒåŸå‹ç”Ÿæˆã€‚</li>
<li>ç”Ÿæˆå™ªå£°å®¹å¿çš„æ··åˆåŸå‹ï¼Œç”±å¹²å‡€å’Œå™ªå£°å®¹å¿çš„åŸå‹ç»„æˆã€‚</li>
<li>å°†ç”Ÿæˆçš„åŸå‹å½¼æ­¤æ‹‰è¿‘ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿæ¸…æ´å’Œå™ªå£°å›¾åƒä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ–¹æ³•è€ƒè™‘äº†å™ªå£°å›¾åƒçš„å¤šæ ·æ€§ï¼Œå¹¶é€šè¿‡æ˜ç¡®åˆ’åˆ†æ¥å…‹æœä¼˜åŒ–å·®å¼‚é—®é¢˜ã€‚</li>
<li>èƒ½ä»å™ªå£°å›¾åƒé›†ä¸­æå–æ˜æ™ºçš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02476v1/page_0_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ff66ce08ceb54f518bae00f93143bc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366d663e1147f3d5235e51645f835d37.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Generalization-Enhanced-Few-Shot-Object-Detection-in-Remote-Sensing"><a href="#Generalization-Enhanced-Few-Shot-Object-Detection-in-Remote-Sensing" class="headerlink" title="Generalization-Enhanced Few-Shot Object Detection in Remote Sensing"></a>Generalization-Enhanced Few-Shot Object Detection in Remote Sensing</h2><p><strong>Authors:Hui Lin, Nan Li, Pengjuan Yao, Kexin Dong, Yuhan Guo, Danfeng Hong, Ying Zhang, Congcong Wen</strong></p>
<p>Remote sensing object detection is particularly challenging due to the high resolution, multi-scale features, and diverse ground object characteristics inherent in satellite and UAV imagery. These challenges necessitate more advanced approaches for effective object detection in such environments. While deep learning methods have achieved remarkable success in remote sensing object detection, they typically rely on large amounts of labeled data. Acquiring sufficient labeled data, particularly for novel or rare objects, is both challenging and time-consuming in remote sensing scenarios, limiting the generalization capabilities of existing models. To address these challenges, few-shot learning (FSL) has emerged as a promising approach, aiming to enable models to learn new classes from limited labeled examples. Building on this concept, few-shot object detection (FSOD) specifically targets object detection challenges in data-limited conditions. However, the generalization capability of FSOD models, particularly in remote sensing, is often constrained by the complex and diverse characteristics of the objects present in such environments. In this paper, we propose the Generalization-Enhanced Few-Shot Object Detection (GE-FSOD) model to improve the generalization capability in remote sensing FSOD tasks. Our model introduces three key innovations: the Cross-Level Fusion Pyramid Attention Network (CFPAN) for enhanced multi-scale feature representation, the Multi-Stage Refinement Region Proposal Network (MRRPN) for more accurate region proposals, and the Generalized Classification Loss (GCL) for improved classification performance in few-shot scenarios. Extensive experiments on the DIOR and NWPU VHR-10 datasets show that our model achieves state-of-the-art performance for few-shot object detection in remote sensing. </p>
<blockquote>
<p>é¥æ„Ÿç›®æ ‡æ£€æµ‹å› å…¶å›ºæœ‰çš„é«˜åˆ†è¾¨ç‡ã€å¤šå°ºåº¦ç‰¹å¾å’Œåœ°é¢ç›®æ ‡ç‰¹æ€§å¤šæ ·æ€§ï¼Œåœ¨å«æ˜Ÿå’Œæ— äººæœºå›¾åƒä¸­é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜è¦æ±‚æ›´å…ˆè¿›çš„æ–¹æ³•åœ¨è¿™æ ·çš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„ç›®æ ‡æ£€æµ‹ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨é¥æ„Ÿç›®æ ‡æ£€æµ‹ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤§é‡çš„æ ‡è®°æ•°æ®ã€‚åœ¨é¥æ„Ÿåœºæ™¯ä¸­ï¼Œè·å–è¶³å¤Ÿçš„æœ‰æ ‡ç­¾æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ–°å‹æˆ–ç¨€æœ‰ç›®æ ‡ï¼Œæ—¢å…·æœ‰æŒ‘æˆ˜æ€§åˆéå¸¸è€—æ—¶ï¼Œè¿™é™åˆ¶äº†ç°æœ‰æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•åº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿä»æœ‰é™çš„æ ‡è®°ç¤ºä¾‹ä¸­å­¦ä¹ æ–°çš„ç±»åˆ«ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰ä¸“é—¨è§£å†³æ•°æ®æœ‰é™æ¡ä»¶ä¸‹çš„ç›®æ ‡æ£€æµ‹æŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒFSODæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é¥æ„Ÿé¢†åŸŸï¼Œå¾€å¾€å—åˆ°æ­¤ç±»ç¯å¢ƒä¸­å¯¹è±¡å¤æ‚å’Œå¤šæ ·ç‰¹æ€§çš„åˆ¶çº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨å¢å¼ºå°æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆGE-FSODï¼‰æ¨¡å‹ï¼Œä»¥æé«˜é¥æ„ŸFSODä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šç”¨äºå¢å¼ºå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºçš„è·¨çº§èåˆé‡‘å­—å¡”æ³¨æ„åŠ›ç½‘ç»œï¼ˆCFPANï¼‰ã€ç”¨äºæ›´ç²¾ç¡®åŒºåŸŸæè®®çš„å¤šé˜¶æ®µç»†åŒ–åŒºåŸŸæè®®ç½‘ç»œï¼ˆMRRPNï¼‰ä»¥åŠç”¨äºæ”¹è¿›å°æ ·æœ¬åœºæ™¯ä¸­åˆ†ç±»æ€§èƒ½çš„å¹¿ä¹‰åˆ†ç±»æŸå¤±ï¼ˆGCLï¼‰ã€‚åœ¨DIORå’ŒNWPU VHR-10æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é¥æ„Ÿå°æ ·æœ¬ç›®æ ‡æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¥æ„Ÿç‰©ä½“æ£€æµ‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é«˜åˆ†è¾¨ç‡ã€å¤šå°ºåº¦ç‰¹å¾å’Œåœ°é¢ç‰©ä½“ç‰¹æ€§çš„å¤šæ ·æ€§ã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä¾èµ–äºå¤§é‡æ ‡è®°æ•°æ®ã€‚ä¸ºè§£å†³è·å–è¶³å¤Ÿæ ‡è®°æ•°æ®çš„æŒ‘æˆ˜ï¼Œæ–‡ä¸­æå‡ºäº†åŸºäºæ³›åŒ–èƒ½åŠ›æå‡çš„å°‘é•œå¤´ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆGE-FSODï¼‰ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè·¨å±‚æ¬¡èåˆé‡‘å­—å¡”æ³¨æ„åŠ›ç½‘ç»œï¼ˆCFPANï¼‰ç”¨äºå¢å¼ºå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼Œå¤šé˜¶æ®µç»†åŒ–åŒºåŸŸæè®®ç½‘ç»œï¼ˆMRRPNï¼‰ç”¨äºæ›´å‡†ç¡®çš„åŒºåŸŸæè®®ï¼Œä»¥åŠå¹¿ä¹‰åˆ†ç±»æŸå¤±ï¼ˆGCLï¼‰ç”¨äºæé«˜å°‘é•œå¤´åœºæ™¯ä¸­çš„åˆ†ç±»æ€§èƒ½ã€‚åœ¨DIORå’ŒNWPU VHR-10æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¥æ„Ÿå°‘é•œå¤´ç›®æ ‡æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¥æ„Ÿç‰©ä½“æ£€æµ‹é¢ä¸´é«˜åˆ†è¾¨ç‡ã€å¤šå°ºåº¦ç‰¹å¾å’Œåœ°é¢ç‰©ä½“å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨é¥æ„Ÿç‰©ä½“æ£€æµ‹ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¾èµ–å¤§é‡æ ‡è®°æ•°æ®ã€‚</li>
<li>è·å–è¶³å¤Ÿæ ‡è®°æ•°æ®åœ¨é¥æ„Ÿåœºæ™¯ä¸­å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ—¶é—´æˆæœ¬ã€‚</li>
<li>å°‘é•œå¤´å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»æœ‰é™æ ‡è®°ç¤ºä¾‹ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼Œæ˜¯è§£å†³ä¸Šè¿°æŒ‘æˆ˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºæ³›åŒ–èƒ½åŠ›æå‡çš„å°‘é•œå¤´ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆGE-FSODï¼‰ã€‚</li>
<li>GE-FSODæ¨¡å‹å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šCFPANã€MRRPNå’ŒGCLã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1649ee025e78126ba0c4fd31374651ad.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02474v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02474v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Advancing-Pancreatic-Cancer-Prediction-with-a-Next-Visit-Token-Prediction-Head-on-top-of-Med-BERT"><a href="#Advancing-Pancreatic-Cancer-Prediction-with-a-Next-Visit-Token-Prediction-Head-on-top-of-Med-BERT" class="headerlink" title="Advancing Pancreatic Cancer Prediction with a Next Visit Token   Prediction Head on top of Med-BERT"></a>Advancing Pancreatic Cancer Prediction with a Next Visit Token   Prediction Head on top of Med-BERT</h2><p><strong>Authors:Jianping He, Laila Rasmy, Degui Zhi, Cui Tao</strong></p>
<p>Background: Recently, numerous foundation models pretrained on extensive data have demonstrated efficacy in disease prediction using Electronic Health Records (EHRs). However, there remains some unanswered questions on how to best utilize such models especially with very small fine-tuning cohorts. Methods: We utilized Med-BERT, an EHR-specific foundation model, and reformulated the disease binary prediction task into a token prediction task and a next visit mask token prediction task to align with Med-BERTâ€™s pretraining task format in order to improve the accuracy of pancreatic cancer (PaCa) prediction in both few-shot and fully supervised settings. Results: The reformulation of the task into a token prediction task, referred to as Med-BERT-Sum, demonstrates slightly superior performance in both few-shot scenarios and larger data samples. Furthermore, reformulating the prediction task as a Next Visit Mask Token Prediction task (Med-BERT-Mask) significantly outperforms the conventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to 7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These findings highlight that aligning the downstream task with Med-BERTâ€™s pretraining objectives substantially enhances the modelâ€™s predictive capabilities, thereby improving its effectiveness in predicting both rare and common diseases. Conclusion: Reformatting disease prediction tasks to align with the pretraining of foundation models enhances prediction accuracy, leading to earlier detection and timely intervention. This approach improves treatment effectiveness, survival rates, and overall patient outcomes for PaCa and potentially other cancers. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæœ€è¿‘ï¼Œè®¸å¤šåœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹åœ¨åˆ©ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è¿›è¡Œç–¾ç—…é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•æœ€å¥½åœ¨éå¸¸å°çš„å¾®è°ƒç¾¤ä½“ä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹çš„é—®é¢˜ä»æœªå¾—åˆ°è§£ç­”ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨äº†é’ˆå¯¹EHRçš„åŸºçŸ³æ¨¡å‹Med-BERTï¼Œå¹¶å°†ç–¾ç—…äºŒåˆ†ç±»é¢„æµ‹ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºæ ‡è®°é¢„æµ‹ä»»åŠ¡å’Œä¸‹æ¬¡å°±è¯Šæ©ç æ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œä»¥ä¸Med-BERTçš„é¢„è®­ç»ƒä»»åŠ¡æ ¼å¼å¯¹é½ï¼Œä»¥æé«˜èƒ°è…ºç™Œï¼ˆPaCaï¼‰åœ¨å°‘é‡é•œå¤´å’Œå®Œå…¨ç›‘ç£è®¾ç½®ä¸­çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ç»“æœï¼šå°†ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºæ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼ˆç§°ä¸ºMed-BERT-Sumï¼‰åœ¨å°‘é‡é•œå¤´åœºæ™¯å’Œè¾ƒå¤§çš„æ•°æ®æ ·æœ¬ä¸­éƒ½è¡¨ç°å‡ºç•¥å¾®ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†é¢„æµ‹ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºä¸‹æ¬¡å°±è¯Šæ©ç æ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼ˆMed-BERT-Maskï¼‰æ¯”ä¼ ç»Ÿçš„äºŒåˆ†ç±»ï¼ˆBCï¼‰é¢„æµ‹ä»»åŠ¡ï¼ˆMed-BERT-BCï¼‰åœ¨æ ·æœ¬é‡ä»10åˆ°500çš„å°‘é‡é•œå¤´åœºæ™¯ä¸­é«˜å‡º3%è‡³7%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡ä¸Med-BERTçš„é¢„è®­ç»ƒç›®æ ‡å¯¹é½å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œä»è€Œæé«˜å…¶åœ¨é¢„æµ‹ç½•è§ç–¾ç—…å’Œå¸¸è§ç–¾ç—…æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç»“è®ºï¼šé‡æ–°æ ¼å¼åŒ–ç–¾ç—…é¢„æµ‹ä»»åŠ¡ï¼Œä»¥ä¸åŸºçŸ³æ¨¡å‹çš„é¢„è®­ç»ƒå¯¹é½ï¼Œå¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œä»è€Œå®ç°æ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„ã€‚è¿™ç§æ–¹æ³•æé«˜äº†èƒ°è…ºç™Œå’Œå…¶ä»–æ½œåœ¨ç™Œç—‡çš„æ²»ç–—æ•ˆç‡ã€å­˜æ´»ç‡å’Œæ•´ä½“æ‚£è€…ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨Med-BERTè¿™ä¸€é’ˆå¯¹ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç–¾ç—…é¢„æµ‹çš„æ–¹æ³•ã€‚é€šè¿‡æ”¹é©ä»»åŠ¡å½¢å¼ï¼Œå°†ç–¾ç—…äºŒåˆ†ç±»é¢„æµ‹ä»»åŠ¡è½¬åŒ–ä¸ºtokené¢„æµ‹ä»»åŠ¡å’Œä¸‹æ¬¡å°±è¯Šmask tokené¢„æµ‹ä»»åŠ¡ï¼Œæé«˜äº†èƒ°è…ºç™Œé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æ•°æ®åœºæ™¯ä¸‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¹é©åçš„æ¨¡å‹æ€§èƒ½åœ¨å°‘æ•°æ ·æœ¬å’Œå¤§æ ·æœ¬ä¸‹å‡è¡¨ç°æ›´ä¼˜ï¼Œæ˜¾ç¤ºäº†å¯¹é½ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒä»»åŠ¡èƒ½æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹ç½•è§ç—…æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‡ç”¨Med-BERTæ¨¡å‹è¿›è¡Œç–¾ç—…é¢„æµ‹å…·æœ‰æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</li>
<li>ä»»åŠ¡å½¢å¼çš„æ”¹é©ï¼Œå¦‚tokené¢„æµ‹ä»»åŠ¡å’Œä¸‹æ¬¡å°±è¯Šmask tokené¢„æµ‹ä»»åŠ¡ï¼Œèƒ½æé«˜èƒ°è…ºç™Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å°æ ·æ•°æ®åœºæ™¯ä¸‹ï¼Œæ”¹é©åçš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>å¯¹é½ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒä»»åŠ¡èƒ½æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å¯èƒ½å¯¹é¢„æµ‹ç½•è§ç–¾ç—…å’Œå¸¸è§ç–¾ç—…éƒ½æœ‰æ•ˆã€‚</li>
<li>æ”¹é©åçš„æ¨¡å‹æœ‰åŠ©äºæé«˜èƒ°è…ºç™ŒåŠå…¶ä»–ç™Œç—‡çš„æ²»ç–—æ•ˆæœå’Œæ‚£è€…ç”Ÿå­˜ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2501.02044v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Training-Free-Point-Cloud-Recognition-Based-on-Geometric-and-Semantic-Information-Fusion"><a href="#Training-Free-Point-Cloud-Recognition-Based-on-Geometric-and-Semantic-Information-Fusion" class="headerlink" title="Training-Free Point Cloud Recognition Based on Geometric and Semantic   Information Fusion"></a>Training-Free Point Cloud Recognition Based on Geometric and Semantic   Information Fusion</h2><p><strong>Authors:Yan Chen, Di Huang, Zhichao Liao, Xi Cheng, Xinghui Li, Lone Zeng</strong></p>
<p>The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we are the first to propose a novel training-free method that integrates both geometric and semantic features. For the geometric branch, we adopt a non-parametric strategy to extract geometric features. In the semantic branch, we leverage a model aligned with text features to obtain semantic features. Additionally, we introduce the GFE module to complement the geometric information of point clouds and the MFF module to improve performance in few-shot settings. Experimental results demonstrate that our method outperforms existing state-of-the-art training-free approaches on mainstream benchmark datasets, including ModelNet and ScanObiectNN. </p>
<blockquote>
<p>é‡‡ç”¨æ— è®­ç»ƒæ–¹æ³•å¤„ç†ç‚¹äº‘è¯†åˆ«çš„è¶‹åŠ¿æ­£è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒèƒ½æ˜¾è‘—å‡å°‘è®¡ç®—èµ„æºå’Œæ—¶é—´æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä»…é™äºæå–å‡ ä½•æˆ–è¯­ä¹‰ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èåˆäº†å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾ã€‚åœ¨å‡ ä½•åˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨éå‚æ•°ç­–ç•¥æ¥æå–å‡ ä½•ç‰¹å¾ã€‚åœ¨è¯­ä¹‰åˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½çš„æ¨¡å‹æ¥è·å–è¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†GFEæ¨¡å—æ¥è¡¥å……ç‚¹äº‘çš„å‡ ä½•ä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†MFFæ¨¡å—æ¥æé«˜å°æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»æµåŸºå‡†æ•°æ®é›†ï¼ˆåŒ…æ‹¬ModelNetå’ŒScanObiectNNï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€æ–°æ— è®­ç»ƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04760v4">PDF</a> </p>
<p><strong>Summary</strong><br>è®­ç»ƒå…è´¹çš„æ–¹æ³•åœ¨ç‚¹äº‘è¯†åˆ«ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒèƒ½æ˜¾è‘—å‡å°‘è®¡ç®—èµ„æºå’Œæ—¶é—´æˆæœ¬ã€‚ä¸€ç§æ–°å‹è®­ç»ƒå…è´¹æ–¹æ³•é¦–æ¬¡è¢«æå‡ºï¼Œå®ƒæ•´åˆäº†å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾ï¼Œæé«˜äº†ç‚¹äº‘è¯†åˆ«çš„æ€§èƒ½ã€‚é‡‡ç”¨éå‚æ•°ç­–ç•¥æå–å‡ ä½•ç‰¹å¾ï¼Œåˆ©ç”¨ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½çš„æ¨¡å‹è·å–è¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†GFEæ¨¡å—æ¥è¡¥å……ç‚¹äº‘çš„å‡ ä½•ä¿¡æ¯ï¼ŒMFFæ¨¡å—æå‡åœ¨few-shotåœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµåŸºå‡†æ•°æ®é›†ModelNetå’ŒScanObiectNNä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›è®­ç»ƒå…è´¹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒå…è´¹æ–¹æ³•åœ¨ç‚¹äº‘è¯†åˆ«ä¸­é€æ¸æ™®åŠï¼Œä¸»è¦å¾—ç›Šäºå…¶å‡å°‘çš„è®¡ç®—èµ„æºå’Œæ—¶é—´æˆæœ¬ã€‚</li>
<li>æ–°å‹è®­ç»ƒå…è´¹æ–¹æ³•é¦–æ¬¡ç»“åˆå‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾è¿›è¡Œç‚¹äº‘è¯†åˆ«ã€‚</li>
<li>é‡‡ç”¨éå‚æ•°ç­–ç•¥æå–å‡ ä½•ç‰¹å¾ï¼Œåˆ©ç”¨ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½çš„æ¨¡å‹è·å–è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>GFEæ¨¡å—çš„å¼•å…¥ç”¨äºè¡¥å……ç‚¹äº‘çš„å‡ ä½•ä¿¡æ¯ã€‚</li>
<li>MFFæ¨¡å—æ—¨åœ¨æå‡åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ï¼ˆfew-shotï¼‰ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒå…è´¹æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ½œåŠ›ä¸ºç‚¹äº‘è¯†åˆ«é¢†åŸŸå¸¦æ¥é©æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2409.04760v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Distillation-Learning-Guided-by-Image-Reconstruction-for-One-Shot-Medical-Image-Segmentation"><a href="#Distillation-Learning-Guided-by-Image-Reconstruction-for-One-Shot-Medical-Image-Segmentation" class="headerlink" title="Distillation Learning Guided by Image Reconstruction for One-Shot   Medical Image Segmentation"></a>Distillation Learning Guided by Image Reconstruction for One-Shot   Medical Image Segmentation</h2><p><strong>Authors:Feng Zhou, Yanjie Zhou, Longjie Wang, Yun Peng, David E. Carlson, Liyun Tu</strong></p>
<p>Traditional one-shot medical image segmentation (MIS) methods use registration networks to propagate labels from a reference atlas or rely on comprehensive sampling strategies to generate synthetic labeled data for training. However, these methods often struggle with registration errors and low-quality synthetic images, leading to poor performance and generalization. To overcome this, we introduce a novel one-shot MIS framework based on knowledge distillation, which allows the network to directly â€˜seeâ€™ real images through a distillation process guided by image reconstruction. It focuses on anatomical structures in a single labeled image and a few unlabeled ones. A registration-based data augmentation network creates realistic, labeled samples, while a feature distillation module helps the student network learn segmentation from these samples, guided by the teacher network. During inference, the streamlined student network accurately segments new images. Evaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen CT, and VerSe for vertebrae CT) show superior segmentation performance and generalization across different medical image datasets and modalities compared to leading methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NoviceFodder/OS-MedSeg">https://github.com/NoviceFodder/OS-MedSeg</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å•é•œå¤´åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰æ–¹æ³•ä½¿ç”¨æ³¨å†Œç½‘ç»œæ¥ä¼ æ’­æ¥è‡ªå‚è€ƒå›¾è°±çš„æ ‡ç­¾ï¼Œæˆ–è€…ä¾èµ–äºå…¨é¢çš„é‡‡æ ·ç­–ç•¥æ¥ç”Ÿæˆåˆæˆæ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸å—åˆ°æ³¨å†Œé”™è¯¯å’Œä½è´¨é‡åˆæˆå›¾åƒçš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œæ³›åŒ–èƒ½åŠ›å·®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„æ–°å‹å•é•œå¤´MISæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç½‘ç»œé€šè¿‡ç”±å›¾åƒé‡å»ºå¼•å¯¼çš„è’¸é¦è¿‡ç¨‹ç›´æ¥â€œæŸ¥çœ‹â€çœŸå®å›¾åƒã€‚å®ƒä¸“æ³¨äºå•ä¸ªæ ‡è®°å›¾åƒå’Œå°‘é‡æœªæ ‡è®°å›¾åƒä¸­çš„è§£å‰–ç»“æ„ã€‚åŸºäºæ³¨å†Œçš„æ•°æ®å¢å¼ºç½‘ç»œåˆ›å»ºé€¼çœŸçš„æ ‡è®°æ ·æœ¬ï¼Œè€Œç‰¹å¾è’¸é¦æ¨¡å—åˆ™å¸®åŠ©å­¦ç”Ÿç½‘ç»œä»è¿™äº›æ ·æœ¬ä¸­å­¦ä¹ åˆ†å‰²ï¼Œå¹¶å—æ•™å¸ˆç½‘ç»œçš„æŒ‡å¯¼ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç®€åŒ–çš„å­¦ç”Ÿç½‘ç»œèƒ½å¤Ÿå‡†ç¡®åœ°å¯¹æ–°å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆOASISç”¨äºT1è„‘MRIã€BCVç”¨äºè…¹éƒ¨CTå’ŒVerSeç”¨äºæ¤ä½“CTï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸é¢†å…ˆçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†å’Œä¸åŒæ¨¡æ€çš„åˆ†å‰²æ€§èƒ½åŠæ³›åŒ–èƒ½åŠ›ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NoviceFodder/OS-MedSeg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NoviceFodder/OS-MedSegä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.03616v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„æ–°å‹å•æ¬¡åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æ³¨å†Œè¯¯å·®å’Œä½è´¨é‡åˆæˆå›¾åƒé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å›¾åƒé‡å»ºå¼•å¯¼è’¸é¦è¿‡ç¨‹ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿç›´æ¥â€œçœ‹åˆ°â€çœŸå®å›¾åƒã€‚å®ƒä¸“æ³¨äºå•å¼ æ ‡è®°å›¾åƒå’Œå°‘é‡æœªæ ‡è®°å›¾åƒä¸­çš„è§£å‰–ç»“æ„ã€‚æ³¨å†Œå¼æ•°æ®å¢å¼ºç½‘ç»œåˆ›å»ºé€¼çœŸçš„æ ‡è®°æ ·æœ¬ï¼Œç‰¹å¾è’¸é¦æ¨¡å—å¸®åŠ©å­¦ç”Ÿç½‘ç»œä»è¿™äº›æ ·æœ¬ä¸­å­¦ä¹ åˆ†å‰²ï¼Œå—æ•™å¸ˆç½‘ç»œçš„å¼•å¯¼ã€‚åœ¨æ–°å›¾åƒä¸Šçš„è¯„ä¼°è¡¨ç°å‡ºå“è¶Šçš„åˆ†å‰²æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹å•æ¬¡åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶å¼•å…¥çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•çš„æ³¨å†Œè¯¯å·®å’Œä½è´¨é‡åˆæˆå›¾åƒé—®é¢˜ã€‚</li>
<li>æ¡†æ¶å…è®¸ç½‘ç»œé€šè¿‡è’¸é¦è¿‡ç¨‹ç›´æ¥â€œçœ‹åˆ°â€çœŸå®å›¾åƒã€‚</li>
<li>æ¡†æ¶ä¸“æ³¨äºå•å¼ æ ‡è®°å›¾åƒå’Œå°‘é‡æœªæ ‡è®°å›¾åƒä¸­çš„è§£å‰–ç»“æ„ã€‚</li>
<li>ä½¿ç”¨æ³¨å†Œå¼æ•°æ®å¢å¼ºç½‘ç»œåˆ›å»ºé€¼çœŸçš„æ ‡è®°æ ·æœ¬ã€‚</li>
<li>ç‰¹å¾è’¸é¦æ¨¡å—å¸®åŠ©å­¦ç”Ÿç½‘ç»œä»æ ‡è®°æ ·æœ¬ä¸­å­¦ä¹ åˆ†å‰²ï¼Œå—æ•™å¸ˆç½‘ç»œçš„å¼•å¯¼ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶çš„åˆ†å‰²æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¼˜äºé¢†å…ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.03616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2408.03616v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Recent-Backdoor-Attacks-and-Defenses-in-Large-Language-Models"><a href="#A-Survey-of-Recent-Backdoor-Attacks-and-Defenses-in-Large-Language-Models" class="headerlink" title="A Survey of Recent Backdoor Attacks and Defenses in Large Language   Models"></a>A Survey of Recent Backdoor Attacks and Defenses in Large Language   Models</h2><p><strong>Authors:Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Xiaoyu Xu, Xiaobao Wu, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan</strong></p>
<p>Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¼¥åˆäº†äººç±»è¯­è¨€ç†è§£ä¸å¤æ‚é—®é¢˜æ±‚è§£ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå¹¶åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­ã€‚å°½ç®¡LLMçš„æ•ˆç”¨å·²ç»å¾—åˆ°äº†è¯æ˜ï¼Œä½†ç”±äºè®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œç”¨æˆ·ä¸å¾—ä¸ä½¿ç”¨å¼€æºè¯­è¨€æ¨¡å‹æˆ–å°†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¤–åŒ…ç»™ç¬¬ä¸‰æ–¹å¹³å°ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹å®¹æ˜“å—åˆ°æ½œåœ¨çš„å®‰å…¨æ¼æ´çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨åé—¨æ”»å‡»ä¸­ã€‚åé—¨æ”»å‡»æ—¨åœ¨é€šè¿‡æ¯’å®³è®­ç»ƒæ ·æœ¬æˆ–æ¨¡å‹æƒé‡æ¥å¼•å…¥æœ‰é’ˆå¯¹æ€§çš„æ¼æ´ï¼Œä»è€Œä½¿æ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡æ¶æ„è§¦å‘å› ç´ æ¥æ“çºµæ¨¡å‹å“åº”ã€‚è™½ç„¶å…³äºåé—¨æ”»å‡»çš„ç°æœ‰è°ƒæŸ¥æä¾›äº†å…¨é¢çš„æ¦‚è¿°ï¼Œä½†å®ƒä»¬ç¼ºä¹å¯¹é’ˆå¯¹LLMçš„åé—¨æ”»å‡»çš„æ·±å…¥ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·å¹¶äº†è§£è¯¥é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…³äºLLMåé—¨æ”»å‡»çš„æ–°è§†è§’ï¼Œé‡ç‚¹å…³æ³¨å¾®è°ƒæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åé—¨æ”»å‡»ç³»ç»Ÿåœ°åˆ†ä¸ºä¸‰ç±»ï¼šå…¨å‚æ•°å¾®è°ƒã€å‚æ•°æœ‰æ•ˆå¾®è°ƒå’Œæ— å¾®è°ƒã€‚åŸºäºå¤§é‡çš„å®¡æŸ¥ç»“æœï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æœªæ¥ç ”ç©¶åé—¨æ”»å‡»çš„å…³é”®é—®é¢˜ï¼Œå¦‚è¿›ä¸€æ­¥æ¢ç´¢ä¸éœ€è¦å¾®è°ƒçš„æ”»å‡»ç®—æ³•ï¼Œæˆ–å¼€å‘æ›´éšè”½çš„æ”»å‡»ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06852v5">PDF</a> Accepted in TMLR</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·ä¾‹å’Œé›¶æ ·ä¾‹è®¾ç½®ä¸‹ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—èµ„æºé™åˆ¶ï¼Œç”¨æˆ·éœ€è¦ä½¿ç”¨å¼€æºè¯­è¨€æ¨¡å‹æˆ–å°†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¤–åŒ…ç»™ç¬¬ä¸‰æ–¹å¹³å°ã€‚æ­¤å¤–ï¼Œè¯­è¨€æ¨¡å‹å®¹æ˜“é­å—å®‰å…¨æ¼æ´æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯åé—¨æ”»å‡»ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†é’ˆå¯¹LLMçš„åé—¨æ”»å‡»çš„æ–°è§†è§’ï¼Œé‡ç‚¹ä»‹ç»äº†å¾®è°ƒæ–¹æ³•ã€‚æˆ‘ä»¬å°†åé—¨æ”»å‡»ç³»ç»Ÿåœ°åˆ†ä¸ºå…¨å‚æ•°å¾®è°ƒã€å‚æ•°æœ‰æ•ˆå¾®è°ƒå’Œæ— å¾®è°ƒä¸‰ç±»ï¼Œå¹¶åŸºäºå¤§é‡ç ”ç©¶è®¨è®ºäº†æœªæ¥ç ”ç©¶çš„å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨å°æ ·ä¾‹å’Œé›¶æ ·ä¾‹è®¾ç½®ä¸‹ã€‚</li>
<li>ç”±äºè®¡ç®—èµ„æºé™åˆ¶ï¼Œç”¨æˆ·éœ€ä¾èµ–å¼€æºè¯­è¨€æ¨¡å‹æˆ–ç¬¬ä¸‰æ–¹å¹³å°è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯­è¨€æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ç­‰å®‰å…¨æ¼æ´çš„å½±å“ã€‚</li>
<li>åé—¨æ”»å‡»å¯ä»¥é€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬æˆ–æ¨¡å‹æƒé‡è¿›è¡Œæ¯’å®³ï¼Œæ“çºµæ¨¡å‹å“åº”ã€‚</li>
<li>ç°æœ‰åé—¨æ”»å‡»è°ƒæŸ¥ç¼ºä¹é’ˆå¯¹LLMsçš„æ·±å…¥ç ”ç©¶ã€‚</li>
<li>æœ¬æ–‡å¯¹åé—¨æ”»å‡»é’ˆå¯¹LLMsè¿›è¡Œäº†åˆ†ç±»ï¼ŒåŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€å‚æ•°æœ‰æ•ˆå¾®è°ƒå’Œæ— å¾®è°ƒä¸‰ç§ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2406.06852v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RETTA-Retrieval-Enhanced-Test-Time-Adaptation-for-Zero-Shot-Video-Captioning"><a href="#RETTA-Retrieval-Enhanced-Test-Time-Adaptation-for-Zero-Shot-Video-Captioning" class="headerlink" title="RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video   Captioning"></a>RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video   Captioning</h2><p><strong>Authors:Yunchuan Ma, Laiyun Qing, Guorong Li, Yuankai Qi, Amin Beheshti, Quan Z. Sheng, Qingming Huang</strong></p>
<p>Despite the significant progress of fully-supervised video captioning, zero-shot methods remain much less explored. In this paper, we propose a novel zero-shot video captioning framework named Retrieval-Enhanced Test-Time Adaptation (RETTA), which takes advantage of existing pretrained large-scale vision and language models to directly generate captions with test-time adaptation. Specifically, we bridge video and text using four key models: a general video-text retrieval model XCLIP, a general image-text matching model CLIP, a text alignment model AnglE, and a text generation model GPT-2, due to their source-code availability. The main challenge is how to enable the text generation model to be sufficiently aware of the content in a given video so as to generate corresponding captions. To address this problem, we propose using learnable tokens as a communication medium among these four frozen models GPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains these tokens with training data, we propose to learn these tokens with soft targets of the inference data under several carefully crafted loss functions, which enable the tokens to absorb video information catered for GPT-2. This procedure can be efficiently done in just a few iterations (we use 16 iterations in the experiments) and does not require ground truth data. Extensive experimental results on three widely used datasets, MSR-VTT, MSVD, and VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric CIDEr compared to several state-of-the-art zero-shot video captioning methods. </p>
<blockquote>
<p>å°½ç®¡å…¨ç›‘ç£è§†é¢‘æè¿°å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†é›¶æ ·æœ¬æ–¹æ³•ä»ç„¶è¢«ç ”ç©¶å¾—è¾ƒå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬è§†é¢‘æè¿°æ¡†æ¶ï¼Œåä¸ºâ€œåŸºäºæ£€ç´¢çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆRETTAï¼‰â€ï¼Œå®ƒåˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ç›´æ¥ç”Ÿæˆæè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å››ä¸ªå…³é”®æ¨¡å‹æ¥è¿æ¥è§†é¢‘å’Œæ–‡æœ¬ï¼šé€šç”¨çš„è§†é¢‘æ–‡æœ¬æ£€ç´¢æ¨¡å‹XCLIPï¼Œé€šç”¨çš„å›¾åƒæ–‡æœ¬åŒ¹é…æ¨¡å‹CLIPï¼Œæ–‡æœ¬å¯¹é½æ¨¡å‹AnglEï¼Œä»¥åŠæ–‡æœ¬ç”Ÿæˆæ¨¡å‹GPT-2ï¼ŒåŸå› æ˜¯è¿™äº›æ¨¡å‹æœ‰æºä»£ç å¯ä¾›ä½¿ç”¨ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•è®©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å……åˆ†æ„è¯†åˆ°ç»™å®šè§†é¢‘çš„å†…å®¹ï¼Œä»è€Œç”Ÿæˆç›¸åº”çš„æè¿°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¯å­¦ä¹ çš„ä»¤ç‰Œä½œä¸ºè¿™å››ä¸ªå†»ç»“æ¨¡å‹GPT-2ã€XCLIPã€CLIPå’ŒAnglEä¹‹é—´çš„é€šä¿¡åª’ä»‹ã€‚ä¸åŒäºä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒè¿™äº›ä»¤ç‰Œçš„ä¼ ç»Ÿæ–¹å¼ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°ä¸‹çš„æ¨ç†æ•°æ®çš„è½¯ç›®æ ‡æ¥å­¦ä¹ è¿™äº›ä»¤ç‰Œï¼Œä½¿ä»¤ç‰Œèƒ½å¤Ÿå¸æ”¶é’ˆå¯¹GPT-2çš„è§†é¢‘ä¿¡æ¯ã€‚è¿™ä¸ªè¿‡ç¨‹åªéœ€å‡ æ¬¡è¿­ä»£å°±èƒ½é«˜æ•ˆå®Œæˆï¼ˆæˆ‘ä»¬åœ¨å®éªŒä¸­ä½¿ç”¨16æ¬¡è¿­ä»£ï¼‰ï¼Œå¹¶ä¸”ä¸éœ€è¦çœŸå®æ•°æ®ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†MSR-VTTã€MSVDå’ŒVATEXä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å‡ ç§æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è§†é¢‘æè¿°æ–¹æ³•ç›¸æ¯”ï¼Œä¸»è¦æŒ‡æ ‡CIDErçš„ç»å¯¹æ”¹è¿›ç‡ä¸º5.1%~32.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07046v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRetrieval-Enhanced Test-Time Adaptationï¼ˆRETTAï¼‰çš„é›¶æ ·æœ¬è§†é¢‘æè¿°æ¡†æ¶ï¼Œåˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æµ‹è¯•æ—¶é€‚åº”æŠ€æœ¯ç›´æ¥ç”Ÿæˆè§†é¢‘æè¿°ã€‚è¯¥æ¡†æ¶ç»“åˆå››ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬è§†é¢‘æ–‡æœ¬æ£€ç´¢æ¨¡å‹XCLIPã€å›¾åƒæ–‡æœ¬åŒ¹é…æ¨¡å‹CLIPã€æ–‡æœ¬å¯¹é½æ¨¡å‹AnglEå’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹GPT-2ï¼Œé€šè¿‡å¯å­¦ä¹ ä»¤ç‰Œä½œä¸ºæ²Ÿé€šåª’ä»‹ï¼Œåœ¨å‡ ä¸ªç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°ä¸‹ï¼Œä½¿ä»¤ç‰Œå¸æ”¶è§†é¢‘ä¿¡æ¯å¹¶é€‚åº”GPT-2ï¼Œä»¥ç”Ÿæˆç›¸åº”çš„è§†é¢‘æè¿°ã€‚è¿™ä¸€æµç¨‹åœ¨ä»…å‡ æ¬¡è¿­ä»£ä¸­å³å¯å®Œæˆï¼Œæ— éœ€çœŸå®æ•°æ®ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›åº”ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è§†é¢‘æè¿°æ–¹æ³•ç›¸æ¯”ï¼Œä¸»è¦æŒ‡æ ‡CIDEræœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åä¸ºRETTAçš„é›¶æ ·æœ¬è§†é¢‘æè¿°æ¡†æ¶ï¼Œç»“åˆé¢„è®­ç»ƒçš„å¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹è¿›è¡Œè§†é¢‘æè¿°ç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨å››ç§å…³é”®æ¨¡å‹ï¼šXCLIPã€CLIPã€AnglEå’ŒGPT-2ï¼Œé€šè¿‡å¯å­¦ä¹ ä»¤ç‰Œä½œä¸ºåª’ä»‹è¿›è¡Œä¿¡æ¯äº¤äº’ã€‚</li>
<li>æå‡ºäº†ä½¿ç”¨è½¯ç›®æ ‡åœ¨æµ‹è¯•æ—¶å­¦ä¹ ä»¤ç‰Œçš„æ–¹æ³•ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°ä½¿ä»¤ç‰Œå¸æ”¶è§†é¢‘ä¿¡æ¯å¹¶é€‚åº”GPT-2ã€‚</li>
<li>è¯¥æµç¨‹æ— éœ€çœŸå®æ•°æ®ï¼Œå¯åœ¨ä»…å‡ æ¬¡è¿­ä»£ä¸­å®Œæˆã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¹¿æ³›åº”ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>RETTAæ¡†æ¶åœ¨é›¶æ ·æœ¬è§†é¢‘æè¿°æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç›¸å…³ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2405.07046v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2405.07046v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Few-Shot/2405.07046v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8aa9463cad6600699429fa08e88254a4.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  Plasma-CycleGAN Plasma Biomarker-Guided MRI to PET Cross-modality   Translation Using Conditional CycleGAN
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ac39c89b14e8b743c89e1578ca0993f4.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">8671.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
