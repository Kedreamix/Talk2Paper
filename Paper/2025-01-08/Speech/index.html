<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-08  Noise-Robust Target-Speaker Voice Activity Detection Through   Self-Supervised Pretraining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-95c9de2f7a50b74ab7b668cd8983aeee.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-08-更新"><a href="#2025-01-08-更新" class="headerlink" title="2025-01-08 更新"></a>2025-01-08 更新</h1><h2 id="Noise-Robust-Target-Speaker-Voice-Activity-Detection-Through-Self-Supervised-Pretraining"><a href="#Noise-Robust-Target-Speaker-Voice-Activity-Detection-Through-Self-Supervised-Pretraining" class="headerlink" title="Noise-Robust Target-Speaker Voice Activity Detection Through   Self-Supervised Pretraining"></a>Noise-Robust Target-Speaker Voice Activity Detection Through   Self-Supervised Pretraining</h2><p><strong>Authors:Holger Severin Bovbjerg, Jan Østergaard, Jesper Jensen, Zheng-Hua Tan</strong></p>
<p>Target-Speaker Voice Activity Detection (TS-VAD) is the task of detecting the presence of speech from a known target-speaker in an audio frame. Recently, deep neural network-based models have shown good performance in this task. However, training these models requires extensive labelled data, which is costly and time-consuming to obtain, particularly if generalization to unseen environments is crucial. To mitigate this, we propose a causal, Self-Supervised Learning (SSL) pretraining framework, called Denoising Autoregressive Predictive Coding (DN-APC), to enhance TS-VAD performance in noisy conditions. We also explore various speaker conditioning methods and evaluate their performance under different noisy conditions. Our experiments show that DN-APC improves performance in noisy conditions, with a general improvement of approx. 2% in both seen and unseen noise. Additionally, we find that FiLM conditioning provides the best overall performance. Representation analysis via tSNE plots reveals robust initial representations of speech and non-speech from pretraining. This underscores the effectiveness of SSL pretraining in improving the robustness and performance of TS-VAD models in noisy environments. </p>
<blockquote>
<p>目标说话人语音活动检测（TS-VAD）的任务是在音频帧中检测已知目标说话人的语音是否存在。最近，基于深度神经网络的模型在此任务中表现出良好的性能。然而，训练这些模型需要大量的标注数据，获取这些数据成本高昂且耗时，特别是在推广到未见过的环境至关重要时。为了缓解这一问题，我们提出了一种名为去噪自回归预测编码（DN-APC）的因果自监督学习（SSL）预训练框架，以提高TS-VAD在嘈杂条件下的性能。我们还探索了各种说话人调节方法，并评估了它们在不同的嘈杂条件下的性能。我们的实验表明，DN-APC在嘈杂条件下提高了性能，在可见和不可见噪声条件下总体提高了约2%。此外，我们发现Film调节提供了最佳的整体性能。通过t-SNE图进行的表示分析揭示了预训练中对语音和非语音的稳健初始表示。这突出了SSL预训练在提高TS-VAD模型在嘈杂环境中的稳健性和性能方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03184v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing for possible publication. 12 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>目标说话人语音活动检测（TS-VAD）任务旨在从音频帧中检测已知目标说话人的语音。深度神经网络模型在该任务中表现出良好的性能，但其训练需要大量标注数据，获取成本高且耗时。为改善在噪声环境下的TS-VAD性能，本文提出了一种名为去噪自回归预测编码（DN-APC）的自监督学习预训练框架。同时，本文探索了多种说话人调节方法，并在不同噪声条件下评估其性能。实验表明，DN-APC在噪声环境下提高了性能，且在可见和不可见噪声下总体提高了约2%。此外，发现FiLM调节提供最佳的整体性能。通过t-SNE图进行表征分析显示，预训练对语音和非语音的初步表征稳健，突显了自监督学习预训练在提升TS-VAD模型在噪声环境中的稳健性和性能方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目标说话人语音活动检测（TS-VAD）是识别音频帧中已知目标说话人语音的任务。</li>
<li>深度神经网络模型在TS-VAD中表现良好，但训练需要大量标注数据。</li>
<li>提出了一种自监督学习预训练框架——去噪自回归预测编码（DN-APC），以提高模型在噪声环境下的性能。</li>
<li>DN-APC在可见和不可见噪声下总体提高了约2%的性能。</li>
<li>FiLM调节方法提供最佳的整体性能。</li>
<li>表征分析显示预训练对语音和非语音的初步表征稳健。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4684c1666402f7df500941f724503cc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70d5985c6ca7c6dbfb3f0054b344c83a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53cceb7a6d7c3bad1569e427daa395be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Samba-asr-state-of-the-art-speech-recognition-leveraging-structured-state-space-models"><a href="#Samba-asr-state-of-the-art-speech-recognition-leveraging-structured-state-space-models" class="headerlink" title="Samba-asr state-of-the-art speech recognition leveraging structured   state-space models"></a>Samba-asr state-of-the-art speech recognition leveraging structured   state-space models</h2><p><strong>Authors:Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</strong></p>
<p>We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research. </p>
<blockquote>
<p>我们提出了Samba ASR，这是首个利用新型Mamba架构作为编码器和解码器的先进自动语音识别（ASR）模型，该模型基于状态空间模型（SSMs）构建。不同于基于变压器的ASR模型，它依赖于自注意力机制来捕捉依赖关系，Samba ASR通过使用高效的状态空间动力学有效地对局部和全局时间依赖关系进行建模，实现了显著的性能提升。通过解决变压器模型的局限性，如输入长度的二次扩展和处理长距离依赖关系的困难，Samba ASR在准确性和效率上达到了卓越的水平。</p>
</blockquote>
<p>实验结果证明，Samba ASR在各种标准基准测试上超越了现有的开源基于变压器的ASR模型，成为ASR领域的新技术标杆。在基准数据集上的广泛评估显示，其在单词错误率（WER）上有显著改善，即使在资源匮乏的场景中也具有竞争力。此外，Mamba架构的计算效率和参数优化使得Samba ASR成为各种ASR任务的可扩展和稳健解决方案。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02832v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于状态空间模型（SSMs）的新颖Mamba架构，提出了一款领先的自动语音识别（ASR）模型——Samba ASR。该模型在编码器和解码器中都使用Mamba架构，有效捕捉语音序列的局部和全局时间依赖性，实现了显著的性能提升。与依赖自注意力机制的基于变压器的ASR模型相比，Samba ASR通过解决二次扩展输入长度和难以处理长距离依赖性问题等缺点，实现了更高的准确性和效率。Samba ASR在多种标准基准测试中均超过了现有的开源基于变压器的ASR模型，且在低资源场景中也有出色的表现。同时，Mamba架构的计算效率和参数优化使得Samba ASR成为多样ASR任务的可扩展和稳健解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Samba ASR是基于状态空间模型（SSMs）的Mamba架构的自动语音识别（ASR）模型。</li>
<li>与基于变压器的ASR模型相比，Samba ASR更有效地捕捉语音序列的局部和全局时间依赖性。</li>
<li>Samba ASR解决了基于变压器模型的缺点，如二次扩展输入长度和长距离依赖性问题处理困难等。</li>
<li>Samba ASR在多种标准基准测试中表现优异，超过了现有的开源基于变压器的ASR模型。</li>
<li>Samba ASR在低资源场景中也有出色的表现，且具备良好的计算效率和参数优化。</li>
<li>深入研究分析了Samba ASR的可行性，验证了它作为无变压器替代方案的高效性和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-beff2ade285eb90e0d49e772856f854f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Efficient-Long-Speech-Sequence-Modelling-for-Time-Domain-Depression-Level-Estimation"><a href="#Efficient-Long-Speech-Sequence-Modelling-for-Time-Domain-Depression-Level-Estimation" class="headerlink" title="Efficient Long Speech Sequence Modelling for Time-Domain Depression   Level Estimation"></a>Efficient Long Speech Sequence Modelling for Time-Domain Depression   Level Estimation</h2><p><strong>Authors:Shuanglin Li, Zhijie Xie, Syed Mohsen Naqvi</strong></p>
<p>Depression significantly affects emotions, thoughts, and daily activities. Recent research indicates that speech signals contain vital cues about depression, sparking interest in audio-based deep-learning methods for estimating its severity. However, most methods rely on time-frequency representations of speech which have recently been criticized for their limitations due to the loss of information when performing time-frequency projections, e.g. Fourier transform, and Mel-scale transformation. Furthermore, segmenting real-world speech into brief intervals risks losing critical interconnections between recordings. Additionally, such an approach may not adequately reflect real-world scenarios, as individuals with depression often pause and slow down in their conversations and interactions. Building on these observations, we present an efficient method for depression level estimation using long speech signals in the time domain. The proposed method leverages a state space model coupled with the dual-path structure-based long sequence modelling module and temporal external attention module to reconstruct and enhance the detection of depression-related cues hidden in the raw audio waveforms. Experimental results on the AVEC2013 and AVEC2014 datasets show promising results in capturing consequential long-sequence depression cues and demonstrate outstanding performance over the state-of-the-art. </p>
<blockquote>
<p>抑郁症显著影响情绪、思维和日常活动。最新研究表明，语音信号包含关于抑郁症的重要线索，这引发了人们对基于音频的深度学习方法在估计其严重程度方面的兴趣。然而，大多数方法依赖于语音的时间-频率表示，由于在进行时间-频率投影（例如傅立叶变换和梅尔尺度变换）时存在信息丢失而受到批评，这些时间频率表示存在局限性。此外，将现实世界中的语音分割成简短的间隔区间可能丢失关键录音之间的连接。再者，由于抑郁症患者通常在对话和互动中会停顿并放慢速度，这种方法可能无法充分反映现实场景。基于这些观察，我们提出了一种在时域内使用长语音信号进行抑郁程度估计的有效方法。该方法利用状态空间模型与基于双路径结构的长期序列建模模块和临时外部注意力模块相结合，重建并提高对隐藏在原始音频波形中的抑郁相关线索的检测。在AVEC2013和AVEC2014数据集上的实验结果表明，该方法在捕捉重要的长期抑郁线索方面表现出良好的潜力，并在最新的技术研究中展现出卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02512v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>抑郁症严重影响情绪、思维及日常活动。最新研究表明，语音信号中包含关于抑郁症的重要线索，引发音频深度学习方法估算其严重性的兴趣。但多数方法依赖于语音的时间频率表示，此法因投影过程中的信息丢失而受到批评。为克服这些限制，我们提出了一种在时域使用长语音信号进行抑郁症水平估计的有效方法。该方法结合状态空间模型、双路径结构的长序列建模模块和临时外部注意力模块，重建并增强原始音频波形中隐藏的抑郁症相关线索的检测。在AVEC2013和AVEC2014数据集上的实验结果表明，该方法在捕捉长期抑郁症线索方面表现出良好效果，并在最新技术上展现了卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>抑郁症对情绪、思维和日常活动产生显著影响。</li>
<li>语音信号包含关于抑郁症的重要线索，引发音频深度学习方法的兴趣。</li>
<li>当前方法主要依赖于语音的时间频率表示，但此法存在信息丢失的批评。</li>
<li>提出一种在时域使用长语音信号的有效方法，以克服上述限制。</li>
<li>所提出的方法结合状态空间模型、双路径结构建模和临时外部注意力模块。</li>
<li>方法在捕捉长期抑郁症线索方面表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-95c9de2f7a50b74ab7b668cd8983aeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a40ea5393cf8a32516f446f50c4e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4b41843ae691c3a11538e710152f552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-296babf5dc7f55deb974137ef55fb6c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3f5868a0db63d0b422cdb07ecf0b34d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52baf576711aafbd014f1077112d1a5d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding"><a href="#Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding" class="headerlink" title="Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding"></a>Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding</h2><p><strong>Authors:Jiahui Zhao, Hao Shi, Chenrui Cui, Tianrui Wang, Hexin Liu, Zhaoheng Ni, Lingxuan Ye, Longbiao Wang</strong></p>
<p>Code-switching (CS) automatic speech recognition (ASR) faces challenges due to the language confusion resulting from accents, auditory similarity, and seamless language switches. Adaptation on the pre-trained multi-lingual model has shown promising performance for CS-ASR. In this paper, we adapt Whisper, which is a large-scale multilingual pre-trained speech recognition model, to CS from both encoder and decoder parts. First, we propose an encoder refiner to enhance the encoder’s capacity of intra-sentence swithching. Second, we propose using two sets of language-aware adapters with different language prompt embeddings to achieve language-specific decoding information in each decoder layer. Then, a fusion module is added to fuse the language-aware decoding. The experimental results using the SEAME dataset show that, compared with the baseline model, the proposed approach achieves a relative MER reduction of 4.1% and 7.2% on the dev_man and dev_sge test sets, respectively, surpassing state-of-the-art methods. Through experiments, we found that the proposed method significantly improves the performance on non-native language in CS speech, indicating that our approach enables Whisper to better distinguish between the two languages. </p>
<blockquote>
<p>代码切换（CS）自动语音识别（ASR）面临着由于口音、听觉相似性和无缝语言切换导致的语言混淆所带来的挑战。预训练的多元语言模型的适配已经显示出对CS-ASR的有前途的性能。在本文中，我们适配了whisper这一大规模的多语种预训练语音识别模型，用于句子内和句子间的代码切换。首先，我们提出了一种编码器优化器，以提高编码器在句子内部切换的能力。其次，我们建议使用两组带有不同语言提示嵌入的语言感知适配器，以实现每个解码器层中的语言特定解码信息。然后，添加一个融合模块来融合语言感知解码。使用SEAME数据集的实验结果表明，与基线模型相比，所提出的方法在dev_man和dev_sge测试集上分别实现了相对误差率（MER）降低4.1%和7.2%，超过了最新方法。通过实验，我们发现该方法在非母语语言的CS语音性能上有了显著提高，这表明我们的方法使whisper能够更好地区分两种语言。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16507v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对代码切换（CS）自动语音识别（ASR）中的语言混淆、口音、听觉相似性和无缝语言切换等问题，本文采用预训练的多语言模型进行适应并展现出良好性能。研究团队对大型预训练语音识别模型Whisper进行了适应，从编码器和解码器两个方面进行改进。通过提出编码器精炼器增强了编码器对句子内切换的处理能力，同时使用两组语言感知适配器和不同的语言提示嵌入来实现解码器的语言特异性。添加融合模块以融合语言感知的解码结果。在SEAME数据集上的实验结果表明，与基线模型相比，该方法在dev_man和dev_sge测试集上相对实现了4.1%和7.2%的单词错误率（MER）降低，超越了现有最先进的方法。实验还发现，该方法对非本族语的CS语音表现有明显的提升，证明该方法能更好地区分两种语言。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码切换（CS）自动语音识别（ASR）面临多种挑战，包括口音、听觉相似性导致的语言混淆和无缝语言切换问题。</li>
<li>预训练的多语言模型适应于CS-ASR显示出良好性能。</li>
<li>研究团队通过改进编码器和解码器两部分来增强模型处理CS语音的能力。</li>
<li>编码器精炼器增强了编码器处理句子内切换的能力。</li>
<li>使用语言感知适配器和不同的语言提示嵌入来实现解码器的语言特异性。</li>
<li>融合模块用于融合不同语言的解码结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-564f5f18dfe3de21d21a61818b4664d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480fe73cba4fd25dd76098c4a3b3909e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b4712943f54b75148744a7a47f2461.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation"><a href="#OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation" class="headerlink" title="OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation"></a>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation</h2><p><strong>Authors:Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu</strong></p>
<p>Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid">https://fudan-generative-vision.github.io/OpenHumanVid</a> </p>
<blockquote>
<p>随着视觉生成技术的最新进展，视频数据集的数量和可用性显著增加，这对于训练有效的视频生成模型至关重要。然而，高质量、以人类为中心的视频数据集的匮乏给该领域的进步带来了挑战。为了弥补这一差距，我们推出了OpenHumanVid，这是一个大规模、高质量、以人类为中心的视频数据集，其特点是拥有精确详细的字幕，涵盖了人类外观和运动状态，还包括额外的人类运动条件，如骨骼序列和语音音频。为了验证该数据集和相关训练策略的有效性，我们对现有的经典扩散变压器架构进行了扩展，并在所提出的数据集上对我们的模型进行了进一步的预训练。我们的研究发现两个关键见解：首先，使用大规模、高质量的数据集可以显著提高生成的人类视频的评价指标，同时保留了一般视频生成任务中的性能；其次，文本与人类外观、人类运动和面部运动的有效对齐对于生成高质量视频输出至关重要。基于这些见解和相应的方法论，在所提出的数据集上训练的简单扩展网络在人类为中心的视频生成方面显示出明显的改进。项目页面 <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid">https://fudan-generative-vision.github.io/OpenHumanVid</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00115v3">PDF</a> 11 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>随着视觉生成技术的进步，视频数据集规模和可用性的显著提升对训练有效的视频生成模型至关重要。然而，缺乏高质量的人类中心视频数据集成为该领域进展的挑战。为此，我们推出了OpenHumanVid，这是一个大规模、高质量的人类中心视频数据集，以精确详细的字幕为特色，涵盖人类外观和运动状态，以及包括骨架序列和语音音频等额外人类运动条件。为了验证该数据集和相关训练策略的有效性，我们对现有的经典扩散变压器架构进行了扩展，并在该数据集上对我们的模型进行了预训练。研究发现，大规模高质量数据集的引入显著提高了生成的人类视频的评价指标，同时保持了通用视频生成任务的性能；文本与人类外观、运动和面部运动的有效对齐对于生成高质量视频输出至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频生成技术因视觉生成技术的进展而得到显著提升，对大规模高质量视频数据集的需求增加。</li>
<li>缺乏高质量的人类中心视频数据集是当前的挑战。</li>
<li>OpenHumanVid是一个大规模、高质量的人类中心视频数据集，包含详细字幕、人类外观和运动状态以及额外的人类运动条件。</li>
<li>数据集的引入显著提高了生成的人类视频的评价指标。</li>
<li>文本与人类外观、运动和面部运动的对齐是生成高质量视频的关键。</li>
<li>通过对现有经典扩散变压器架构的扩展和在该数据集上的预训练，模型在生成人类中心视频时表现出明显的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4eaf2c10d7def6c48f0f7aea62e1ecf4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd9ce755a74992124dd116371eaf2b67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a27d7479c2651f1df22cb1a4aeeea7d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d0076d4045ef34d2154075203f15b62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-310e4cb20848c5672f8d891ccd5da16d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13b1eee368b8ad773fc88f08041e8b6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0c8869ccbd53abd95503fa590524380.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="How-to-Learn-a-New-Language-An-Efficient-Solution-for-Self-Supervised-Learning-Models-Unseen-Languages-Adaption-in-Low-Resource-Scenario"><a href="#How-to-Learn-a-New-Language-An-Efficient-Solution-for-Self-Supervised-Learning-Models-Unseen-Languages-Adaption-in-Low-Resource-Scenario" class="headerlink" title="How to Learn a New Language? An Efficient Solution for Self-Supervised   Learning Models Unseen Languages Adaption in Low-Resource Scenario"></a>How to Learn a New Language? An Efficient Solution for Self-Supervised   Learning Models Unseen Languages Adaption in Low-Resource Scenario</h2><p><strong>Authors:Shih-Heng Wang, Zih-Ching Chen, Jiatong Shi, Ming-To Chuang, Guan-Ting Lin, Kuan-Po Huang, David Harwath, Shang-Wen Li, Hung-yi Lee</strong></p>
<p>The utilization of speech Self-Supervised Learning (SSL) models achieves impressive performance on Automatic Speech Recognition (ASR). However, in low-resource language ASR, they encounter the domain mismatch problem between pre-trained and low-resource languages. Typical solutions like fine-tuning the SSL model suffer from high computation costs while using frozen SSL models as feature extractors comes with poor performance. To handle these issues, we extend a conventional efficient fine-tuning scheme based on the adapter. We add an extra intermediate adaptation to warm up the adapter and downstream model initialization. Remarkably, we update only 1-5% of the total model parameters to achieve the adaptation. Experimental results on the ML-SUPERB dataset show that our solution outperforms conventional efficient fine-tuning. It achieves up to a 28% relative improvement in the Character&#x2F;Phoneme error rate when adapting to unseen languages. </p>
<blockquote>
<p>语音自监督学习（SSL）模型的利用在自动语音识别（ASR）方面取得了令人印象深刻的性能。然而，在低资源语言ASR中，它们遇到了预训练模型与低资源语言之间的域不匹配问题。典型的解决方案，如微调SSL模型，存在计算成本高的问题，而使用冻结的SSL模型作为特征提取器则性能较差。为了解决这些问题，我们基于适配器扩展了一种常规的精细调整方案。我们添加一个额外的中间适应过程来预热适配器和下游模型初始化。值得注意的是，我们只需要更新模型总参数的1-5%就可以实现适配。在ML-SUPERB数据集上的实验结果表明，我们的解决方案优于传统的有效微调方法。在适应未见语言时，它实现了高达28%的相对改进在字符&#x2F;音素错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18217v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>语音自监督学习（SSL）模型在自动语音识别（ASR）方面表现出卓越性能。但在低资源语言ASR中，它们面临预训练模型与低资源语言之间领域不匹配的问题。常见的解决方案如微调SSL模型计算成本高，而使用冻结的SSL模型作为特征提取器性能较差。为了解决这个问题，我们基于适配器扩展了传统的有效微调方案，并添加了额外的中间适应来预热适配器和下游模型初始化。我们仅更新模型总参数的1-5%即可实现适应。在ML-SUPERB数据集上的实验结果表明，我们的解决方案优于传统的有效微调方法，在适应未见语言时，字符&#x2F;音素错误率相对提高了高达28%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音自监督学习（SSL）模型在自动语音识别（ASR）中具有卓越性能。</li>
<li>在低资源语言ASR中，存在预训练模型与低资源语言之间的领域不匹配问题。</li>
<li>传统的SSL模型微调方案计算成本高，而使用冻结模型作为特征提取器效果欠佳。</li>
<li>通过基于适配器的扩展方案，实现了高效的模型适应。</li>
<li>额外中间适应步骤用于预热适配器和下游模型初始化。</li>
<li>仅更新少量模型参数（1-5%）即可实现适应。</li>
<li>在ML-SUPERB数据集上的实验表明，新方法在适应未见语言时相对改进了28%的字符&#x2F;音素错误率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18217">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-41bd6330683dee044529af7dfe7f2be4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdf3edf90daa1dc958374233d9c261a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af191bad6b7acffb3d8d7054b446b813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9062a9256e0c062db6231b58d1f4336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4b8eced3e7b0c789b78dd45f57aed4b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Modulating-State-Space-Model-with-SlowFast-Framework-for-Compute-Efficient-Ultra-Low-Latency-Speech-Enhancement"><a href="#Modulating-State-Space-Model-with-SlowFast-Framework-for-Compute-Efficient-Ultra-Low-Latency-Speech-Enhancement" class="headerlink" title="Modulating State Space Model with SlowFast Framework for   Compute-Efficient Ultra Low-Latency Speech Enhancement"></a>Modulating State Space Model with SlowFast Framework for   Compute-Efficient Ultra Low-Latency Speech Enhancement</h2><p><strong>Authors:Longbiao Cheng, Ashutosh Pandey, Buye Xu, Tobi Delbruck, Vamsi Krishna Ithapu, Shih-Chii Liu</strong></p>
<p>Deep learning-based speech enhancement (SE) methods often face significant computational challenges when needing to meet low-latency requirements because of the increased number of frames to be processed. This paper introduces the SlowFast framework which aims to reduce computation costs specifically when low-latency enhancement is needed. The framework consists of a slow branch that analyzes the acoustic environment at a low frame rate, and a fast branch that performs SE in the time domain at the needed higher frame rate to match the required latency. Specifically, the fast branch employs a state space model where its state transition process is dynamically modulated by the slow branch. Experiments on a SE task with a 2 ms algorithmic latency requirement using the Voice Bank + Demand dataset show that our approach reduces computation cost by 70% compared to a baseline single-branch network with equivalent parameters, without compromising enhancement performance. Furthermore, by leveraging the SlowFast framework, we implemented a network that achieves an algorithmic latency of just 62.5 {\mu}s (one sample point at 16 kHz sample rate) with a computation cost of 100 M MACs&#x2F;s, while scoring a PESQ-NB of 3.12 and SISNR of 16.62. </p>
<blockquote>
<p>基于深度学习的语音增强（SE）方法在满足低延迟要求时面临着巨大的计算挑战，因为需要处理的帧数增加。本文介绍了SlowFast框架，该框架旨在减少在低延迟增强时所需的计算成本。框架包含一个慢速分支，以低帧率分析声学环境，以及一个快速分支，以所需的高帧率在时域执行SE，以匹配所需的延迟。具体来说，快速分支采用状态空间模型，其状态转换过程受到慢速分支的动态调制。在Voice Bank + Demand数据集上使用具有2毫秒算法延迟要求的语音增强任务实验表明，与具有等效参数的基线单分支网络相比，我们的方法在计算成本方面降低了70%，同时不损害增强性能。此外，通过利用SlowFast框架，我们实现了一个网络，该网络达到了仅62.5μs的算法延迟（在16kHz采样率下的一个样本点），计算成本为每秒1亿次乘累加（MACs），同时获得PESQ-NB得分为3.12和SISNR得分为16.62。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02019v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于深度学习的语音增强方法在低延迟要求下所面临的计算挑战，并为此引入了SlowFast框架。该框架包括一个以低帧率分析声学环境的慢分支，以及一个以所需的高帧率进行时域语音增强的快分支。实验表明，相较于基线单分支网络，SlowFast框架在降低计算成本的同时，不妥协增强性能。此外，利用SlowFast框架实现的网络在算法延迟仅为62.5微秒的情况下，计算成本为每秒百万次乘累加（MACs）运算，性能评估指标PESQ-NB和SISNR分别达到了3.12和16.62。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习的语音增强方法在低延迟环境下存在计算挑战。</li>
<li>SlowFast框架旨在降低低延迟增强时的计算成本。</li>
<li>SlowFast框架包括一个慢分支用于分析声学环境，一个快分支用于时域增强。</li>
<li>快分支采用状态空间模型，其状态转换过程受慢分支动态调制。</li>
<li>实验表明，相较于基线网络，SlowFast框架在计算成本降低70%的同时，不妥协增强性能。</li>
<li>利用SlowFast框架实现的网络在算法延迟仅为62.5微秒时表现出良好的性能。</li>
<li>该网络的计算成本为每秒百万次乘累加（MACs）运算，性能评估指标高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02019">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0cded0719fe53918baa47a70926bf70d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e4752b3b2d27f0ad4d948b83fe24553.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-332213bba2c502fda0b5fa8a3618b583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2712d5b51b00ea239a609394369a4b3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca10f0ed18861a3b3ec68a60e0d3a13c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LlamaPartialSpoof-An-LLM-Driven-Fake-Speech-Dataset-Simulating-Disinformation-Generation"><a href="#LlamaPartialSpoof-An-LLM-Driven-Fake-Speech-Dataset-Simulating-Disinformation-Generation" class="headerlink" title="LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating   Disinformation Generation"></a>LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating   Disinformation Generation</h2><p><strong>Authors:Hieu-Thi Luong, Haoyang Li, Lin Zhang, Kong Aik Lee, Eng Siong Chng</strong></p>
<p>Previous fake speech datasets were constructed from a defender’s perspective to develop countermeasure (CM) systems without considering diverse motivations of attackers. To better align with real-life scenarios, we created LlamaPartialSpoof, a 130-hour dataset that contains both fully and partially fake speech, using a large language model (LLM) and voice cloning technologies to evaluate the robustness of CMs. By examining valuable information for both attackers and defenders, we identify several key vulnerabilities in current CM systems, which can be exploited to enhance attack success rates, including biases toward certain text-to-speech models or concatenation methods. Our experimental results indicate that the current fake speech detection system struggle to generalize to unseen scenarios, achieving a best performance of 24.49% equal error rate. </p>
<blockquote>
<p>之前的虚假语音数据集是从防御者的角度构建的，旨在开发对抗措施（CM）系统，而没有考虑到攻击者的不同动机。为了更好地符合现实场景，我们创建了LlamaPartialSpoof数据集，这是一个包含完全和部分虚假语音的130小时数据集，利用大型语言模型（LLM）和语音克隆技术来评估对抗措施的稳健性。通过对攻击者和防御者有价值信息的考察，我们发现了当前对抗措施系统中的几个关键漏洞，这些漏洞可以被利用来提高攻击成功率，包括对某些文本到语音模型的偏见或拼接方法。我们的实验结果表明，当前的虚假语音检测系统很难推广到未见过的场景，最佳性能为达到24.49%的等误码率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14743v2">PDF</a> 5 pages, ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文创建了一个名为LlamaPartialSpoof的新假语音数据集，用于评估现有语音反篡改系统的鲁棒性。数据集包括完全和部分假语音，采用大型语言模型和语音克隆技术生成。通过分析攻击者和防御者的有价值信息，发现当前反篡改系统的关键漏洞，这些漏洞可被利用提高攻击成功率。实验结果表明，现有假语音检测系统在新场景下的泛化能力较弱。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了名为LlamaPartialSpoof的新假语音数据集，包含完全和部分假语音。</li>
<li>利用大型语言模型和语音克隆技术生成假语音数据。</li>
<li>通过对数据集的分析，发现了现有语音反篡改系统的关键漏洞。</li>
<li>这些漏洞可被攻击者利用以提高攻击成功率。</li>
<li>当前假语音检测系统在新场景下的泛化能力较弱。</li>
<li>最佳性能的反假语音系统实现了24.49%的误判率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14743">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3d8d0e47721d073dcd08751248d6fe4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-080eaa7731b784bb78736c8780ef1aaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b22ec77ff428eea32bca62bba1e0761a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-318cd9b242a3495dc1d0307e3ee8b7fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-dfa3d8c03770e2a1e3d63df40c81942f.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-08  STAR Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-382898e406c573683e406a518fee98a2.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-08  Phase-contrast imaging of a dense atomic cloud
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
