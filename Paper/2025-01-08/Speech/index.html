<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  Noise-Robust Target-Speaker Voice Activity Detection Through   Self-Supervised Pretraining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-95c9de2f7a50b74ab7b668cd8983aeee.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    31 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-08-æ›´æ–°"><a href="#2025-01-08-æ›´æ–°" class="headerlink" title="2025-01-08 æ›´æ–°"></a>2025-01-08 æ›´æ–°</h1><h2 id="Noise-Robust-Target-Speaker-Voice-Activity-Detection-Through-Self-Supervised-Pretraining"><a href="#Noise-Robust-Target-Speaker-Voice-Activity-Detection-Through-Self-Supervised-Pretraining" class="headerlink" title="Noise-Robust Target-Speaker Voice Activity Detection Through   Self-Supervised Pretraining"></a>Noise-Robust Target-Speaker Voice Activity Detection Through   Self-Supervised Pretraining</h2><p><strong>Authors:Holger Severin Bovbjerg, Jan Ã˜stergaard, Jesper Jensen, Zheng-Hua Tan</strong></p>
<p>Target-Speaker Voice Activity Detection (TS-VAD) is the task of detecting the presence of speech from a known target-speaker in an audio frame. Recently, deep neural network-based models have shown good performance in this task. However, training these models requires extensive labelled data, which is costly and time-consuming to obtain, particularly if generalization to unseen environments is crucial. To mitigate this, we propose a causal, Self-Supervised Learning (SSL) pretraining framework, called Denoising Autoregressive Predictive Coding (DN-APC), to enhance TS-VAD performance in noisy conditions. We also explore various speaker conditioning methods and evaluate their performance under different noisy conditions. Our experiments show that DN-APC improves performance in noisy conditions, with a general improvement of approx. 2% in both seen and unseen noise. Additionally, we find that FiLM conditioning provides the best overall performance. Representation analysis via tSNE plots reveals robust initial representations of speech and non-speech from pretraining. This underscores the effectiveness of SSL pretraining in improving the robustness and performance of TS-VAD models in noisy environments. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰çš„ä»»åŠ¡æ˜¯åœ¨éŸ³é¢‘å¸§ä¸­æ£€æµ‹å·²çŸ¥ç›®æ ‡è¯´è¯äººçš„è¯­éŸ³æ˜¯å¦å­˜åœ¨ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œè·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„ç¯å¢ƒè‡³å…³é‡è¦æ—¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå»å™ªè‡ªå›å½’é¢„æµ‹ç¼–ç ï¼ˆDN-APCï¼‰çš„å› æœè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œä»¥æé«˜TS-VADåœ¨å˜ˆæ‚æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†å„ç§è¯´è¯äººè°ƒèŠ‚æ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨ä¸åŒçš„å˜ˆæ‚æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDN-APCåœ¨å˜ˆæ‚æ¡ä»¶ä¸‹æé«˜äº†æ€§èƒ½ï¼Œåœ¨å¯è§å’Œä¸å¯è§å™ªå£°æ¡ä»¶ä¸‹æ€»ä½“æé«˜äº†çº¦2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°Filmè°ƒèŠ‚æä¾›äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡t-SNEå›¾è¿›è¡Œçš„è¡¨ç¤ºåˆ†ææ­ç¤ºäº†é¢„è®­ç»ƒä¸­å¯¹è¯­éŸ³å’Œéè¯­éŸ³çš„ç¨³å¥åˆå§‹è¡¨ç¤ºã€‚è¿™çªå‡ºäº†SSLé¢„è®­ç»ƒåœ¨æé«˜TS-VADæ¨¡å‹åœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03184v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing for possible publication. 12 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>ç›®æ ‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰ä»»åŠ¡æ—¨åœ¨ä»éŸ³é¢‘å¸§ä¸­æ£€æµ‹å·²çŸ¥ç›®æ ‡è¯´è¯äººçš„è¯­éŸ³ã€‚æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†å…¶è®­ç»ƒéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè·å–æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚ä¸ºæ”¹å–„åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„TS-VADæ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå»å™ªè‡ªå›å½’é¢„æµ‹ç¼–ç ï¼ˆDN-APCï¼‰çš„è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒæ¡†æ¶ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æ¢ç´¢äº†å¤šç§è¯´è¯äººè°ƒèŠ‚æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒå™ªå£°æ¡ä»¶ä¸‹è¯„ä¼°å…¶æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒDN-APCåœ¨å™ªå£°ç¯å¢ƒä¸‹æé«˜äº†æ€§èƒ½ï¼Œä¸”åœ¨å¯è§å’Œä¸å¯è§å™ªå£°ä¸‹æ€»ä½“æé«˜äº†çº¦2%ã€‚æ­¤å¤–ï¼Œå‘ç°FiLMè°ƒèŠ‚æä¾›æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡t-SNEå›¾è¿›è¡Œè¡¨å¾åˆ†ææ˜¾ç¤ºï¼Œé¢„è®­ç»ƒå¯¹è¯­éŸ³å’Œéè¯­éŸ³çš„åˆæ­¥è¡¨å¾ç¨³å¥ï¼Œçªæ˜¾äº†è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒåœ¨æå‡TS-VADæ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰æ˜¯è¯†åˆ«éŸ³é¢‘å¸§ä¸­å·²çŸ¥ç›®æ ‡è¯´è¯äººè¯­éŸ³çš„ä»»åŠ¡ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨TS-VADä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†è®­ç»ƒéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒæ¡†æ¶â€”â€”å»å™ªè‡ªå›å½’é¢„æµ‹ç¼–ç ï¼ˆDN-APCï¼‰ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>DN-APCåœ¨å¯è§å’Œä¸å¯è§å™ªå£°ä¸‹æ€»ä½“æé«˜äº†çº¦2%çš„æ€§èƒ½ã€‚</li>
<li>FiLMè°ƒèŠ‚æ–¹æ³•æä¾›æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚</li>
<li>è¡¨å¾åˆ†ææ˜¾ç¤ºé¢„è®­ç»ƒå¯¹è¯­éŸ³å’Œéè¯­éŸ³çš„åˆæ­¥è¡¨å¾ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4684c1666402f7df500941f724503cc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70d5985c6ca7c6dbfb3f0054b344c83a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53cceb7a6d7c3bad1569e427daa395be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Samba-asr-state-of-the-art-speech-recognition-leveraging-structured-state-space-models"><a href="#Samba-asr-state-of-the-art-speech-recognition-leveraging-structured-state-space-models" class="headerlink" title="Samba-asr state-of-the-art speech recognition leveraging structured   state-space models"></a>Samba-asr state-of-the-art speech recognition leveraging structured   state-space models</h2><p><strong>Authors:Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</strong></p>
<p>We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Samba ASRï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨æ–°å‹Mambaæ¶æ„ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„å…ˆè¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰æ„å»ºã€‚ä¸åŒäºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œå®ƒä¾èµ–äºè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰ä¾èµ–å…³ç³»ï¼ŒSamba ASRé€šè¿‡ä½¿ç”¨é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦æœ‰æ•ˆåœ°å¯¹å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡è§£å†³å˜å‹å™¨æ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚è¾“å…¥é•¿åº¦çš„äºŒæ¬¡æ‰©å±•å’Œå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»çš„å›°éš¾ï¼ŒSamba ASRåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šè¾¾åˆ°äº†å“è¶Šçš„æ°´å¹³ã€‚</p>
</blockquote>
<p>å®éªŒç»“æœè¯æ˜ï¼ŒSamba ASRåœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œæˆä¸ºASRé¢†åŸŸçš„æ–°æŠ€æœ¯æ ‡æ†ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå³ä½¿åœ¨èµ„æºåŒ®ä¹çš„åœºæ™¯ä¸­ä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒMambaæ¶æ„çš„è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿å¾—Samba ASRæˆä¸ºå„ç§ASRä»»åŠ¡çš„å¯æ‰©å±•å’Œç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02832v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„æ–°é¢–Mambaæ¶æ„ï¼Œæå‡ºäº†ä¸€æ¬¾é¢†å…ˆçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹â€”â€”Samba ASRã€‚è¯¥æ¨¡å‹åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­éƒ½ä½¿ç”¨Mambaæ¶æ„ï¼Œæœ‰æ•ˆæ•æ‰è¯­éŸ³åºåˆ—çš„å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¾èµ–è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ç›¸æ¯”ï¼ŒSamba ASRé€šè¿‡è§£å†³äºŒæ¬¡æ‰©å±•è¾“å…¥é•¿åº¦å’Œéš¾ä»¥å¤„ç†é•¿è·ç¦»ä¾èµ–æ€§é—®é¢˜ç­‰ç¼ºç‚¹ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚Samba ASRåœ¨å¤šç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¿‡äº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œä¸”åœ¨ä½èµ„æºåœºæ™¯ä¸­ä¹Ÿæœ‰å‡ºè‰²çš„è¡¨ç°ã€‚åŒæ—¶ï¼ŒMambaæ¶æ„çš„è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿å¾—Samba ASRæˆä¸ºå¤šæ ·ASRä»»åŠ¡çš„å¯æ‰©å±•å’Œç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Samba ASRæ˜¯åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„Mambaæ¶æ„çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚</li>
<li>ä¸åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ç›¸æ¯”ï¼ŒSamba ASRæ›´æœ‰æ•ˆåœ°æ•æ‰è¯­éŸ³åºåˆ—çš„å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>Samba ASRè§£å†³äº†åŸºäºå˜å‹å™¨æ¨¡å‹çš„ç¼ºç‚¹ï¼Œå¦‚äºŒæ¬¡æ‰©å±•è¾“å…¥é•¿åº¦å’Œé•¿è·ç¦»ä¾èµ–æ€§é—®é¢˜å¤„ç†å›°éš¾ç­‰ã€‚</li>
<li>Samba ASRåœ¨å¤šç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ã€‚</li>
<li>Samba ASRåœ¨ä½èµ„æºåœºæ™¯ä¸­ä¹Ÿæœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œä¸”å…·å¤‡è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ã€‚</li>
<li>æ·±å…¥ç ”ç©¶åˆ†æäº†Samba ASRçš„å¯è¡Œæ€§ï¼ŒéªŒè¯äº†å®ƒä½œä¸ºæ— å˜å‹å™¨æ›¿ä»£æ–¹æ¡ˆçš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-beff2ade285eb90e0d49e772856f854f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Efficient-Long-Speech-Sequence-Modelling-for-Time-Domain-Depression-Level-Estimation"><a href="#Efficient-Long-Speech-Sequence-Modelling-for-Time-Domain-Depression-Level-Estimation" class="headerlink" title="Efficient Long Speech Sequence Modelling for Time-Domain Depression   Level Estimation"></a>Efficient Long Speech Sequence Modelling for Time-Domain Depression   Level Estimation</h2><p><strong>Authors:Shuanglin Li, Zhijie Xie, Syed Mohsen Naqvi</strong></p>
<p>Depression significantly affects emotions, thoughts, and daily activities. Recent research indicates that speech signals contain vital cues about depression, sparking interest in audio-based deep-learning methods for estimating its severity. However, most methods rely on time-frequency representations of speech which have recently been criticized for their limitations due to the loss of information when performing time-frequency projections, e.g. Fourier transform, and Mel-scale transformation. Furthermore, segmenting real-world speech into brief intervals risks losing critical interconnections between recordings. Additionally, such an approach may not adequately reflect real-world scenarios, as individuals with depression often pause and slow down in their conversations and interactions. Building on these observations, we present an efficient method for depression level estimation using long speech signals in the time domain. The proposed method leverages a state space model coupled with the dual-path structure-based long sequence modelling module and temporal external attention module to reconstruct and enhance the detection of depression-related cues hidden in the raw audio waveforms. Experimental results on the AVEC2013 and AVEC2014 datasets show promising results in capturing consequential long-sequence depression cues and demonstrate outstanding performance over the state-of-the-art. </p>
<blockquote>
<p>æŠ‘éƒç—‡æ˜¾è‘—å½±å“æƒ…ç»ªã€æ€ç»´å’Œæ—¥å¸¸æ´»åŠ¨ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œè¯­éŸ³ä¿¡å·åŒ…å«å…³äºæŠ‘éƒç—‡çš„é‡è¦çº¿ç´¢ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹åŸºäºéŸ³é¢‘çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ä¼°è®¡å…¶ä¸¥é‡ç¨‹åº¦æ–¹é¢çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºè¯­éŸ³çš„æ—¶é—´-é¢‘ç‡è¡¨ç¤ºï¼Œç”±äºåœ¨è¿›è¡Œæ—¶é—´-é¢‘ç‡æŠ•å½±ï¼ˆä¾‹å¦‚å‚…ç«‹å¶å˜æ¢å’Œæ¢…å°”å°ºåº¦å˜æ¢ï¼‰æ—¶å­˜åœ¨ä¿¡æ¯ä¸¢å¤±è€Œå—åˆ°æ‰¹è¯„ï¼Œè¿™äº›æ—¶é—´é¢‘ç‡è¡¨ç¤ºå­˜åœ¨å±€é™æ€§ã€‚æ­¤å¤–ï¼Œå°†ç°å®ä¸–ç•Œä¸­çš„è¯­éŸ³åˆ†å‰²æˆç®€çŸ­çš„é—´éš”åŒºé—´å¯èƒ½ä¸¢å¤±å…³é”®å½•éŸ³ä¹‹é—´çš„è¿æ¥ã€‚å†è€…ï¼Œç”±äºæŠ‘éƒç—‡æ‚£è€…é€šå¸¸åœ¨å¯¹è¯å’Œäº’åŠ¨ä¸­ä¼šåœé¡¿å¹¶æ”¾æ…¢é€Ÿåº¦ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½æ— æ³•å……åˆ†åæ˜ ç°å®åœºæ™¯ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ—¶åŸŸå†…ä½¿ç”¨é•¿è¯­éŸ³ä¿¡å·è¿›è¡ŒæŠ‘éƒç¨‹åº¦ä¼°è®¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸åŸºäºåŒè·¯å¾„ç»“æ„çš„é•¿æœŸåºåˆ—å»ºæ¨¡æ¨¡å—å’Œä¸´æ—¶å¤–éƒ¨æ³¨æ„åŠ›æ¨¡å—ç›¸ç»“åˆï¼Œé‡å»ºå¹¶æé«˜å¯¹éšè—åœ¨åŸå§‹éŸ³é¢‘æ³¢å½¢ä¸­çš„æŠ‘éƒç›¸å…³çº¿ç´¢çš„æ£€æµ‹ã€‚åœ¨AVEC2013å’ŒAVEC2014æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•æ‰é‡è¦çš„é•¿æœŸæŠ‘éƒçº¿ç´¢æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ï¼Œå¹¶åœ¨æœ€æ–°çš„æŠ€æœ¯ç ”ç©¶ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02512v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æŠ‘éƒç—‡ä¸¥é‡å½±å“æƒ…ç»ªã€æ€ç»´åŠæ—¥å¸¸æ´»åŠ¨ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œè¯­éŸ³ä¿¡å·ä¸­åŒ…å«å…³äºæŠ‘éƒç—‡çš„é‡è¦çº¿ç´¢ï¼Œå¼•å‘éŸ³é¢‘æ·±åº¦å­¦ä¹ æ–¹æ³•ä¼°ç®—å…¶ä¸¥é‡æ€§çš„å…´è¶£ã€‚ä½†å¤šæ•°æ–¹æ³•ä¾èµ–äºè¯­éŸ³çš„æ—¶é—´é¢‘ç‡è¡¨ç¤ºï¼Œæ­¤æ³•å› æŠ•å½±è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±è€Œå—åˆ°æ‰¹è¯„ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ—¶åŸŸä½¿ç”¨é•¿è¯­éŸ³ä¿¡å·è¿›è¡ŒæŠ‘éƒç—‡æ°´å¹³ä¼°è®¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€åŒè·¯å¾„ç»“æ„çš„é•¿åºåˆ—å»ºæ¨¡æ¨¡å—å’Œä¸´æ—¶å¤–éƒ¨æ³¨æ„åŠ›æ¨¡å—ï¼Œé‡å»ºå¹¶å¢å¼ºåŸå§‹éŸ³é¢‘æ³¢å½¢ä¸­éšè—çš„æŠ‘éƒç—‡ç›¸å…³çº¿ç´¢çš„æ£€æµ‹ã€‚åœ¨AVEC2013å’ŒAVEC2014æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•æ‰é•¿æœŸæŠ‘éƒç—‡çº¿ç´¢æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ•ˆæœï¼Œå¹¶åœ¨æœ€æ–°æŠ€æœ¯ä¸Šå±•ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æŠ‘éƒç—‡å¯¹æƒ…ç»ªã€æ€ç»´å’Œæ—¥å¸¸æ´»åŠ¨äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>è¯­éŸ³ä¿¡å·åŒ…å«å…³äºæŠ‘éƒç—‡çš„é‡è¦çº¿ç´¢ï¼Œå¼•å‘éŸ³é¢‘æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å…´è¶£ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¯­éŸ³çš„æ—¶é—´é¢‘ç‡è¡¨ç¤ºï¼Œä½†æ­¤æ³•å­˜åœ¨ä¿¡æ¯ä¸¢å¤±çš„æ‰¹è¯„ã€‚</li>
<li>æå‡ºä¸€ç§åœ¨æ—¶åŸŸä½¿ç”¨é•¿è¯­éŸ³ä¿¡å·çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä»¥å…‹æœä¸Šè¿°é™åˆ¶ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•ç»“åˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€åŒè·¯å¾„ç»“æ„å»ºæ¨¡å’Œä¸´æ—¶å¤–éƒ¨æ³¨æ„åŠ›æ¨¡å—ã€‚</li>
<li>æ–¹æ³•åœ¨æ•æ‰é•¿æœŸæŠ‘éƒç—‡çº¿ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-95c9de2f7a50b74ab7b668cd8983aeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a40ea5393cf8a32516f446f50c4e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4b41843ae691c3a11538e710152f552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-296babf5dc7f55deb974137ef55fb6c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3f5868a0db63d0b422cdb07ecf0b34d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52baf576711aafbd014f1077112d1a5d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding"><a href="#Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding" class="headerlink" title="Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding"></a>Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding</h2><p><strong>Authors:Jiahui Zhao, Hao Shi, Chenrui Cui, Tianrui Wang, Hexin Liu, Zhaoheng Ni, Lingxuan Ye, Longbiao Wang</strong></p>
<p>Code-switching (CS) automatic speech recognition (ASR) faces challenges due to the language confusion resulting from accents, auditory similarity, and seamless language switches. Adaptation on the pre-trained multi-lingual model has shown promising performance for CS-ASR. In this paper, we adapt Whisper, which is a large-scale multilingual pre-trained speech recognition model, to CS from both encoder and decoder parts. First, we propose an encoder refiner to enhance the encoderâ€™s capacity of intra-sentence swithching. Second, we propose using two sets of language-aware adapters with different language prompt embeddings to achieve language-specific decoding information in each decoder layer. Then, a fusion module is added to fuse the language-aware decoding. The experimental results using the SEAME dataset show that, compared with the baseline model, the proposed approach achieves a relative MER reduction of 4.1% and 7.2% on the dev_man and dev_sge test sets, respectively, surpassing state-of-the-art methods. Through experiments, we found that the proposed method significantly improves the performance on non-native language in CS speech, indicating that our approach enables Whisper to better distinguish between the two languages. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢ä¸´ç€ç”±äºå£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å’Œæ— ç¼è¯­è¨€åˆ‡æ¢å¯¼è‡´çš„è¯­è¨€æ··æ·†æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é¢„è®­ç»ƒçš„å¤šå…ƒè¯­è¨€æ¨¡å‹çš„é€‚é…å·²ç»æ˜¾ç¤ºå‡ºå¯¹CS-ASRçš„æœ‰å‰é€”çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€‚é…äº†whisperè¿™ä¸€å¤§è§„æ¨¡çš„å¤šè¯­ç§é¢„è®­ç»ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œç”¨äºå¥å­å†…å’Œå¥å­é—´çš„ä»£ç åˆ‡æ¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¼–ç å™¨ä¼˜åŒ–å™¨ï¼Œä»¥æé«˜ç¼–ç å™¨åœ¨å¥å­å†…éƒ¨åˆ‡æ¢çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸¤ç»„å¸¦æœ‰ä¸åŒè¯­è¨€æç¤ºåµŒå…¥çš„è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨ï¼Œä»¥å®ç°æ¯ä¸ªè§£ç å™¨å±‚ä¸­çš„è¯­è¨€ç‰¹å®šè§£ç ä¿¡æ¯ã€‚ç„¶åï¼Œæ·»åŠ ä¸€ä¸ªèåˆæ¨¡å—æ¥èåˆè¯­è¨€æ„ŸçŸ¥è§£ç ã€‚ä½¿ç”¨SEAMEæ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨dev_manå’Œdev_sgeæµ‹è¯•é›†ä¸Šåˆ†åˆ«å®ç°äº†ç›¸å¯¹è¯¯å·®ç‡ï¼ˆMERï¼‰é™ä½4.1%å’Œ7.2%ï¼Œè¶…è¿‡äº†æœ€æ–°æ–¹æ³•ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯¥æ–¹æ³•åœ¨éæ¯è¯­è¯­è¨€çš„CSè¯­éŸ³æ€§èƒ½ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä½¿whisperèƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†ä¸¤ç§è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16507v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„è¯­è¨€æ··æ·†ã€å£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å’Œæ— ç¼è¯­è¨€åˆ‡æ¢ç­‰é—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œé€‚åº”å¹¶å±•ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹å¤§å‹é¢„è®­ç»ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹Whisperè¿›è¡Œäº†é€‚åº”ï¼Œä»ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤ä¸ªæ–¹é¢è¿›è¡Œæ”¹è¿›ã€‚é€šè¿‡æå‡ºç¼–ç å™¨ç²¾ç‚¼å™¨å¢å¼ºäº†ç¼–ç å™¨å¯¹å¥å­å†…åˆ‡æ¢çš„å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä½¿ç”¨ä¸¤ç»„è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨å’Œä¸åŒçš„è¯­è¨€æç¤ºåµŒå…¥æ¥å®ç°è§£ç å™¨çš„è¯­è¨€ç‰¹å¼‚æ€§ã€‚æ·»åŠ èåˆæ¨¡å—ä»¥èåˆè¯­è¨€æ„ŸçŸ¥çš„è§£ç ç»“æœã€‚åœ¨SEAMEæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨dev_manå’Œdev_sgeæµ‹è¯•é›†ä¸Šç›¸å¯¹å®ç°äº†4.1%å’Œ7.2%çš„å•è¯é”™è¯¯ç‡ï¼ˆMERï¼‰é™ä½ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å®éªŒè¿˜å‘ç°ï¼Œè¯¥æ–¹æ³•å¯¹éæœ¬æ—è¯­çš„CSè¯­éŸ³è¡¨ç°æœ‰æ˜æ˜¾çš„æå‡ï¼Œè¯æ˜è¯¥æ–¹æ³•èƒ½æ›´å¥½åœ°åŒºåˆ†ä¸¤ç§è¯­è¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å¯¼è‡´çš„è¯­è¨€æ··æ·†å’Œæ— ç¼è¯­è¨€åˆ‡æ¢é—®é¢˜ã€‚</li>
<li>é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹é€‚åº”äºCS-ASRæ˜¾ç¤ºå‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ”¹è¿›ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤éƒ¨åˆ†æ¥å¢å¼ºæ¨¡å‹å¤„ç†CSè¯­éŸ³çš„èƒ½åŠ›ã€‚</li>
<li>ç¼–ç å™¨ç²¾ç‚¼å™¨å¢å¼ºäº†ç¼–ç å™¨å¤„ç†å¥å­å†…åˆ‡æ¢çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨å’Œä¸åŒçš„è¯­è¨€æç¤ºåµŒå…¥æ¥å®ç°è§£ç å™¨çš„è¯­è¨€ç‰¹å¼‚æ€§ã€‚</li>
<li>èåˆæ¨¡å—ç”¨äºèåˆä¸åŒè¯­è¨€çš„è§£ç ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-564f5f18dfe3de21d21a61818b4664d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480fe73cba4fd25dd76098c4a3b3909e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b4712943f54b75148744a7a47f2461.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation"><a href="#OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation" class="headerlink" title="OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation"></a>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation</h2><p><strong>Authors:Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu</strong></p>
<p>Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid">https://fudan-generative-vision.github.io/OpenHumanVid</a> </p>
<blockquote>
<p>éšç€è§†è§‰ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œè§†é¢‘æ•°æ®é›†çš„æ•°é‡å’Œå¯ç”¨æ€§æ˜¾è‘—å¢åŠ ï¼Œè¿™å¯¹äºè®­ç»ƒæœ‰æ•ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘æ•°æ®é›†çš„åŒ®ä¹ç»™è¯¥é¢†åŸŸçš„è¿›æ­¥å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenHumanVidï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘æ•°æ®é›†ï¼Œå…¶ç‰¹ç‚¹æ˜¯æ‹¥æœ‰ç²¾ç¡®è¯¦ç»†çš„å­—å¹•ï¼Œæ¶µç›–äº†äººç±»å¤–è§‚å’Œè¿åŠ¨çŠ¶æ€ï¼Œè¿˜åŒ…æ‹¬é¢å¤–çš„äººç±»è¿åŠ¨æ¡ä»¶ï¼Œå¦‚éª¨éª¼åºåˆ—å’Œè¯­éŸ³éŸ³é¢‘ã€‚ä¸ºäº†éªŒè¯è¯¥æ•°æ®é›†å’Œç›¸å…³è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰çš„ç»å…¸æ‰©æ•£å˜å‹å™¨æ¶æ„è¿›è¡Œäº†æ‰©å±•ï¼Œå¹¶åœ¨æ‰€æå‡ºçš„æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº†è¿›ä¸€æ­¥çš„é¢„è®­ç»ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ä¸¤ä¸ªå…³é”®è§è§£ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†å¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆçš„äººç±»è§†é¢‘çš„è¯„ä»·æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼›å…¶æ¬¡ï¼Œæ–‡æœ¬ä¸äººç±»å¤–è§‚ã€äººç±»è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨çš„æœ‰æ•ˆå¯¹é½å¯¹äºç”Ÿæˆé«˜è´¨é‡è§†é¢‘è¾“å‡ºè‡³å…³é‡è¦ã€‚åŸºäºè¿™äº›è§è§£å’Œç›¸åº”çš„æ–¹æ³•è®ºï¼Œåœ¨æ‰€æå‡ºçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç®€å•æ‰©å±•ç½‘ç»œåœ¨äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ”¹è¿›ã€‚é¡¹ç›®é¡µé¢ <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid">https://fudan-generative-vision.github.io/OpenHumanVid</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00115v3">PDF</a> 11 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>éšç€è§†è§‰ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œè§†é¢‘æ•°æ®é›†è§„æ¨¡å’Œå¯ç”¨æ€§çš„æ˜¾è‘—æå‡å¯¹è®­ç»ƒæœ‰æ•ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¼ºä¹é«˜è´¨é‡çš„äººç±»ä¸­å¿ƒè§†é¢‘æ•°æ®é›†æˆä¸ºè¯¥é¢†åŸŸè¿›å±•çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenHumanVidï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„äººç±»ä¸­å¿ƒè§†é¢‘æ•°æ®é›†ï¼Œä»¥ç²¾ç¡®è¯¦ç»†çš„å­—å¹•ä¸ºç‰¹è‰²ï¼Œæ¶µç›–äººç±»å¤–è§‚å’Œè¿åŠ¨çŠ¶æ€ï¼Œä»¥åŠåŒ…æ‹¬éª¨æ¶åºåˆ—å’Œè¯­éŸ³éŸ³é¢‘ç­‰é¢å¤–äººç±»è¿åŠ¨æ¡ä»¶ã€‚ä¸ºäº†éªŒè¯è¯¥æ•°æ®é›†å’Œç›¸å…³è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰çš„ç»å…¸æ‰©æ•£å˜å‹å™¨æ¶æ„è¿›è¡Œäº†æ‰©å±•ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒã€‚ç ”ç©¶å‘ç°ï¼Œå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„å¼•å…¥æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„äººç±»è§†é¢‘çš„è¯„ä»·æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨è§†é¢‘ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ï¼›æ–‡æœ¬ä¸äººç±»å¤–è§‚ã€è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨çš„æœ‰æ•ˆå¯¹é½å¯¹äºç”Ÿæˆé«˜è´¨é‡è§†é¢‘è¾“å‡ºè‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”ŸæˆæŠ€æœ¯å› è§†è§‰ç”ŸæˆæŠ€æœ¯çš„è¿›å±•è€Œå¾—åˆ°æ˜¾è‘—æå‡ï¼Œå¯¹å¤§è§„æ¨¡é«˜è´¨é‡è§†é¢‘æ•°æ®é›†çš„éœ€æ±‚å¢åŠ ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡çš„äººç±»ä¸­å¿ƒè§†é¢‘æ•°æ®é›†æ˜¯å½“å‰çš„æŒ‘æˆ˜ã€‚</li>
<li>OpenHumanVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„äººç±»ä¸­å¿ƒè§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«è¯¦ç»†å­—å¹•ã€äººç±»å¤–è§‚å’Œè¿åŠ¨çŠ¶æ€ä»¥åŠé¢å¤–çš„äººç±»è¿åŠ¨æ¡ä»¶ã€‚</li>
<li>æ•°æ®é›†çš„å¼•å…¥æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„äººç±»è§†é¢‘çš„è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>æ–‡æœ¬ä¸äººç±»å¤–è§‚ã€è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨çš„å¯¹é½æ˜¯ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„å…³é”®ã€‚</li>
<li>é€šè¿‡å¯¹ç°æœ‰ç»å…¸æ‰©æ•£å˜å‹å™¨æ¶æ„çš„æ‰©å±•å’Œåœ¨è¯¥æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒï¼Œæ¨¡å‹åœ¨ç”Ÿæˆäººç±»ä¸­å¿ƒè§†é¢‘æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4eaf2c10d7def6c48f0f7aea62e1ecf4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd9ce755a74992124dd116371eaf2b67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a27d7479c2651f1df22cb1a4aeeea7d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d0076d4045ef34d2154075203f15b62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-310e4cb20848c5672f8d891ccd5da16d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13b1eee368b8ad773fc88f08041e8b6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0c8869ccbd53abd95503fa590524380.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="How-to-Learn-a-New-Language-An-Efficient-Solution-for-Self-Supervised-Learning-Models-Unseen-Languages-Adaption-in-Low-Resource-Scenario"><a href="#How-to-Learn-a-New-Language-An-Efficient-Solution-for-Self-Supervised-Learning-Models-Unseen-Languages-Adaption-in-Low-Resource-Scenario" class="headerlink" title="How to Learn a New Language? An Efficient Solution for Self-Supervised   Learning Models Unseen Languages Adaption in Low-Resource Scenario"></a>How to Learn a New Language? An Efficient Solution for Self-Supervised   Learning Models Unseen Languages Adaption in Low-Resource Scenario</h2><p><strong>Authors:Shih-Heng Wang, Zih-Ching Chen, Jiatong Shi, Ming-To Chuang, Guan-Ting Lin, Kuan-Po Huang, David Harwath, Shang-Wen Li, Hung-yi Lee</strong></p>
<p>The utilization of speech Self-Supervised Learning (SSL) models achieves impressive performance on Automatic Speech Recognition (ASR). However, in low-resource language ASR, they encounter the domain mismatch problem between pre-trained and low-resource languages. Typical solutions like fine-tuning the SSL model suffer from high computation costs while using frozen SSL models as feature extractors comes with poor performance. To handle these issues, we extend a conventional efficient fine-tuning scheme based on the adapter. We add an extra intermediate adaptation to warm up the adapter and downstream model initialization. Remarkably, we update only 1-5% of the total model parameters to achieve the adaptation. Experimental results on the ML-SUPERB dataset show that our solution outperforms conventional efficient fine-tuning. It achieves up to a 28% relative improvement in the Character&#x2F;Phoneme error rate when adapting to unseen languages. </p>
<blockquote>
<p>è¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„åˆ©ç”¨åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨ä½èµ„æºè¯­è¨€ASRä¸­ï¼Œå®ƒä»¬é‡åˆ°äº†é¢„è®­ç»ƒæ¨¡å‹ä¸ä½èµ„æºè¯­è¨€ä¹‹é—´çš„åŸŸä¸åŒ¹é…é—®é¢˜ã€‚å…¸å‹çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚å¾®è°ƒSSLæ¨¡å‹ï¼Œå­˜åœ¨è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œè€Œä½¿ç”¨å†»ç»“çš„SSLæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨åˆ™æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åŸºäºé€‚é…å™¨æ‰©å±•äº†ä¸€ç§å¸¸è§„çš„ç²¾ç»†è°ƒæ•´æ–¹æ¡ˆã€‚æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªé¢å¤–çš„ä¸­é—´é€‚åº”è¿‡ç¨‹æ¥é¢„çƒ­é€‚é…å™¨å’Œä¸‹æ¸¸æ¨¡å‹åˆå§‹åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åªéœ€è¦æ›´æ–°æ¨¡å‹æ€»å‚æ•°çš„1-5%å°±å¯ä»¥å®ç°é€‚é…ã€‚åœ¨ML-SUPERBæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆä¼˜äºä¼ ç»Ÿçš„æœ‰æ•ˆå¾®è°ƒæ–¹æ³•ã€‚åœ¨é€‚åº”æœªè§è¯­è¨€æ—¶ï¼Œå®ƒå®ç°äº†é«˜è¾¾28%çš„ç›¸å¯¹æ”¹è¿›åœ¨å­—ç¬¦&#x2F;éŸ³ç´ é”™è¯¯ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18217v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä½†åœ¨ä½èµ„æºè¯­è¨€ASRä¸­ï¼Œå®ƒä»¬é¢ä¸´é¢„è®­ç»ƒæ¨¡å‹ä¸ä½èµ„æºè¯­è¨€ä¹‹é—´é¢†åŸŸä¸åŒ¹é…çš„é—®é¢˜ã€‚å¸¸è§çš„è§£å†³æ–¹æ¡ˆå¦‚å¾®è°ƒSSLæ¨¡å‹è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œä½¿ç”¨å†»ç»“çš„SSLæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åŸºäºé€‚é…å™¨æ‰©å±•äº†ä¼ ç»Ÿçš„æœ‰æ•ˆå¾®è°ƒæ–¹æ¡ˆï¼Œå¹¶æ·»åŠ äº†é¢å¤–çš„ä¸­é—´é€‚åº”æ¥é¢„çƒ­é€‚é…å™¨å’Œä¸‹æ¸¸æ¨¡å‹åˆå§‹åŒ–ã€‚æˆ‘ä»¬ä»…æ›´æ–°æ¨¡å‹æ€»å‚æ•°çš„1-5%å³å¯å®ç°é€‚åº”ã€‚åœ¨ML-SUPERBæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆä¼˜äºä¼ ç»Ÿçš„æœ‰æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œåœ¨é€‚åº”æœªè§è¯­è¨€æ—¶ï¼Œå­—ç¬¦&#x2F;éŸ³ç´ é”™è¯¯ç‡ç›¸å¯¹æé«˜äº†é«˜è¾¾28%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨ä½èµ„æºè¯­è¨€ASRä¸­ï¼Œå­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸ä½èµ„æºè¯­è¨€ä¹‹é—´çš„é¢†åŸŸä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿçš„SSLæ¨¡å‹å¾®è°ƒæ–¹æ¡ˆè®¡ç®—æˆæœ¬é«˜ï¼Œè€Œä½¿ç”¨å†»ç»“æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨æ•ˆæœæ¬ ä½³ã€‚</li>
<li>é€šè¿‡åŸºäºé€‚é…å™¨çš„æ‰©å±•æ–¹æ¡ˆï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚</li>
<li>é¢å¤–ä¸­é—´é€‚åº”æ­¥éª¤ç”¨äºé¢„çƒ­é€‚é…å™¨å’Œä¸‹æ¸¸æ¨¡å‹åˆå§‹åŒ–ã€‚</li>
<li>ä»…æ›´æ–°å°‘é‡æ¨¡å‹å‚æ•°ï¼ˆ1-5%ï¼‰å³å¯å®ç°é€‚åº”ã€‚</li>
<li>åœ¨ML-SUPERBæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨é€‚åº”æœªè§è¯­è¨€æ—¶ç›¸å¯¹æ”¹è¿›äº†28%çš„å­—ç¬¦&#x2F;éŸ³ç´ é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41bd6330683dee044529af7dfe7f2be4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdf3edf90daa1dc958374233d9c261a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af191bad6b7acffb3d8d7054b446b813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9062a9256e0c062db6231b58d1f4336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4b8eced3e7b0c789b78dd45f57aed4b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Modulating-State-Space-Model-with-SlowFast-Framework-for-Compute-Efficient-Ultra-Low-Latency-Speech-Enhancement"><a href="#Modulating-State-Space-Model-with-SlowFast-Framework-for-Compute-Efficient-Ultra-Low-Latency-Speech-Enhancement" class="headerlink" title="Modulating State Space Model with SlowFast Framework for   Compute-Efficient Ultra Low-Latency Speech Enhancement"></a>Modulating State Space Model with SlowFast Framework for   Compute-Efficient Ultra Low-Latency Speech Enhancement</h2><p><strong>Authors:Longbiao Cheng, Ashutosh Pandey, Buye Xu, Tobi Delbruck, Vamsi Krishna Ithapu, Shih-Chii Liu</strong></p>
<p>Deep learning-based speech enhancement (SE) methods often face significant computational challenges when needing to meet low-latency requirements because of the increased number of frames to be processed. This paper introduces the SlowFast framework which aims to reduce computation costs specifically when low-latency enhancement is needed. The framework consists of a slow branch that analyzes the acoustic environment at a low frame rate, and a fast branch that performs SE in the time domain at the needed higher frame rate to match the required latency. Specifically, the fast branch employs a state space model where its state transition process is dynamically modulated by the slow branch. Experiments on a SE task with a 2 ms algorithmic latency requirement using the Voice Bank + Demand dataset show that our approach reduces computation cost by 70% compared to a baseline single-branch network with equivalent parameters, without compromising enhancement performance. Furthermore, by leveraging the SlowFast framework, we implemented a network that achieves an algorithmic latency of just 62.5 {\mu}s (one sample point at 16 kHz sample rate) with a computation cost of 100 M MACs&#x2F;s, while scoring a PESQ-NB of 3.12 and SISNR of 16.62. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•åœ¨æ»¡è¶³ä½å»¶è¿Ÿè¦æ±‚æ—¶é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦å¤„ç†çš„å¸§æ•°å¢åŠ ã€‚æœ¬æ–‡ä»‹ç»äº†SlowFastæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å‡å°‘åœ¨ä½å»¶è¿Ÿå¢å¼ºæ—¶æ‰€éœ€çš„è®¡ç®—æˆæœ¬ã€‚æ¡†æ¶åŒ…å«ä¸€ä¸ªæ…¢é€Ÿåˆ†æ”¯ï¼Œä»¥ä½å¸§ç‡åˆ†æå£°å­¦ç¯å¢ƒï¼Œä»¥åŠä¸€ä¸ªå¿«é€Ÿåˆ†æ”¯ï¼Œä»¥æ‰€éœ€çš„é«˜å¸§ç‡åœ¨æ—¶åŸŸæ‰§è¡ŒSEï¼Œä»¥åŒ¹é…æ‰€éœ€çš„å»¶è¿Ÿã€‚å…·ä½“æ¥è¯´ï¼Œå¿«é€Ÿåˆ†æ”¯é‡‡ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå…¶çŠ¶æ€è½¬æ¢è¿‡ç¨‹å—åˆ°æ…¢é€Ÿåˆ†æ”¯çš„åŠ¨æ€è°ƒåˆ¶ã€‚åœ¨Voice Bank + Demandæ•°æ®é›†ä¸Šä½¿ç”¨å…·æœ‰2æ¯«ç§’ç®—æ³•å»¶è¿Ÿè¦æ±‚çš„è¯­éŸ³å¢å¼ºä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œä¸å…·æœ‰ç­‰æ•ˆå‚æ•°çš„åŸºçº¿å•åˆ†æ”¯ç½‘ç»œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ–¹é¢é™ä½äº†70%ï¼ŒåŒæ—¶ä¸æŸå®³å¢å¼ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨SlowFastæ¡†æ¶ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç½‘ç»œï¼Œè¯¥ç½‘ç»œè¾¾åˆ°äº†ä»…62.5Î¼sçš„ç®—æ³•å»¶è¿Ÿï¼ˆåœ¨16kHzé‡‡æ ·ç‡ä¸‹çš„ä¸€ä¸ªæ ·æœ¬ç‚¹ï¼‰ï¼Œè®¡ç®—æˆæœ¬ä¸ºæ¯ç§’1äº¿æ¬¡ä¹˜ç´¯åŠ ï¼ˆMACsï¼‰ï¼ŒåŒæ—¶è·å¾—PESQ-NBå¾—åˆ†ä¸º3.12å’ŒSISNRå¾—åˆ†ä¸º16.62ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02019v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ–¹æ³•åœ¨ä½å»¶è¿Ÿè¦æ±‚ä¸‹æ‰€é¢ä¸´çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ­¤å¼•å…¥äº†SlowFastæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªä»¥ä½å¸§ç‡åˆ†æå£°å­¦ç¯å¢ƒçš„æ…¢åˆ†æ”¯ï¼Œä»¥åŠä¸€ä¸ªä»¥æ‰€éœ€çš„é«˜å¸§ç‡è¿›è¡Œæ—¶åŸŸè¯­éŸ³å¢å¼ºçš„å¿«åˆ†æ”¯ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿å•åˆ†æ”¯ç½‘ç»œï¼ŒSlowFastæ¡†æ¶åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¸å¦¥åå¢å¼ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨SlowFastæ¡†æ¶å®ç°çš„ç½‘ç»œåœ¨ç®—æ³•å»¶è¿Ÿä»…ä¸º62.5å¾®ç§’çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—æˆæœ¬ä¸ºæ¯ç§’ç™¾ä¸‡æ¬¡ä¹˜ç´¯åŠ ï¼ˆMACsï¼‰è¿ç®—ï¼Œæ€§èƒ½è¯„ä¼°æŒ‡æ ‡PESQ-NBå’ŒSISNRåˆ†åˆ«è¾¾åˆ°äº†3.12å’Œ16.62ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ–¹æ³•åœ¨ä½å»¶è¿Ÿç¯å¢ƒä¸‹å­˜åœ¨è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>SlowFastæ¡†æ¶æ—¨åœ¨é™ä½ä½å»¶è¿Ÿå¢å¼ºæ—¶çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>SlowFastæ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ…¢åˆ†æ”¯ç”¨äºåˆ†æå£°å­¦ç¯å¢ƒï¼Œä¸€ä¸ªå¿«åˆ†æ”¯ç”¨äºæ—¶åŸŸå¢å¼ºã€‚</li>
<li>å¿«åˆ†æ”¯é‡‡ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå…¶çŠ¶æ€è½¬æ¢è¿‡ç¨‹å—æ…¢åˆ†æ”¯åŠ¨æ€è°ƒåˆ¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿ç½‘ç»œï¼ŒSlowFastæ¡†æ¶åœ¨è®¡ç®—æˆæœ¬é™ä½70%çš„åŒæ—¶ï¼Œä¸å¦¥åå¢å¼ºæ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨SlowFastæ¡†æ¶å®ç°çš„ç½‘ç»œåœ¨ç®—æ³•å»¶è¿Ÿä»…ä¸º62.5å¾®ç§’æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç½‘ç»œçš„è®¡ç®—æˆæœ¬ä¸ºæ¯ç§’ç™¾ä¸‡æ¬¡ä¹˜ç´¯åŠ ï¼ˆMACsï¼‰è¿ç®—ï¼Œæ€§èƒ½è¯„ä¼°æŒ‡æ ‡é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cded0719fe53918baa47a70926bf70d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e4752b3b2d27f0ad4d948b83fe24553.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-332213bba2c502fda0b5fa8a3618b583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2712d5b51b00ea239a609394369a4b3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca10f0ed18861a3b3ec68a60e0d3a13c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LlamaPartialSpoof-An-LLM-Driven-Fake-Speech-Dataset-Simulating-Disinformation-Generation"><a href="#LlamaPartialSpoof-An-LLM-Driven-Fake-Speech-Dataset-Simulating-Disinformation-Generation" class="headerlink" title="LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating   Disinformation Generation"></a>LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating   Disinformation Generation</h2><p><strong>Authors:Hieu-Thi Luong, Haoyang Li, Lin Zhang, Kong Aik Lee, Eng Siong Chng</strong></p>
<p>Previous fake speech datasets were constructed from a defenderâ€™s perspective to develop countermeasure (CM) systems without considering diverse motivations of attackers. To better align with real-life scenarios, we created LlamaPartialSpoof, a 130-hour dataset that contains both fully and partially fake speech, using a large language model (LLM) and voice cloning technologies to evaluate the robustness of CMs. By examining valuable information for both attackers and defenders, we identify several key vulnerabilities in current CM systems, which can be exploited to enhance attack success rates, including biases toward certain text-to-speech models or concatenation methods. Our experimental results indicate that the current fake speech detection system struggle to generalize to unseen scenarios, achieving a best performance of 24.49% equal error rate. </p>
<blockquote>
<p>ä¹‹å‰çš„è™šå‡è¯­éŸ³æ•°æ®é›†æ˜¯ä»é˜²å¾¡è€…çš„è§’åº¦æ„å»ºçš„ï¼Œæ—¨åœ¨å¼€å‘å¯¹æŠ—æªæ–½ï¼ˆCMï¼‰ç³»ç»Ÿï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°æ”»å‡»è€…çš„ä¸åŒåŠ¨æœºã€‚ä¸ºäº†æ›´å¥½åœ°ç¬¦åˆç°å®åœºæ™¯ï¼Œæˆ‘ä»¬åˆ›å»ºäº†LlamaPartialSpoofæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å®Œå…¨å’Œéƒ¨åˆ†è™šå‡è¯­éŸ³çš„130å°æ—¶æ•°æ®é›†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯æ¥è¯„ä¼°å¯¹æŠ—æªæ–½çš„ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹æ”»å‡»è€…å’Œé˜²å¾¡è€…æœ‰ä»·å€¼ä¿¡æ¯çš„è€ƒå¯Ÿï¼Œæˆ‘ä»¬å‘ç°äº†å½“å‰å¯¹æŠ—æªæ–½ç³»ç»Ÿä¸­çš„å‡ ä¸ªå…³é”®æ¼æ´ï¼Œè¿™äº›æ¼æ´å¯ä»¥è¢«åˆ©ç”¨æ¥æé«˜æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬å¯¹æŸäº›æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„åè§æˆ–æ‹¼æ¥æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„è™šå‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿå¾ˆéš¾æ¨å¹¿åˆ°æœªè§è¿‡çš„åœºæ™¯ï¼Œæœ€ä½³æ€§èƒ½ä¸ºè¾¾åˆ°24.49%çš„ç­‰è¯¯ç ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14743v2">PDF</a> 5 pages, ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªåä¸ºLlamaPartialSpoofçš„æ–°å‡è¯­éŸ³æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ç°æœ‰è¯­éŸ³åç¯¡æ”¹ç³»ç»Ÿçš„é²æ£’æ€§ã€‚æ•°æ®é›†åŒ…æ‹¬å®Œå…¨å’Œéƒ¨åˆ†å‡è¯­éŸ³ï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯ç”Ÿæˆã€‚é€šè¿‡åˆ†ææ”»å‡»è€…å’Œé˜²å¾¡è€…çš„æœ‰ä»·å€¼ä¿¡æ¯ï¼Œå‘ç°å½“å‰åç¯¡æ”¹ç³»ç»Ÿçš„å…³é”®æ¼æ´ï¼Œè¿™äº›æ¼æ´å¯è¢«åˆ©ç”¨æé«˜æ”»å‡»æˆåŠŸç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰å‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿåœ¨æ–°åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åä¸ºLlamaPartialSpoofçš„æ–°å‡è¯­éŸ³æ•°æ®é›†ï¼ŒåŒ…å«å®Œå…¨å’Œéƒ¨åˆ†å‡è¯­éŸ³ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯ç”Ÿæˆå‡è¯­éŸ³æ•°æ®ã€‚</li>
<li>é€šè¿‡å¯¹æ•°æ®é›†çš„åˆ†æï¼Œå‘ç°äº†ç°æœ‰è¯­éŸ³åç¯¡æ”¹ç³»ç»Ÿçš„å…³é”®æ¼æ´ã€‚</li>
<li>è¿™äº›æ¼æ´å¯è¢«æ”»å‡»è€…åˆ©ç”¨ä»¥æé«˜æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>å½“å‰å‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿåœ¨æ–°åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„åå‡è¯­éŸ³ç³»ç»Ÿå®ç°äº†24.49%çš„è¯¯åˆ¤ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d8d0e47721d073dcd08751248d6fe4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-080eaa7731b784bb78736c8780ef1aaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b22ec77ff428eea32bca62bba1e0761a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-318cd9b242a3495dc1d0307e3ee8b7fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-dfa3d8c03770e2a1e3d63df40c81942f.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  STAR Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-382898e406c573683e406a518fee98a2.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  Phase-contrast imaging of a dense atomic cloud
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
