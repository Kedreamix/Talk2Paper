<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  Rate-My-LoRA Efficient and Adaptive Federated Model Tuning for Cardiac   MRI Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1645de84bae1baeeeb3d2d6e7a66bded.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-08-æ›´æ–°"><a href="#2025-01-08-æ›´æ–°" class="headerlink" title="2025-01-08 æ›´æ–°"></a>2025-01-08 æ›´æ–°</h1><h2 id="Rate-My-LoRA-Efficient-and-Adaptive-Federated-Model-Tuning-for-Cardiac-MRI-Segmentation"><a href="#Rate-My-LoRA-Efficient-and-Adaptive-Federated-Model-Tuning-for-Cardiac-MRI-Segmentation" class="headerlink" title="Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac   MRI Segmentation"></a>Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac   MRI Segmentation</h2><p><strong>Authors:Xiaoxiao He, Haizhou Shi, Ligong Han, Chaowei Tan, Bo Liu, Zihao Xu, Meng Ye, Leon Axel, Kang Li, Dimitris Metaxas</strong></p>
<p>Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health problems in the United States. Precise cardiac image segmentation is crucial for extracting quantitative measures that help categorize cardiac dyssynchrony. However, achieving high accuracy often depends on centralizing large datasets from different hospitals, which can be challenging due to privacy concerns. To solve this problem, Federated Learning (FL) is proposed to enable decentralized model training on such data without exchanging sensitive information. However, bandwidth limitations and data heterogeneity remain as significant challenges in conventional FL algorithms. In this paper, we propose a novel efficient and adaptive federate learning method for cardiac segmentation that improves model performance while reducing the bandwidth requirement. Our method leverages the low-rank adaptation (LoRA) to regularize model weight update and reduce communication overhead. We also propose a \mymethod{} aggregation technique to address data heterogeneity among clients. This technique adaptively penalizes the aggregated weights from different clients by comparing the validation accuracy in each client, allowing better generalization performance and fast local adaptation. In-client and cross-client evaluations on public cardiac MR datasets demonstrate the superiority of our method over other LoRA-based federate learning approaches. </p>
<blockquote>
<p>å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰å’Œå¿ƒè„å¤±åŒæ­¥æ˜¯ç¾å›½ä¸»è¦çš„å…¬å…±å«ç”Ÿé—®é¢˜ã€‚ç²¾ç¡®çš„å¿ƒè„å›¾åƒåˆ†å‰²å¯¹äºæå–å®šé‡æŒ‡æ ‡ï¼Œå¸®åŠ©åˆ†ç±»å¿ƒè„å¤±åŒæ­¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå®ç°é«˜ç²¾ç¡®åº¦é€šå¸¸å–å†³äºä»ä¸åŒåŒ»é™¢é›†ä¸­å¤§é‡æ•°æ®é›†ï¼Œè¿™å¯èƒ½ä¼šå› éšç§æ‹…å¿§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ï¼Œèƒ½å¤Ÿåœ¨ä¸äº¤æ¢æ•æ„Ÿä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¯¹è¿™ç±»æ•°æ®è¿›è¡Œåˆ†æ•£å¼æ¨¡å‹è®­ç»ƒã€‚ç„¶è€Œï¼Œå¸¦å®½é™åˆ¶å’Œæ•°æ®å¼‚è´¨æ€§ä»æ˜¯ä¼ ç»Ÿè”é‚¦å­¦ä¹ ç®—æ³•ä¸­çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ•ˆè‡ªé€‚åº”è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¿ƒè„åˆ†å‰²ï¼Œåœ¨é™ä½å¸¦å®½è¦æ±‚çš„åŒæ—¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¥è§„èŒƒæ¨¡å‹æƒé‡æ›´æ–°å¹¶å‡å°‘é€šä¿¡å¼€é”€ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åä¸ºã€æˆ‘çš„æ–¹æ³•ã€‘çš„èšåˆæŠ€æœ¯æ¥è§£å†³å®¢æˆ·ç«¯ä¹‹é—´çš„æ•°æ®å¼‚è´¨æ€§ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ¯”è¾ƒæ¯ä¸ªå®¢æˆ·ç«¯çš„éªŒè¯ç²¾åº¦æ¥è‡ªé€‚åº”åœ°æƒ©ç½šæ¥è‡ªä¸åŒå®¢æˆ·ç«¯çš„èšåˆæƒé‡ï¼Œä»è€Œå®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½å’Œå¿«é€Ÿæœ¬åœ°é€‚åº”ã€‚åœ¨å…¬å…±å¿ƒè„MRæ•°æ®é›†ä¸Šçš„å®¢æˆ·ç«¯å’Œè·¨å®¢æˆ·ç«¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–åŸºäºLoRAçš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03223v1">PDF</a> Accepted in ISBI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„é«˜æ•ˆè‡ªé€‚åº”è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¿ƒè„å›¾åƒåˆ†å‰²ï¼Œåœ¨ä¸éœ€è¦äº¤æ¢æ•æ„Ÿä¿¡æ¯çš„å‰æä¸‹ï¼Œè§£å†³åˆ†æ•£æ•°æ®é›†ä¸­å¿ƒè„ç–¾ç—…çš„æ¨¡å‹è®­ç»ƒé—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯æ¥ä¼˜åŒ–æ¨¡å‹æƒé‡æ›´æ–°å¹¶å‡å°‘é€šä¿¡å¼€é”€ï¼Œå¹¶æå‡ºä¸€ç§åä¸º\mymethod{}çš„èšåˆæŠ€æœ¯æ¥è§£å†³å®¢æˆ·é—´æ•°æ®å¼‚è´¨æ€§é—®é¢˜ã€‚åœ¨å…¬å…±å¿ƒè„MRæ•°æ®é›†ä¸Šçš„å†…éƒ¨å’Œå¤–éƒ¨è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–åŸºäºLoRAçš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè¡€ç®¡ç–¾ç—…å’Œå¿ƒè„å¤±åŒæ­¥æ˜¯ç¾å›½ä¸»è¦çš„å…¬å…±å«ç”Ÿé—®é¢˜ï¼Œç²¾ç¡®çš„å¿ƒè„å›¾åƒåˆ†å‰²å¯¹äºæå–é‡åŒ–æŒ‡æ ‡ä»¥åˆ†ç±»å¿ƒè„å¤±åŒæ­¥è‡³å…³é‡è¦ã€‚</li>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯è§£å†³åˆ†æ•£æ•°æ®é›†ä¸­å¿ƒè„ç–¾ç—…æ¨¡å‹è®­ç»ƒçš„æœ‰æ•ˆæ–¹æ³•ï¼Œæ— éœ€äº¤æ¢æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>å¸¦å®½é™åˆ¶å’Œæ•°æ®å¼‚è´¨æ€§æ˜¯è”é‚¦å­¦ä¹ ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹é«˜æ•ˆè‡ªé€‚åº”è”é‚¦å­¦ä¹ æ–¹æ³•ç”¨äºå¿ƒè„åˆ†å‰²ï¼Œåˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ä¼˜åŒ–æ¨¡å‹æƒé‡æ›´æ–°å¹¶å‡å°‘é€šä¿¡å¼€é”€ã€‚</li>
<li>æå‡ºä¸€ç§åä¸º\mymethod{}çš„èšåˆæŠ€æœ¯æ¥è§£å†³å®¢æˆ·é—´æ•°æ®å¼‚è´¨æ€§é—®é¢˜ï¼Œå®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½å’Œå¿«é€Ÿæœ¬åœ°é€‚åº”ã€‚</li>
<li>åœ¨å…¬å…±å¿ƒè„MRæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–åŸºäºLoRAçš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f77cac9cdcc9867d06b39e15299b3552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4028cfc48a3f4a703be7c97cc12e82de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4ac0fd8116418cb1055205342862048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aab4b755f428bbe3dcffc3f16e15854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccbf5da86a605aa5738fb478f0f96677.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e851f729f5436180ff38e2984b1d4875.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Statistical-Reconstruction-For-Anisotropic-X-ray-Dark-Field-Tomography"><a href="#Statistical-Reconstruction-For-Anisotropic-X-ray-Dark-Field-Tomography" class="headerlink" title="Statistical Reconstruction For Anisotropic X-ray Dark-Field Tomography"></a>Statistical Reconstruction For Anisotropic X-ray Dark-Field Tomography</h2><p><strong>Authors:David Frank, Cederik HÃ¶fs, Tobias Lasser</strong></p>
<p>Anisotropic X-ray Dark-Field Tomography (AXDT) is a novel imaging technology that enables the extraction of fiber structures on the micrometer scale, far smaller than standard X-ray Computed Tomography (CT) setups. Directional and structural information is relevant in medical diagnostics and material testing. Compared to existing solutions, AXDT could prove a viable alternative. Reconstruction methods in AXDT have so far been driven by practicality. Improved methods could make AXDT more accessible. We contribute numerically stable implementations and validation of advanced statistical reconstruction methods that incorporate the statistical noise behavior of the imaging system. We further provide a new statistical reconstruction formulation that retains the advanced noise assumptions of the imaging setup while being efficient and easy to optimize. Finally, we provide a detailed analysis of the optimization behavior for all models regarding AXDT. Our experiments show that statistical reconstruction outperforms the previously used model, and particularly the noise performance is superior. While the previously proposed statistical method is effective, it is computationally expensive, and our newly proposed formulation proves highly efficient with identical performance. Our theoretical analysis opens the possibility to new and more advanced reconstruction algorithms, which in turn enable future research in AXDT. </p>
<blockquote>
<p>åŸºäºæ”¾å°„å­¦çš„æš—åœºå±‚ææˆåƒæœ¯ï¼ˆAXDTï¼‰æ˜¯ä¸€ç§æ–°å…´æˆåƒæŠ€æœ¯ï¼Œèƒ½åœ¨å¾®ç±³å°ºåº¦ä¸Šæå–çº¤ç»´ç»“æ„ï¼Œè¿œå°äºæ ‡å‡†æ”¾å°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰è£…ç½®ã€‚æ–¹å‘æ€§å’Œç»“æ„ä¿¡æ¯åœ¨åŒ»å­¦è¯Šæ–­å’Œææ–™æµ‹è¯•ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒAXDTå¯èƒ½æˆä¸ºå¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚è¿„ä»Šä¸ºæ­¢ï¼ŒAXDTçš„é‡å»ºæ–¹æ³•ä¸€ç›´å—åˆ°å®ç”¨æ€§çš„é©±åŠ¨ã€‚æ”¹è¿›çš„æ–¹æ³•å¯ä»¥ä½¿AXDTæ›´åŠ æ˜“äºè·å–ã€‚æˆ‘ä»¬ä¸ºå…ˆè¿›çš„ç»Ÿè®¡é‡å»ºæ–¹æ³•æä¾›äº†æ•°å€¼ç¨³å®šçš„å®ç°å’ŒéªŒè¯ï¼Œè¿™äº›æ–¹æ³•ç»“åˆäº†æˆåƒç³»ç»Ÿçš„ç»Ÿè®¡å™ªå£°è¡Œä¸ºã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç§æ–°çš„ç»Ÿè®¡é‡å»ºå…¬å¼ï¼Œå®ƒåœ¨ä¿ç•™æˆåƒè£…ç½®çš„é«˜çº§å™ªå£°å‡è®¾çš„åŒæ—¶ï¼Œè¿˜å…·æœ‰é«˜æ•ˆä¸”æ˜“äºä¼˜åŒ–çš„ç‰¹ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å…³äºAXDTçš„æ¨¡å‹ä¼˜åŒ–è¡Œä¸ºè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œç»Ÿè®¡é‡å»ºæ–¹æ³•ä¼˜äºä»¥å‰ä½¿ç”¨çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°æ€§èƒ½æ–¹é¢è¡¨ç°æ›´ä¼˜è¶Šã€‚è™½ç„¶ä¹‹å‰æå‡ºçš„ç»Ÿè®¡æ–¹æ³•æœ‰æ•ˆï¼Œä½†å…¶è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œè€Œæˆ‘ä»¬æ–°æå‡ºçš„å…¬å¼åœ¨æ€§èƒ½ç›¸åŒçš„æƒ…å†µä¸‹å…·æœ‰é«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æä¸ºæ–°çš„æ›´å…ˆè¿›çš„é‡å»ºç®—æ³•æ‰“å¼€äº†å¯èƒ½æ€§ï¼Œè¿™å°†ä¸ºAXDTçš„æœªæ¥ç ”ç©¶æä¾›å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AXDTï¼ˆå„å‘å¼‚æ€§Xå°„çº¿æš—åœºå±‚ææˆåƒæŠ€æœ¯ï¼‰æ˜¯ä¸€ç§æ–°å…´æˆåƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å¾®ç±³å°ºåº¦ä¸Šæå–çº¤ç»´ç»“æ„ï¼Œè¿œå°äºæ ‡å‡†Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰è®¾å¤‡ã€‚è¯¥æŠ€æœ¯å¯¹äºåŒ»å­¦è¯Šæ–­å’Œææ–™æµ‹è¯•ä¸­çš„æ–¹å‘å’Œç»“æ„ä¿¡æ¯è‡³å…³é‡è¦ã€‚ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒAXDTå¯èƒ½æ˜¯ä¸€ç§å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡ä»‹ç»äº†AXDTçš„é‡å»ºæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä»¥å®ç”¨æ€§ä¸ºå¯¼å‘ï¼Œä½†æ”¹è¿›çš„æ–¹æ³•å¯ä»¥ä½¿AXDTæ›´åŠ æ™®åŠã€‚æœ¬æ–‡æä¾›äº†æ•°å€¼ç¨³å®šçš„å®ç°å’Œå¯¹ç»“åˆäº†æˆåƒç³»ç»Ÿå™ªå£°è¡Œä¸ºçš„å…ˆè¿›ç»Ÿè®¡é‡å»ºæ–¹æ³•çš„éªŒè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§æ–°çš„ç»Ÿè®¡é‡å»ºå…¬å¼ï¼Œåœ¨ä¿ç•™æˆåƒè®¾ç½®çš„å…ˆè¿›å™ªå£°å‡è®¾çš„åŒæ—¶ï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œæ˜“äºä¼˜åŒ–çš„ç‰¹ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å…³äºAXDTçš„æ¨¡å‹çš„ä¼˜åŒ–è¡Œä¸ºè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œç»Ÿè®¡é‡å»ºä¼˜äºä»¥å‰ä½¿ç”¨çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å™ªå£°æ€§èƒ½æ›´ä¸ºä¼˜è¶Šã€‚è™½ç„¶ä¹‹å‰æå‡ºçš„ç»Ÿè®¡æ–¹æ³•æœ‰æ•ˆï¼Œä½†è®¡ç®—é‡å¤§ï¼Œæˆ‘ä»¬æ–°æå‡ºçš„å…¬å¼æ•ˆç‡é«˜ä¸”æ€§èƒ½ç›¸åŒã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æå¯èƒ½ä¸ºæ–°çš„å’Œæ›´å…ˆè¿›çš„é‡å»ºç®—æ³•æ‰“å¼€å¯èƒ½æ€§ï¼Œè¿™åè¿‡æ¥åˆä¸ºAXDTçš„æœªæ¥ç ”ç©¶æä¾›äº†æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AXDTæ˜¯ä¸€ç§æ–°å…´æˆåƒæŠ€æœ¯ï¼Œå¯æå–å¾®ç±³å°ºåº¦çš„çº¤ç»´ç»“æ„ä¿¡æ¯ã€‚</li>
<li>AXDTæŠ€æœ¯å¯¹äºåŒ»å­¦è¯Šæ–­å’Œææ–™æµ‹è¯•å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
<li>ä¸ç°æœ‰æˆåƒæŠ€æœ¯ç›¸æ¯”ï¼ŒAXDTå…·å¤‡æ›¿ä»£æ½œåŠ›ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†å…ˆè¿›çš„ç»Ÿè®¡é‡å»ºæ–¹æ³•å’Œæ•°å€¼ç¨³å®šçš„å®ç°éªŒè¯ã€‚</li>
<li>æ–°æå‡ºçš„ç»Ÿè®¡é‡å»ºå…¬å¼æ—¢é«˜æ•ˆåˆæ˜“äºä¼˜åŒ–ï¼ŒåŒæ—¶è€ƒè™‘äº†æˆåƒç³»ç»Ÿçš„å™ªå£°è¡Œä¸ºã€‚</li>
<li>ç»Ÿè®¡é‡å»ºæ–¹æ³•æ˜¾è‘—ä¼˜äºä¹‹å‰ä½¿ç”¨çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°æ€§èƒ½æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-403373c68e4adba113e19145d76cd492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793b7b8c3d0620696fb6d54676541f47.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AIF-SFDA-Autonomous-Information-Filter-driven-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#AIF-SFDA-Autonomous-Information-Filter-driven-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain   Adaptation for Medical Image Segmentation"></a>AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain   Adaptation for Medical Image Segmentation</h2><p><strong>Authors:Haojin Li, Heng Li, Jianyu Chen, Rihan Zhong, Ke Niu, Huazhu Fu, Jiang Liu</strong></p>
<p>Decoupling domain-variant information (DVI) from domain-invariant information (DII) serves as a prominent strategy for mitigating domain shifts in the practical implementation of deep learning algorithms. However, in medical settings, concerns surrounding data collection and privacy often restrict access to both training and test data, hindering the empirical decoupling of information by existing methods. To tackle this issue, we propose an Autonomous Information Filter-driven Source-free Domain Adaptation (AIF-SFDA) algorithm, which leverages a frequency-based learnable information filter to autonomously decouple DVI and DII. Information Bottleneck (IB) and Self-supervision (SS) are incorporated to optimize the learnable frequency filter. The IB governs the information flow within the filter to diminish redundant DVI, while SS preserves DII in alignment with the specific task and image modality. Thus, the autonomous information filter can overcome domain shifts relying solely on target data. A series of experiments covering various medical image modalities and segmentation tasks were conducted to demonstrate the benefits of AIF-SFDA through comparisons with leading algorithms and ablation studies. The code is available at <a target="_blank" rel="noopener" href="https://github.com/JingHuaMan/AIF-SFDA">https://github.com/JingHuaMan/AIF-SFDA</a>. </p>
<blockquote>
<p>å°†é¢†åŸŸå˜ä½“ä¿¡æ¯ï¼ˆDVIï¼‰ä»é¢†åŸŸä¸å˜ä¿¡æ¯ï¼ˆDIIï¼‰ä¸­è§£è€¦ï¼Œæ˜¯ç¼“è§£æ·±åº¦å­¦ä¹ ç®—æ³•å®é™…åº”ç”¨ä¸­é¢†åŸŸåç§»é—®é¢˜çš„çªå‡ºç­–ç•¥ã€‚ç„¶è€Œï¼Œåœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œå…³äºæ•°æ®æ”¶é›†å’Œéšç§çš„æ‹…å¿§å¸¸å¸¸é™åˆ¶äº†å¯¹è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„è®¿é—®ï¼Œé˜»ç¢äº†ç°æœ‰æ–¹æ³•è¿›è¡Œä¿¡æ¯çš„å®è¯è§£è€¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªä¸»ä¿¡æ¯æ»¤æ³¢å™¨çš„æ— æºåŸŸè‡ªé€‚åº”ï¼ˆAIF-SFDAï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨åŸºäºé¢‘ç‡çš„å¯å­¦ä¹ ä¿¡æ¯æ»¤æ³¢å™¨æ¥è‡ªä¸»è§£è€¦DVIå’ŒDIIã€‚ä¿¡æ¯ç“¶é¢ˆï¼ˆIBï¼‰å’Œè‡ªæˆ‘ç›‘ç£ï¼ˆSSï¼‰è¢«ç”¨æ¥ä¼˜åŒ–å¯å­¦ä¹ çš„é¢‘ç‡æ»¤æ³¢å™¨ã€‚IBæ§åˆ¶æ»¤æ³¢å™¨å†…çš„ä¿¡æ¯æµï¼Œä»¥å‡å°‘å†—ä½™çš„DVIï¼Œè€ŒSSåˆ™ä¿ç•™ä¸ç‰¹å®šä»»åŠ¡å’Œå›¾åƒæ¨¡æ€ç›¸ç¬¦çš„DIIã€‚å› æ­¤ï¼Œè‡ªä¸»ä¿¡æ¯æ»¤æ³¢å™¨å¯ä»¥ä»…ä¾èµ–ç›®æ ‡æ•°æ®å…‹æœé¢†åŸŸåç§»é—®é¢˜ã€‚è¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼Œæ¶µç›–å¤šç§åŒ»å­¦å›¾åƒæ¨¡æ€å’Œåˆ†å‰²ä»»åŠ¡ï¼Œé€šè¿‡ä¸å…¶ä»–é¢†å…ˆç®—æ³•çš„æ¯”è¾ƒå’Œæ¶ˆèç ”ç©¶ï¼Œè¯æ˜äº†AIF-SFDAçš„ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JingHuaMan/AIF-SFDA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JingHuaMan/AIF-SFDAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03074v1">PDF</a> 9 pages total (7 pages main text, 2 pages references), 6 figures,   accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦é¢†åŸŸåœ¨æ·±åº¦å­¦ä¹ ä¸­å­˜åœ¨é¢†åŸŸåç§»é—®é¢˜ï¼Œä¸ºæ­¤ç°æœ‰æ–¹æ³•å°è¯•å°†é¢†åŸŸå˜é‡ä¿¡æ¯ï¼ˆDVIï¼‰ä¸é¢†åŸŸä¸å˜ä¿¡æ¯ï¼ˆDIIï¼‰åˆ†ç¦»ã€‚ç„¶è€Œï¼Œåœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œæ•°æ®æ”¶é›†å’Œéšç§æ‹…å¿§é™åˆ¶äº†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„è®¿é—®ï¼Œé˜»ç¢äº†ç°æœ‰æ–¹æ³•çš„å®è¯åˆ†ç¦»æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªä¸»ä¿¡æ¯è¿‡æ»¤å™¨çš„æ— æºåŸŸé€‚åº”ï¼ˆAIF-SFDAï¼‰ç®—æ³•ï¼Œåˆ©ç”¨åŸºäºé¢‘ç‡çš„å¯å­¦ä¹ ä¿¡æ¯è¿‡æ»¤å™¨è‡ªä¸»åˆ†ç¦»DVIå’ŒDIIã€‚ç»“åˆä¿¡æ¯ç“¶é¢ˆï¼ˆIBï¼‰å’Œè‡ªæˆ‘ç›‘ç£ï¼ˆSSï¼‰ä¼˜åŒ–è¯¥è¿‡æ»¤å™¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•èƒ½æœ‰æ•ˆåº”å¯¹ä¸åŒåŒ»å­¦å›¾åƒæ¨¡æ€å’Œåˆ†å‰²ä»»åŠ¡çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œä¸é¢†å…ˆç®—æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¢†åŸŸåç§»åœ¨åŒ»å­¦æ·±åº¦å­¦ä¹ ä¸­æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾åˆ†ç¦»é¢†åŸŸå˜é‡ä¿¡æ¯ï¼ˆDVIï¼‰å’Œé¢†åŸŸä¸å˜ä¿¡æ¯ï¼ˆDIIï¼‰ã€‚</li>
<li>åœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œæ•°æ®æ”¶é›†å’Œéšç§é™åˆ¶å¯¹å®æ–½è¿™äº›æ–¹æ³•æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„AIF-SFDAç®—æ³•ä½¿ç”¨è‡ªä¸»ä¿¡æ¯è¿‡æ»¤å™¨æ¥åˆ†ç¦»DVIå’ŒDIIã€‚</li>
<li>ä¿¡æ¯ç“¶é¢ˆï¼ˆIBï¼‰å’Œè‡ªæˆ‘ç›‘ç£ï¼ˆSSï¼‰ç”¨äºä¼˜åŒ–è¯¥è¿‡æ»¤å™¨çš„æ€§èƒ½ã€‚</li>
<li>AIF-SFDAç®—æ³•é€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯æ˜å…¶åœ¨ä¸åŒåŒ»å­¦å›¾åƒæ¨¡æ€å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥ç®—æ³•çš„ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-22e41d116401ea8235b5d00b548cfe95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-268f0e0fbd948c1805d89eb8e4450b01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05b50d5f5001a09eac9d0ac3330b64a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7eeea6a4c7aaf0dee9dbad5dd19942.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df0ec98b50f89fbf5a2774bfd5da66a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d46ef6a632c34056c202fb4851abcca4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Inverse-receptive-field-attention-for-naturalistic-image-reconstruction-from-the-brain"><a href="#Inverse-receptive-field-attention-for-naturalistic-image-reconstruction-from-the-brain" class="headerlink" title="Inverse receptive field attention for naturalistic image reconstruction   from the brain"></a>Inverse receptive field attention for naturalistic image reconstruction   from the brain</h2><p><strong>Authors:Lynn Le, Thirza Dado, Katja Seeliger, Paolo Papale, Antonio Lozano, Pieter Roelfsema, YaÄŸmur GÃ¼Ã§lÃ¼tÃ¼rk, Marcel van Gerven, Umut GÃ¼Ã§lÃ¼</strong></p>
<p>Visual perception in the brain largely depends on the organization of neuronal receptive fields. Although extensive research has delineated the coding principles of receptive fields, most studies have been constrained by their foundational assumptions. Moreover, while machine learning has successfully been used to reconstruct images from brain data, this approach faces significant challenges, including inherent feature biases in the model and the complexities of brain structure and function. In this study, we introduce an inverse receptive field attention (IRFA) model, designed to reconstruct naturalistic images from neurophysiological data in an end-to-end fashion. This approach aims to elucidate the tuning properties and representational transformations within the visual cortex. The IRFA model incorporates an attention mechanism that determines the inverse receptive field for each pixel, weighting neuronal responses across the visual field and feature spaces. This method allows for an examination of the dynamics of neuronal representations across stimuli in both spatial and feature dimensions. Our results show highly accurate reconstructions of naturalistic data, independent of pre-trained models. Notably, IRF models trained on macaque V1, V4, and IT regions yield remarkably consistent spatial receptive fields across different stimuli, while the features to which neuronal representations are selective exhibit significant variation. Additionally, we propose a data-driven method to explore representational clustering within various visual areas, further providing testable hypotheses. </p>
<blockquote>
<p>å¤§è„‘ä¸­çš„è§†è§‰æ„ŸçŸ¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºç¥ç»å…ƒæ„Ÿå—é‡çš„ç»„ç»‡ã€‚å°½ç®¡å·²ç»æœ‰å¾ˆå¤šç ”ç©¶é˜è¿°äº†æ„Ÿå—é‡çš„ç¼–ç åŸåˆ™ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶éƒ½å—åˆ°äº†å…¶åŸºç¡€å‡è®¾çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè™½ç„¶æœºå™¨å­¦ä¹ å·²æˆåŠŸç”¨äºä»è„‘æ•°æ®ä¸­é‡å»ºå›¾åƒï¼Œä½†è¿™ç§æ–¹æ³•é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹ä¸­çš„å›ºæœ‰ç‰¹å¾åè§å’Œè„‘ç»“æ„å’ŒåŠŸèƒ½çš„å¤æ‚æ€§ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€†å‘æ„Ÿå—é‡æ³¨æ„åŠ›ï¼ˆIRFAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä»ç¥ç»ç”Ÿç†æ•°æ®ä¸­é‡å»ºè‡ªç„¶å›¾åƒã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é˜æ˜è§†è§‰çš®å±‚å†…çš„è°ƒèŠ‚å±æ€§å’Œä»£è¡¨æ€§è½¬åŒ–ã€‚IRFAæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºç¡®å®šæ¯ä¸ªåƒç´ çš„é€†å‘æ„Ÿå—é‡ï¼Œå¯¹è§†è§‰åœºå’Œç‰¹å¾ç©ºé—´ä¸­çš„ç¥ç»å…ƒå“åº”è¿›è¡ŒåŠ æƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç ”ç©¶åˆºæ¿€åœ¨ç©ºé—´å’Œç‰¹å¾ç»´åº¦ä¸Šç¥ç»å…ƒè¡¨ç¤ºçš„åŠ¨åŠ›å­¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œéƒ½èƒ½é«˜åº¦å‡†ç¡®åœ°é‡å»ºè‡ªç„¶æ•°æ®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨çŒ•çŒ´V1ã€V4å’ŒITåŒºåŸŸè®­ç»ƒçš„IRFæ¨¡å‹åœ¨ä¸åŒåˆºæ¿€ä¸‹äº§ç”Ÿéå¸¸ä¸€è‡´çš„ç©ºé—´æ„Ÿå—é‡ï¼Œè€Œç¥ç»å…ƒè¡¨ç¤ºæ‰€é€‰æ‹©çš„ç‰¹å¾åˆ™è¡¨ç°å‡ºæ˜¾è‘—çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥æ¢ç´¢å„ç§è§†è§‰åŒºåŸŸå†…çš„ä»£è¡¨æ€§èšç±»ï¼Œä¸ºè¿›ä¸€æ­¥æä¾›å¯æµ‹è¯•å‡è®¾æä¾›äº†ä¾æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03051v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºé€†æ„Ÿå—é‡æ³¨æ„åŠ›ï¼ˆIRFAï¼‰çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä»ç¥ç»ç”Ÿç†æ•°æ®ä¸­é‡å»ºè‡ªç„¶å›¾åƒï¼Œä»¥é˜æ˜è§†è§‰çš®å±‚çš„è°ƒèŠ‚ç‰¹æ€§å’Œè¡¨å¾è½¬æ¢ã€‚IRFAæ¨¡å‹å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®å®šæ¯ä¸ªåƒç´ çš„é€†æ„Ÿå—é‡ï¼Œå¯¹è§†è§‰åœºå’Œç‰¹å¾ç©ºé—´çš„ç¥ç»å…ƒååº”è¿›è¡ŒåŠ æƒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç ”ç©¶ç¥ç»å…ƒè¡¨å¾åœ¨åˆºæ¿€ã€ç©ºé—´å’Œç‰¹å¾ç»´åº¦ä¸Šçš„åŠ¨æ€å˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»å…ƒæ„Ÿå—é‡çš„ç»„ç»‡å¯¹å¤§è„‘ä¸­çš„è§†è§‰æ„ŸçŸ¥è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„ç ”ç©¶åœ¨ç†è§£æ„Ÿå—é‡ç¼–ç åŸåˆ™æ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†ä»å—åˆ°åŸºç¡€å‡è®¾çš„é™åˆ¶ã€‚</li>
<li>æœºå™¨å­¦ä¹ å·²æˆåŠŸç”¨äºä»è„‘æ•°æ®ä¸­é‡å»ºå›¾åƒï¼Œä½†é¢ä¸´æ¨¡å‹å›ºæœ‰ç‰¹å¾åè§å’Œå¤§è„‘ç»“æ„åŠŸèƒ½å¤æ‚æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„é€†æ„Ÿå—é‡æ³¨æ„åŠ›ï¼ˆIRFAï¼‰æ¨¡å‹æ—¨åœ¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä»ç¥ç»ç”Ÿç†æ•°æ®ä¸­é‡å»ºè‡ªç„¶å›¾åƒã€‚</li>
<li>IRFAæ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç¡®å®šæ¯ä¸ªåƒç´ çš„é€†æ„Ÿå—é‡ï¼Œè€ƒè™‘è§†è§‰åœºå’Œç‰¹å¾ç©ºé—´çš„ç¥ç»å…ƒååº”ã€‚</li>
<li>IRFAæ¨¡å‹èƒ½å¤Ÿåœ¨ç©ºé—´å’Œç‰¹å¾ç»´åº¦ä¸Šç ”ç©¶ç¥ç»å…ƒè¡¨å¾çš„åŠ¨æ€å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53cebee6f8ea8c3ef5f9b7c9f84692f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6450fb5b1641d654b2f2eee172b06992.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-759e968bb4af22257fa73af03ab283e0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GLFC-Unified-Global-Local-Feature-and-Contrast-Learning-with-Mamba-Enhanced-UNet-for-Synthetic-CT-Generation-from-CBCT"><a href="#GLFC-Unified-Global-Local-Feature-and-Contrast-Learning-with-Mamba-Enhanced-UNet-for-Synthetic-CT-Generation-from-CBCT" class="headerlink" title="GLFC: Unified Global-Local Feature and Contrast Learning with   Mamba-Enhanced UNet for Synthetic CT Generation from CBCT"></a>GLFC: Unified Global-Local Feature and Contrast Learning with   Mamba-Enhanced UNet for Synthetic CT Generation from CBCT</h2><p><strong>Authors:Xianhao Zhou, Jianghao Wu, Huangxuan Zhao, Lei Chen, Shaoting Zhang, Guotai Wang, Guotai Wang</strong></p>
<p>Generating synthetic Computed Tomography (CT) images from Cone Beam Computed Tomography (CBCT) is desirable for improving the image quality of CBCT. Existing synthetic CT (sCT) generation methods using Convolutional Neural Networks (CNN) and Transformers often face difficulties in effectively capturing both global and local features and contrasts for high-quality sCT generation. In this work, we propose a Global-Local Feature and Contrast learning (GLFC) framework for sCT generation. First, a Mamba-Enhanced UNet (MEUNet) is introduced by integrating Mamba blocks into the skip connections of a high-resolution UNet for effective global and local feature learning. Second, we propose a Multiple Contrast Loss (MCL) that calculates synthetic loss at different intensity windows to improve quality for both soft tissues and bone regions. Experiments on the SynthRAD2023 dataset demonstrate that GLFC improved the SSIM of sCT from 77.91% to 91.50% compared with the original CBCT, and significantly outperformed several existing methods for sCT generation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/intelland/GLFC">https://github.com/intelland/GLFC</a> </p>
<blockquote>
<p>ä»é”¥æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰ç”Ÿæˆåˆæˆè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒæ˜¯æé«˜CBCTå›¾åƒè´¨é‡çš„ç†æƒ³é€‰æ‹©ã€‚ç°æœ‰çš„ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒTransformerçš„åˆæˆCTï¼ˆsCTï¼‰ç”Ÿæˆæ–¹æ³•é€šå¸¸éš¾ä»¥æœ‰æ•ˆåœ°æ•è·å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ä»¥åŠå¯¹æ¯”åº¦ï¼Œä»¥å®ç°é«˜è´¨é‡çš„sCTç”Ÿæˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºsCTç”Ÿæˆçš„Global-Localç‰¹å¾å’Œå¯¹æ¯”åº¦å­¦ä¹ ï¼ˆGLFCï¼‰æ¡†æ¶ã€‚é¦–å…ˆï¼Œé€šè¿‡åœ¨é«˜åˆ†è¾¨ç‡UNetçš„è·³è¿‡è¿æ¥ä¸­é›†æˆMambaå—ï¼Œå¼•å…¥äº†Mambaå¢å¼ºUNetï¼ˆMEUNetï¼‰ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾å­¦ä¹ ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé‡å¯¹æ¯”åº¦æŸå¤±ï¼ˆMCLï¼‰ï¼Œè¯¥æŸå¤±åœ¨ä¸åŒçš„å¼ºåº¦çª—å£ä¸Šè®¡ç®—åˆæˆæŸå¤±ï¼Œä»¥æé«˜è½¯ç»„ç»‡å’Œéª¨åŒºåŸŸçš„å›¾åƒè´¨é‡ã€‚åœ¨SynthRAD2023æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸå§‹CBCTç›¸æ¯”ï¼ŒGLFCå°†sCTçš„SSIMä»77.91%æé«˜åˆ°91.50%ï¼Œå¹¶ä¸”åœ¨sCTç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºå‡ ç§ç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/intelland/GLFC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/intelland/GLFCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02992v1">PDF</a> Accepted by ISBI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå…¨çƒæœ¬åœ°ç‰¹å¾ä¸å¯¹æ¯”åº¦å­¦ä¹ ï¼ˆGLFCï¼‰çš„åŒ»å­¦å›¾åƒåˆæˆæ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨é›†æˆMambaå—çš„å¢å¼ºUNetï¼ˆMEUNetï¼‰æ¨¡å‹ä»¥åŠå¤šé‡å¯¹æ¯”åº¦æŸå¤±ï¼ˆMCLï¼‰ï¼Œå®ç°äº†åœ¨åˆæˆCTå›¾åƒä¸­å¯¹å…¨å±€å’Œå±€éƒ¨ç‰¹å¾çš„æ•æ‰ä»¥åŠå¯¹ä¸åŒç»„ç»‡å¯¹æ¯”åº¦çš„ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGLFCæ–¹æ³•æ˜¾è‘—æé«˜äº†åˆæˆCTå›¾åƒçš„è´¨é‡ï¼Œä¸åŸå§‹CBCTç›¸æ¯”ï¼ŒSSIMæŒ‡æ•°ä»77.91%æé«˜åˆ°91.50%ï¼Œå¹¶ä¼˜äºå…¶ä»–å‡ ç§ç°æœ‰çš„åˆæˆCTç”Ÿæˆæ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºå…¨çƒæœ¬åœ°ç‰¹å¾ä¸å¯¹æ¯”åº¦å­¦ä¹ ï¼ˆGLFCï¼‰çš„åŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ã€‚</li>
<li>ä½¿ç”¨é›†æˆMambaå—çš„å¢å¼ºUNetï¼ˆMEUNetï¼‰æ¨¡å‹è¿›è¡Œå…¨å±€å’Œå±€éƒ¨ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¤šé‡å¯¹æ¯”åº¦æŸå¤±ï¼ˆMCLï¼‰ä¼˜åŒ–äº†å›¾åƒçš„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯è½¯ç»„ç»‡å’Œéª¨éª¼åŒºåŸŸçš„å¯¹æ¯”åº¦ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†GLFCæ–¹æ³•åœ¨åˆæˆCTå›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œæ˜¾è‘—æé«˜äº†SSIMæŒ‡æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–ç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>å…¬å¼€äº†ç›¸å…³ä»£ç ä»¥ä¾¿è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee3a3e81926b3a6cf9a1b739eb0bdd93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae4c77f0071dfe65a883f49284382eb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-382898e406c573683e406a518fee98a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddd748cd10066c5dbe0faa3f5f770983.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb910b63dfebbb483f7b27f0e1744731.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Two-electron-one-photon-process-in-collision-of-1-8-2-1-MeV-neon-on-aluminum"><a href="#Two-electron-one-photon-process-in-collision-of-1-8-2-1-MeV-neon-on-aluminum" class="headerlink" title="Two-electron one-photon process in collision of 1.8-2.1 MeV neon on   aluminum"></a>Two-electron one-photon process in collision of 1.8-2.1 MeV neon on   aluminum</h2><p><strong>Authors:Shashank Singh, Narendra Kumar, Soumya Chatterjee, Deepak Swami, Alok Kumar Singh Jha, Mumtaz Oswal, K. P. Singh, T. Nandi</strong></p>
<p>X-ray emissions due to the two-electron one-photon (TEOP) process in the neon projectile and aluminum target have been successfully observed for the beam energy window of 1.8-2.1 MeV. Experimental TEOP transition energies have been compared with theoretical predictions of flexible atomic structure code (FAC) and General-purpose Relativistic Atomic Structure (GRASP) package. Present results have been verified with reported theoretical and experimental values. Transition rates of the TEOP transitions have also been studied using the said codes. The observed lines have been assigned when the measured transition energies are in good agreement with the theoretical values. Such assignments have further been validated with the good agreements between the experimental and theoretical transition rates. Note that only the TEOP lines in projectile ions are seen with 1.8 MeV energy. In contrast, the TEOP lines in target ions are also observed well with 2.1 MeV energy. Thus, this study sheds useful light on the excitation mechanism of the TEOP processes in the low energy regimes. </p>
<blockquote>
<p>åœ¨èƒ½é‡çª—å£ä¸º1.8-2.1MeVçš„æŸæµä¸­ï¼ŒæˆåŠŸè§‚å¯Ÿåˆ°ç”±äºæ°–å°„å¼¹å’Œé“é¶ä¸­çš„ä¸¤ç”µå­å•å…‰å­ï¼ˆTEOPï¼‰è¿‡ç¨‹äº§ç”Ÿçš„Xå°„çº¿å‘å°„ã€‚å®éªŒä¸­çš„TEOPè·ƒè¿èƒ½é‡ä¸çµæ´»åŸå­ç»“æ„ä»£ç ï¼ˆFACï¼‰å’Œé€šç”¨ç›¸å¯¹è®ºåŸå­ç»“æ„ï¼ˆGRASPï¼‰è½¯ä»¶åŒ…çš„ç†è®ºé¢„æµ‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç›®å‰çš„ç»“æœå·²ç»ä¸æŠ¥å‘Šçš„ç†è®ºå€¼å’Œå®éªŒå€¼è¿›è¡Œäº†éªŒè¯ã€‚è¿˜ä½¿ç”¨ä¸Šè¿°ä»£ç ç ”ç©¶äº†TEOPè·ƒè¿çš„è·ƒè¿ç‡ã€‚å½“æµ‹é‡çš„è·ƒè¿èƒ½é‡ä¸ç†è®ºå€¼ä¸€è‡´æ—¶ï¼Œå·²è§‚å¯Ÿåˆ°è¿™äº›è°±çº¿ã€‚è¿™ç§åˆ†é…å·²é€šè¿‡å®éªŒå’Œç†è®ºè·ƒè¿ç‡ä¹‹é—´çš„è‰¯å¥½åè®®è¿›ä¸€æ­¥å¾—åˆ°éªŒè¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåªæœ‰1.8MeVèƒ½é‡ä¸‹çš„å°„å¼¹ç¦»å­æ‰ä¼šå‡ºç°TEOPçº¿ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›®æ ‡ç¦»å­åœ¨2.1MeVèƒ½é‡ä¸‹çš„TEOPçº¿ä¹Ÿè¢«å¾ˆå¥½åœ°è§‚å¯Ÿåˆ°ã€‚å› æ­¤ï¼Œè¿™é¡¹ç ”ç©¶å¯¹äºäº†è§£ä½èƒ½çŠ¶æ€ä¸‹çš„TEOPè¿‡ç¨‹çš„æ¿€å‘æœºåˆ¶å…·æœ‰ç§¯ææ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02967v1">PDF</a> 6 pages, 2 figures, 3 tables. arXiv admin note: text overlap with   arXiv:2201.02566</p>
<p><strong>Summary</strong></p>
<p>è¯¥é¡¹ç›®æˆåŠŸè§‚å¯Ÿåˆ°ç”µå­æ•°é‡ä¸ºä¸¤ä¸ªï¼Œå…‰å­ä¸ºä¸€ä¸ªï¼ˆTEOPè¿‡ç¨‹ï¼‰åœ¨æ°–åŸå­é¡¹ç›®å’Œé“é¶ä¸­çš„Xå°„çº¿è¾å°„ç°è±¡ã€‚å®éªŒæ¶‰åŠçš„æŸèƒ½çª—å£ä¸º1.8-2.1MeVã€‚å®éªŒä¸­çš„TEOPè·ƒè¿èƒ½é‡ä¸æŸ”æ€§åŸå­ç»“æ„ä»£ç ï¼ˆFACï¼‰å’Œé€šç”¨ç›¸å¯¹è®ºåŸå­ç»“æ„ï¼ˆGRASPï¼‰è½¯ä»¶åŒ…çš„ç†è®ºé¢„æµ‹å€¼è¿›è¡Œäº†æ¯”è¾ƒéªŒè¯ã€‚æ­¤å¤–ï¼Œè¿˜ç ”ç©¶äº†TEOPè·ƒè¿çš„è·ƒè¿é€Ÿç‡ã€‚å½“è§‚å¯Ÿåˆ°çš„è°±çº¿ä¸ç†è®ºå€¼ä¸€è‡´æ—¶ï¼Œå·²è¢«æˆåŠŸåˆ†é…ã€‚åŒæ—¶è¯å®å…¶æ­£ç¡®æ€§ï¼Œå‘ç°å®éªŒçš„è·ƒè¿é€Ÿç‡ä¸ç†è®ºå€¼ç›¸ç¬¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨èƒ½é‡ä¸º1.8MeVæ—¶ï¼Œä»…åœ¨é¡¹ç›®ç¦»å­ä¸­è§‚å¯Ÿåˆ°TEOPçº¿ï¼›è€Œåœ¨èƒ½é‡ä¸º2.1MeVæ—¶ï¼Œç›®æ ‡ç¦»å­ä¸­ä¹Ÿè§‚å¯Ÿåˆ°TEOPçº¿ã€‚æ­¤ç ”ç©¶æœ‰åŠ©äºç†è§£ä½èƒ½æ€ä¸‹çš„TEOPè¿‡ç¨‹çš„æ¿€å‘æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ¬¡å®éªŒçš„ä¸»è¦å‘ç°å’Œè¦ç‚¹æ€»ç»“ï¼š</p>
<ol>
<li>Xå°„çº¿è¾å°„ç°è±¡æˆåŠŸè§‚å¯Ÿåˆ°ç”µå­æ•°é‡ä¸ºä¸¤ä¸ªã€å…‰å­ä¸ºä¸€ä¸ªï¼ˆTEOPè¿‡ç¨‹ï¼‰åœ¨æ°–åŸå­é¡¹ç›®å’Œé“é¶ä¸­çš„ååº”ã€‚</li>
<li>å®éªŒæ¶‰åŠçš„æŸèƒ½çª—å£ä¸ºç‰¹å®šçš„èƒ½é‡èŒƒå›´ï¼Œå³1.8-2.1MeVä¹‹é—´ã€‚è¿™æ˜¯å¯¹ç‰¹å®šçš„èƒ½æ®µè¿›è¡Œè§‚å¯Ÿçš„ä¸»è¦èƒ½é‡åŒºé—´ï¼Œè¯æ˜æ­¤æ¬¡è§‚æµ‹æœ‰æ•ˆæ€§åœ¨è¿™ä¸ªèŒƒå›´ä¹‹å†…ç»™å‡ºäº†ç›´æ¥çš„çº¿ç´¢ï¼Œå¯¹è¯¥æœºåˆ¶çš„è¿›ä¸€æ­¥ç ”ç©¶æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å®éªŒä¸­çš„TEOPè·ƒè¿èƒ½é‡ä¸FACå’ŒGRASPè½¯ä»¶åŒ…çš„ç†è®ºé¢„æµ‹å€¼è¿›è¡Œäº†æ¯”è¾ƒéªŒè¯ï¼Œè¯æ˜äº†å®éªŒç»“æœçš„æœ‰æ•ˆæ€§åŠç†è®ºçš„å‡†ç¡®æ€§ã€‚å®éªŒå€¼ä¸ç†è®ºé¢„æµ‹çš„ä¸€è‡´æ˜¯åç»­ç ”ç©¶çš„è‰¯å¥½ä¾æ®å’ŒåŸºç¡€ã€‚è¿™å¯¹æ›´æ·±å…¥äº†è§£ç”µå­è·ƒè¿æœºåˆ¶æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7fe1936626f2718d6cfd7df4d29605ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-282cca925cc25b32771b567b4763bd5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4adf79c9381071bc04caaa4bddea4e51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2ddd7e4d036789a6cc18da4a39d0d58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758bf05d86e9e254042da0f1332c23ab.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PARF-Net-integrating-pixel-wise-adaptive-receptive-fields-into-hybrid-Transformer-CNN-network-for-medical-image-segmentation"><a href="#PARF-Net-integrating-pixel-wise-adaptive-receptive-fields-into-hybrid-Transformer-CNN-network-for-medical-image-segmentation" class="headerlink" title="PARF-Net: integrating pixel-wise adaptive receptive fields into hybrid   Transformer-CNN network for medical image segmentation"></a>PARF-Net: integrating pixel-wise adaptive receptive fields into hybrid   Transformer-CNN network for medical image segmentation</h2><p><strong>Authors:Xu Ma, Mengsheng Chen, Junhui Zhang, Lijuan Song, Fang Du, Zhenhua Yu</strong></p>
<p>Convolutional neural networks (CNNs) excel in local feature extraction while Transformers are superior in processing global semantic information. By leveraging the strengths of both, hybrid Transformer-CNN networks have become the major architectures in medical image segmentation tasks. However, existing hybrid methods still suffer deficient learning of local semantic features due to the fixed receptive fields of convolutions, and also fall short in effectively integrating local and long-range dependencies. To address these issues, we develop a new method PARF-Net to integrate convolutions of Pixel-wise Adaptive Receptive Fields (Conv-PARF) into hybrid Network for medical image segmentation. The Conv-PARF is introduced to cope with inter-pixel semantic differences and dynamically adjust convolutional receptive fields for each pixel, thus providing distinguishable features to disentangle the lesions with varying shapes and scales from the background. The features derived from the Conv-PARF layers are further processed using hybrid Transformer-CNN blocks under a lightweight manner, to effectively capture local and long-range dependencies, thus boosting the segmentation performance. By assessing PARF-Net on four widely used medical image datasets including MoNuSeg, GlaS, DSB2018 and multi-organ Synapse, we showcase the advantages of our method over the state-of-the-arts. For instance, PARF-Net achieves 84.27% mean Dice on the Synapse dataset, surpassing existing methods by a large margin. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨å±€éƒ¨ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒTransformeråœ¨å¤„ç†å…¨å±€è¯­ä¹‰ä¿¡æ¯æ–¹é¢æ›´æ“…é•¿ã€‚é€šè¿‡ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œæ··åˆTransformer-CNNç½‘ç»œå·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„ä¸»è¦æ¶æ„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ··åˆæ–¹æ³•ä»ç„¶å­˜åœ¨ä¸€äº›ç¼ºé™·ï¼Œç”±äºå·ç§¯çš„å›ºå®šæ„Ÿå—é‡ï¼Œå…¶å¯¹å±€éƒ¨è¯­ä¹‰ç‰¹å¾çš„å­¦ä¹ ä¸è¶³ï¼Œå¹¶ä¸”åœ¨æœ‰æ•ˆæ•´åˆå±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–æ–¹é¢ä¹Ÿå­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„æ–¹æ³•PARF-Netï¼Œå®ƒå°†åƒç´ çº§è‡ªé€‚åº”æ„Ÿå—é‡ï¼ˆConv-PARFï¼‰çš„å·ç§¯èå…¥åˆ°æ··åˆç½‘ç»œä¸­è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚Conv-PARFçš„å¼•å…¥æ˜¯ä¸ºäº†åº”å¯¹åƒç´ é—´è¯­ä¹‰å·®å¼‚ï¼Œå¹¶åŠ¨æ€è°ƒæ•´æ¯ä¸ªåƒç´ çš„å·ç§¯æ„Ÿå—é‡ï¼Œä»è€Œä¸ºä¸åŒå½¢çŠ¶å’Œå°ºåº¦çš„ç—…å˜æä¾›å¯è¯†åˆ«çš„ç‰¹å¾ï¼Œå°†å…¶ä¸èƒŒæ™¯åˆ†ç¦»ã€‚ä»Conv-PARFå±‚æ´¾ç”Ÿçš„ç‰¹å¾è¿›ä¸€æ­¥ä½¿ç”¨æ··åˆTransformer-CNNå—ä»¥è½»ä¾¿çš„æ–¹å¼è¿›è¡Œå¤„ç†ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·å±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŒ»å­¦å›¾åƒæ•°æ®é›†MoNuSegã€GlaSã€DSB2018å’Œå¤šå™¨å®˜Synapseä¸Šè¯„ä¼°PARF-Netï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æ‰€å…·æœ‰çš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œåœ¨Synapseæ•°æ®é›†ä¸Šï¼ŒPARF-Netçš„å¹³å‡Diceç³»æ•°ä¸º84.27%ï¼Œå¤§å¤§è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02882v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ“…é•¿å±€éƒ¨ç‰¹å¾æå–ï¼Œè€ŒTransformeråœ¨å¤„ç†å…¨å±€è¯­ä¹‰ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é€šè¿‡ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œæ··åˆTransformer-CNNç½‘ç»œå·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„ä¸»è¦æ¶æ„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ··åˆæ–¹æ³•ä»ç„¶å› ä¸ºå·ç§¯çš„å›ºå®šæ„Ÿå—é‡è€Œå­˜åœ¨ç€å¯¹å±€éƒ¨è¯­ä¹‰ç‰¹å¾å­¦ä¹ ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨æ•´åˆå±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–æ–¹é¢ä¹Ÿå­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„æ–¹æ³•PARF-Netï¼Œå°†åƒç´ çº§è‡ªé€‚åº”æ„Ÿå—é‡ï¼ˆConv-PARFï¼‰çš„å·ç§¯èå…¥åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ··åˆç½‘ç»œã€‚Conv-PARFè¢«å¼•å…¥ä»¥å¤„ç†åƒç´ é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå¹¶åŠ¨æ€è°ƒæ•´æ¯ä¸ªåƒç´ çš„å·ç§¯æ„Ÿå—é‡ï¼Œä»è€Œæä¾›å¯åŒºåˆ†ç‰¹å¾æ¥åŒºåˆ†ä¸åŒå½¢çŠ¶å’Œå°ºåº¦çš„ç—…å˜ä¸èƒŒæ™¯ã€‚ä»Conv-PARFå±‚æ´¾ç”Ÿçš„ç‰¹å¾è¿›ä¸€æ­¥ä½¿ç”¨æ··åˆTransformer-CNNå—ä»¥è½»ä¾¿çš„æ–¹å¼å¤„ç†ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·å±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¯„ä¼°PARF-Netï¼ŒåŒ…æ‹¬MoNuSegã€GlaSã€DSB2018å’Œå¤šå™¨å®˜Synapseæ•°æ®é›†ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œåœ¨Synapseæ•°æ®é›†ä¸Šï¼ŒPARF-Netçš„å¹³å‡Diceç³»æ•°ä¸º84.27%ï¼Œå¤§å¤§è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒTransformeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å„æœ‰ä¼˜åŠ¿ï¼ŒCNNæ“…é•¿å±€éƒ¨ç‰¹å¾æå–ï¼Œè€ŒTransformeræ“…é•¿å¤„ç†å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ··åˆæ–¹æ³•å­˜åœ¨å¯¹å±€éƒ¨è¯­ä¹‰ç‰¹å¾å­¦ä¹ ä¸è¶³ä»¥åŠæ•´åˆå±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æ–°æ–¹æ³•PARF-Neté€šè¿‡å¼•å…¥åƒç´ çº§è‡ªé€‚åº”æ„Ÿå—é‡ï¼ˆConv-PARFï¼‰çš„å·ç§¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>Conv-PARFèƒ½å¤Ÿå¤„ç†åƒç´ é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å·ç§¯æ„Ÿå—é‡ï¼Œä»¥æä¾›å¯åŒºåˆ†ç‰¹å¾ã€‚</li>
<li>PARF-Netä½¿ç”¨æ··åˆTransformer-CNNå—ä»¥è½»ä¾¿æ–¹å¼å¤„ç†ç‰¹å¾ï¼Œæœ‰æ•ˆæ•è·å±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>PARF-Netåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4de3e628a9c00be997e88b5bbc61f13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2bcfe698f94bcd44afcc40e3b26f5c4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Two-Dimensional-Unknown-View-Tomography-from-Unknown-Angle-Distributions"><a href="#Two-Dimensional-Unknown-View-Tomography-from-Unknown-Angle-Distributions" class="headerlink" title="Two-Dimensional Unknown View Tomography from Unknown Angle Distributions"></a>Two-Dimensional Unknown View Tomography from Unknown Angle Distributions</h2><p><strong>Authors:Kaishva Chintan Shah, Karthik S. Gurumoorthy, Ajit Rajwade</strong></p>
<p>This study presents a technique for 2D tomography under unknown viewing angles when the distribution of the viewing angles is also unknown. Unknown view tomography (UVT) is a problem encountered in cryo-electron microscopy and in the geometric calibration of CT systems. There exists a moderate-sized literature on the 2D UVT problem, but most existing 2D UVT algorithms assume knowledge of the angle distribution which is not available usually. Our proposed methodology formulates the problem as an optimization task based on cross-validation error, to estimate the angle distribution jointly with the underlying 2D structure in an alternating fashion. We explore the algorithmâ€™s capabilities for the case of two probability distribution models: a semi-parametric mixture of von Mises densities and a probability mass function model. We evaluate our algorithmâ€™s performance under noisy projections using a PCA-based denoising technique and Graph Laplacian Tomography (GLT) driven by order statistics of the estimated distribution, to ensure near-perfect ordering, and compare our algorithm to intuitive baselines. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åœ¨æœªçŸ¥è§†è§’å’Œè§†è§’åˆ†å¸ƒæœªçŸ¥çš„æƒ…å†µä¸‹è¿›è¡ŒäºŒç»´å±‚ææˆåƒçš„æŠ€æœ¯ã€‚æœªçŸ¥è§†è§’å±‚ææˆåƒï¼ˆUVTï¼‰æ˜¯å†·å†»ç”µå­æ˜¾å¾®é•œå’Œè®¡ç®—æœºæ–­å±‚æ‰«æç³»ç»Ÿå‡ ä½•æ ¡å‡†ä¸­é‡åˆ°çš„é—®é¢˜ã€‚å…³äºäºŒç»´UVTé—®é¢˜çš„æ–‡çŒ®æ•°é‡é€‚ä¸­ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„äºŒç»´UVTç®—æ³•éƒ½å‡è®¾å·²çŸ¥è§’åº¦åˆ†å¸ƒï¼Œè¿™åœ¨ç°å®ä¸­é€šå¸¸æ— æ³•è·å¾—ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å°†é—®é¢˜è¡¨è¿°ä¸ºä¸€ä¸ªåŸºäºäº¤å‰éªŒè¯è¯¯å·®çš„ä¼˜åŒ–ä»»åŠ¡ï¼Œä»¥äº¤æ›¿æ–¹å¼ä¼°è®¡è§’åº¦åˆ†å¸ƒå’Œæ½œåœ¨çš„äºŒç»´ç»“æ„ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹ä¸‹è¯¥ç®—æ³•çš„èƒ½åŠ›ï¼šåŠå‚æ•°çš„å†¯ç±³å¡æ–¯å¯†åº¦æ··åˆæ¨¡å‹å’Œæ¦‚ç‡è´¨é‡å‡½æ•°æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºä¸»æˆåˆ†åˆ†æçš„å»å™ªæŠ€æœ¯å’Œç”±ä¼°è®¡åˆ†å¸ƒçš„æ’åºç»Ÿè®¡é©±åŠ¨çš„å›¾å½¢æ‹‰æ™®æ‹‰æ–¯å±‚ææˆåƒï¼ˆGLTï¼‰ï¼Œç¡®ä¿è¿‘ä¹å®Œç¾çš„æ’åºï¼Œåœ¨å™ªå£°æŠ•å½±ä¸‹è¯„ä¼°æˆ‘ä»¬çš„ç®—æ³•æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ç›´è§‚åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02872v1">PDF</a> Accepted to the International Conference on Acoustics, Speech, and   Signal Processing (ICASSP) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³äºŒç»´æœªçŸ¥è§†è§’å±‚ææˆåƒæŠ€æœ¯çš„æ–¹æ³•ï¼Œå½“è§†è§’åˆ†å¸ƒæœªçŸ¥æ—¶å°¤ä¸ºé€‚ç”¨ã€‚è¯¥æŠ€æœ¯ä¸»è¦åº”ç”¨äºå†·å†»ç”µå­æ˜¾å¾®é•œå’Œè®¡ç®—æœºæ–­å±‚æ‰«æç³»ç»Ÿçš„å‡ ä½•æ ¡å‡†ç­‰é¢†åŸŸã€‚å°½ç®¡å…³äºäºŒç»´æœªçŸ¥è§†è§’å±‚ææˆåƒé—®é¢˜çš„ç ”ç©¶å·²æœ‰ä¸€å®šè§„æ¨¡ï¼Œä½†å¤§å¤šæ•°ç°æœ‰ç®—æ³•éƒ½å‡è®¾å·²çŸ¥è§’åº¦åˆ†å¸ƒï¼Œè¿™åœ¨ç°å®ä¸­é€šå¸¸æ— æ³•è·å¾—ã€‚æœ¬ç ”ç©¶å°†é—®é¢˜å»ºæ¨¡ä¸ºåŸºäºäº¤å‰éªŒè¯è¯¯å·®çš„ä¼˜åŒ–ä»»åŠ¡ï¼Œæ—¨åœ¨äº¤æ›¿ä¼°è®¡è§’åº¦åˆ†å¸ƒå’Œæ½œåœ¨çš„äºŒç»´ç»“æ„ã€‚ç ”ç©¶æ¢è®¨äº†ä¸¤ç§æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹ä¸‹çš„ç®—æ³•èƒ½åŠ›ï¼šåŠå‚æ•°æ··åˆå†¯ç±³å¡æ–¯å¯†åº¦å’Œæ¦‚ç‡è´¨é‡å‡½æ•°æ¨¡å‹ã€‚é€šè¿‡ä¸»æˆåˆ†åˆ†æå»å™ªæŠ€æœ¯å’ŒåŸºäºåˆ†å¸ƒä¼°è®¡é¡ºåºç»Ÿè®¡çš„å›¾æ‹‰æ™®æ‹‰æ–¯å±‚ææˆåƒæŠ€æœ¯è¯„ä¼°ç®—æ³•æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ç›´è§‚åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§£å†³äºŒç»´æœªçŸ¥è§†è§’å±‚ææˆåƒï¼ˆUVTï¼‰çš„æ–°æŠ€æœ¯ã€‚</li>
<li>UVTé—®é¢˜æ˜¯å†·å†»ç”µå­æ˜¾å¾®é•œå’ŒCTç³»ç»Ÿå‡ ä½•æ ¡å‡†ä¸­çš„å¸¸è§é—®é¢˜ã€‚</li>
<li>ç°æœ‰å¤§å¤šæ•°UVTç®—æ³•å‡è®¾å·²çŸ¥è§’åº¦åˆ†å¸ƒï¼Œä½†è¿™åœ¨ç°å®ä¸­é€šå¸¸æ— æ³•è·å¾—ã€‚</li>
<li>æœ¬ç ”ç©¶å°†UVTé—®é¢˜å»ºæ¨¡ä¸ºä¼˜åŒ–ä»»åŠ¡ï¼Œæ—¨åœ¨ä¼°è®¡è§’åº¦åˆ†å¸ƒå’Œæ½œåœ¨äºŒç»´ç»“æ„ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠä¸¤ç§æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹ï¼šåŠå‚æ•°æ··åˆå†¯ç±³å¡æ–¯å¯†åº¦å’Œæ¦‚ç‡è´¨é‡å‡½æ•°æ¨¡å‹ã€‚</li>
<li>é€šè¿‡PCAå»å™ªæŠ€æœ¯å’Œå›¾æ‹‰æ™®æ‹‰æ–¯å±‚ææˆåƒæŠ€æœ¯è¯„ä¼°ç®—æ³•æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf89c9ccd9ab999dfc170f85eb2900bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-492d8f4bd89743a49cc3a3b0aea6d4a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b02e0d7591c3a8a267dd97b8ca4ca8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6b99f184aef0aa936a796b3cc0cb31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828fe7d20a27d905061589f069c65250.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="COph100-A-comprehensive-fundus-image-registration-dataset-from-infants-constituting-the-â€œRIDIRPâ€-database"><a href="#COph100-A-comprehensive-fundus-image-registration-dataset-from-infants-constituting-the-â€œRIDIRPâ€-database" class="headerlink" title="COph100: A comprehensive fundus image registration dataset from infants   constituting the â€œRIDIRPâ€ database"></a>COph100: A comprehensive fundus image registration dataset from infants   constituting the â€œRIDIRPâ€ database</h2><p><strong>Authors:Yan Hu, Mingdao Gong, Zhongxi Qiu, Jiabao Liu, Hongli Shen, Mingzhen Yuan, Xiaoqing Zhang, Heng Li, Hai Lu, Jiang Liu</strong></p>
<p>Retinal image registration is vital for diagnostic therapeutic applications within the field of ophthalmology. Existing public datasets, focusing on adult retinal pathologies with high-quality images, have limited number of image pairs and neglect clinical challenges. To address this gap, we introduce COph100, a novel and challenging dataset known as the Comprehensive Ophthalmology Retinal Image Registration dataset for infants with a wide range of image quality issues constituting the public â€œRIDIRPâ€ database. COph100 consists of 100 eyes, each with 2 to 9 examination sessions, amounting to a total of 491 image pairs carefully selected from the publicly available dataset. We manually labeled the corresponding ground truth image points and provided automatic vessel segmentation masks for each image. We have assessed COph100 in terms of image quality and registration outcomes using state-of-the-art algorithms. This resource enables a robust comparison of retinal registration methodologies and aids in the analysis of disease progression in infants, thereby deepening our understanding of pediatric ophthalmic conditions. </p>
<blockquote>
<p>è§†ç½‘è†œå›¾åƒé…å‡†å¯¹äºçœ¼ç§‘é¢†åŸŸçš„è¯Šæ–­å’Œæ²»ç–—åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å…¬å…±æ•°æ®é›†ä¸»è¦å…³æ³¨å…·æœ‰é«˜è´¨é‡å›¾åƒçš„æˆäººè§†ç½‘è†œç—…ç†ï¼Œå›¾åƒå¯¹æ•°é‡æœ‰é™ï¼Œå¹¶å¿½ç•¥äº†ä¸´åºŠæŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ä»‹ç»äº†COph100ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œåä¸ºâ€œç»¼åˆçœ¼ç§‘è§†ç½‘è†œå›¾åƒé…å‡†æ•°æ®é›†â€ï¼Œä¸“é—¨é’ˆå¯¹å©´å¹¼å„¿ï¼Œæ¶µç›–å¹¿æ³›çš„å›¾åƒè´¨é‡é—®é¢˜ï¼Œæ„æˆå…¬å…±â€œRIDIRPâ€æ•°æ®åº“ã€‚COph100åŒ…å«100åªçœ¼ç›ï¼Œæ¯åªçœ¼ç›æœ‰2è‡³9æ¬¡æ£€æŸ¥ä¼šè¯ï¼Œæ€»å…±ä»å…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸­ç²¾å¿ƒæŒ‘é€‰äº†491å¼ å›¾åƒå¯¹ã€‚æˆ‘ä»¬æ‰‹åŠ¨æ ‡è®°äº†ç›¸åº”çš„çœŸå®å›¾åƒç‚¹ï¼Œå¹¶ä¸ºæ¯å¼ å›¾åƒæä¾›äº†è‡ªåŠ¨è¡€ç®¡åˆ†å‰²æ©è†œã€‚æˆ‘ä»¬é‡‡ç”¨æœ€å…ˆè¿›çš„ç®—æ³•å¯¹COph100çš„å›¾åƒè´¨é‡å’Œé…å‡†ç»“æœè¿›è¡Œäº†è¯„ä¼°ã€‚è¿™ä¸€èµ„æºèƒ½å¤Ÿç¨³å¥åœ°æ¯”è¾ƒè§†ç½‘è†œé…å‡†æ–¹æ³•ï¼Œæœ‰åŠ©äºåˆ†æå©´å¹¼å„¿ç–¾ç—…è¿›å±•ï¼Œä»è€ŒåŠ æ·±å¯¹å„¿ç§‘çœ¼ç§‘çŠ¶å†µçš„ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02800v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†ç½‘è†œå›¾åƒé…å‡†åœ¨çœ¼ç§‘è¯Šæ–­å’Œæ²»ç–—æ–¹æ³•ä¸­çš„é‡è¦æ€§ã€‚ç°æœ‰çš„å…¬å…±æ•°æ®é›†ä¸»è¦å…³æ³¨æˆäººè§†ç½‘è†œç—…ç†çš„é«˜å“è´¨å›¾åƒï¼Œä½†å›¾åƒå¯¹æ•°é‡æœ‰é™ï¼Œå¿½è§†äº†ä¸´åºŠæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†COph100æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„çœ¼ç§‘è§†ç½‘è†œå›¾åƒé…å‡†æ•°æ®é›†ï¼Œä¸“é—¨é’ˆå¯¹å©´å¹¼å„¿ï¼Œå¹¶æ¶µç›–äº†å¹¿æ³›çš„å›¾åƒè´¨é‡é—®é¢˜ï¼Œæ„æˆå…¬å…±â€œRIDIRPâ€æ•°æ®åº“ã€‚COph100ç”±æ¥è‡ªå…¬å¼€æ•°æ®é›†çš„491ä¸ªå›¾åƒå¯¹ç»„æˆï¼Œè¿™äº›å›¾åƒå¯¹æ˜¯ä»100åªçœ¼ç›ã€æ¯åªçœ¼ç›æœ‰2è‡³9æ¬¡æ£€æŸ¥ä¼šè¯ä¸­ç²¾å¿ƒæŒ‘é€‰å‡ºæ¥çš„ã€‚æœ¬æ–‡æ‰‹åŠ¨æ ‡è®°äº†ç›¸åº”çš„åœ°é¢çœŸå®å›¾åƒç‚¹ï¼Œå¹¶ä¸ºæ¯å¼ å›¾åƒæä¾›äº†è‡ªåŠ¨è¡€ç®¡åˆ†å‰²æ©è†œã€‚å·²ç»ä½¿ç”¨æœ€å…ˆè¿›çš„ç®—æ³•è¯„ä¼°äº†COph100çš„å›¾åƒè´¨é‡å’Œé…å‡†ç»“æœã€‚æ­¤èµ„æºèƒ½å¤Ÿå¯é åœ°æ¯”è¾ƒè§†ç½‘è†œé…å‡†æ–¹æ³•ï¼Œå¹¶æœ‰åŠ©äºåˆ†æå©´å„¿çš„ç–¾ç—…è¿›å±•ï¼Œä»è€ŒåŠ æ·±å¯¹å„¿ç§‘çœ¼ç§‘çŠ¶å†µçš„ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œå›¾åƒé…å‡†åœ¨çœ¼ç§‘è¯Šæ–­å’Œæ²»ç–—ä¸­éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰å…¬å…±æ•°æ®é›†ä¸»è¦å…³æ³¨æˆäººè§†ç½‘è†œç—…ç†çš„é«˜å“è´¨å›¾åƒï¼Œä½†å¯¹å©´å¹¼å„¿çš„ä¸´åºŠæŒ‘æˆ˜é‡è§†ä¸è¶³ã€‚</li>
<li>COph100æ•°æ®é›†çš„ä»‹ç»ï¼Œè¯¥æ•°æ®é›†æ˜¯ä¸“é—¨é’ˆå¯¹å©´å¹¼å„¿çš„çœ¼ç§‘è§†ç½‘è†œå›¾åƒé…å‡†æ•°æ®é›†ã€‚</li>
<li>COph100åŒ…å«æ¥è‡ªå…¬å¼€æ•°æ®åº“çš„491ä¸ªå›¾åƒå¯¹ï¼Œæ¶µç›–äº†å¹¿æ³›çš„å›¾åƒè´¨é‡é—®é¢˜ã€‚</li>
<li>æ‰‹åŠ¨æ ‡è®°äº†å¯¹åº”çš„åœ°é¢çœŸå®å›¾åƒç‚¹ï¼Œå¹¶ä¸ºæ¯ä¸ªå›¾åƒæä¾›äº†è‡ªåŠ¨è¡€ç®¡åˆ†å‰²æ©è†œã€‚</li>
<li>ä½¿ç”¨æœ€å…ˆè¿›çš„ç®—æ³•è¯„ä¼°äº†COph100çš„å›¾åƒè´¨é‡å’Œé…å‡†æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3715dcd7b05dcd9f653554795a6860bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-065249cecd958cbee6d31b5152b4fb7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0332c0547cb508ad18e8bec53355929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5c7214224387b6e96282b42ff00b26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d490fe747169dbb167d3768e2cc0fc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a6715ef4fb9eaf489ae30ea57eb5f2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fab4889bdb7518e765251663a1037b3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GLoG-CSUnet-Enhancing-Vision-Transformers-with-Adaptable-Radiomic-Features-for-Medical-Image-Segmentation"><a href="#GLoG-CSUnet-Enhancing-Vision-Transformers-with-Adaptable-Radiomic-Features-for-Medical-Image-Segmentation" class="headerlink" title="GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic   Features for Medical Image Segmentation"></a>GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic   Features for Medical Image Segmentation</h2><p><strong>Authors:Niloufar Eghbali, Hassan Bagher-Ebadian, Tuka Alhanai, Mohammad M. Ghassemi</strong></p>
<p>Vision Transformers (ViTs) have shown promise in medical image semantic segmentation (MISS) by capturing long-range correlations. However, ViTs often struggle to model local spatial information effectively, which is essential for accurately segmenting fine anatomical details, particularly when applied to small datasets without extensive pre-training. We introduce Gabor and Laplacian of Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture enhancing Transformer-based models by incorporating learnable radiomic features. This approach integrates dynamically adaptive Gabor and Laplacian of Gaussian (LoG) filters to capture texture, edge, and boundary information, enhancing the feature representation processed by the Transformer model. Our method uniquely combines the long-range dependency modeling of Transformers with the texture analysis capabilities of Gabor and LoG features. Evaluated on the Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet demonstrates significant improvements over state-of-the-art models, achieving a 1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal computational overhead (only 15 and 30 additional parameters, respectively). GLoG-CSUnetâ€™s flexible design allows integration with various base models, offering a promising approach for incorporating radiomics-inspired feature extraction in Transformer architectures for medical image analysis. The code implementation is available on GitHub at: <a target="_blank" rel="noopener" href="https://github.com/HAAIL/GLoG-CSUnet">https://github.com/HAAIL/GLoG-CSUnet</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰é€šè¿‡æ•æ‰é•¿ç¨‹å…³è”åœ¨åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆMISSï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒViTsåœ¨å»ºæ¨¡å±€éƒ¨ç©ºé—´ä¿¡æ¯æ–¹é¢å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™å¯¹äºå‡†ç¡®åˆ†å‰²ç²¾ç»†çš„è§£å‰–ç»†èŠ‚è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æœªè¿›è¡Œå¹¿æ³›é¢„è®­ç»ƒçš„æƒ…å†µä¸‹åº”ç”¨äºå°è§„æ¨¡æ•°æ®é›†æ—¶ã€‚æˆ‘ä»¬å¼•å…¥äº†Gaborå’ŒLaplacian of Gaussianå·ç§¯Swinç½‘ç»œï¼ˆGLoG-CSUnetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé€šè¿‡èå…¥å¯å­¦ä¹ çš„æ”¾å°„å­¦ç‰¹å¾æ¥å¢å¼ºåŸºäºTransformerçš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†åŠ¨æ€è‡ªé€‚åº”çš„Gaborå’ŒLaplacian of Gaussianï¼ˆLoGï¼‰æ»¤æ³¢å™¨ï¼Œä»¥æ•æ‰çº¹ç†ã€è¾¹ç¼˜å’Œè¾¹ç•Œä¿¡æ¯ï¼Œå¢å¼ºTransformeræ¨¡å‹å¤„ç†çš„ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç‹¬ç‰¹åœ°ç»“åˆäº†Transformerçš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡å’ŒGaborä¸LoGç‰¹å¾çš„çº¹ç†åˆ†æèƒ½åŠ›ã€‚åœ¨Synapseå¤šå™¨å®˜å’ŒACDCå¿ƒè„åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒGLoG-CSUnetåœ¨æœ€æ–°æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒSynapseçš„Diceå¾—åˆ†å¢åŠ äº†1.14%ï¼ŒACDCå¢åŠ äº†0.99%ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€æå°ï¼ˆä»…å¢åŠ äº†15ä¸ªå’Œ30ä¸ªé¢å¤–å‚æ•°ï¼‰ã€‚GLoG-CSUnetçš„çµæ´»è®¾è®¡å¯èå…¥å„ç§åŸºç¡€æ¨¡å‹ï¼Œä¸ºåœ¨Transformeræ¶æ„ä¸­èå…¥æ”¾å°„å­¦å¯å‘ç‰¹å¾æå–æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚ä»£ç å®ç°å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/HAAIL/GLoG-CSUnet%E3%80%82">https://github.com/HAAIL/GLoG-CSUnetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02788v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆMISSï¼‰ï¼Œæœ¬æ–‡æå‡ºä¸€ç§èåˆGaborå’ŒLaplacian of Gaussianå·ç§¯ç¥ç»ç½‘ç»œçš„æ–°å‹æ¶æ„GLoG-CSUnetã€‚å®ƒé€šè¿‡é›†æˆå¯å­¦ä¹ çš„æ”¾å°„å­¦ç‰¹å¾ï¼Œæé«˜äº†åŸºäºTransformerçš„æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ¶æ„ç»“åˆäº†Transformerçš„é•¿ç¨‹ä¾èµ–æ€§å»ºæ¨¡ä¸Gaborå’ŒLoGæ»¤æ³¢å™¨çš„çº¹ç†åˆ†æåŠŸèƒ½ï¼Œå¯åœ¨Synapseå¤šå™¨å®˜å’ŒACDCå¿ƒè„åˆ†å‰²æ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViTs) åœ¨åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­å±•ç°æ½œåŠ›ï¼Œä½†éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡å±€éƒ¨ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>GLoG-CSUnetæ–°å‹æ¶æ„é€šè¿‡ç»“åˆå¯å­¦ä¹ çš„æ”¾å°„å­¦ç‰¹å¾ï¼Œæé«˜äº†Transformeræ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>GLoG-CSUnetèåˆäº†Transformerçš„é•¿ç¨‹ä¾èµ–æ€§å»ºæ¨¡ä¸Gaborå’ŒLoGæ»¤æ³¢å™¨çš„çº¹ç†åˆ†æåŠŸèƒ½ã€‚</li>
<li>åœ¨Synapseå¤šå™¨å®˜å’ŒACDCå¿ƒè„åˆ†å‰²æ•°æ®é›†ä¸Šï¼ŒGLoG-CSUnetè¾ƒç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒDiceå¾—åˆ†åˆ†åˆ«æé«˜1.14%å’Œ0.99%ã€‚</li>
<li>GLoG-CSUnetè®¾è®¡çµæ´»ï¼Œå¯é›†æˆäºå„ç§åŸºç¡€æ¨¡å‹ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­èå…¥æ”¾å°„å­¦ç‰¹å¾æå–çš„Transformeræ¶æ„æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-97433c8ec53e481b6c987c08f059e210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18b5af29fffe6c2d7aefee3e9997ad83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bdc56bb40c9fe435e45291e090aeb35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aeee68bf7381bd82029b9972f1a7b895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe35d5743deff00266f5461b716bd679.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GIT-CXR-End-to-End-Transformer-for-Chest-X-Ray-Report-Generation"><a href="#GIT-CXR-End-to-End-Transformer-for-Chest-X-Ray-Report-Generation" class="headerlink" title="GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation"></a>GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation</h2><p><strong>Authors:Iustin SÃ®rbu, Iulia-Renata SÃ®rbu, Jasmina Bogojeska, Traian Rebedea</strong></p>
<p>Medical imaging is crucial for diagnosing, monitoring, and treating medical conditions. The medical reports of radiology images are the primary medium through which medical professionals attest their findings, but their writing is time consuming and requires specialized clinical expertise. The automated generation of radiography reports has thus the potential to improve and standardize patient care and significantly reduce clinicians workload. Through our work, we have designed and evaluated an end-to-end transformer-based method to generate accurate and factually complete radiology reports for X-ray images. Additionally, we are the first to introduce curriculum learning for end-to-end transformers in medical imaging and demonstrate its impact in obtaining improved performance. The experiments have been conducted using the MIMIC-CXR-JPG database, the largest available chest X-ray dataset. The results obtained are comparable with the current state-of-the-art on the natural language generation (NLG) metrics BLEU and ROUGE-L, while setting new state-of-the-art results on F1 examples-averaged, F1-macro and F1-micro metrics for clinical accuracy and on the METEOR metric widely used for NLG. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒå¯¹äºç–¾ç—…çš„è¯Šæ–­ã€ç›‘æµ‹å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚æ”¾å°„å½±åƒæŠ¥å‘Šçš„åŒ»å­¦æŠ¥å‘Šæ˜¯åŒ»å­¦ä¸“ä¸šäººå£«è¯æ˜å…¶å‘ç°çš„ä¸»è¦åª’ä»‹ï¼Œä½†å…¶å†™ä½œè€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçš„ä¸´åºŠç»éªŒã€‚å› æ­¤ï¼Œæ”¾å°„æŠ¥å‘Šçš„è‡ªåŠ¨ç”Ÿæˆå…·æœ‰æ”¹å–„å’Œæ ‡å‡†åŒ–æ‚£è€…æŠ¤ç†ä»¥åŠæ˜¾è‘—å‡å°‘ä¸´åºŠåŒ»ç”Ÿå·¥ä½œé‡çš„æ½œåŠ›ã€‚é€šè¿‡æˆ‘ä»¬çš„å·¥ä½œï¼Œæˆ‘ä»¬è®¾è®¡å¹¶è¯„ä¼°äº†ä¸€ç§åŸºäºç«¯åˆ°ç«¯è½¬æ¢å™¨çš„æ–¹æ³•ï¼Œä¸ºXå°„çº¿å›¾åƒç”Ÿæˆå‡†ç¡®ä¸”äº‹å®å®Œæ•´çš„æ”¾å°„æŠ¥å‘Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡åœ¨åŒ»å­¦æˆåƒç«¯åˆ°ç«¯è½¬æ¢å™¨ä¸­å¼•å…¥è¯¾ç¨‹å­¦ä¹ ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è·å¾—æ”¹è¿›æ€§èƒ½æ–¹é¢çš„ä½œç”¨ã€‚å®éªŒä½¿ç”¨çš„æ˜¯æœ€å¤§çš„å¯ç”¨èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†MIMIC-CXR-JPGæ•°æ®åº“ã€‚æ‰€è·å¾—çš„ç»“æœä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æŒ‡æ ‡BLEUå’ŒROUGE-Lçš„å½“å‰æœ€æ–°æ°´å¹³ç›¸å½“ï¼ŒåŒæ—¶åœ¨F1ç¤ºä¾‹å¹³å‡å€¼ã€F1å®å’ŒF1å¾®è§‚åº¦é‡æŒ‡æ ‡çš„ä¸´åºŠå‡†ç¡®æ€§å’Œå¹¿æ³›åº”ç”¨äºNLGçš„METEORæŒ‡æ ‡ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å½±åƒå­¦å¯¹äºç–¾ç—…çš„è¯Šæ–­ã€ç›‘æµ‹å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚åŒ»å­¦ä¸“å®¶ä¸»è¦é€šè¿‡æ”¾å°„å½±åƒå­¦æŠ¥å‘Šçš„åŒ»å­¦å½±åƒæ¥éªŒè¯ä»–ä»¬çš„å‘ç°ï¼Œä½†æ‰‹å†™æŠ¥å‘Šè€—æ—¶ä¸”éœ€è¦ä¸“ä¸šä¸´åºŠç»éªŒã€‚å› æ­¤ï¼Œè‡ªåŠ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šå…·æœ‰æ”¹å–„å’Œæ ‡å‡†åŒ–æ‚£è€…æŠ¤ç†ä»¥åŠå¤§å¹…å‡è½»ä¸´åºŠåŒ»ç”Ÿå·¥ä½œé‡çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶è®¾è®¡å¹¶è¯„ä¼°äº†ä¸€ç§åŸºäºç«¯åˆ°ç«¯è½¬æ¢å™¨çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹Xå…‰å›¾åƒçš„å‡†ç¡®ä¸”äº‹å®å®Œæ•´çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡å¼•å…¥åŒ»å­¦æˆåƒç«¯åˆ°ç«¯è½¬æ¢å™¨çš„è¯¾ç¨‹å­¦ä¹ ï¼Œå¹¶å±•ç¤ºäº†å…¶å¯¹æé«˜æ€§èƒ½çš„å½±å“ã€‚å®éªŒé‡‡ç”¨æœ€å¤§çš„å¯ç”¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†MIMIC-CXR-JPGæ•°æ®åº“è¿›è¡Œï¼Œç»“æœä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨ä¸´åºŠå‡†ç¡®æ€§çš„F1ç¤ºä¾‹å¹³å‡ã€F1å®è§‚å’ŒF1å¾®è§‚æŒ‡æ ‡ä»¥åŠå¹¿æ³›ä½¿ç”¨çš„NLGåº¦é‡æŒ‡æ ‡METEORä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒå­¦åœ¨ç–¾ç—…è¯Šæ–­ã€ç›‘æµ‹å’Œæ²»ç–—ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>åŒ»å­¦ä¸“å®¶é€šè¿‡æ”¾å°„å½±åƒå­¦æŠ¥å‘ŠéªŒè¯ä»–ä»¬çš„å‘ç°ï¼Œä½†æŠ¥å‘Šç¼–å†™è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šç»éªŒã€‚</li>
<li>è‡ªåŠ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šèƒ½å¤Ÿæ”¹å–„å’Œæ ‡å‡†åŒ–æ‚£è€…æŠ¤ç†ï¼Œå¹¶å¤§å¹…å‡è½»ä¸´åºŠåŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚</li>
<li>ç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§åŸºäºç«¯åˆ°ç«¯è½¬æ¢å™¨çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹Xå…‰å›¾åƒçš„å‡†ç¡®ä¸”äº‹å®å®Œæ•´çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡åœ¨åŒ»å­¦æˆåƒä¸­å¼•å…¥è¯¾ç¨‹å­¦ä¹ ï¼Œä»¥æé«˜ç«¯åˆ°ç«¯è½¬æ¢å™¨çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒé‡‡ç”¨MIMIC-CXR-JPGæ•°æ®åº“è¿›è¡Œï¼Œç»“æœä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56f0455f018110ba41997ceac1b7c6a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29078a2d076fa6d59441eaa6e21883e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f619545d8505138f3f31803e9908adc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c403d08709970285ab560d34bba0b7cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bd72c6c61bf995a845dae28d62bf3f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="KM-UNet-KAN-Mamba-UNet-for-medical-image-segmentation"><a href="#KM-UNet-KAN-Mamba-UNet-for-medical-image-segmentation" class="headerlink" title="KM-UNet KAN Mamba UNet for medical image segmentation"></a>KM-UNet KAN Mamba UNet for medical image segmentation</h2><p><strong>Authors:Yibo Zhang</strong></p>
<p>Medical image segmentation is a critical task in medical imaging analysis. Traditional CNN-based methods struggle with modeling long-range dependencies, while Transformer-based models, despite their success, suffer from quadratic computational complexity. To address these limitations, we propose KM-UNet, a novel U-shaped network architecture that combines the strengths of Kolmogorov-Arnold Networks (KANs) and state-space models (SSMs). KM-UNet leverages the Kolmogorov-Arnold representation theorem for efficient feature representation and SSMs for scalable long-range modeling, achieving a balance between accuracy and computational efficiency. We evaluate KM-UNet on five benchmark datasets: ISIC17, ISIC18, CVC, BUSI, and GLAS. Experimental results demonstrate that KM-UNet achieves competitive performance compared to state-of-the-art methods in medical image segmentation tasks. To the best of our knowledge, KM-UNet is the first medical image segmentation framework integrating KANs and SSMs. This work provides a valuable baseline and new insights for the development of more efficient and interpretable medical image segmentation systems. The code is open source at <a target="_blank" rel="noopener" href="https://github.com/2760613195/KM_UNet">https://github.com/2760613195/KM_UNet</a>   Keywords:KAN,Manba, state-space models,UNet, Medical image segmentation, Deep learning </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å½±åƒåˆ†æä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„åŸºäºCNNçš„æ–¹æ³•åœ¨å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»æ—¶é‡åˆ°å›°éš¾ï¼Œè€ŒåŸºäºTransformerçš„æ¨¡å‹è™½ç„¶å–å¾—äº†æˆåŠŸï¼Œä½†å­˜åœ¨è®¡ç®—å¤æ‚åº¦ä¸ºäºŒæ¬¡æ–¹çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†KM-UNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹Uå‹ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„ä¼˜ç‚¹ã€‚KM-UNetåˆ©ç”¨Kolmogorov-Arnoldè¡¨ç¤ºå®šç†è¿›è¡Œé«˜æ•ˆçš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨SSMè¿›è¡Œå¯æ‰©å±•çš„é•¿è·ç¦»å»ºæ¨¡ï¼Œåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†KM-UNetï¼šISIC17ã€ISIC18ã€CVCã€BUSIå’ŒGLASã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKM-UNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒKM-UNetæ˜¯ç¬¬ä¸€ä¸ªå°†KANså’ŒSSMsæ•´åˆåˆ°åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ä¸­çš„ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘æ›´é«˜æ•ˆå’Œå¯è§£é‡Šçš„åŒ»å­¦å›¾åƒåˆ†å‰²ç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†å’Œæ–°çš„è§è§£ã€‚ä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/2760613195/KM_UNet%E3%80%82%E5%85%B3%E9%94%AE%E8%AF%8D%EF%BC%9AKAN%EF%BC%8CManba%EF%BC%8C%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%EF%BC%8CUNet%EF%BC%8C%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%82">https://github.com/2760613195/KM_UNetã€‚å…³é”®è¯ï¼šKANï¼ŒManbaï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ŒUNetï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæ·±åº¦å­¦ä¹ ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02559v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œæå‡ºKM-UNetæ¨¡å‹ï¼ŒèåˆKolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œåœ¨æ•ˆç‡å’Œç²¾åº¦ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä¸”æ˜¯é¦–ä¸ªç»“åˆä¸¤è€…åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨æ¡†æ¶ã€‚é€šè¿‡å¤šé¡¹åŸºå‡†æµ‹è¯•è¯æ˜æ€§èƒ½ä¼˜è¶Šæ€§ï¼Œå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶æœ‰é‡è¦æ„ä¹‰ã€‚ç›¸å…³ä»£ç å·²å¼€æºå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸCNNæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å±€é™æ€§åœ¨äºéš¾ä»¥å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>Transformeræ¨¡å‹è™½ç„¶æˆåŠŸï¼Œä½†è®¡ç®—å¤æ‚åº¦ä¸ºäºŒæ¬¡æ–¹ã€‚</li>
<li>æå‡ºKM-UNetæ¨¡å‹ï¼Œç»“åˆKolmogorov-Arnoldç½‘ç»œå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>KM-UNetåˆ©ç”¨Kolmogorov-Arnoldè¡¨ç¤ºå®šç†è¿›è¡Œé«˜æ•ˆç‰¹å¾è¡¨ç¤ºå’ŒSSMè¿›è¡Œå¯æ‰©å±•çš„é•¿è·ç¦»å»ºæ¨¡ã€‚</li>
<li>åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜KM-UNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>KM-UNetæ˜¯é¦–ä¸ªç»“åˆKANså’ŒSSMsçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e91b3426be19192d4cebe8291e87b8bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65817db3b1bdfe3bcba1588456c1fa90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9c0ec6fe49255229035248db7a13281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2642c1d7d100f5b424d31795dc3f6cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f9b508ea68dfc75b9424d0444a5f9ab.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Framework-for-lung-CT-image-segmentation-based-on-UNet"><a href="#Framework-for-lung-CT-image-segmentation-based-on-UNet" class="headerlink" title="Framework for lung CT image segmentation based on UNet++"></a>Framework for lung CT image segmentation based on UNet++</h2><p><strong>Authors:Hao Ziang, Jingsi Zhang, Lixian Li</strong></p>
<p>Recently, the state-of-art models for medical image segmentation is U-Net and their variants. These networks, though succeeding in deriving notable results, ignore the practical problem hanging over the medical segmentation field: overfitting and small dataset. The over-complicated deep neural networks unnecessarily extract meaningless information, and a majority of them are not suitable for lung slice CT image segmentation task. To overcome the two limitations, we proposed a new whole-process network merging advanced UNet++ model. The network comprises three main modules: data augmentation, optimized neural network, parameter fine-tuning. By incorporating diverse methods, the training results demonstrate a significant advantage over similar works, achieving leading accuracy of 98.03% with the lowest overfitting. potential. Our network is remarkable as one of the first to target on lung slice CT images. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„æœ€å…ˆè¿›çš„æ¨¡å‹ä¸ºU-NetåŠå…¶å˜ä½“ã€‚è¿™äº›ç½‘ç»œè™½ç„¶æˆåŠŸå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œä½†å¿½ç•¥äº†åŒ»å­¦åˆ†å‰²é¢†åŸŸå­˜åœ¨çš„å®é™…é—®é¢˜ï¼šè¿‡æ‹Ÿåˆå’Œå°æ•°æ®é›†ã€‚è¿‡äºå¤æ‚çš„æ·±åº¦ç¥ç»ç½‘ç»œä¼šä¸å¿…è¦åœ°æå–æ— æ„ä¹‰çš„ä¿¡æ¯ï¼Œè€Œä¸”å¤§å¤šæ•°ç½‘ç»œéƒ½ä¸é€‚ç”¨äºè‚ºéƒ¨CTå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸¤ä¸ªå±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å…¨è¿‡ç¨‹ç½‘ç»œï¼Œèåˆäº†å…ˆè¿›çš„UNet++æ¨¡å‹ã€‚è¯¥ç½‘ç»œåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°æ®å¢å¼ºã€ä¼˜åŒ–ç¥ç»ç½‘ç»œã€å‚æ•°å¾®è°ƒã€‚é€šè¿‡ç»“åˆå¤šç§æ–¹æ³•ï¼Œè®­ç»ƒç»“æœç›¸è¾ƒäºç±»ä¼¼å·¥ä½œå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä»¥æœ€ä½çš„è¿‡æ‹Ÿåˆé£é™©å®ç°äº†é«˜è¾¾98.03%çš„é¢†å…ˆå‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ç½‘ç»œä½œä¸ºé¦–æ‰¹ä¸“æ³¨äºè‚ºéƒ¨CTå›¾åƒçš„ç½‘ç»œä¹‹ä¸€ï¼Œè¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02428v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå­˜åœ¨çš„è¿‡æ‹Ÿåˆå’Œå°æ•°æ®é›†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èåˆUNet++æ¨¡å‹çš„æ–°å…¨ç¨‹ç½‘ç»œã€‚è¯¥ç½‘ç»œåŒ…å«æ•°æ®å¢å¼ºã€ä¼˜åŒ–ç¥ç»ç½‘ç»œå’Œå‚æ•°å¾®è°ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œé€šè¿‡é‡‡ç”¨å¤šç§æ–¹æ³•ï¼Œåœ¨CTè‚ºåˆ‡ç‰‡å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†98.03%ï¼Œå¹¶å…·æœ‰è¾ƒä½è¿‡æ‹Ÿåˆæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹å¦‚U-NetåŠå…¶å˜ä½“è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆå’Œå°æ•°æ®é›†é—®é¢˜ã€‚</li>
<li>è¿‡å¤æ‚çš„æ·±åº¦ç¥ç»ç½‘ç»œå¯èƒ½æå–æ— æ„ä¹‰çš„ä¿¡æ¯ï¼Œä¸”å¤šæ•°æ¨¡å‹ä¸é€‚ç”¨äºè‚ºåˆ‡ç‰‡CTå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å…¨ç¨‹ç½‘ç»œï¼Œèåˆäº†UNet++æ¨¡å‹ã€‚</li>
<li>è¯¥ç½‘ç»œåŒ…å«æ•°æ®å¢å¼ºã€ä¼˜åŒ–ç¥ç»ç½‘ç»œå’Œå‚æ•°å¾®è°ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šç§æ–¹æ³•ï¼Œè¯¥ç½‘ç»œåœ¨CTè‚ºåˆ‡ç‰‡å›¾åƒåˆ†å‰²ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå‡†ç¡®ç‡é«˜è¾¾98.03%ã€‚</li>
<li>è¯¥ç½‘ç»œå…·æœ‰è¾ƒä½è¿‡æ‹Ÿåˆæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53eb5228a1b33f7fd0755ee1dd7a57b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-030cd8a7c50025e4d86dbcaff7a6817a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69f4543f9fea796fbc470f275fb9f4c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09c969e718683cba2a6b107357523f06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2f99c3777436a20fb985c5214dc5191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2ca15cf495b809166f8fa60b9d3f9c3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="tCURLoRA-Tensor-CUR-Decomposition-Based-Low-Rank-Parameter-Adaptation-for-Medical-Image-Segmentation"><a href="#tCURLoRA-Tensor-CUR-Decomposition-Based-Low-Rank-Parameter-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation   for Medical Image Segmentation"></a>tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation   for Medical Image Segmentation</h2><p><strong>Authors:Guanghua He, Wangang Cheng, Hancan Zhu, Xiaohao Cai, Gaohang Yu</strong></p>
<p>Transfer learning, by leveraging knowledge from pre-trained models, has significantly enhanced the performance of target tasks. However, as deep neural networks scale up, full fine-tuning introduces substantial computational and storage challenges in resource-constrained environments, limiting its widespread adoption. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed to reduce computational complexity and storage requirements by minimizing the number of updated parameters. While matrix decomposition-based PEFT methods, such as LoRA, show promise, they struggle to fully capture the high-dimensional structural characteristics of model weights. In contrast, high-dimensional tensors offer a more natural representation of neural network weights, allowing for a more comprehensive capture of higher-order features and multi-dimensional interactions. In this paper, we propose tCURLoRA, a novel fine-tuning method based on tensor CUR decomposition. By concatenating pre-trained weight matrices into a three-dimensional tensor and applying tensor CUR decomposition, we update only the lower-order tensor components during fine-tuning, effectively reducing computational and storage overhead. Experimental results demonstrate that tCURLoRA outperforms existing PEFT methods in medical image segmentation tasks. </p>
<blockquote>
<p>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è¿›è¡Œè¿ç§»å­¦ä¹ å·²æ˜¾è‘—æé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œå…¨å¾®è°ƒåœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¼•å…¥äº†å·¨å¤§çš„è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼€å‘äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æ›´æ–°å‚æ•°çš„æ•°é‡æ¥é™ä½è®¡ç®—å¤æ‚æ€§å’Œå­˜å‚¨è¦æ±‚ã€‚è™½ç„¶åŸºäºçŸ©é˜µåˆ†è§£çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥å®Œå…¨æ•è·æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚ç›¸åï¼Œé«˜ç»´å¼ é‡æä¾›äº†ç¥ç»ç½‘ç»œæƒé‡çš„æ›´è‡ªç„¶è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ é‡CURåˆ†è§£çš„æ–°å‹å¾®è°ƒæ–¹æ³•tCURLoRAã€‚é€šè¿‡å°†é¢„è®­ç»ƒæƒé‡çŸ©é˜µæ‹¼æ¥æˆä¸‰ç»´å¼ é‡å¹¶åº”ç”¨å¼ é‡CURåˆ†è§£ï¼Œæˆ‘ä»¬åªåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ï¼Œä»è€Œæœ‰æ•ˆåœ°é™ä½äº†è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒtCURLoRAä¼˜äºç°æœ‰çš„PEFTæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02227v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œè¿ç§»å­¦ä¹ å·²æ˜¾è‘—æé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒå¸¦æ¥äº†èµ„æºå—é™ç¯å¢ƒä¸­è®¡ç®—å’Œå­˜å‚¨æ–¹é¢çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼€å‘äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä»¥å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¤æ‚æ€§ï¼Œå¹¶æœ€å°åŒ–æ›´æ–°çš„å‚æ•°æ•°é‡ã€‚å°½ç®¡åŸºäºçŸ©é˜µåˆ†è§£çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥å®Œå…¨æ•è·æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚ç›¸åï¼Œé«˜ç»´å¼ é‡æä¾›æ›´è‡ªç„¶çš„ç¥ç»ç½‘ç»œæƒé‡è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ é‡CURåˆ†è§£çš„æ–°å‹å¾®è°ƒæ–¹æ³•tCURLoRAã€‚é€šè¿‡å°†é¢„è®­ç»ƒæƒé‡çŸ©é˜µè¿æ¥æˆä¸‰ç»´å¼ é‡å¹¶åº”ç”¨å¼ é‡CURåˆ†è§£ï¼Œæˆ‘ä»¬åªåœ¨å¾®è°ƒæœŸé—´æ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ï¼Œæœ‰æ•ˆå‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚å®éªŒç»“æœè¯æ˜tCURLoRAåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰PEFTæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿ç§»å­¦ä¹ åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†æå‡äº†ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å®Œå…¨å¾®è°ƒæ·±åº¦ç¥ç»ç½‘ç»œåœ¨èµ„æºå—é™ç¯å¢ƒä¸­é¢ä¸´è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ—¨åœ¨å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰çŸ©é˜µåˆ†è§£æ–¹æ³•å¦‚LoRAéš¾ä»¥æ•è·æ¨¡å‹æƒé‡çš„å…¨éƒ¨é«˜ç»´ç»“æ„ç‰¹å¾ã€‚</li>
<li>é«˜ç»´å¼ é‡ä¸ºç¥ç»ç½‘ç»œæƒé‡æä¾›æ›´è‡ªç„¶çš„è¡¨ç¤ºï¼Œæœ‰åŠ©äºæ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚</li>
<li>tCURLoRAæ˜¯ä¸€ç§æ–°å‹å¾®è°ƒæ–¹æ³•ï¼ŒåŸºäºå¼ é‡CURåˆ†è§£ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°æ›´æ–°æ¨¡å‹æƒé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0f4daf143d5aa7fab7c3e6851c4aec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6bf710db4bd5452d28845b6d6998552.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Phase-Retrieval-by-Quaternionic-Reweighted-Amplitude-Flow-on-Image-Reconstruction"><a href="#Phase-Retrieval-by-Quaternionic-Reweighted-Amplitude-Flow-on-Image-Reconstruction" class="headerlink" title="Phase Retrieval by Quaternionic Reweighted Amplitude Flow on Image   Reconstruction"></a>Phase Retrieval by Quaternionic Reweighted Amplitude Flow on Image   Reconstruction</h2><p><strong>Authors:Ren Hu, Pan Lian</strong></p>
<p>Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images, demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches. </p>
<blockquote>
<p>å››å…ƒä¿¡å·å¤„ç†æŠ€æœ¯æä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼Œèƒ½å¤Ÿé€šè¿‡å››å…ƒä»£æ•°ä¿ç•™ä¿¡å·ç»´åº¦ä¹‹é—´çš„å†…åœ¨å…³è”æ¥æœ‰æ•ˆåœ°ç®¡ç†å½©è‰²ä¿¡å·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å››å…ƒç›¸ä½æ¢å¤é—®é¢˜ï¼Œç³»ç»Ÿåœ°å¼€å‘äº†åŸºäºæŒ¯å¹…æ¨¡å‹çš„æ–°å‹ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†å››å…ƒé‡åŠ æƒæŒ¯å¹…æµï¼ˆQRAFï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡å…¶ä¸‰ç§å˜ä½“å¾—åˆ°äº†è¿›ä¸€æ­¥çš„å¢å¼ºï¼šå¢é‡ã€åŠ é€Ÿå’Œè‡ªé€‚åº”QRAFç®—æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å››å…ƒæ‰°åŠ¨æŒ¯å¹…æµï¼ˆQPAFï¼‰ç®—æ³•ï¼Œå®ƒå…·æœ‰çº¿æ€§æ”¶æ•›æ€§ã€‚å¯¹åˆæˆæ•°æ®å’ŒçœŸå®å›¾åƒçš„å¤§é‡æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ¢å¤æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02180v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¤„ç†ä¸­ï¼Œå››å…ƒæ•°ä¿¡å·å¤„ç†é€šè¿‡ä¿ç•™ä¿¡å·ç»´åº¦é—´çš„å†…åœ¨å…³è”æä¾›äº†æœ‰æ•ˆçš„å·¥å…·ã€‚æœ¬æ–‡è§£å†³å››å…ƒæ•°ç›¸ä½æ¢å¤é—®é¢˜ï¼ŒåŸºäºæŒ¯å¹…æ¨¡å‹ç³»ç»Ÿåœ°å¼€å‘æ–°å‹ç®—æ³•ã€‚æå‡ºå››å…ƒæ•°é‡åŠ æƒæŒ¯å¹…æµç®—æ³•åŠå…¶å¢é‡ã€åŠ é€Ÿå’Œè‡ªé€‚åº”å˜ä½“ï¼Œå¹¶å¼•å…¥å››å…ƒæ•°æ‰°åŠ¨æŒ¯å¹…æµç®—æ³•ï¼Œå…·æœ‰çº¿æ€§æ”¶æ•›æ€§ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•æ˜¾è‘—æé«˜äº†æ¢å¤æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å››å…ƒæ•°ä¿¡å·å¤„ç†èƒ½å¤Ÿé«˜æ•ˆåœ°ç®¡ç†é¢œè‰²ä¿¡å·ï¼Œä¿æŒä¿¡å·ç»´åº¦é—´çš„å†…åœ¨å…³è”ã€‚</li>
<li>æœ¬æ–‡è§£å†³å››å…ƒæ•°ç›¸ä½æ¢å¤é—®é¢˜ï¼Œè¿™æ˜¯ä¿¡å·å¤„ç†ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºæŒ¯å¹…æ¨¡å‹çš„å››å…ƒæ•°é‡åŠ æƒæŒ¯å¹…æµç®—æ³•ï¼ˆQRAFï¼‰åŠå…¶æ”¹è¿›ç‰ˆæœ¬ã€‚</li>
<li>å¼•å…¥å››å…ƒæ•°æ‰°åŠ¨æŒ¯å¹…æµï¼ˆQPAFï¼‰ç®—æ³•ï¼Œå…·æœ‰çº¿æ€§æ”¶æ•›ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¤§é‡æ•°å€¼å®éªŒéªŒè¯äº†æ‰€æç®—æ³•åœ¨åˆæˆæ•°æ®å’ŒçœŸå®å›¾åƒä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æç®—æ³•æ˜¾è‘—æé«˜æ¢å¤æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8516cf6fc276882e4a1b1e95221afecc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Tree-NET-Enhancing-Medical-Image-Segmentation-Through-Efficient-Low-Level-Feature-Training"><a href="#Tree-NET-Enhancing-Medical-Image-Segmentation-Through-Efficient-Low-Level-Feature-Training" class="headerlink" title="Tree-NET: Enhancing Medical Image Segmentation Through Efficient   Low-Level Feature Training"></a>Tree-NET: Enhancing Medical Image Segmentation Through Efficient   Low-Level Feature Training</h2><p><strong>Authors:Orhan Demirci, Bulent Yilmaz</strong></p>
<p>This paper introduces Tree-NET, a novel framework for medical image segmentation that leverages bottleneck feature supervision to enhance both segmentation accuracy and computational efficiency. While previous studies have employed bottleneck feature supervision, their applications have largely been limited to the training phase, offering no computational benefits during training or evaluation. To the best of our knowledge, this study is the first to propose a framework that incorporates two additional training phases for segmentation models, utilizing bottleneck features at both input and output stages. This approach significantly improves computational performance by reducing input and output dimensions with a negligible addition to parameter count, without compromising accuracy. Tree-NET features a three-layer architecture comprising Encoder-Net and Decoder-Net, which are autoencoders designed to compress input and label data, respectively, and Bridge-Net, a segmentation framework that supervises the bottleneck features. By focusing on dense, compressed representations, Tree-NET enhances operational efficiency and can be seamlessly integrated into existing segmentation models without altering their internal structures or increasing model size. We evaluate Tree-NET on two critical segmentation tasks â€“ skin lesion and polyp segmentation â€“ using various backbone models, including U-NET variants and Polyp-PVT. Experimental results demonstrate that Tree-NET reduces FLOPs by a factor of 4 to 13 and decreases memory usage, while achieving comparable or superior accuracy compared to the original architectures. These findings underscore Tree-NETâ€™s potential as a robust and efficient solution for medical image segmentation. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Tree-NETï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚å®ƒåˆ©ç”¨ç“¶é¢ˆç‰¹å¾ç›‘ç£æ¥æé«˜åˆ†å‰²ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²ç»ä½¿ç”¨äº†ç“¶é¢ˆç‰¹å¾ç›‘ç£ï¼Œä½†å®ƒä»¬çš„åº”ç”¨å¤§å¤šä»…é™äºè®­ç»ƒé˜¶æ®µï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­å¹¶æ²¡æœ‰æä¾›è®¡ç®—ä¸Šçš„ä¼˜åŠ¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡æå‡ºäº†ä¸€ç§ç»“åˆä¸¤ä¸ªé¢å¤–è®­ç»ƒé˜¶æ®µçš„åˆ†å‰²æ¨¡å‹æ¡†æ¶ï¼Œåœ¨è¾“å…¥å’Œè¾“å‡ºé˜¶æ®µéƒ½åˆ©ç”¨ç“¶é¢ˆç‰¹å¾ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å‡å°‘è¾“å…¥å’Œè¾“å‡ºç»´åº¦ï¼Œåœ¨å‚æ•°è®¡æ•°å¢åŠ å¾®ä¹å…¶å¾®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ€§èƒ½ï¼ŒåŒæ—¶ä¸æŸå®³å‡†ç¡®æ€§ã€‚Tree-NETé‡‡ç”¨ä¸‰å±‚æ¶æ„ï¼ŒåŒ…æ‹¬Encoder-Netå’ŒDecoder-Netï¼Œå®ƒä»¬æ˜¯åˆ†åˆ«è®¾è®¡æ¥å‹ç¼©è¾“å…¥å’Œæ ‡ç­¾æ•°æ®çš„è‡ªç¼–ç å™¨ï¼Œä»¥åŠBridge-Netï¼Œä¸€ä¸ªç›‘ç£ç“¶é¢ˆç‰¹å¾çš„åˆ†å‰²æ¡†æ¶ã€‚é€šè¿‡å…³æ³¨å¯†é›†ã€å‹ç¼©çš„è¡¨ç¤ºå½¢å¼ï¼ŒTree-NETæé«˜äº†æ“ä½œæ•ˆç‡ï¼Œå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ç°æœ‰çš„åˆ†å‰²æ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€æ”¹å˜å…¶å†…éƒ¨ç»“æ„æˆ–å¢åŠ æ¨¡å‹å¤§å°ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…³é”®çš„åˆ†å‰²ä»»åŠ¡â€”â€”çš®è‚¤ç—…å˜å’Œæ¯è‚‰åˆ†å‰²ä¸Šè¯„ä¼°äº†Tree-NETï¼Œä½¿ç”¨äº†å„ç§åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬U-NETå˜ä½“å’ŒPolyp-PVTã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTree-NETå°†FLOPså‡å°‘äº†4åˆ°13å€ï¼Œå¹¶å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶å®ç°äº†ä¸åŸå§‹æ¶æ„ç›¸å½“æˆ–æ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¿™äº›å‘ç°çªå‡ºäº†Tree-NETä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç¨³å¥é«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02140v1">PDF</a> This manuscript is 10 pages long, includes 10 figures and 3 tables,   and presents a novel framework for medical image segmentation. It has been   submitted to the Medical Image Analysis journal for review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTree-NETçš„æ–°å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç“¶é¢ˆç‰¹å¾ç›‘ç£æ¥æé«˜åˆ†å‰²ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚Tree-NETé‡‡ç”¨ä¸‰å±‚æ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºå‹ç¼©è¾“å…¥æ•°æ®çš„Encoder-Netå’Œç”¨äºå‹ç¼©æ ‡ç­¾æ•°æ®çš„Decoder-Netï¼Œä»¥åŠBridge-Netåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹ç“¶é¢ˆç‰¹å¾è¿›è¡Œç›‘ç£ã€‚å®ƒé€šè¿‡å…³æ³¨å¯†é›†ã€å‹ç¼©çš„è¡¨ç¤ºå½¢å¼æ¥æé«˜æ“ä½œæ•ˆç‡ï¼Œå¹¶å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ç°æœ‰çš„åˆ†å‰²æ¨¡å‹ä¸­ï¼Œè€Œä¸æ”¹å˜å…¶å†…éƒ¨ç»“æ„æˆ–å¢åŠ æ¨¡å‹å¤§å°ã€‚åœ¨çš®è‚¤ç—…å˜å’Œæ¯è‚‰åˆ†å‰²ç­‰å…³é”®åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTree-NETåœ¨è¾¾åˆ°æˆ–è¶…è¿‡åŸå§‹æ¶æ„å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†FLOPså’Œå†…å­˜ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tree-NETæ˜¯ä¸€ä¸ªæ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨ç“¶é¢ˆç‰¹å¾ç›‘ç£æ¥æé«˜åˆ†å‰²ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>ç›¸æ¯”ä¹‹å‰çš„ç ”ç©¶ï¼ŒTree-NETé¦–æ¬¡å°†ç“¶é¢ˆç‰¹å¾ç›‘ç£åº”ç”¨äºè¾“å…¥å’Œè¾“å‡ºé˜¶æ®µï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ€§èƒ½ã€‚</li>
<li>Tree-NETå…·æœ‰ä¸‰å±‚æ¶æ„ï¼ŒåŒ…æ‹¬Encoder-Netã€Decoder-Netå’ŒBridge-Netï¼Œåˆ†åˆ«ç”¨äºå‹ç¼©è¾“å…¥æ•°æ®å’Œæ ‡ç­¾æ•°æ®ï¼Œä»¥åŠè¿›è¡Œåˆ†å‰²ã€‚</li>
<li>Tree-NETå…³æ³¨å¯†é›†ã€å‹ç¼©çš„è¡¨ç¤ºå½¢å¼ï¼Œæé«˜äº†æ“ä½œæ•ˆç‡ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰åˆ†å‰²æ¨¡å‹ä¸­ã€‚</li>
<li>Tree-NETåœ¨çš®è‚¤ç—…å˜å’Œæ¯è‚‰åˆ†å‰²ç­‰å…³é”®ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æ½œåŠ›ã€‚</li>
<li>Tree-NETåœ¨å‡å°‘FLOPså’Œå†…å­˜ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c447ad40434b406dd4f659312ceb4aa5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a14e00656918f7e3cc0396f1b83695f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d7ff40e2decefa879be8cb2a0a8299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64247b6a9562a73c9c2716fc2af05613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8209933b757fc55f1e35e6f4a69f699d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e10911260718f9f2aaf5b68ba4707de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c48410ad19c174dada82a72a2c1e5579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b56878f330d79f1559e4586f51aa643e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RadHop-Net-A-Lightweight-Radiomics-to-Error-Regression-for-False-Positive-Reduction-In-MRI-Prostate-Cancer-Detection"><a href="#RadHop-Net-A-Lightweight-Radiomics-to-Error-Regression-for-False-Positive-Reduction-In-MRI-Prostate-Cancer-Detection" class="headerlink" title="RadHop-Net: A Lightweight Radiomics-to-Error Regression for False   Positive Reduction In MRI Prostate Cancer Detection"></a>RadHop-Net: A Lightweight Radiomics-to-Error Regression for False   Positive Reduction In MRI Prostate Cancer Detection</h2><p><strong>Authors:Vasileios Magoulianitis, Jiaxin Yang, Catherine A. Alexander, C. -C. Jay Kuo</strong></p>
<p>Clinically significant prostate cancer (csPCa) is a leading cause of cancer death in men, yet it has a high survival rate if diagnosed early. Bi-parametric MRI (bpMRI) reading has become a prominent screening test for csPCa. However, this process has a high false positive (FP) rate, incurring higher diagnostic costs and patient discomfort. This paper introduces RadHop-Net, a novel and lightweight CNN for FP reduction. The pipeline consists of two stages: Stage 1 employs data driven radiomics to extract candidate ROIs. In contrast, Stage 2 expands the receptive field about each ROI using RadHop-Net to compensate for the predicted error from Stage 1. Moreover, a novel loss function for regression problems is introduced to balance the influence between FPs and true positives (TPs). RadHop-Net is trained in a radiomics-to-error manner, thus decoupling from the common voxel-to-label approach. The proposed Stage 2 improves the average precision (AP) in lesion detection from 0.407 to 0.468 in the publicly available pi-cai dataset, also maintaining a significantly smaller model size than the state-of-the-art. </p>
<blockquote>
<p>ä¸´åºŠæ˜¾è‘—å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰æ˜¯ç”·æ€§ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œä½†æ—©æœŸè¯Šæ–­åçš„å­˜æ´»ç‡è¾ƒé«˜ã€‚åŒå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆbpMRIï¼‰é˜…è¯»å·²æˆä¸ºcsPCaçš„é‡è¦ç­›æŸ¥æµ‹è¯•ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹å…·æœ‰è¾ƒé«˜çš„å‡é˜³æ€§ç‡ï¼ˆFPï¼‰ï¼Œå¯¼è‡´è¯Šæ–­æˆæœ¬å¢åŠ å’Œæ‚£è€…ä¸é€‚ã€‚æœ¬æ–‡ä»‹ç»äº†RadHop-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è½»é‡çº§CNNï¼Œç”¨äºé™ä½FPç‡ã€‚è¯¥ç®¡é“åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé‡‡ç”¨æ•°æ®é©±åŠ¨æ”¾å°„å­¦æ–¹æ³•æå–å€™é€‰ROIã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨RadHop-Netæ‰©å¤§æ¯ä¸ªROIçš„æ¥æ”¶åœºï¼Œä»¥å¼¥è¡¥ç¬¬ä¸€é˜¶æ®µé¢„æµ‹è¯¯å·®ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§ç”¨äºå›å½’é—®é¢˜çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡FPå’ŒçœŸé˜³æ€§ï¼ˆTPï¼‰ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚RadHop-Netä»¥æ”¾å°„å­¦åˆ°è¯¯å·®çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»è€Œä¸å¸¸è§çš„ä½“ç´ åˆ°æ ‡ç­¾æ–¹æ³•è§£è€¦ã€‚æ‰€æå‡ºç¬¬äºŒé˜¶æ®µåœ¨å…¬å¼€çš„pi-caiæ•°æ®é›†ä¸Šå°†ç—…å˜æ£€æµ‹çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ä»0.407æé«˜åˆ°0.468ï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°ä¹Ÿæ˜¾è‘—å°äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02066v1">PDF</a> 5 pages, 4 figures - Accepted to IEEE International Symposium on   Biomedical Imaging (ISBI 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå‡å°‘å‰åˆ—è…ºç™ŒåŒå‚æ•°MRIï¼ˆbpMRIï¼‰æ£€æµ‹ä¸­å‡é˜³æ€§ç‡ï¼ˆFPï¼‰çš„æ–°å‹è½»é‡åŒ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆRadHop-Netï¼‰ã€‚RadHop-NetåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ•°æ®é©±åŠ¨æ”¾å°„å­¦æå–å€™é€‰æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨RadHop-Netæ‰©å¤§æ¯ä¸ªROIçš„æ„ŸçŸ¥åœºä»¥è¡¥å¿ç¬¬ä¸€é˜¶æ®µçš„é¢„æµ‹è¯¯å·®ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é€‚ç”¨äºå›å½’é—®é¢˜çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡å‡é˜³æ€§å’ŒçœŸé˜³æ€§ä¹‹é—´çš„æ¯”ç‡ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„ä½“ç´ åˆ°æ ‡ç­¾æ–¹æ³•ï¼ŒRadHop-Neté€šè¿‡æ”¾å°„å­¦åˆ°è¯¯å·®çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å…¬å¼€æ•°æ®é›†pi-caiä¸Šå®ç°äº†è¾ƒé«˜çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ï¼ŒåŒæ—¶æ¨¡å‹ä½“ç§¯è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadHop-Netæ˜¯ä¸€ç§æ–°å‹çš„è½»é‡åŒ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œç”¨äºå‡å°‘å‰åˆ—è…ºç™ŒåŒå‚æ•°MRIæ£€æµ‹ä¸­çš„å‡é˜³æ€§ç‡ã€‚</li>
<li>RadHop-NetåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µæå–å€™é€‰æ„Ÿå…´è¶£åŒºåŸŸï¼Œç¬¬äºŒé˜¶æ®µæ‰©å¤§æ„ŸçŸ¥åœºä»¥è¡¥å¿é¢„æµ‹è¯¯å·®ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§é€‚ç”¨äºå›å½’é—®é¢˜çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡å‡é˜³æ€§å’ŒçœŸé˜³æ€§ä¹‹é—´çš„æ¯”ç‡ã€‚</li>
<li>RadHop-Netåœ¨å…¬å¼€æ•°æ®é›†pi-caiä¸Šå®ç°äº†è¾ƒé«˜çš„å¹³å‡ç²¾åº¦ï¼Œä»0.407æé«˜åˆ°äº†0.468ã€‚</li>
<li>RadHop-Neté€šè¿‡æ”¾å°„å­¦åˆ°è¯¯å·®çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä¸ä¼ ç»Ÿçš„ä½“ç´ åˆ°æ ‡ç­¾æ–¹æ³•ä¸åŒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿è¯æ£€æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ¨¡å‹ä½“ç§¯è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6aac59a67f5aaaab7baca80b17dc04d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50392b5d3a714f2e5fe08374ec6fdbe7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebb1c53ac18287e385eccdd03ad81b88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff465a0c4a3992529eb8498ca51e3da8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Advancing-Pancreatic-Cancer-Prediction-with-a-Next-Visit-Token-Prediction-Head-on-top-of-Med-BERT"><a href="#Advancing-Pancreatic-Cancer-Prediction-with-a-Next-Visit-Token-Prediction-Head-on-top-of-Med-BERT" class="headerlink" title="Advancing Pancreatic Cancer Prediction with a Next Visit Token   Prediction Head on top of Med-BERT"></a>Advancing Pancreatic Cancer Prediction with a Next Visit Token   Prediction Head on top of Med-BERT</h2><p><strong>Authors:Jianping He, Laila Rasmy, Degui Zhi, Cui Tao</strong></p>
<p>Background: Recently, numerous foundation models pretrained on extensive data have demonstrated efficacy in disease prediction using Electronic Health Records (EHRs). However, there remains some unanswered questions on how to best utilize such models especially with very small fine-tuning cohorts. Methods: We utilized Med-BERT, an EHR-specific foundation model, and reformulated the disease binary prediction task into a token prediction task and a next visit mask token prediction task to align with Med-BERTâ€™s pretraining task format in order to improve the accuracy of pancreatic cancer (PaCa) prediction in both few-shot and fully supervised settings. Results: The reformulation of the task into a token prediction task, referred to as Med-BERT-Sum, demonstrates slightly superior performance in both few-shot scenarios and larger data samples. Furthermore, reformulating the prediction task as a Next Visit Mask Token Prediction task (Med-BERT-Mask) significantly outperforms the conventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to 7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These findings highlight that aligning the downstream task with Med-BERTâ€™s pretraining objectives substantially enhances the modelâ€™s predictive capabilities, thereby improving its effectiveness in predicting both rare and common diseases. Conclusion: Reformatting disease prediction tasks to align with the pretraining of foundation models enhances prediction accuracy, leading to earlier detection and timely intervention. This approach improves treatment effectiveness, survival rates, and overall patient outcomes for PaCa and potentially other cancers. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæœ€è¿‘ï¼Œè®¸å¤šåœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åœ¨åˆ©ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è¿›è¡Œç–¾ç—…é¢„æµ‹æ–¹é¢æ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•æœ€å¥½åœ°åˆ©ç”¨è¿™äº›æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨å¾®è°ƒæ•°æ®é›†éå¸¸å°çš„æƒ…å†µä¸‹ï¼‰ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æœªè§£å†³çš„é—®é¢˜ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨äº†é’ˆå¯¹EHRçš„Med-BERTæ¨¡å‹ï¼Œå¹¶å°†ç–¾ç—…äºŒåˆ†ç±»é¢„æµ‹ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºä»¤ç‰Œé¢„æµ‹ä»»åŠ¡å’Œä¸‹æ¬¡å°±è¯Šæ©ç ä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ï¼Œä»¥ä¾¿ä¸Med-BERTçš„é¢„è®­ç»ƒä»»åŠ¡æ ¼å¼å¯¹é½ï¼Œä»è€Œæé«˜åœ¨å°‘æ ·æœ¬å’Œå®Œå…¨ç›‘ç£è®¾ç½®ä¸‹èƒ°è…ºç™Œï¼ˆPaCaï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç»“æœï¼šå°†ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ï¼ˆç§°ä¸ºMed-BERT-Sumï¼‰åœ¨å°‘æ ·æœ¬åœºæ™¯å’Œè¾ƒå¤§çš„æ•°æ®æ ·æœ¬ä¸­éƒ½è¡¨ç°å‡ºç•¥å¾®ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†é¢„æµ‹ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºä¸‹æ¬¡å°±è¯Šæ©ç ä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ï¼ˆMed-BERT-Maskï¼‰åœ¨æ•°æ®è§„æ¨¡ä»10åˆ°500ä¸ªæ ·æœ¬çš„å°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œä¸ä¼ ç»Ÿçš„äºŒåˆ†ç±»ï¼ˆBCï¼‰é¢„æµ‹ä»»åŠ¡ï¼ˆMed-BERT-BCï¼‰ç›¸æ¯”ï¼Œå…¶è¡¨ç°æé«˜äº†3%è‡³7%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡ä¸Med-BERTçš„é¢„è®­ç»ƒç›®æ ‡å¯¹é½ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œä»è€Œæé«˜å…¶åœ¨é¢„æµ‹ç½•è§å’Œå¸¸è§ç–¾ç—…æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç»“è®ºï¼šé‡æ–°æ ¼å¼åŒ–ç–¾ç—…é¢„æµ‹ä»»åŠ¡ï¼Œä»¥ä¸åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒå¯¹é½ï¼Œå¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œä»è€Œå®ç°æ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„ã€‚è¿™ç§æ–¹æ³•æé«˜äº†èƒ°è…ºç™Œå’Œå…¶ä»–æ½œåœ¨ç™Œç—‡çš„æ²»ç–—æ•ˆæœã€å­˜æ´»ç‡å’Œæ€»ä½“æ‚£è€…é¢„åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02044v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨Med-BERTè¿™ä¸€ç”µå­å¥åº·è®°å½•ç‰¹å®šåŸºç¡€æ¨¡å‹è¿›è¡Œç–¾ç—…é¢„æµ‹ä»»åŠ¡çš„é‡æ„ï¼ŒåŒ…æ‹¬æ ‡è®°é¢„æµ‹ä»»åŠ¡å’Œåç»­è®¿é—®æ©ç æ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜èƒ°è…ºç™Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨å°‘é‡æ ·æœ¬å’Œå®Œå…¨ç›‘ç£æ¡ä»¶ä¸‹ï¼Œä»»åŠ¡é‡æ„å‡å±•ç°å‡ºè½»å¾®ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡é‡æ„ä¸ºNext Visit Mask Token Predictionï¼ˆMed-BERT-Maskï¼‰æ—¶ï¼Œå…¶åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­ç›¸è¾ƒäºä¼ ç»Ÿçš„Binary Classificationï¼ˆMed-BERT-BCï¼‰é¢„æµ‹ä»»åŠ¡æœ‰3%-7%çš„æå‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸‹æ¸¸ä»»åŠ¡ä¸Med-BERTé¢„è®­ç»ƒç›®æ ‡çš„å¯¹é½èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œå¯¹ç½•è§ç–¾ç—…å’Œå¸¸è§ç–¾ç—…çš„é¢„æµ‹å‡æœ‰ç§¯æå½±å“ã€‚æ”¹é©ç–¾ç—…é¢„æµ‹ä»»åŠ¡æ ¼å¼èƒ½æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œä»è€Œæœ‰åŠ©äºæ—©æœŸå‘ç°å¹¶åŠæ—¶å¹²é¢„ï¼Œå¯¹èƒ°è…ºç™Œå’Œå…¶ä»–ç™Œç—‡çš„æ²»ç–—æ•ˆæœå’Œæ‚£è€…ç”Ÿå­˜ç‡äº§ç”Ÿç§¯æå½±å“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>åˆ©ç”¨Med-BERTè¿›è¡Œç–¾ç—…é¢„æµ‹ä»»åŠ¡çš„é‡æ„æ˜¯æé«˜å‡†ç¡®æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å°†é¢„æµ‹ä»»åŠ¡è½¬æ¢ä¸ºæ ‡è®°é¢„æµ‹ä»»åŠ¡å’Œåç»­è®¿é—®æ©ç æ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œè½»å¾®æå‡äº†åœ¨å°‘é‡æ ·æœ¬å’Œå®Œå…¨ç›‘ç£æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>Next Visit Mask Token Predictionä»»åŠ¡ï¼ˆMed-BERT-Maskï¼‰åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Binary Classificationé¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>å¯¹é½ä¸‹æ¸¸ä»»åŠ¡ä¸Med-BERTçš„é¢„è®­ç»ƒç›®æ ‡èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>æ”¹é©ç–¾ç—…é¢„æµ‹ä»»åŠ¡æ ¼å¼æœ‰åŠ©äºæé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¯¹æ—©æœŸå‘ç°å’ŒåŠæ—¶å¹²é¢„æœ‰ç§¯æå½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c525512bdd540e09cc6cd369dcffa4e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01233c475e6a9e153ec2963b640eecd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1645de84bae1baeeeb3d2d6e7a66bded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc715c90c41ac1c62b633afa31665676.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CRRG-CLIP-Automatic-Generation-of-Chest-Radiology-Reports-and-Classification-of-Chest-Radiographs"><a href="#CRRG-CLIP-Automatic-Generation-of-Chest-Radiology-Reports-and-Classification-of-Chest-Radiographs" class="headerlink" title="CRRG-CLIP: Automatic Generation of Chest Radiology Reports and   Classification of Chest Radiographs"></a>CRRG-CLIP: Automatic Generation of Chest Radiology Reports and   Classification of Chest Radiographs</h2><p><strong>Authors:Jianfei Xu, Thanet Markchom, Huizhi Liang</strong></p>
<p>The complexity of stacked imaging and the massive number of radiographs make writing radiology reports complex and inefficient. Even highly experienced radiologists struggle to maintain accuracy and consistency in interpreting radiographs under prolonged high-intensity work. To address these issues, this work proposes the CRRG-CLIP Model (Chest Radiology Report Generation and Radiograph Classification Model), an end-to-end model for automated report generation and radiograph classification. The model consists of two modules: the radiology report generation module and the radiograph classification module. The generation module uses Faster R-CNN to identify anatomical regions in radiographs, a binary classifier to select key regions, and GPT-2 to generate semantically coherent reports. The classification module uses the unsupervised Contrastive Language Image Pretraining (CLIP) model, addressing the challenges of high-cost labelled datasets and insufficient features. The results show that the generation module performs comparably to high-performance baseline models on BLEU, METEOR, and ROUGE-L metrics, and outperformed the GPT-4o model on BLEU-2, BLEU-3, BLEU-4, and ROUGE-L metrics. The classification module significantly surpasses the state-of-the-art model in AUC and Accuracy. This demonstrates that the proposed model achieves high accuracy, readability, and fluency in report generation, while multimodal contrastive training with unlabelled radiograph-report pairs enhances classification performance. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒçš„å¤æ‚æ€§ä»¥åŠå¤§é‡çš„æ”¾å°„å½±åƒä½¿å¾—ä¹¦å†™æ”¾å°„å­¦æŠ¥å‘Šå˜å¾—å¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚å³ä½¿åœ¨é•¿æ—¶é—´é«˜å¼ºåº¦å·¥ä½œä¸‹ï¼Œç»éªŒä¸°å¯Œçš„æ”¾å°„ç§‘åŒ»ç”Ÿä¹Ÿå¾ˆéš¾ä¿æŒè§£è¯»æ”¾å°„å½±åƒçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†CRRG-CLIPæ¨¡å‹ï¼ˆèƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œæ”¾å°„å½±åƒåˆ†ç±»æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆå’Œæ”¾å°„å½±åƒåˆ†ç±»çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å—å’Œæ”¾å°„å½±åƒåˆ†ç±»æ¨¡å—ã€‚ç”Ÿæˆæ¨¡å—ä½¿ç”¨Faster R-CNNè¯†åˆ«æ”¾å°„å½±åƒä¸­çš„è§£å‰–åŒºåŸŸï¼Œä½¿ç”¨äºŒå…ƒåˆ†ç±»å™¨é€‰æ‹©å…³é”®åŒºåŸŸï¼Œå¹¶ä½¿ç”¨GPT-2ç”Ÿæˆè¯­ä¹‰è¿è´¯çš„æŠ¥å‘Šã€‚åˆ†ç±»æ¨¡å—ä½¿ç”¨æ— ç›‘ç£çš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ï¼Œè§£å†³äº†é«˜æˆæœ¬æ ‡ç­¾æ•°æ®é›†å’Œç‰¹å¾ä¸è¶³çš„æŒ‘æˆ˜ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨BLEUã€METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šï¼Œç”Ÿæˆæ¨¡å—çš„æ€§èƒ½ä¸é«˜æ€§èƒ½åŸºçº¿æ¨¡å‹ç›¸å½“ï¼Œåœ¨BLEU-2ã€BLEU-3ã€BLEU-4å’ŒROUGE-LæŒ‡æ ‡ä¸Šä¼˜äºGPT-4oæ¨¡å‹ã€‚åˆ†ç±»æ¨¡å—åœ¨AUCå’Œå‡†ç¡®åº¦æ–¹é¢è¿œè¿œè¶…è¿‡äº†æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™è¯æ˜æ‰€æå‡ºçš„æ¨¡å‹åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†é«˜å‡†ç¡®æ€§ã€å¯è¯»æ€§å’Œæµç•…æ€§ï¼Œä½¿ç”¨æ— æ ‡ç­¾çš„æ”¾å°„å½±åƒ-æŠ¥å‘Šå¯¹è¿›è¡Œå¤šæ¨¡å¼å¯¹æ¯”è®­ç»ƒå¢å¼ºäº†åˆ†ç±»æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01989v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä½¿ç”¨CRRG-CLIPæ¨¡å‹è§£å†³æ”¾å°„å½±åƒå­¦æŠ¥å‘Šç”Ÿæˆå’Œæ”¾å°„å›¾åƒåˆ†ç±»é—®é¢˜çš„å¤æ‚æ€§å’ŒæŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å—å’Œæ”¾å°„å›¾åƒåˆ†ç±»æ¨¡å—ã€‚ç”Ÿæˆæ¨¡å—ä½¿ç”¨Faster R-CNNè¯†åˆ«æ”¾å°„å›¾åƒä¸­çš„è§£å‰–åŒºåŸŸï¼Œä½¿ç”¨äºŒå…ƒåˆ†ç±»å™¨é€‰æ‹©å…³é”®åŒºåŸŸï¼Œå¹¶ä½¿ç”¨GPT-2ç”Ÿæˆè¯­ä¹‰è¿è´¯çš„æŠ¥å‘Šã€‚åˆ†ç±»æ¨¡å—é‡‡ç”¨æ— ç›‘ç£å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ï¼Œè§£å†³äº†é«˜æˆæœ¬æ ‡ç­¾æ•°æ®é›†å’Œç‰¹å¾ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€å¯è¯»æ€§å’Œæµç•…æ€§ï¼Œå¹¶åœ¨åˆ†ç±»æ€§èƒ½æ–¹é¢é€šè¿‡å¤šæ¨¡æ€å¯¹æ¯”è®­ç»ƒå¢å¼ºäº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œæ”¾å°„å›¾åƒåˆ†ç±»é¢ä¸´å¤æ‚æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>CRRG-CLIPæ¨¡å‹åŒ…æ‹¬æŠ¥å‘Šç”Ÿæˆå’Œå›¾åƒåˆ†ç±»ä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>ç”Ÿæˆæ¨¡å—ä½¿ç”¨Faster R-CNNè¯†åˆ«è§£å‰–åŒºåŸŸï¼ŒäºŒå…ƒåˆ†ç±»å™¨é€‰å…³é”®åŒºåŸŸï¼ŒGPT-2ç”ŸæˆæŠ¥å‘Šã€‚</li>
<li>åˆ†ç±»æ¨¡å—é‡‡ç”¨æ— ç›‘ç£çš„CLIPæ¨¡å‹ï¼Œè§£å†³é«˜æˆæœ¬æ ‡ç­¾æ•°æ®é›†å’Œç‰¹å¾ä¸è¶³é—®é¢˜ã€‚</li>
<li>æ¨¡å‹åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€å¯è¯»æ€§å’Œæµç•…æ€§ã€‚</li>
<li>å¤šæ¨¡æ€å¯¹æ¯”è®­ç»ƒå¢å¼ºäº†æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16f89145e5091936150bd5b51d472e38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d77e0aaa6f81158e02b849925f51e2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5be0615d9e09e6c5e872049ad3082c9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Scale-wise-Bidirectional-Alignment-Network-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Scale-wise-Bidirectional-Alignment-Network-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Scale-wise Bidirectional Alignment Network for Referring Remote Sensing   Image Segmentation"></a>Scale-wise Bidirectional Alignment Network for Referring Remote Sensing   Image Segmentation</h2><p><strong>Authors:Kun Li, George Vosselman, Michael Ying Yang</strong></p>
<p>The goal of referring remote sensing image segmentation (RRSIS) is to extract specific pixel-level regions within an aerial image via a natural language expression. Recent advancements, particularly Transformer-based fusion designs, have demonstrated remarkable progress in this domain. However, existing methods primarily focus on refining visual features using language-aware guidance during the cross-modal fusion stage, neglecting the complementary vision-to-language flow. This limitation often leads to irrelevant or suboptimal representations. In addition, the diverse spatial scales of ground objects in aerial images pose significant challenges to the visual perception capabilities of existing models when conditioned on textual inputs. In this paper, we propose an innovative framework called Scale-wise Bidirectional Alignment Network (SBANet) to address these challenges for RRSIS. Specifically, we design a Bidirectional Alignment Module (BAM) with learnable query tokens to selectively and effectively represent visual and linguistic features, emphasizing regions associated with key tokens. BAM is further enhanced with a dynamic feature selection block, designed to provide both macro- and micro-level visual features, preserving global context and local details to facilitate more effective cross-modal interaction. Furthermore, SBANet incorporates a text-conditioned channel and spatial aggregator to bridge the gap between the encoder and decoder, enhancing cross-scale information exchange in complex aerial scenarios. Extensive experiments demonstrate that our proposed method achieves superior performance in comparison to previous state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both quantitatively and qualitatively. The code will be released after publication. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼æå–èˆªç©ºå›¾åƒä¸­ç‰¹å®šçš„åƒç´ çº§åŒºåŸŸã€‚æœ€è¿‘çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„èåˆè®¾è®¡ï¼Œå·²ç»åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è·¨æ¨¡æ€èåˆé˜¶æ®µçš„ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„æŒ‡å¯¼ä¸‹å¯¹è§†è§‰ç‰¹å¾çš„ç²¾ç‚¼ï¼Œå¿½è§†äº†äº’è¡¥çš„è§†åˆ°è¯­è¨€çš„æµç¨‹ã€‚è¿™ä¸€å±€é™æ€§é€šå¸¸ä¼šå¯¼è‡´æ— å…³æˆ–æ¬¡ä¼˜çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œèˆªç©ºå›¾åƒä¸­åœ°é¢å¯¹è±¡å¤šæ ·çš„ç©ºé—´å°ºåº¦ç»™ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬è¾“å…¥æ¡ä»¶ä¸‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºScale-wise Bidirectional Alignment Networkï¼ˆSBANetï¼‰çš„åˆ›æ–°æ¡†æ¶ï¼Œä»¥è§£å†³RRSISçš„è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¸¦æœ‰å¯å­¦ä¹ æŸ¥è¯¢ä»¤ç‰Œçš„åŒå‘å¯¹é½æ¨¡å—ï¼ˆBAMï¼‰ï¼Œä»¥é€‰æ‹©æ€§å’Œæœ‰æ•ˆåœ°è¡¨ç¤ºè§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¹¶å¼ºè°ƒä¸å…³é”®ä»¤ç‰Œç›¸å…³çš„åŒºåŸŸã€‚BAMè¿›ä¸€æ­¥é€šè¿‡ä¸€ä¸ªåŠ¨æ€ç‰¹å¾é€‰æ‹©å—å¢å¼ºåŠŸèƒ½ï¼Œæ—¨åœ¨æä¾›å®è§‚å’Œå¾®è§‚å±‚é¢çš„è§†è§‰ç‰¹å¾ï¼Œä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼ŒSBANetè¿˜é‡‡ç”¨äº†æ–‡æœ¬æ¡ä»¶é€šé“å’Œç©ºé—´èšåˆå™¨ï¼Œä»¥ç¼©å°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å·®è·ï¼Œå¢å¼ºå¤æ‚èˆªç©ºåœºæ™¯ä¸­è·¨å°ºåº¦ä¿¡æ¯çš„äº¤æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸RRSIS-Då’ŒRefSegRSæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢å‡å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨å‘è¡¨åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00851v2">PDF</a> Under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼æå–èˆªç©ºå›¾åƒä¸­çš„ç‰¹å®šåƒç´ çº§åŒºåŸŸã€‚æœ€è¿‘çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„èåˆè®¾è®¡ï¼Œå·²ç»åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨è¯­è¨€æ„ŸçŸ¥æŒ‡å¯¼æ¥ä¼˜åŒ–è§†è§‰ç‰¹å¾ï¼Œå¿½è§†äº†ä»è§†è§‰åˆ°è¯­è¨€çš„äº’è¡¥æµåŠ¨ã€‚è¿™ç§å±€é™æ€§é€šå¸¸ä¼šå¯¼è‡´è¡¨ç¤ºä¸ç›¸å…³æˆ–æ¬¡ä¼˜ã€‚æ­¤å¤–ï¼Œèˆªç©ºå›¾åƒä¸­åœ°é¢å¯¹è±¡çš„ç©ºé—´å°ºåº¦å¤šæ ·æ€§å¯¹ç°æœ‰æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºScale-wise Bidirectional Alignment Network (SBANet)çš„åˆ›æ–°æ¡†æ¶ï¼Œä»¥è§£å†³RRSISçš„è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¾è®¡äº†å¸¦æœ‰å¯å­¦ä¹ æŸ¥è¯¢ä»¤ç‰Œçš„åŒå‘å¯¹é½æ¨¡å—ï¼ˆBAMï¼‰ï¼Œä»¥é€‰æ‹©å’Œæœ‰æ•ˆåœ°è¡¨ç¤ºè§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¹¶å¼ºè°ƒä¸å…³é”®ä»¤ç‰Œç›¸å…³çš„åŒºåŸŸã€‚BAMè¿›ä¸€æ­¥é€šè¿‡åŠ¨æ€ç‰¹å¾é€‰æ‹©å—å¾—åˆ°å¢å¼ºï¼Œæ—¨åœ¨æä¾›å®è§‚å’Œå¾®è§‚çš„è§†è§‰ç‰¹å¾ï¼Œä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼ŒSBANetç»“åˆäº†æ–‡æœ¬æ¡ä»¶é€šé“å’Œç©ºé—´èšåˆå™¨ï¼Œä»¥ç¼©å°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å·®è·ï¼Œå¢å¼ºå¤æ‚èˆªç©ºåœºæ™¯ä¸­çš„è·¨å°ºåº¦ä¿¡æ¯äº¤æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸RRSIS-Då’ŒRefSegRSæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨å‘è¡¨åå‘å¸ƒã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰æ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼ä»èˆªç©ºå›¾åƒä¸­æå–ç‰¹å®šåƒç´ çº§åŒºåŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºä½¿ç”¨è¯­è¨€æ„ŸçŸ¥æŒ‡å¯¼åœ¨è·¨æ¨¡æ€èåˆé˜¶æ®µä¼˜åŒ–è§†è§‰ç‰¹å¾ï¼Œä½†å¿½è§†äº†ä»è§†è§‰åˆ°è¯­è¨€çš„äº’è¡¥æµåŠ¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSBANetçš„åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡åŒå‘å¯¹é½æ¨¡å—ï¼ˆBAMï¼‰å’ŒåŠ¨æ€ç‰¹å¾é€‰æ‹©å—æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>BAMèƒ½å¤Ÿé€‰æ‹©å’Œæœ‰æ•ˆåœ°è¡¨ç¤ºè§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¹¶å¼ºè°ƒä¸å…³é”®ä»¤ç‰Œç›¸å…³çš„åŒºåŸŸã€‚</li>
<li>SBANetç»“åˆäº†æ–‡æœ¬æ¡ä»¶é€šé“å’Œç©ºé—´èšåˆå™¨ï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€äº¤äº’å’Œè·¨å°ºåº¦ä¿¡æ¯äº¤æ¢ã€‚</li>
<li>åœ¨RRSIS-Då’ŒRefSegRSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSBANetåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</li>
<li>ä»£ç å°†åœ¨å‘è¡¨åå…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1681bdff4427cc5556d8eb305d378db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae0bdaa2996ad99ef9e3525e7d18f10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac78ec9652d3e9182b19fbd228a1402c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1d509c1b91da142ba209a1f1ceed360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0608761fac1c4b53ee49cce9fba5b77f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-080eaa7731b784bb78736c8780ef1aaa.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  LlamaPartialSpoof An LLM-Driven Fake Speech Dataset Simulating   Disinformation Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-51b9ea5615eb65a30a2a57f256e8d2d2.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  STAR Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">8671.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
