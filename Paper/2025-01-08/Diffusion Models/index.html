<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  STAR Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-51b9ea5615eb65a30a2a57f256e8d2d2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-08-æ›´æ–°"><a href="#2025-01-08-æ›´æ–°" class="headerlink" title="2025-01-08 æ›´æ–°"></a>2025-01-08 æ›´æ–°</h1><h2 id="STAR-Spatial-Temporal-Augmentation-with-Text-to-Video-Models-for-Real-World-Video-Super-Resolution"><a href="#STAR-Spatial-Temporal-Augmentation-with-Text-to-Video-Models-for-Real-World-Video-Super-Resolution" class="headerlink" title="STAR: Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution"></a>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution</h2><p><strong>Authors:Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, Ying Tai</strong></p>
<p>Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce\textbf{<del>\name} (\textbf{S}patial-\textbf{T}emporal \textbf{A}ugmentation with T2V models for \textbf{R}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate\textbf{</del>\name}~outperforms state-of-the-art methods on both synthetic and real-world datasets. </p>
<blockquote>
<p>å›¾åƒæ‰©æ•£æ¨¡å‹å·²è¢«é€‚åº”äºçœŸå®ä¸–ç•Œè§†é¢‘è¶…åˆ†è¾¨ç‡ï¼Œä»¥è§£å†³åŸºäºGANçš„æ–¹æ³•ä¸­çš„è¿‡åº¦å¹³æ»‘é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç»´æŒæ—¶é—´ä¸€è‡´æ€§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åœ¨é™æ€å›¾åƒä¸Šè®­ç»ƒçš„ï¼Œæ— æ³•æœ‰æ•ˆåœ°æ•æ‰æ—¶é—´åŠ¨æ€ã€‚å°†æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹æ•´åˆåˆ°è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­ä»¥æ”¹è¿›æ—¶é—´å»ºæ¨¡æ˜¯ç›´æ¥çš„ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç°å®ä¸–ç•Œåœºæ™¯ä¸­å¤æ‚é€€åŒ–å¼•èµ·çš„ä¼ªå½±ï¼Œä»¥åŠç”±äºå¼ºå¤§çš„T2Væ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›è€Œç‰ºç‰²çš„ä¿çœŸåº¦ï¼ˆä¾‹å¦‚CogVideoX-5Bï¼‰ã€‚ä¸ºäº†æé«˜æ¢å¤è§†é¢‘çš„æ—¶ç©ºè´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºSTARçš„æ–°æ–¹æ³•ï¼ˆåˆ©ç”¨T2Væ¨¡å‹è¿›è¡ŒçœŸå®ä¸–ç•Œè§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ—¶ç©ºå¢å¼ºï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å…¨å±€æ³¨æ„åŠ›å—ä¹‹å‰å¼•å…¥äº†ä¸€ä¸ªå±€éƒ¨ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼ˆLIEMï¼‰ï¼Œä»¥ä¸°å¯Œå±€éƒ¨ç»†èŠ‚å¹¶å‡è½»é€€åŒ–ä¼ªå½±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€é¢‘ç‡ï¼ˆDFï¼‰æŸå¤±æ¥åŠ å¼ºä¿çœŸåº¦ï¼Œå¼•å¯¼æ¨¡å‹åœ¨ä¸åŒçš„æ‰©æ•£æ­¥éª¤ä¸­å…³æ³¨ä¸åŒçš„é¢‘ç‡åˆ†é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSTARåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02976v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­é‡åˆ°çš„è¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç°å®ä¸–ç•Œçš„è§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ—¶ç©ºè´¨é‡ã€‚é€šè¿‡å¼•å…¥å±€éƒ¨ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼ˆLIEMï¼‰å’ŒåŠ¨æ€é¢‘ç‡ï¼ˆDFï¼‰æŸå¤±ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå®ç°çœŸå®çš„ç©ºé—´ç»†èŠ‚å’Œç¨³å¥çš„ä¿çœŸåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ‰©æ•£æ¨¡å‹è¢«ç”¨äºç°å®ä¸–ç•Œçš„è§†é¢‘è¶…åˆ†è¾¨ç‡ä»¥æé«˜æ€§èƒ½å¹¶è§£å†³è¿‡åº¦å¹³æ»‘é—®é¢˜ã€‚</li>
<li>ç»“åˆæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ä»¥æ”¹å–„è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­çš„æ—¶é—´å»ºæ¨¡ã€‚</li>
<li>é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç°å®åœºæ™¯ä¸­å¤æ‚é€€åŒ–å¼•å…¥çš„ä¼ªå½±ä»¥åŠå¼ºå¤§T2Væ¨¡å‹å¦¥åçš„ä¿çœŸåº¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•â€”â€”STARï¼ˆæ—¶ç©ºå¢å¼ºä¸T2Væ¨¡å‹ç»“åˆç”¨äºç°å®ä¸–ç•Œçš„è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼‰ï¼Œå®ç°äº†çœŸå®çš„ç©ºé—´ç»†èŠ‚å’Œç¨³å¥çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨STARæ–¹æ³•ä¸­ï¼Œå¼•å…¥äº†å±€éƒ¨ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼ˆLIEMï¼‰æ¥ä¸°å¯Œå±€éƒ¨ç»†èŠ‚å¹¶å‡è½»é€€åŒ–ä¼ªå½±ã€‚</li>
<li>æå‡ºäº†åŠ¨æ€é¢‘ç‡ï¼ˆDFï¼‰æŸå¤±ä»¥å¢å¼ºä¿çœŸåº¦ï¼Œå¼•å¯¼æ¨¡å‹åœ¨æ‰©æ•£æ­¥éª¤ä¸­å…³æ³¨ä¸åŒçš„é¢‘ç‡æˆåˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ebdced5b4b81215f9c4c078eeb4ce12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43da50125f0eea42467b29cbd3249d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d627edcce481be1431c3221a56fa2020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b580c8988ca4433443f09f059ec23d40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65fd9f311e3460d89e73f7887602354d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f190fa195252ea7359e88f449465ba9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SceneVTG-Controllable-Multilingual-Visual-Text-Generation-in-the-Wild"><a href="#SceneVTG-Controllable-Multilingual-Visual-Text-Generation-in-the-Wild" class="headerlink" title="SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild"></a>SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild</h2><p><strong>Authors:Jiawei Liu, Yuanzhi Zhu, Feiyu Gao, Zhibo Yang, Peng Wang, Junyang Lin, Xinggang Wang, Wenyu Liu</strong></p>
<p>Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, cartoons, etc.), the text in natural scene images needs to meet the following four key criteria: (1) Fidelity: the generated text should appear as realistic as a photograph and be completely accurate, with no errors in any of the strokes. (2) Reasonability: the text should be generated on reasonable carrier areas (such as boards, signs, walls, etc.), and the generated text content should also be relevant to the scene. (3) Utility: the generated text can facilitate to the training of natural scene OCR (Optical Character Recognition) tasks. (4) Controllability: The attribute of the text (such as font and color) should be controllable as needed.In this paper, we propose a two stage method, SceneVTG++, which simultaneously satisfies the four aspects mentioned above. SceneVTG++ consists of a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former utilizes the world knowledge of multi modal large language models to find reasonable text areas and recommend text content according to the nature scene background images, while the latter generates controllable multilingual text based on the diffusion model. Through extensive experiments, we respectively verified the effectiveness of TLCG and CLTD, and demonstrated the state-of-the-art text generation performance of SceneVTG++. In addition, the generated images have superior utility in OCR tasks like text detection and text recognition. Codes and datasets will be available. </p>
<blockquote>
<p>ç”Ÿæˆè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„è§†è§‰æ–‡æœ¬æ˜¯ä¸€é¡¹å…·æœ‰è®¸å¤šæœªè§£å†³é—®é¢˜æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸åœ¨äººå·¥è®¾è®¡çš„å›¾åƒï¼ˆå¦‚æµ·æŠ¥ã€å°é¢ã€æ¼«ç”»ç­‰ï¼‰ä¸Šç”Ÿæˆæ–‡æœ¬ä¸åŒï¼Œè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„æ–‡æœ¬éœ€è¦æ»¡è¶³ä»¥ä¸‹å››ä¸ªå…³é”®æ ‡å‡†ï¼šï¼ˆ1ï¼‰ä¿çœŸåº¦ï¼šç”Ÿæˆçš„æ–‡æœ¬åº”çœ‹èµ·æ¥å°½å¯èƒ½é€¼çœŸï¼Œå¹¶ä¸”å®Œå…¨å‡†ç¡®ï¼Œæ²¡æœ‰ä»»ä½•ç¬”è§¦é”™è¯¯ã€‚ï¼ˆ2ï¼‰åˆç†æ€§ï¼šæ–‡æœ¬åº”ç”Ÿæˆåœ¨åˆç†çš„è½½ä½“åŒºåŸŸï¼ˆå¦‚æ¿æŠ¥ã€æ ‡å¿—ã€å¢™å£ç­‰ï¼‰ï¼Œå¹¶ä¸”ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ä¹Ÿåº”ä¸åœºæ™¯ç›¸å…³ã€‚ï¼ˆ3ï¼‰å®ç”¨æ€§ï¼šç”Ÿæˆçš„æ–‡æœ¬æœ‰åŠ©äºè‡ªç„¶åœºæ™¯OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰ä»»åŠ¡çš„è®­ç»ƒã€‚ï¼ˆ4ï¼‰å¯æ§æ€§ï¼šæ–‡æœ¬å±æ€§ï¼ˆå¦‚å­—ä½“å’Œé¢œè‰²ï¼‰åº”æ ¹æ®éœ€è¦å¯æ§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•SceneVTG++ï¼ŒåŒæ—¶æ»¡è¶³ä¸Šè¿°å››ä¸ªæ–¹é¢ã€‚SceneVTG++åŒ…æ‹¬æ–‡æœ¬å¸ƒå±€å’Œå†…å®¹ç”Ÿæˆå™¨ï¼ˆTLCGï¼‰å’Œå¯æ§å±€éƒ¨æ–‡æœ¬æ‰©æ•£ï¼ˆCLTDï¼‰ã€‚å‰è€…åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œæ ¹æ®è‡ªç„¶åœºæ™¯èƒŒæ™¯å›¾åƒæ‰¾åˆ°åˆç†çš„æ–‡æœ¬åŒºåŸŸï¼Œå¹¶æ¨èæ–‡æœ¬å†…å®¹ï¼›åè€…åŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯æ§çš„å¤šè¯­è¨€æ–‡æœ¬ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬åˆ†åˆ«éªŒè¯äº†TLCGå’ŒCLTDçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†SceneVTG++çš„å…ˆè¿›æ–‡æœ¬ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„å›¾åƒåœ¨OCRä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ï¼‰ä¸­å…·æœ‰å‡ºè‰²çš„å®ç”¨æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02962v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•SceneVTG++ï¼Œç”¨äºç”Ÿæˆè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„è§†è§‰æ–‡æœ¬ï¼ŒåŒæ—¶æ»¡è¶³çœŸå®æ€§ã€åˆç†æ€§ã€å®ç”¨æ€§å’Œå¯æ§æ€§å››ä¸ªå…³é”®æ ‡å‡†ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æ–‡æœ¬å¸ƒå±€å’Œå†…å®¹ç”Ÿæˆå™¨ï¼ˆTLCGï¼‰å’Œå¯æ§å±€éƒ¨æ–‡æœ¬æ‰©æ•£ï¼ˆCLTDï¼‰ã€‚TLCGåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œæ ¹æ®è‡ªç„¶åœºæ™¯èƒŒæ™¯å›¾åƒæ‰¾åˆ°åˆç†çš„æ–‡æœ¬åŒºåŸŸå¹¶æ¨èæ–‡æœ¬å†…å®¹ã€‚CLTDåˆ™åŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯æ§çš„å¤šè¯­è¨€æ–‡æœ¬ã€‚å®éªŒè¯æ˜TLCGå’ŒCLTDçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†SceneVTG++åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å…ˆè¿›æ€§ï¼ŒåŒæ—¶ç”Ÿæˆçš„å›¾åƒåœ¨OCRä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„å®ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”Ÿæˆè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„è§†è§‰æ–‡æœ¬æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦æ»¡è¶³çœŸå®æ€§ã€åˆç†æ€§ã€å®ç”¨æ€§å’Œå¯æ§æ€§å››ä¸ªå…³é”®æ ‡å‡†ã€‚</li>
<li>SceneVTG++æ˜¯ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬æ–‡æœ¬å¸ƒå±€å’Œå†…å®¹ç”Ÿæˆå™¨ï¼ˆTLCGï¼‰å’Œå¯æ§å±€éƒ¨æ–‡æœ¬æ‰©æ•£ï¼ˆCLTDï¼‰ã€‚</li>
<li>TLCGåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œæ ¹æ®è‡ªç„¶åœºæ™¯èƒŒæ™¯å›¾åƒç”Ÿæˆåˆç†çš„æ–‡æœ¬å¸ƒå±€å’Œå†…å®¹ã€‚</li>
<li>CLTDåŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯æ§çš„å¤šè¯­è¨€æ–‡æœ¬ï¼Œæ»¡è¶³åœºæ™¯æ–‡æœ¬ç”Ÿæˆçš„éœ€æ±‚ã€‚</li>
<li>å®éªŒè¯æ˜TLCGå’ŒCLTDçš„æœ‰æ•ˆæ€§ï¼ŒSceneVTG++åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå…ˆè¿›æ€§ã€‚</li>
<li>ç”Ÿæˆçš„å›¾åƒåœ¨OCRä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„å®ç”¨æ€§ï¼Œå¦‚æ–‡æœ¬æ£€æµ‹å’Œæ–‡æœ¬è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ae7e1a31f968463fc8e487096d7aec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0667badd4bd08bcb00579d9252616336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f884bbd1bc53163cbbc43f4705ebd05e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InpDiffusion-Image-Inpainting-Localization-via-Conditional-Diffusion-Models"><a href="#InpDiffusion-Image-Inpainting-Localization-via-Conditional-Diffusion-Models" class="headerlink" title="InpDiffusion: Image Inpainting Localization via Conditional Diffusion   Models"></a>InpDiffusion: Image Inpainting Localization via Conditional Diffusion   Models</h2><p><strong>Authors:Kai Wang, Shaozhang Niu, Qixian Hao, Jiwei Zhang</strong></p>
<p>As artificial intelligence advances rapidly, particularly with the advent of GANs and diffusion models, the accuracy of Image Inpainting Localization (IIL) has become increasingly challenging. Current IIL methods face two main challenges: a tendency towards overconfidence, leading to incorrect predictions; and difficulty in detecting subtle tampering boundaries in inpainted images. In response, we propose a new paradigm that treats IIL as a conditional mask generation task utilizing diffusion models. Our method, InpDiffusion, utilizes the denoising process enhanced by the integration of image semantic conditions to progressively refine predictions. During denoising, we employ edge conditions and introduce a novel edge supervision strategy to enhance the modelâ€™s perception of edge details in inpainted objects. Balancing the diffusion modelâ€™s stochastic sampling with edge supervision of tampered image regions mitigates the risk of incorrect predictions from overconfidence and prevents the loss of subtle boundaries that can result from overly stochastic processes. Furthermore, we propose an innovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting multi-scale features, enhancing feature representation by considering both semantic and edge conditions of the inpainted images. Extensive experiments across challenging datasets demonstrate that the InpDiffusion significantly outperforms existing state-of-the-art methods in IIL tasks, while also showcasing excellent generalization capabilities and robustness. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å…´èµ·ï¼Œå›¾åƒä¿®å¤å®šä½ï¼ˆIILï¼‰çš„å‡†ç¡®æ€§è¶Šæ¥è¶Šå—åˆ°æŒ‘æˆ˜ã€‚å½“å‰çš„IILæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯è¿‡äºè‡ªä¿¡ï¼Œå¯¼è‡´é¢„æµ‹é”™è¯¯ï¼›äºŒæ˜¯éš¾ä»¥æ£€æµ‹ä¿®å¤å›¾åƒä¸­ç»†å¾®çš„ç¯¡æ”¹è¾¹ç•Œã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†IILè§†ä¸ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶é®ç½©ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•InpDiffusionåˆ©ç”¨ç»“åˆå›¾åƒè¯­ä¹‰æ¡ä»¶å¢å¼ºçš„å»å™ªè¿‡ç¨‹æ¥é€æ­¥ä¼˜åŒ–é¢„æµ‹ã€‚åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨è¾¹ç¼˜æ¡ä»¶å¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹è¾¹ç¼˜ç›‘ç£ç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ä¿®å¤ç‰©ä½“è¾¹ç¼˜ç»†èŠ‚çš„è®¤çŸ¥ã€‚å¹³è¡¡æ‰©æ•£æ¨¡å‹çš„éšæœºé‡‡æ ·ä¸ç¯¡æ”¹å›¾åƒåŒºåŸŸçš„è¾¹ç¼˜ç›‘ç£ï¼Œé™ä½äº†å› è¿‡äºè‡ªä¿¡è€Œå¯¼è‡´é”™è¯¯é¢„æµ‹çš„é£é™©ï¼Œå¹¶é˜²æ­¢äº†å› è¿‡äºéšæœºçš„è¿‡ç¨‹è€Œä¸¢å¤±ç»†å¾®çš„è¾¹ç•Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ›æ–°æ€§çš„åŒæµå¤šå°ºåº¦ç‰¹å¾æå–å™¨ï¼ˆDMFEï¼‰ï¼Œç”¨äºæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œé€šè¿‡è€ƒè™‘ä¿®å¤å›¾åƒçš„è¯­ä¹‰å’Œè¾¹ç¼˜æ¡ä»¶æ¥å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒInpDiffusionåœ¨IILä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02816v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼Œå›¾åƒä¿®å¤å®šä½ï¼ˆIILï¼‰çš„å‡†ç¡®æ€§è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹å½“å‰IILæ–¹æ³•é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡åº¦è‡ªä¿¡å¯¼è‡´é¢„æµ‹é”™è¯¯å’Œéš¾ä»¥æ£€æµ‹ä¿®å¤å›¾åƒä¸­çš„å¾®å¦™ç¯¡æ”¹è¾¹ç•Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„IILæ–¹æ³•â€”â€”InpDiffusionã€‚è¯¥æ–¹æ³•å°†IILè§†ä¸ºæ¡ä»¶é®ç½©ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨å›¾åƒè¯­ä¹‰æ¡ä»¶å¢å¼ºçš„å»å™ªè¿‡ç¨‹é€æ­¥ä¼˜åŒ–é¢„æµ‹ã€‚é€šè¿‡å¼•å…¥è¾¹ç¼˜æ¡ä»¶å’Œä¸€ç§æ–°çš„è¾¹ç¼˜ç›‘ç£ç­–ç•¥ï¼Œå¹³è¡¡æ‰©æ•£æ¨¡å‹çš„éšæœºé‡‡æ ·å’Œç¯¡æ”¹å›¾åƒåŒºåŸŸçš„è¾¹ç¼˜ç›‘ç£ï¼Œå‡å°‘è¿‡åº¦è‡ªä¿¡å¯¼è‡´çš„é¢„æµ‹é”™è¯¯ï¼Œé˜²æ­¢è¿‡äºéšæœºçš„è¿‡ç¨‹å¯¼è‡´çš„å¾®å¦™è¾¹ç•ŒæŸå¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŒæµå¤šå°ºåº¦ç‰¹å¾æå–å™¨ï¼ˆDMFEï¼‰ï¼Œç”¨äºæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œé€šè¿‡è€ƒè™‘ä¿®å¤å›¾åƒçš„è¯­ä¹‰å’Œè¾¹ç¼˜æ¡ä»¶ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒInpDiffusionåœ¨IILä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸€æµæ–¹æ³•ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯GANså’Œæ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œä½¿å¾—å›¾åƒä¿®å¤å®šä½ï¼ˆIILï¼‰çš„å‡†ç¡®æ€§é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰IILæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè¿‡åº¦è‡ªä¿¡å¯¼è‡´é¢„æµ‹é”™è¯¯å’Œéš¾ä»¥æ£€æµ‹ä¿®å¤å›¾åƒä¸­çš„å¾®å¦™ç¯¡æ”¹è¾¹ç•Œã€‚</li>
<li>InpDiffusionæ–¹æ³•å°†IILè§†ä¸ºæ¡ä»¶é®ç½©ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚</li>
<li>InpDiffusionåˆ©ç”¨å›¾åƒè¯­ä¹‰æ¡ä»¶å¢å¼ºçš„å»å™ªè¿‡ç¨‹é€æ­¥ä¼˜åŒ–é¢„æµ‹ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¾¹ç¼˜æ¡ä»¶å’Œè¾¹ç¼˜ç›‘ç£ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹å¯¹ä¿®å¤ç‰©ä½“è¾¹ç¼˜ç»†èŠ‚çš„æ„ŸçŸ¥ã€‚</li>
<li>åŒæµå¤šå°ºåº¦ç‰¹å¾æå–å™¨ï¼ˆDMFEï¼‰ç”¨äºæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d278b4d36bf165c5a97961f8c4324d48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1719852b3b0636a49728c4f44f4efb2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b133d88d2ed0d8210005e3a347ce595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea42b78e280b49955aad93d4894ccadc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01aca24244e2b4af3417beed744209f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54b0f8bc49efb19ba53011160e891cee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Face-MakeUp-Multimodal-Facial-Prompts-for-Text-to-Image-Generation"><a href="#Face-MakeUp-Multimodal-Facial-Prompts-for-Text-to-Image-Generation" class="headerlink" title="Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation"></a>Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation</h2><p><strong>Authors:Dawei Dai, Mingming Jia, Yinxiu Zhou, Hang Xing, Chenghang Li</strong></p>
<p>Facial images have extensive practical applications. Although the current large-scale text-image diffusion models exhibit strong generation capabilities, it is challenging to generate the desired facial images using only text prompt. Image prompts are a logical choice. However, current methods of this type generally focus on general domain. In this paper, we aim to optimize image makeup techniques to generate the desired facial images. Specifically, (1) we built a dataset of 4 million high-quality face image-text pairs (FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to maintain consistency with the reference facial image, we extract&#x2F;learn multi-scale content features and pose features for the facial image, integrating these into the diffusion model to enhance the preservation of facial identity features for diffusion models. Validation on two face-related test datasets demonstrates that our Face-MakeUp can achieve the best comprehensive performance.All codes are available at:<a target="_blank" rel="noopener" href="https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp">https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp</a> </p>
<blockquote>
<p>é¢éƒ¨å›¾åƒå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚è™½ç„¶ç°æœ‰çš„å¤§è§„æ¨¡æ–‡æœ¬-å›¾åƒæ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†ä»…ä½¿ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆæ‰€éœ€çš„é¢éƒ¨å›¾åƒæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å›¾åƒæç¤ºæ˜¯ä¸€ç§åˆç†çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–¹æ³•é€šå¸¸é›†ä¸­åœ¨ä¸€èˆ¬é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨ä¼˜åŒ–åŒ–å¦†æŠ€æœ¯æ¥ç”Ÿæˆæ‰€éœ€çš„é¢éƒ¨å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œ(1)æˆ‘ä»¬åŸºäºLAION-Faceæ„å»ºäº†åŒ…å«4ç™¾ä¸‡ä¸ªé«˜è´¨é‡é¢éƒ¨å›¾åƒæ–‡æœ¬å¯¹çš„æ•°æ®é›†FaceCaptionHQ-4Mï¼Œç”¨äºè®­ç»ƒæˆ‘ä»¬çš„Face-MakeUpæ¨¡å‹ï¼›(2)ä¸ºäº†ä¿æŒä¸å‚è€ƒé¢éƒ¨å›¾åƒçš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå–&#x2F;å­¦ä¹ é¢éƒ¨å›¾åƒçš„å¤šå°ºåº¦å†…å®¹ç‰¹å¾å’Œå§¿æ€ç‰¹å¾ï¼Œå°†è¿™äº›ç‰¹å¾é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹å¯¹é¢éƒ¨èº«ä»½ç‰¹å¾çš„ä¿ç•™èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªä¸é¢éƒ¨ç›¸å…³çš„æµ‹è¯•æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„Face-MakeUpå¯ä»¥è¾¾åˆ°æœ€ä½³çš„ç»¼åˆæ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp">https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02523v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹é¢éƒ¨å›¾åƒç”Ÿæˆä¼˜åŒ–çš„ç ”ç©¶ã€‚æ–‡ä¸­æ„å»ºäº†å¤§è§„æ¨¡çš„é¢éƒ¨å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†FaceCaptionHQ-4Mï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†Face-MakeUpæ¨¡å‹ã€‚æ¨¡å‹åœ¨é›†æˆæ‰©æ•£æ¨¡å‹æ—¶ï¼Œæå–å¤šå°ºåº¦å†…å®¹å’Œå§¿åŠ¿ç‰¹å¾ä»¥ä¿æŒä¸å‚è€ƒé¢éƒ¨å›¾åƒçš„ä¸€è‡´æ€§ï¼Œå¢å¼ºäº†é¢éƒ¨ç‰¹å¾çš„ä¿æŠ¤èƒ½åŠ›ã€‚åœ¨ä¸¤é¡¹é¢éƒ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒFace-MakeUpå–å¾—äº†æœ€ä½³çš„ç»¼åˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ä¸»è¦å…³æ³¨é¢éƒ¨å›¾åƒçš„ç”Ÿæˆä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„é¢éƒ¨å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†FaceCaptionHQ-4Mç”¨äºè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>Face-MakeUpæ¨¡å‹é€šè¿‡æå–å¤šå°ºåº¦å†…å®¹å’Œå§¿åŠ¿ç‰¹å¾æ¥ä¿æŒä¸å‚è€ƒé¢éƒ¨å›¾åƒçš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨é›†æˆæ‰©æ•£æ¨¡å‹æ—¶å¢å¼ºäº†é¢éƒ¨ç‰¹å¾çš„ä¿æŠ¤èƒ½åŠ›ã€‚</li>
<li>Face-MakeUpæ¨¡å‹åœ¨ä¸¤é¡¹é¢éƒ¨æµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³çš„ç»¼åˆæ€§èƒ½ã€‚</li>
<li>æä¾›çš„é“¾æ¥ä¸­åŒ…å«æ¨¡å‹çš„ä»£ç èµ„æºä¾›ä¸‹è½½å’Œå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e974e905af6a350d16f4fbb90c58b1cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c89135600b6792eee3266eaf3b1d84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f6ddf112692682b9f9086760dd080f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab3b11bd0c3d285f80b7215755e0ad1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00e4bd65da6ec90cdb2acb0407700d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b9ea5615eb65a30a2a57f256e8d2d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbba83f1527c5043a46c49007d842e5e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ACE-Instruction-Based-Image-Creation-and-Editing-via-Context-Aware-Content-Filling"><a href="#ACE-Instruction-Based-Image-Creation-and-Editing-via-Context-Aware-Content-Filling" class="headerlink" title="ACE++: Instruction-Based Image Creation and Editing via Context-Aware   Content Filling"></a>ACE++: Instruction-Based Image Creation and Editing via Context-Aware   Content Filling</h2><p><strong>Authors:Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, Jingren Zhou</strong></p>
<p>We report ACE++, an instruction-based diffusion framework that tackles various image generation and editing tasks. Inspired by the input format for the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context Condition Unit (LCU) introduced in ACE and extend this input paradigm to any editing and generation tasks. To take full advantage of image generative priors, we develop a two-stage training scheme to minimize the efforts of finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the first stage, we pre-train the model using task data with the 0-ref tasks from the text-to-image model. There are many models in the community based on the post-training of text-to-image foundational models that meet this training paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with painting tasks and can be used as an initialization to accelerate the training process. In the second stage, we finetune the above model to support the general instructions using all tasks defined in ACE. To promote the widespread application of ACE++ in different scenarios, we provide a comprehensive set of models that cover both full finetuning and lightweight finetuning, while considering general applicability and applicability in vertical scenarios. The qualitative analysis showcases the superiority of ACE++ in terms of generating image quality and prompt following ability. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†ACE++ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ‰©æ•£æ¡†æ¶ï¼Œç”¨äºå¤„ç†å„ç§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚æˆ‘ä»¬å—åˆ°FLUX.1-Fill-devæå‡ºçš„å¡«å……ä»»åŠ¡è¾“å…¥æ ¼å¼çš„å¯å‘ï¼Œæ”¹è¿›äº†ACEä¸­çš„é•¿ä¸Šä¸‹æ–‡æ¡ä»¶å•å…ƒï¼ˆLCUï¼‰ï¼Œå¹¶å°†è¿™ä¸€è¾“å…¥èŒƒå¼æ‰©å±•åˆ°ä»»ä½•ç¼–è¾‘å’Œç”Ÿæˆä»»åŠ¡ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å›¾åƒç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥æœ€å°åŒ–è°ƒæ•´å¼ºå¤§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚FLUX.1-devï¼‰çš„åŠªåŠ›ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„0-refä»»åŠ¡æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ç¤¾åŒºä¸­æœ‰è®¸å¤šåŸºäºæ–‡æœ¬åˆ°å›¾åƒåŸºç¡€æ¨¡å‹çš„åç»­è®­ç»ƒæ¨¡å‹ï¼Œç¬¦åˆç¬¬ä¸€é˜¶æ®µçš„è¿™ç§è®­ç»ƒèŒƒå¼ã€‚ä¾‹å¦‚ï¼ŒFLUX.1-Fill-devä¸»è¦å¤„ç†ç»˜ç”»ä»»åŠ¡ï¼Œå¹¶å¯ä»¥ç”¨ä½œåˆå§‹åŒ–æ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¯¹ä¸Šè¿°æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ä½¿ç”¨ACEä¸­å®šä¹‰çš„æ‰€æœ‰ä»»åŠ¡æ¥æ”¯æŒä¸€èˆ¬æŒ‡ä»¤ã€‚ä¸ºäº†ä¿ƒè¿›ACE++åœ¨ä¸åŒåœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€å¥—å…¨é¢çš„æ¨¡å‹ï¼Œæ¶µç›–å…¨é‡å¾®è°ƒå’Œè½»é‡çº§å¾®è°ƒï¼ŒåŒæ—¶è€ƒè™‘é€šç”¨æ€§å’Œå‚ç›´åœºæ™¯é€‚ç”¨æ€§ã€‚å®šæ€§åˆ†æå±•ç¤ºäº†ACE++åœ¨ç”Ÿæˆå›¾åƒè´¨é‡å’Œéµå¾ªæç¤ºæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02487v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æŠ¥å‘Šäº†ACE++ï¼Œä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ‰©æ•£æ¡†æ¶ï¼Œç”¨äºå¤„ç†å„ç§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚å—FLUX.1-Fill-devå¯¹è¡¥å…¨ä»»åŠ¡è¾“å…¥æ ¼å¼çš„å¯å‘ï¼Œæ”¹è¿›äº†ACEä¸­çš„é•¿ä¸Šä¸‹æ–‡æ¡ä»¶å•å…ƒï¼ˆLCUï¼‰ï¼Œå¹¶å°†æ­¤è¾“å…¥èŒƒå¼æ‰©å±•åˆ°ä»»ä½•ç¼–è¾‘å’Œç”Ÿæˆä»»åŠ¡ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å›¾åƒç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œå¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥æœ€å°åŒ–å¾®è°ƒå¼ºå¤§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚FLUX.1-devï¼‰çš„åŠªåŠ›ã€‚é¦–å…ˆä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„0-refä»»åŠ¡è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œæ¨¡å‹å¾®è°ƒä»¥æ”¯æŒé€šç”¨æŒ‡ä»¤é›†ã€‚æ­¤å¤–ï¼Œæä¾›äº†æ¶µç›–å…¨é‡å¾®è°ƒä¸è½»é‡çº§å¾®è°ƒçš„ç»¼åˆæ¨¡å‹é›†ï¼Œè€ƒè™‘äº†é€šç”¨åº”ç”¨å’Œå‚ç›´åœºæ™¯çš„åº”ç”¨ã€‚ACE++åœ¨ç”Ÿæˆå›¾åƒè´¨é‡å’Œéµå¾ªæç¤ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ACE++æ˜¯ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ‰©æ•£æ¡†æ¶ï¼Œç”¨äºå¤„ç†å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>æ”¹è¿›äº†ACEä¸­çš„é•¿ä¸Šä¸‹æ–‡æ¡ä»¶å•å…ƒï¼ˆLCUï¼‰ï¼Œå¹¶æ‰©å±•äº†è¾“å…¥èŒƒå¼ä»¥æ”¯æŒå¤šç§ä»»åŠ¡ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥åˆ©ç”¨å›¾åƒç”Ÿæˆå…ˆéªŒçŸ¥è¯†å¹¶ç®€åŒ–å¾®è°ƒè¿‡ç¨‹ã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„0-refä»»åŠ¡æ•°æ®ã€‚</li>
<li>FLUX.1-Fill-devç­‰æ¨¡å‹ç¬¦åˆç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œå¯ç”¨äºåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æä¾›æ¶µç›–å…¨é‡å¾®è°ƒä¸è½»é‡çº§å¾®è°ƒçš„æ¨¡å‹é›†ä»¥é€‚åº”ä¸åŒåº”ç”¨åœºæ™¯çš„éœ€æ±‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4db4169f9d5e968125558d9a50e98f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51c331198cddbfc467b25dc11bccb943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fd0b1f89cfebba62a8550ba5afd889c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DeTrack-In-model-Latent-Denoising-Learning-for-Visual-Object-Tracking"><a href="#DeTrack-In-model-Latent-Denoising-Learning-for-Visual-Object-Tracking" class="headerlink" title="DeTrack: In-model Latent Denoising Learning for Visual Object Tracking"></a>DeTrack: In-model Latent Denoising Learning for Visual Object Tracking</h2><p><strong>Authors:Xinyu Zhou, Jinglun Li, Lingyi Hong, Kaixun Jiang, Pinxue Guo, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the modelâ€™s robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets. </p>
<blockquote>
<p>å…ˆå‰è§†è§‰å¯¹è±¡è·Ÿè¸ªæ–¹æ³•é‡‡ç”¨å›¾åƒç‰¹å¾å›å½’æ¨¡å‹æˆ–åæ ‡è‡ªå›å½’æ¨¡å‹è¿›è¡Œè¾¹ç•Œæ¡†é¢„æµ‹ã€‚å›¾åƒç‰¹å¾å›å½’æ–¹æ³•ä¸¥é‡ä¾èµ–äºåŒ¹é…ç»“æœï¼Œå¹¶ä¸åˆ©ç”¨ä½ç½®å…ˆéªŒï¼Œè€Œè‡ªå›å½’æ–¹æ³•åªèƒ½ä½¿ç”¨è®­ç»ƒé›†ä¸­å¯ç”¨çš„è¾¹ç•Œæ¡†è¿›è¡Œè®­ç»ƒï¼Œåœ¨æµ‹è¯•æœªè§æ•°æ®æ—¶å¯èƒ½è¡¨ç°ä¸ä½³ã€‚å—æ‰©æ•£æ¨¡å‹çš„å¯å‘ï¼Œå»å™ªå­¦ä¹ æé«˜äº†æ¨¡å‹å¯¹æœªè§æ•°æ®çš„é²æ£’æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‘è¾¹ç•Œæ¡†å¼•å…¥å™ªå£°ï¼Œç”Ÿæˆç”¨äºè®­ç»ƒçš„å™ªå£°æ¡†ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„é²æ£’æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œå°†è§†è§‰å¯¹è±¡è·Ÿè¸ªé—®é¢˜è¡¨è¿°ä¸ºä¸€ä¸ªå»å™ªå­¦ä¹ è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œé€šå¸¸è¦æ±‚è·Ÿè¸ªç®—æ³•å®æ—¶è¿è¡Œï¼Œç›´æ¥å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå¯¹è±¡è·Ÿè¸ªä¼šä¸¥é‡æŸå®³è·Ÿè¸ªé€Ÿåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å»å™ªå­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºæ¨¡å‹å†…çš„æ¯ä¸ªå»å™ªå—ï¼Œè€Œä¸æ˜¯å¤šæ¬¡è¿è¡Œæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•æ¦‚æ‹¬ä¸ºæ¨¡å‹å†…çš„æ½œåœ¨å»å™ªå­¦ä¹ è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å»å™ªè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œå®ƒç”±å¤šä¸ªå»å™ªå—ç»„æˆã€‚åœ¨å»å™ªå—ä¸­ï¼Œæ¨¡æ¿å’Œæœç´¢åµŒå…¥è¢«æŠ•å°„åˆ°æ¯ä¸ªå»å™ªå—ä½œä¸ºæ¡ä»¶ã€‚å»å™ªå—è´Ÿè´£å»é™¤é¢„æµ‹è¾¹ç•Œæ¡†ä¸­çš„å™ªå£°ï¼Œå¤šä¸ªå †å çš„å»å™ªå—ååŒå®Œæˆæ•´ä¸ªå»å™ªè¿‡ç¨‹ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨å›¾åƒç‰¹å¾å’Œè½¨è¿¹ä¿¡æ¯æ¥ä¼˜åŒ–å»å™ªåçš„è¾¹ç•Œæ¡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨è½¨è¿¹è®°å¿†å’Œè§†è§‰è®°å¿†æ¥æé«˜è·Ÿè¸ªçš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02467v1">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è§†è§‰ç‰©ä½“è·Ÿè¸ªæ–¹æ³•ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å»å™ªå­¦ä¹ æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæé«˜æ¨¡å‹é²æ£’æ€§ï¼Œå¼•å…¥å™ªå£°åˆ°è¾¹ç•Œæ¡†è®­ç»ƒä¸­ï¼Œå¹¶å°†ç‰©ä½“è·Ÿè¸ªé—®é¢˜è½¬åŒ–ä¸ºå»å™ªå­¦ä¹ è¿‡ç¨‹ã€‚ä¸ºæé«˜è·Ÿè¸ªé€Ÿåº¦ï¼Œå°†å»å™ªå­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºæ¨¡å‹å†…çš„å„ä¸ªå»å™ªå—ï¼Œè€Œéå¤šæ¬¡è¿è¡Œæ¨¡å‹ã€‚æå‡ºä¸€ç§å»å™ªè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼ŒåŒ…å«å¤šä¸ªå»å™ªå—ï¼Œåˆ©ç”¨å›¾åƒç‰¹å¾å’Œè½¨è¿¹ä¿¡æ¯ä¼˜åŒ–å»å™ªè¾¹ç•Œæ¡†ï¼ŒåŒæ—¶ä½¿ç”¨è½¨è¿¹è®°å¿†å’Œè§†è§‰è®°å¿†æé«˜è·Ÿè¸ªç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆï¼Œåœ¨å¤šä¸ªæŒ‘æˆ˜æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ‰©æ•£æ¨¡å‹çš„å»å™ªå­¦ä¹ æå‡è§†è§‰ç‰©ä½“è·Ÿè¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥å™ªå£°æé«˜æ¨¡å‹å¯¹æœªè§æ•°æ®çš„é²æ£’æ€§ã€‚</li>
<li>å°†è§†è§‰ç‰©ä½“è·Ÿè¸ªé—®é¢˜è½¬åŒ–ä¸ºå»å™ªå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>ä¸ºæé«˜è·Ÿè¸ªé€Ÿåº¦ï¼Œå°†å»å™ªå­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºæ¨¡å‹å†…çš„å„ä¸ªå»å™ªå—ã€‚</li>
<li>æå‡ºä¸€ç§å»å™ªè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼ŒåŒ…å«å¤šä¸ªå»å™ªå—ã€‚</li>
<li>åˆ©ç”¨å›¾åƒç‰¹å¾å’Œè½¨è¿¹ä¿¡æ¯ä¼˜åŒ–å»å™ªè¾¹ç•Œæ¡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-006f1af14d5b80dc3561fc05ba2a586e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-742cd1775bb7bf69b0589966b39f0dee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5734442099b4dbc31e001d4ff021705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74a541aff93b5c52a4ab0ebecd1f7912.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2665de08831ba71dc330c8513de96fb3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CorrFill-Enhancing-Faithfulness-in-Reference-based-Inpainting-with-Correspondence-Guidance-in-Diffusion-Models"><a href="#CorrFill-Enhancing-Faithfulness-in-Reference-based-Inpainting-with-Correspondence-Guidance-in-Diffusion-Models" class="headerlink" title="CorrFill: Enhancing Faithfulness in Reference-based Inpainting with   Correspondence Guidance in Diffusion Models"></a>CorrFill: Enhancing Faithfulness in Reference-based Inpainting with   Correspondence Guidance in Diffusion Models</h2><p><strong>Authors:Kuan-Hung Liu, Cheng-Kun Yang, Min-Hung Chen, Yu-Lun Liu, Yen-Yu Lin</strong></p>
<p>In the task of reference-based image inpainting, an additional reference image is provided to restore a damaged target image to its original state. The advancement of diffusion models, particularly Stable Diffusion, allows for simple formulations in this task. However, existing diffusion-based methods often lack explicit constraints on the correlation between the reference and damaged images, resulting in lower faithfulness to the reference images in the inpainting results. In this work, we propose CorrFill, a training-free module designed to enhance the awareness of geometric correlations between the reference and target images. This enhancement is achieved by guiding the inpainting process with correspondence constraints estimated during inpainting, utilizing attention masking in self-attention layers and an objective function to update the input tensor according to the constraints. Experimental results demonstrate that CorrFill significantly enhances the performance of multiple baseline diffusion-based methods, including state-of-the-art approaches, by emphasizing faithfulness to the reference images. </p>
<blockquote>
<p>åœ¨åŸºäºå‚è€ƒçš„å›¾åƒä¿®å¤ä»»åŠ¡ä¸­ï¼Œæä¾›é¢å¤–çš„å‚è€ƒå›¾åƒä»¥å°†æŸåçš„ç›®æ ‡å›¾åƒæ¢å¤åˆ°å…¶åŸå§‹çŠ¶æ€ã€‚æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionï¼Œä¸ºæ­¤ä»»åŠ¡æä¾›äº†ç®€å•çš„å…¬å¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨å‚è€ƒå›¾åƒå’Œå—æŸå›¾åƒä¹‹é—´çš„ç›¸å…³æ€§ä¸Šå¾€å¾€ç¼ºä¹æ˜ç¡®çš„çº¦æŸï¼Œå¯¼è‡´ä¿®å¤ç»“æœå¯¹å‚è€ƒå›¾åƒçš„å¿ å®åº¦è¾ƒä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CorrFillï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„è®¾è®¡æ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºå‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´å‡ ä½•å…³ç³»çš„æ„è¯†ã€‚è¿™ç§å¢å¼ºæ˜¯é€šè¿‡åœ¨ä¿®å¤è¿‡ç¨‹ä¸­ä½¿ç”¨åœ¨ä¿®å¤è¿‡ç¨‹ä¸­ä¼°è®¡çš„å¯¹åº”çº¦æŸæ¥å®ç°çš„ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›å±‚ä¸­çš„æ³¨æ„åŠ›æ©ç å’Œå®¢è§‚å‡½æ•°æ¥æ ¹æ®çº¦æŸæ›´æ–°è¾“å…¥å¼ é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCorrFillé€šè¿‡å¼ºè°ƒå¯¹å‚è€ƒå›¾åƒçš„å¿ å®åº¦ï¼Œæ˜¾è‘—æé«˜äº†å¤šç§åŸºçº¿æ‰©æ•£æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02355v1">PDF</a> WACV 2025. Project page: <a target="_blank" rel="noopener" href="https://corrfill.github.io/">https://corrfill.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå‚è€ƒå›¾åƒçš„ä¿®å¤ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°¤å…¶æ˜¯Stable Diffusionçš„è¿›æ­¥ç®€åŒ–äº†ä»»åŠ¡ã€‚ç„¶è€Œç°æœ‰æ‰©æ•£æ–¹æ³•ç¼ºä¹å‚è€ƒå›¾åƒä¸å—æŸå›¾åƒé—´å…³è”çš„æ˜¾å¼çº¦æŸï¼Œå¯¼è‡´ä¿®å¤ç»“æœå¯¹å‚è€ƒå›¾åƒçš„å¿ å®åº¦è¾ƒä½ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨¡å—CorrFillï¼Œé€šè¿‡ä¼°è®¡ä¿®å¤è¿‡ç¨‹ä¸­çš„å¯¹åº”çº¦æŸæ¥å¢å¼ºä¸¤è€…ä¹‹é—´çš„å‡ ä½•å…³è”ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æ©ç å’Œå®¢è§‚å‡½æ•°æ¥æ›´æ–°è¾“å…¥å¼ é‡ä»¥é€‚åº”è¿™äº›çº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCorrFillèƒ½æ˜¾è‘—æé«˜å¤šç§åŸºçº¿æ‰©æ•£æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œé‡ç‚¹æé«˜ç»“æœå¯¹å‚è€ƒå›¾åƒçš„å¿ å®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚è€ƒå›¾åƒä¿®å¤ä»»åŠ¡ä¸­å¼•å…¥äº†æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionï¼Œç®€åŒ–äº†ä»»åŠ¡æµç¨‹ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ–¹æ³•ç¼ºä¹å¯¹å‚è€ƒä¸å—æŸå›¾åƒé—´å…³è”çš„æ˜¾å¼çº¦æŸã€‚</li>
<li>CorrFillæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºå‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´çš„å‡ ä½•å…³è”ã€‚</li>
<li>CorrFillé€šè¿‡ä¼°è®¡ä¿®å¤è¿‡ç¨‹ä¸­çš„å¯¹åº”çº¦æŸæ¥æŒ‡å¯¼ä¿®å¤è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨è‡ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æ©ç å®ç°CorrFillçš„å¢å¼ºåŠŸèƒ½ã€‚</li>
<li>é€šè¿‡å®¢è§‚å‡½æ•°æ›´æ–°è¾“å…¥å¼ é‡ä»¥é€‚åº”å¯¹åº”çº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f7d1f9569452ba0d375bcf70616d514.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f4a78e323c6c8817730efd322bbbeb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44d0506c8872dc2e8243c895ebc3f865.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce375f7f4735f2715294779da2f7f06e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7edd4a1da45054724c2ffa8acc9666b1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models"><a href="#Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models" class="headerlink" title="Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models"></a>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models</h2><p><strong>Authors:Jingfeng Yao, Xinggang Wang</strong></p>
<p>Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochsâ€“representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a>. </p>
<blockquote>
<p>åŸºäºTransformeræ¶æ„çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†è¿™ç§ä¸¤é˜¶æ®µè®¾è®¡ä¸­çš„ä¼˜åŒ–å›°å¢ƒï¼šè™½ç„¶å¢åŠ è§†è§‰æ ‡è®°å™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦æé«˜äº†é‡å»ºè´¨é‡ï¼Œä½†è¦å®ç°ç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ï¼Œéœ€è¦å¤§é‡çš„æ‰©æ•£æ¨¡å‹ä»¥åŠæ›´å¤šçš„è®­ç»ƒè¿­ä»£æ¬¡æ•°ã€‚å› æ­¤ï¼Œç°æœ‰ç³»ç»Ÿé€šå¸¸é€‰æ‹©æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆï¼Œè¦ä¹ˆç”±äºæ ‡è®°å™¨ä¸­çš„ä¿¡æ¯ä¸¢å¤±è€Œäº§ç”Ÿè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆç”±äºè®¡ç®—æˆæœ¬é«˜æ˜‚è€Œæ— æ³•å®Œå…¨æ”¶æ•›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§å›°å¢ƒæºäºå­¦ä¹ æ— çº¦æŸçš„é«˜ç»´æ½œåœ¨ç©ºé—´çš„å›ºæœ‰éš¾åº¦ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåœ¨è®­ç»ƒè§†è§‰æ ‡è®°å™¨æ—¶ï¼Œå°†å…¶æ½œåœ¨ç©ºé—´ä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬æå‡ºçš„VA-VAEï¼ˆè§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½å˜åˆ†è‡ªç¼–ç å™¨ï¼‰æ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œï¼Œä½¿æ‰©æ•£Transformerï¼ˆDiTï¼‰åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­æ›´å¿«æ”¶æ•›ã€‚ä¸ºäº†å……åˆ†å‘æŒ¥VA-VAEçš„æ½œåŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨æ”¹è¿›çš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¢å¼ºçš„DiTåŸºçº¿ï¼Œç§°ä¸ºLightningDiTã€‚é›†æˆç³»ç»Ÿåœ¨ImageNet 256x256ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼ŒFIDå¾—åˆ†ä¸º1.35ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„è®­ç»ƒæ•ˆç‡ï¼Œä»…åœ¨64ä¸ªå‘¨æœŸå†…å°±è¾¾åˆ°äº†2.11çš„FIDå¾—åˆ†ï¼Œä¸åŸå§‹DiTç›¸æ¯”ï¼Œå®ç°äº†è¶…è¿‡21å€çš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚æ¨¡å‹å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT%E3%80%82">https://github.com/hustvl/LightningDiTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01423v2">PDF</a> Models and codes are available at:   <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformeræ¶æ„çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ã€‚ç ”ç©¶æ­ç¤ºäº†è¯¥æ¨¡å‹çš„ä¸¤é˜¶æ®µè®¾è®¡ä¸­çš„ä¼˜åŒ–å›°å¢ƒï¼šæé«˜è§†è§‰æ ‡è®°å™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦å¯æé«˜é‡å»ºè´¨é‡ï¼Œä½†éœ€è¦æ›´å¤§çš„æ‰©æ•£æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒè¿­ä»£æ¥è¾¾åˆ°ç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½è®­ç»ƒè§†è§‰æ ‡è®°å™¨çš„æ–¹æ³•ï¼Œå¹¶æ¨å‡ºäº†VA-VAEï¼ˆä¸è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼‰ã€‚è¿™æ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œï¼Œä½¿æ‰©æ•£Transformerï¼ˆDiTï¼‰åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­æ›´å¿«æ”¶æ•›ã€‚é…åˆå¢å¼ºçš„DiTåŸºçº¿ï¼ˆLightningDiTï¼‰ï¼Œåœ¨ImageNet 256x256ç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼ŒFIDå¾—åˆ†ä¸º1.35ï¼Œå¹¶åœ¨ä»…64ä¸ªå‘¨æœŸå†…è¾¾åˆ°2.11çš„FIDå¾—åˆ†ï¼Œå®ç°äº†è¶…è¿‡21å€çš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹å¯ç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼Œä½†å­˜åœ¨ä¼˜åŒ–å›°å¢ƒï¼šæé«˜è§†è§‰æ ‡è®°å™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦ä¼šæé«˜é‡å»ºè´¨é‡ï¼Œä½†å¢åŠ æ‰©æ•£æ¨¡å‹å¤§å°å’Œè®­ç»ƒè¿­ä»£æ¬¡æ•°ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå¸¸äº§ç”Ÿè§†è§‰ä¼ªå½±æˆ–è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œæ— æ³•å®Œå…¨æ”¶æ•›ã€‚</li>
<li>é—®é¢˜çš„æ ¹æºåœ¨äºå­¦ä¹ æ— çº¦æŸçš„é«˜ç»´æ½œåœ¨ç©ºé—´çš„å†…åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºVA-VAEï¼ˆä¸è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼‰æ¥è§£å†³æ­¤é—®é¢˜ï¼Œæ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œã€‚</li>
<li>VA-VAEé…åˆå¢å¼ºDiTï¼ˆLightningDiTï¼‰å®ç°å¿«é€Ÿæ”¶æ•›ï¼Œåœ¨ImageNet 256x256ç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</li>
<li>LightningDiTåœ¨ImageNetä¸Šå®ç°äº†FID 1.35çš„ä¼˜å¼‚æˆç»©ï¼Œå¹¶åœ¨ä»…64ä¸ªå‘¨æœŸå†…è¾¾åˆ°FID 2.11ï¼Œç›¸æ¯”åŸå§‹DiTæœ‰æ˜¾è‘—çš„è®­ç»ƒæ•ˆç‡æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb1d13f141b61e6932647a2c74b65b37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d0ce253c5009144c05686797926f35d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8877c5be4035d26b5ebd6da24d11bd30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e60dfd4398a443f913d36d2291ba73dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0320301a76d4f34cf5debcf77a8b2246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a5491fc00b9cb05f455c3ee8b31acdd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LVMark-Robust-Watermark-for-latent-video-diffusion-models"><a href="#LVMark-Robust-Watermark-for-latent-video-diffusion-models" class="headerlink" title="LVMark: Robust Watermark for latent video diffusion models"></a>LVMark: Robust Watermark for latent video diffusion models</h2><p><strong>Authors:MinHyuk Jang, Youngdong Jang, JaeHyeok Lee, Kodai Kawamura, Feng Yang, Sangpil Kim</strong></p>
<p>Rapid advancements in generative models have made it possible to create hyper-realistic videos. As their applicability increases, their unauthorized use has raised significant concerns, leading to the growing demand for techniques to protect the ownership of the generative model itself. While existing watermarking methods effectively embed watermarks into image-generative models, they fail to account for temporal information, resulting in poor performance when applied to video-generative models. To address this issue, we introduce a novel watermarking method called LVMark, which embeds watermarks into video diffusion models. A key component of LVMark is a selective weight modulation strategy that efficiently embeds watermark messages into the video diffusion model while preserving the quality of the generated videos. To accurately decode messages in the presence of malicious attacks, we design a watermark decoder that leverages spatio-temporal information in the 3D wavelet domain through a cross-attention module. To the best of our knowledge, our approach is the first to highlight the potential of video-generative model watermarking as a valuable tool for enhancing the effectiveness of ownership protection in video-generative models. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—åˆ›å»ºè¶…ç°å®è§†é¢‘æˆä¸ºå¯èƒ½ã€‚éšç€å…¶é€‚ç”¨æ€§çš„å¢å¼ºï¼Œæœªç»æˆæƒçš„å¹¿æ³›ä½¿ç”¨å¼•å‘äº†é‡å¤§æ‹…å¿§ï¼Œå› æ­¤å¯¹ä¿æŠ¤ç”Ÿæˆæ¨¡å‹æœ¬èº«æ‰€æœ‰æƒçš„æŠ€æœ¯éœ€æ±‚ä¸æ–­å¢é•¿ã€‚è™½ç„¶ç°æœ‰çš„æ°´å°æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æ°´å°åµŒå…¥å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä½†å®ƒä»¬æ— æ³•è€ƒè™‘æ—¶é—´ä¿¡æ¯ï¼Œå½“åº”ç”¨äºè§†é¢‘ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ°´å°æ–¹æ³•â€”â€”LVMarkï¼Œè¯¥æ–¹æ³•å°†æ°´å°åµŒå…¥è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚LVMarkçš„å…³é”®ç»„æˆéƒ¨åˆ†æ˜¯é€‰æ‹©æ€§æƒé‡è°ƒåˆ¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé«˜æ•ˆåœ°å°†æ°´å°ä¿¡æ¯åµŒå…¥è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚ä¸ºäº†å‡†ç¡®è§£ç æ¶æ„æ”»å‡»ä¸­çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ°´å°è§£ç å™¨ï¼Œå®ƒåˆ©ç”¨ä¸‰ç»´å°æ³¢åŸŸä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œé€šè¿‡äº¤å‰æ³¨æ„æ¨¡å—å®ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªå¼ºè°ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹æ°´å°åœ¨å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹æ‰€æœ‰æƒä¿æŠ¤æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09122v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹æŠ€æœ¯è¿…é€Ÿå´›èµ·ï¼Œèƒ½å¤Ÿåœ¨è§†é¢‘é¢†åŸŸç”Ÿæˆè¶…é€¼çœŸçš„å†…å®¹ã€‚éšç€å…¶åº”ç”¨çš„å¹¿æ³›ï¼Œæœªç»æˆæƒçš„ä½¿ç”¨å¼•å‘äº†äººä»¬å¯¹ä¿æŠ¤ç”Ÿæˆæ¨¡å‹æ‰€æœ‰æƒçš„å¼ºçƒˆéœ€æ±‚ã€‚ç°æœ‰çš„æ°´å°æ–¹æ³•è™½ç„¶èƒ½åµŒå…¥å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä½†æ— æ³•å¤„ç†è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ï¼Œå› æ­¤åœ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„åº”ç”¨è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ç§åä¸ºLVMarkçš„æ–°å‹æ°´å°æ–¹æ³•ï¼Œå®ƒèƒ½é€‰æ‹©æ€§åœ°åµŒå…¥è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºé€‰æ‹©æ€§æƒé‡è°ƒåˆ¶ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯è§†é¢‘è´¨é‡çš„åŒæ—¶é«˜æ•ˆåµŒå…¥æ°´å°ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ°´å°è§£ç å™¨ï¼Œé€šè¿‡åˆ©ç”¨ä¸‰ç»´å°æ³¢åŸŸä¸­çš„æ—¶ç©ºä¿¡æ¯ä»¥åŠäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œæ¥æŠµå¾¡æ¶æ„æ”»å‡»å¹¶å‡†ç¡®è§£ç ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼€è¾Ÿäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹æ°´å°æŠ€æœ¯çš„æ–°å¯èƒ½ï¼Œæé«˜äº†ä¿æŠ¤è§†é¢‘ç”Ÿæˆæ¨¡å‹æ‰€æœ‰æƒçš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹æŠ€æœ¯è¿…é€Ÿå‘å±•ï¼Œå¸¦æ¥é«˜åº¦é€¼çœŸçš„è§†é¢‘å†…å®¹åˆ›ä½œèƒ½åŠ›ã€‚</li>
<li>æœªç»æˆæƒçš„ç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¼•å‘æ‰€æœ‰æƒä¿æŠ¤éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ°´å°æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ã€‚</li>
<li>LVMarkæ–¹æ³•é€šè¿‡é€‰æ‹©æ€§æƒé‡è°ƒåˆ¶ç­–ç•¥åµŒå…¥æ°´å°ä¿¡æ¯ã€‚</li>
<li>LVMarkèƒ½å¤Ÿä¿æŠ¤è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ‰€æœ‰æƒã€‚</li>
<li>è®¾è®¡çš„æ°´å°è§£ç å™¨åˆ©ç”¨æ—¶ç©ºä¿¡æ¯åŠäº¤å‰æ³¨æ„åŠ›æ¨¡å—åº”å¯¹æ¶æ„æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae0f609915f120d45fa97a9e73fc7c28.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Diffusion Models/2412.09122v2/page_2_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4730f5db513ba8ee55f22bde7b0dd23.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AnyDressing-Customizable-Multi-Garment-Virtual-Dressing-via-Latent-Diffusion-Models"><a href="#AnyDressing-Customizable-Multi-Garment-Virtual-Dressing-via-Latent-Diffusion-Models" class="headerlink" title="AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent   Diffusion Models"></a>AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent   Diffusion Models</h2><p><strong>Authors:Xinghui Li, Qichao Sun, Pengze Zhang, Fulong Ye, Zhichao Liao, Wanquan Feng, Songtao Zhao, Qian He</strong></p>
<p>Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬å’Œå›¾åƒæç¤ºçš„æœè£…ä¸­å¿ƒå›¾åƒç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸æ”¯æŒå¤šç§æœè£…ç»„åˆï¼Œå¹¶ä¸”åœ¨ä¿æŒå¯¹æ–‡æœ¬æç¤ºçš„å¿ å®åº¦çš„åŒæ—¶ï¼Œéš¾ä»¥ä¿ç•™æœè£…ç»†èŠ‚ï¼Œè¿™åœ¨å¤šç§åœºæ™¯ä¸­é™åˆ¶äº†å®ƒä»¬çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå³å¤šæœè£…è™šæ‹Ÿæ¢è£…ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„AnyDressingæ–¹æ³•ï¼Œç”¨äºæ ¹æ®ä»»ä½•æœè£…ç»„åˆå’Œä»»ä½•ä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºè¿›è¡Œè§’è‰²å®šåˆ¶ã€‚AnyDressingä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç½‘ç»œï¼Œå³GarmentsNetå’ŒDressingNetï¼Œå®ƒä»¬åˆ†åˆ«ä¸“æ³¨äºæå–è¯¦ç»†çš„æœè£…ç‰¹å¾å’Œç”Ÿæˆå®šåˆ¶å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨GarmentsNetä¸­æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ¨¡å—ï¼Œç§°ä¸ºæœè£…ç‰¹å®šç‰¹å¾æå–å™¨ï¼Œä»¥å¹¶è¡Œæ–¹å¼å•ç‹¬ç¼–ç æœè£…çº¹ç†ã€‚è¿™ç§è®¾è®¡å¯ä»¥é˜²æ­¢æœè£…æ··æ·†ï¼ŒåŒæ—¶ç¡®ä¿ç½‘ç»œæ•ˆç‡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åœ¨DressingNetä¸­è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”çš„ç€è£…æ³¨æ„åŠ›æœºåˆ¶å’Œä¸€ç§æ–°é¢–çš„å®ä¾‹çº§æœè£…å®šä½å­¦ä¹ ç­–ç•¥ï¼Œä»¥å‡†ç¡®åœ°å°†å¤šæœè£…ç‰¹å¾æ³¨å…¥å…¶ç›¸åº”åŒºåŸŸã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å°†å¤šæœè£…çº¹ç†çº¿ç´¢èå…¥ç”Ÿæˆçš„å›¾åƒä¸­ï¼Œå¹¶è¿›ä¸€æ­¥å¢å¼ºäº†æ–‡æœ¬-å›¾åƒçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æœè£…å¢å¼ºçº¹ç†å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜æœè£…çš„ç»†ç²’åº¦çº¹ç†ç»†èŠ‚ã€‚ç”±äºæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡ï¼ŒAnyDressingå¯ä»¥ä½œä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—è½»æ¾é›†æˆåˆ°ä»»ä½•æ‰©æ•£æ¨¡å‹çš„ç¤¾åŒºæ§åˆ¶æ‰©å±•ä¸­ï¼Œæé«˜åˆæˆå›¾åƒçš„å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAnyDressingè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04146v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://crayon-shinchan.github.io/AnyDressing/">https://crayon-shinchan.github.io/AnyDressing/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è¡£ç‰©å¯¼å‘å›¾åƒç”ŸæˆæŠ€æœ¯çš„æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ä¸åŒè¡£ç‰©ç»„åˆçš„æ”¯æŒä»¥åŠåœ¨ä¿æŒè¡£ç‰©ç»†èŠ‚å’Œå¿ å®äºæ–‡æœ¬æç¤ºæ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•AnyDressingã€‚AnyDressingåŒ…å«ä¸¤ä¸ªä¸»è¦ç½‘ç»œï¼šGarmentsNetå’ŒDressingNetï¼Œåˆ†åˆ«ç”¨äºæå–è¡£ç‰©è¯¦ç»†ç‰¹å¾å’Œç”Ÿæˆå®šåˆ¶å›¾åƒã€‚é€šè¿‡é‡‡ç”¨é«˜æ•ˆçš„Garment-Specific Feature Extractorå’Œåˆ›æ–°çš„Dressing-Attentionæœºåˆ¶ç­‰æŠ€æœ¯ï¼ŒAnyDressingèƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒä¸­å‡†ç¡®èå…¥å¤šè¡£ç‰©çº¹ç†ï¼Œå¹¶æå‡æ–‡æœ¬ä¸å›¾åƒçš„å¥‘åˆåº¦ã€‚æ­¤å¤–ï¼ŒAnyDressingè¿˜å¯ä½œä¸ºæ’ä»¶æ¨¡å—è½»æ¾é›†æˆåˆ°ä»»ä½•æ‰©æ•£æ¨¡å‹çš„ç¤¾åŒºæ§åˆ¶æ‰©å±•ä¸­ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è¡£ç‰©å¯¼å‘å›¾åƒç”Ÿæˆçš„æ–°è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä¸åŒè¡£ç‰©ç»„åˆåŠä¿æŒè¡£ç‰©ç»†èŠ‚å’Œæ–‡æœ¬æç¤ºå¿ å®åº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>AnyDressingæ–¹æ³•åŒ…å«GarmentsNetå’ŒDressingNetä¸¤ä¸ªä¸»è¦ç½‘ç»œï¼Œåˆ†åˆ«ç”¨äºæå–è¡£ç‰©ç‰¹å¾å’Œç”Ÿæˆå®šåˆ¶å›¾åƒã€‚</li>
<li>AnyDressingé‡‡ç”¨äº†é«˜æ•ˆçš„Garment-Specific Feature Extractoræ¥å¹¶è¡Œç¼–ç è¡£ç‰©çº¹ç†ï¼Œé˜²æ­¢è¡£ç‰©æ··æ·†å¹¶ä¿è¯ç½‘ç»œæ•ˆç‡ã€‚</li>
<li>åˆ›æ–°çš„Dressing-Attentionæœºåˆ¶å’ŒInstance-Level Garment Localization Learningç­–ç•¥ç”¨äºå‡†ç¡®å°†å¤šè¡£ç‰©ç‰¹å¾æ³¨å…¥å¯¹åº”åŒºåŸŸï¼Œå¹¶æå‡æ–‡æœ¬ä¸å›¾åƒçš„å¥‘åˆåº¦ã€‚</li>
<li>å¼•å…¥Garment-Enhanced Texture Learningç­–ç•¥ï¼Œä»¥æé«˜è¡£ç‰©çš„ç²¾ç»†çº¹ç†ç»†èŠ‚ã€‚</li>
<li>AnyDressingå¯ä½œä¸ºæ’ä»¶æ¨¡å—é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aaff9028ff5be81c50f74590a661decb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e14838690b0fdeb10796fc5300b15095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b40d7fb02fea182faeaab9e064f607ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f86299f8bedc0c644a9f7bceb386c1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TC-KANRecon-High-Quality-and-Accelerated-MRI-Reconstruction-via-Adaptive-KAN-Mechanisms-and-Intelligent-Feature-Scaling"><a href="#TC-KANRecon-High-Quality-and-Accelerated-MRI-Reconstruction-via-Adaptive-KAN-Mechanisms-and-Intelligent-Feature-Scaling" class="headerlink" title="TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via   Adaptive KAN Mechanisms and Intelligent Feature Scaling"></a>TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via   Adaptive KAN Mechanisms and Intelligent Feature Scaling</h2><p><strong>Authors:Ruiquan Ge, Xiao Yu, Yifei Chen, Guanyu Zhou, Fan Jia, Shenghao Zhu, Junhao Jia, Chenyan Zhang, Yifei Sun, Dong Zeng, Changmiao Wang, Qiegen Liu, Shanzhou Niu</strong></p>
<p>Magnetic Resonance Imaging (MRI) has become essential in clinical diagnosis due to its high resolution and multiple contrast mechanisms. However, the relatively long acquisition time limits its broader application. To address this issue, this study presents an innovative conditional guided diffusion model, named as TC-KANRecon, which incorporates the Multi-Free U-KAN (MF-UKAN) module and a dynamic clipping strategy. TC-KANRecon model aims to accelerate the MRI reconstruction process through deep learning methods while maintaining the quality of the reconstructed images. The MF-UKAN module can effectively balance the tradeoff between image denoising and structure preservation. Specifically, it presents the multi-head attention mechanisms and scalar modulation factors, which significantly enhances the modelâ€™s robustness and structure preservation capabilities in complex noise environments. Moreover, the dynamic clipping strategy in TC-KANRecon adjusts the cropping interval according to the sampling steps, thereby mitigating image detail loss typicalching the visual features of the images. Furthermore, the MC-Model incorporates full-sampling k-space information, realizing efficient fusion of conditional information, enhancing the modelâ€™s ability to process complex data, and improving the realism and detail richness of reconstructed images. Experimental results demonstrate that the proposed method outperforms other MRI reconstruction methods in both qualitative and quantitative evaluations. Notably, TC-KANRecon method exhibits excellent reconstruction results when processing high-noise, low-sampling-rate MRI data. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/lcbkmm/TC-KANRecon">https://github.com/lcbkmm/TC-KANRecon</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å› å…¶é«˜åˆ†è¾¨ç‡å’Œå¤šç§å¯¹æ¯”æœºåˆ¶åœ¨ä¸´åºŠè¯Šæ–­ä¸­å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›¸å¯¹è¾ƒé•¿çš„é‡‡é›†æ—¶é—´é™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æœ‰æ¡ä»¶å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œåä¸ºTC-KANReconã€‚å®ƒç»“åˆäº†å¤šè‡ªç”±åº¦U-KANï¼ˆMF-UKANï¼‰æ¨¡å—å’ŒåŠ¨æ€è£å‰ªç­–ç•¥ã€‚TC-KANReconæ¨¡å‹æ—¨åœ¨é€šè¿‡æ·±åº¦å­¦ä¹ æ–¹æ³•åŠ é€ŸMRIé‡å»ºè¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒé‡å»ºå›¾åƒçš„è´¨é‡ã€‚MF-UKANæ¨¡å—å¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡å›¾åƒå»å™ªå’Œç»“æ„ä¿æŒä¹‹é—´çš„æƒè¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå¼•å…¥äº†å¤šå¤´æ³¨æ„æœºåˆ¶å’Œæ ‡é‡è°ƒåˆ¶å› å­ï¼Œè¿™æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚å™ªå£°ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œç»“æ„ä¿æŒèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒTC-KANReconä¸­çš„åŠ¨æ€è£å‰ªç­–ç•¥æ ¹æ®é‡‡æ ·æ­¥éª¤è°ƒæ•´è£å‰ªé—´éš”ï¼Œä»è€Œå‡è½»å›¾åƒç»†èŠ‚æŸå¤±ï¼Œä¿ç•™å›¾åƒè§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒMCæ¨¡å‹ç»“åˆäº†å…¨é‡‡æ ·kç©ºé—´ä¿¡æ¯ï¼Œå®ç°äº†æ¡ä»¶ä¿¡æ¯çš„æœ‰æ•ˆèåˆï¼Œæé«˜äº†æ¨¡å‹å¤„ç†å¤æ‚æ•°æ®çš„èƒ½åŠ›ï¼Œæé«˜äº†é‡å»ºå›¾åƒçš„é€¼çœŸåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢éƒ½ä¼˜äºå…¶ä»–MRIé‡å»ºæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTC-KANReconæ–¹æ³•åœ¨å¤„ç†é«˜å™ªå£°ã€ä½é‡‡æ ·ç‡çš„MRIæ•°æ®æ—¶è¡¨ç°å‡ºä¼˜å¼‚çš„é‡å»ºæ•ˆæœã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lcbkmm/TC-KANRecon">https://github.com/lcbkmm/TC-KANRecon</a>ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05705v2">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºTC-KANReconçš„æ¡ä»¶å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåŠ é€ŸMRIé‡å»ºè¿‡ç¨‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†Multi-Free U-KANæ¨¡å—å’ŒåŠ¨æ€è£å‰ªç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡æ·±åº¦å­¦ä¹ æ–¹æ³•æé«˜MRIå›¾åƒé‡å»ºçš„é€Ÿåº¦å’Œè´¨é‡ã€‚TC-KANReconåœ¨å¤æ‚å™ªå£°ç¯å¢ƒä¸­å…·æœ‰å‡ºè‰²çš„é²æ£’æ€§å’Œç»“æ„ä¿æŒèƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆå¹³è¡¡å›¾åƒå»å™ªå’Œç»“æ„ä¿æŒä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„åŠ¨æ€è£å‰ªç­–ç•¥å¯æ ¹æ®é‡‡æ ·æ­¥éª¤è°ƒæ•´è£å‰ªé—´éš”ï¼Œå‡å°‘å›¾åƒç»†èŠ‚æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸Šéƒ½ä¼˜äºå…¶ä»–MRIé‡å»ºæ–¹æ³•ï¼Œå°¤å…¶é€‚ç”¨äºå¤„ç†é«˜å™ªå£°ã€ä½é‡‡æ ·ç‡çš„MRIæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TC-KANReconæ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¡ä»¶å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨åŠ é€ŸMRIå›¾åƒé‡å»ºè¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†Multi-Free U-KANæ¨¡å—ï¼Œå…·æœ‰å‡ºè‰²çš„é²æ£’æ€§å’Œç»“æ„ä¿æŒèƒ½åŠ›ï¼Œèƒ½åœ¨å¤æ‚å™ªå£°ç¯å¢ƒä¸­æœ‰æ•ˆå¹³è¡¡å›¾åƒå»å™ªå’Œç»“æ„ä¿æŒä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>TC-KANReconé‡‡ç”¨åŠ¨æ€è£å‰ªç­–ç•¥ï¼Œå¯æ ¹æ®é‡‡æ ·æ­¥éª¤è°ƒæ•´è£å‰ªé—´éš”ï¼Œå‡å°‘å›¾åƒç»†èŠ‚æŸå¤±ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTC-KANReconåœ¨MRIå›¾åƒé‡å»ºæ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜å™ªå£°ã€ä½é‡‡æ ·ç‡çš„MRIæ•°æ®æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TC-KANReconæ¨¡å‹å®ç°äº†å…¨é‡‡æ ·k-ç©ºé—´ä¿¡æ¯çš„èåˆï¼Œæé«˜äº†æ¨¡å‹å¤„ç†å¤æ‚æ•°æ®çš„èƒ½åŠ›ï¼Œå¢å¼ºäº†é‡å»ºå›¾åƒçš„çœŸå®æ„Ÿå’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚</li>
<li>æ¨¡å‹çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨æŒ‡å®šGitHubä»“åº“ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å’ŒåŒ»ç”Ÿç­‰äººå‘˜è·å–å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afb2b65a29e55cd143da8bc2f5e9b2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cd794284f8b47cf03d76f198abe3bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79929f4327c25052aee5265e83e3a03e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-609e44b5a16734bcb86e88967327aac9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Image-Diffusion-Generation-of-Image-Sequence-and-Application-in-MRI"><a href="#Autoregressive-Image-Diffusion-Generation-of-Image-Sequence-and-Application-in-MRI" class="headerlink" title="Autoregressive Image Diffusion: Generation of Image Sequence and   Application in MRI"></a>Autoregressive Image Diffusion: Generation of Image Sequence and   Application in MRI</h2><p><strong>Authors:Guanxiong Luo, Shoujin Huang, Martin Uecker</strong></p>
<p>Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality. However, a persistent challenge lies in balancing image quality with imaging speed. This trade-off is primarily constrained by k-space measurements, which traverse specific trajectories in the spatial Fourier domain (k-space). These measurements are often undersampled to shorten acquisition times, resulting in image artifacts and compromised quality. Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled k-space data. In this work, we present the autoregressive image diffusion (AID) model for image sequences and use it to sample the posterior for accelerated MRI reconstruction. The algorithm incorporates both undersampled k-space and pre-existing information. Models trained with fastMRI dataset are evaluated comprehensively. The results show that the AID model can robustly generate sequentially coherent image sequences. In MRI applications, the AID can outperform the standard diffusion model and reduce hallucinations, due to the learned inter-image dependencies. The project code is available at <a target="_blank" rel="noopener" href="https://github.com/mrirecon/aid">https://github.com/mrirecon/aid</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„éä¾µå…¥æ€§æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼ŒæŒç»­é¢ä¸´çš„æŒ‘æˆ˜åœ¨äºå¹³è¡¡å›¾åƒè´¨é‡ä¸æˆåƒé€Ÿåº¦ã€‚è¿™ç§æƒè¡¡ä¸»è¦å—åˆ°kç©ºé—´æµ‹é‡çš„é™åˆ¶ï¼Œè¿™äº›æµ‹é‡åœ¨ç©ºé—´å‚…é‡Œå¶åŸŸï¼ˆkç©ºé—´ï¼‰æ²¿ç‰¹å®šè½¨è¿¹è¡Œè¿›ã€‚ä¸ºäº†å‡å°‘é‡‡é›†æ—¶é—´ï¼Œè¿™äº›æµ‹é‡å¾€å¾€è¿›è¡Œæ¬ é‡‡æ ·ï¼Œå¯¼è‡´å›¾åƒå‡ºç°ä¼ªå½±å’Œè´¨é‡ä¸‹é™ã€‚ç”Ÿæˆæ¨¡å‹å­¦ä¹ å›¾åƒåˆ†å¸ƒï¼Œå¯ç”¨äºä»æ¬ é‡‡æ ·çš„kç©ºé—´æ•°æ®ä¸­é‡å»ºé«˜è´¨é‡å›¾åƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå›¾åƒåºåˆ—çš„è‡ªå›å½’å›¾åƒæ‰©æ•£ï¼ˆAIDï¼‰æ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºé‡‡æ ·ååŠ é€ŸMRIé‡å»ºã€‚è¯¥ç®—æ³•ç»“åˆäº†æ¬ é‡‡æ ·çš„kç©ºé—´å’Œç°æœ‰ä¿¡æ¯ã€‚ç”¨fastMRIæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒAIDæ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°ç”Ÿæˆè¿ç»­çš„å›¾åƒåºåˆ—ã€‚åœ¨MRIåº”ç”¨ä¸­ï¼Œç”±äºå­¦ä¹ åˆ°çš„å›¾åƒé—´ä¾èµ–æ€§ï¼ŒAIDå¯ä»¥ä¼˜äºæ ‡å‡†æ‰©æ•£æ¨¡å‹å¹¶å‡å°‘å¹»è§‰ã€‚é¡¹ç›®ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mrirecon/aid%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/mrirecon/aidè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14327v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³åœ¨å›¾åƒè´¨é‡ä¸æˆåƒé€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚æ–‡ç« æå‡ºä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä»‹ç»äº†ä¸€ç§åä¸ºautoregressive image diffusionï¼ˆAIDï¼‰çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å›¾åƒåˆ†å¸ƒï¼Œä»æ¬ é‡‡æ ·çš„k-spaceæ•°æ®ä¸­é‡å»ºé«˜è´¨é‡å›¾åƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAIDæ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°ç”Ÿæˆè¿ç»­çš„å›¾åƒåºåˆ—ï¼Œå¹¶åœ¨MRIåº”ç”¨ä¸­è¡¨ç°å‡ºä¼˜äºæ ‡å‡†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨å›¾åƒè´¨é‡ä¸æˆåƒé€Ÿåº¦ä¹‹é—´å­˜åœ¨æƒè¡¡æŒ‘æˆ˜ã€‚</li>
<li>k-spaceæµ‹é‡åœ¨MRIä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½†å…¶æ¬ é‡‡æ ·ä¼šå¯¼è‡´å›¾åƒä¼ªå½±å’Œè´¨é‡ä¸‹é™ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹å¦‚autoregressive image diffusionï¼ˆAIDï¼‰èƒ½å¤Ÿä»æ¬ é‡‡æ ·çš„k-spaceæ•°æ®ä¸­é‡å»ºé«˜è´¨é‡å›¾åƒã€‚</li>
<li>AIDæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å›¾åƒåºåˆ—çš„åˆ†å¸ƒï¼Œå¹¶ç”Ÿæˆè¿ç»­çš„å›¾åƒåºåˆ—ã€‚</li>
<li>åœ¨MRIåº”ç”¨ä¸­ï¼ŒAIDæ¨¡å‹è¡¨ç°å‡ºä¼˜äºæ ‡å‡†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>AIDæ¨¡å‹é€šè¿‡åˆ©ç”¨å›¾åƒé—´çš„ä¾èµ–æ€§ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡çš„å‡ºç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8317a3be8b2934d92e9af14c6daa2905.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3115b5c8d9ec06680d45598aac0116b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e5e04727103a2cfdd6c975b8e684ce6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MV-VTON-Multi-View-Virtual-Try-On-with-Diffusion-Models"><a href="#MV-VTON-Multi-View-Virtual-Try-On-with-Diffusion-Models" class="headerlink" title="MV-VTON: Multi-View Virtual Try-On with Diffusion Models"></a>MV-VTON: Multi-View Virtual Try-On with Diffusion Models</h2><p><strong>Authors:Haoyu Wang, Zhilu Zhang, Donglin Di, Shiliang Zhang, Wangmeng Zuo</strong></p>
<p>The goal of image-based virtual try-on is to generate an image of the target person naturally wearing the given clothing. However, existing methods solely focus on the frontal try-on using the frontal clothing. When the views of the clothing and person are significantly inconsistent, particularly when the personâ€™s view is non-frontal, the results are unsatisfactory. To address this challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims to reconstruct the dressing results from multiple views using the given clothes. Given that single-view clothes provide insufficient information for MV-VTON, we instead employ two images, i.e., the frontal and back views of the clothing, to encompass the complete view as much as possible. Moreover, we adopt diffusion models that have demonstrated superior abilities to perform our MV-VTON. In particular, we propose a view-adaptive selection method where hard-selection and soft-selection are applied to the global and local clothing feature extraction, respectively. This ensures that the clothing features are roughly fit to the personâ€™s view. Subsequently, we suggest joint attention blocks to align and fuse clothing features with person features. Additionally, we collect a MV-VTON dataset MVG, in which each person has multiple photos with diverse views and poses. Experiments show that the proposed method not only achieves state-of-the-art results on MV-VTON task using our MVG dataset, but also has superiority on frontal-view virtual try-on task using VITON-HD and DressCode datasets. </p>
<blockquote>
<p>åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•ç©¿çš„ç›®æ ‡æ˜¯ç”Ÿæˆç›®æ ‡äººç‰©è‡ªç„¶ç©¿ç€ç»™å®šæœè£…çš„å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…ä¸“æ³¨äºä½¿ç”¨æ­£é¢æœè£…è¿›è¡Œæ­£é¢è¯•ç©¿ã€‚å½“æœè£…å’Œäººç‰©çš„è§†è§’å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯äººç‰©è§†è§’éæ­£é¢æ—¶ï¼Œç»“æœå¾€å¾€ä¸å°½äººæ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šè§†è§’è™šæ‹Ÿè¯•ç©¿ï¼ˆMV-VTONï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨ç»™å®šçš„æœè£…ä»å¤šä¸ªè§†è§’é‡å»ºç€è£…æ•ˆæœã€‚é‰´äºå•è§†è§’è¡£ç‰©ä¸ºMV-VTONæä¾›çš„ä¿¡æ¯ä¸è¶³ï¼Œæˆ‘ä»¬è½¬è€Œä½¿ç”¨ä¸¤å¼ å›¾åƒï¼Œå³æœè£…çš„æ­£é¢å’ŒèƒŒé¢è§†å›¾ï¼Œä»¥å°½å¯èƒ½æ¶µç›–å®Œæ•´çš„è§†å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨MV-VTONä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†é€‚åº”é€‰æ‹©æ–¹æ³•ï¼Œå…¶ä¸­ç¡¬é€‰æ‹©å’Œè½¯é€‰æ‹©åˆ†åˆ«åº”ç”¨äºå…¨å±€å’Œå±€éƒ¨æœè£…ç‰¹å¾æå–ï¼Œä»¥ç¡®ä¿æœè£…ç‰¹å¾å¤§è‡´é€‚åº”äººç‰©çš„è§†è§’ã€‚éšåï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨è”åˆæ³¨æ„åŠ›å—æ¥å¯¹é½å’Œèåˆæœè£…ç‰¹å¾ä¸äººç‰©ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªMV-VTONæ•°æ®é›†MVGï¼Œå…¶ä¸­æ¯ä¸ªäººç‰©éƒ½æœ‰å¤šå¼ å…·æœ‰ä¸åŒè§†è§’å’Œå§¿æ€çš„ç…§ç‰‡ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ä»…åœ¨æˆ‘ä»¬è‡ªå·±çš„MVGæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„MV-VTONä»»åŠ¡ç»“æœï¼Œè€Œä¸”åœ¨VITON-HDå’ŒDressCodeæ•°æ®é›†ä¸Šçš„æ­£é¢è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸­ä¹Ÿå…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.17364v4">PDF</a> Accept by AAAI 2025. Project url:   <a target="_blank" rel="noopener" href="https://hywang2002.github.io/MV-VTON/">https://hywang2002.github.io/MV-VTON/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šè§†è§’è™šæ‹Ÿè¯•ç©¿ï¼ˆMV-VTONï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨é€šè¿‡ç»™å®šçš„è¡£ç‰©ä»å¤šä¸ªè§†è§’é‡å»ºç€è£…æ•ˆæœã€‚ä¸ºè§£å†³å•ä¸€è§†è§’è¡£ç‰©ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œé‡‡ç”¨å‰è§†å’Œåè§†ä¸¤å¼ è¡£ç‰©å›¾åƒæ¥å°½å¯èƒ½å…¨é¢åœ°è¦†ç›–è§†è§’ã€‚é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ‰§è¡ŒMV-VTONä»»åŠ¡ï¼Œå¹¶æå‡ºä¸€ç§è§†å›¾è‡ªé€‚åº”é€‰æ‹©æ–¹æ³•ï¼Œå¯¹å…¨å±€å’Œå±€éƒ¨è¡£ç‰©ç‰¹å¾è¿›è¡Œç¡¬é€‰æ‹©å’Œè½¯é€‰æ‹©ã€‚æ­¤å¤–ï¼Œå»ºè®®é‡‡ç”¨è”åˆæ³¨æ„åŠ›å—å¯¹é½å’Œèåˆè¡£ç‰©ç‰¹å¾ä¸äººåƒç‰¹å¾ã€‚æœ€åæ”¶é›†MVGæ•°æ®é›†ç”¨äºå®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨MV-VTONä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MV-VTONæŠ€æœ¯æ—¨åœ¨é€šè¿‡ç»™å®šçš„è¡£ç‰©ä»å¤šä¸ªè§†è§’é‡å»ºç€è£…æ•ˆæœï¼Œè§£å†³è§†è§’ä¸ä¸€è‡´å¯¼è‡´çš„ä¸æ»¡æ„ç»“æœé—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å‰è§†å’Œåè§†ä¸¤å¼ è¡£ç‰©å›¾åƒæ¥æ›´å…¨é¢è¦†ç›–è§†è§’ï¼Œè§£å†³å•ä¸€è§†è§’è¡£ç‰©ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ‰§è¡ŒMV-VTONä»»åŠ¡ï¼Œè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ç§è§†å›¾è‡ªé€‚åº”é€‰æ‹©æ–¹æ³•ï¼Œå¯¹å…¨å±€å’Œå±€éƒ¨è¡£ç‰©ç‰¹å¾è¿›è¡Œç¡¬é€‰æ‹©å’Œè½¯é€‰æ‹©ï¼Œç¡®ä¿è¡£ç‰©ç‰¹å¾ä¸äººåƒè§†è§’ç›¸é€‚åº”ã€‚</li>
<li>é‡‡ç”¨è”åˆæ³¨æ„åŠ›å—æ¥å¯¹é½å’Œèåˆè¡£ç‰©ç‰¹å¾ä¸äººåƒç‰¹å¾ã€‚</li>
<li>æ”¶é›†MVGæ•°æ®é›†ç”¨äºå®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.17364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-512bea25c788c532f9f8e94707e57a0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c673850bcfb92eeb93c984965bd942a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8bd13ba864f1694145d6130bac4d201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb781d57314717524175a281c589f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18cf22952a17ef555b25f50eb1dadcd8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SRAGAN-Saliency-Regularized-and-Attended-Generative-Adversarial-Network-for-Chinese-Ink-wash-Painting-Generation"><a href="#SRAGAN-Saliency-Regularized-and-Attended-Generative-Adversarial-Network-for-Chinese-Ink-wash-Painting-Generation" class="headerlink" title="SRAGAN: Saliency Regularized and Attended Generative Adversarial Network   for Chinese Ink-wash Painting Generation"></a>SRAGAN: Saliency Regularized and Attended Generative Adversarial Network   for Chinese Ink-wash Painting Generation</h2><p><strong>Authors:Xiang Gao, Yuqi Zhang</strong></p>
<p>Recent style transfer problems are still largely dominated by Generative Adversarial Network (GAN) from the perspective of cross-domain image-to-image (I2I) translation, where the pivotal issue is to learn and transfer target-domain style patterns onto source-domain content images. This paper handles the problem of translating real pictures into traditional Chinese ink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though a wide range of I2I models tackle this problem, a notable challenge is that the content details of the source image could be easily erased or corrupted due to the transfer of ink-wash style elements. To remedy this issue, we propose to incorporate saliency detection into the unpaired I2I framework to regularize image content, where the detected saliency map is utilized from two aspects: (\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize object content structure by enforcing saliency consistency before and after image stylization; (\romannumeral2) we propose saliency adaptive normalization (SANorm) which implicitly enhances object structure integrity of the generated paintings by dynamically injecting image saliency information into the generator to guide stylization process. Besides, we also propose saliency attended discriminator which harnesses image saliency information to focus generative adversarial attention onto the drawn objects, contributing to generating more vivid and delicate brush strokes and ink-wash textures. Extensive qualitative and quantitative experiments demonstrate superiority of our approach over related advanced image stylization methods in both GAN and diffusion model paradigms. </p>
<blockquote>
<p>è¿‘æœŸé£æ ¼è¿ç§»é—®é¢˜ä»ç„¶ä¸»è¦ä»è·¨åŸŸå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢çš„è§’åº¦ç”±ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸»å¯¼ï¼Œå…³é”®åœ¨äºå­¦ä¹ å’Œå°†ç›®æ ‡åŸŸçš„é£æ ¼æ¨¡å¼è½¬ç§»åˆ°æºåŸŸå†…å®¹å›¾åƒä¸Šã€‚æœ¬æ–‡å¤„ç†å°†çœŸå®å›¾ç‰‡ç¿»è¯‘æˆä¼ ç»Ÿæ°´å¢¨ç”»çš„é—®é¢˜ï¼Œå³æ°´å¢¨ç”»é£æ ¼è½¬æ¢ã€‚å°½ç®¡æœ‰è®¸å¤šI2Iæ¨¡å‹å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä½†ä¸€ä¸ªæ˜¾è‘—çš„æŒ‘æˆ˜æ˜¯æºå›¾åƒçš„å†…å®¹ç»†èŠ‚åœ¨è½¬ç§»æ°´å¢¨é£æ ¼å…ƒç´ æ—¶å®¹æ˜“è¢«æŠ¹å»æˆ–ç ´åã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†æ˜¾è‘—æ€§æ£€æµ‹èå…¥éé…å¯¹I2Iæ¡†æ¶ä»¥è§„èŒƒå›¾åƒå†…å®¹ï¼Œä»ä¸¤ä¸ªæ–¹é¢åˆ©ç”¨æ£€æµ‹åˆ°çš„æ˜¾è‘—æ€§å›¾ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºæ˜¾è‘—æ€§IOUï¼ˆSIOUï¼‰æŸå¤±ï¼Œé€šè¿‡å¼ºåˆ¶æ˜¾è‘—æ€§ä¸€è‡´æ€§åœ¨å›¾åƒé£æ ¼åŒ–ä¹‹å‰å’Œä¹‹åæ˜¾å¼åœ°è§„èŒƒå¯¹è±¡å†…å®¹ç»“æ„ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºæ˜¾è‘—æ€§è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆSANormï¼‰ï¼Œé€šè¿‡åŠ¨æ€å°†å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯æ³¨å…¥ç”Ÿæˆå™¨ä»¥æŒ‡å¯¼é£æ ¼åŒ–è¿‡ç¨‹ï¼Œéšå¼åœ°å¢å¼ºç”Ÿæˆç”»ä½œçš„å¯¹è±¡ç»“æ„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ˜¾è‘—æ€§å…³æ³¨é‰´åˆ«å™¨ï¼Œå®ƒåˆ©ç”¨å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯å°†ç”Ÿæˆå¯¹æŠ—æ€§æ³¨æ„åŠ›é›†ä¸­åœ¨ç»˜åˆ¶çš„å¯¹è±¡ä¸Šï¼Œæœ‰åŠ©äºç”Ÿæˆæ›´ç”ŸåŠ¨ã€æ›´ç²¾ç»†çš„ç¬”è§¦å’Œæ°´å¢¨çº¹ç†ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹èŒƒå¼ä¸­å‡ä¼˜äºç›¸å…³çš„å…ˆè¿›å›¾åƒé£æ ¼åŒ–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15743v2">PDF</a> 34 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†å°†çœŸå®å›¾ç‰‡è½¬åŒ–ä¸ºä¸­å›½ä¼ ç»Ÿæ°´å¢¨ç”»çš„é—®é¢˜ï¼Œå³æ°´å¢¨ç”»é£æ ¼è½¬æ¢ã€‚æ–‡ç« å¼•å…¥äº†æ˜¾è‘—æ€§æ£€æµ‹æ¥è§£å†³åœ¨é£æ ¼è½¬æ¢è¿‡ç¨‹ä¸­æºå›¾åƒå†…å®¹ç»†èŠ‚å®¹æ˜“ä¸¢å¤±æˆ–æŸåçš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆæ— é…å¯¹å›¾åƒåˆ°å›¾åƒçš„æ¡†æ¶ï¼Œæå‡ºä¸¤ç§ç­–ç•¥ï¼šä¸€æ˜¯æ˜¾è‘—æ€§IOUï¼ˆSIOUï¼‰æŸå¤±ï¼Œé€šè¿‡å¼ºåˆ¶é£æ ¼åŒ–å‰åçš„æ˜¾è‘—æ€§ä¸€è‡´æ€§æ¥æ˜ç¡®è§„èŒƒå¯¹è±¡å†…å®¹ç»“æ„ï¼›äºŒæ˜¯æ˜¾è‘—æ€§è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆSANormï¼‰ï¼Œé€šè¿‡åŠ¨æ€æ³¨å…¥å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯æ¥éšå¼å¢å¼ºç”Ÿæˆç”»ä½œçš„å¯¹è±¡ç»“æ„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ˜¾è‘—æ€§å…³æ³¨é‰´åˆ«å™¨ï¼Œåˆ©ç”¨å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯å°†ç”Ÿæˆå¯¹æŠ—æ€§æ³¨æ„åŠ›é›†ä¸­åœ¨ç»˜åˆ¶å¯¹è±¡ä¸Šï¼Œæœ‰åŠ©äºç”Ÿæˆæ›´ç”ŸåŠ¨ã€ç»†è…»çš„ç”»ç¬”ç¬”è§¦å’Œå¢¨æ°´çº¹ç†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨GANå’Œæ‰©æ•£æ¨¡å‹èŒƒå¼ä¸­å‡ä¼˜äºå…¶ä»–å…ˆè¿›çš„å›¾åƒé£æ ¼åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« è§£å†³äº†çœŸå®å›¾ç‰‡è½¬åŒ–ä¸ºæ°´å¢¨ç”»é£æ ¼çš„éš¾é¢˜ï¼Œå³é£æ ¼è½¬æ¢é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹é£æ ¼è½¬æ¢è¿‡ç¨‹ä¸­æºå›¾åƒå†…å®¹ç»†èŠ‚å¯èƒ½ä¸¢å¤±çš„é—®é¢˜ï¼Œå¼•å…¥äº†æ˜¾è‘—æ€§æ£€æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§ç­–ç•¥æ¥ç»“åˆæ˜¾è‘—æ€§æ£€æµ‹ï¼šSIOUæŸå¤±å’ŒSANormæ–¹æ³•ã€‚</li>
<li>SHOUæŸå¤±é€šè¿‡å¼ºåˆ¶é£æ ¼åŒ–å‰åçš„æ˜¾è‘—æ€§ä¸€è‡´æ€§æ¥è§„èŒƒå¯¹è±¡å†…å®¹ç»“æ„ã€‚</li>
<li>SANormæ–¹æ³•é€šè¿‡åŠ¨æ€æ³¨å…¥å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯å¢å¼ºç”Ÿæˆç”»ä½œçš„å¯¹è±¡ç»“æ„å®Œæ•´æ€§ã€‚</li>
<li>å¼•å…¥æ˜¾è‘—æ€§å…³æ³¨é‰´åˆ«å™¨ï¼Œå°†ç”Ÿæˆå¯¹æŠ—æ€§æ³¨æ„åŠ›é›†ä¸­åœ¨ç»˜åˆ¶å¯¹è±¡ä¸Šï¼Œæé«˜ç”Ÿæˆç”»ä½œçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8aa9463cad6600699429fa08e88254a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfa3d8c03770e2a1e3d63df40c81942f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdb1867304a1d698ec5a97aee4cd1b34.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generating-Counterfactual-Trajectories-with-Latent-Diffusion-Models-for-Concept-Discovery"><a href="#Generating-Counterfactual-Trajectories-with-Latent-Diffusion-Models-for-Concept-Discovery" class="headerlink" title="Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery"></a>Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery</h2><p><strong>Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed</strong></p>
<p>Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction. </p>
<blockquote>
<p>åœ¨åŒ»å­¦ç­‰é«˜é£é™©é¢†åŸŸï¼Œå¯ä¿¡æ€§æ˜¯å®‰å…¨åº”ç”¨ä¸é€æ˜çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸»è¦å…ˆå†³æ¡ä»¶ã€‚ç†è§£å†³ç­–è¿‡ç¨‹ä¸ä»…æœ‰åŠ©äºå»ºç«‹ä¿¡ä»»ï¼Œè¿˜å¯èƒ½æ­ç¤ºå¤æ‚æ¨¡å‹çš„å…ˆå‰æœªçŸ¥å†³ç­–æ ‡å‡†ï¼Œä»è€Œæ¨åŠ¨åŒ»å­¦ç ”ç©¶çš„å‘å±•ã€‚ä»é»‘ç®±æ¨¡å‹ä¸­å‘ç°ä¸å†³ç­–ç›¸å…³çš„æ¦‚å¿µæ˜¯ä¸€é¡¹ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æå‡ºäº†é€šè¿‡åŸºäºæ½œåœ¨æ‰©æ•£çš„åäº‹å®è½¨è¿¹è¿›è¡Œæ¦‚å¿µå‘ç°ï¼ˆCDCTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¼˜è¶Šå›¾åƒåˆæˆèƒ½åŠ›è¿›è¡Œæ¦‚å¿µå‘ç°çš„æ–°å‹ä¸‰æ­¥æ¡†æ¶ã€‚é¦–å…ˆï¼ŒCDCTä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ç”Ÿæˆåäº‹å®è½¨è¿¹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ç”¨äºä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ´¾ç”Ÿå‡ºä¸åˆ†ç±»ç›¸å…³çš„æ¦‚å¿µçš„è„±èŠ‚è¡¨ç¤ºã€‚æœ€åï¼Œåº”ç”¨æœç´¢ç®—æ³•æ¥è¯†åˆ«è„±èŠ‚æ½œåœ¨ç©ºé—´ä¸­çš„ç›¸å…³æ¦‚å¿µã€‚å°†CDCTåº”ç”¨äºåœ¨æœ€å¤§çš„å…¬å…±çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œä¸ä»…å‘ç°äº†å¤šç§åè§ï¼Œè¿˜å‘ç°äº†æœ‰æ„ä¹‰çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æ­¤å¤–ï¼ŒCDCTå†…äº§ç”Ÿçš„åäº‹å®äº‹å®è¡¨ç°å‡ºæ¯”å…ˆå‰å»ºç«‹çš„æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„FIDåˆ†æ•°ï¼ŒåŒæ—¶èµ„æºæ•ˆç‡æé«˜äº†12å€ã€‚æ— ç›‘ç£æ¦‚å¿µå‘ç°åœ¨å¯ä¿¡äººå·¥æ™ºèƒ½çš„åº”ç”¨å’Œå„ä¸ªé¢†åŸŸäººç±»çŸ¥è¯†çš„è¿›ä¸€æ­¥å‘å±•æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚CDCTæ˜¯æœç€è¿™ä¸ªæ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10356v2">PDF</a> Published at International Conference on Pattern Recognition (ICPR)   2024</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºCDCTçš„æ–°æ¦‚å¿µå‘ç°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Œå¯¹æ·±åº¦å­¦ä¹ ä¸­å†³ç­–ç›¸å…³çš„æ¦‚å¿µè¿›è¡Œå‘ç°ã€‚CDCTåŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šç”Ÿæˆå¯¹æŠ—æ€§è½¨è¿¹æ•°æ®é›†ã€ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨è¿›è¡Œæ¦‚å¿µè§£è€¦è¡¨ç¤ºï¼Œä»¥åŠåœ¨è§£è€¦çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæœç´¢è¯†åˆ«ç›¸å…³æ¦‚å¿µã€‚åº”ç”¨äºçš®è‚¤ç—…å˜æ•°æ®é›†åˆ†ç±»å™¨çš„åº”ç”¨æ˜¾ç¤ºï¼ŒCDCTä¸ä»…æ­ç¤ºäº†åè§ï¼Œè¿˜å‘ç°äº†æœ‰æ„ä¹‰çš„ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´é«˜æ•ˆã€‚è¿™ä¸€å‘ç°å¯¹å¯ä¿¡èµ–äººå·¥æ™ºèƒ½çš„åº”ç”¨å’Œå„ä¸ªé¢†åŸŸçš„äººç±»çŸ¥è¯†å‘å±•å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼ºè°ƒäº†åœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸï¼Œå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯ä¿¡åº¦çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºæ–°æ¦‚å¿µå‘ç°æ¡†æ¶CDCTï¼Œç”¨äºæ­ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†³ç­–æ ‡å‡†ã€‚</li>
<li>CDCTåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹æŠ—æ€§è½¨è¿¹æ•°æ®é›†ï¼Œä»¥è§£å¼€åˆ†ç±»ç›¸å…³çš„æ¦‚å¿µã€‚</li>
<li>ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¿›è¡Œæ¦‚å¿µè§£è€¦è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡æœç´¢ç®—æ³•åœ¨è§£è€¦çš„æ½œåœ¨ç©ºé—´ä¸­è¯†åˆ«ç›¸å…³æ¦‚å¿µã€‚</li>
<li>åœ¨çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šçš„å®éªŒæ­ç¤ºäº†æ¨¡å‹çš„åè§å’Œæœ‰æ„ä¹‰çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a227c108a3b3766161edb3f6b3768856.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe8d0c84a32b5a6f077cba2a38a25f38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8444517902aeddc867e5f33708d20696.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Polyp-DDPM-Diffusion-Based-Semantic-Polyp-Synthesis-for-Enhanced-Segmentation"><a href="#Polyp-DDPM-Diffusion-Based-Semantic-Polyp-Synthesis-for-Enhanced-Segmentation" class="headerlink" title="Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation"></a>Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation</h2><p><strong>Authors:Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao</strong></p>
<p>This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/mobaidoctor/polyp-ddpm">https://github.com/mobaidoctor/polyp-ddpm</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†Polyp-DDPMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ©è†œç”Ÿæˆé€¼çœŸçš„æ¯è‚‰å›¾åƒï¼Œæ—¨åœ¨æé«˜èƒƒè‚ é“ï¼ˆGIï¼‰æ¯è‚‰çš„åˆ†å‰²æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†ä¸åŒ»å­¦å›¾åƒç›¸å…³çš„æ•°æ®é™åˆ¶ã€é«˜æ ‡æ³¨æˆæœ¬å’Œéšç§å…³æ³¨ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹é‡‡ç”¨åˆ†å‰²æ©è†œï¼ˆä»£è¡¨å¼‚å¸¸åŒºåŸŸçš„äºŒè¿›åˆ¶æ©è†œï¼‰ï¼ŒPolyp-DDPMåœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚åœ¨å›¾åƒè´¨é‡æ–¹é¢ï¼Œå…¶å¼—é›·æ­‡å› å¡ç‰¹è·ç¦»ï¼ˆFIDï¼‰å¾—åˆ†ä¸º78.47ï¼ˆé«˜äº83.79ï¼‰ï¼Œè€Œåœ¨åˆ†å‰²æ€§èƒ½æ–¹é¢ï¼Œå…¶äº¤å¹¶æ¯”ï¼ˆIoUï¼‰è¾¾åˆ°0.7156ï¼ˆåŸºçº¿æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒä½äº0.6694ï¼ŒçœŸå®æ•°æ®ä¸º0.7067ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åˆæˆæ•°æ®é›†ç”¨äºè®­ç»ƒï¼Œä»è€Œæé«˜äº†æ¯è‚‰åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸çœŸå®å›¾åƒç›¸å½“ï¼Œå¹¶æä¾›æ›´å¤§çš„æ•°æ®å¢å¼ºèƒ½åŠ›ä»¥æ”¹è¿›åˆ†å‰²æ¨¡å‹ã€‚Polyp-DDPMçš„æºä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/mobaidoctor/polyp-ddpm%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/mobaidoctor/polyp-ddpmå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.04031v2">PDF</a> This preprint has been accepted for publication in the proceedings of   the IEEE Engineering in Medicine and Biology Society (EMBC 2024). The final   published version is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1109/EMBC53108.2024.10782077">https://doi.org/10.1109/EMBC53108.2024.10782077</a>. The copyright for this work   has been transferred to IEEE</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†Polyp-DDPMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ©è†œç”Ÿæˆé€¼çœŸçš„æ¯è‚‰å›¾åƒï¼Œæ—¨åœ¨æé«˜èƒƒè‚ é“ï¼ˆGIï¼‰æ¯è‚‰çš„åˆ†å‰²æ•ˆæœã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¸åŒ»å­¦å›¾åƒç›¸å…³çš„æ•°æ®é™åˆ¶ã€é«˜æ ‡æ³¨æˆæœ¬å’Œéšç§å…³æ³¨çš„æŒ‘æˆ˜ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹å¯¹åˆ†å‰²æ©è†œçš„æ¡ä»¶åŒ–â€”â€”ä»£è¡¨å¼‚å¸¸åŒºåŸŸçš„äºŒè¿›åˆ¶æ©è†œï¼ŒPolyp-DDPMåœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚åœ¨å›¾åƒè´¨é‡æ–¹é¢ï¼Œå…¶Frechet Inception Distanceï¼ˆFIDï¼‰å¾—åˆ†ä¸º78.47ï¼ˆä¼˜äº83.79ï¼‰ï¼Œåœ¨åˆ†å‰²æ€§èƒ½æ–¹é¢ï¼Œå…¶Intersection over Unionï¼ˆIoUï¼‰å¾—åˆ†ä¸º0.7156ï¼ˆä¼˜äºåŸºçº¿æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒçš„0.6694åŠä»¥ä¸‹å¾—åˆ†å’ŒçœŸå®æ•°æ®çš„0.7067ï¼‰ã€‚è¯¥æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åˆæˆæ•°æ®é›†ï¼Œå¯ç”¨äºè®­ç»ƒï¼Œä½¿æ¯è‚‰åˆ†å‰²æ¨¡å‹å¯ä¸çœŸå®å›¾åƒç›¸åª²ç¾ï¼Œå¹¶æä¾›æ›´å¤§çš„æ•°æ®å¢å¼ºèƒ½åŠ›ï¼Œä»¥æé«˜åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚Polyp-DDPMçš„æºä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²å…¬å¼€æä¾›äº<a target="_blank" rel="noopener" href="https://github.com/mobaidoctor/polyp-ddpm%E3%80%82">https://github.com/mobaidoctor/polyp-ddpmã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>Polyp-DDPMæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ©è†œç”Ÿæˆé€¼çœŸçš„æ¯è‚‰å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒä¸­çš„æ•°æ®é™åˆ¶ã€é«˜æ ‡æ³¨æˆæœ¬å’Œéšç§å…³æ³¨çš„æŒ‘æˆ˜ã€‚</li>
<li>Polyp-DDPMé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²æ€§èƒ½ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å›¾åƒè´¨é‡æ–¹é¢ï¼Œå…¶FIDå¾—åˆ†ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>åœ¨åˆ†å‰²æ€§èƒ½æ–¹é¢ï¼Œå…¶IoUå¾—åˆ†ä¹Ÿé«˜äºåŸºçº¿æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒå’ŒçœŸå®æ•°æ®ã€‚</li>
<li>Polyp-DDPMç”Ÿæˆçš„é«˜è´¨é‡ã€å¤šæ ·åŒ–åˆæˆæ•°æ®é›†å¯ç”¨äºè®­ç»ƒï¼Œæé«˜æ¯è‚‰åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„æºä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²å…¬å¼€æä¾›ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.04031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9bf79a830a62ae44664c6ef3ee743ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab8b48f00e4ff12693b68c086e1559c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3407fac6823c4e76f7ea595ff4e0854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873897e8b443db04b52f243086ce9e6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Textual-and-Visual-Prompt-Fusion-for-Image-Editing-via-Step-Wise-Alignment"><a href="#Textual-and-Visual-Prompt-Fusion-for-Image-Editing-via-Step-Wise-Alignment" class="headerlink" title="Textual and Visual Prompt Fusion for Image Editing via Step-Wise   Alignment"></a>Textual and Visual Prompt Fusion for Image Editing via Step-Wise   Alignment</h2><p><strong>Authors:Zhanbo Feng, Zenan Ling, Xinyu Lu, Ci Gong, Feng Zhou, Wugedele Bao, Jie Li, Fan Yang, Robert C. Qiu</strong></p>
<p>The use of denoising diffusion models is becoming increasingly popular in the field of image editing. However, current approaches often rely on either image-guided methods, which provide a visual reference but lack control over semantic consistency, or text-guided methods, which ensure alignment with the text guidance but compromise visual quality. To resolve this issue, we propose a framework that integrates a fusion of generated visual references and text guidance into the semantic latent space of a \textit{frozen} pre-trained diffusion model. Using only a tiny neural network, our framework provides control over diverse content and attributes, driven intuitively by the text prompt. Compared to state-of-the-art methods, the framework generates images of higher quality while providing realistic editing effects across various benchmark datasets. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå›¾åƒå¼•å¯¼æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æä¾›äº†è§†è§‰å‚è€ƒï¼Œä½†ç¼ºä¹è¯­ä¹‰ä¸€è‡´æ€§çš„æ§åˆ¶ï¼Œæˆ–è€…æ–‡æœ¬å¼•å¯¼æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ç¡®ä¿äº†ä¸æ–‡æœ¬æŒ‡å¯¼çš„å¯¹é½ï¼Œä½†ç‰ºç‰²äº†è§†è§‰è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒå°†ç”Ÿæˆçš„è§†è§‰å‚è€ƒå’Œæ–‡æœ¬æŒ‡å¯¼èåˆåˆ°ä¸€ä¸ªâ€œå†»ç»“â€çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­ã€‚ä»…ä½¿ç”¨ä¸€ä¸ªå¾®å°çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°±èƒ½å¤Ÿæ§åˆ¶å¤šæ ·çš„å†…å®¹å’Œå±æ€§ï¼Œå¹¶ç”±æ–‡æœ¬æç¤ºç›´è§‚åœ°é©±åŠ¨ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å›¾åƒè´¨é‡æ›´é«˜ï¼ŒåŒæ—¶åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šæä¾›é€¼çœŸçš„ç¼–è¾‘æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15854v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºä¸€ç§èåˆç”Ÿæˆè§†è§‰å‚è€ƒå’Œæ–‡æœ¬æŒ‡å¯¼çš„æ¡†æ¶ï¼Œè§£å†³äº†å½“å‰å›¾åƒç¼–è¾‘é¢†åŸŸä¸­ä½¿ç”¨çš„é™å™ªæ‰©æ•£æ¨¡å‹æ‰€é¢ä¸´çš„è§†è§‰å‚è€ƒç¼ºä¹è¯­ä¹‰ä¸€è‡´æ€§å’Œæ–‡æœ¬æŒ‡å¯¼å½±å“è§†è§‰è´¨é‡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å†»ç»“çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´å†…é›†æˆè§†è§‰å‚è€ƒå’Œæ–‡æœ¬æŒ‡å¯¼çš„èåˆï¼Œä»…ä½¿ç”¨å¾®å°çš„ç¥ç»ç½‘ç»œå°±èƒ½å®ç°å¯¹å¤šæ ·åŒ–å†…å®¹å’Œå±æ€§çš„æ§åˆ¶ï¼Œå¹¶ç”±æ–‡æœ¬æç¤ºç›´è§‰é©±åŠ¨ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å›¾åƒè´¨é‡æ›´é«˜ï¼ŒåŒæ—¶åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šå®ç°é€¼çœŸçš„ç¼–è¾‘æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å›¾åƒç¼–è¾‘ä¸­çš„é™å™ªæ‰©æ•£æ¨¡å‹é¢ä¸´è§†è§‰å‚è€ƒç¼ºä¹è¯­ä¹‰ä¸€è‡´æ€§å’Œæ–‡æœ¬æŒ‡å¯¼å½±å“è§†è§‰è´¨é‡çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶èåˆäº†ç”Ÿæˆçš„è§†è§‰å‚è€ƒå’Œæ–‡æœ¬æŒ‡å¯¼ã€‚</li>
<li>æ¡†æ¶åœ¨å†»ç»“çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´å†…æ“ä½œã€‚</li>
<li>ä»…ä½¿ç”¨å¾®å°çš„ç¥ç»ç½‘ç»œï¼Œå®ç°å¯¹å¤šæ ·åŒ–å†…å®¹å’Œå±æ€§çš„æ§åˆ¶ã€‚</li>
<li>æ¡†æ¶ç”±æ–‡æœ¬æç¤ºç›´è§‰é©±åŠ¨ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å›¾åƒè´¨é‡æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.15854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-483c8e24322a9cad64ca610eb8c3333d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83a89897438297bbb9da7780cd4de8aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-625bbb5d234bcf13b7a4f0938dca9118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e3b313e114a3e139aba3a6081683eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-393a229079647a7df368ed6a6bbc5e1e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1645de84bae1baeeeb3d2d6e7a66bded.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  Rate-My-LoRA Efficient and Adaptive Federated Model Tuning for Cardiac   MRI Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5ea507d6dd336a1ff2913c08f29f1e2d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  AE-NeRF Augmenting Event-Based Neural Radiance Fields for Non-ideal   Conditions and Larger Scene
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">9690.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
