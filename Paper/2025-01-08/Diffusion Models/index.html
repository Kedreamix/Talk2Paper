<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-08  STAR Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-51b9ea5615eb65a30a2a57f256e8d2d2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    70 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-08-更新"><a href="#2025-01-08-更新" class="headerlink" title="2025-01-08 更新"></a>2025-01-08 更新</h1><h2 id="STAR-Spatial-Temporal-Augmentation-with-Text-to-Video-Models-for-Real-World-Video-Super-Resolution"><a href="#STAR-Spatial-Temporal-Augmentation-with-Text-to-Video-Models-for-Real-World-Video-Super-Resolution" class="headerlink" title="STAR: Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution"></a>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution</h2><p><strong>Authors:Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, Ying Tai</strong></p>
<p>Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce\textbf{<del>\name} (\textbf{S}patial-\textbf{T}emporal \textbf{A}ugmentation with T2V models for \textbf{R}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate\textbf{</del>\name}~outperforms state-of-the-art methods on both synthetic and real-world datasets. </p>
<blockquote>
<p>图像扩散模型已被适应于真实世界视频超分辨率，以解决基于GAN的方法中的过度平滑问题。然而，这些模型在维持时间一致性方面遇到困难，因为它们是在静态图像上训练的，无法有效地捕捉时间动态。将文本到视频（T2V）模型整合到视频超分辨率中以改进时间建模是直接的。然而，还有两个主要挑战：现实世界场景中复杂退化引起的伪影，以及由于强大的T2V模型的生成能力而牺牲的保真度（例如CogVideoX-5B）。为了提高恢复视频的时空质量，我们引入了名为STAR的新方法（利用T2V模型进行真实世界视频超分辨率的时空增强）。具体来说，我们在全局注意力块之前引入了一个局部信息增强模块（LIEM），以丰富局部细节并减轻退化伪影。此外，我们提出了一种动态频率（DF）损失来加强保真度，引导模型在不同的扩散步骤中关注不同的频率分量。大量实验表明，STAR在合成和真实世界数据集上都优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02976v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对图像扩散模型在视频超分辨率中遇到的过度平滑问题，本文提出了一种结合文本到视频（T2V）模型的新方法，旨在提高现实世界的视频超分辨率的时空质量。通过引入局部信息增强模块（LIEM）和动态频率（DF）损失，该方法能够在保持时间一致性的同时，实现真实的空间细节和稳健的保真度。实验表明，该方法在合成和真实数据集上均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像扩散模型被用于现实世界的视频超分辨率以提高性能并解决过度平滑问题。</li>
<li>结合文本到视频（T2V）模型以改善视频超分辨率中的时间建模。</li>
<li>面临两个主要挑战：现实场景中复杂退化引入的伪影以及强大T2V模型妥协的保真度。</li>
<li>引入了一种新方法——STAR（时空增强与T2V模型结合用于现实世界的视频超分辨率），实现了真实的空间细节和稳健的时间一致性。</li>
<li>在STAR方法中，引入了局部信息增强模块（LIEM）来丰富局部细节并减轻退化伪影。</li>
<li>提出了动态频率（DF）损失以增强保真度，引导模型在扩散步骤中关注不同的频率成分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ebdced5b4b81215f9c4c078eeb4ce12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43da50125f0eea42467b29cbd3249d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d627edcce481be1431c3221a56fa2020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b580c8988ca4433443f09f059ec23d40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65fd9f311e3460d89e73f7887602354d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f190fa195252ea7359e88f449465ba9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SceneVTG-Controllable-Multilingual-Visual-Text-Generation-in-the-Wild"><a href="#SceneVTG-Controllable-Multilingual-Visual-Text-Generation-in-the-Wild" class="headerlink" title="SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild"></a>SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild</h2><p><strong>Authors:Jiawei Liu, Yuanzhi Zhu, Feiyu Gao, Zhibo Yang, Peng Wang, Junyang Lin, Xinggang Wang, Wenyu Liu</strong></p>
<p>Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, cartoons, etc.), the text in natural scene images needs to meet the following four key criteria: (1) Fidelity: the generated text should appear as realistic as a photograph and be completely accurate, with no errors in any of the strokes. (2) Reasonability: the text should be generated on reasonable carrier areas (such as boards, signs, walls, etc.), and the generated text content should also be relevant to the scene. (3) Utility: the generated text can facilitate to the training of natural scene OCR (Optical Character Recognition) tasks. (4) Controllability: The attribute of the text (such as font and color) should be controllable as needed.In this paper, we propose a two stage method, SceneVTG++, which simultaneously satisfies the four aspects mentioned above. SceneVTG++ consists of a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former utilizes the world knowledge of multi modal large language models to find reasonable text areas and recommend text content according to the nature scene background images, while the latter generates controllable multilingual text based on the diffusion model. Through extensive experiments, we respectively verified the effectiveness of TLCG and CLTD, and demonstrated the state-of-the-art text generation performance of SceneVTG++. In addition, the generated images have superior utility in OCR tasks like text detection and text recognition. Codes and datasets will be available. </p>
<blockquote>
<p>生成自然场景图像中的视觉文本是一项具有许多未解决问题挑战性的任务。与在人工设计的图像（如海报、封面、漫画等）上生成文本不同，自然场景图像中的文本需要满足以下四个关键标准：（1）保真度：生成的文本应看起来尽可能逼真，并且完全准确，没有任何笔触错误。（2）合理性：文本应生成在合理的载体区域（如板报、标志、墙壁等），并且生成的文本内容也应与场景相关。（3）实用性：生成的文本有助于自然场景OCR（光学字符识别）任务的训练。（4）可控性：文本属性（如字体和颜色）应根据需要可控。在本文中，我们提出了一种两阶段方法SceneVTG++，同时满足上述四个方面。SceneVTG++包括文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）。前者利用多模态大型语言模型的世界知识，根据自然场景背景图像找到合理的文本区域，并推荐文本内容；后者基于扩散模型生成可控的多语言文本。通过大量实验，我们分别验证了TLCG和CLTD的有效性，并展示了SceneVTG++的先进文本生成性能。此外，生成的图像在OCR任务（如文本检测和识别）中具有出色的实用性。代码和数据集将可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02962v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出一种两阶段方法SceneVTG++，用于生成自然场景图像中的视觉文本，同时满足真实性、合理性、实用性和可控性四个关键标准。该方法包括文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）。TLCG利用多模态大型语言模型的世界知识，根据自然场景背景图像找到合理的文本区域并推荐文本内容。CLTD则基于扩散模型生成可控的多语言文本。实验证明TLCG和CLTD的有效性，并展示了SceneVTG++在文本生成方面的先进性，同时生成的图像在OCR任务中具有出色的实用性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>生成自然场景图像中的视觉文本是一项具有挑战性的任务，需要满足真实性、合理性、实用性和可控性四个关键标准。</li>
<li>SceneVTG++是一种两阶段方法，包括文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）。</li>
<li>TLCG利用多模态大型语言模型的世界知识，根据自然场景背景图像生成合理的文本布局和内容。</li>
<li>CLTD基于扩散模型生成可控的多语言文本，满足场景文本生成的需求。</li>
<li>实验证明TLCG和CLTD的有效性，SceneVTG++在文本生成方面表现出先进性。</li>
<li>生成的图像在OCR任务中具有出色的实用性，如文本检测和文本识别。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7ae7e1a31f968463fc8e487096d7aec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0667badd4bd08bcb00579d9252616336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f884bbd1bc53163cbbc43f4705ebd05e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InpDiffusion-Image-Inpainting-Localization-via-Conditional-Diffusion-Models"><a href="#InpDiffusion-Image-Inpainting-Localization-via-Conditional-Diffusion-Models" class="headerlink" title="InpDiffusion: Image Inpainting Localization via Conditional Diffusion   Models"></a>InpDiffusion: Image Inpainting Localization via Conditional Diffusion   Models</h2><p><strong>Authors:Kai Wang, Shaozhang Niu, Qixian Hao, Jiwei Zhang</strong></p>
<p>As artificial intelligence advances rapidly, particularly with the advent of GANs and diffusion models, the accuracy of Image Inpainting Localization (IIL) has become increasingly challenging. Current IIL methods face two main challenges: a tendency towards overconfidence, leading to incorrect predictions; and difficulty in detecting subtle tampering boundaries in inpainted images. In response, we propose a new paradigm that treats IIL as a conditional mask generation task utilizing diffusion models. Our method, InpDiffusion, utilizes the denoising process enhanced by the integration of image semantic conditions to progressively refine predictions. During denoising, we employ edge conditions and introduce a novel edge supervision strategy to enhance the model’s perception of edge details in inpainted objects. Balancing the diffusion model’s stochastic sampling with edge supervision of tampered image regions mitigates the risk of incorrect predictions from overconfidence and prevents the loss of subtle boundaries that can result from overly stochastic processes. Furthermore, we propose an innovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting multi-scale features, enhancing feature representation by considering both semantic and edge conditions of the inpainted images. Extensive experiments across challenging datasets demonstrate that the InpDiffusion significantly outperforms existing state-of-the-art methods in IIL tasks, while also showcasing excellent generalization capabilities and robustness. </p>
<blockquote>
<p>随着人工智能的快速发展，尤其是生成对抗网络（GANs）和扩散模型的兴起，图像修复定位（IIL）的准确性越来越受到挑战。当前的IIL方法面临两大挑战：一是过于自信，导致预测错误；二是难以检测修复图像中细微的篡改边界。针对这些问题，我们提出了一种新的方法，将IIL视为利用扩散模型的条件遮罩生成任务。我们的方法InpDiffusion利用结合图像语义条件增强的去噪过程来逐步优化预测。在去噪过程中，我们采用边缘条件并引入了一种新型边缘监督策略，以增强模型对修复物体边缘细节的认知。平衡扩散模型的随机采样与篡改图像区域的边缘监督，降低了因过于自信而导致错误预测的风险，并防止了因过于随机的过程而丢失细微的边界。此外，我们还提出了一种创新性的双流多尺度特征提取器（DMFE），用于提取多尺度特征，通过考虑修复图像的语义和边缘条件来增强特征表示。在具有挑战性的数据集上进行的广泛实验表明，InpDiffusion在IIL任务上显著优于现有最先进的方法，同时显示出卓越泛化能力和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02816v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着人工智能的快速发展，尤其是生成对抗网络（GANs）和扩散模型的出现，图像修复定位（IIL）的准确性越来越具有挑战性。针对当前IIL方法面临的主要挑战，包括过度自信导致预测错误和难以检测修复图像中的微妙篡改边界，我们提出了一种新的基于扩散模型的IIL方法——InpDiffusion。该方法将IIL视为条件遮罩生成任务，利用图像语义条件增强的去噪过程逐步优化预测。通过引入边缘条件和一种新的边缘监督策略，平衡扩散模型的随机采样和篡改图像区域的边缘监督，减少过度自信导致的预测错误，防止过于随机的过程导致的微妙边界损失。此外，我们还提出了一种创新的双流多尺度特征提取器（DMFE），用于提取多尺度特征，通过考虑修复图像的语义和边缘条件，增强特征表示。实验表明，InpDiffusion在IIL任务上显著优于现有的一流方法，同时显示出优秀的泛化能力和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能的进步，尤其是GANs和扩散模型的发展，使得图像修复定位（IIL）的准确性面临挑战。</li>
<li>当前IIL方法存在两个主要挑战：过度自信导致预测错误和难以检测修复图像中的微妙篡改边界。</li>
<li>InpDiffusion方法将IIL视为条件遮罩生成任务，利用扩散模型进行处理。</li>
<li>InpDiffusion利用图像语义条件增强的去噪过程逐步优化预测。</li>
<li>通过引入边缘条件和边缘监督策略，提高了模型对修复物体边缘细节的感知。</li>
<li>双流多尺度特征提取器（DMFE）用于提取多尺度特征，增强特征表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d278b4d36bf165c5a97961f8c4324d48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1719852b3b0636a49728c4f44f4efb2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b133d88d2ed0d8210005e3a347ce595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea42b78e280b49955aad93d4894ccadc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01aca24244e2b4af3417beed744209f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54b0f8bc49efb19ba53011160e891cee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Face-MakeUp-Multimodal-Facial-Prompts-for-Text-to-Image-Generation"><a href="#Face-MakeUp-Multimodal-Facial-Prompts-for-Text-to-Image-Generation" class="headerlink" title="Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation"></a>Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation</h2><p><strong>Authors:Dawei Dai, Mingming Jia, Yinxiu Zhou, Hang Xing, Chenghang Li</strong></p>
<p>Facial images have extensive practical applications. Although the current large-scale text-image diffusion models exhibit strong generation capabilities, it is challenging to generate the desired facial images using only text prompt. Image prompts are a logical choice. However, current methods of this type generally focus on general domain. In this paper, we aim to optimize image makeup techniques to generate the desired facial images. Specifically, (1) we built a dataset of 4 million high-quality face image-text pairs (FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to maintain consistency with the reference facial image, we extract&#x2F;learn multi-scale content features and pose features for the facial image, integrating these into the diffusion model to enhance the preservation of facial identity features for diffusion models. Validation on two face-related test datasets demonstrates that our Face-MakeUp can achieve the best comprehensive performance.All codes are available at:<a target="_blank" rel="noopener" href="https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp">https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp</a> </p>
<blockquote>
<p>面部图像具有广泛的应用。虽然现有的大规模文本-图像扩散模型表现出强大的生成能力，但仅使用文本提示生成所需的面部图像是一个挑战。图像提示是一种合理的选择。然而，目前的方法通常集中在一般领域。本文旨在优化化妆技术来生成所需的面部图像。具体来说，(1)我们基于LAION-Face构建了包含4百万个高质量面部图像文本对的数据集FaceCaptionHQ-4M，用于训练我们的Face-MakeUp模型；(2)为了保持与参考面部图像的一致性，我们提取&#x2F;学习面部图像的多尺度内容特征和姿态特征，将这些特征集成到扩散模型中，以提高扩散模型对面部身份特征的保留能力。在两个与面部相关的测试数据集上的验证表明，我们的Face-MakeUp可以达到最佳的综合性能。所有代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp">https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02523v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了针对面部图像生成优化的研究。文中构建了大规模的面部图像文本对数据集FaceCaptionHQ-4M，并基于此训练了Face-MakeUp模型。模型在集成扩散模型时，提取多尺度内容和姿势特征以保持与参考面部图像的一致性，增强了面部特征的保护能力。在两项面部测试数据集上的验证表明，Face-MakeUp取得了最佳的综合性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本主要关注面部图像的生成优化问题。</li>
<li>构建了一个大规模的面部图像文本对数据集FaceCaptionHQ-4M用于训练模型。</li>
<li>Face-MakeUp模型通过提取多尺度内容和姿势特征来保持与参考面部图像的一致性。</li>
<li>该模型在集成扩散模型时增强了面部特征的保护能力。</li>
<li>Face-MakeUp模型在两项面部测试数据集上实现了最佳的综合性能。</li>
<li>提供的链接中包含模型的代码资源供下载和学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02523">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e974e905af6a350d16f4fbb90c58b1cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c89135600b6792eee3266eaf3b1d84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f6ddf112692682b9f9086760dd080f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab3b11bd0c3d285f80b7215755e0ad1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00e4bd65da6ec90cdb2acb0407700d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b9ea5615eb65a30a2a57f256e8d2d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbba83f1527c5043a46c49007d842e5e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ACE-Instruction-Based-Image-Creation-and-Editing-via-Context-Aware-Content-Filling"><a href="#ACE-Instruction-Based-Image-Creation-and-Editing-via-Context-Aware-Content-Filling" class="headerlink" title="ACE++: Instruction-Based Image Creation and Editing via Context-Aware   Content Filling"></a>ACE++: Instruction-Based Image Creation and Editing via Context-Aware   Content Filling</h2><p><strong>Authors:Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, Jingren Zhou</strong></p>
<p>We report ACE++, an instruction-based diffusion framework that tackles various image generation and editing tasks. Inspired by the input format for the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context Condition Unit (LCU) introduced in ACE and extend this input paradigm to any editing and generation tasks. To take full advantage of image generative priors, we develop a two-stage training scheme to minimize the efforts of finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the first stage, we pre-train the model using task data with the 0-ref tasks from the text-to-image model. There are many models in the community based on the post-training of text-to-image foundational models that meet this training paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with painting tasks and can be used as an initialization to accelerate the training process. In the second stage, we finetune the above model to support the general instructions using all tasks defined in ACE. To promote the widespread application of ACE++ in different scenarios, we provide a comprehensive set of models that cover both full finetuning and lightweight finetuning, while considering general applicability and applicability in vertical scenarios. The qualitative analysis showcases the superiority of ACE++ in terms of generating image quality and prompt following ability. </p>
<blockquote>
<p>我们报告了ACE++，这是一个基于指令的扩散框架，用于处理各种图像生成和编辑任务。我们受到FLUX.1-Fill-dev提出的填充任务输入格式的启发，改进了ACE中的长上下文条件单元（LCU），并将这一输入范式扩展到任何编辑和生成任务。为了充分利用图像生成的先验知识，我们开发了一种两阶段训练方案，以最小化调整强大文本到图像扩散模型（如FLUX.1-dev）的努力。在第一阶段，我们使用文本到图像模型的0-ref任务数据对模型进行预训练。社区中有许多基于文本到图像基础模型的后续训练模型，符合第一阶段的这种训练范式。例如，FLUX.1-Fill-dev主要处理绘画任务，并可以用作初始化来加速训练过程。在第二阶段，我们对上述模型进行微调，以使用ACE中定义的所有任务来支持一般指令。为了促进ACE++在不同场景中的广泛应用，我们提供了一套全面的模型，涵盖全量微调和轻量级微调，同时考虑通用性和垂直场景适用性。定性分析展示了ACE++在生成图像质量和遵循提示方面的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02487v1">PDF</a> </p>
<p><strong>Summary</strong><br>     报告了ACE++，一个基于指令的扩散框架，用于处理各种图像生成和编辑任务。受FLUX.1-Fill-dev对补全任务输入格式的启发，改进了ACE中的长上下文条件单元（LCU），并将此输入范式扩展到任何编辑和生成任务。为了充分利用图像生成先验知识，开发了一个两阶段训练方案，以最小化微调强大文本到图像扩散模型（如FLUX.1-dev）的努力。首先使用文本到图像模型的0-ref任务进行模型预训练，然后进行模型微调以支持通用指令集。此外，提供了涵盖全量微调与轻量级微调的综合模型集，考虑了通用应用和垂直场景的应用。ACE++在生成图像质量和遵循提示方面表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ACE++是一个基于指令的扩散框架，用于处理图像生成和编辑任务。</li>
<li>改进了ACE中的长上下文条件单元（LCU），并扩展了输入范式以支持多种任务。</li>
<li>开发了一个两阶段训练方案，以利用图像生成先验知识并简化微调过程。</li>
<li>预训练阶段使用文本到图像模型的0-ref任务数据。</li>
<li>FLUX.1-Fill-dev等模型符合第一阶段训练模式，可用于加速训练过程。</li>
<li>提供涵盖全量微调与轻量级微调的模型集以适应不同应用场景的需求。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02487">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4db4169f9d5e968125558d9a50e98f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51c331198cddbfc467b25dc11bccb943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fd0b1f89cfebba62a8550ba5afd889c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DeTrack-In-model-Latent-Denoising-Learning-for-Visual-Object-Tracking"><a href="#DeTrack-In-model-Latent-Denoising-Learning-for-Visual-Object-Tracking" class="headerlink" title="DeTrack: In-model Latent Denoising Learning for Visual Object Tracking"></a>DeTrack: In-model Latent Denoising Learning for Visual Object Tracking</h2><p><strong>Authors:Xinyu Zhou, Jinglun Li, Lingyi Hong, Kaixun Jiang, Pinxue Guo, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model’s robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets. </p>
<blockquote>
<p>先前视觉对象跟踪方法采用图像特征回归模型或坐标自回归模型进行边界框预测。图像特征回归方法严重依赖于匹配结果，并不利用位置先验，而自回归方法只能使用训练集中可用的边界框进行训练，在测试未见数据时可能表现不佳。受扩散模型的启发，去噪学习提高了模型对未见数据的鲁棒性。因此，我们向边界框引入噪声，生成用于训练的噪声框，从而提高模型在测试数据上的鲁棒性。我们提出了一种新的范式，将视觉对象跟踪问题表述为一个去噪学习过程。然而，通常要求跟踪算法实时运行，直接将扩散模型应用于对象跟踪会严重损害跟踪速度。因此，我们将去噪学习过程分解为模型内的每个去噪块，而不是多次运行模型，因此我们将所提出的方法概括为模型内的潜在去噪学习过程。具体来说，我们提出了一种去噪视觉转换器（ViT），它由多个去噪块组成。在去噪块中，模板和搜索嵌入被投射到每个去噪块作为条件。去噪块负责去除预测边界框中的噪声，多个堆叠的去噪块协同完成整个去噪过程。然后，我们利用图像特征和轨迹信息来优化去噪后的边界框。此外，我们还利用轨迹记忆和视觉记忆来提高跟踪的稳定性。实验结果验证了我们的方法的有效性，在几个具有挑战性的数据集上实现了具有竞争力的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02467v1">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个新的视觉物体跟踪方法，采用扩散模型中的去噪学习来提升模型性能。为提高模型鲁棒性，引入噪声到边界框训练中，并将物体跟踪问题转化为去噪学习过程。为提高跟踪速度，将去噪学习过程分解为模型内的各个去噪块，而非多次运行模型。提出一种去噪视觉转换器（ViT），包含多个去噪块，利用图像特征和轨迹信息优化去噪边界框，同时使用轨迹记忆和视觉记忆提高跟踪稳定性。实验结果表明该方法有效，在多个挑战数据集上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入扩散模型的去噪学习提升视觉物体跟踪模型的性能。</li>
<li>通过在训练中引入噪声提高模型对未见数据的鲁棒性。</li>
<li>将视觉物体跟踪问题转化为去噪学习过程。</li>
<li>为提高跟踪速度，将去噪学习过程分解为模型内的各个去噪块。</li>
<li>提出一种去噪视觉转换器（ViT），包含多个去噪块。</li>
<li>利用图像特征和轨迹信息优化去噪边界框。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-006f1af14d5b80dc3561fc05ba2a586e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-742cd1775bb7bf69b0589966b39f0dee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5734442099b4dbc31e001d4ff021705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74a541aff93b5c52a4ab0ebecd1f7912.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2665de08831ba71dc330c8513de96fb3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CorrFill-Enhancing-Faithfulness-in-Reference-based-Inpainting-with-Correspondence-Guidance-in-Diffusion-Models"><a href="#CorrFill-Enhancing-Faithfulness-in-Reference-based-Inpainting-with-Correspondence-Guidance-in-Diffusion-Models" class="headerlink" title="CorrFill: Enhancing Faithfulness in Reference-based Inpainting with   Correspondence Guidance in Diffusion Models"></a>CorrFill: Enhancing Faithfulness in Reference-based Inpainting with   Correspondence Guidance in Diffusion Models</h2><p><strong>Authors:Kuan-Hung Liu, Cheng-Kun Yang, Min-Hung Chen, Yu-Lun Liu, Yen-Yu Lin</strong></p>
<p>In the task of reference-based image inpainting, an additional reference image is provided to restore a damaged target image to its original state. The advancement of diffusion models, particularly Stable Diffusion, allows for simple formulations in this task. However, existing diffusion-based methods often lack explicit constraints on the correlation between the reference and damaged images, resulting in lower faithfulness to the reference images in the inpainting results. In this work, we propose CorrFill, a training-free module designed to enhance the awareness of geometric correlations between the reference and target images. This enhancement is achieved by guiding the inpainting process with correspondence constraints estimated during inpainting, utilizing attention masking in self-attention layers and an objective function to update the input tensor according to the constraints. Experimental results demonstrate that CorrFill significantly enhances the performance of multiple baseline diffusion-based methods, including state-of-the-art approaches, by emphasizing faithfulness to the reference images. </p>
<blockquote>
<p>在基于参考的图像修复任务中，提供额外的参考图像以将损坏的目标图像恢复到其原始状态。扩散模型的进步，特别是Stable Diffusion，为此任务提供了简单的公式。然而，现有的基于扩散的方法在参考图像和受损图像之间的相关性上往往缺乏明确的约束，导致修复结果对参考图像的忠实度较低。在这项工作中，我们提出了CorrFill，这是一个无需训练的设计模块，旨在增强参考图像和目标图像之间几何关系的意识。这种增强是通过在修复过程中使用在修复过程中估计的对应约束来实现的，利用自注意力层中的注意力掩码和客观函数来根据约束更新输入张量。实验结果表明，CorrFill通过强调对参考图像的忠实度，显著提高了多种基线扩散方法的性能，包括最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02355v1">PDF</a> WACV 2025. Project page: <a target="_blank" rel="noopener" href="https://corrfill.github.io/">https://corrfill.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于参考图像的修复任务中，利用扩散模型尤其是Stable Diffusion的进步简化了任务。然而现有扩散方法缺乏参考图像与受损图像间关联的显式约束，导致修复结果对参考图像的忠实度较低。本研究提出一种无需训练的模块CorrFill，通过估计修复过程中的对应约束来增强两者之间的几何关联，利用自注意力层的注意力掩码和客观函数来更新输入张量以适应这些约束。实验结果显示，CorrFill能显著提高多种基线扩散方法的性能，包括最先进的方法，重点提高结果对参考图像的忠实度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>参考图像修复任务中引入了扩散模型，特别是Stable Diffusion，简化了任务流程。</li>
<li>现有扩散方法缺乏对参考与受损图像间关联的显式约束。</li>
<li>CorrFill是一种无需训练的模块，旨在增强参考图像和目标图像之间的几何关联。</li>
<li>CorrFill通过估计修复过程中的对应约束来指导修复过程。</li>
<li>利用自注意力层的注意力掩码实现CorrFill的增强功能。</li>
<li>通过客观函数更新输入张量以适应对应约束。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02355">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4f7d1f9569452ba0d375bcf70616d514.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f4a78e323c6c8817730efd322bbbeb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44d0506c8872dc2e8243c895ebc3f865.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce375f7f4735f2715294779da2f7f06e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7edd4a1da45054724c2ffa8acc9666b1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models"><a href="#Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models" class="headerlink" title="Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models"></a>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models</h2><p><strong>Authors:Jingfeng Yao, Xinggang Wang</strong></p>
<p>Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs–representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a>. </p>
<blockquote>
<p>基于Transformer架构的潜在扩散模型在生成高保真图像方面表现出色。然而，最近的研究揭示了这种两阶段设计中的优化困境：虽然增加视觉标记器中的每令牌特征维度提高了重建质量，但要实现相当的生成性能，需要大量的扩散模型以及更多的训练迭代次数。因此，现有系统通常选择次优解决方案，要么由于标记器中的信息丢失而产生视觉伪影，要么由于计算成本高昂而无法完全收敛。我们认为，这种困境源于学习无约束的高维潜在空间的固有难度。针对这一问题，我们提出在训练视觉标记器时，将其潜在空间与预训练的视觉基础模型进行对齐。我们提出的VA-VAE（视觉基础模型对齐变分自编码器）显著扩展了潜在扩散模型的重建-生成边界，使扩散Transformer（DiT）在高维潜在空间中更快收敛。为了充分发挥VA-VAE的潜力，我们采用改进的训练策略和架构设计，建立了一个增强的DiT基线，称为LightningDiT。集成系统在ImageNet 256x256生成任务上实现了最新性能，FID得分为1.35，同时显示出显著的训练效率，仅在64个周期内就达到了2.11的FID得分，与原始DiT相比，实现了超过21倍的收敛速度提升。模型和代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT%E3%80%82">https://github.com/hustvl/LightningDiT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01423v2">PDF</a> Models and codes are available at:   <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于Transformer架构的潜在扩散模型在高保真图像生成方面的优势。研究揭示了该模型的两阶段设计中的优化困境：提高视觉标记器中的每令牌特征维度可提高重建质量，但需要更大的扩散模型和更多的训练迭代来达到相当的生成性能。为此，提出了与预训练视觉基础模型对齐训练视觉标记器的方法，并推出了VA-VAE（与视觉基础模型对齐的变分自动编码器）。这显著扩展了潜在扩散模型的重建-生成边界，使扩散Transformer（DiT）在高维潜在空间中更快收敛。配合增强的DiT基线（LightningDiT），在ImageNet 256x256生成方面达到最新性能，FID得分为1.35，并在仅64个周期内达到2.11的FID得分，实现了超过21倍的收敛速度提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜在扩散模型可生成高保真图像，但存在优化困境：提高视觉标记器中的每令牌特征维度会提高重建质量，但增加扩散模型大小和训练迭代次数。</li>
<li>现有解决方案常产生视觉伪影或计算成本高昂，无法完全收敛。</li>
<li>问题的根源在于学习无约束的高维潜在空间的内在困难。</li>
<li>提出VA-VAE（与视觉基础模型对齐的变分自动编码器）来解决此问题，显著扩展了潜在扩散模型的重建-生成边界。</li>
<li>VA-VAE配合增强DiT（LightningDiT）实现快速收敛，在ImageNet 256x256生成方面达到最新性能。</li>
<li>LightningDiT在ImageNet上实现了FID 1.35的优异成绩，并在仅64个周期内达到FID 2.11，相比原始DiT有显著的训练效率提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01423">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fb1d13f141b61e6932647a2c74b65b37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d0ce253c5009144c05686797926f35d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8877c5be4035d26b5ebd6da24d11bd30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e60dfd4398a443f913d36d2291ba73dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0320301a76d4f34cf5debcf77a8b2246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a5491fc00b9cb05f455c3ee8b31acdd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LVMark-Robust-Watermark-for-latent-video-diffusion-models"><a href="#LVMark-Robust-Watermark-for-latent-video-diffusion-models" class="headerlink" title="LVMark: Robust Watermark for latent video diffusion models"></a>LVMark: Robust Watermark for latent video diffusion models</h2><p><strong>Authors:MinHyuk Jang, Youngdong Jang, JaeHyeok Lee, Kodai Kawamura, Feng Yang, Sangpil Kim</strong></p>
<p>Rapid advancements in generative models have made it possible to create hyper-realistic videos. As their applicability increases, their unauthorized use has raised significant concerns, leading to the growing demand for techniques to protect the ownership of the generative model itself. While existing watermarking methods effectively embed watermarks into image-generative models, they fail to account for temporal information, resulting in poor performance when applied to video-generative models. To address this issue, we introduce a novel watermarking method called LVMark, which embeds watermarks into video diffusion models. A key component of LVMark is a selective weight modulation strategy that efficiently embeds watermark messages into the video diffusion model while preserving the quality of the generated videos. To accurately decode messages in the presence of malicious attacks, we design a watermark decoder that leverages spatio-temporal information in the 3D wavelet domain through a cross-attention module. To the best of our knowledge, our approach is the first to highlight the potential of video-generative model watermarking as a valuable tool for enhancing the effectiveness of ownership protection in video-generative models. </p>
<blockquote>
<p>生成模型的快速发展使得创建超现实视频成为可能。随着其适用性的增强，未经授权的广泛使用引发了重大担忧，因此对保护生成模型本身所有权的技术需求不断增长。虽然现有的水印方法能够有效地将水印嵌入图像生成模型中，但它们无法考虑时间信息，当应用于视频生成模型时，表现不佳。为了解决这一问题，我们提出了一种新型水印方法——LVMark，该方法将水印嵌入视频扩散模型中。LVMark的关键组成部分是选择性权重调制策略，该策略能够高效地将水印信息嵌入视频扩散模型中，同时保持生成视频的质量。为了准确解码恶意攻击中的信息，我们设计了一种水印解码器，它利用三维小波域中的时空信息，通过交叉注意模块实现。据我们所知，我们的方法是第一个强调视频生成模型水印在增强视频生成模型所有权保护方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09122v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>新一代生成模型技术迅速崛起，能够在视频领域生成超逼真的内容。随着其应用的广泛，未经授权的使用引发了人们对保护生成模型所有权的强烈需求。现有的水印方法虽然能嵌入图像生成模型中，但无法处理视频中的时间信息，因此在视频生成模型中的应用表现不佳。为解决这一问题，我们推出了一种名为LVMark的新型水印方法，它能选择性地嵌入视频扩散模型中。该方法的关键在于选择性权重调制策略，能够在保证视频质量的同时高效嵌入水印信息。同时，我们还设计了一种水印解码器，通过利用三维小波域中的时空信息以及交叉注意力模块，来抵御恶意攻击并准确解码信息。我们的方法开辟了视频生成模型水印技术的新可能，提高了保护视频生成模型所有权的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型技术迅速发展，带来高度逼真的视频内容创作能力。</li>
<li>未经授权的生成模型使用引发所有权保护需求。</li>
<li>现有水印方法无法有效处理视频中的时间信息。</li>
<li>LVMark方法通过选择性权重调制策略嵌入水印信息。</li>
<li>LVMark能够保护视频生成模型的所有权。</li>
<li>设计的水印解码器利用时空信息及交叉注意力模块应对恶意攻击。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae0f609915f120d45fa97a9e73fc7c28.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-08\./crop_Diffusion Models/2412.09122v2/page_2_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4730f5db513ba8ee55f22bde7b0dd23.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AnyDressing-Customizable-Multi-Garment-Virtual-Dressing-via-Latent-Diffusion-Models"><a href="#AnyDressing-Customizable-Multi-Garment-Virtual-Dressing-via-Latent-Diffusion-Models" class="headerlink" title="AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent   Diffusion Models"></a>AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent   Diffusion Models</h2><p><strong>Authors:Xinghui Li, Qichao Sun, Pengze Zhang, Fulong Ye, Zhichao Liao, Wanquan Feng, Songtao Zhao, Qian He</strong></p>
<p>Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results. </p>
<blockquote>
<p>基于扩散模型的文本和图像提示的服装中心图像生成方面的最新进展令人印象深刻。然而，现有方法不支持多种服装组合，并且在保持对文本提示的忠实度的同时，难以保留服装细节，这在多种场景中限制了它们的性能。在本文中，我们专注于一个新的任务，即多服装虚拟换装，并提出了一种新的AnyDressing方法，用于根据任何服装组合和任何个性化文本提示进行角色定制。AnyDressing主要包括两个网络，即GarmentsNet和DressingNet，它们分别专注于提取详细的服装特征和生成定制图像。具体来说，我们在GarmentsNet中提出了一个高效且可扩展的模块，称为服装特定特征提取器，以并行方式单独编码服装纹理。这种设计可以防止服装混淆，同时确保网络效率。同时，我们在DressingNet中设计了一种自适应的着装注意力机制和一种新颖的实例级服装定位学习策略，以准确地将多服装特征注入其相应区域。这种方法有效地将多服装纹理线索融入生成的图像中，并进一步增强了文本-图像的一致性。此外，我们还引入了一种服装增强纹理学习策略，以提高服装的细粒度纹理细节。由于我们精心设计，AnyDressing可以作为一个插件模块轻松集成到任何扩散模型的社区控制扩展中，提高合成图像的多样性和可控性。大量实验表明，AnyDressing达到了最先进的成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04146v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://crayon-shinchan.github.io/AnyDressing/">https://crayon-shinchan.github.io/AnyDressing/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的衣物导向图像生成技术的新进展。针对现有方法缺乏对不同衣物组合的支持以及在保持衣物细节和忠实于文本提示方面的挑战，提出了一种新的方法AnyDressing。AnyDressing包含两个主要网络：GarmentsNet和DressingNet，分别用于提取衣物详细特征和生成定制图像。通过采用高效的Garment-Specific Feature Extractor和创新的Dressing-Attention机制等技术，AnyDressing能够在生成图像中准确融入多衣物纹理，并提升文本与图像的契合度。此外，AnyDressing还可作为插件模块轻松集成到任何扩散模型的社区控制扩展中，提高了生成图像的多样性和可控性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了基于扩散模型的衣物导向图像生成的新进展。</li>
<li>现有方法在面对不同衣物组合及保持衣物细节和文本提示忠实度方面存在挑战。</li>
<li>AnyDressing方法包含GarmentsNet和DressingNet两个主要网络，分别用于提取衣物特征和生成定制图像。</li>
<li>AnyDressing采用了高效的Garment-Specific Feature Extractor来并行编码衣物纹理，防止衣物混淆并保证网络效率。</li>
<li>创新的Dressing-Attention机制和Instance-Level Garment Localization Learning策略用于准确将多衣物特征注入对应区域，并提升文本与图像的契合度。</li>
<li>引入Garment-Enhanced Texture Learning策略，以提高衣物的精细纹理细节。</li>
<li>AnyDressing可作为插件模块集成到扩散模型中，提高了生成图像的多样性和可控性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04146">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aaff9028ff5be81c50f74590a661decb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e14838690b0fdeb10796fc5300b15095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b40d7fb02fea182faeaab9e064f607ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f86299f8bedc0c644a9f7bceb386c1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TC-KANRecon-High-Quality-and-Accelerated-MRI-Reconstruction-via-Adaptive-KAN-Mechanisms-and-Intelligent-Feature-Scaling"><a href="#TC-KANRecon-High-Quality-and-Accelerated-MRI-Reconstruction-via-Adaptive-KAN-Mechanisms-and-Intelligent-Feature-Scaling" class="headerlink" title="TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via   Adaptive KAN Mechanisms and Intelligent Feature Scaling"></a>TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via   Adaptive KAN Mechanisms and Intelligent Feature Scaling</h2><p><strong>Authors:Ruiquan Ge, Xiao Yu, Yifei Chen, Guanyu Zhou, Fan Jia, Shenghao Zhu, Junhao Jia, Chenyan Zhang, Yifei Sun, Dong Zeng, Changmiao Wang, Qiegen Liu, Shanzhou Niu</strong></p>
<p>Magnetic Resonance Imaging (MRI) has become essential in clinical diagnosis due to its high resolution and multiple contrast mechanisms. However, the relatively long acquisition time limits its broader application. To address this issue, this study presents an innovative conditional guided diffusion model, named as TC-KANRecon, which incorporates the Multi-Free U-KAN (MF-UKAN) module and a dynamic clipping strategy. TC-KANRecon model aims to accelerate the MRI reconstruction process through deep learning methods while maintaining the quality of the reconstructed images. The MF-UKAN module can effectively balance the tradeoff between image denoising and structure preservation. Specifically, it presents the multi-head attention mechanisms and scalar modulation factors, which significantly enhances the model’s robustness and structure preservation capabilities in complex noise environments. Moreover, the dynamic clipping strategy in TC-KANRecon adjusts the cropping interval according to the sampling steps, thereby mitigating image detail loss typicalching the visual features of the images. Furthermore, the MC-Model incorporates full-sampling k-space information, realizing efficient fusion of conditional information, enhancing the model’s ability to process complex data, and improving the realism and detail richness of reconstructed images. Experimental results demonstrate that the proposed method outperforms other MRI reconstruction methods in both qualitative and quantitative evaluations. Notably, TC-KANRecon method exhibits excellent reconstruction results when processing high-noise, low-sampling-rate MRI data. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/lcbkmm/TC-KANRecon">https://github.com/lcbkmm/TC-KANRecon</a>. </p>
<blockquote>
<p>磁共振成像（MRI）因其高分辨率和多种对比机制在临床诊断中变得至关重要。然而，相对较长的采集时间限制了其更广泛的应用。为解决这一问题，本研究提出了一种创新的有条件引导扩散模型，名为TC-KANRecon。它结合了多自由度U-KAN（MF-UKAN）模块和动态裁剪策略。TC-KANRecon模型旨在通过深度学习方法加速MRI重建过程，同时保持重建图像的质量。MF-UKAN模块可以有效地平衡图像去噪和结构保持之间的权衡。具体来说，它引入了多头注意机制和标量调制因子，这显著增强了模型在复杂噪声环境中的稳健性和结构保持能力。此外，TC-KANRecon中的动态裁剪策略根据采样步骤调整裁剪间隔，从而减轻图像细节损失，保留图像视觉特征。此外，MC模型结合了全采样k空间信息，实现了条件信息的有效融合，提高了模型处理复杂数据的能力，提高了重建图像的逼真度和细节丰富度。实验结果表明，所提出的方法在定性和定量评估方面都优于其他MRI重建方法。值得注意的是，TC-KANRecon方法在处理高噪声、低采样率的MRI数据时表现出优异的重建效果。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/lcbkmm/TC-KANRecon">https://github.com/lcbkmm/TC-KANRecon</a>上获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05705v2">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong><br>     该研究提出了一种名为TC-KANRecon的条件引导扩散模型，用于加速MRI重建过程。该模型结合了Multi-Free U-KAN模块和动态裁剪策略，旨在通过深度学习方法提高MRI图像重建的速度和质量。TC-KANRecon在复杂噪声环境中具有出色的鲁棒性和结构保持能力，并能有效平衡图像去噪和结构保持之间的权衡。此外，该模型的动态裁剪策略可根据采样步骤调整裁剪间隔，减少图像细节损失。实验结果表明，该方法在定性和定量评估上都优于其他MRI重建方法，尤其适用于处理高噪声、低采样率的MRI数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TC-KANRecon是一种基于深度学习的条件引导扩散模型，旨在加速MRI图像重建过程。</li>
<li>该模型结合了Multi-Free U-KAN模块，具有出色的鲁棒性和结构保持能力，能在复杂噪声环境中有效平衡图像去噪和结构保持之间的权衡。</li>
<li>TC-KANRecon采用动态裁剪策略，可根据采样步骤调整裁剪间隔，减少图像细节损失。</li>
<li>实验结果表明，TC-KANRecon在MRI图像重建方面优于其他方法，特别是在处理高噪声、低采样率的MRI数据时表现优异。</li>
<li>TC-KANRecon模型实现了全采样k-空间信息的融合，提高了模型处理复杂数据的能力，增强了重建图像的真实感和细节丰富度。</li>
<li>模型的源代码已公开发布在指定GitHub仓库，方便研究者和医生等人员获取和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05705">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-afb2b65a29e55cd143da8bc2f5e9b2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cd794284f8b47cf03d76f198abe3bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79929f4327c25052aee5265e83e3a03e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-609e44b5a16734bcb86e88967327aac9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Image-Diffusion-Generation-of-Image-Sequence-and-Application-in-MRI"><a href="#Autoregressive-Image-Diffusion-Generation-of-Image-Sequence-and-Application-in-MRI" class="headerlink" title="Autoregressive Image Diffusion: Generation of Image Sequence and   Application in MRI"></a>Autoregressive Image Diffusion: Generation of Image Sequence and   Application in MRI</h2><p><strong>Authors:Guanxiong Luo, Shoujin Huang, Martin Uecker</strong></p>
<p>Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality. However, a persistent challenge lies in balancing image quality with imaging speed. This trade-off is primarily constrained by k-space measurements, which traverse specific trajectories in the spatial Fourier domain (k-space). These measurements are often undersampled to shorten acquisition times, resulting in image artifacts and compromised quality. Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled k-space data. In this work, we present the autoregressive image diffusion (AID) model for image sequences and use it to sample the posterior for accelerated MRI reconstruction. The algorithm incorporates both undersampled k-space and pre-existing information. Models trained with fastMRI dataset are evaluated comprehensively. The results show that the AID model can robustly generate sequentially coherent image sequences. In MRI applications, the AID can outperform the standard diffusion model and reduce hallucinations, due to the learned inter-image dependencies. The project code is available at <a target="_blank" rel="noopener" href="https://github.com/mrirecon/aid">https://github.com/mrirecon/aid</a>. </p>
<blockquote>
<p>磁共振成像（MRI）是一种广泛使用的非侵入性成像方式。然而，持续面临的挑战在于平衡图像质量与成像速度。这种权衡主要受到k空间测量的限制，这些测量在空间傅里叶域（k空间）沿特定轨迹行进。为了减少采集时间，这些测量往往进行欠采样，导致图像出现伪影和质量下降。生成模型学习图像分布，可用于从欠采样的k空间数据中重建高质量图像。在这项工作中，我们提出了用于图像序列的自回归图像扩散（AID）模型，并将其用于采样后加速MRI重建。该算法结合了欠采样的k空间和现有信息。用fastMRI数据集训练的模型进行了全面的评估。结果表明，AID模型能够稳健地生成连续的图像序列。在MRI应用中，由于学习到的图像间依赖性，AID可以优于标准扩散模型并减少幻觉。项目代码可在<a target="_blank" rel="noopener" href="https://github.com/mrirecon/aid%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/mrirecon/aid获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14327v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在磁共振成像（MRI）中面临的挑战，即在图像质量与成像速度之间的平衡。文章提出使用生成模型来解决这一问题，特别是介绍了一种名为autoregressive image diffusion（AID）的模型。该模型能够学习图像分布，从欠采样的k-space数据中重建高质量图像。研究结果表明，AID模型能够稳健地生成连续的图像序列，并在MRI应用中表现出优于标准扩散模型的性能，减少了幻觉现象。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>磁共振成像（MRI）在图像质量与成像速度之间存在权衡挑战。</li>
<li>k-space测量在MRI中起到关键作用，但其欠采样会导致图像伪影和质量下降。</li>
<li>生成模型如autoregressive image diffusion（AID）能够从欠采样的k-space数据中重建高质量图像。</li>
<li>AID模型能够学习图像序列的分布，并生成连续的图像序列。</li>
<li>在MRI应用中，AID模型表现出优于标准扩散模型的性能。</li>
<li>AID模型通过利用图像间的依赖性，减少了幻觉现象的出现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8317a3be8b2934d92e9af14c6daa2905.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3115b5c8d9ec06680d45598aac0116b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e5e04727103a2cfdd6c975b8e684ce6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MV-VTON-Multi-View-Virtual-Try-On-with-Diffusion-Models"><a href="#MV-VTON-Multi-View-Virtual-Try-On-with-Diffusion-Models" class="headerlink" title="MV-VTON: Multi-View Virtual Try-On with Diffusion Models"></a>MV-VTON: Multi-View Virtual Try-On with Diffusion Models</h2><p><strong>Authors:Haoyu Wang, Zhilu Zhang, Donglin Di, Shiliang Zhang, Wangmeng Zuo</strong></p>
<p>The goal of image-based virtual try-on is to generate an image of the target person naturally wearing the given clothing. However, existing methods solely focus on the frontal try-on using the frontal clothing. When the views of the clothing and person are significantly inconsistent, particularly when the person’s view is non-frontal, the results are unsatisfactory. To address this challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims to reconstruct the dressing results from multiple views using the given clothes. Given that single-view clothes provide insufficient information for MV-VTON, we instead employ two images, i.e., the frontal and back views of the clothing, to encompass the complete view as much as possible. Moreover, we adopt diffusion models that have demonstrated superior abilities to perform our MV-VTON. In particular, we propose a view-adaptive selection method where hard-selection and soft-selection are applied to the global and local clothing feature extraction, respectively. This ensures that the clothing features are roughly fit to the person’s view. Subsequently, we suggest joint attention blocks to align and fuse clothing features with person features. Additionally, we collect a MV-VTON dataset MVG, in which each person has multiple photos with diverse views and poses. Experiments show that the proposed method not only achieves state-of-the-art results on MV-VTON task using our MVG dataset, but also has superiority on frontal-view virtual try-on task using VITON-HD and DressCode datasets. </p>
<blockquote>
<p>基于图像的虚拟试穿的目标是生成目标人物自然穿着给定服装的图像。然而，现有方法仅专注于使用正面服装进行正面试穿。当服装和人物的视角存在显著差异，特别是人物视角非正面时，结果往往不尽人意。为了解决这一挑战，我们引入了多视角虚拟试穿（MV-VTON），旨在利用给定的服装从多个视角重建着装效果。鉴于单视角衣物为MV-VTON提供的信息不足，我们转而使用两张图像，即服装的正面和背面视图，以尽可能涵盖完整的视图。此外，我们采用了扩散模型，该模型在MV-VTON任务中表现出了卓越的能力。特别是，我们提出了一种视适应选择方法，其中硬选择和软选择分别应用于全局和局部服装特征提取，以确保服装特征大致适应人物的视角。随后，我们建议使用联合注意力块来对齐和融合服装特征与人物特征。此外，我们收集了一个MV-VTON数据集MVG，其中每个人物都有多张具有不同视角和姿态的照片。实验表明，所提出的方法不仅在我们自己的MVG数据集上实现了最先进的MV-VTON任务结果，而且在VITON-HD和DressCode数据集上的正面虚拟试穿任务中也具有优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.17364v4">PDF</a> Accept by AAAI 2025. Project url:   <a target="_blank" rel="noopener" href="https://hywang2002.github.io/MV-VTON/">https://hywang2002.github.io/MV-VTON/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了多视角虚拟试穿（MV-VTON）技术，该技术旨在通过给定的衣物从多个视角重建着装效果。为解决单一视角衣物信息不足的问题，采用前视和后视两张衣物图像来尽可能全面地覆盖视角。采用扩散模型执行MV-VTON任务，并提出一种视图自适应选择方法，对全局和局部衣物特征进行硬选择和软选择。此外，建议采用联合注意力块对齐和融合衣物特征与人像特征。最后收集MVG数据集用于实验验证，结果显示该方法在MV-VTON任务上表现优秀。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MV-VTON技术旨在通过给定的衣物从多个视角重建着装效果，解决视角不一致导致的不满意结果问题。</li>
<li>采用前视和后视两张衣物图像来更全面覆盖视角，解决单一视角衣物信息不足的问题。</li>
<li>采用扩散模型执行MV-VTON任务，表现出卓越的能力。</li>
<li>提出一种视图自适应选择方法，对全局和局部衣物特征进行硬选择和软选择，确保衣物特征与人像视角相适应。</li>
<li>采用联合注意力块来对齐和融合衣物特征与人像特征。</li>
<li>收集MVG数据集用于实验验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.17364">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-512bea25c788c532f9f8e94707e57a0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c673850bcfb92eeb93c984965bd942a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8bd13ba864f1694145d6130bac4d201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb781d57314717524175a281c589f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18cf22952a17ef555b25f50eb1dadcd8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SRAGAN-Saliency-Regularized-and-Attended-Generative-Adversarial-Network-for-Chinese-Ink-wash-Painting-Generation"><a href="#SRAGAN-Saliency-Regularized-and-Attended-Generative-Adversarial-Network-for-Chinese-Ink-wash-Painting-Generation" class="headerlink" title="SRAGAN: Saliency Regularized and Attended Generative Adversarial Network   for Chinese Ink-wash Painting Generation"></a>SRAGAN: Saliency Regularized and Attended Generative Adversarial Network   for Chinese Ink-wash Painting Generation</h2><p><strong>Authors:Xiang Gao, Yuqi Zhang</strong></p>
<p>Recent style transfer problems are still largely dominated by Generative Adversarial Network (GAN) from the perspective of cross-domain image-to-image (I2I) translation, where the pivotal issue is to learn and transfer target-domain style patterns onto source-domain content images. This paper handles the problem of translating real pictures into traditional Chinese ink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though a wide range of I2I models tackle this problem, a notable challenge is that the content details of the source image could be easily erased or corrupted due to the transfer of ink-wash style elements. To remedy this issue, we propose to incorporate saliency detection into the unpaired I2I framework to regularize image content, where the detected saliency map is utilized from two aspects: (\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize object content structure by enforcing saliency consistency before and after image stylization; (\romannumeral2) we propose saliency adaptive normalization (SANorm) which implicitly enhances object structure integrity of the generated paintings by dynamically injecting image saliency information into the generator to guide stylization process. Besides, we also propose saliency attended discriminator which harnesses image saliency information to focus generative adversarial attention onto the drawn objects, contributing to generating more vivid and delicate brush strokes and ink-wash textures. Extensive qualitative and quantitative experiments demonstrate superiority of our approach over related advanced image stylization methods in both GAN and diffusion model paradigms. </p>
<blockquote>
<p>近期风格迁移问题仍然主要从跨域图像到图像（I2I）转换的角度由生成对抗网络（GAN）主导，关键在于学习和将目标域的风格模式转移到源域内容图像上。本文处理将真实图片翻译成传统水墨画的问题，即水墨画风格转换。尽管有许多I2I模型处理这个问题，但一个显著的挑战是源图像的内容细节在转移水墨风格元素时容易被抹去或破坏。为了解决这个问题，我们提出将显著性检测融入非配对I2I框架以规范图像内容，从两个方面利用检测到的显著性图：（1）我们提出显著性IOU（SIOU）损失，通过强制显著性一致性在图像风格化之前和之后显式地规范对象内容结构；（2）我们提出显著性自适应归一化（SANorm），通过动态将图像显著性信息注入生成器以指导风格化过程，隐式地增强生成画作的对象结构完整性。此外，我们还提出了显著性关注鉴别器，它利用图像显著性信息将生成对抗性注意力集中在绘制的对象上，有助于生成更生动、更精细的笔触和水墨纹理。大量的定性和定量实验表明，我们的方法在生成对抗网络和扩散模型范式中均优于相关的先进图像风格化方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15743v2">PDF</a> 34 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>本文解决了将真实图片转化为中国传统水墨画的问题，即水墨画风格转换。文章引入了显著性检测来解决在风格转换过程中源图像内容细节容易丢失或损坏的问题。通过结合无配对图像到图像的框架，提出两种策略：一是显著性IOU（SIOU）损失，通过强制风格化前后的显著性一致性来明确规范对象内容结构；二是显著性自适应归一化（SANorm），通过动态注入图像显著性信息来隐式增强生成画作的对象结构完整性。此外，还提出了显著性关注鉴别器，利用图像显著性信息将生成对抗性注意力集中在绘制对象上，有助于生成更生动、细腻的画笔笔触和墨水纹理。实验证明，该方法在GAN和扩散模型范式中均优于其他先进的图像风格化方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章解决了真实图片转化为水墨画风格的难题，即风格转换问题。</li>
<li>针对风格转换过程中源图像内容细节可能丢失的问题，引入了显著性检测。</li>
<li>提出了两种策略来结合显著性检测：SIOU损失和SANorm方法。</li>
<li>SHOU损失通过强制风格化前后的显著性一致性来规范对象内容结构。</li>
<li>SANorm方法通过动态注入图像显著性信息增强生成画作的对象结构完整性。</li>
<li>引入显著性关注鉴别器，将生成对抗性注意力集中在绘制对象上，提高生成画作的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15743">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8aa9463cad6600699429fa08e88254a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfa3d8c03770e2a1e3d63df40c81942f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdb1867304a1d698ec5a97aee4cd1b34.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generating-Counterfactual-Trajectories-with-Latent-Diffusion-Models-for-Concept-Discovery"><a href="#Generating-Counterfactual-Trajectories-with-Latent-Diffusion-Models-for-Concept-Discovery" class="headerlink" title="Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery"></a>Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery</h2><p><strong>Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed</strong></p>
<p>Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction. </p>
<blockquote>
<p>在医学等高风险领域，可信性是安全应用不透明的深度学习模型的主要先决条件。理解决策过程不仅有助于建立信任，还可能揭示复杂模型的先前未知决策标准，从而推动医学研究的发展。从黑箱模型中发现与决策相关的概念是一项特别具有挑战性的任务。本研究提出了通过基于潜在扩散的反事实轨迹进行概念发现（CDCT），这是一种利用扩散模型的优越图像合成能力进行概念发现的新型三步框架。首先，CDCT使用潜在扩散模型（LDM）生成反事实轨迹数据集。该数据集用于使用变分自编码器（VAE）派生出与分类相关的概念的脱节表示。最后，应用搜索算法来识别脱节潜在空间中的相关概念。将CDCT应用于在最大的公共皮肤病变数据集上训练的分类器，不仅发现了多种偏见，还发现了有意义的生物标志物。此外，CDCT内产生的反事实事实表现出比先前建立的最先进方法更好的FID分数，同时资源效率提高了12倍。无监督概念发现在可信人工智能的应用和各个领域人类知识的进一步发展方面具有巨大潜力。CDCT是朝着这个方向迈出的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10356v2">PDF</a> Published at International Conference on Pattern Recognition (ICPR)   2024</p>
<p><strong>Summary</strong><br>     本研究提出一种名为CDCT的新概念发现框架，通过利用扩散模型的图像合成能力，对深度学习中决策相关的概念进行发现。CDCT包括三个步骤：生成对抗性轨迹数据集、使用变分自编码器进行概念解耦表示，以及在解耦的潜在空间中进行搜索识别相关概念。应用于皮肤病变数据集分类器的应用显示，CDCT不仅揭示了偏见，还发现了有意义的生物标志物，且比现有方法更高效。这一发现对可信赖人工智能的应用和各个领域的人类知识发展具有巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究强调了在医疗等高风险领域，对深度学习模型的可信度的需求。</li>
<li>提出新概念发现框架CDCT，用于揭示深度学习模型的决策标准。</li>
<li>CDCT利用扩散模型生成对抗性轨迹数据集，以解开分类相关的概念。</li>
<li>使用变分自编码器（VAE）进行概念解耦表示。</li>
<li>通过搜索算法在解耦的潜在空间中识别相关概念。</li>
<li>在皮肤病变数据集上的实验揭示了模型的偏见和有意义的生物标志物。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a227c108a3b3766161edb3f6b3768856.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe8d0c84a32b5a6f077cba2a38a25f38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8444517902aeddc867e5f33708d20696.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Polyp-DDPM-Diffusion-Based-Semantic-Polyp-Synthesis-for-Enhanced-Segmentation"><a href="#Polyp-DDPM-Diffusion-Based-Semantic-Polyp-Synthesis-for-Enhanced-Segmentation" class="headerlink" title="Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation"></a>Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation</h2><p><strong>Authors:Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao</strong></p>
<p>This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/mobaidoctor/polyp-ddpm">https://github.com/mobaidoctor/polyp-ddpm</a>. </p>
<blockquote>
<p>本研究介绍了Polyp-DDPM，这是一种基于扩散的方法，用于根据掩膜生成逼真的息肉图像，旨在提高胃肠道（GI）息肉的分割效果。我们的方法解决了与医学图像相关的数据限制、高标注成本和隐私关注等挑战。通过扩散模型采用分割掩膜（代表异常区域的二进制掩膜），Polyp-DDPM在图像质量和分割性能上均优于现有先进技术。在图像质量方面，其弗雷歇因塞特距离（FID）得分为78.47（高于83.79），而在分割性能方面，其交并比（IoU）达到0.7156（基线模型生成的合成图像低于0.6694，真实数据为0.7067）。我们的方法生成高质量、多样化的合成数据集用于训练，从而提高了息肉分割模型的性能，使其与真实图像相当，并提供更大的数据增强能力以改进分割模型。Polyp-DDPM的源代码和预训练权重已在<a target="_blank" rel="noopener" href="https://github.com/mobaidoctor/polyp-ddpm%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/mobaidoctor/polyp-ddpm公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.04031v2">PDF</a> This preprint has been accepted for publication in the proceedings of   the IEEE Engineering in Medicine and Biology Society (EMBC 2024). The final   published version is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1109/EMBC53108.2024.10782077">https://doi.org/10.1109/EMBC53108.2024.10782077</a>. The copyright for this work   has been transferred to IEEE</p>
<p><strong>摘要</strong><br>    本研究提出了Polyp-DDPM，这是一种基于扩散的方法，用于根据掩膜生成逼真的息肉图像，旨在提高胃肠道（GI）息肉的分割效果。该方法解决了与医学图像相关的数据限制、高标注成本和隐私关注的挑战。通过扩散模型对分割掩膜的条件化——代表异常区域的二进制掩膜，Polyp-DDPM在图像质量和分割性能上均优于现有先进技术。在图像质量方面，其Frechet Inception Distance（FID）得分为78.47（优于83.79），在分割性能方面，其Intersection over Union（IoU）得分为0.7156（优于基线模型生成的合成图像的0.6694及以下得分和真实数据的0.7067）。该方法生成高质量、多样化的合成数据集，可用于训练，使息肉分割模型可与真实图像相媲美，并提供更大的数据增强能力，以提高分割模型的性能。Polyp-DDPM的源代码和预训练权重已公开提供于<a target="_blank" rel="noopener" href="https://github.com/mobaidoctor/polyp-ddpm%E3%80%82">https://github.com/mobaidoctor/polyp-ddpm。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>Polyp-DDPM是一种基于扩散的方法，用于根据掩膜生成逼真的息肉图像。</li>
<li>该方法旨在解决医学图像中的数据限制、高标注成本和隐私关注的挑战。</li>
<li>Polyp-DDPM通过条件扩散模型在图像质量和分割性能上均表现出卓越的性能。</li>
<li>在图像质量方面，其FID得分优于现有技术。</li>
<li>在分割性能方面，其IoU得分也高于基线模型生成的合成图像和真实数据。</li>
<li>Polyp-DDPM生成的高质量、多样化合成数据集可用于训练，提高息肉分割模型的性能。</li>
<li>该研究的源代码和预训练权重已公开提供，方便其他研究者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.04031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9bf79a830a62ae44664c6ef3ee743ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab8b48f00e4ff12693b68c086e1559c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3407fac6823c4e76f7ea595ff4e0854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873897e8b443db04b52f243086ce9e6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Textual-and-Visual-Prompt-Fusion-for-Image-Editing-via-Step-Wise-Alignment"><a href="#Textual-and-Visual-Prompt-Fusion-for-Image-Editing-via-Step-Wise-Alignment" class="headerlink" title="Textual and Visual Prompt Fusion for Image Editing via Step-Wise   Alignment"></a>Textual and Visual Prompt Fusion for Image Editing via Step-Wise   Alignment</h2><p><strong>Authors:Zhanbo Feng, Zenan Ling, Xinyu Lu, Ci Gong, Feng Zhou, Wugedele Bao, Jie Li, Fan Yang, Robert C. Qiu</strong></p>
<p>The use of denoising diffusion models is becoming increasingly popular in the field of image editing. However, current approaches often rely on either image-guided methods, which provide a visual reference but lack control over semantic consistency, or text-guided methods, which ensure alignment with the text guidance but compromise visual quality. To resolve this issue, we propose a framework that integrates a fusion of generated visual references and text guidance into the semantic latent space of a \textit{frozen} pre-trained diffusion model. Using only a tiny neural network, our framework provides control over diverse content and attributes, driven intuitively by the text prompt. Compared to state-of-the-art methods, the framework generates images of higher quality while providing realistic editing effects across various benchmark datasets. </p>
<blockquote>
<p>去噪扩散模型在图像编辑领域的应用越来越受欢迎。然而，当前的方法通常依赖于图像引导方法，这些方法提供了视觉参考，但缺乏语义一致性的控制，或者文本引导方法，这些方法确保了与文本指导的对齐，但牺牲了视觉质量。为了解决这一问题，我们提出了一个框架，它将生成的视觉参考和文本指导融合到一个“冻结”的预训练扩散模型的语义潜在空间中。仅使用一个微小的神经网络，我们的框架就能够控制多样的内容和属性，并由文本提示直观地驱动。与最先进的方法相比，该框架生成的图像质量更高，同时在各种基准数据集上提供逼真的编辑效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15854v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本提出一种融合生成视觉参考和文本指导的框架，解决了当前图像编辑领域中使用的降噪扩散模型所面临的视觉参考缺乏语义一致性和文本指导影响视觉质量的问题。该框架在冻结的预训练扩散模型的语义潜在空间内集成视觉参考和文本指导的融合，仅使用微小的神经网络就能实现对多样化内容和属性的控制，并由文本提示直觉驱动。相较于现有方法，该框架生成的图像质量更高，同时在各种基准数据集上实现逼真的编辑效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前图像编辑中的降噪扩散模型面临视觉参考缺乏语义一致性和文本指导影响视觉质量的问题。</li>
<li>提出的框架融合了生成的视觉参考和文本指导。</li>
<li>框架在冻结的预训练扩散模型的语义潜在空间内操作。</li>
<li>仅使用微小的神经网络，实现对多样化内容和属性的控制。</li>
<li>框架由文本提示直觉驱动。</li>
<li>与现有方法相比，该框架生成的图像质量更高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.15854">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-483c8e24322a9cad64ca610eb8c3333d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83a89897438297bbb9da7780cd4de8aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-625bbb5d234bcf13b7a4f0938dca9118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e3b313e114a3e139aba3a6081683eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-393a229079647a7df368ed6a6bbc5e1e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1645de84bae1baeeeb3d2d6e7a66bded.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-08  Rate-My-LoRA Efficient and Adaptive Federated Model Tuning for Cardiac   MRI Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5ea507d6dd336a1ff2913c08f29f1e2d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-08  AE-NeRF Augmenting Event-Based Neural Radiance Fields for Non-ideal   Conditions and Larger Scene
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">9690.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
