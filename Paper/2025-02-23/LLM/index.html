<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-23  OpenCharacter Training Customizable Role-Playing LLMs with Large-Scale   Synthetic Personas">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a47f2f13235fbbbaee0fbc6127180b0e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-23-更新"><a href="#2025-02-23-更新" class="headerlink" title="2025-02-23 更新"></a>2025-02-23 更新</h1><h2 id="OpenCharacter-Training-Customizable-Role-Playing-LLMs-with-Large-Scale-Synthetic-Personas"><a href="#OpenCharacter-Training-Customizable-Role-Playing-LLMs-with-Large-Scale-Synthetic-Personas" class="headerlink" title="OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale   Synthetic Personas"></a>OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale   Synthetic Personas</h2><p><strong>Authors:Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, Dong Yu</strong></p>
<p>Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs with character generalization capabilities. We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses. To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release our synthetic characters and instruction-tuning dialogues to support public research. </p>
<blockquote>
<p>在大规模语言模型（LLM）中的角色定制，也被称为角色泛化，因其角色扮演对话代理的开发和部署中的灵活性和成本效益而受到越来越多的关注。本研究探索了一种大规模数据合成方法来为LLM提供角色泛化能力。我们首先使用Persona Hub中的人物来合成大规模的角色特征，然后探索两种策略：响应重写和响应生成，来创建与角色对齐的指令响应。为了验证我们合成指令调整数据对于角色泛化的有效性，我们使用LLaMA-3 8B模型进行有监督微调（SFT）。我们表现最好的模型强化了原始的LLaMA-3 8B指令模型，并在角色扮演对话方面达到了与GPT-4o模型相当的性能。我们发布我们的合成角色和指令调整对话以支持公开研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15427v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）中的角色定制能力，即角色泛化能力，因其灵活性和开发部署成本效益而受到越来越多的关注。本研究探索了一种大规模数据合成方法来为LLM提供角色泛化能力。通过合成大规模角色配置文件并使用Persona Hub中的个性进行初步研究，我们探索了两种策略：响应重写和响应生成，以创建与角色对齐的指令响应。为了验证我们的合成指令微调数据对角色泛化的有效性，我们使用LLaMA-3 8B模型进行有监督微调（SFT）。我们表现最佳的模型增强了原始的LLaMA-3 8B指令模型在角色扮演对话上的表现，达到了GPT-4o模型的性能水平。我们发布我们的合成角色和指令调整对话以支持公开研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的角色定制能力因其灵活性和成本效益而受到关注。</li>
<li>研究者通过合成大规模角色配置文件来赋予LLM角色泛化能力。</li>
<li>使用Persona Hub中的个性进行初步研究，探索了响应重写和响应生成两种策略来创建与角色对齐的指令响应。</li>
<li>通过有监督微调（SFT）验证了合成指令微调数据的有效性。</li>
<li>最佳模型的性能与GPT-4o模型在角色扮演对话上的表现相当。</li>
<li>研究者发布了合成角色和指令调整对话以支持公开研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d36fdd01cf33ef39caa45b2ab2d3b5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4932b18bf311842a7de635f677db8e99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7647264287bb7eda0c1b6aa99caea2a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-457ab20813c063a0001592a1adf9fc34.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LinguaLIFT-An-Effective-Two-stage-Instruction-Tuning-Framework-for-Low-Resource-Language-Reasoning"><a href="#LinguaLIFT-An-Effective-Two-stage-Instruction-Tuning-Framework-for-Low-Resource-Language-Reasoning" class="headerlink" title="LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for   Low-Resource Language Reasoning"></a>LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for   Low-Resource Language Reasoning</h2><p><strong>Authors:Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang</strong></p>
<p>Large language models (LLMs) have exhibited impressive multilingual reasoning capabilities, driven by extensive multilingual pre-training corpora and instruction fine-tuning data. However, a performance gap exists between high- and low-resource language reasoning tasks due to the language imbalance in the pre-training corpus, which is exacerbated by evaluation bias in existing reasoning benchmarks lacking low-resource language coverage. To alleviate this issue, we propose LinguaLIFT, a two-stage instruction tuning framework for advancing low-resource language reasoning. LinguaLIFT employs a language alignment layer to capture multilingual alignment in a code-switched tuning way without requiring multilingual instruction or parallel data, thereby transferring the cross-lingual reasoning capabilities to low-resource languages through English-only instruction tuning data. To comprehensively evaluate the multilingual reasoning capabilities, we introduce the Multilingual Math World Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and 10 high-resource languages. Experimental results show that LinguaLIFT outperforms several competitive baselines across MMWP and four widely used benchmarks. </p>
<blockquote>
<p>大型语言模型（LLM）表现出了令人印象深刻的跨语言推理能力，这得益于大量的跨语言预训练语料库和指令微调数据。然而，由于预训练语料库中的语言不平衡，高资源语言推理任务与低资源语言推理任务之间存在性能差距，而现有推理基准测试中的评估偏见缺乏低资源语言覆盖，这一差距进一步加剧。为了缓解这一问题，我们提出了LinguaLIFT，这是一个两阶段的指令调整框架，用于提升低资源语言推理。LinguaLIFT采用语言对齐层来捕捉代码切换调整方式中的多语言对齐，无需多语言指令或平行数据，从而通过仅英语指令调整数据转移跨语言推理能力到低资源语言。为了全面评估多语言推理能力，我们引入了多语言数学世界问题（MMWP）基准测试，它涵盖21种低资源语言、17种中资源语言和1.0种高资源语言。实验结果表明，LinguaLIFT在MMWP和四个广泛使用的基准测试中超越了几个竞争基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12499v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）具备强大的跨语言推理能力，得益于丰富的跨语言预训练语料库和指令微调数据。然而，由于预训练语料库中的语言不平衡和现有推理基准测试对低资源语言的覆盖不足，高资源语言与低资源语言之间的推理任务存在性能差距。为解决这一问题，我们提出LinguaLIFT，这是一个两阶段的指令调整框架，旨在提升低资源语言的推理能力。LinguaLIFT采用语言对齐层，以代码切换的方式捕捉多语言对齐，无需多语言指令或平行数据，通过仅使用英语指令调整数据，将跨语言推理能力转移到低资源语言。为全面评估多语言推理能力，我们引入了跨多种语言的数学世界问题（MMWP）基准测试，涵盖21种低资源、17种中等资源和10种高资源语言。实验结果表明，LinguaLIFT在MMWP和四个广泛使用的基准测试中超越了几个竞争对手。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM具备强大的跨语言推理能力，得益于预训练语料库和指令微调数据。</li>
<li>高资源语言与低资源语言之间的推理任务存在性能差距，主要由于预训练语料库中的语言不平衡和现有推理基准测试对低资源语言的覆盖不足。</li>
<li>LinguaLIFT是一个两阶段的指令调整框架，旨在提升低资源语言的推理能力，采用语言对齐层以代码切换方式捕捉多语言对齐。</li>
<li>LinguaLIFT无需多语言指令或平行数据，通过英语指令调整数据转移跨语言推理能力到低资源语言。</li>
<li>MMWP基准测试用于全面评估多语言推理能力，涵盖多种不同资源的语言。</li>
<li>实验表明，LinguaLIFT在多个基准测试中性能优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8feef172a6c8f5c1600a192e3e87023.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69fd76cc1314fbd48ea80fa111caee50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53251d00ec390c9cd2e158ad6fc19c43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa9f0814077b26adea978ded1f0a4aea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e56073a1ca89c1cafb838e804e04b8a9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MaLei-at-the-PLABA-Track-of-TREC-2024-RoBERTa-for-Term-Replacement-–-LLaMA3-1-and-GPT-4o-for-Complete-Abstract-Adaptation"><a href="#MaLei-at-the-PLABA-Track-of-TREC-2024-RoBERTa-for-Term-Replacement-–-LLaMA3-1-and-GPT-4o-for-Complete-Abstract-Adaptation" class="headerlink" title="MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement –   LLaMA3.1 and GPT-4o for Complete Abstract Adaptation"></a>MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement –   LLaMA3.1 and GPT-4o for Complete Abstract Adaptation</h2><p><strong>Authors:Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic</strong></p>
<p>This report is the system description of the MaLei team (Manchester and Leiden) for the shared task Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024 (we had an earlier name BeeManc following last year), affiliated with TREC2024 (33rd Text REtrieval Conference <a target="_blank" rel="noopener" href="https://ir.nist.gov/evalbase/conf/trec-2024">https://ir.nist.gov/evalbase/conf/trec-2024</a>). This report contains two sections corresponding to the two sub-tasks in PLABA-2024. In task one (term replacement), we applied fine-tuned ReBERTa-Base models to identify and classify the difficult terms, jargon, and acronyms in the biomedical abstracts and reported the F1 score (Task 1A and 1B). In task two (complete abstract adaptation), we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot prompts to complete the abstract adaptation and reported the scores in BLEU, SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024 on Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd and 2nd respectively on the two sub-tasks, and the 1st on averaged F1 scores across the two tasks from 9 evaluated systems. Our LLaMA-3.1-70B-instructed model achieved the highest Completeness score for Task 2. We share our source codes, fine-tuned models, and related resources at <a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/PLABA2024">https://github.com/HECTA-UoM/PLABA2024</a> </p>
<blockquote>
<p>这份报告是MaLei团队（曼彻斯特与莱顿）关于2024年生物医学摘要的平语适应（PLABA）共享任务的体系描述（我们去年使用了一个较早的名字BeeManc），该任务与TREC2024（第33次文本检索会议）相关联。本报告包含两个部分，分别对应PLABA-2024的两个子任务。在任务一（术语替换）中，我们应用了经过微调过的ReBERTa-Base模型来识别和分类生物医学摘要中的难词、术语和缩写词，并报告了F1分数（任务1A和1B）。在任务二（完整摘要适应）中，我们利用Llamma3.1-70B-Instruct和GPT-4o的一次性提示来完成摘要的适应，并在BLEU、SARI、BERTScore、LENS和SALSA中报告了得分。根据PLABA-2024的官方评估结果，我们的较小的微调RoBERTa-Base模型在两个子任务中分别排名第3和第2，而在两个任务的平均F1分数上排名第1（共评价了9个系统）。我们的LLaMA-3.1-70B-Instruct模型在任务2中获得了最高的完整性得分。我们在<a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/PLABA2024">https://github.com/HECTA-UoM/PLABA2024</a>分享了我们的源代码、微调模型和相关资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07381v4">PDF</a> ongoing work - system report for PLABA2024 with TREC-2024</p>
<p><strong>Summary</strong></p>
<p>本文介绍了MaLei团队在PLABA 2024任务中的系统描述，该任务包括两个子任务：术语替换和完整摘要改编。团队使用了fine-tuned ReBERTa-Base模型和Llamma3.1-70B-Instruct与GPT-4o等工具进行任务处理，并在两个子任务中取得了优异的成绩。团队还分享了源代码、fine-tuned模型和有关资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaLei团队参与了PLABA 2024任务，该任务旨在实现生物医学摘要的通俗化改编。</li>
<li>任务分为两个子任务：术语替换和完整摘要改编。</li>
<li>团队使用了fine-tuned ReBERTa-Base模型来识别和分类生物医学摘要中的困难术语、行话和缩写。</li>
<li>在任务二中使用Llamma3.1-70B-Instruct和GPT-4o等工具进行完整摘要改编。</li>
<li>MaLei团队在两个子任务中均取得了优异的成绩，平均F1得分在所有评价的系统中排名第二和第三，且在任务二的完整性得分上排名第一。</li>
<li>MaLei团队分享了源代码、fine-tuned模型和有关资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07381">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4cfa7d27f1eac6f4701ade77ba2b468f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f31701651d208c73cbaf232881b42c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ae377ab16483abdb04881154e5da346.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6b0d06097d79a72c4260b77379adcdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b425be18f56947587819315e665ed486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82d5c175d739c60b42b44a8d58d3929f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-distributional-simplicity-bias-in-the-learning-dynamics-of-transformers"><a href="#A-distributional-simplicity-bias-in-the-learning-dynamics-of-transformers" class="headerlink" title="A distributional simplicity bias in the learning dynamics of   transformers"></a>A distributional simplicity bias in the learning dynamics of   transformers</h2><p><strong>Authors:Riccardo Rende, Federica Gerace, Alessandro Laio, Sebastian Goldt</strong></p>
<p>The remarkable capability of over-parameterised neural networks to generalise effectively has been explained by invoking a &#96;&#96;simplicity bias’’: neural networks prevent overfitting by initially learning simple classifiers before progressing to more complex, non-linear functions. While simplicity biases have been described theoretically and experimentally in feed-forward networks for supervised learning, the extent to which they also explain the remarkable success of transformers trained with self-supervised techniques remains unclear. In our study, we demonstrate that transformers, trained on natural language data, also display a simplicity bias. Specifically, they sequentially learn many-body interactions among input tokens, reaching a saturation point in the prediction error for low-degree interactions while continuing to learn high-degree interactions. To conduct this analysis, we develop a procedure to generate \textit{clones} of a given natural language data set, which rigorously capture the interactions between tokens up to a specified order. This approach opens up the possibilities of studying how interactions of different orders in the data affect learning, in natural language processing and beyond. </p>
<blockquote>
<p>神经网络在过度参数化后仍能有效泛化的显著能力，可以通过引入“简洁性偏见”来解释：神经网络通过首先学习简单的分类器，然后逐渐转向更复杂、非线性的函数，从而防止过度拟合。虽然简洁偏见在理论上和实验上已在用于监督学习的前馈网络中得到了描述，但它们是否也能解释使用自监督技术训练的变压器的显著成功程度仍然不清楚。在我们的研究中，我们证明了在自然语言数据上训练的变压器也表现出简洁偏见。具体来说，它们会按顺序学习输入标记之间的多体交互作用，在预测低阶交互的错误时达到饱和点，同时继续学习高阶交互。为了进行这种分析，我们开发了一种程序来生成给定自然语言数据集的副本，这些副本严格捕获标记之间的交互作用到指定的顺序。这种方法开辟了研究不同顺序的交互如何影响自然语言处理和其他领域学习的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19637v2">PDF</a> 10 pages, 5 figures, NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>神经网络通过引入“简洁性偏见”来解释其有效的泛化能力：神经网络通过首先学习简单的分类器来防止过拟合，然后再逐步学习更复杂的非线性函数。本研究证明，使用自然语言数据训练的变压器也表现出简洁性偏见。特别是，他们按顺序学习输入标记之间的多体交互作用，在预测低阶交互的错误时达到饱和点，同时继续学习高阶交互。为此分析，我们开发了一种生成给定自然语言数据集的“克隆”的程序，该程序严格捕获了标记之间的交互作用直到指定的顺序。这为研究不同顺序的交互作用如何影响自然语言处理等领域的学习提供了可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络通过引入“简洁性偏见”来解释其泛化能力，即通过学习简单的分类器来防止过拟合。</li>
<li>变压器模型在训练过程中也显示出简洁性偏见，即先学习低阶交互，再逐渐学习高阶交互。</li>
<li>本研究开发了一种生成自然语言数据集克隆的程序，用于捕获标记之间的交互作用。</li>
<li>该程序可以严格地研究不同阶数的交互作用对数据学习的影响。</li>
<li>此方法对于研究自然语言处理领域的学习具有应用价值。</li>
<li>变压器模型在自我监督技术训练下的成功部分归因于简洁性偏见。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19637">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-743613dcf83e41e74b6b4b107ff2853c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81bc88c2433b905990886048b2d2a0f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f365ca2ce72c3037ee52c0cfa1dd2f95.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Text2Chart31-Instruction-Tuning-for-Chart-Generation-with-Automatic-Feedback"><a href="#Text2Chart31-Instruction-Tuning-for-Chart-Generation-with-Automatic-Feedback" class="headerlink" title="Text2Chart31: Instruction Tuning for Chart Generation with Automatic   Feedback"></a>Text2Chart31: Instruction Tuning for Chart Generation with Automatic   Feedback</h2><p><strong>Authors:Fatemeh Pesaran Zadeh, Juyeon Kim, Jin-Hwa Kim, Gunhee Kim</strong></p>
<p>Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks. We make the code and dataset available at <a target="_blank" rel="noopener" href="https://github.com/fatemehpesaran310/Text2Chart31">https://github.com/fatemehpesaran310/Text2Chart31</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各种语言任务中表现出了强大的能力，尤其是指令调整方法。然而，LLM在通过图表可视化复杂、现实世界数据方面面临挑战。首先，现有数据集很少涵盖全方位的图表类型，如3D、体积和网格图表。其次，监督微调方法并没有充分利用丰富数据集中的复杂关系，包括文本、代码和图表。为了解决这些挑战，我们提出了一个分层的管道和一个新的数据集来进行图表生成。我们的数据集Text2Chart31包括Matplotlib库中引用的31种独特图表类型，包含11.1K个描述、代码、数据表和图表的元组。此外，我们引入了一种基于强化学习的指令调整技术，用于图表生成任务，无需人工反馈。我们的实验表明，这种方法显著提高了模型性能，使小型模型在数据可视化任务中超越大型开源模型，并与最先进的专有模型相当。我们在<a target="_blank" rel="noopener" href="https://github.com/fatemehpesaran310/Text2Chart31">https://github.com/fatemehpesaran310/Text2Chart31</a>提供了代码和数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04064v2">PDF</a> EMNLP 2024 Main Oral. Code and dataset are released at   <a target="_blank" rel="noopener" href="https://github.com/fatemehpesaran310/Text2Chart31">https://github.com/fatemehpesaran310/Text2Chart31</a></p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在各类语言任务中展现出强大的能力，尤其在指令微调方法方面尤为突出。然而，LLM在可视化复杂、现实世界数据方面存在挑战，如处理3D、体积和网格图表等类型的数据。为解决这些问题，本研究提出了一种层次化管道和新的数据集Text2Chart31，用于图表生成。该数据集包含Matplotlib库中涉及的31种独特图表类型，共计11.1K个描述、代码、数据表和图表的组合。此外，研究引入了基于强化学习的指令微调技术，用于图表生成任务，无需人工反馈。实验表明，该方法显著提高了模型性能，使得小型模型在数据可视化任务上的表现优于大型开源模型，并与业界顶尖模型相媲美。我们已将代码和数据集发布在<a target="_blank" rel="noopener" href="https://github.com/fatemehpesaran310/Text2Chart31%E3%80%82">https://github.com/fatemehpesaran310/Text2Chart31。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多种语言任务中表现优异，尤其擅长指令微调方法。</li>
<li>LLM在可视化复杂、现实世界数据方面面临挑战，如处理多种图表类型。</li>
<li>现有数据集很少覆盖全面的图表类型，如3D、体积和网格图表。</li>
<li>提出了一个新的数据集Text2Chart31，包含多种图表类型，以支持更广泛的图表生成任务。</li>
<li>引入了一种基于强化学习的指令微调技术，用于图表生成任务，提高了模型性能。</li>
<li>该方法使得小型模型在数据可视化任务上的表现优于某些大型开源模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b0e8b940e4acae3b35aa321ed6bc9f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b867d81b662c28ad1cf9e991d9417d4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-365043a9318f6d3fb281efaa8e096d69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10c8f001250a2878477634aeef4d9c55.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Robin3D-Improving-3D-Large-Language-Model-via-Robust-Instruction-Tuning"><a href="#Robin3D-Improving-3D-Large-Language-Model-via-Robust-Instruction-Tuning" class="headerlink" title="Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning"></a>Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning</h2><p><strong>Authors:Weitai Kang, Haifeng Huang, Yuzhang Shang, Mubarak Shah, Yan Yan</strong></p>
<p>Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model’s discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model’s generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8% improvement in the grounding task (Multi3DRefer) and a 6.9% improvement in the captioning task (Scan2Cap). </p>
<blockquote>
<p>近年来，3D大型语言模型（3DLLM）的进步凸显了其在构建3D现实世界通用代理的潜力，但由于缺乏高质量、稳健的指令遵循数据，仍存在挑战，导致3DLLM的辨别力和泛化能力有限。在本文中，我们介绍了Robin3D，这是一种由我们的新型数据引擎——Robust Instruction Generation（RIG）引擎生成的大规模指令遵循数据训练出来的强大3DLLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00255v2">PDF</a> 8 pages</p>
<p><strong>摘要</strong></p>
<p>最近3D大型语言模型（3DLLM）的进展突显了其在构建现实世界中通用代理的潜力，但仍面临缺乏高质量稳健指令跟踪数据的挑战，限制了其鉴别力和泛化能力。本文介绍了Robin3D，这是一种强大的由我们新型数据引擎Robust Instruction Generation（RIG）生成大规模指令跟踪数据训练的3DLLM。RIG生成两种关键指令数据：对抗性指令跟踪数据和多样性指令跟踪数据，分别用于增强模型的鉴别理解和提高模型的泛化能力。因此，我们构建了包含对抗性样本、多样性样本和基准训练集样本的各一百万条指令跟踪数据。Robin3D采用Relation-Augmented Projector以增强空间理解力，通过ID-Feature Bonding强化对象指代和定位能力。在五个广泛使用的3D多媒体学习基准测试中，Robin3D均表现优于先前的方法，无需特定任务的微调。特别是在定位任务（Multi3DRefer）上实现了7.8%的提升和描述任务（Scan2Cap）上实现了6.9%的提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Robin3D模型是由大规模指令跟踪数据训练的强大模型，使用了创新的Robust Instruction Generation引擎来创建数据集。该模型对正面和负面样本进行混合，增强了模型的鉴别能力。</li>
<li>Robin3D通过包含多种指令风格的数据训练，增强了模型的泛化能力。对抗性和多样性的数据策略是提升其性能的关键因素。</li>
<li>Robin3D成功结合使用Relation-Augmented Projector和ID-Feature Bonding技术，提高了空间理解力和对象指代及定位能力。这使得模型在各种复杂的指令环境下表现出卓越的性能。</li>
<li>Robin3D在五个广泛的3D多媒体学习基准测试中表现优于其他方法，无需特定任务的微调。这表明其具有良好的通用性和适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5758c1dfaa2d29d446604a4288bb88a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f57c2e649e7e861f8caf88c51095ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e32735a7e48225d3093d227d508879e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31453f6295406b26f846b67904af69f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80a9a8395bc05a6b5e62d1efdac9530b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Relation-Also-Knows-Rethinking-the-Recall-and-Editing-of-Factual-Associations-in-Auto-Regressive-Transformer-Language-Models"><a href="#Relation-Also-Knows-Rethinking-the-Recall-and-Editing-of-Factual-Associations-in-Auto-Regressive-Transformer-Language-Models" class="headerlink" title="Relation Also Knows: Rethinking the Recall and Editing of Factual   Associations in Auto-Regressive Transformer Language Models"></a>Relation Also Knows: Rethinking the Recall and Editing of Factual   Associations in Auto-Regressive Transformer Language Models</h2><p><strong>Authors:Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, Weiping Wang</strong></p>
<p>The storage and recall of factual associations in auto-regressive transformer language models (LMs) have drawn a great deal of attention, inspiring knowledge editing by directly modifying the located model weights. Most editing works achieve knowledge editing under the guidance of existing interpretations of knowledge recall that mainly focus on subject knowledge. However, these interpretations are seriously flawed, neglecting relation information and leading to the over-generalizing problem for editing. In this work, we discover a novel relation-focused perspective to interpret the knowledge recall of transformer LMs during inference and apply it on single knowledge editing to avoid over-generalizing. Experimental results on the dataset supplemented with a new R-Specificity criterion demonstrate that our editing approach significantly alleviates over-generalizing while remaining competitive on other criteria, breaking the domination of subject-focused editing for future research. </p>
<blockquote>
<p>在自回归transformer语言模型（LMs）中，事实关联存储和回忆引起了广泛关注，激发了通过直接修改定位模型权重进行知识编辑的灵感。大多数编辑工作在知识回忆的现有解释指导下进行知识编辑，这些解释主要集中在主题知识上。然而，这些解释存在严重缺陷，忽略了关系信息，导致编辑过度概括。在这项工作中，我们发现了一种新型的以关系为中心的角度来解释推理过程中transformer LMs的知识回忆，并将其应用于单一知识编辑中以避免过度概括。在新增数据集上进行的实验采用了新的R-特异性标准作为评价指标，结果表明我们的编辑方法显著减轻了过度概括的问题，同时在其他标准上仍具有竞争力，打破了未来研究中以主题为中心的编辑的主导地位。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15091v2">PDF</a> Accepted by AAAI25</p>
<p><strong>Summary</strong>：<br>在自回归Transformer语言模型中，知识关联存储和回忆的问题已引起广泛关注，激发了通过直接修改模型权重进行知识编辑的方法。然而，现有知识回忆的解释主要集中在主题知识上，忽视了关系信息，导致编辑时出现过度泛化问题。本研究从关系视角出发，解读Transformer模型在推理过程中的知识回忆，并将其应用于单一知识编辑中以避免过度泛化。实验结果表明，新的编辑方法显著减轻了过度泛化问题，同时在其他标准上仍具有竞争力，打破了主题导向编辑的局限。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Transformer语言模型中的知识关联存储和回忆已受到关注。</li>
<li>直接修改模型权重进行知识编辑的方法已被提出。</li>
<li>现有知识回忆的解释主要集中在主题知识上，忽略了关系信息。</li>
<li>关系视角对理解Transformer模型在推理过程中的知识回忆至关重要。</li>
<li>基于关系视角的知识编辑方法有助于避免过度泛化问题。</li>
<li>实验结果表明，新的编辑方法在缓解过度泛化问题上效果显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-12d926a2ce9fa543703a66c90bd4fd23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fee26da49c8313c3a8a21f5bd19957cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e06b9499c65bd09299f422a231a90e5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7921d748106a0e5f66f5d9927ee0f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66c99ab4449455656b7406c5c36e6740.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37119858d85dc16b28481b9945634f7c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Triple-Preference-Optimization-Achieving-Better-Alignment-using-a-Single-Step-Optimization"><a href="#Triple-Preference-Optimization-Achieving-Better-Alignment-using-a-Single-Step-Optimization" class="headerlink" title="Triple Preference Optimization: Achieving Better Alignment using a   Single Step Optimization"></a>Triple Preference Optimization: Achieving Better Alignment using a   Single Step Optimization</h2><p><strong>Authors:Amir Saeidi, Shivanshu Verma, Aswin RRV, Kashif Rasul, Chitta Baral</strong></p>
<p>Reinforcement Learning with Human Feedback (RLHF) enhances the alignment of Large Language Models (LLMs). However, its limitations have led to the development of Direct Preference Optimization (DPO), an RL-free approach designed to overcome these shortcomings. While studies have shown that DPO improves instruction-following capabilities, it negatively impacts the reasoning ability of LLMs. Additionally, DPO is highly sensitive to judgment noise in preference datasets and the size of the training set. Although several modifications to DPO have been proposed, they still fail to fully resolve these issues. To address these limitations, we propose Triple Preference Optimization (TPO), a new preference learning method designed to enhance both reasoning and instruction-following abilities through one-step optimization. We compare TPO against DPO and its recent variants using state-of-the-art training setups, including both base and instruction-tuned models such as Mistral and Llama 3. Our evaluation covers a comprehensive range of chat-based and reasoning benchmarks. The results demonstrate that TPO achieves significant improvements over existing methods without substantially increasing response length across different dataset sizes. Specifically, TPO outperforms DPO and SimPO by up to 7.0% and 7.3% points on Arena-Hard, 12.2% and 13.3% points on MixEval-Hard, 10.4% and 10.1% points on MMLU-Pro, and 19.0% and 19.2% points on GSM8K, respectively. Furthermore, TPO achieves these improvements while requiring less data than DPO. </p>
<blockquote>
<p>强化学习结合人类反馈（RLHF）增强了大型语言模型（LLM）的一致性。然而，其局限性促使了无强化学习的直接偏好优化（DPO）方法的发展，旨在克服这些不足。研究表明，尽管DPO提高了指令执行能力，但它对LLM的推理能力产生了负面影响。此外，DPO对偏好数据集中的判断噪声和训练集大小高度敏感。尽管已经提出了对DPO的几项修改，但它们仍然未能完全解决这些问题。为了解决这些局限性，我们提出了三重偏好优化（TPO）这一新的偏好学习方法，旨在通过一步优化提高推理和指令执行能力。我们将TPO与DPO及其最近变种进行了比较，使用的是最先进的训练设置，包括基础模型和指令调整模型，如Mistral和Llama 3。我们的评估涵盖了基于聊天的综合范围和推理基准测试。结果表明，TPO在不同的数据集大小上实现了对现有方法的显著改进，而响应长度没有大幅增加。具体来说，TPO在Arena-Hard上超越了DPO和SimPO高达7.0%和7.3%点，在MixEval-Hard上分别提高了12.2%和13.3%点，在MMLU-Pro上分别提高了10.4%和10.1%点，在GSM8K上分别提高了19.0%和19.2%点。此外，TPO在需要的数据量上少于DPO就实现了这些改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16681v2">PDF</a> </p>
<p><strong>摘要</strong><br>强化学习结合人类反馈（RLHF）提高了大型语言模型（LLM）的对齐程度。但其局限性促使了无需强化学习的直接偏好优化（DPO）方法的开发。尽管DPO改善了指令遵循能力，但它对LLM的推理能力产生负面影响，并且对偏好数据集里的判断噪声和训练集大小高度敏感。为解决这些问题，我们提出了三重偏好优化（TPO）这一新的偏好学习方法，旨在通过一步优化增强推理和指令遵循能力。对比DPO及其最新变体，TPO在包括Mistral和Llama 3等基础及指令调优模型在内的最先进的训练设置中表现优异。评估涵盖广泛的聊天和推理基准测试，结果表明TPO在Arena-Hard、MixEval-Hard、MMLU-Pro和GSM8K等数据集上分别比DPO和SimPO高出7.0%和7.3%、12.2%和13.3%、10.4%和10.1%、以及19.0%和19.2%的准确度。此外，TPO在需要更少数据的情况下实现了这些改进。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RLHF提高了LLM的对齐程度，但其局限性促使了DPO方法的开发。</li>
<li>DPO在改善指令遵循能力的同时，对LLM的推理能力产生负面影响。</li>
<li>DPO对偏好数据集中的判断噪声和训练集大小高度敏感。</li>
<li>提出的TPO方法旨在通过一步优化同时提高推理和指令遵循能力。</li>
<li>TPO在多种基准测试中表现优于DPO及其变体。</li>
<li>TPO在不同数据集上实现了显著的性能提升，并且在需要更少数据的情况下做到这一点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d80909c0f50ee4b4837f988bf6eab0ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ced7c39a59aaa5319b98a0a5cba1528d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b366f343853ff74731d0b600df3040.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aa61fc4647c646a9db4f9bf4f3f3c07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a47f2f13235fbbbaee0fbc6127180b0e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DP-DyLoRA-Fine-Tuning-Transformer-Based-Models-On-Device-under-Differentially-Private-Federated-Learning-using-Dynamic-Low-Rank-Adaptation"><a href="#DP-DyLoRA-Fine-Tuning-Transformer-Based-Models-On-Device-under-Differentially-Private-Federated-Learning-using-Dynamic-Low-Rank-Adaptation" class="headerlink" title="DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under   Differentially Private Federated Learning using Dynamic Low-Rank Adaptation"></a>DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under   Differentially Private Federated Learning using Dynamic Low-Rank Adaptation</h2><p><strong>Authors:Jie Xu, Karthikeyan Saravanan, Rogier van Dalen, Haaris Mehmood, David Tuckey, Mete Ozay</strong></p>
<p>Federated learning (FL) allows clients to collaboratively train a global model without sharing their local data with a server. However, clients’ contributions to the server can still leak sensitive information. Differential privacy (DP) addresses such leakage by providing formal privacy guarantees, with mechanisms that add randomness to the clients’ contributions. The randomness makes it infeasible to train large transformer-based models, common in modern federated learning systems. In this work, we empirically evaluate the practicality of fine-tuning large scale on-device transformer-based models with differential privacy in a federated learning system. We conduct comprehensive experiments on various system properties for tasks spanning a multitude of domains: speech recognition, computer vision (CV) and natural language understanding (NLU). Our results show that full fine-tuning under differentially private federated learning (DP-FL) generally leads to huge performance degradation which can be alleviated by reducing the dimensionality of contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks of existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA) consistently outperforms other methods. An even more promising approach, DyLoRA, which makes the low rank variable, when naively combined with FL would straightforwardly break differential privacy. We therefore propose an adaptation method that can be combined with differential privacy and call it DP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word error rate (WER) increase due to DP to less than 2% and 7% respectively with 1 million clients and a stringent privacy budget of $\epsilon&#x3D;2$. </p>
<blockquote>
<p>联邦学习（FL）允许客户端在没有将数据共享给服务器的情况下共同训练全局模型。然而，客户端对服务器的贡献仍然可能泄露敏感信息。差分隐私（DP）通过提供正式的隐私保证来解决此类泄露问题，其中包含向客户端的贡献增加随机性的机制。这种随机性使得在联邦学习系统中对基于大型变换的模型进行微调变得不切实际，这种模型在现代联邦学习系统中很常见。在这项工作中，我们实证评估了在联邦学习系统中对基于大型变换的模型进行差分隐私微调的实际可行性。我们在涵盖多个领域的任务上对各种系统属性进行了全面的实验，包括语音识别、计算机视觉（CV）和自然语言理解（NLU）。我们的结果表明，在差分私有联邦学习（DP-FL）下进行全面微调通常会导致巨大的性能下降，这可以通过通过参数高效微调（PEFT）减少贡献的维度来缓解。我们对现有的DP-PEFT方法的基准测试表明，DP-LoRA持续优于其他方法。一个更有前途的方法是DyLoRA，它使低阶变量发生变化，但当它与FL简单结合时，将直接破坏差分隐私。因此，我们提出了一种可以与差分隐私相结合的自适应方法，我们称之为DP-DyLoRA。最后，我们能够将由于DP导致的准确度下降和词错误率（WER）增加减少到低于2%和7%，同时拥有1百万客户端和严格的隐私预算ε&#x3D;2。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.06368v4">PDF</a> 16 pages, 10 figures, 5 tables</p>
<p><strong>Summary</strong><br>    本文探讨了差分隐私与联邦学习结合的问题，指出了在使用差分隐私进行联邦学习时面临的挑战。通过对大规模设备上基于转换器的模型进行微调实验，发现差分隐私下的联邦学习会导致性能下降。通过参数高效的微调方法可以缓解这一问题，其中DP-LoRA方法表现最佳。为进一步提高性能，本文提出了结合差分隐私的DP-DyLoRA方法，能在保持隐私的同时减少性能损失。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习允许客户端在没有将数据发送给服务器的情况下共同训练全局模型。</li>
<li>客户端对服务器的贡献可能泄露敏感信息，差分隐私可提供正式的隐私保障来解决这一问题。</li>
<li>在联邦学习系统中对大规模转换器模型进行微调时，差分隐私的随机性会导致性能大幅下降。</li>
<li>通过参数高效的微调方法（如DP-LoRA）可以缓解差分隐私带来的性能下降问题。</li>
<li>DP-DyLoRA是一种新的结合差分隐私的方法，能够在保持隐私的同时提高性能。</li>
<li>实验结果表明，在严格的隐私预算下，使用DP-DyLoRA方法可以将由于差分隐私导致的准确率下降和词错误率增加减少到不到2%和7%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.06368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-02b124820ae35e0a04bbe7641a8bdc8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7942df17f1929358b40f2197af4e94fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58aafe448f1b218e1be4f4a4d0c54d94.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Heterogeneous-Chiplet-Architecture-for-Accelerating-End-to-End-Transformer-Models"><a href="#A-Heterogeneous-Chiplet-Architecture-for-Accelerating-End-to-End-Transformer-Models" class="headerlink" title="A Heterogeneous Chiplet Architecture for Accelerating End-to-End   Transformer Models"></a>A Heterogeneous Chiplet Architecture for Accelerating End-to-End   Transformer Models</h2><p><strong>Authors:Harsh Sharma, Pratyush Dhingra, Janardhan Rao Doppa, Umit Ogras, Partha Pratim Pande</strong></p>
<p>Transformers have revolutionized deep learning and generative modeling, enabling advancements in natural language processing tasks. However, the size of transformer models is increasing continuously, driven by enhanced capabilities across various deep learning tasks. This trend of ever-increasing model size has given rise to new challenges in terms of memory and compute requirements. Conventional computing platforms, including GPUs, suffer from suboptimal performance due to the memory demands imposed by models with millions&#x2F;billions of parameters. The emerging chiplet-based platforms provide a new avenue for compute- and data-intensive machine learning (ML) applications enabled by a Network-on-Interposer (NoI). However, designing suitable hardware accelerators for executing Transformer inference workloads is challenging due to a wide variety of complex computing kernels in the Transformer architecture. In this paper, we leverage chiplet-based heterogeneous integration (HI) to design a high-performance and energy-efficient multi-chiplet platform to accelerate transformer workloads. We demonstrate that the proposed NoI architecture caters to the data access patterns inherent in a transformer model. The optimized placement of the chiplets and the associated NoI links and routers enable superior performance compared to the state-of-the-art hardware accelerators. The proposed NoI-based architecture demonstrates scalability across varying transformer models and improves latency and energy efficiency by up to 11.8x and 2.36x, respectively when compared with the existing state-of-the-art architecture HAIMA. </p>
<blockquote>
<p>转换器（Transformers）已经深刻地改变了深度学习和生成建模，并促进了自然语言处理任务的进步。然而，由于各项深度学习任务的性能提升，转换器模型的大小也在持续增长。这种模型规模不断增大的趋势给内存和计算需求带来了新的挑战。传统的计算平台，包括GPU，由于模型参数庞大（达数百万或数十亿），对内存的需求使得其性能受到限制。基于芯片片的平台（chiplet-based platforms）通过整合网络芯片间连接器（Network-on-Interposer，NoI）为计算和数据密集型机器学习（ML）应用提供了新的机会。然而，设计适合执行Transformer推理工作负载的硬件加速器是一项挑战，因为Transformer架构中的计算核心种类繁多且复杂。在本文中，我们利用基于芯片片的异构集成（Heterogeneous Integration，HI）设计了一个高性能和能效的多芯片片平台，以加速Transformer工作负载。我们证明所提出的NoI架构可以满足Transformer模型中固有的数据访问模式。芯片片的优化放置以及相关NoI链接和路由器使得其性能超越最新的硬件加速器。与其他现有最先进的架构HAIMA相比，所提出的基于NoI的架构在不同的Transformer模型中表现出可扩展性，在延迟和能效方面分别提高了高达11.8倍和2.36倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.11750v2">PDF</a> To appear in ACM Transactions on Design Automation of Electronic   Systems, 2025</p>
<p><strong>Summary</strong><br>     随着Transformer模型在自然语言处理任务中应用的快速发展，模型规模持续增大，对计算和内存的需求也随之增加。传统的计算平台如GPU难以满足这些需求。新兴的基于芯片小片的平台通过片上网络提供新的解决方案，但在Transformer架构的复杂计算内核中设计合适的硬件加速器仍具挑战。本研究利用基于芯片小片的异构集成技术，设计了一个高性能、低能耗的跨芯片小片平台以加速Transformer推理工作负载。结果表明，该方案能有效满足Transformer模型的数据访问需求，在先进的硬件加速器中表现优越，在不同Transformer模型中具有可扩展性，与现有最先进的架构相比，延迟和能效分别提高了最多达11.8倍和2.36倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型的应用带动了自然语言处理任务的进步，但模型规模的增大带来了计算和内存需求的挑战。</li>
<li>传统计算平台如GPU难以满足大规模Transformer模型的内存和计算需求。</li>
<li>基于芯片小片的平台通过片上网络为计算密集型和数据密集型机器学习应用提供了新的解决方案。</li>
<li>设计适合Transformer架构的硬件加速器是一项挑战，因为该架构包含多种复杂的计算内核。</li>
<li>研究提出了一种基于芯片小片的异构集成技术的高性能、低能耗跨芯片小片平台，用于加速Transformer推理工作负载。</li>
<li>该方案能有效满足Transformer模型的数据访问需求，表现优于先进的硬件加速器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.11750">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c1e78baaa22a3efb429d5087a9bde47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7cc5a3b0f20417a102b66b0cc3793c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d5aa28db4c39b33b1a20895483f1325.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="NetPanorama-A-Declarative-Grammar-for-Network-Construction-Transformation-and-Visualization"><a href="#NetPanorama-A-Declarative-Grammar-for-Network-Construction-Transformation-and-Visualization" class="headerlink" title="NetPanorama: A Declarative Grammar for Network Construction,   Transformation, and Visualization"></a>NetPanorama: A Declarative Grammar for Network Construction,   Transformation, and Visualization</h2><p><strong>Authors:James Scott-Brown, Alexis Pister, Benjamin Bach</strong></p>
<p>This paper introduces NetPanorama, a domain-specific language and declarative grammar for interactive network visualization design that supports multivariate, temporal, and geographic networks. NetPanorama allows users to specify network visualizations as combinations of primitives and building blocks. These support network creation and transformation, including computing metrics; orderings, seriations and layouts; visual encodings, including glyphs, faceting, and label visibility; and interaction for exploration and modifying styling. This approach allows the creation of a range of visualizations including many types of node-link diagrams, adjacency matrices using diverse cell encodings and node orderings, arc diagrams, PivotGraph, small multiples, time-arcs, geographic map visualizations, and hybrid techniques such as NodeTrix. NetPanorama aims to remove the need to use multiple libraries for analysis, wrangling, and visualization. Consequently, NetPanorama supports the agile development of applications for visual exploration of networks and data-driven storytelling. Documentation, source code, further examples, and an interactive online editor can be found online: <a target="_blank" rel="noopener" href="https://netpanorama.netlify.app/">https://netpanorama.netlify.app/</a>. </p>
<blockquote>
<p>本文介绍了NetPanorama，这是一种针对交互式网络可视化设计的领域特定语言和声明性语法，支持多元、时间和地理网络。NetPanorama允许用户将网络可视化指定为原始元素和构建块的组合。这些支持网络创建和转换，包括计算指标；排序、系列布局；视觉编码，包括字形、分面和标签可见性；以及用于探索和修改样式的交互。这种方法可以创建各种可视化效果，包括多种类型的节点链接图、使用各种单元格编码和节点排序的邻接矩阵、弧图、PivotGraph、小型复制品、时间弧、地理地图可视化以及诸如NodeTrix之类的混合技术。NetPanorama的目标是消除在分析、整理和可视化过程中需要使用多个库的需求。因此，NetPanorama支持网络可视化探索应用程序的敏捷开发和数据驱动的故事讲述。文档、源代码、更多示例以及在线交互式编辑器可以在线找到：<a target="_blank" rel="noopener" href="https://netpanorama.netlify.app/">https://netpanorama.netlify.app/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18902v2">PDF</a> </p>
<p><strong>Summary</strong><br>    本文介绍了NetPanorama，这是一种针对交互式网络可视化设计的领域特定语言和声明性语法。它支持多元、时态和地理网络的可视化，允许用户将网络可视化指定为原始元素和构建块的组合，支持网络创建和转换，包括计算指标、排序、序列化、布局、视觉编码以及探索和修改样式的交互。NetPanorama旨在消除在分析、数据整理和可视化过程中使用多个库的需求，支持网络可视化应用程序的敏捷开发和数据驱动的故事叙述。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NetPanorama是一种用于交互式网络可视化设计的领域特定语言和声明性语法。</li>
<li>它支持多元、时态和地理网络的可视化。</li>
<li>NetPanorama允许用户通过组合原始元素和构建块来创建各种网络可视化。</li>
<li>该方法涵盖了网络的创建和转换，包括计算指标、排序、布局和视觉编码等方面。</li>
<li>NetPanorama支持交互探索和网络可视化的敏捷应用程序开发。</li>
<li>NetPanorama旨在消除在分析、数据整理和可视化过程中使用多个库的需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.18902">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-179a2180271de8ed7a5eac95441c0710.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d199d68a6c006291fa9c6f12cb7a01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-481822f6e81a7b7809a1465b85112859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ae268e97afa4b443c449509c3f94c5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d271c75ab9ed1e355d519c9915a252f2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Differentially-Private-Optimization-for-Non-Decomposable-Objective-Functions"><a href="#Differentially-Private-Optimization-for-Non-Decomposable-Objective-Functions" class="headerlink" title="Differentially Private Optimization for Non-Decomposable Objective   Functions"></a>Differentially Private Optimization for Non-Decomposable Objective   Functions</h2><p><strong>Authors:Weiwei Kong, Andrés Muñoz Medina, Mónica Ribero</strong></p>
<p>Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as contrastive loss, that favor minimizing the distance between similar inputs and maximizing the distance between distinct inputs. As privacy concerns mount, training these models using differential privacy has become more important. However, due to how inputs are generated for these losses, one of their undesirable properties is that their $L_2$ sensitivity grows with the batch size. This property is particularly disadvantageous for differentially private training methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD variant for similarity based loss functions – in particular, the commonly-used contrastive loss – that manipulates gradients of the objective function in a novel way to obtain a sensitivity of the summed gradient that is $O(1)$ for batch size $n$. We test our DP-SGD variant on some CIFAR-10 pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our method’s performance comes close to that of a non-private model and generally outperforms DP-SGD applied directly to the contrastive loss. </p>
<blockquote>
<p>无监督预训练是计算机视觉模型和大语言模型开发中的常见步骤。在这种背景下，由于没有标签，需要使用基于相似度的损失函数，如对比损失，这些函数有利于缩小相似输入之间的距离并最大化不同输入之间的距离。随着对隐私的担忧日益增加，使用差分隐私训练这些模型变得越来越重要。然而，由于这些损失如何生成输入，其不希望有的属性之一是它们的L2敏感性随批次大小的增长而增长。这一属性对于差分私有训练方法（如DP-SGD）尤其不利。为了克服这个问题，我们为基于相似性的损失函数开发了一种新型的DP-SGD变体——特别是常用的对比损失——以新颖的方式操作目标函数的梯度，以获得对批次大小为n的梯度总和的O（1）灵敏度。我们在CIFAR-10的预训练和CIFAR-100的微调任务上测试了我们的DP-SGD变体，并显示在这两个任务中，我们的方法的性能接近非私有模型，并且通常优于直接应用于对比损失的DP-SGD。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03104v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了无监督预训练在计算机视觉模型和大型语言模型中的应用，以及在这种设置下使用基于相似性的损失函数（如对比损失）进行训练的重要性。随着隐私问题的日益突出，使用差分隐私进行模型训练变得更为重要。然而，由于这些损失函数生成输入的方式，其L2敏感性会随着批次大小的增长而增长，这对于差分私有训练方法（如DP-SGD）尤为不利。为解决这一问题，研究者开发了一种新的针对基于相似性损失函数的DP-SGD变体，通过以新颖的方式操作目标函数的梯度，使批次大小为n时的梯度敏感度为O(1)。测试表明，在新的DP-SGD变体在CIFAR-10预训练和CIFAR-100微调任务中的性能接近非私有模型，并且通常优于直接应用于对比损失的DP-SGD。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无监督预训练在计算机视觉模型和大型语言模型中广泛应用，使用基于相似性的损失函数如对比损失进行训练是关键步骤。</li>
<li>随着隐私问题的重视，差分隐私在模型训练中的应用变得重要。</li>
<li>基于相似性损失函数的L2敏感性随批次大小增长的问题对于差分私有训练方法不利。</li>
<li>研究者开发了一种新的针对基于相似性损失函数的DP-SGD变体，通过操作目标函数的梯度来解决上述问题。</li>
<li>新的DP-SGD变体在CIFAR-10预训练和CIFAR-100微调任务中的性能接近非私有模型。</li>
<li>新的DP-SGD变体通常优于直接应用于对比损失的DP-SGD。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.03104">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7bf6b81d8bd8fb520153a9623e008677.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Detecting-Phishing-Sites-Using-ChatGPT"><a href="#Detecting-Phishing-Sites-Using-ChatGPT" class="headerlink" title="Detecting Phishing Sites Using ChatGPT"></a>Detecting Phishing Sites Using ChatGPT</h2><p><strong>Authors:Takashi Koide, Naoki Fukushi, Hiroki Nakano, Daiki Chiba</strong></p>
<p>The emergence of Large Language Models (LLMs), including ChatGPT, is having a significant impact on a wide range of fields. While LLMs have been extensively researched for tasks such as code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, has been largely unexplored. To combat the rising tide of cyber attacks due to the misuse of LLMs, it is important to automate detection by leveraging the advanced capabilities of LLMs.   In this paper, we propose a novel system called ChatPhishDetector that utilizes LLMs to detect phishing sites. Our system involves leveraging a web crawler to gather information from websites, generating prompts for LLMs based on the crawled data, and then retrieving the detection results from the responses generated by the LLMs. The system enables us to detect multilingual phishing sites with high accuracy by identifying impersonated brands and social engineering techniques in the context of the entire website, without the need to train machine learning models. To evaluate the performance of our system, we conducted experiments on our own dataset and compared it with baseline systems and several LLMs. The experimental results using GPT-4V demonstrated outstanding performance, with a precision of 98.7% and a recall of 99.6%, outperforming the detection results of other LLMs and existing systems. These findings highlight the potential of LLMs for protecting users from online fraudulent activities and have important implications for enhancing cybersecurity measures. </p>
<blockquote>
<p>大型语言模型（LLM）的出现，包括ChatGPT，对广泛领域产生了重大影响。虽然LLM在代码生成和文本合成等任务上已经被广泛研究，但它们在检测恶意网页内容，尤其是钓鱼网站方面的应用却被大大忽视了。为了应对因LLM的误用而日益增多的网络攻击浪潮，利用LLM的先进功能来自动化检测至关重要。在本文中，我们提出了一种名为ChatPhishDetector的新型系统，该系统利用LLM检测钓鱼网站。我们的系统通过网页爬虫从网站上收集信息，根据爬取的数据为LLM生成提示，然后从LLM生成的响应中检索检测结果。该系统使我们能够在不训练机器学习模型的情况下，通过识别假冒品牌和网站整体上下文中的社会工程技术，以高精确度检测多语言钓鱼网站。为了评估我们系统的性能，我们在自己的数据集上进行了实验，并与基线系统和几种LLM进行了比较。使用GPT-4V的实验结果表现出色，精确度为98.7%，召回率为99.6%，优于其他LLM和现有系统的检测结果。这些发现凸显了LLM在保护用户免受在线欺诈活动方面的潜力，对于增强网络安全措施具有重要的启示意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.05816v3">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型（LLM）如ChatGPT的出现，对多个领域产生了深远影响。尽管LLM在代码生成和文本合成等方面已有广泛研究，但其在检测恶意网页内容，尤其是钓鱼网站方面的应用却被忽视。为应对因LLM误用而引发的网络攻击潮，利用LLM的先进功能进行自动化检测至关重要。本文提出了一种名为ChatPhishDetector的新系统，该系统利用LLM检测钓鱼网站。该系统通过网页爬虫收集网站信息，基于爬取数据为LLM生成提示，并从LLM生成的响应中检索检测结果。该系统能够准确检测多语言钓鱼网站，通过识别假冒品牌和网站上下文中的社会工程技术，无需训练机器学习模型。使用GPT-4V进行的实验表明，该系统表现优异，精确度为98.7%，召回率为99.6%，优于其他LLM和现有系统。这些发现突显了LLM在保护用户免受在线欺诈活动方面的潜力，对加强网络安全措施具有重要意义。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）对多个领域产生影响，但在检测恶意网页内容方面的应用尚未被充分探索。</li>
<li>钓鱼网站检测面临挑战，需要新的解决方案来应对。</li>
<li>ChatPhishDetector系统利用LLM进行钓鱼网站检测，通过网页爬虫收集信息并生成针对LLM的提示。</li>
<li>系统能够在无需训练机器学习模型的情况下，准确检测多语言钓鱼网站。</li>
<li>ChatPhishDetector系统通过识别假冒品牌和网站上下文中的社会工程技术来检测钓鱼网站。</li>
<li>使用GPT-4V的实验结果表明，该系统表现优异，精确度和召回率均很高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.05816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5b04c4e96e3592eba11a344eb47e137d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e85fc94c559d9c60f8805952ee8bc3ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9703d66c2c769bf0d9ec7f6fde9bdf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d356e5c56189dfe3ab156a3748b7a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5aec823d569921f8b6ee8fb99a03af.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1dad80a4c1a092eeeaa1fdaccf907497.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-02-26  Introducing Visual Perception Token into Multimodal Large Language Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be6d752ce401869c5e645a63c25c02e9.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-02-22  NeRF-3DTalker Neural Radiance Field with 3D Prior Aided Audio   Disentanglement for Talking Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19758k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
