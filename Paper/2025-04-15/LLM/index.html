<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-15  Steering CLIP&#39;s vision transformer with sparse autoencoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-ff68f3b629529eea02d7b494c7b5137a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-15-更新"><a href="#2025-04-15-更新" class="headerlink" title="2025-04-15 更新"></a>2025-04-15 更新</h1><h2 id="Steering-CLIP’s-vision-transformer-with-sparse-autoencoders"><a href="#Steering-CLIP’s-vision-transformer-with-sparse-autoencoders" class="headerlink" title="Steering CLIP’s vision transformer with sparse autoencoders"></a>Steering CLIP’s vision transformer with sparse autoencoders</h2><p><strong>Authors:Sonia Joseph, Praneet Suresh, Ethan Goldfarb, Lorenz Hufe, Yossi Gandelsman, Robert Graham, Danilo Bzdok, Wojciech Samek, Blake Aaron Richards</strong></p>
<p>While vision models are highly capable, their internal mechanisms remain poorly understood – a challenge which sparse autoencoders (SAEs) have helped address in language, but which remains underexplored in vision. We address this gap by training SAEs on CLIP’s vision transformer and uncover key differences between vision and language processing, including distinct sparsity patterns for SAEs trained across layers and token types. We then provide the first systematic analysis on the steerability of CLIP’s vision transformer by introducing metrics to quantify how precisely SAE features can be steered to affect the model’s output. We find that 10-15% of neurons and features are steerable, with SAEs providing thousands more steerable features than the base model. Through targeted suppression of SAE features, we then demonstrate improved performance on three vision disentanglement tasks (CelebA, Waterbirds, and typographic attacks), finding optimal disentanglement in middle model layers, and achieving state-of-the-art performance on defense against typographic attacks. </p>
<blockquote>
<p>虽然视觉模型功能强大，但其内部机制仍知之甚少——这是一个稀疏自动编码器（SAEs）已在语言方面有所帮助但仍在视觉领域未得到充分探索的挑战。我们通过训练CLIP的视觉Transformer来构建SAE并揭示视觉与语言处理之间的关键差异，包括在不同层和标记类型上训练的SAE的独特稀疏模式。然后，我们通过引入指标来量化SAE特征能够精确地引导模型输出的程度，首次系统地分析了CLIP的视觉Transformer的可引导性。我们发现，可引导神经元和特征占10-15%，SAE提供了比基础模型更多数千个可引导特征。通过有针对性地对SAE特征进行抑制，我们在三个视觉分解任务（CelebA、Waterbirds和印刷攻击）上展示了性能提升，发现在中层模型中达到最佳分解效果，并在防御印刷攻击方面取得了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08729v1">PDF</a> 8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic   Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>本文探索了使用稀疏自动编码器（SAE）对CLIP视觉转换器的训练，并分析了其与语言处理的差异。研究发现，SAE特征的可操控性较高，能影响模型输出。通过针对性地抑制SAE特征，优化了三个视觉解纠缠任务（CelebA、Waterbirds和字体攻击防御）的性能，实现了对字体攻击防御的先进性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAEs被应用于CLIP的视觉转换器训练，有助于解决视觉模型内部机制理解不足的问题。</li>
<li>视觉和语言的处理在SAE中存在明显的差异，包括跨层和标记类型的稀疏模式。</li>
<li>SAE特征具有高度的可操控性，能够精确影响模型输出。</li>
<li>在训练过程中发现约10-15%的神经元和特征是可操控的，且SAE提供的可操控特征数量远超基础模型。</li>
<li>通过针对性地抑制SAE特征，优化了多个视觉解纠缠任务性能。</li>
<li>最优解纠缠出现在模型中层。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08729">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4bce2b2add8367d46f52f8e00ac59792.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c4e800f5cbcbec05faed55f07e37dda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa7add51d779db2e070b41c735b26991.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea8a329a826d75009387a2af6d1e4ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a176cdf86d3a4ee424ead0fa762c1b6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Chronicles-Using-Multimodal-LLMs-to-Analyze-Massive-Collections-of-Images"><a href="#Visual-Chronicles-Using-Multimodal-LLMs-to-Analyze-Massive-Collections-of-Images" class="headerlink" title="Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections   of Images"></a>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections   of Images</h2><p><strong>Authors:Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser</strong></p>
<p>We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (“trends”) across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., “what are the frequent types of changes in the city?”) without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., “addition of outdoor dining,”, “overpass was painted blue,” etc.). See more results and interactive demos at <a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles">https://boyangdeng.com/visual-chronicles</a>. </p>
<blockquote>
<p>我们提出了一种使用多模态大型语言模型（MLLMs）的系统，用于分析包含数亿张在不同时间捕获的图像的大型数据库，旨在发现时间变化的模式。具体来说，我们的目标是捕捉城市某一时期频繁同时发生的变化（“趋势”）。与传统的视觉分析不同，我们的分析能够回答开放式查询（例如，“城市中最常见的变化类型是什么？”），而无需任何预定的目标主题或训练标签。这些特性使得基于先前学习或无监督的视觉分析工具不适用。我们认为MLLMs是一种新型工具，具有开放式语义理解能力。然而，我们的数据集规模庞大，超出了MLLM能够处理的上下文范围。因此，我们引入了自下而上的程序，将大规模视觉分析问题分解为更容易解决的子问题。我们针对每个子问题精心设计基于MLLM的解决方案。通过与我们系统的实验和消融研究，我们发现它明显优于基线，并能够发现从大城市图像中捕捉到的有趣趋势（例如，“增加户外用餐”，“高架桥被涂成蓝色”等）。更多结果和交互演示请访问：<a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles%E3%80%82">https://boyangdeng.com/visual-chronicles。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08727v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles">https://boyangdeng.com/visual-chronicles</a>; second and   third listed authors have equal contributions</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个使用多模态大型语言模型（MLLMs）分析包含数亿张在不同时间捕获的图像的大型数据库的系统。旨在发现时间变化的模式，尤其是城市环境中的频繁发生的趋势。该研究使用一种自下而上的方法，将大规模视觉分析问题分解为更易于处理的问题，并使用MLLMs解决每个子问题。实验和消融研究表明，该系统显著优于基线方法，能够从大型城市的图像中发现有趣的趋势。有关更多结果和交互式演示，请访问：<a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles">网站链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究者提出了一个利用多模态大型语言模型（MLLMs）的系统来检测大规模数据库中的图像的时间变化模式。</li>
<li>系统的目标是发现城市中频繁出现的趋势变化。</li>
<li>与传统的视觉分析工具不同，该系统能够回答开放式查询，无需预设目标主题或训练标签。</li>
<li>多模态大型语言模型被用作此系统的核心工具，由于其具有开放式的语义理解能力。</li>
<li>面对巨大的数据量问题，研究人员引入了自下而上的策略将大型问题分解成若干小问题来处理。并且对每个子问题使用特定的MLLM解决方案进行处理。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7395fea7fbd1496ef229c94e2274e3f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33bffbc52d7c22558153a631d8efdb95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47cb602006d1f58e3134e2d7b065cf09.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation"><a href="#DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation" class="headerlink" title="DocAgent: A Multi-Agent System for Automated Code Documentation   Generation"></a>DocAgent: A Multi-Agent System for Automated Code Documentation   Generation</h2><p><strong>Authors:Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang</strong></p>
<p>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories. </p>
<blockquote>
<p>高质量的代码文档对软件开发至关重要，特别是在人工智能时代。然而，使用大型语言模型（LLM）自动生成代码文档仍然具有挑战性，因为现有方法通常会产生不完整、无帮助或事实错误的输出。我们引入了DocAgent，这是一种新的多智能体协作系统，采用拓扑代码处理来进行增量上下文构建。专门的智能体（阅读者、搜索者、编写者、验证者、协调者）协同生成文档。我们还提出了一个多方面的评估框架，评估文档的完整性、帮助性和真实性。综合实验表明，DocAgent持续且显著地优于基线。我们的消融研究证实了拓扑处理顺序的重要作用。DocAgent为复杂和专有存储库中的可靠代码文档生成提供了稳健的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08725v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>高质代码文档对软件开发至关重要，尤其在AI时代。然而，使用大型语言模型（LLM）自动生成文档仍存在挑战，现有方法常产生不完整、无帮助或事实错误的输出。我们推出DocAgent，一种新型多智能体协作系统，采用拓扑代码处理进行增量上下文构建。专业智能体（阅读器、搜索器、编写器、核查器、协调器）协同生成文档。我们还提出一个多方面的评估框架，评估完整性、帮助性和真实性。综合实验显示，DocAgent持续且显著优于基线。我们的消融研究证实了拓扑处理顺序的关键作用。DocAgent为复杂和专有存储库中的可靠代码文档生成提供了稳健的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>高质代码文档在软件开发中至关重要，特别是在AI时代。</li>
<li>使用大型语言模型（LLM）自动生成代码文档具有挑战性，因为现有方法常产生不完整的、无帮助的或事实错误的输出。</li>
<li>DocAgent是一种新型的多智能体协作系统，通过拓扑代码处理进行增量上下文构建来生成文档。</li>
<li>DocAgent包括五个专业智能体：阅读器、搜索器、编写器、核查器和协调器。</li>
<li>提出了一个多面的评估框架，评估生成的文档在完整性、帮助性和真实性方面的表现。</li>
<li>综合实验显示，DocAgent在性能上显著优于其他方法。</li>
<li>DocAgent的消融研究证实了拓扑处理顺序在文档生成中的重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08725">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84dea2119fbe38fca6011cfa93c867fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fd0534526e9f43e1ed4683ac785090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933e5dcbc145e1cb1361f03356dd2b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28680bc575a8c0f69c2ab5c8868cc5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cae1fe7424320522f3e73e14023dd673.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Beyond-Black-Box-Predictions-Identifying-Marginal-Feature-Effects-in-Tabular-Transformer-Networks"><a href="#Beyond-Black-Box-Predictions-Identifying-Marginal-Feature-Effects-in-Tabular-Transformer-Networks" class="headerlink" title="Beyond Black-Box Predictions: Identifying Marginal Feature Effects in   Tabular Transformer Networks"></a>Beyond Black-Box Predictions: Identifying Marginal Feature Effects in   Tabular Transformer Networks</h2><p><strong>Authors:Anton Thielmann, Arik Reuter, Benjamin Saefken</strong></p>
<p>In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in these areas. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts. To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the model’s predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/OpenTabular/NAMpy">https://github.com/OpenTabular/NAMpy</a>. </p>
<blockquote>
<p>近年来，深度神经网络在各种任务中展示了其预测能力。除了自然语言处理，Transformer架构在解决表格数据问题和挑战中表现出了高效性，并挑战了这些领域中之前占主导地位的基于梯度的决策树。然而，这种预测能力是以可解释性为代价的：深度表格Transformer网络的黑盒性质几乎完全丧失了边缘特征效应。使用经典统计回归模型的加法约束的替代架构可以保持可理解的边缘特征效应，但在预测能力方面通常不如其更复杂的对应物。为了弥可解释性与性能之间的鸿沟，我们提出了一种适应表格Transformer网络的改进方法，旨在识别边缘特征效应。我们提供了理论证明可以准确地识别边缘特征效应，并且我们的消融研究证明，该模型可以有效地检测这些效应，即使在复杂的特征交互中也是如此。为了展示模型的预测能力，我们将其与几种可解释的黑盒模型进行了比较，发现它在保持可解释性的同时，可以匹配黑盒的性能。源代码可在<a target="_blank" rel="noopener" href="https://github.com/OpenTabular/NAMpy%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OpenTabular/NAMpy找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08712v1">PDF</a> </p>
<p><strong>Summary</strong><br>深度神经网络在各种任务中展现了强大的预测能力，尤其是转换器架构在处理表格数据问题时效率较高，但牺牲了可解释性。为兼顾可解释性和性能，研究团队提出一种改进型表格转换器网络，可准确识别边际特征效应。该模型既保持了深度网络的预测能力，又具备经典统计回归模型的可解释性。模型的源代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络在多种任务中表现出强大的预测能力，特别是在处理表格数据问题时，转换器架构的效率较高。</li>
<li>传统深度神经网络（如表格转换器网络）在预测时牺牲了可解释性，难以识别边际特征效应。</li>
<li>研究团队提出了一种改进型表格转换器网络，旨在准确识别边际特征效应。</li>
<li>该模型结合了深度网络的预测能力与经典统计回归模型的可解释性。</li>
<li>模型通过理论验证和实验验证，能够准确检测边际特征效应。</li>
<li>与其他可解释和不可解释的模型相比，该模型在预测能力上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08712">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6811a8a5e556aa8c356182c6c50870f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-482d3b9c5d9c3a88ff6d0d108c2004d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2b9552deea60814cba21c9e8325f84e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e6c8bb546f5beca9a11ee51948d357e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e4c62bba13ec4ace717d7ea74544d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SWE-PolyBench-A-multi-language-benchmark-for-repository-level-evaluation-of-coding-agents"><a href="#SWE-PolyBench-A-multi-language-benchmark-for-repository-level-evaluation-of-coding-agents" class="headerlink" title="SWE-PolyBench: A multi-language benchmark for repository level   evaluation of coding agents"></a>SWE-PolyBench: A multi-language benchmark for repository level   evaluation of coding agents</h2><p><strong>Authors:Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buccholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, Laurent Callot</strong></p>
<p>Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/SWE-PolyBench">https://github.com/amazon-science/SWE-PolyBench</a> </p>
<blockquote>
<p>由大型语言模型驱动的编码代理在软件工程任务中表现出了令人印象深刻的能力，但在多种编程语言和现实场景中对它们的性能进行评估仍然具有挑战性。我们介绍了SWE-PolyBench，这是一个新的多语言基准测试，用于对编码代理进行仓库级别的基于执行的评价。SWE-PolyBench包含来自21个仓库的2110个实例，包括Java（165个）、JavaScript（1017个）、TypeScript（729个）和Python（199个）的任务，涵盖错误修复、功能添加和代码重构。我们提供了一个任务和仓库分层子样本（SWE-PolyBench500），并发布了一个评估工具，可以实现全自动评估。为了更全面地对编码代理进行比较，这项工作还提出了基于语法树分析的新指标集。我们在SWE-PolyBench上评估了领先的开源编码代理，揭示了它们在语言、任务类型和复杂度类别方面的优势和局限性。我们的实验表明，当前代理在不同语言之间的性能表现不均衡，对复杂问题感到困难，而在简单任务上表现较好。SWE-PolyBench旨在推动开发更通用和更稳健的AI编码助理，用于现实世界的软件工程。我们的数据集和代码可在：<a target="_blank" rel="noopener" href="https://github.com/amazon-science/SWE-PolyBench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/amazon-science/SWE-PolyBench上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08703v1">PDF</a> 20 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型驱动的编码代理在软件工程任务中展现出令人印象深刻的能力，但在多种编程语言和现实场景中对它们的性能评估仍具有挑战性。为此，我们推出了SWE-PolyBench，这是一个新的多语言基准测试，用于对编码代理进行存储库级别的执行评估。它包含了来自21个存储库的2110个实例，涵盖了Java、JavaScript、TypeScript和Python中的任务，包括错误修复、功能添加和代码重构。我们还提供了一个任务分层子样本（SWE-PolyBench500），并发布了一个评估工具，可以实现完全自动化评估。为了更全面地比较编码代理，本文还提出了基于语法树分析的新指标集。我们在SWE-PolyBench上评估了领先的开源编码代理，揭示了它们在语言、任务类型和复杂度类别方面的优势和局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>编码代理在软件工程任务中表现出强大的能力，但在多种语言和现实场景中的性能评估具有挑战性。</li>
<li>SWE-PolyBench是一个新的多语言基准测试，用于评估编码代理在多种编程语言中的性能。</li>
<li>SWE-PolyBench包含来自不同编程语言的多种任务，如错误修复、功能添加和代码重构。</li>
<li>推出了SWE-PolyBench500任务分层子样本和自动化评估工具。</li>
<li>新的评估指标集基于语法树分析，以更全面地比较编码代理。</li>
<li>当前编码代理在不同语言和任务复杂度上的表现不均衡，对复杂问题有挑战，对简单任务表现较好。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08703">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2233545647eb40492497e0edf74ab1dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d3231822b18b5ffc11752e8d6afeae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-965a32ce7c5dd61107255472f6573e11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc5f22efe3e57492b7454d3afcf96a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c75445987ff4e3d61bad871380cc85f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TP-RAG-Benchmarking-Retrieval-Augmented-Large-Language-Model-Agents-for-Spatiotemporal-Aware-Travel-Planning"><a href="#TP-RAG-Benchmarking-Retrieval-Augmented-Large-Language-Model-Agents-for-Spatiotemporal-Aware-Travel-Planning" class="headerlink" title="TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for   Spatiotemporal-Aware Travel Planning"></a>TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for   Spatiotemporal-Aware Travel Planning</h2><p><strong>Authors:Hang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, Hao Liu</strong></p>
<p>Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs’ intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents. </p>
<blockquote>
<p>大型语言模型（LLM）在自动化旅行规划方面显示出巨大的潜力，但它们往往在处理微妙的时空理性方面存在不足。虽然现有的基准测试侧重于基本的计划有效性，但它们忽视了诸如路线效率、兴趣点吸引力和实时适应性等关键方面。本文介绍了针对检索增强、时空感知旅行规划量身定制的TP-RAG基准测试。我们的数据集包含2348个真实旅行查询、85575个精细颗粒度标注的兴趣点和18784个高质量旅行轨迹参考，这些参考源于在线旅游文档，可实现动态和上下文感知的规划。通过广泛的实验，我们发现集成参考轨迹可以显著提高旅行计划的空间效率和兴趣点的合理性，而由于存在冲突的参考和嘈杂的数据，在通用性和稳健性方面仍存在挑战。为了解决这些问题，我们提出了EvoRAG，这是一个进化框架，能够有力地协同多样化的检索轨迹与LLM的内在推理。EvoRAG达到了最先进的性能，与自下而上和检索增强的基线相比，提高了时空合规性并减少了常识违规。我们的工作强调了混合Web知识与LLM驱动优化的潜力，为更可靠和自适应的旅行规划代理铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08694v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型在旅行规划自动化方面展现出潜力，但在应对复杂时空理性方面存在不足。现有基准测试主要关注基本计划的有效性，忽略了路线效率、兴趣点吸引力和实时适应性等关键方面。本文引入TP-RAG，首个针对检索增强、时空感知旅行规划的基准测试。数据集包含2348个真实旅行查询、85575个精细颗粒度标注的兴趣点和18784条高质量旅行轨迹参考，可实现动态和上下文感知规划。通过广泛实验，我们发现集成参考轨迹可显著提高旅行计划的空间效率和合理性，而由于冲突参考和嘈杂数据，仍存在普遍性和稳健性挑战。为解决这些问题，我们提出EvoRAG，一个强大的进化框架，有效协同各种检索轨迹与LLM的内在推理。EvoRAG达到最新性能水平，提高时空合规性，减少常识违规，相较于基于地面和检索增强的基线。我们的工作突显了混合Web知识与LLM驱动优化的潜力，为更可靠和自适应的旅行规划代理铺平道路。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMs在自动化旅行规划方面展现出潜力，但仍需解决复杂时空理性问题。</li>
<li>现有基准测试主要关注基本计划的有效性，忽略了路线效率、兴趣点吸引力等关键方面。</li>
<li>引入TP-RAG基准测试，包含真实旅行查询、精细颗粒度标注的兴趣点和高质量旅行轨迹参考。</li>
<li>集成参考轨迹可提高旅行计划的空间效率和合理性。</li>
<li>仍存在普遍性和稳健性挑战，主要由于冲突参考和嘈杂数据。</li>
<li>提出EvoRAG框架，有效协同检索轨迹与LLM的内在推理，达到最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08694">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bac2860dc03f9f129669bf226fe977d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e6f5d6f6556c07174a68b593fd81313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69a0f63c0dc5a1a801bb660d86f1ec48.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Quality-evaluation-of-Tabby-coding-assistant-using-real-source-code-snippets"><a href="#Quality-evaluation-of-Tabby-coding-assistant-using-real-source-code-snippets" class="headerlink" title="Quality evaluation of Tabby coding assistant using real source code   snippets"></a>Quality evaluation of Tabby coding assistant using real source code   snippets</h2><p><strong>Authors:Marta Borek, Robert Nowak</strong></p>
<p>Large language models have become a popular tool in software development, providing coding assistance. The proper measurement of the accuracy and reliability of the code produced by such tools is a challenge due to natural language prompts.   We propose a simple pipeline that uses state-of-the-art implementation of classic and universal genres of algorithms and data structures. We focus on measuring the quality of TabbyML code assistant due to its open licence and the flexibility in the choice of the language model.   Our results presented as cyclomatic complexity, Halstead’s Bugs &amp; Effort and four text-based similarity matrices depict the usability of TabbyML in coding assistance tasks. </p>
<blockquote>
<p>大型语言模型已成为软件开发中的热门工具，为编码提供辅助。由于自然语言提示，准确衡量此类工具产生的代码准确性和可靠性是一个挑战。我们提出了一种简单的管道，该管道使用经典和通用算法和数据结构的最先进实现。我们专注于衡量TabbyML代码助手的品质，因其开放许可证和语言模型选择的灵活性。我们的结果以循环复杂度、Halstead的Bug＆Effort以及四个基于文本的相似矩阵呈现，描绘了TabbyML在编码辅助任务中的可用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08650v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在软件开发中提供编程辅助，其生成的代码准确性和可靠性评估是一大挑战。本研究提出一个简单流程，借助最新算法和数据结构实现经典和通用类型算法，重点评估TabbyML代码助理的质量。通过循环复杂性、Halstead的Bug与Effort以及四种基于文本相似性的矩阵展示其在编程辅助任务中的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型已成为软件开发中的热门工具，提供编程辅助。</li>
<li>评估此类工具生成的代码准确性和可靠性是一大挑战。</li>
<li>研究提出一个简单流程来评估TabbyML代码助理的质量。</li>
<li>重点使用循环复杂性、Halstead的Bug与Effort等标准来度量质量。</li>
<li>通过四种基于文本相似性的矩阵来展示其实用性。</li>
<li>TabbyML具有开放许可证和语言模型选择的灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-168fa0e46a725c39f4eabacfee530ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f671e790ec6bfe0c55ddecf2567da63.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MooseAgent-A-LLM-Based-Multi-agent-Framework-for-Automating-Moose-Simulation"><a href="#MooseAgent-A-LLM-Based-Multi-agent-Framework-for-Automating-Moose-Simulation" class="headerlink" title="MooseAgent: A LLM Based Multi-agent Framework for Automating Moose   Simulation"></a>MooseAgent: A LLM Based Multi-agent Framework for Automating Moose   Simulation</h2><p><strong>Authors:Tao Zhang, Zhenhai Liu, Yong Xin, Yongjun Jiao</strong></p>
<p>The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at <a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent">https://github.com/taozhan18/MooseAgent</a> </p>
<blockquote>
<p>有限元方法（FEM）在工程和科学计算中得到了广泛应用，但其预处理、求解器配置和后处理阶段通常耗时且需要专业知识。本文针对多物理仿真框架MOOSE，提出了一种自动化解决方案框架MooseAgent。该框架结合大规模预训练语言模型（LLM）与多智能体系统。框架使用LLM理解用户描述的自然语言仿真要求，并采用任务分解和多轮迭代验证策略自动生成MOOSE输入文件。为了提高精度并减少模型幻觉，系统构建并利用一个包含注释的MOOSE输入卡片和功能文档的向量数据库。我们在多个典型案例上进行了实验评估，包括传热、力学、相场和多物理耦合。结果表明，MooseAgent可以在一定程度上自动化MOOSE仿真过程，尤其是在处理相对简单的单物理问题时表现出较高的成功率。本研究的主要贡献是提出了针对MOOSE的多智能体自动化框架，验证了其在简化有限元仿真过程、降低用户门槛方面的潜力，为智能有限元仿真软件的开发提供了新的思路。本文提出的MooseAgent框架的代码已经开源，可在<a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent">https://github.com/taozhan18/MooseAgent</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08621v1">PDF</a> 7 pages, 2 Figs</p>
<p><strong>摘要</strong><br>本研究提出了一个针对多物理仿真框架MOOSE的自动化解决方案框架——MooseAgent。该框架结合了大规模预训练语言模型（LLMs）与多智能体系统，用于理解用户描述的自然语言仿真要求并自动生成MOOSE输入文件。研究通过实验评估了几个典型案例，证明了MooseAgent能够在一定程度上自动化MOOSE仿真过程，特别是在处理相对简单的单物理问题时表现出较高的成功率。本研究的主要贡献在于提出了一个针对MOOSE的多智能体自动化框架，为简化有限元仿真过程、降低用户门槛提供了新思路。该论文提出的MooseAgent框架代码已开源，可在<a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/taozhan18/MooseAgent获取。</a></p>
<p><strong>要点掌握</strong></p>
<ol>
<li>MooseAgent是一个结合了大规模预训练语言模型和多智能体系统的自动化解决方案框架，用于多物理仿真框架MOOSE。</li>
<li>它使用语言模型理解自然语言描述的仿真要求，并自动生成MOOSE输入文件。</li>
<li>通过实验评估，MooseAgent能够在一定程度上自动化MOOSE仿真过程。</li>
<li>在处理相对简单的单物理问题时，MooseAgent表现出较高的成功率。</li>
<li>该研究为简化有限元仿真过程、降低用户门槛提供了新的思路。</li>
<li>MooseAgent框架代码已开源，便于获取和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7f785e2bbadd5f7b0a69d3b85ceb37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895778a4defe0d18c5accfc76dd27ee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0e80dadc6623fd0ae96114217ef5154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50a9a88f863c4a7203b020812f710cb7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UoB-NLP-at-SemEval-2025-Task-11-Leveraging-Adapters-for-Multilingual-and-Cross-Lingual-Emotion-Detection"><a href="#UoB-NLP-at-SemEval-2025-Task-11-Leveraging-Adapters-for-Multilingual-and-Cross-Lingual-Emotion-Detection" class="headerlink" title="UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual   and Cross-Lingual Emotion Detection"></a>UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual   and Cross-Lingual Emotion Detection</h2><p><strong>Authors:Frances Laureano De Leon, Yixiao Wang, Yue Feng, Mark G. Lee</strong></p>
<p>Emotion detection in natural language processing is a challenging task due to the complexity of human emotions and linguistic diversity. While significant progress has been made in high-resource languages, emotion detection in low-resource languages remains underexplored. In this work, we address multilingual and cross-lingual emotion detection by leveraging adapter-based fine-tuning with multilingual pre-trained language models. Adapters introduce a small number of trainable parameters while keeping the pre-trained model weights fixed, offering a parameter-efficient approach to adaptation. We experiment with different adapter tuning strategies, including task-only adapters, target-language-ready task adapters, and language-family-based adapters. Our results show that target-language-ready task adapters achieve the best overall performance, particularly for low-resource African languages with our team ranking 7th for Tigrinya, and 8th for Kinyarwanda in Track A. In Track C, our system ranked 3rd for Amharic, and 4th for Oromo, Tigrinya, Kinyarwanda, Hausa, and Igbo. Our approach outperforms large language models in 11 languages and matches their performance in four others, despite our models having significantly fewer parameters. Furthermore, we find that adapter-based models retain cross-linguistic transfer capabilities while requiring fewer computational resources compared to full fine-tuning for each language. </p>
<blockquote>
<p>自然语言处理中的情感检测是一项具有挑战性的任务，这主要是由于人类情感的复杂性和语言多样性。虽然在高资源语言方面已经取得了重大进展，但在低资源语言中的情感检测仍然被忽视。在这项工作中，我们利用基于适配器的微调与多语言预训练语言模型来解决多语言和跨语言情感检测问题。适配器引入了少量的可训练参数，同时保持预训练模型权重固定，为适应性提供了一种参数高效的解决方案。我们尝试了不同的适配器调整策略，包括仅任务适配器、面向目标语言的任务适配器和基于语言家族的适配器。我们的结果表明，面向目标语言的任务适配器取得了最佳的整体性能，特别是在低资源的非洲语言中，我们的团队在A赛道上分别以第7名和第8名的成绩完成了提格里尼亚语和基尼亚卢旺达语的任务。在C赛道中，我们的系统在阿姆哈拉语中排名第3，在奥罗莫语、提格里尼亚语、基尼亚卢旺达语、豪萨语和伊博语中排名第4。我们的方法在许多语言上的表现优于大型语言模型，并在其他四种语言中与之匹配表现水平，尽管我们的模型参数明显更少。此外，我们发现基于适配器的模型既保留了跨语言的迁移能力，又相对于每种语言的完全微调方法减少了计算资源需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08543v1">PDF</a> Accepted to appear in Proceedings of the 19th International Workshop   on Semantic Evaluation (SemEval-2025)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了自然语言处理中的情感检测任务，尤其是在低资源语言中的挑战。研究团队通过利用基于适配器的微调与多语言预训练语言模型来解决多语言和跨语言情感检测问题。适配器引入少量可训练参数，同时保持预训练模型权重不变，提供了一种高效的参数化适应方法。实验结果显示，针对特定语言的适配器在非洲低资源语言中表现最佳，其中某些系统在特定赛道中名列前茅。此方法在多语言中表现优于大型语言模型，并保留跨语言迁移能力，同时减少了计算资源需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感检测是自然语言处理中的一项挑战，特别是在低资源语言中。</li>
<li>研究团队通过适配器微调与多语言预训练语言模型来解决该问题。</li>
<li>适配器是一种高效的参数化适应方法，可以引入少量可训练参数同时保持预训练模型权重不变。</li>
<li>针对特定语言的适配器在非洲低资源语言中表现最佳。</li>
<li>研究系统在特定赛道中名列前茅，展示了该方法的有效性。</li>
<li>该方法在多语言中表现优于大型语言模型，并保留跨语言迁移能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08543">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-704d2ea873626670d1be38460e630025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112e5f9ab364118b21342bf0db6f78ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66fcba804635cbdd9ed0d047ffd3ee05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ba62f5242b739af216c04e3eb9f7f49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d610f696ccb5a61f038afcb1b8f77cf1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Embodied-Image-Captioning-Self-supervised-Learning-Agents-for-Spatially-Coherent-Image-Descriptions"><a href="#Embodied-Image-Captioning-Self-supervised-Learning-Agents-for-Spatially-Coherent-Image-Descriptions" class="headerlink" title="Embodied Image Captioning: Self-supervised Learning Agents for Spatially   Coherent Image Descriptions"></a>Embodied Image Captioning: Self-supervised Learning Agents for Spatially   Coherent Image Descriptions</h2><p><strong>Authors:Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo Natale</strong></p>
<p>We present a self-supervised method to improve an agent’s abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a> </p>
<blockquote>
<p>我们提出了一种自监督方法，用于提高智能体在主动探索通用环境时描述任意物体的能力。这是一个具有挑战性的问题，因为当前模型由于不同相机视角和混乱的场景，难以获得连贯的图像描述。我们提出了一个三阶段框架来微调现有的描述模型，通过共识机制提高跨不同视角的描述准确性和一致性。首先，智能体探索环境，收集带有噪声的图像描述配对。然后，使用大型语言模型通过共识为每个对象实例提炼出一致的伪描述。最后，这些伪描述被用于微调现成的描述模型，并添加对比学习。我们在手动标记的测试集上分析了结合描述模型、探索策略、伪标签方法和微调策略的性能。结果表明，可以训练一种策略来挖掘与传统基线相比具有更高分歧的样本。我们的伪描述方法结合了所有策略后，与现有其他方法相比具有更高的语义相似性，并且微调能够显著提高描述的准确性和一致性。代码和测试集注释可在 <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08531v1">PDF</a> 11 pages, 8 figures, 5 tables, code and test set annotations   available at <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a></p>
<p><strong>Summary</strong>：<br>提出一种自监督方法，通过三阶段框架优化智能体描述任意对象的能力，同时主动探索通用环境。首先收集噪声图像-字幕对，然后通过共识机制蒸馏出每个对象实例的一致伪字幕，最后使用对比学习对现成的字幕模型进行微调。实验结果表明，该方法提高了字幕的准确性和一致性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出一种自监督方法用于智能体描述对象能力优化，在通用环境中主动探索。</li>
<li>采用三阶段框架对现有的字幕模型进行微调，提高字幕准确性和跨视图的一致性。</li>
<li>通过共识机制收集噪声图像-字幕对，并蒸馏出对象实例的一致伪字幕。</li>
<li>对比学习用于增强模型的性能。</li>
<li>实验结果表明，训练策略可以挖掘样本中的高分歧点，与传统基线相比具有优势。</li>
<li>伪字幕方法与所有策略相结合，在语义相似性方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-32ee73d10a6c67b6e03a51af34e767c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8a5136d3b534254bce7a55f0c6c267a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dca6b6e4a0b845d2ff18f67c0b643262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291bfe9746c2573272726ddc95826c02.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Task-Memory-Engine-TME-Enhancing-State-Awareness-for-Multi-Step-LLM-Agent-Tasks"><a href="#Task-Memory-Engine-TME-Enhancing-State-Awareness-for-Multi-Step-LLM-Agent-Tasks" class="headerlink" title="Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM   Agent Tasks"></a>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM   Agent Tasks</h2><p><strong>Authors:Ye Ye</strong></p>
<p>Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a>. </p>
<blockquote>
<p>大型语言模型（LLM）越来越多地被用作多步骤任务的自主代理。然而，现有的大多数框架无法维持对任务状态的结构化理解，通常依赖于线性提示串联或浅层内存缓冲区。这导致性能不稳定、经常产生幻觉和长期连贯性差。在这项工作中，我们提出了任务记忆引擎（TME），这是一个轻量级、结构化的内存模块，使用分层任务记忆树（TMT）来跟踪任务执行情况。树中的每个节点对应于一个任务步骤，存储相关的输入、输出、状态和子任务关系。我们引入了一种提示合成方法，该方法基于活动节点路径动态生成LLM提示，显著提高了执行一致性上下文定位。通过对多步骤代理任务的案例研究和比较实验，我们证明了TME在提高任务完成准确性和解释性行为方面具有优势，并且实现开销极小。TME的完整实现可在<a target="_blank" rel="noopener" href="https://github.com/biubiotomato/TME-Agent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/biubiotomato/TME-Agent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08525v1">PDF</a> 14 pages, 5 figures. Preprint prepared for future submission.   Includes implementation and token-efficiency analysis. Code at   <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a></p>
<p><strong>Summary</strong></p>
<p>LLM作为多步骤任务自主代理的使用日益普遍，但现有框架难以维持任务状态的结构化理解，导致性能不稳定、频繁出现幻觉和长程连贯性差。本文提出Task Memory Engine（TME），一个轻量级结构化内存模块，使用分层任务记忆树（TMT）追踪任务执行。树中每个节点对应任务步骤，存储相关输入、输出、状态和子任务关系。引入基于活动节点路径动态生成LLM提示的提示合成方法，显著提高执行一致性及上下文定位能力。通过案例研究和多步骤代理任务的对比实验，证明TME能提高任务完成准确率并增强行为可解释性，且实现开销极小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多步骤任务中作为自主代理的应用逐渐增加，但存在对任务状态理解不稳定的问题。</li>
<li>当前大多数框架无法维持任务状态的结构化理解，导致性能不稳定和长程连贯性差。</li>
<li>Task Memory Engine（TME）是一个轻量级结构化内存模块，用于追踪任务执行，解决上述问题。</li>
<li>TME使用分层任务记忆树（TMT）来组织任务信息，每个节点存储相关输入、输出、状态和子任务关系。</li>
<li>TME引入提示合成方法，根据活动节点路径动态生成LLM提示，提高执行一致性及上下文定位能力。</li>
<li>通过案例研究和实验验证，TME能显著提高任务完成准确率并增强行为可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-acec3be0434fd047eae93d6b8bec6ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56cfc4f925f501a97f8aa2f5539a97f4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VLMT-Vision-Language-Multimodal-Transformer-for-Multimodal-Multi-hop-Question-Answering"><a href="#VLMT-Vision-Language-Multimodal-Transformer-for-Multimodal-Multi-hop-Question-Answering" class="headerlink" title="VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop   Question Answering"></a>VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop   Question Answering</h2><p><strong>Authors:Qi Zhi Lim, Chin Poo Lee, Kian Ming Lim, Kalaiarasi Sonai Muthu Anbananthen</strong></p>
<p>The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) often suffer from limited reasoning capabilities, reliance on modality conversion, and inadequate alignment between visual and textual representations. To address these limitations, this paper introduces Vision-Language Multimodal Transformer (VLMT), a unified architecture that integrates a transformer-based vision encoder with a sequence-to-sequence language model. VLMT employs a direct token-level injection mechanism to fuse visual and textual inputs within a shared embedding space, eliminating the need for intermediate projection layers. To enhance cross-modal alignment and reasoning, a three-stage pretraining strategy is proposed to progressively align vision-language representations and improve the model’s capacity for multimodal understanding. Based on the pretrained backbone, two task-specific modules are instantiated to form a two-stage MMQA framework: a multimodal reranker that predicts document relevance scores and utilizes a relative threshold with top-k strategy for context retrieval, and a multimodal question answering model that generates contextually grounded answers based on the retrieved evidence. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed approach. On MultimodalQA validation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1, outperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8% in F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as PERQA by +3.2. These results highlight VLMT’s strong capabilities in multimodal reasoning and its potential to advance real-world information retrieval and question answering systems. </p>
<blockquote>
<p>随着文本、表格和图像中多模态数据的日益普及，为开发能够处理复杂跨模态推理的模型带来了新的挑战。现有的多模态多跳问答（MMQA）方法往往存在推理能力有限、依赖模态转换以及视觉和文本表示之间对齐不足的问题。为了解决这个问题，本文引入了视觉语言多模态转换器（VLMT），这是一个统一的架构，它将基于转换器的视觉编码器与序列到序列的语言模型进行了集成。VLMT采用直接的token级注入机制，在共享嵌入空间中融合视觉和文本输入，从而无需中间投影层。为了提高跨模态对齐和推理能力，提出了一种三阶段预训练策略，以逐步对齐视觉语言表示并增强模型的多模态理解能力。基于预训练的骨干网，实例化两个特定任务的模块，形成两阶段MMQA框架：一个多模态排序器，用于预测文档相关性分数，并使用相对阈值与top-k策略进行上下文检索；一个多模态问答模型，根据检索到的证据生成上下文相关的答案。在两个基准数据集上的综合实验证明了所提出方法的有效性。在MultimodalQA验证集上，VLMT-Large达到76.5%的精确匹配率和80.1%的F1得分，在精确匹配和F1得分上分别优于之前的最先进方法+9.1%和+8.8%。在WebQA上，它达到了47.6的QA得分，超越了之前的模型，如PERQA+3.2。这些结果突显了VLMT在多模态推理方面的强大能力，以及其推动现实世界信息检索和问答系统的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08269v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>面向文本、表格和图像的多模态数据日益丰富，为发展能够处理复杂跨模态推理的模型带来了新的挑战。现有跨模态多跳问答（MMQA）方法存在推理能力有限、依赖模态转换以及视觉和文本表示对齐不足的问题。为解决这些问题，本文提出一种名为视觉语言多模态转换器（VLMT）的统一架构，该架构融合了基于转换器的视觉编码器和序列到序列的语言模型。VLMT采用直接的令牌级注入机制，在共享嵌入空间中融合视觉和文本输入，无需中间投影层。为增强跨模态对齐和推理能力，提出了一种三阶段预训练策略，逐步对齐视觉语言表示并增强模型的多模态理解能力。基于预训练的主干，实例化两个任务特定模块，形成两阶段MMQA框架：多模态排名器用于预测文档相关性分数，并使用相对阈值与top-k策略进行上下文检索；多模态问答模型根据检索到的证据生成上下文相关的答案。在MultimodalQA验证集上，VLMT-Large达到76.5%的精确匹配率和80.1%的F1得分，相较于之前的最优模型在精确匹配率和F1得分上分别提高了9.1%和8.8%。在WebQA上，其问答得分达到47.6，超越了PERQA等先前模型。这些结果突显了VLMT在跨模态推理方面的强大能力及其对现实信息检索和问答系统推进的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态数据的日益丰富带来了新的挑战，需要发展具有复杂跨模态推理能力的模型。</li>
<li>现有MMQA方法存在推理能力有限、依赖模态转换和对齐不足的问题。</li>
<li>VLMT通过整合视觉和文本输入在一个共享嵌入空间内，解决了上述问题。</li>
<li>VLMT采用直接令牌级注入机制，避免了中间投影层的需要。</li>
<li>三阶段预训练策略用于增强跨模态对齐和推理能力。</li>
<li>VLMT在MultimodalQA和WebQA数据集上的表现优于先前的方法，展示了其强大的跨模态推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d815530ed34c5c95cf42ef50406f636d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5609be8d8263b710b6306e147d9e6246.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c321d75012e05bebdbbe529cce56ad32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04555a5a93bb693cd21e978965e0238b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Metamorphic-Testing-for-Fairness-Evaluation-in-Large-Language-Models-Identifying-Intersectional-Bias-in-LLaMA-and-GPT"><a href="#Metamorphic-Testing-for-Fairness-Evaluation-in-Large-Language-Models-Identifying-Intersectional-Bias-in-LLaMA-and-GPT" class="headerlink" title="Metamorphic Testing for Fairness Evaluation in Large Language Models:   Identifying Intersectional Bias in LLaMA and GPT"></a>Metamorphic Testing for Fairness Evaluation in Large Language Models:   Identifying Intersectional Bias in LLaMA and GPT</h2><p><strong>Authors:Harishwar Reddy, Madhusudan Srinivasan, Upulee Kanewala</strong></p>
<p>Large Language Models (LLMs) have made significant strides in Natural Language Processing but remain vulnerable to fairness-related issues, often reflecting biases inherent in their training data. These biases pose risks, particularly when LLMs are deployed in sensitive areas such as healthcare, finance, and law. This paper introduces a metamorphic testing approach to systematically identify fairness bugs in LLMs. We define and apply a set of fairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT model, a state-of-the-art LLM, across diverse demographic inputs. Our methodology includes generating source and follow-up test cases for each MR and analyzing model responses for fairness violations. The results demonstrate the effectiveness of MT in exposing bias patterns, especially in relation to tone and sentiment, and highlight specific intersections of sensitive attributes that frequently reveal fairness faults. This research improves fairness testing in LLMs, providing a structured approach to detect and mitigate biases and improve model robustness in fairness-sensitive applications. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理方面取得了显著进展，但仍易受到公平性问题的影响，这些问题通常反映了其训练数据中的固有偏见。当大型语言模型部署在医疗、金融和法律等敏感领域时，这些偏见带来的风险尤为突出。本文介绍了一种变异测试方法，以系统地识别大型语言模型中的公平漏洞。我们定义并应用了一套面向公平的变异关系（MRs）来评估最先进的LLaMA和GPT模型，跨越不同的人口统计输入。我们的方法包括为每个MR生成源测试用例和后续测试用例，并分析模型对公平违规的响应。结果表明，变异测试在揭示偏见模式方面尤其有效，特别是在语调和情感方面，并强调了敏感属性特定的交集，这些属性经常揭示公平故障。本研究提高了大型语言模型中的公平测试水平，提供了一种结构化的方法来检测和缓解偏见，并提高模型在敏感公平性应用的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07982v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>大型语言模型在自然语言处理领域取得显著进展，但仍存在公平性相关问题，反映其训练数据中的固有偏见。本文提出一种元测试方法，通过定义和应用面向公平性的元形态关系，系统地识别大型语言模型中的公平性问题漏洞。测试对象为当前先进的LLaMA和GPT模型，通过不同人口统计学输入进行评估。研究结果表明，元测试在揭示偏见模式方面尤为有效，特别是在语调和情感方面。本研究提高了大型语言模型在公平性测试方面的稳健性，提供了一种检测并缓解偏见问题的结构化方法。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型在自然语言处理方面取得显著进展，但存在公平性相关问题的脆弱性。</li>
<li>模型中的偏见反映在其训练数据中。</li>
<li>元测试方法用于系统地识别大型语言模型中的公平性问题漏洞。</li>
<li>测试对象包括LLaMA和GPT等先进的大型语言模型。</li>
<li>元测试特别有效于揭示模型中的偏见模式，特别是在语调和情感方面。</li>
<li>测试揭示了特定敏感属性交集经常出现的公平性问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eadc46d4bb7fffc522eff70064b630f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f57d2f14d437be2a4931dbbda203e0a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities"><a href="#An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities" class="headerlink" title="An Empirical Study of GPT-4o Image Generation Capabilities"></a>An Empirical Study of GPT-4o Image Generation Capabilities</h2><p><strong>Authors:Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi</strong></p>
<p>The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o’s image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling. For a high-definition version of the PDF, please refer to the link on GitHub: \href{<a target="_blank" rel="noopener" href="https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen%7D%7Bhttps://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen%7D">https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen}{https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen}</a>. </p>
<blockquote>
<p>图像生成领域已经迅速演变，从早期的基于GAN的方法到扩散模型，再到最近的寻求理解和生成任务之间桥梁的统一生成架构。最近的进展，尤其是GPT-4o，已经证明了高保真度多媒体生成的可行性，但其架构设计仍然神秘且未公开。这引发了人们关于图像和文本生成是否已经成功集成到这些方法中的统一框架的问题。在这项工作中，我们对GPT-4o的图像生成能力进行了实证研究，并将其与领先的开源和商业模型进行了比较。我们的评估涵盖了四个主要类别，包括文本到图像、图像到图像、图像到3D和图像到X生成，涵盖超过20项任务。我们的分析突出了GPT-4o在不同设置下的优势和局限性，并将其置于更广泛的生成模型演变中。通过这项调查，我们为未来的统一生成模型指明了有前景的方向，并强调了架构设计和数据规模扩大的作用。如需PDF的高清版本，请参阅GitHub上的链接：<a target="_blank" rel="noopener" href="https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen">链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05979v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了GPT-4o的图像生成能力，对比了现有的开源和商业模型。通过对GPT-4o进行多方面的评估，包括文本到图像、图像到图像、图像到三维和图像到X生成等四个主要类别超过二十项任务，本文总结了GPT-4o在不同场景下的优势和局限，并探讨了未来统一生成模型的发展方向，特别是架构设计和数据规模的作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4o在图像生成领域展现出强大的能力，特别是在高保真度多媒体生成方面。</li>
<li>GPT-4o的图像生成能力通过多项任务评估，包括文本到图像、图像到图像、图像到三维和图像到X生成。</li>
<li>GPT-4o在多种设置下表现出优势和局限，与其他开源和商业模型相比具有竞争力。</li>
<li>GPT-4o的架构设计对于其性能至关重要，但具体细节尚未公开。</li>
<li>数据规模对GPT-4o的图像生成性能有重要影响。</li>
<li>统一生成模型的研究仍处于发展初期，未来有很大的改进空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2010a11edf3032c967a7ee9f6ab24b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8b70545b43ecb6ffa3d082185a785b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653e211f893e469876b0aff041185c8b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MCP-Safety-Audit-LLMs-with-the-Model-Context-Protocol-Allow-Major-Security-Exploits"><a href="#MCP-Safety-Audit-LLMs-with-the-Model-Context-Protocol-Allow-Major-Security-Exploits" class="headerlink" title="MCP Safety Audit: LLMs with the Model Context Protocol Allow Major   Security Exploits"></a>MCP Safety Audit: LLMs with the Model Context Protocol Allow Major   Security Exploits</h2><p><strong>Authors:Brandon Radosevich, John Halloran</strong></p>
<p>To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer’s system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server’s tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.   The described MCP server auditing tool, MCPSafetyScanner, is freely available at: <a target="_blank" rel="noopener" href="https://github.com/johnhalloran321/mcpSafetyScanner">https://github.com/johnhalloran321/mcpSafetyScanner</a> </p>
<blockquote>
<p>为了降低开发成本，并使任何给定的生成式AI应用中的潜在组件无缝集成，模型上下文协议（MCP）（Anthropic，2024）最近已被发布并随后被广泛采用。MCP是一个开放协议，它标准化了对大型语言模型（LLM）、数据源和智能工具的API调用。通过连接多个MCP服务器（每个服务器都定义了一组工具、资源和提示），用户能够定义由LLM完全驱动的自动化工作流程。然而，我们表明，当前的MCP设计存在广泛的安全风险，会对终端用户造成威胁。特别是，我们通过演示证明，行业领先的LLM可能被诱导使用MCP工具，通过各种攻击来破坏AI开发者的系统，例如恶意代码执行、远程访问控制和凭证盗窃。为了积极缓解这些和相关攻击，我们引入了安全审计工具MCPSafetyScanner，它是第一个评估任意MCP服务器安全性的智能工具。MCPScanner使用多个智能代理来（a）根据MCP服务器的工具和资源自动确定对抗样本；（b）基于这些样本搜索相关漏洞和补救措施；（c）生成包含所有发现的安全报告。我们的工作强调了通用智能工作流程中的严重安全问题，同时提供了一个积极的工具来审计MCP服务器的安全性，并在部署之前解决检测到的漏洞。描述的MCP服务器审计工具MCPSafetyScanner可在以下网址免费获得：<a target="_blank" rel="noopener" href="https://github.com/johnhalloran321/mcpSafetyScanner">https://github.com/johnhalloran321/mcpSafetyScanner</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03767v2">PDF</a> 27 pages, 21 figures, and 2 Tables. Cleans up the TeX source</p>
<p><strong>摘要</strong></p>
<p>模型上下文协议（MCP）是一个开放协议，用于标准化对大型语言模型（LLM）、数据源和代理工具的API调用，以实现生成式AI应用程序各组件之间的无缝集成，减少开发成本。然而，当前MCP设计存在多种安全风险。本研究展示了一种针对MCP工具的安全审计工具MCPSafetyScanner，它能够评估任意MCP服务器的安全性，通过自动确定对抗样本、搜索相关漏洞和补救措施并生成安全报告来详细记录所有发现。本研究强调了通用代理工作流程的严重安全问题，同时提供了在部署前审计MCP服务器安全性并解决检测到的漏洞的主动工具。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>MCP是一个用于整合大型语言模型和其他AI工具的开放协议，促进了AI应用的开发。</li>
<li>当前MCP设计存在安全风险，可能导致AI系统被恶意攻击者利用。</li>
<li>LLMs可能被诱导通过MCP工具进行恶意代码执行、远程访问控制和凭证盗窃等攻击。</li>
<li>MCPSafetyScanner是第一个评估任意MCP服务器安全性的代理工具。</li>
<li>MCPSafetyScanner能够自动确定对抗样本、搜索相关漏洞和补救措施。</li>
<li>该工具能够生成详细的安全报告，记录所有发现的问题。</li>
<li>研究强调了通用代理工作流程的安全问题，并提供了在部署前解决这些安全问题的工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3614c95a7b2ed443baea9afc20dfe5ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c488ae57e82f554be331fe0b2361cee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d34221ff9d37a1630bdab898013718b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4ffbb5576a199085596b39112d7114b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>大型语言模型（LLM）在推进科学知识和应对复杂挑战方面表现出了显著潜力。在这项工作中，我们介绍了OmniScience，这是一个针对通用科学的专门大型推理模型，通过三个关键组件开发：（1）在精心筛选的科学文献语料库上进行领域自适应预训练，（2）在专门数据集上进行指令调整，以指导模型执行特定领域的任务，（3）通过微调进行基于推理的知识蒸馏，以显著提高其生成语境相关和逻辑严谨响应的能力。我们通过开发电池代理来展示OmniScience的通用性，该代理能够高效地排列分子作为潜在的电解质溶剂或添加剂。综合评估表明，OmniScience在GPQA Diamond和特定领域的电池基准测试上可与最新的大型推理模型相竞争，同时在参数数量相似的所有公共推理和非推理模型中表现最佳。我们还通过剔除实验进一步证明，领域自适应预训练和基于推理的知识蒸馏对于达到我们的性能水平至关重要，跨多个基准测试都是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在推动科学知识和应对复杂挑战方面展现出显著潜力。本研究介绍了一款针对通用科学的特殊大型推理模型——OmniScience。它经过三个关键组件开发而成：（1）在精心筛选的科学文献语料库上进行领域自适应预训练；（2）在特定数据集上进行指令调整，以指导模型执行特定领域的任务；（3）通过微调进行基于推理的知识蒸馏，以显著提高其生成语境相关和逻辑严谨响应的能力。OmniScience的通用性通过开发电池代理得到了验证，该代理能够高效地排列分子作为潜在的电解质溶剂或添加剂。综合评估表明，OmniScience在GPQA Diamond和特定领域电池基准测试上具备与最新大型推理模型竞争的能力，同时优于所有参数相似的公共推理和非推理模型。我们还通过消融实验证明，领域自适应预训练和基于推理的知识蒸馏对于达到我们的性能水平至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在推动科学知识和应对挑战方面有显著潜力。</li>
<li>OmniScience模型是通过领域自适应预训练、指令调整和基于推理的知识蒸馏三个关键组件开发而成的。</li>
<li>OmniScience模型具备通用性，能够应用于电池代理的开发，排列分子作为潜在的电解质溶剂或添加剂。</li>
<li>OmniScience在GPQA Diamond和特定领域电池基准测试上表现出竞争力。</li>
<li>消融实验证明，领域自适应预训练和基于推理的知识蒸馏对OmniScience的性能至关重要。</li>
<li>OmniScience模型优于参数相似的其他公共推理和非推理模型。</li>
<li>该研究展示了LLM在特定领域的应用潜力，为未来的科学研究提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5d6944cf303fea9494c3ca74eadb42e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval"><a href="#A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval" class="headerlink" title="A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval"></a>A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval</h2><p><strong>Authors:Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li</strong></p>
<p>Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>. </p>
<blockquote>
<p>信息技术深刻改变了人类与信息的交互方式。网上创建、共享和传播的大量内容使得获取相关信息的难度越来越大。在过去的二十年中，推荐系统和搜索（统称为信息检索系统）已经发生了显著变化以应对这些挑战。大型语言模型（LLM）的最新进展在各种语言相关任务中表现出了超越人类的性能，并展现出一般理解、推理和决策能力。本文探讨了LLM代理在增强推荐系统和搜索系统方面的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来阐述现有研究。我们强调了LLM代理在应对推荐和搜索方面的当前挑战的巨大潜力，为未来的研究方向提供了见解。本文首次系统回顾和分类了这些领域中的LLM代理研究，为如何利用这一先进的AI技术进行信息检索提供了新颖的视角。为了理解现有工作，我们在以下链接列出了关于LLM代理推荐和搜索的现有论文：<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search%E3%80%82">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05659v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>信息技术的发展深刻改变了人类与信息的交互方式。随着网上创建、共享和传播的内容大量增加，获取相关信息变得越来越困难。推荐系统和搜索（统称为信息检索系统）在过去的二十年中已经显著发展，以应对这些挑战。大型语言模型（LLM）的最新进展已展现出在各种语言任务中超越人类性能的能力，并展现出一般理解、推理和决策能力。本文探讨了LLM代理在增强推荐和搜索系统方面的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来详细阐述现有的研究。我们强调了LLM代理在应对推荐和搜索方面的当前挑战的巨大潜力，并为未来的研究方向提供见解。本文首次系统地回顾和分类了LLM代理在这些领域的研究，为如何利用这种先进的AI技术进行信息检索提供了新颖的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>信息技术改变了人类与信息的交互方式。</li>
<li>网上内容的大量增加使得获取相关信息更为困难。</li>
<li>推荐系统和搜索在信息检索中起到重要作用。</li>
<li>大型语言模型（LLM）在多种语言任务中表现超越人类，具备理解、推理和决策能力。</li>
<li>LLM代理在增强推荐和搜索系统的潜力方面展现出巨大的变革性。</li>
<li>LLM代理的动机和角色在推动其发展上起到关键作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7582776c0e259c15939fabf692554e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9003206c7a510ca96f8efc0e6a109ce5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning"><a href="#An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning" class="headerlink" title="An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning"></a>An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning</h2><p><strong>Authors:Navdeep Kaur, Lachlan McPheat, Alessandra Russo, Anthony G Cohn, Pranava Madhyastha</strong></p>
<p>In this paper, we examine the use of Conformal Language Modelling (CLM) alongside Answer Set Programming (ASP) to enhance the performance of standard open-weight LLMs on complex multi-step reasoning tasks. Using the StepGame dataset, which requires spatial reasoning, we apply CLM to generate sets of ASP programs from an LLM, providing statistical guarantees on the correctness of the outputs. Experimental results show that CLM significantly outperforms baseline models that use standard sampling methods, achieving substantial accuracy improvements across different levels of reasoning complexity. Additionally, the LLM-as-Judge metric enhances CLM’s performance, especially in assessing structurally and logically correct ASP outputs. However, calibrating CLM with diverse calibration sets did not improve generalizability for tasks requiring much longer reasoning steps, indicating limitations in handling more complex tasks. </p>
<blockquote>
<p>本文探讨了将顺应语言建模（CLM）与答案集编程（ASP）相结合，以提高标准开放权重大型语言模型在复杂多步骤推理任务上的性能。我们利用需要空间推理的StepGame数据集，应用CLM从大型语言模型生成ASP程序集，为输出的正确性提供统计保证。实验结果表明，CLM显著优于使用标准采样方法的基线模型，并在不同层次的推理复杂度上实现了实质性的准确性提高。另外，大型语言模型作为评估者的指标提高了CLM的性能，特别是在评估结构上和逻辑上正确的ASP输出方面。然而，用多种校准集校准CLM并没有提高对需要更长时间推理步骤的任务的通用性，这表明在处理更复杂的任务时存在局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了将Conformal语言建模（CLM）与答案集编程（ASP）结合使用，以提高标准开放权重大型语言模型（LLM）在复杂多步骤推理任务上的性能。研究使用StepGame数据集，该数据集要求空间推理能力。通过CLM生成ASP程序集，为输出结果提供统计正确性保证。实验结果表明，CLM显著优于使用标准采样方法的基线模型，在不同层次的推理复杂度上都实现了实质性的准确性提高。此外，大型语言模型作为评判者的指标也增强了CLM的性能，特别是在评估结构化和逻辑正确的ASP输出方面。然而，用各种校准集校准CLM并没有提高对需要更长期推理任务的通用性，这表明在处理更复杂的任务时存在局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文结合Conformal语言建模（CLM）与答案集编程（ASP），旨在提高大型语言模型（LLM）在复杂多步骤推理任务上的性能。</li>
<li>使用StepGame数据集进行实证研究，该数据集强调空间推理能力。</li>
<li>CLM能够生成ASP程序集，并为输出结果提供统计正确性保证。</li>
<li>实验结果显示CLM在准确性上显著优于基线模型，特别是在不同推理复杂度层次上。</li>
<li>LLM-as-Judge指标增强了CLM在评估ASP输出结构和逻辑正确性方面的性能。</li>
<li>使用多种校准集对CLM进行校准并未显著提高其处理更长期或更复杂推理任务的通用性，表明存在一定局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f87f0f4665100c6d9270c099f5c50cf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Retrieval-Augmented-Generation-for-Visual-Question-Answering"><a href="#Fine-Grained-Retrieval-Augmented-Generation-for-Visual-Question-Answering" class="headerlink" title="Fine-Grained Retrieval-Augmented Generation for Visual Question   Answering"></a>Fine-Grained Retrieval-Augmented Generation for Visual Question   Answering</h2><p><strong>Authors:Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang</strong></p>
<p>Visual Question Answering (VQA) focuses on providing answers to natural language questions by utilizing information from images. Although cutting-edge multimodal large language models (MLLMs) such as GPT-4o achieve strong performance on VQA tasks, they frequently fall short in accessing domain-specific or the latest knowledge. To mitigate this issue, retrieval-augmented generation (RAG) leveraging external knowledge bases (KBs), referred to as KB-VQA, emerges as a promising approach. Nevertheless, conventional unimodal retrieval techniques, which translate images into textual descriptions, often result in the loss of critical visual details. This study presents fine-grained knowledge units, which merge textual snippets with entity images stored in vector databases. Furthermore, we introduce a knowledge unit retrieval-augmented generation framework (KU-RAG) that integrates fine-grained retrieval with MLLMs. The proposed KU-RAG framework ensures precise retrieval of relevant knowledge and enhances reasoning capabilities through a knowledge correction chain. Experimental findings demonstrate that our approach significantly boosts the performance of leading KB-VQA methods, achieving an average improvement of approximately 3% and up to 11% in the best case. </p>
<blockquote>
<p>视觉问答（VQA）专注于通过利用图像中的信息来回答自然语言问题。尽管最前沿的多模态大型语言模型（如GPT-4o）在VQA任务上表现出强大的性能，但它们通常在访问特定领域或最新知识时显得不足。为了缓解这个问题，利用外部知识库的检索增强生成（RAG）技术，被称为KB-VQA，正成为一种有前景的方法。然而，传统的单模态检索技术将图像翻译成文本描述，这往往会导致关键视觉细节的丢失。本研究提出了精细知识单元，将文本片段与存储在向量数据库中的实体图像合并。此外，我们引入了知识单元检索增强生成框架（KU-RAG），将精细检索与MLLMs相结合。所提出的KU-RAG框架确保精确检索相关知识，并通过知识校正链增强推理能力。实验结果表明，我们的方法显著提高了领先的KB-VQA方法的性能，平均提高了约3%，在最佳情况下提高了高达11%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20964v2">PDF</a> </p>
<p><strong>摘要</strong><br>    本文关注视觉问答（VQA）领域，旨在通过图像信息回答自然语言问题。虽然当前先进的多模态大型语言模型（MLLMs）如GPT-4o在VQA任务上表现出强大的性能，但在访问特定领域或最新知识时常常存在缺陷。为解决这一问题，利用外部知识库（KBs）的检索增强生成（RAG）方法，即KB-VQA，展现出巨大潜力。然而，传统的单模态检索技术将图像转换为文本描述，往往会导致关键视觉细节的丢失。本研究提出精细粒度知识单元，将文本片段与存储在向量数据库中的实体图像合并。此外，还引入了知识单元检索增强生成框架（KU-RAG），将精细粒度检索与MLLMs结合。KU-RAG框架确保精确检索相关知识，并通过知识校正链增强推理能力。实验结果表明，该方法显著提升了领先的KB-VQA方法的性能，平均提高了约3%，最高提高了11%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视觉问答（VQA）是利用图像信息回答自然语言问题的研究领域。</li>
<li>多模态大型语言模型（MLLMs）在VQA任务上表现出强大的性能，但在访问特定领域或最新知识时存在局限。</li>
<li>检索增强生成（RAG）方法利用外部知识库（KBs）来解决MLLMs的缺陷，展现出巨大潜力。</li>
<li>传统单模态检索技术存在将图像转换为文本描述时丢失关键视觉细节的问题。</li>
<li>精细粒度知识单元将文本和图像结合，提高了检索的精确度。</li>
<li>引入的知识单元检索增强生成框架（KU-RAG）结合了精细粒度检索与MLLMs，提高了知识检索和推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20964">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1e45b66104a5fa6178fe0ff035e30304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5758906f148b4255a03cc17b0798fe41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f8fe0f761d70564f5946a712e974c1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c7bf2b909c1a099340f62d5bcb58120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8a41afeaa723a1d259a0d3144de9ae3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization"><a href="#ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization" class="headerlink" title="ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization"></a>ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization</h2><p><strong>Authors:Xuefeng Liu, Songhao Jiang, Ian Foster, Jinbo Xu, Rick Stevens</strong></p>
<p>Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A uniquely designed two-phase incremental training approach for pre-training the drug optimization GPT on molecule scaffold with enhanced performance. (3) A token-level decoding optimization strategy, TOP-N, that enabling controlled, reward-guided generation using pretrained&#x2F;finetuned GPT. We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing desired properties. </p>
<blockquote>
<p>在病毒快速变异和药物耐药性癌细胞面前，药物优化变得愈发关键。然而，这一任务仍然充满挑战，因为它需要在保留原始药物的有益特性的同时，还增强超出其范围之外的所需属性。在这项工作中，我们旨在通过引入ScaffoldGPT来解决这一挑战，这是一种基于分子支架的新型生成预训练转换器（GPT），专为药物优化而设计。我们的工作包括三个关键部分：（1）一个三阶段的药品优化方法，该方法整合了预训练、微调和解码优化。（2）一个独特设计的两阶段增量训练法，用于在分子支架上预训练药品优化GPT，提高性能。（3）一种令牌级别的解码优化策略TOP-N，它通过利用预训练&#x2F;微调的GPT实现受控的、奖励引导的生成。我们通过针对COVID和癌症基准测试的综合评估证明，ScaffoldGPT在药物优化基准测试中优于竞争对手，尤其擅长保留原始功能支架并增强所需属性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06891v2">PDF</a> </p>
<p><strong>摘要</strong><br>药物优化面临新的挑战，需要在快速变异的病毒菌株和耐药癌细胞背景下保持药物的原有优势属性，同时增强所需属性。本研究引入ScaffoldGPT，一种用于药物优化的新型生成预训练转换器（GPT），旨在解决这一挑战。研究包括三个关键部分：1. 三阶段药物优化方法，整合预训练、微调和解码优化；2. 独特设计的两阶段增量训练方法来预训练药物优化GPT；3. 一种token级别的解码优化策略TOP-N，通过奖励引导使用预训练或微调后的GPT生成内容。通过对COVID和癌症基准测试的综合评估，证明ScaffoldGPT在药物优化基准测试中优于竞争对手，同时在保持原始功能骨架和增强所需属性方面表现出色。</p>
<p><strong>要点</strong></p>
<ol>
<li>药物优化面临新的挑战，需要在保持原有优势属性的同时增强所需属性。</li>
<li>研究引入了ScaffoldGPT，一种新型生成预训练转换器（GPT），用于解决药物优化挑战。</li>
<li>ScaffoldGPT采用三阶段药物优化方法，包括预训练、微调和解码优化。</li>
<li>研究采用独特的两阶段增量训练方法来预训练药物优化GPT。</li>
<li>TOP-N是一种token级别的解码优化策略，可以通过奖励引导控制生成内容。</li>
<li>ScaffoldGPT在COVID和癌症基准测试中表现优异，优于竞争对手。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe9dae3a50317fedc0aa7a47523b9aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff68f3b629529eea02d7b494c7b5137a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9622a0e405fc2a4437ce21c7de2d4c72.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
