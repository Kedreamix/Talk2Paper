<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  Steering CLIP&#39;s vision transformer with sparse autoencoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-ff68f3b629529eea02d7b494c7b5137a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-15-æ›´æ–°"><a href="#2025-04-15-æ›´æ–°" class="headerlink" title="2025-04-15 æ›´æ–°"></a>2025-04-15 æ›´æ–°</h1><h2 id="Steering-CLIPâ€™s-vision-transformer-with-sparse-autoencoders"><a href="#Steering-CLIPâ€™s-vision-transformer-with-sparse-autoencoders" class="headerlink" title="Steering CLIPâ€™s vision transformer with sparse autoencoders"></a>Steering CLIPâ€™s vision transformer with sparse autoencoders</h2><p><strong>Authors:Sonia Joseph, Praneet Suresh, Ethan Goldfarb, Lorenz Hufe, Yossi Gandelsman, Robert Graham, Danilo Bzdok, Wojciech Samek, Blake Aaron Richards</strong></p>
<p>While vision models are highly capable, their internal mechanisms remain poorly understood â€“ a challenge which sparse autoencoders (SAEs) have helped address in language, but which remains underexplored in vision. We address this gap by training SAEs on CLIPâ€™s vision transformer and uncover key differences between vision and language processing, including distinct sparsity patterns for SAEs trained across layers and token types. We then provide the first systematic analysis on the steerability of CLIPâ€™s vision transformer by introducing metrics to quantify how precisely SAE features can be steered to affect the modelâ€™s output. We find that 10-15% of neurons and features are steerable, with SAEs providing thousands more steerable features than the base model. Through targeted suppression of SAE features, we then demonstrate improved performance on three vision disentanglement tasks (CelebA, Waterbirds, and typographic attacks), finding optimal disentanglement in middle model layers, and achieving state-of-the-art performance on defense against typographic attacks. </p>
<blockquote>
<p>è™½ç„¶è§†è§‰æ¨¡å‹åŠŸèƒ½å¼ºå¤§ï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘â€”â€”è¿™æ˜¯ä¸€ä¸ªç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEsï¼‰å·²åœ¨è¯­è¨€æ–¹é¢æœ‰æ‰€å¸®åŠ©ä½†ä»åœ¨è§†è§‰é¢†åŸŸæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒCLIPçš„è§†è§‰Transformeræ¥æ„å»ºSAEå¹¶æ­ç¤ºè§†è§‰ä¸è¯­è¨€å¤„ç†ä¹‹é—´çš„å…³é”®å·®å¼‚ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒå±‚å’Œæ ‡è®°ç±»å‹ä¸Šè®­ç»ƒçš„SAEçš„ç‹¬ç‰¹ç¨€ç–æ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æŒ‡æ ‡æ¥é‡åŒ–SAEç‰¹å¾èƒ½å¤Ÿç²¾ç¡®åœ°å¼•å¯¼æ¨¡å‹è¾“å‡ºçš„ç¨‹åº¦ï¼Œé¦–æ¬¡ç³»ç»Ÿåœ°åˆ†æäº†CLIPçš„è§†è§‰Transformerçš„å¯å¼•å¯¼æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯å¼•å¯¼ç¥ç»å…ƒå’Œç‰¹å¾å 10-15%ï¼ŒSAEæä¾›äº†æ¯”åŸºç¡€æ¨¡å‹æ›´å¤šæ•°åƒä¸ªå¯å¼•å¯¼ç‰¹å¾ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§åœ°å¯¹SAEç‰¹å¾è¿›è¡ŒæŠ‘åˆ¶ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªè§†è§‰åˆ†è§£ä»»åŠ¡ï¼ˆCelebAã€Waterbirdså’Œå°åˆ·æ”»å‡»ï¼‰ä¸Šå±•ç¤ºäº†æ€§èƒ½æå‡ï¼Œå‘ç°åœ¨ä¸­å±‚æ¨¡å‹ä¸­è¾¾åˆ°æœ€ä½³åˆ†è§£æ•ˆæœï¼Œå¹¶åœ¨é˜²å¾¡å°åˆ·æ”»å‡»æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08729v1">PDF</a> 8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic   Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å¯¹CLIPè§†è§‰è½¬æ¢å™¨çš„è®­ç»ƒï¼Œå¹¶åˆ†æäº†å…¶ä¸è¯­è¨€å¤„ç†çš„å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼ŒSAEç‰¹å¾çš„å¯æ“æ§æ€§è¾ƒé«˜ï¼Œèƒ½å½±å“æ¨¡å‹è¾“å‡ºã€‚é€šè¿‡é’ˆå¯¹æ€§åœ°æŠ‘åˆ¶SAEç‰¹å¾ï¼Œä¼˜åŒ–äº†ä¸‰ä¸ªè§†è§‰è§£çº ç¼ ä»»åŠ¡ï¼ˆCelebAã€Waterbirdså’Œå­—ä½“æ”»å‡»é˜²å¾¡ï¼‰çš„æ€§èƒ½ï¼Œå®ç°äº†å¯¹å­—ä½“æ”»å‡»é˜²å¾¡çš„å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAEsè¢«åº”ç”¨äºCLIPçš„è§†è§‰è½¬æ¢å™¨è®­ç»ƒï¼Œæœ‰åŠ©äºè§£å†³è§†è§‰æ¨¡å‹å†…éƒ¨æœºåˆ¶ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>è§†è§‰å’Œè¯­è¨€çš„å¤„ç†åœ¨SAEä¸­å­˜åœ¨æ˜æ˜¾çš„å·®å¼‚ï¼ŒåŒ…æ‹¬è·¨å±‚å’Œæ ‡è®°ç±»å‹çš„ç¨€ç–æ¨¡å¼ã€‚</li>
<li>SAEç‰¹å¾å…·æœ‰é«˜åº¦çš„å¯æ“æ§æ€§ï¼Œèƒ½å¤Ÿç²¾ç¡®å½±å“æ¨¡å‹è¾“å‡ºã€‚</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°çº¦10-15%çš„ç¥ç»å…ƒå’Œç‰¹å¾æ˜¯å¯æ“æ§çš„ï¼Œä¸”SAEæä¾›çš„å¯æ“æ§ç‰¹å¾æ•°é‡è¿œè¶…åŸºç¡€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡é’ˆå¯¹æ€§åœ°æŠ‘åˆ¶SAEç‰¹å¾ï¼Œä¼˜åŒ–äº†å¤šä¸ªè§†è§‰è§£çº ç¼ ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æœ€ä¼˜è§£çº ç¼ å‡ºç°åœ¨æ¨¡å‹ä¸­å±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bce2b2add8367d46f52f8e00ac59792.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c4e800f5cbcbec05faed55f07e37dda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa7add51d779db2e070b41c735b26991.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea8a329a826d75009387a2af6d1e4ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a176cdf86d3a4ee424ead0fa762c1b6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Chronicles-Using-Multimodal-LLMs-to-Analyze-Massive-Collections-of-Images"><a href="#Visual-Chronicles-Using-Multimodal-LLMs-to-Analyze-Massive-Collections-of-Images" class="headerlink" title="Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections   of Images"></a>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections   of Images</h2><p><strong>Authors:Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser</strong></p>
<p>We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (â€œtrendsâ€) across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., â€œwhat are the frequent types of changes in the city?â€) without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., â€œaddition of outdoor dining,â€, â€œoverpass was painted blue,â€ etc.). See more results and interactive demos at <a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles">https://boyangdeng.com/visual-chronicles</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç³»ç»Ÿï¼Œç”¨äºåˆ†æåŒ…å«æ•°äº¿å¼ åœ¨ä¸åŒæ—¶é—´æ•è·çš„å›¾åƒçš„å¤§å‹æ•°æ®åº“ï¼Œæ—¨åœ¨å‘ç°æ—¶é—´å˜åŒ–çš„æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ•æ‰åŸå¸‚æŸä¸€æ—¶æœŸé¢‘ç¹åŒæ—¶å‘ç”Ÿçš„å˜åŒ–ï¼ˆâ€œè¶‹åŠ¿â€ï¼‰ã€‚ä¸ä¼ ç»Ÿçš„è§†è§‰åˆ†æä¸åŒï¼Œæˆ‘ä»¬çš„åˆ†æèƒ½å¤Ÿå›ç­”å¼€æ”¾å¼æŸ¥è¯¢ï¼ˆä¾‹å¦‚ï¼Œâ€œåŸå¸‚ä¸­æœ€å¸¸è§çš„å˜åŒ–ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼‰ï¼Œè€Œæ— éœ€ä»»ä½•é¢„å®šçš„ç›®æ ‡ä¸»é¢˜æˆ–è®­ç»ƒæ ‡ç­¾ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—åŸºäºå…ˆå‰å­¦ä¹ æˆ–æ— ç›‘ç£çš„è§†è§‰åˆ†æå·¥å…·ä¸é€‚ç”¨ã€‚æˆ‘ä»¬è®¤ä¸ºMLLMsæ˜¯ä¸€ç§æ–°å‹å·¥å…·ï¼Œå…·æœ‰å¼€æ”¾å¼è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ•°æ®é›†è§„æ¨¡åºå¤§ï¼Œè¶…å‡ºäº†MLLMèƒ½å¤Ÿå¤„ç†çš„ä¸Šä¸‹æ–‡èŒƒå›´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªä¸‹è€Œä¸Šçš„ç¨‹åºï¼Œå°†å¤§è§„æ¨¡è§†è§‰åˆ†æé—®é¢˜åˆ†è§£ä¸ºæ›´å®¹æ˜“è§£å†³çš„å­é—®é¢˜ã€‚æˆ‘ä»¬é’ˆå¯¹æ¯ä¸ªå­é—®é¢˜ç²¾å¿ƒè®¾è®¡åŸºäºMLLMçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä¸æˆ‘ä»¬ç³»ç»Ÿçš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å®ƒæ˜æ˜¾ä¼˜äºåŸºçº¿ï¼Œå¹¶èƒ½å¤Ÿå‘ç°ä»å¤§åŸå¸‚å›¾åƒä¸­æ•æ‰åˆ°çš„æœ‰è¶£è¶‹åŠ¿ï¼ˆä¾‹å¦‚ï¼Œâ€œå¢åŠ æˆ·å¤–ç”¨é¤â€ï¼Œâ€œé«˜æ¶æ¡¥è¢«æ¶‚æˆè“è‰²â€ç­‰ï¼‰ã€‚æ›´å¤šç»“æœå’Œäº¤äº’æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles%E3%80%82">https://boyangdeng.com/visual-chroniclesã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08727v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles">https://boyangdeng.com/visual-chronicles</a>; second and   third listed authors have equal contributions</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ†æåŒ…å«æ•°äº¿å¼ åœ¨ä¸åŒæ—¶é—´æ•è·çš„å›¾åƒçš„å¤§å‹æ•°æ®åº“çš„ç³»ç»Ÿã€‚æ—¨åœ¨å‘ç°æ—¶é—´å˜åŒ–çš„æ¨¡å¼ï¼Œå°¤å…¶æ˜¯åŸå¸‚ç¯å¢ƒä¸­çš„é¢‘ç¹å‘ç”Ÿçš„è¶‹åŠ¿ã€‚è¯¥ç ”ç©¶ä½¿ç”¨ä¸€ç§è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ï¼Œå°†å¤§è§„æ¨¡è§†è§‰åˆ†æé—®é¢˜åˆ†è§£ä¸ºæ›´æ˜“äºå¤„ç†çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨MLLMsè§£å†³æ¯ä¸ªå­é—®é¢˜ã€‚å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å¤§å‹åŸå¸‚çš„å›¾åƒä¸­å‘ç°æœ‰è¶£çš„è¶‹åŠ¿ã€‚æœ‰å…³æ›´å¤šç»“æœå’Œäº¤äº’å¼æ¼”ç¤ºï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://boyangdeng.com/visual-chronicles">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç³»ç»Ÿæ¥æ£€æµ‹å¤§è§„æ¨¡æ•°æ®åº“ä¸­çš„å›¾åƒçš„æ—¶é—´å˜åŒ–æ¨¡å¼ã€‚</li>
<li>ç³»ç»Ÿçš„ç›®æ ‡æ˜¯å‘ç°åŸå¸‚ä¸­é¢‘ç¹å‡ºç°çš„è¶‹åŠ¿å˜åŒ–ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„è§†è§‰åˆ†æå·¥å…·ä¸åŒï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå›ç­”å¼€æ”¾å¼æŸ¥è¯¢ï¼Œæ— éœ€é¢„è®¾ç›®æ ‡ä¸»é¢˜æˆ–è®­ç»ƒæ ‡ç­¾ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¢«ç”¨ä½œæ­¤ç³»ç»Ÿçš„æ ¸å¿ƒå·¥å…·ï¼Œç”±äºå…¶å…·æœ‰å¼€æ”¾å¼çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚</li>
<li>é¢å¯¹å·¨å¤§çš„æ•°æ®é‡é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†è‡ªä¸‹è€Œä¸Šçš„ç­–ç•¥å°†å¤§å‹é—®é¢˜åˆ†è§£æˆè‹¥å¹²å°é—®é¢˜æ¥å¤„ç†ã€‚å¹¶ä¸”å¯¹æ¯ä¸ªå­é—®é¢˜ä½¿ç”¨ç‰¹å®šçš„MLLMè§£å†³æ–¹æ¡ˆè¿›è¡Œå¤„ç†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7395fea7fbd1496ef229c94e2274e3f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33bffbc52d7c22558153a631d8efdb95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47cb602006d1f58e3134e2d7b065cf09.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation"><a href="#DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation" class="headerlink" title="DocAgent: A Multi-Agent System for Automated Code Documentation   Generation"></a>DocAgent: A Multi-Agent System for Automated Code Documentation   Generation</h2><p><strong>Authors:Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang</strong></p>
<p>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories. </p>
<blockquote>
<p>é«˜è´¨é‡çš„ä»£ç æ–‡æ¡£å¯¹è½¯ä»¶å¼€å‘è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆä»£ç æ–‡æ¡£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚æˆ‘ä»¬å¼•å…¥äº†DocAgentï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé‡‡ç”¨æ‹“æ‰‘ä»£ç å¤„ç†æ¥è¿›è¡Œå¢é‡ä¸Šä¸‹æ–‡æ„å»ºã€‚ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼ˆé˜…è¯»è€…ã€æœç´¢è€…ã€ç¼–å†™è€…ã€éªŒè¯è€…ã€åè°ƒè€…ï¼‰ååŒç”Ÿæˆæ–‡æ¡£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æ–‡æ¡£çš„å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDocAgentæŒç»­ä¸”æ˜¾è‘—åœ°ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºçš„é‡è¦ä½œç”¨ã€‚DocAgentä¸ºå¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­çš„å¯é ä»£ç æ–‡æ¡£ç”Ÿæˆæä¾›äº†ç¨³å¥çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08725v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é«˜è´¨ä»£ç æ–‡æ¡£å¯¹è½¯ä»¶å¼€å‘è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨AIæ—¶ä»£ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¸¸äº§ç”Ÿä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚æˆ‘ä»¬æ¨å‡ºDocAgentï¼Œä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé‡‡ç”¨æ‹“æ‰‘ä»£ç å¤„ç†è¿›è¡Œå¢é‡ä¸Šä¸‹æ–‡æ„å»ºã€‚ä¸“ä¸šæ™ºèƒ½ä½“ï¼ˆé˜…è¯»å™¨ã€æœç´¢å™¨ã€ç¼–å†™å™¨ã€æ ¸æŸ¥å™¨ã€åè°ƒå™¨ï¼‰ååŒç”Ÿæˆæ–‡æ¡£ã€‚æˆ‘ä»¬è¿˜æå‡ºä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚ç»¼åˆå®éªŒæ˜¾ç¤ºï¼ŒDocAgentæŒç»­ä¸”æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºçš„å…³é”®ä½œç”¨ã€‚DocAgentä¸ºå¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­çš„å¯é ä»£ç æ–‡æ¡£ç”Ÿæˆæä¾›äº†ç¨³å¥çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é«˜è´¨ä»£ç æ–‡æ¡£åœ¨è½¯ä»¶å¼€å‘ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨AIæ—¶ä»£ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆä»£ç æ–‡æ¡£å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•å¸¸äº§ç”Ÿä¸å®Œæ•´çš„ã€æ— å¸®åŠ©çš„æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚</li>
<li>DocAgentæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé€šè¿‡æ‹“æ‰‘ä»£ç å¤„ç†è¿›è¡Œå¢é‡ä¸Šä¸‹æ–‡æ„å»ºæ¥ç”Ÿæˆæ–‡æ¡£ã€‚</li>
<li>DocAgentåŒ…æ‹¬äº”ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šé˜…è¯»å™¨ã€æœç´¢å™¨ã€ç¼–å†™å™¨ã€æ ¸æŸ¥å™¨å’Œåè°ƒå™¨ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¤šé¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°ç”Ÿæˆçš„æ–‡æ¡£åœ¨å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ç»¼åˆå®éªŒæ˜¾ç¤ºï¼ŒDocAgentåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>DocAgentçš„æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºåœ¨æ–‡æ¡£ç”Ÿæˆä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84dea2119fbe38fca6011cfa93c867fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fd0534526e9f43e1ed4683ac785090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933e5dcbc145e1cb1361f03356dd2b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28680bc575a8c0f69c2ab5c8868cc5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cae1fe7424320522f3e73e14023dd673.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Beyond-Black-Box-Predictions-Identifying-Marginal-Feature-Effects-in-Tabular-Transformer-Networks"><a href="#Beyond-Black-Box-Predictions-Identifying-Marginal-Feature-Effects-in-Tabular-Transformer-Networks" class="headerlink" title="Beyond Black-Box Predictions: Identifying Marginal Feature Effects in   Tabular Transformer Networks"></a>Beyond Black-Box Predictions: Identifying Marginal Feature Effects in   Tabular Transformer Networks</h2><p><strong>Authors:Anton Thielmann, Arik Reuter, Benjamin Saefken</strong></p>
<p>In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in these areas. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts. To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the modelâ€™s predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/OpenTabular/NAMpy">https://github.com/OpenTabular/NAMpy</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶é¢„æµ‹èƒ½åŠ›ã€‚é™¤äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒTransformeræ¶æ„åœ¨è§£å†³è¡¨æ ¼æ•°æ®é—®é¢˜å’ŒæŒ‘æˆ˜ä¸­è¡¨ç°å‡ºäº†é«˜æ•ˆæ€§ï¼Œå¹¶æŒ‘æˆ˜äº†è¿™äº›é¢†åŸŸä¸­ä¹‹å‰å ä¸»å¯¼åœ°ä½çš„åŸºäºæ¢¯åº¦çš„å†³ç­–æ ‘ã€‚ç„¶è€Œï¼Œè¿™ç§é¢„æµ‹èƒ½åŠ›æ˜¯ä»¥å¯è§£é‡Šæ€§ä¸ºä»£ä»·çš„ï¼šæ·±åº¦è¡¨æ ¼Transformerç½‘ç»œçš„é»‘ç›’æ€§è´¨å‡ ä¹å®Œå…¨ä¸§å¤±äº†è¾¹ç¼˜ç‰¹å¾æ•ˆåº”ã€‚ä½¿ç”¨ç»å…¸ç»Ÿè®¡å›å½’æ¨¡å‹çš„åŠ æ³•çº¦æŸçš„æ›¿ä»£æ¶æ„å¯ä»¥ä¿æŒå¯ç†è§£çš„è¾¹ç¼˜ç‰¹å¾æ•ˆåº”ï¼Œä½†åœ¨é¢„æµ‹èƒ½åŠ›æ–¹é¢é€šå¸¸ä¸å¦‚å…¶æ›´å¤æ‚çš„å¯¹åº”ç‰©ã€‚ä¸ºäº†å¼¥å¯è§£é‡Šæ€§ä¸æ€§èƒ½ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€‚åº”è¡¨æ ¼Transformerç½‘ç»œçš„æ”¹è¿›æ–¹æ³•ï¼Œæ—¨åœ¨è¯†åˆ«è¾¹ç¼˜ç‰¹å¾æ•ˆåº”ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºè¯æ˜å¯ä»¥å‡†ç¡®åœ°è¯†åˆ«è¾¹ç¼˜ç‰¹å¾æ•ˆåº”ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯æ˜ï¼Œè¯¥æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹è¿™äº›æ•ˆåº”ï¼Œå³ä½¿åœ¨å¤æ‚çš„ç‰¹å¾äº¤äº’ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†å±•ç¤ºæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†å…¶ä¸å‡ ç§å¯è§£é‡Šçš„é»‘ç›’æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°å®ƒåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå¯ä»¥åŒ¹é…é»‘ç›’çš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OpenTabular/NAMpy%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OpenTabular/NAMpyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08712v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯è½¬æ¢å™¨æ¶æ„åœ¨å¤„ç†è¡¨æ ¼æ•°æ®é—®é¢˜æ—¶æ•ˆç‡è¾ƒé«˜ï¼Œä½†ç‰ºç‰²äº†å¯è§£é‡Šæ€§ã€‚ä¸ºå…¼é¡¾å¯è§£é‡Šæ€§å’Œæ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§æ”¹è¿›å‹è¡¨æ ¼è½¬æ¢å™¨ç½‘ç»œï¼Œå¯å‡†ç¡®è¯†åˆ«è¾¹é™…ç‰¹å¾æ•ˆåº”ã€‚è¯¥æ¨¡å‹æ—¢ä¿æŒäº†æ·±åº¦ç½‘ç»œçš„é¢„æµ‹èƒ½åŠ›ï¼Œåˆå…·å¤‡ç»å…¸ç»Ÿè®¡å›å½’æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æ¨¡å‹çš„æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¡¨æ ¼æ•°æ®é—®é¢˜æ—¶ï¼Œè½¬æ¢å™¨æ¶æ„çš„æ•ˆç‡è¾ƒé«˜ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚è¡¨æ ¼è½¬æ¢å™¨ç½‘ç»œï¼‰åœ¨é¢„æµ‹æ—¶ç‰ºç‰²äº†å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥è¯†åˆ«è¾¹é™…ç‰¹å¾æ•ˆåº”ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ”¹è¿›å‹è¡¨æ ¼è½¬æ¢å™¨ç½‘ç»œï¼Œæ—¨åœ¨å‡†ç¡®è¯†åˆ«è¾¹é™…ç‰¹å¾æ•ˆåº”ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†æ·±åº¦ç½‘ç»œçš„é¢„æµ‹èƒ½åŠ›ä¸ç»å…¸ç»Ÿè®¡å›å½’æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç†è®ºéªŒè¯å’Œå®éªŒéªŒè¯ï¼Œèƒ½å¤Ÿå‡†ç¡®æ£€æµ‹è¾¹é™…ç‰¹å¾æ•ˆåº”ã€‚</li>
<li>ä¸å…¶ä»–å¯è§£é‡Šå’Œä¸å¯è§£é‡Šçš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6811a8a5e556aa8c356182c6c50870f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-482d3b9c5d9c3a88ff6d0d108c2004d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2b9552deea60814cba21c9e8325f84e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e6c8bb546f5beca9a11ee51948d357e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e4c62bba13ec4ace717d7ea74544d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SWE-PolyBench-A-multi-language-benchmark-for-repository-level-evaluation-of-coding-agents"><a href="#SWE-PolyBench-A-multi-language-benchmark-for-repository-level-evaluation-of-coding-agents" class="headerlink" title="SWE-PolyBench: A multi-language benchmark for repository level   evaluation of coding agents"></a>SWE-PolyBench: A multi-language benchmark for repository level   evaluation of coding agents</h2><p><strong>Authors:Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buccholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, Laurent Callot</strong></p>
<p>Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/SWE-PolyBench">https://github.com/amazon-science/SWE-PolyBench</a> </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç¼–ç ä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œç°å®åœºæ™¯ä¸­å¯¹å®ƒä»¬çš„æ€§èƒ½è¿›è¡Œè¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†SWE-PolyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹ç¼–ç ä»£ç†è¿›è¡Œä»“åº“çº§åˆ«çš„åŸºäºæ‰§è¡Œçš„è¯„ä»·ã€‚SWE-PolyBenchåŒ…å«æ¥è‡ª21ä¸ªä»“åº“çš„2110ä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬Javaï¼ˆ165ä¸ªï¼‰ã€JavaScriptï¼ˆ1017ä¸ªï¼‰ã€TypeScriptï¼ˆ729ä¸ªï¼‰å’ŒPythonï¼ˆ199ä¸ªï¼‰çš„ä»»åŠ¡ï¼Œæ¶µç›–é”™è¯¯ä¿®å¤ã€åŠŸèƒ½æ·»åŠ å’Œä»£ç é‡æ„ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä»»åŠ¡å’Œä»“åº“åˆ†å±‚å­æ ·æœ¬ï¼ˆSWE-PolyBench500ï¼‰ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªè¯„ä¼°å·¥å…·ï¼Œå¯ä»¥å®ç°å…¨è‡ªåŠ¨è¯„ä¼°ã€‚ä¸ºäº†æ›´å…¨é¢åœ°å¯¹ç¼–ç ä»£ç†è¿›è¡Œæ¯”è¾ƒï¼Œè¿™é¡¹å·¥ä½œè¿˜æå‡ºäº†åŸºäºè¯­æ³•æ ‘åˆ†æçš„æ–°æŒ‡æ ‡é›†ã€‚æˆ‘ä»¬åœ¨SWE-PolyBenchä¸Šè¯„ä¼°äº†é¢†å…ˆçš„å¼€æºç¼–ç ä»£ç†ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è¯­è¨€ã€ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦ç±»åˆ«æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰ä»£ç†åœ¨ä¸åŒè¯­è¨€ä¹‹é—´çš„æ€§èƒ½è¡¨ç°ä¸å‡è¡¡ï¼Œå¯¹å¤æ‚é—®é¢˜æ„Ÿåˆ°å›°éš¾ï¼Œè€Œåœ¨ç®€å•ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚SWE-PolyBenchæ—¨åœ¨æ¨åŠ¨å¼€å‘æ›´é€šç”¨å’Œæ›´ç¨³å¥çš„AIç¼–ç åŠ©ç†ï¼Œç”¨äºç°å®ä¸–ç•Œçš„è½¯ä»¶å·¥ç¨‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/amazon-science/SWE-PolyBench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/amazon-science/SWE-PolyBenchä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08703v1">PDF</a> 20 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç¼–ç ä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œç°å®åœºæ™¯ä¸­å¯¹å®ƒä»¬çš„æ€§èƒ½è¯„ä¼°ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SWE-PolyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹ç¼–ç ä»£ç†è¿›è¡Œå­˜å‚¨åº“çº§åˆ«çš„æ‰§è¡Œè¯„ä¼°ã€‚å®ƒåŒ…å«äº†æ¥è‡ª21ä¸ªå­˜å‚¨åº“çš„2110ä¸ªå®ä¾‹ï¼Œæ¶µç›–äº†Javaã€JavaScriptã€TypeScriptå’ŒPythonä¸­çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬é”™è¯¯ä¿®å¤ã€åŠŸèƒ½æ·»åŠ å’Œä»£ç é‡æ„ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªä»»åŠ¡åˆ†å±‚å­æ ·æœ¬ï¼ˆSWE-PolyBench500ï¼‰ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªè¯„ä¼°å·¥å…·ï¼Œå¯ä»¥å®ç°å®Œå…¨è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚ä¸ºäº†æ›´å…¨é¢åœ°æ¯”è¾ƒç¼–ç ä»£ç†ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäºè¯­æ³•æ ‘åˆ†æçš„æ–°æŒ‡æ ‡é›†ã€‚æˆ‘ä»¬åœ¨SWE-PolyBenchä¸Šè¯„ä¼°äº†é¢†å…ˆçš„å¼€æºç¼–ç ä»£ç†ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è¯­è¨€ã€ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦ç±»åˆ«æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç¼–ç ä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤šç§è¯­è¨€å’Œç°å®åœºæ™¯ä¸­çš„æ€§èƒ½è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>SWE-PolyBenchæ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç¼–ç ä»£ç†åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€ä¸­çš„æ€§èƒ½ã€‚</li>
<li>SWE-PolyBenchåŒ…å«æ¥è‡ªä¸åŒç¼–ç¨‹è¯­è¨€çš„å¤šç§ä»»åŠ¡ï¼Œå¦‚é”™è¯¯ä¿®å¤ã€åŠŸèƒ½æ·»åŠ å’Œä»£ç é‡æ„ã€‚</li>
<li>æ¨å‡ºäº†SWE-PolyBench500ä»»åŠ¡åˆ†å±‚å­æ ·æœ¬å’Œè‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·ã€‚</li>
<li>æ–°çš„è¯„ä¼°æŒ‡æ ‡é›†åŸºäºè¯­æ³•æ ‘åˆ†æï¼Œä»¥æ›´å…¨é¢åœ°æ¯”è¾ƒç¼–ç ä»£ç†ã€‚</li>
<li>å½“å‰ç¼–ç ä»£ç†åœ¨ä¸åŒè¯­è¨€å’Œä»»åŠ¡å¤æ‚åº¦ä¸Šçš„è¡¨ç°ä¸å‡è¡¡ï¼Œå¯¹å¤æ‚é—®é¢˜æœ‰æŒ‘æˆ˜ï¼Œå¯¹ç®€å•ä»»åŠ¡è¡¨ç°è¾ƒå¥½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2233545647eb40492497e0edf74ab1dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d3231822b18b5ffc11752e8d6afeae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-965a32ce7c5dd61107255472f6573e11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc5f22efe3e57492b7454d3afcf96a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c75445987ff4e3d61bad871380cc85f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TP-RAG-Benchmarking-Retrieval-Augmented-Large-Language-Model-Agents-for-Spatiotemporal-Aware-Travel-Planning"><a href="#TP-RAG-Benchmarking-Retrieval-Augmented-Large-Language-Model-Agents-for-Spatiotemporal-Aware-Travel-Planning" class="headerlink" title="TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for   Spatiotemporal-Aware Travel Planning"></a>TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for   Spatiotemporal-Aware Travel Planning</h2><p><strong>Authors:Hang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, Hao Liu</strong></p>
<p>Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMsâ€™ intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ—…è¡Œè§„åˆ’æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨å¤„ç†å¾®å¦™çš„æ—¶ç©ºç†æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚è™½ç„¶ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¾§é‡äºåŸºæœ¬çš„è®¡åˆ’æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬å¿½è§†äº†è¯¸å¦‚è·¯çº¿æ•ˆç‡ã€å…´è¶£ç‚¹å¸å¼•åŠ›å’Œå®æ—¶é€‚åº”æ€§ç­‰å…³é”®æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ£€ç´¢å¢å¼ºã€æ—¶ç©ºæ„ŸçŸ¥æ—…è¡Œè§„åˆ’é‡èº«å®šåˆ¶çš„TP-RAGåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«2348ä¸ªçœŸå®æ—…è¡ŒæŸ¥è¯¢ã€85575ä¸ªç²¾ç»†é¢—ç²’åº¦æ ‡æ³¨çš„å…´è¶£ç‚¹å’Œ18784ä¸ªé«˜è´¨é‡æ—…è¡Œè½¨è¿¹å‚è€ƒï¼Œè¿™äº›å‚è€ƒæºäºåœ¨çº¿æ—…æ¸¸æ–‡æ¡£ï¼Œå¯å®ç°åŠ¨æ€å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§„åˆ’ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°é›†æˆå‚è€ƒè½¨è¿¹å¯ä»¥æ˜¾è‘—æé«˜æ—…è¡Œè®¡åˆ’çš„ç©ºé—´æ•ˆç‡å’Œå…´è¶£ç‚¹çš„åˆç†æ€§ï¼Œè€Œç”±äºå­˜åœ¨å†²çªçš„å‚è€ƒå’Œå˜ˆæ‚çš„æ•°æ®ï¼Œåœ¨é€šç”¨æ€§å’Œç¨³å¥æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EvoRAGï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰åŠ›åœ°ååŒå¤šæ ·åŒ–çš„æ£€ç´¢è½¨è¿¹ä¸LLMçš„å†…åœ¨æ¨ç†ã€‚EvoRAGè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸è‡ªä¸‹è€Œä¸Šå’Œæ£€ç´¢å¢å¼ºçš„åŸºçº¿ç›¸æ¯”ï¼Œæé«˜äº†æ—¶ç©ºåˆè§„æ€§å¹¶å‡å°‘äº†å¸¸è¯†è¿è§„ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†æ··åˆWebçŸ¥è¯†ä¸LLMé©±åŠ¨ä¼˜åŒ–çš„æ½œåŠ›ï¼Œä¸ºæ›´å¯é å’Œè‡ªé€‚åº”çš„æ—…è¡Œè§„åˆ’ä»£ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08694v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—…è¡Œè§„åˆ’è‡ªåŠ¨åŒ–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨åº”å¯¹å¤æ‚æ—¶ç©ºç†æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨åŸºæœ¬è®¡åˆ’çš„æœ‰æ•ˆæ€§ï¼Œå¿½ç•¥äº†è·¯çº¿æ•ˆç‡ã€å…´è¶£ç‚¹å¸å¼•åŠ›å’Œå®æ—¶é€‚åº”æ€§ç­‰å…³é”®æ–¹é¢ã€‚æœ¬æ–‡å¼•å…¥TP-RAGï¼Œé¦–ä¸ªé’ˆå¯¹æ£€ç´¢å¢å¼ºã€æ—¶ç©ºæ„ŸçŸ¥æ—…è¡Œè§„åˆ’çš„åŸºå‡†æµ‹è¯•ã€‚æ•°æ®é›†åŒ…å«2348ä¸ªçœŸå®æ—…è¡ŒæŸ¥è¯¢ã€85575ä¸ªç²¾ç»†é¢—ç²’åº¦æ ‡æ³¨çš„å…´è¶£ç‚¹å’Œ18784æ¡é«˜è´¨é‡æ—…è¡Œè½¨è¿¹å‚è€ƒï¼Œå¯å®ç°åŠ¨æ€å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è§„åˆ’ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°é›†æˆå‚è€ƒè½¨è¿¹å¯æ˜¾è‘—æé«˜æ—…è¡Œè®¡åˆ’çš„ç©ºé—´æ•ˆç‡å’Œåˆç†æ€§ï¼Œè€Œç”±äºå†²çªå‚è€ƒå’Œå˜ˆæ‚æ•°æ®ï¼Œä»å­˜åœ¨æ™®éæ€§å’Œç¨³å¥æ€§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºEvoRAGï¼Œä¸€ä¸ªå¼ºå¤§çš„è¿›åŒ–æ¡†æ¶ï¼Œæœ‰æ•ˆååŒå„ç§æ£€ç´¢è½¨è¿¹ä¸LLMçš„å†…åœ¨æ¨ç†ã€‚EvoRAGè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œæé«˜æ—¶ç©ºåˆè§„æ€§ï¼Œå‡å°‘å¸¸è¯†è¿è§„ï¼Œç›¸è¾ƒäºåŸºäºåœ°é¢å’Œæ£€ç´¢å¢å¼ºçš„åŸºçº¿ã€‚æˆ‘ä»¬çš„å·¥ä½œçªæ˜¾äº†æ··åˆWebçŸ¥è¯†ä¸LLMé©±åŠ¨ä¼˜åŒ–çš„æ½œåŠ›ï¼Œä¸ºæ›´å¯é å’Œè‡ªé€‚åº”çš„æ—…è¡Œè§„åˆ’ä»£ç†é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMsåœ¨è‡ªåŠ¨åŒ–æ—…è¡Œè§„åˆ’æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»éœ€è§£å†³å¤æ‚æ—¶ç©ºç†æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨åŸºæœ¬è®¡åˆ’çš„æœ‰æ•ˆæ€§ï¼Œå¿½ç•¥äº†è·¯çº¿æ•ˆç‡ã€å…´è¶£ç‚¹å¸å¼•åŠ›ç­‰å…³é”®æ–¹é¢ã€‚</li>
<li>å¼•å…¥TP-RAGåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«çœŸå®æ—…è¡ŒæŸ¥è¯¢ã€ç²¾ç»†é¢—ç²’åº¦æ ‡æ³¨çš„å…´è¶£ç‚¹å’Œé«˜è´¨é‡æ—…è¡Œè½¨è¿¹å‚è€ƒã€‚</li>
<li>é›†æˆå‚è€ƒè½¨è¿¹å¯æé«˜æ—…è¡Œè®¡åˆ’çš„ç©ºé—´æ•ˆç‡å’Œåˆç†æ€§ã€‚</li>
<li>ä»å­˜åœ¨æ™®éæ€§å’Œç¨³å¥æ€§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå†²çªå‚è€ƒå’Œå˜ˆæ‚æ•°æ®ã€‚</li>
<li>æå‡ºEvoRAGæ¡†æ¶ï¼Œæœ‰æ•ˆååŒæ£€ç´¢è½¨è¿¹ä¸LLMçš„å†…åœ¨æ¨ç†ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bac2860dc03f9f129669bf226fe977d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e6f5d6f6556c07174a68b593fd81313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69a0f63c0dc5a1a801bb660d86f1ec48.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Quality-evaluation-of-Tabby-coding-assistant-using-real-source-code-snippets"><a href="#Quality-evaluation-of-Tabby-coding-assistant-using-real-source-code-snippets" class="headerlink" title="Quality evaluation of Tabby coding assistant using real source code   snippets"></a>Quality evaluation of Tabby coding assistant using real source code   snippets</h2><p><strong>Authors:Marta Borek, Robert Nowak</strong></p>
<p>Large language models have become a popular tool in software development, providing coding assistance. The proper measurement of the accuracy and reliability of the code produced by such tools is a challenge due to natural language prompts.   We propose a simple pipeline that uses state-of-the-art implementation of classic and universal genres of algorithms and data structures. We focus on measuring the quality of TabbyML code assistant due to its open licence and the flexibility in the choice of the language model.   Our results presented as cyclomatic complexity, Halsteadâ€™s Bugs &amp; Effort and four text-based similarity matrices depict the usability of TabbyML in coding assistance tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆä¸ºè½¯ä»¶å¼€å‘ä¸­çš„çƒ­é—¨å·¥å…·ï¼Œä¸ºç¼–ç æä¾›è¾…åŠ©ã€‚ç”±äºè‡ªç„¶è¯­è¨€æç¤ºï¼Œå‡†ç¡®è¡¡é‡æ­¤ç±»å·¥å…·äº§ç”Ÿçš„ä»£ç å‡†ç¡®æ€§å’Œå¯é æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„ç®¡é“ï¼Œè¯¥ç®¡é“ä½¿ç”¨ç»å…¸å’Œé€šç”¨ç®—æ³•å’Œæ•°æ®ç»“æ„çš„æœ€å…ˆè¿›å®ç°ã€‚æˆ‘ä»¬ä¸“æ³¨äºè¡¡é‡TabbyMLä»£ç åŠ©æ‰‹çš„å“è´¨ï¼Œå› å…¶å¼€æ”¾è®¸å¯è¯å’Œè¯­è¨€æ¨¡å‹é€‰æ‹©çš„çµæ´»æ€§ã€‚æˆ‘ä»¬çš„ç»“æœä»¥å¾ªç¯å¤æ‚åº¦ã€Halsteadçš„Bugï¼†Effortä»¥åŠå››ä¸ªåŸºäºæ–‡æœ¬çš„ç›¸ä¼¼çŸ©é˜µå‘ˆç°ï¼Œæç»˜äº†TabbyMLåœ¨ç¼–ç è¾…åŠ©ä»»åŠ¡ä¸­çš„å¯ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08650v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å¼€å‘ä¸­æä¾›ç¼–ç¨‹è¾…åŠ©ï¼Œå…¶ç”Ÿæˆçš„ä»£ç å‡†ç¡®æ€§å’Œå¯é æ€§è¯„ä¼°æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªç®€å•æµç¨‹ï¼Œå€ŸåŠ©æœ€æ–°ç®—æ³•å’Œæ•°æ®ç»“æ„å®ç°ç»å…¸å’Œé€šç”¨ç±»å‹ç®—æ³•ï¼Œé‡ç‚¹è¯„ä¼°TabbyMLä»£ç åŠ©ç†çš„è´¨é‡ã€‚é€šè¿‡å¾ªç¯å¤æ‚æ€§ã€Halsteadçš„Bugä¸Effortä»¥åŠå››ç§åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§çš„çŸ©é˜µå±•ç¤ºå…¶åœ¨ç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆä¸ºè½¯ä»¶å¼€å‘ä¸­çš„çƒ­é—¨å·¥å…·ï¼Œæä¾›ç¼–ç¨‹è¾…åŠ©ã€‚</li>
<li>è¯„ä¼°æ­¤ç±»å·¥å…·ç”Ÿæˆçš„ä»£ç å‡†ç¡®æ€§å’Œå¯é æ€§æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ä¸ªç®€å•æµç¨‹æ¥è¯„ä¼°TabbyMLä»£ç åŠ©ç†çš„è´¨é‡ã€‚</li>
<li>é‡ç‚¹ä½¿ç”¨å¾ªç¯å¤æ‚æ€§ã€Halsteadçš„Bugä¸Effortç­‰æ ‡å‡†æ¥åº¦é‡è´¨é‡ã€‚</li>
<li>é€šè¿‡å››ç§åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§çš„çŸ©é˜µæ¥å±•ç¤ºå…¶å®ç”¨æ€§ã€‚</li>
<li>TabbyMLå…·æœ‰å¼€æ”¾è®¸å¯è¯å’Œè¯­è¨€æ¨¡å‹é€‰æ‹©çš„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-168fa0e46a725c39f4eabacfee530ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f671e790ec6bfe0c55ddecf2567da63.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MooseAgent-A-LLM-Based-Multi-agent-Framework-for-Automating-Moose-Simulation"><a href="#MooseAgent-A-LLM-Based-Multi-agent-Framework-for-Automating-Moose-Simulation" class="headerlink" title="MooseAgent: A LLM Based Multi-agent Framework for Automating Moose   Simulation"></a>MooseAgent: A LLM Based Multi-agent Framework for Automating Moose   Simulation</h2><p><strong>Authors:Tao Zhang, Zhenhai Liu, Yong Xin, Yongjun Jiao</strong></p>
<p>The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at <a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent">https://github.com/taozhan18/MooseAgent</a> </p>
<blockquote>
<p>æœ‰é™å…ƒæ–¹æ³•ï¼ˆFEMï¼‰åœ¨å·¥ç¨‹å’Œç§‘å­¦è®¡ç®—ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶é¢„å¤„ç†ã€æ±‚è§£å™¨é…ç½®å’Œåå¤„ç†é˜¶æ®µé€šå¸¸è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬æ–‡é’ˆå¯¹å¤šç‰©ç†ä»¿çœŸæ¡†æ¶MOOSEï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¡†æ¶MooseAgentã€‚è¯¥æ¡†æ¶ç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚æ¡†æ¶ä½¿ç”¨LLMç†è§£ç”¨æˆ·æè¿°çš„è‡ªç„¶è¯­è¨€ä»¿çœŸè¦æ±‚ï¼Œå¹¶é‡‡ç”¨ä»»åŠ¡åˆ†è§£å’Œå¤šè½®è¿­ä»£éªŒè¯ç­–ç•¥è‡ªåŠ¨ç”ŸæˆMOOSEè¾“å…¥æ–‡ä»¶ã€‚ä¸ºäº†æé«˜ç²¾åº¦å¹¶å‡å°‘æ¨¡å‹å¹»è§‰ï¼Œç³»ç»Ÿæ„å»ºå¹¶åˆ©ç”¨ä¸€ä¸ªåŒ…å«æ³¨é‡Šçš„MOOSEè¾“å…¥å¡ç‰‡å’ŒåŠŸèƒ½æ–‡æ¡£çš„å‘é‡æ•°æ®åº“ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…¸å‹æ¡ˆä¾‹ä¸Šè¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ä¼ çƒ­ã€åŠ›å­¦ã€ç›¸åœºå’Œå¤šç‰©ç†è€¦åˆã€‚ç»“æœè¡¨æ˜ï¼ŒMooseAgentå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè‡ªåŠ¨åŒ–MOOSEä»¿çœŸè¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç›¸å¯¹ç®€å•çš„å•ç‰©ç†é—®é¢˜æ—¶è¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®æ˜¯æå‡ºäº†é’ˆå¯¹MOOSEçš„å¤šæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ¡†æ¶ï¼ŒéªŒè¯äº†å…¶åœ¨ç®€åŒ–æœ‰é™å…ƒä»¿çœŸè¿‡ç¨‹ã€é™ä½ç”¨æˆ·é—¨æ§›æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ™ºèƒ½æœ‰é™å…ƒä»¿çœŸè½¯ä»¶çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚æœ¬æ–‡æå‡ºçš„MooseAgentæ¡†æ¶çš„ä»£ç å·²ç»å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent">https://github.com/taozhan18/MooseAgent</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08621v1">PDF</a> 7 pages, 2 Figs</p>
<p><strong>æ‘˜è¦</strong><br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤šç‰©ç†ä»¿çœŸæ¡†æ¶MOOSEçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¡†æ¶â€”â€”MooseAgentã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºç†è§£ç”¨æˆ·æè¿°çš„è‡ªç„¶è¯­è¨€ä»¿çœŸè¦æ±‚å¹¶è‡ªåŠ¨ç”ŸæˆMOOSEè¾“å…¥æ–‡ä»¶ã€‚ç ”ç©¶é€šè¿‡å®éªŒè¯„ä¼°äº†å‡ ä¸ªå…¸å‹æ¡ˆä¾‹ï¼Œè¯æ˜äº†MooseAgentèƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šè‡ªåŠ¨åŒ–MOOSEä»¿çœŸè¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç›¸å¯¹ç®€å•çš„å•ç‰©ç†é—®é¢˜æ—¶è¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹MOOSEçš„å¤šæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œä¸ºç®€åŒ–æœ‰é™å…ƒä»¿çœŸè¿‡ç¨‹ã€é™ä½ç”¨æˆ·é—¨æ§›æä¾›äº†æ–°æ€è·¯ã€‚è¯¥è®ºæ–‡æå‡ºçš„MooseAgentæ¡†æ¶ä»£ç å·²å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/taozhan18/MooseAgentè·å–ã€‚</a></p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>MooseAgentæ˜¯ä¸€ä¸ªç»“åˆäº†å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¡†æ¶ï¼Œç”¨äºå¤šç‰©ç†ä»¿çœŸæ¡†æ¶MOOSEã€‚</li>
<li>å®ƒä½¿ç”¨è¯­è¨€æ¨¡å‹ç†è§£è‡ªç„¶è¯­è¨€æè¿°çš„ä»¿çœŸè¦æ±‚ï¼Œå¹¶è‡ªåŠ¨ç”ŸæˆMOOSEè¾“å…¥æ–‡ä»¶ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°ï¼ŒMooseAgentèƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šè‡ªåŠ¨åŒ–MOOSEä»¿çœŸè¿‡ç¨‹ã€‚</li>
<li>åœ¨å¤„ç†ç›¸å¯¹ç®€å•çš„å•ç‰©ç†é—®é¢˜æ—¶ï¼ŒMooseAgentè¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç®€åŒ–æœ‰é™å…ƒä»¿çœŸè¿‡ç¨‹ã€é™ä½ç”¨æˆ·é—¨æ§›æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
<li>MooseAgentæ¡†æ¶ä»£ç å·²å¼€æºï¼Œä¾¿äºè·å–å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7f785e2bbadd5f7b0a69d3b85ceb37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895778a4defe0d18c5accfc76dd27ee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0e80dadc6623fd0ae96114217ef5154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50a9a88f863c4a7203b020812f710cb7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UoB-NLP-at-SemEval-2025-Task-11-Leveraging-Adapters-for-Multilingual-and-Cross-Lingual-Emotion-Detection"><a href="#UoB-NLP-at-SemEval-2025-Task-11-Leveraging-Adapters-for-Multilingual-and-Cross-Lingual-Emotion-Detection" class="headerlink" title="UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual   and Cross-Lingual Emotion Detection"></a>UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual   and Cross-Lingual Emotion Detection</h2><p><strong>Authors:Frances Laureano De Leon, Yixiao Wang, Yue Feng, Mark G. Lee</strong></p>
<p>Emotion detection in natural language processing is a challenging task due to the complexity of human emotions and linguistic diversity. While significant progress has been made in high-resource languages, emotion detection in low-resource languages remains underexplored. In this work, we address multilingual and cross-lingual emotion detection by leveraging adapter-based fine-tuning with multilingual pre-trained language models. Adapters introduce a small number of trainable parameters while keeping the pre-trained model weights fixed, offering a parameter-efficient approach to adaptation. We experiment with different adapter tuning strategies, including task-only adapters, target-language-ready task adapters, and language-family-based adapters. Our results show that target-language-ready task adapters achieve the best overall performance, particularly for low-resource African languages with our team ranking 7th for Tigrinya, and 8th for Kinyarwanda in Track A. In Track C, our system ranked 3rd for Amharic, and 4th for Oromo, Tigrinya, Kinyarwanda, Hausa, and Igbo. Our approach outperforms large language models in 11 languages and matches their performance in four others, despite our models having significantly fewer parameters. Furthermore, we find that adapter-based models retain cross-linguistic transfer capabilities while requiring fewer computational resources compared to full fine-tuning for each language. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æƒ…æ„Ÿæ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºäººç±»æƒ…æ„Ÿçš„å¤æ‚æ€§å’Œè¯­è¨€å¤šæ ·æ€§ã€‚è™½ç„¶åœ¨é«˜èµ„æºè¯­è¨€æ–¹é¢å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„æƒ…æ„Ÿæ£€æµ‹ä»ç„¶è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºé€‚é…å™¨çš„å¾®è°ƒä¸å¤šè¯­è¨€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥è§£å†³å¤šè¯­è¨€å’Œè·¨è¯­è¨€æƒ…æ„Ÿæ£€æµ‹é—®é¢˜ã€‚é€‚é…å™¨å¼•å…¥äº†å°‘é‡çš„å¯è®­ç»ƒå‚æ•°ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹æƒé‡å›ºå®šï¼Œä¸ºé€‚åº”æ€§æä¾›äº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°è¯•äº†ä¸åŒçš„é€‚é…å™¨è°ƒæ•´ç­–ç•¥ï¼ŒåŒ…æ‹¬ä»…ä»»åŠ¡é€‚é…å™¨ã€é¢å‘ç›®æ ‡è¯­è¨€çš„ä»»åŠ¡é€‚é…å™¨å’ŒåŸºäºè¯­è¨€å®¶æ—çš„é€‚é…å™¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé¢å‘ç›®æ ‡è¯­è¨€çš„ä»»åŠ¡é€‚é…å™¨å–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºçš„éæ´²è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿåœ¨Aèµ›é“ä¸Šåˆ†åˆ«ä»¥ç¬¬7åå’Œç¬¬8åçš„æˆç»©å®Œæˆäº†ææ ¼é‡Œå°¼äºšè¯­å’ŒåŸºå°¼äºšå¢æ—ºè¾¾è¯­çš„ä»»åŠ¡ã€‚åœ¨Cèµ›é“ä¸­ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨é˜¿å§†å“ˆæ‹‰è¯­ä¸­æ’åç¬¬3ï¼Œåœ¨å¥¥ç½—è«è¯­ã€ææ ¼é‡Œå°¼äºšè¯­ã€åŸºå°¼äºšå¢æ—ºè¾¾è¯­ã€è±ªè¨è¯­å’Œä¼Šåšè¯­ä¸­æ’åç¬¬4ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¸å¤šè¯­è¨€ä¸Šçš„è¡¨ç°ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä»–å››ç§è¯­è¨€ä¸­ä¸ä¹‹åŒ¹é…è¡¨ç°æ°´å¹³ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°æ˜æ˜¾æ›´å°‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åŸºäºé€‚é…å™¨çš„æ¨¡å‹æ—¢ä¿ç•™äº†è·¨è¯­è¨€çš„è¿ç§»èƒ½åŠ›ï¼Œåˆç›¸å¯¹äºæ¯ç§è¯­è¨€çš„å®Œå…¨å¾®è°ƒæ–¹æ³•å‡å°‘äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08543v1">PDF</a> Accepted to appear in Proceedings of the 19th International Workshop   on Semantic Evaluation (SemEval-2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æƒ…æ„Ÿæ£€æµ‹ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åˆ©ç”¨åŸºäºé€‚é…å™¨çš„å¾®è°ƒä¸å¤šè¯­è¨€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥è§£å†³å¤šè¯­è¨€å’Œè·¨è¯­è¨€æƒ…æ„Ÿæ£€æµ‹é—®é¢˜ã€‚é€‚é…å™¨å¼•å…¥å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹æƒé‡ä¸å˜ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„å‚æ•°åŒ–é€‚åº”æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹ç‰¹å®šè¯­è¨€çš„é€‚é…å™¨åœ¨éæ´²ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°æœ€ä½³ï¼Œå…¶ä¸­æŸäº›ç³»ç»Ÿåœ¨ç‰¹å®šèµ›é“ä¸­ååˆ—å‰èŒ…ã€‚æ­¤æ–¹æ³•åœ¨å¤šè¯­è¨€ä¸­è¡¨ç°ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¿ç•™è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿæ£€æµ‹æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡é€‚é…å™¨å¾®è°ƒä¸å¤šè¯­è¨€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>é€‚é…å™¨æ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°åŒ–é€‚åº”æ–¹æ³•ï¼Œå¯ä»¥å¼•å…¥å°‘é‡å¯è®­ç»ƒå‚æ•°åŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹æƒé‡ä¸å˜ã€‚</li>
<li>é’ˆå¯¹ç‰¹å®šè¯­è¨€çš„é€‚é…å™¨åœ¨éæ´²ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç ”ç©¶ç³»ç»Ÿåœ¨ç‰¹å®šèµ›é“ä¸­ååˆ—å‰èŒ…ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šè¯­è¨€ä¸­è¡¨ç°ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¿ç•™è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-704d2ea873626670d1be38460e630025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112e5f9ab364118b21342bf0db6f78ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66fcba804635cbdd9ed0d047ffd3ee05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ba62f5242b739af216c04e3eb9f7f49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d610f696ccb5a61f038afcb1b8f77cf1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Embodied-Image-Captioning-Self-supervised-Learning-Agents-for-Spatially-Coherent-Image-Descriptions"><a href="#Embodied-Image-Captioning-Self-supervised-Learning-Agents-for-Spatially-Coherent-Image-Descriptions" class="headerlink" title="Embodied Image Captioning: Self-supervised Learning Agents for Spatially   Coherent Image Descriptions"></a>Embodied Image Captioning: Self-supervised Learning Agents for Spatially   Coherent Image Descriptions</h2><p><strong>Authors:Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo Natale</strong></p>
<p>We present a self-supervised method to improve an agentâ€™s abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œç”¨äºæé«˜æ™ºèƒ½ä½“åœ¨ä¸»åŠ¨æ¢ç´¢é€šç”¨ç¯å¢ƒæ—¶æè¿°ä»»æ„ç‰©ä½“çš„èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºå½“å‰æ¨¡å‹ç”±äºä¸åŒç›¸æœºè§†è§’å’Œæ··ä¹±çš„åœºæ™¯ï¼Œéš¾ä»¥è·å¾—è¿è´¯çš„å›¾åƒæè¿°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶æ¥å¾®è°ƒç°æœ‰çš„æè¿°æ¨¡å‹ï¼Œé€šè¿‡å…±è¯†æœºåˆ¶æé«˜è·¨ä¸åŒè§†è§’çš„æè¿°å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œæ™ºèƒ½ä½“æ¢ç´¢ç¯å¢ƒï¼Œæ”¶é›†å¸¦æœ‰å™ªå£°çš„å›¾åƒæè¿°é…å¯¹ã€‚ç„¶åï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å…±è¯†ä¸ºæ¯ä¸ªå¯¹è±¡å®ä¾‹æç‚¼å‡ºä¸€è‡´çš„ä¼ªæè¿°ã€‚æœ€åï¼Œè¿™äº›ä¼ªæè¿°è¢«ç”¨äºå¾®è°ƒç°æˆçš„æè¿°æ¨¡å‹ï¼Œå¹¶æ·»åŠ å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨æ‰‹åŠ¨æ ‡è®°çš„æµ‹è¯•é›†ä¸Šåˆ†æäº†ç»“åˆæè¿°æ¨¡å‹ã€æ¢ç´¢ç­–ç•¥ã€ä¼ªæ ‡ç­¾æ–¹æ³•å’Œå¾®è°ƒç­–ç•¥çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå¯ä»¥è®­ç»ƒä¸€ç§ç­–ç•¥æ¥æŒ–æ˜ä¸ä¼ ç»ŸåŸºçº¿ç›¸æ¯”å…·æœ‰æ›´é«˜åˆ†æ­§çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„ä¼ªæè¿°æ–¹æ³•ç»“åˆäº†æ‰€æœ‰ç­–ç•¥åï¼Œä¸ç°æœ‰å…¶ä»–æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”å¾®è°ƒèƒ½å¤Ÿæ˜¾è‘—æé«˜æè¿°çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ä»£ç å’Œæµ‹è¯•é›†æ³¨é‡Šå¯åœ¨ <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08531v1">PDF</a> 11 pages, 8 figures, 5 tables, code and test set annotations   available at <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a></p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡ä¸‰é˜¶æ®µæ¡†æ¶ä¼˜åŒ–æ™ºèƒ½ä½“æè¿°ä»»æ„å¯¹è±¡çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸»åŠ¨æ¢ç´¢é€šç”¨ç¯å¢ƒã€‚é¦–å…ˆæ”¶é›†å™ªå£°å›¾åƒ-å­—å¹•å¯¹ï¼Œç„¶åé€šè¿‡å…±è¯†æœºåˆ¶è’¸é¦å‡ºæ¯ä¸ªå¯¹è±¡å®ä¾‹çš„ä¸€è‡´ä¼ªå­—å¹•ï¼Œæœ€åä½¿ç”¨å¯¹æ¯”å­¦ä¹ å¯¹ç°æˆçš„å­—å¹•æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å­—å¹•çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºä¸€ç§è‡ªç›‘ç£æ–¹æ³•ç”¨äºæ™ºèƒ½ä½“æè¿°å¯¹è±¡èƒ½åŠ›ä¼˜åŒ–ï¼Œåœ¨é€šç”¨ç¯å¢ƒä¸­ä¸»åŠ¨æ¢ç´¢ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶å¯¹ç°æœ‰çš„å­—å¹•æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜å­—å¹•å‡†ç¡®æ€§å’Œè·¨è§†å›¾çš„ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡å…±è¯†æœºåˆ¶æ”¶é›†å™ªå£°å›¾åƒ-å­—å¹•å¯¹ï¼Œå¹¶è’¸é¦å‡ºå¯¹è±¡å®ä¾‹çš„ä¸€è‡´ä¼ªå­—å¹•ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç”¨äºå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè®­ç»ƒç­–ç•¥å¯ä»¥æŒ–æ˜æ ·æœ¬ä¸­çš„é«˜åˆ†æ­§ç‚¹ï¼Œä¸ä¼ ç»ŸåŸºçº¿ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ä¼ªå­—å¹•æ–¹æ³•ä¸æ‰€æœ‰ç­–ç•¥ç›¸ç»“åˆï¼Œåœ¨è¯­ä¹‰ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-32ee73d10a6c67b6e03a51af34e767c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8a5136d3b534254bce7a55f0c6c267a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dca6b6e4a0b845d2ff18f67c0b643262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291bfe9746c2573272726ddc95826c02.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Task-Memory-Engine-TME-Enhancing-State-Awareness-for-Multi-Step-LLM-Agent-Tasks"><a href="#Task-Memory-Engine-TME-Enhancing-State-Awareness-for-Multi-Step-LLM-Agent-Tasks" class="headerlink" title="Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM   Agent Tasks"></a>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM   Agent Tasks</h2><p><strong>Authors:Ye Ye</strong></p>
<p>Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œå¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ¡†æ¶æ— æ³•ç»´æŒå¯¹ä»»åŠ¡çŠ¶æ€çš„ç»“æ„åŒ–ç†è§£ï¼Œé€šå¸¸ä¾èµ–äºçº¿æ€§æç¤ºä¸²è”æˆ–æµ…å±‚å†…å­˜ç¼“å†²åŒºã€‚è¿™å¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€ç»å¸¸äº§ç”Ÿå¹»è§‰å’Œé•¿æœŸè¿è´¯æ€§å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€ç»“æ„åŒ–çš„å†…å­˜æ¨¡å—ï¼Œä½¿ç”¨åˆ†å±‚ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰æ¥è·Ÿè¸ªä»»åŠ¡æ‰§è¡Œæƒ…å†µã€‚æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”äºä¸€ä¸ªä»»åŠ¡æ­¥éª¤ï¼Œå­˜å‚¨ç›¸å…³çš„è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æç¤ºåˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæ´»åŠ¨èŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºï¼Œæ˜¾è‘—æé«˜äº†æ‰§è¡Œä¸€è‡´æ€§ä¸Šä¸‹æ–‡å®šä½ã€‚é€šè¿‡å¯¹å¤šæ­¥éª¤ä»£ç†ä»»åŠ¡çš„æ¡ˆä¾‹ç ”ç©¶å’Œæ¯”è¾ƒå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TMEåœ¨æé«˜ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œè§£é‡Šæ€§è¡Œä¸ºæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸”å®ç°å¼€é”€æå°ã€‚TMEçš„å®Œæ•´å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/biubiotomato/TME-Agent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/biubiotomato/TME-Agentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08525v1">PDF</a> 14 pages, 5 figures. Preprint prepared for future submission.   Includes implementation and token-efficiency analysis. Code at   <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a></p>
<p><strong>Summary</strong></p>
<p>LLMä½œä¸ºå¤šæ­¥éª¤ä»»åŠ¡è‡ªä¸»ä»£ç†çš„ä½¿ç”¨æ—¥ç›Šæ™®éï¼Œä½†ç°æœ‰æ¡†æ¶éš¾ä»¥ç»´æŒä»»åŠ¡çŠ¶æ€çš„ç»“æ„åŒ–ç†è§£ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€é¢‘ç¹å‡ºç°å¹»è§‰å’Œé•¿ç¨‹è¿è´¯æ€§å·®ã€‚æœ¬æ–‡æå‡ºTask Memory Engineï¼ˆTMEï¼‰ï¼Œä¸€ä¸ªè½»é‡çº§ç»“æ„åŒ–å†…å­˜æ¨¡å—ï¼Œä½¿ç”¨åˆ†å±‚ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰è¿½è¸ªä»»åŠ¡æ‰§è¡Œã€‚æ ‘ä¸­æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”ä»»åŠ¡æ­¥éª¤ï¼Œå­˜å‚¨ç›¸å…³è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚å¼•å…¥åŸºäºæ´»åŠ¨èŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºçš„æç¤ºåˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜æ‰§è¡Œä¸€è‡´æ€§åŠä¸Šä¸‹æ–‡å®šä½èƒ½åŠ›ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ­¥éª¤ä»£ç†ä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œè¯æ˜TMEèƒ½æé«˜ä»»åŠ¡å®Œæˆå‡†ç¡®ç‡å¹¶å¢å¼ºè¡Œä¸ºå¯è§£é‡Šæ€§ï¼Œä¸”å®ç°å¼€é”€æå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­ä½œä¸ºè‡ªä¸»ä»£ç†çš„åº”ç”¨é€æ¸å¢åŠ ï¼Œä½†å­˜åœ¨å¯¹ä»»åŠ¡çŠ¶æ€ç†è§£ä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°æ¡†æ¶æ— æ³•ç»´æŒä»»åŠ¡çŠ¶æ€çš„ç»“æ„åŒ–ç†è§£ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šå’Œé•¿ç¨‹è¿è´¯æ€§å·®ã€‚</li>
<li>Task Memory Engineï¼ˆTMEï¼‰æ˜¯ä¸€ä¸ªè½»é‡çº§ç»“æ„åŒ–å†…å­˜æ¨¡å—ï¼Œç”¨äºè¿½è¸ªä»»åŠ¡æ‰§è¡Œï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TMEä½¿ç”¨åˆ†å±‚ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰æ¥ç»„ç»‡ä»»åŠ¡ä¿¡æ¯ï¼Œæ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨ç›¸å…³è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚</li>
<li>TMEå¼•å…¥æç¤ºåˆæˆæ–¹æ³•ï¼Œæ ¹æ®æ´»åŠ¨èŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºï¼Œæé«˜æ‰§è¡Œä¸€è‡´æ€§åŠä¸Šä¸‹æ–‡å®šä½èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒéªŒè¯ï¼ŒTMEèƒ½æ˜¾è‘—æé«˜ä»»åŠ¡å®Œæˆå‡†ç¡®ç‡å¹¶å¢å¼ºè¡Œä¸ºå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acec3be0434fd047eae93d6b8bec6ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56cfc4f925f501a97f8aa2f5539a97f4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VLMT-Vision-Language-Multimodal-Transformer-for-Multimodal-Multi-hop-Question-Answering"><a href="#VLMT-Vision-Language-Multimodal-Transformer-for-Multimodal-Multi-hop-Question-Answering" class="headerlink" title="VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop   Question Answering"></a>VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop   Question Answering</h2><p><strong>Authors:Qi Zhi Lim, Chin Poo Lee, Kian Ming Lim, Kalaiarasi Sonai Muthu Anbananthen</strong></p>
<p>The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) often suffer from limited reasoning capabilities, reliance on modality conversion, and inadequate alignment between visual and textual representations. To address these limitations, this paper introduces Vision-Language Multimodal Transformer (VLMT), a unified architecture that integrates a transformer-based vision encoder with a sequence-to-sequence language model. VLMT employs a direct token-level injection mechanism to fuse visual and textual inputs within a shared embedding space, eliminating the need for intermediate projection layers. To enhance cross-modal alignment and reasoning, a three-stage pretraining strategy is proposed to progressively align vision-language representations and improve the modelâ€™s capacity for multimodal understanding. Based on the pretrained backbone, two task-specific modules are instantiated to form a two-stage MMQA framework: a multimodal reranker that predicts document relevance scores and utilizes a relative threshold with top-k strategy for context retrieval, and a multimodal question answering model that generates contextually grounded answers based on the retrieved evidence. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed approach. On MultimodalQA validation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1, outperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8% in F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as PERQA by +3.2. These results highlight VLMTâ€™s strong capabilities in multimodal reasoning and its potential to advance real-world information retrieval and question answering systems. </p>
<blockquote>
<p>éšç€æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒä¸­å¤šæ¨¡æ€æ•°æ®çš„æ—¥ç›Šæ™®åŠï¼Œä¸ºå¼€å‘èƒ½å¤Ÿå¤„ç†å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„æ¨¡å‹å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤šè·³é—®ç­”ï¼ˆMMQAï¼‰æ–¹æ³•å¾€å¾€å­˜åœ¨æ¨ç†èƒ½åŠ›æœ‰é™ã€ä¾èµ–æ¨¡æ€è½¬æ¢ä»¥åŠè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´å¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†è§†è§‰è¯­è¨€å¤šæ¨¡æ€è½¬æ¢å™¨ï¼ˆVLMTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¶æ„ï¼Œå®ƒå°†åŸºäºè½¬æ¢å™¨çš„è§†è§‰ç¼–ç å™¨ä¸åºåˆ—åˆ°åºåˆ—çš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº†é›†æˆã€‚VLMTé‡‡ç”¨ç›´æ¥çš„tokençº§æ³¨å…¥æœºåˆ¶ï¼Œåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­èåˆè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œä»è€Œæ— éœ€ä¸­é—´æŠ•å½±å±‚ã€‚ä¸ºäº†æé«˜è·¨æ¨¡æ€å¯¹é½å’Œæ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥é€æ­¥å¯¹é½è§†è§‰è¯­è¨€è¡¨ç¤ºå¹¶å¢å¼ºæ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚åŸºäºé¢„è®­ç»ƒçš„éª¨å¹²ç½‘ï¼Œå®ä¾‹åŒ–ä¸¤ä¸ªç‰¹å®šä»»åŠ¡çš„æ¨¡å—ï¼Œå½¢æˆä¸¤é˜¶æ®µMMQAæ¡†æ¶ï¼šä¸€ä¸ªå¤šæ¨¡æ€æ’åºå™¨ï¼Œç”¨äºé¢„æµ‹æ–‡æ¡£ç›¸å…³æ€§åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨ç›¸å¯¹é˜ˆå€¼ä¸top-kç­–ç•¥è¿›è¡Œä¸Šä¸‹æ–‡æ£€ç´¢ï¼›ä¸€ä¸ªå¤šæ¨¡æ€é—®ç­”æ¨¡å‹ï¼Œæ ¹æ®æ£€ç´¢åˆ°çš„è¯æ®ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„ç­”æ¡ˆã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨MultimodalQAéªŒè¯é›†ä¸Šï¼ŒVLMT-Largeè¾¾åˆ°76.5%çš„ç²¾ç¡®åŒ¹é…ç‡å’Œ80.1%çš„F1å¾—åˆ†ï¼Œåœ¨ç²¾ç¡®åŒ¹é…å’ŒF1å¾—åˆ†ä¸Šåˆ†åˆ«ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•+9.1%å’Œ+8.8%ã€‚åœ¨WebQAä¸Šï¼Œå®ƒè¾¾åˆ°äº†47.6çš„QAå¾—åˆ†ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ï¼Œå¦‚PERQA+3.2ã€‚è¿™äº›ç»“æœçªæ˜¾äº†VLMTåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œä»¥åŠå…¶æ¨åŠ¨ç°å®ä¸–ç•Œä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08269v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¢å‘æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒçš„å¤šæ¨¡æ€æ•°æ®æ—¥ç›Šä¸°å¯Œï¼Œä¸ºå‘å±•èƒ½å¤Ÿå¤„ç†å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„æ¨¡å‹å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ç°æœ‰è·¨æ¨¡æ€å¤šè·³é—®ç­”ï¼ˆMMQAï¼‰æ–¹æ³•å­˜åœ¨æ¨ç†èƒ½åŠ›æœ‰é™ã€ä¾èµ–æ¨¡æ€è½¬æ¢ä»¥åŠè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºå¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºè§†è§‰è¯­è¨€å¤šæ¨¡æ€è½¬æ¢å™¨ï¼ˆVLMTï¼‰çš„ç»Ÿä¸€æ¶æ„ï¼Œè¯¥æ¶æ„èåˆäº†åŸºäºè½¬æ¢å™¨çš„è§†è§‰ç¼–ç å™¨å’Œåºåˆ—åˆ°åºåˆ—çš„è¯­è¨€æ¨¡å‹ã€‚VLMTé‡‡ç”¨ç›´æ¥çš„ä»¤ç‰Œçº§æ³¨å…¥æœºåˆ¶ï¼Œåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­èåˆè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œæ— éœ€ä¸­é—´æŠ•å½±å±‚ã€‚ä¸ºå¢å¼ºè·¨æ¨¡æ€å¯¹é½å’Œæ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å¯¹é½è§†è§‰è¯­è¨€è¡¨ç¤ºå¹¶å¢å¼ºæ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚åŸºäºé¢„è®­ç»ƒçš„ä¸»å¹²ï¼Œå®ä¾‹åŒ–ä¸¤ä¸ªä»»åŠ¡ç‰¹å®šæ¨¡å—ï¼Œå½¢æˆä¸¤é˜¶æ®µMMQAæ¡†æ¶ï¼šå¤šæ¨¡æ€æ’åå™¨ç”¨äºé¢„æµ‹æ–‡æ¡£ç›¸å…³æ€§åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨ç›¸å¯¹é˜ˆå€¼ä¸top-kç­–ç•¥è¿›è¡Œä¸Šä¸‹æ–‡æ£€ç´¢ï¼›å¤šæ¨¡æ€é—®ç­”æ¨¡å‹æ ¹æ®æ£€ç´¢åˆ°çš„è¯æ®ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„ç­”æ¡ˆã€‚åœ¨MultimodalQAéªŒè¯é›†ä¸Šï¼ŒVLMT-Largeè¾¾åˆ°76.5%çš„ç²¾ç¡®åŒ¹é…ç‡å’Œ80.1%çš„F1å¾—åˆ†ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€ä¼˜æ¨¡å‹åœ¨ç²¾ç¡®åŒ¹é…ç‡å’ŒF1å¾—åˆ†ä¸Šåˆ†åˆ«æé«˜äº†9.1%å’Œ8.8%ã€‚åœ¨WebQAä¸Šï¼Œå…¶é—®ç­”å¾—åˆ†è¾¾åˆ°47.6ï¼Œè¶…è¶Šäº†PERQAç­‰å…ˆå‰æ¨¡å‹ã€‚è¿™äº›ç»“æœçªæ˜¾äº†VLMTåœ¨è·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›åŠå…¶å¯¹ç°å®ä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿæ¨è¿›çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ•°æ®çš„æ—¥ç›Šä¸°å¯Œå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œéœ€è¦å‘å±•å…·æœ‰å¤æ‚è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚</li>
<li>ç°æœ‰MMQAæ–¹æ³•å­˜åœ¨æ¨ç†èƒ½åŠ›æœ‰é™ã€ä¾èµ–æ¨¡æ€è½¬æ¢å’Œå¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>VLMTé€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬è¾“å…¥åœ¨ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´å†…ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>VLMTé‡‡ç”¨ç›´æ¥ä»¤ç‰Œçº§æ³¨å…¥æœºåˆ¶ï¼Œé¿å…äº†ä¸­é—´æŠ•å½±å±‚çš„éœ€è¦ã€‚</li>
<li>ä¸‰é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ç”¨äºå¢å¼ºè·¨æ¨¡æ€å¯¹é½å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>VLMTåœ¨MultimodalQAå’ŒWebQAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d815530ed34c5c95cf42ef50406f636d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5609be8d8263b710b6306e147d9e6246.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c321d75012e05bebdbbe529cce56ad32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04555a5a93bb693cd21e978965e0238b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Metamorphic-Testing-for-Fairness-Evaluation-in-Large-Language-Models-Identifying-Intersectional-Bias-in-LLaMA-and-GPT"><a href="#Metamorphic-Testing-for-Fairness-Evaluation-in-Large-Language-Models-Identifying-Intersectional-Bias-in-LLaMA-and-GPT" class="headerlink" title="Metamorphic Testing for Fairness Evaluation in Large Language Models:   Identifying Intersectional Bias in LLaMA and GPT"></a>Metamorphic Testing for Fairness Evaluation in Large Language Models:   Identifying Intersectional Bias in LLaMA and GPT</h2><p><strong>Authors:Harishwar Reddy, Madhusudan Srinivasan, Upulee Kanewala</strong></p>
<p>Large Language Models (LLMs) have made significant strides in Natural Language Processing but remain vulnerable to fairness-related issues, often reflecting biases inherent in their training data. These biases pose risks, particularly when LLMs are deployed in sensitive areas such as healthcare, finance, and law. This paper introduces a metamorphic testing approach to systematically identify fairness bugs in LLMs. We define and apply a set of fairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT model, a state-of-the-art LLM, across diverse demographic inputs. Our methodology includes generating source and follow-up test cases for each MR and analyzing model responses for fairness violations. The results demonstrate the effectiveness of MT in exposing bias patterns, especially in relation to tone and sentiment, and highlight specific intersections of sensitive attributes that frequently reveal fairness faults. This research improves fairness testing in LLMs, providing a structured approach to detect and mitigate biases and improve model robustness in fairness-sensitive applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»æ˜“å—åˆ°å…¬å¹³æ€§é—®é¢˜çš„å½±å“ï¼Œè¿™äº›é—®é¢˜é€šå¸¸åæ˜ äº†å…¶è®­ç»ƒæ•°æ®ä¸­çš„å›ºæœ‰åè§ã€‚å½“å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰æ•æ„Ÿé¢†åŸŸæ—¶ï¼Œè¿™äº›åè§å¸¦æ¥çš„é£é™©å°¤ä¸ºçªå‡ºã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å˜å¼‚æµ‹è¯•æ–¹æ³•ï¼Œä»¥ç³»ç»Ÿåœ°è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å…¬å¹³æ¼æ´ã€‚æˆ‘ä»¬å®šä¹‰å¹¶åº”ç”¨äº†ä¸€å¥—é¢å‘å…¬å¹³çš„å˜å¼‚å…³ç³»ï¼ˆMRsï¼‰æ¥è¯„ä¼°æœ€å…ˆè¿›çš„LLaMAå’ŒGPTæ¨¡å‹ï¼Œè·¨è¶Šä¸åŒçš„äººå£ç»Ÿè®¡è¾“å…¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸ºæ¯ä¸ªMRç”Ÿæˆæºæµ‹è¯•ç”¨ä¾‹å’Œåç»­æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶åˆ†ææ¨¡å‹å¯¹å…¬å¹³è¿è§„çš„å“åº”ã€‚ç»“æœè¡¨æ˜ï¼Œå˜å¼‚æµ‹è¯•åœ¨æ­ç¤ºåè§æ¨¡å¼æ–¹é¢å°¤å…¶æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è°ƒå’Œæƒ…æ„Ÿæ–¹é¢ï¼Œå¹¶å¼ºè°ƒäº†æ•æ„Ÿå±æ€§ç‰¹å®šçš„äº¤é›†ï¼Œè¿™äº›å±æ€§ç»å¸¸æ­ç¤ºå…¬å¹³æ•…éšœã€‚æœ¬ç ”ç©¶æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å…¬å¹³æµ‹è¯•æ°´å¹³ï¼Œæä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•æ¥æ£€æµ‹å’Œç¼“è§£åè§ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨æ•æ„Ÿå…¬å¹³æ€§åº”ç”¨çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07982v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨å…¬å¹³æ€§ç›¸å…³é—®é¢˜ï¼Œåæ˜ å…¶è®­ç»ƒæ•°æ®ä¸­çš„å›ºæœ‰åè§ã€‚æœ¬æ–‡æå‡ºä¸€ç§å…ƒæµ‹è¯•æ–¹æ³•ï¼Œé€šè¿‡å®šä¹‰å’Œåº”ç”¨é¢å‘å…¬å¹³æ€§çš„å…ƒå½¢æ€å…³ç³»ï¼Œç³»ç»Ÿåœ°è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å…¬å¹³æ€§é—®é¢˜æ¼æ´ã€‚æµ‹è¯•å¯¹è±¡ä¸ºå½“å‰å…ˆè¿›çš„LLaMAå’ŒGPTæ¨¡å‹ï¼Œé€šè¿‡ä¸åŒäººå£ç»Ÿè®¡å­¦è¾“å…¥è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ƒæµ‹è¯•åœ¨æ­ç¤ºåè§æ¨¡å¼æ–¹é¢å°¤ä¸ºæœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è°ƒå’Œæƒ…æ„Ÿæ–¹é¢ã€‚æœ¬ç ”ç©¶æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¬å¹³æ€§æµ‹è¯•æ–¹é¢çš„ç¨³å¥æ€§ï¼Œæä¾›äº†ä¸€ç§æ£€æµ‹å¹¶ç¼“è§£åè§é—®é¢˜çš„ç»“æ„åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨å…¬å¹³æ€§ç›¸å…³é—®é¢˜çš„è„†å¼±æ€§ã€‚</li>
<li>æ¨¡å‹ä¸­çš„åè§åæ˜ åœ¨å…¶è®­ç»ƒæ•°æ®ä¸­ã€‚</li>
<li>å…ƒæµ‹è¯•æ–¹æ³•ç”¨äºç³»ç»Ÿåœ°è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å…¬å¹³æ€§é—®é¢˜æ¼æ´ã€‚</li>
<li>æµ‹è¯•å¯¹è±¡åŒ…æ‹¬LLaMAå’ŒGPTç­‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å…ƒæµ‹è¯•ç‰¹åˆ«æœ‰æ•ˆäºæ­ç¤ºæ¨¡å‹ä¸­çš„åè§æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è°ƒå’Œæƒ…æ„Ÿæ–¹é¢ã€‚</li>
<li>æµ‹è¯•æ­ç¤ºäº†ç‰¹å®šæ•æ„Ÿå±æ€§äº¤é›†ç»å¸¸å‡ºç°çš„å…¬å¹³æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eadc46d4bb7fffc522eff70064b630f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f57d2f14d437be2a4931dbbda203e0a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities"><a href="#An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities" class="headerlink" title="An Empirical Study of GPT-4o Image Generation Capabilities"></a>An Empirical Study of GPT-4o Image Generation Capabilities</h2><p><strong>Authors:Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi</strong></p>
<p>The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4oâ€™s image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling. For a high-definition version of the PDF, please refer to the link on GitHub: \href{<a target="_blank" rel="noopener" href="https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen%7D%7Bhttps://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen%7D">https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen}{https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen}</a>. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆé¢†åŸŸå·²ç»è¿…é€Ÿæ¼”å˜ï¼Œä»æ—©æœŸçš„åŸºäºGANçš„æ–¹æ³•åˆ°æ‰©æ•£æ¨¡å‹ï¼Œå†åˆ°æœ€è¿‘çš„å¯»æ±‚ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´æ¡¥æ¢çš„ç»Ÿä¸€ç”Ÿæˆæ¶æ„ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯GPT-4oï¼Œå·²ç»è¯æ˜äº†é«˜ä¿çœŸåº¦å¤šåª’ä½“ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä½†å…¶æ¶æ„è®¾è®¡ä»ç„¶ç¥ç§˜ä¸”æœªå…¬å¼€ã€‚è¿™å¼•å‘äº†äººä»¬å…³äºå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ˜¯å¦å·²ç»æˆåŠŸé›†æˆåˆ°è¿™äº›æ–¹æ³•ä¸­çš„ç»Ÿä¸€æ¡†æ¶çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå¹¶å°†å…¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†å››ä¸ªä¸»è¦ç±»åˆ«ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆï¼Œæ¶µç›–è¶…è¿‡20é¡¹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶å°†å…¶ç½®äºæ›´å¹¿æ³›çš„ç”Ÿæˆæ¨¡å‹æ¼”å˜ä¸­ã€‚é€šè¿‡è¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬ä¸ºæœªæ¥çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¹¶å¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®è§„æ¨¡æ‰©å¤§çš„ä½œç”¨ã€‚å¦‚éœ€PDFçš„é«˜æ¸…ç‰ˆæœ¬ï¼Œè¯·å‚é˜…GitHubä¸Šçš„é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05979v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¯¹æ¯”äº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚é€šè¿‡å¯¹GPT-4oè¿›è¡Œå¤šæ–¹é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°ä¸‰ç»´å’Œå›¾åƒåˆ°Xç”Ÿæˆç­‰å››ä¸ªä¸»è¦ç±»åˆ«è¶…è¿‡äºŒåé¡¹ä»»åŠ¡ï¼Œæœ¬æ–‡æ€»ç»“äº†GPT-4oåœ¨ä¸åŒåœºæ™¯ä¸‹çš„ä¼˜åŠ¿å’Œå±€é™ï¼Œå¹¶æ¢è®¨äº†æœªæ¥ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å‘å±•æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯æ¶æ„è®¾è®¡å’Œæ•°æ®è§„æ¨¡çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ä¿çœŸåº¦å¤šåª’ä½“ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›é€šè¿‡å¤šé¡¹ä»»åŠ¡è¯„ä¼°ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°ä¸‰ç»´å’Œå›¾åƒåˆ°Xç”Ÿæˆã€‚</li>
<li>GPT-4oåœ¨å¤šç§è®¾ç½®ä¸‹è¡¨ç°å‡ºä¼˜åŠ¿å’Œå±€é™ï¼Œä¸å…¶ä»–å¼€æºå’Œå•†ä¸šæ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>GPT-4oçš„æ¶æ„è®¾è®¡å¯¹äºå…¶æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†å…·ä½“ç»†èŠ‚å°šæœªå…¬å¼€ã€‚</li>
<li>æ•°æ®è§„æ¨¡å¯¹GPT-4oçš„å›¾åƒç”Ÿæˆæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ä»å¤„äºå‘å±•åˆæœŸï¼Œæœªæ¥æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2010a11edf3032c967a7ee9f6ab24b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8b70545b43ecb6ffa3d082185a785b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653e211f893e469876b0aff041185c8b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MCP-Safety-Audit-LLMs-with-the-Model-Context-Protocol-Allow-Major-Security-Exploits"><a href="#MCP-Safety-Audit-LLMs-with-the-Model-Context-Protocol-Allow-Major-Security-Exploits" class="headerlink" title="MCP Safety Audit: LLMs with the Model Context Protocol Allow Major   Security Exploits"></a>MCP Safety Audit: LLMs with the Model Context Protocol Allow Major   Security Exploits</h2><p><strong>Authors:Brandon Radosevich, John Halloran</strong></p>
<p>To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developerâ€™s system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP serverâ€™s tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.   The described MCP server auditing tool, MCPSafetyScanner, is freely available at: <a target="_blank" rel="noopener" href="https://github.com/johnhalloran321/mcpSafetyScanner">https://github.com/johnhalloran321/mcpSafetyScanner</a> </p>
<blockquote>
<p>ä¸ºäº†é™ä½å¼€å‘æˆæœ¬ï¼Œå¹¶ä½¿ä»»ä½•ç»™å®šçš„ç”Ÿæˆå¼AIåº”ç”¨ä¸­çš„æ½œåœ¨ç»„ä»¶æ— ç¼é›†æˆï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ï¼ˆAnthropicï¼Œ2024ï¼‰æœ€è¿‘å·²è¢«å‘å¸ƒå¹¶éšåè¢«å¹¿æ³›é‡‡ç”¨ã€‚MCPæ˜¯ä¸€ä¸ªå¼€æ”¾åè®®ï¼Œå®ƒæ ‡å‡†åŒ–äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€æ•°æ®æºå’Œæ™ºèƒ½å·¥å…·çš„APIè°ƒç”¨ã€‚é€šè¿‡è¿æ¥å¤šä¸ªMCPæœåŠ¡å™¨ï¼ˆæ¯ä¸ªæœåŠ¡å™¨éƒ½å®šä¹‰äº†ä¸€ç»„å·¥å…·ã€èµ„æºå’Œæç¤ºï¼‰ï¼Œç”¨æˆ·èƒ½å¤Ÿå®šä¹‰ç”±LLMå®Œå…¨é©±åŠ¨çš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå½“å‰çš„MCPè®¾è®¡å­˜åœ¨å¹¿æ³›çš„å®‰å…¨é£é™©ï¼Œä¼šå¯¹ç»ˆç«¯ç”¨æˆ·é€ æˆå¨èƒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡æ¼”ç¤ºè¯æ˜ï¼Œè¡Œä¸šé¢†å…ˆçš„LLMå¯èƒ½è¢«è¯±å¯¼ä½¿ç”¨MCPå·¥å…·ï¼Œé€šè¿‡å„ç§æ”»å‡»æ¥ç ´åAIå¼€å‘è€…çš„ç³»ç»Ÿï¼Œä¾‹å¦‚æ¶æ„ä»£ç æ‰§è¡Œã€è¿œç¨‹è®¿é—®æ§åˆ¶å’Œå‡­è¯ç›—çªƒã€‚ä¸ºäº†ç§¯æç¼“è§£è¿™äº›å’Œç›¸å…³æ”»å‡»ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®‰å…¨å®¡è®¡å·¥å…·MCPSafetyScannerï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°ä»»æ„MCPæœåŠ¡å™¨å®‰å…¨æ€§çš„æ™ºèƒ½å·¥å…·ã€‚MCPScannerä½¿ç”¨å¤šä¸ªæ™ºèƒ½ä»£ç†æ¥ï¼ˆaï¼‰æ ¹æ®MCPæœåŠ¡å™¨çš„å·¥å…·å’Œèµ„æºè‡ªåŠ¨ç¡®å®šå¯¹æŠ—æ ·æœ¬ï¼›ï¼ˆbï¼‰åŸºäºè¿™äº›æ ·æœ¬æœç´¢ç›¸å…³æ¼æ´å’Œè¡¥æ•‘æªæ–½ï¼›ï¼ˆcï¼‰ç”ŸæˆåŒ…å«æ‰€æœ‰å‘ç°çš„å®‰å…¨æŠ¥å‘Šã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†é€šç”¨æ™ºèƒ½å·¥ä½œæµç¨‹ä¸­çš„ä¸¥é‡å®‰å…¨é—®é¢˜ï¼ŒåŒæ—¶æä¾›äº†ä¸€ä¸ªç§¯æçš„å·¥å…·æ¥å®¡è®¡MCPæœåŠ¡å™¨çš„å®‰å…¨æ€§ï¼Œå¹¶åœ¨éƒ¨ç½²ä¹‹å‰è§£å†³æ£€æµ‹åˆ°çš„æ¼æ´ã€‚æè¿°çš„MCPæœåŠ¡å™¨å®¡è®¡å·¥å…·MCPSafetyScannerå¯åœ¨ä»¥ä¸‹ç½‘å€å…è´¹è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/johnhalloran321/mcpSafetyScanner">https://github.com/johnhalloran321/mcpSafetyScanner</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03767v2">PDF</a> 27 pages, 21 figures, and 2 Tables. Cleans up the TeX source</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ˜¯ä¸€ä¸ªå¼€æ”¾åè®®ï¼Œç”¨äºæ ‡å‡†åŒ–å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€æ•°æ®æºå’Œä»£ç†å·¥å…·çš„APIè°ƒç”¨ï¼Œä»¥å®ç°ç”Ÿæˆå¼AIåº”ç”¨ç¨‹åºå„ç»„ä»¶ä¹‹é—´çš„æ— ç¼é›†æˆï¼Œå‡å°‘å¼€å‘æˆæœ¬ã€‚ç„¶è€Œï¼Œå½“å‰MCPè®¾è®¡å­˜åœ¨å¤šç§å®‰å…¨é£é™©ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ä¸€ç§é’ˆå¯¹MCPå·¥å…·çš„å®‰å…¨å®¡è®¡å·¥å…·MCPSafetyScannerï¼Œå®ƒèƒ½å¤Ÿè¯„ä¼°ä»»æ„MCPæœåŠ¡å™¨çš„å®‰å…¨æ€§ï¼Œé€šè¿‡è‡ªåŠ¨ç¡®å®šå¯¹æŠ—æ ·æœ¬ã€æœç´¢ç›¸å…³æ¼æ´å’Œè¡¥æ•‘æªæ–½å¹¶ç”Ÿæˆå®‰å…¨æŠ¥å‘Šæ¥è¯¦ç»†è®°å½•æ‰€æœ‰å‘ç°ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†é€šç”¨ä»£ç†å·¥ä½œæµç¨‹çš„ä¸¥é‡å®‰å…¨é—®é¢˜ï¼ŒåŒæ—¶æä¾›äº†åœ¨éƒ¨ç½²å‰å®¡è®¡MCPæœåŠ¡å™¨å®‰å…¨æ€§å¹¶è§£å†³æ£€æµ‹åˆ°çš„æ¼æ´çš„ä¸»åŠ¨å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MCPæ˜¯ä¸€ä¸ªç”¨äºæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œå…¶ä»–AIå·¥å…·çš„å¼€æ”¾åè®®ï¼Œä¿ƒè¿›äº†AIåº”ç”¨çš„å¼€å‘ã€‚</li>
<li>å½“å‰MCPè®¾è®¡å­˜åœ¨å®‰å…¨é£é™©ï¼Œå¯èƒ½å¯¼è‡´AIç³»ç»Ÿè¢«æ¶æ„æ”»å‡»è€…åˆ©ç”¨ã€‚</li>
<li>LLMså¯èƒ½è¢«è¯±å¯¼é€šè¿‡MCPå·¥å…·è¿›è¡Œæ¶æ„ä»£ç æ‰§è¡Œã€è¿œç¨‹è®¿é—®æ§åˆ¶å’Œå‡­è¯ç›—çªƒç­‰æ”»å‡»ã€‚</li>
<li>MCPSafetyScanneræ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°ä»»æ„MCPæœåŠ¡å™¨å®‰å…¨æ€§çš„ä»£ç†å·¥å…·ã€‚</li>
<li>MCPSafetyScannerèƒ½å¤Ÿè‡ªåŠ¨ç¡®å®šå¯¹æŠ—æ ·æœ¬ã€æœç´¢ç›¸å…³æ¼æ´å’Œè¡¥æ•‘æªæ–½ã€‚</li>
<li>è¯¥å·¥å…·èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„å®‰å…¨æŠ¥å‘Šï¼Œè®°å½•æ‰€æœ‰å‘ç°çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†é€šç”¨ä»£ç†å·¥ä½œæµç¨‹çš„å®‰å…¨é—®é¢˜ï¼Œå¹¶æä¾›äº†åœ¨éƒ¨ç½²å‰è§£å†³è¿™äº›å®‰å…¨é—®é¢˜çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3614c95a7b2ed443baea9afc20dfe5ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c488ae57e82f554be331fe0b2361cee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d34221ff9d37a1630bdab898013718b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4ffbb5576a199085596b39112d7114b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniScienceï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘ï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒç­›é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œï¼ˆ2ï¼‰åœ¨ä¸“é—¨æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å“åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ç”µæ± ä»£ç†æ¥å±•ç¤ºOmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°æ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸçš„ç”µæ± åŸºå‡†æµ‹è¯•ä¸Šå¯ä¸æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹ç›¸ç«äº‰ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æ‰€æœ‰å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å‰”é™¤å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ï¼Œè·¨å¤šä¸ªåŸºå‡†æµ‹è¯•éƒ½æ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨åŠ¨ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹é€šç”¨ç§‘å­¦çš„ç‰¹æ®Šå¤§å‹æ¨ç†æ¨¡å‹â€”â€”OmniScienceã€‚å®ƒç»è¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘è€Œæˆï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒç­›é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å“åº”çš„èƒ½åŠ›ã€‚OmniScienceçš„é€šç”¨æ€§é€šè¿‡å¼€å‘ç”µæ± ä»£ç†å¾—åˆ°äº†éªŒè¯ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°æ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸Šå…·å¤‡ä¸æœ€æ–°å¤§å‹æ¨ç†æ¨¡å‹ç«äº‰çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¼˜äºæ‰€æœ‰å‚æ•°ç›¸ä¼¼çš„å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¶ˆèå®éªŒè¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨åŠ¨ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹æŒ‘æˆ˜æ–¹é¢æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>OmniScienceæ¨¡å‹æ˜¯é€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´å’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘è€Œæˆçš„ã€‚</li>
<li>OmniScienceæ¨¡å‹å…·å¤‡é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåº”ç”¨äºç”µæ± ä»£ç†çš„å¼€å‘ï¼Œæ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚</li>
<li>OmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>æ¶ˆèå®éªŒè¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹OmniScienceçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>OmniScienceæ¨¡å‹ä¼˜äºå‚æ•°ç›¸ä¼¼çš„å…¶ä»–å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†LLMåœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç§‘å­¦ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5d6944cf303fea9494c3ca74eadb42e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval"><a href="#A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval" class="headerlink" title="A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval"></a>A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval</h2><p><strong>Authors:Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li</strong></p>
<p>Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>. </p>
<blockquote>
<p>ä¿¡æ¯æŠ€æœ¯æ·±åˆ»æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚ç½‘ä¸Šåˆ›å»ºã€å…±äº«å’Œä¼ æ’­çš„å¤§é‡å†…å®¹ä½¿å¾—è·å–ç›¸å…³ä¿¡æ¯çš„éš¾åº¦è¶Šæ¥è¶Šå¤§ã€‚åœ¨è¿‡å»çš„äºŒåå¹´ä¸­ï¼Œæ¨èç³»ç»Ÿå’Œæœç´¢ï¼ˆç»Ÿç§°ä¸ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰å·²ç»å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†è¶…è¶Šäººç±»çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºä¸€èˆ¬ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†LLMä»£ç†åœ¨å¢å¼ºæ¨èç³»ç»Ÿå’Œæœç´¢ç³»ç»Ÿæ–¹é¢çš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬è®¨è®ºäº†LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†ç±»æ¡†æ¶æ¥é˜è¿°ç°æœ‰ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMä»£ç†åœ¨åº”å¯¹æ¨èå’Œæœç´¢æ–¹é¢çš„å½“å‰æŒ‘æˆ˜çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†è§è§£ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿå›é¡¾å’Œåˆ†ç±»äº†è¿™äº›é¢†åŸŸä¸­çš„LLMä»£ç†ç ”ç©¶ï¼Œä¸ºå¦‚ä½•åˆ©ç”¨è¿™ä¸€å…ˆè¿›çš„AIæŠ€æœ¯è¿›è¡Œä¿¡æ¯æ£€ç´¢æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚ä¸ºäº†ç†è§£ç°æœ‰å·¥ä½œï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥åˆ—å‡ºäº†å…³äºLLMä»£ç†æ¨èå’Œæœç´¢çš„ç°æœ‰è®ºæ–‡ï¼š<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search%E3%80%82">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Searchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05659v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¿¡æ¯æŠ€æœ¯çš„å‘å±•æ·±åˆ»æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚éšç€ç½‘ä¸Šåˆ›å»ºã€å…±äº«å’Œä¼ æ’­çš„å†…å®¹å¤§é‡å¢åŠ ï¼Œè·å–ç›¸å…³ä¿¡æ¯å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æ¨èç³»ç»Ÿå’Œæœç´¢ï¼ˆç»Ÿç§°ä¸ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰åœ¨è¿‡å»çš„äºŒåå¹´ä¸­å·²ç»æ˜¾è‘—å‘å±•ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²å±•ç°å‡ºåœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­è¶…è¶Šäººç±»æ€§èƒ½çš„èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºä¸€èˆ¬ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†LLMä»£ç†åœ¨å¢å¼ºæ¨èå’Œæœç´¢ç³»ç»Ÿæ–¹é¢çš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬è®¨è®ºäº†LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†ç±»æ¡†æ¶æ¥è¯¦ç»†é˜è¿°ç°æœ‰çš„ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMä»£ç†åœ¨åº”å¯¹æ¨èå’Œæœç´¢æ–¹é¢çš„å½“å‰æŒ‘æˆ˜çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›è§è§£ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°å›é¡¾å’Œåˆ†ç±»äº†LLMä»£ç†åœ¨è¿™äº›é¢†åŸŸçš„ç ”ç©¶ï¼Œä¸ºå¦‚ä½•åˆ©ç”¨è¿™ç§å…ˆè¿›çš„AIæŠ€æœ¯è¿›è¡Œä¿¡æ¯æ£€ç´¢æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æŠ€æœ¯æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚</li>
<li>ç½‘ä¸Šå†…å®¹çš„å¤§é‡å¢åŠ ä½¿å¾—è·å–ç›¸å…³ä¿¡æ¯æ›´ä¸ºå›°éš¾ã€‚</li>
<li>æ¨èç³»ç»Ÿå’Œæœç´¢åœ¨ä¿¡æ¯æ£€ç´¢ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶Šäººç±»ï¼Œå…·å¤‡ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
<li>LLMä»£ç†åœ¨å¢å¼ºæ¨èå’Œæœç´¢ç³»ç»Ÿçš„æ½œåŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„å˜é©æ€§ã€‚</li>
<li>LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²åœ¨æ¨åŠ¨å…¶å‘å±•ä¸Šèµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7582776c0e259c15939fabf692554e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9003206c7a510ca96f8efc0e6a109ce5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning"><a href="#An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning" class="headerlink" title="An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning"></a>An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning</h2><p><strong>Authors:Navdeep Kaur, Lachlan McPheat, Alessandra Russo, Anthony G Cohn, Pranava Madhyastha</strong></p>
<p>In this paper, we examine the use of Conformal Language Modelling (CLM) alongside Answer Set Programming (ASP) to enhance the performance of standard open-weight LLMs on complex multi-step reasoning tasks. Using the StepGame dataset, which requires spatial reasoning, we apply CLM to generate sets of ASP programs from an LLM, providing statistical guarantees on the correctness of the outputs. Experimental results show that CLM significantly outperforms baseline models that use standard sampling methods, achieving substantial accuracy improvements across different levels of reasoning complexity. Additionally, the LLM-as-Judge metric enhances CLMâ€™s performance, especially in assessing structurally and logically correct ASP outputs. However, calibrating CLM with diverse calibration sets did not improve generalizability for tasks requiring much longer reasoning steps, indicating limitations in handling more complex tasks. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å°†é¡ºåº”è¯­è¨€å»ºæ¨¡ï¼ˆCLMï¼‰ä¸ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰ç›¸ç»“åˆï¼Œä»¥æé«˜æ ‡å‡†å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨éœ€è¦ç©ºé—´æ¨ç†çš„StepGameæ•°æ®é›†ï¼Œåº”ç”¨CLMä»å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆASPç¨‹åºé›†ï¼Œä¸ºè¾“å‡ºçš„æ­£ç¡®æ€§æä¾›ç»Ÿè®¡ä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLMæ˜¾è‘—ä¼˜äºä½¿ç”¨æ ‡å‡†é‡‡æ ·æ–¹æ³•çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨ä¸åŒå±‚æ¬¡çš„æ¨ç†å¤æ‚åº¦ä¸Šå®ç°äº†å®è´¨æ€§çš„å‡†ç¡®æ€§æé«˜ã€‚å¦å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°è€…çš„æŒ‡æ ‡æé«˜äº†CLMçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°ç»“æ„ä¸Šå’Œé€»è¾‘ä¸Šæ­£ç¡®çš„ASPè¾“å‡ºæ–¹é¢ã€‚ç„¶è€Œï¼Œç”¨å¤šç§æ ¡å‡†é›†æ ¡å‡†CLMå¹¶æ²¡æœ‰æé«˜å¯¹éœ€è¦æ›´é•¿æ—¶é—´æ¨ç†æ­¥éª¤çš„ä»»åŠ¡çš„é€šç”¨æ€§ï¼Œè¿™è¡¨æ˜åœ¨å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†Conformalè¯­è¨€å»ºæ¨¡ï¼ˆCLMï¼‰ä¸ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰ç»“åˆä½¿ç”¨ï¼Œä»¥æé«˜æ ‡å‡†å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç ”ç©¶ä½¿ç”¨StepGameæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è¦æ±‚ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡CLMç”ŸæˆASPç¨‹åºé›†ï¼Œä¸ºè¾“å‡ºç»“æœæä¾›ç»Ÿè®¡æ­£ç¡®æ€§ä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLMæ˜¾è‘—ä¼˜äºä½¿ç”¨æ ‡å‡†é‡‡æ ·æ–¹æ³•çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¸åŒå±‚æ¬¡çš„æ¨ç†å¤æ‚åº¦ä¸Šéƒ½å®ç°äº†å®è´¨æ€§çš„å‡†ç¡®æ€§æé«˜ã€‚æ­¤å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…çš„æŒ‡æ ‡ä¹Ÿå¢å¼ºäº†CLMçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°ç»“æ„åŒ–å’Œé€»è¾‘æ­£ç¡®çš„ASPè¾“å‡ºæ–¹é¢ã€‚ç„¶è€Œï¼Œç”¨å„ç§æ ¡å‡†é›†æ ¡å‡†CLMå¹¶æ²¡æœ‰æé«˜å¯¹éœ€è¦æ›´é•¿æœŸæ¨ç†ä»»åŠ¡çš„é€šç”¨æ€§ï¼Œè¿™è¡¨æ˜åœ¨å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç»“åˆConformalè¯­è¨€å»ºæ¨¡ï¼ˆCLMï¼‰ä¸ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨StepGameæ•°æ®é›†è¿›è¡Œå®è¯ç ”ç©¶ï¼Œè¯¥æ•°æ®é›†å¼ºè°ƒç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CLMèƒ½å¤Ÿç”ŸæˆASPç¨‹åºé›†ï¼Œå¹¶ä¸ºè¾“å‡ºç»“æœæä¾›ç»Ÿè®¡æ­£ç¡®æ€§ä¿è¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºCLMåœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒæ¨ç†å¤æ‚åº¦å±‚æ¬¡ä¸Šã€‚</li>
<li>LLM-as-JudgeæŒ‡æ ‡å¢å¼ºäº†CLMåœ¨è¯„ä¼°ASPè¾“å‡ºç»“æ„å’Œé€»è¾‘æ­£ç¡®æ€§æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å¤šç§æ ¡å‡†é›†å¯¹CLMè¿›è¡Œæ ¡å‡†å¹¶æœªæ˜¾è‘—æé«˜å…¶å¤„ç†æ›´é•¿æœŸæˆ–æ›´å¤æ‚æ¨ç†ä»»åŠ¡çš„é€šç”¨æ€§ï¼Œè¡¨æ˜å­˜åœ¨ä¸€å®šå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f87f0f4665100c6d9270c099f5c50cf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Retrieval-Augmented-Generation-for-Visual-Question-Answering"><a href="#Fine-Grained-Retrieval-Augmented-Generation-for-Visual-Question-Answering" class="headerlink" title="Fine-Grained Retrieval-Augmented Generation for Visual Question   Answering"></a>Fine-Grained Retrieval-Augmented Generation for Visual Question   Answering</h2><p><strong>Authors:Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang</strong></p>
<p>Visual Question Answering (VQA) focuses on providing answers to natural language questions by utilizing information from images. Although cutting-edge multimodal large language models (MLLMs) such as GPT-4o achieve strong performance on VQA tasks, they frequently fall short in accessing domain-specific or the latest knowledge. To mitigate this issue, retrieval-augmented generation (RAG) leveraging external knowledge bases (KBs), referred to as KB-VQA, emerges as a promising approach. Nevertheless, conventional unimodal retrieval techniques, which translate images into textual descriptions, often result in the loss of critical visual details. This study presents fine-grained knowledge units, which merge textual snippets with entity images stored in vector databases. Furthermore, we introduce a knowledge unit retrieval-augmented generation framework (KU-RAG) that integrates fine-grained retrieval with MLLMs. The proposed KU-RAG framework ensures precise retrieval of relevant knowledge and enhances reasoning capabilities through a knowledge correction chain. Experimental findings demonstrate that our approach significantly boosts the performance of leading KB-VQA methods, achieving an average improvement of approximately 3% and up to 11% in the best case. </p>
<blockquote>
<p>è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸“æ³¨äºé€šè¿‡åˆ©ç”¨å›¾åƒä¸­çš„ä¿¡æ¯æ¥å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚å°½ç®¡æœ€å‰æ²¿çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰åœ¨VQAä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨è®¿é—®ç‰¹å®šé¢†åŸŸæˆ–æœ€æ–°çŸ¥è¯†æ—¶æ˜¾å¾—ä¸è¶³ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œè¢«ç§°ä¸ºKB-VQAï¼Œæ­£æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•æ¨¡æ€æ£€ç´¢æŠ€æœ¯å°†å›¾åƒç¿»è¯‘æˆæ–‡æœ¬æè¿°ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´å…³é”®è§†è§‰ç»†èŠ‚çš„ä¸¢å¤±ã€‚æœ¬ç ”ç©¶æå‡ºäº†ç²¾ç»†çŸ¥è¯†å•å…ƒï¼Œå°†æ–‡æœ¬ç‰‡æ®µä¸å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­çš„å®ä½“å›¾åƒåˆå¹¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¥è¯†å•å…ƒæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆKU-RAGï¼‰ï¼Œå°†ç²¾ç»†æ£€ç´¢ä¸MLLMsç›¸ç»“åˆã€‚æ‰€æå‡ºçš„KU-RAGæ¡†æ¶ç¡®ä¿ç²¾ç¡®æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œå¹¶é€šè¿‡çŸ¥è¯†æ ¡æ­£é“¾å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é¢†å…ˆçš„KB-VQAæ–¹æ³•çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†çº¦3%ï¼Œåœ¨æœ€ä½³æƒ…å†µä¸‹æé«˜äº†é«˜è¾¾11%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20964v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡å…³æ³¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡å›¾åƒä¿¡æ¯å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚è™½ç„¶å½“å‰å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oåœ¨VQAä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨è®¿é—®ç‰¹å®šé¢†åŸŸæˆ–æœ€æ–°çŸ¥è¯†æ—¶å¸¸å¸¸å­˜åœ¨ç¼ºé™·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆKBsï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œå³KB-VQAï¼Œå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•æ¨¡æ€æ£€ç´¢æŠ€æœ¯å°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œå¾€å¾€ä¼šå¯¼è‡´å…³é”®è§†è§‰ç»†èŠ‚çš„ä¸¢å¤±ã€‚æœ¬ç ”ç©¶æå‡ºç²¾ç»†ç²’åº¦çŸ¥è¯†å•å…ƒï¼Œå°†æ–‡æœ¬ç‰‡æ®µä¸å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­çš„å®ä½“å›¾åƒåˆå¹¶ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†çŸ¥è¯†å•å…ƒæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆKU-RAGï¼‰ï¼Œå°†ç²¾ç»†ç²’åº¦æ£€ç´¢ä¸MLLMsç»“åˆã€‚KU-RAGæ¡†æ¶ç¡®ä¿ç²¾ç¡®æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œå¹¶é€šè¿‡çŸ¥è¯†æ ¡æ­£é“¾å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†é¢†å…ˆçš„KB-VQAæ–¹æ³•çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†çº¦3%ï¼Œæœ€é«˜æé«˜äº†11%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ˜¯åˆ©ç”¨å›¾åƒä¿¡æ¯å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜çš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨VQAä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨è®¿é—®ç‰¹å®šé¢†åŸŸæˆ–æœ€æ–°çŸ¥è¯†æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆKBsï¼‰æ¥è§£å†³MLLMsçš„ç¼ºé™·ï¼Œå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ä¼ ç»Ÿå•æ¨¡æ€æ£€ç´¢æŠ€æœ¯å­˜åœ¨å°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°æ—¶ä¸¢å¤±å…³é”®è§†è§‰ç»†èŠ‚çš„é—®é¢˜ã€‚</li>
<li>ç²¾ç»†ç²’åº¦çŸ¥è¯†å•å…ƒå°†æ–‡æœ¬å’Œå›¾åƒç»“åˆï¼Œæé«˜äº†æ£€ç´¢çš„ç²¾ç¡®åº¦ã€‚</li>
<li>å¼•å…¥çš„çŸ¥è¯†å•å…ƒæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆKU-RAGï¼‰ç»“åˆäº†ç²¾ç»†ç²’åº¦æ£€ç´¢ä¸MLLMsï¼Œæé«˜äº†çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e45b66104a5fa6178fe0ff035e30304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5758906f148b4255a03cc17b0798fe41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f8fe0f761d70564f5946a712e974c1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c7bf2b909c1a099340f62d5bcb58120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8a41afeaa723a1d259a0d3144de9ae3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization"><a href="#ScaffoldGPT-A-Scaffold-based-GPT-Model-for-Drug-Optimization" class="headerlink" title="ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization"></a>ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization</h2><p><strong>Authors:Xuefeng Liu, Songhao Jiang, Ian Foster, Jinbo Xu, Rick Stevens</strong></p>
<p>Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A uniquely designed two-phase incremental training approach for pre-training the drug optimization GPT on molecule scaffold with enhanced performance. (3) A token-level decoding optimization strategy, TOP-N, that enabling controlled, reward-guided generation using pretrained&#x2F;finetuned GPT. We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing desired properties. </p>
<blockquote>
<p>åœ¨ç—…æ¯’å¿«é€Ÿå˜å¼‚å’Œè¯ç‰©è€è¯æ€§ç™Œç»†èƒé¢å‰ï¼Œè¯ç‰©ä¼˜åŒ–å˜å¾—æ„ˆå‘å…³é”®ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä»»åŠ¡ä»ç„¶å……æ»¡æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨ä¿ç•™åŸå§‹è¯ç‰©çš„æœ‰ç›Šç‰¹æ€§çš„åŒæ—¶ï¼Œè¿˜å¢å¼ºè¶…å‡ºå…¶èŒƒå›´ä¹‹å¤–çš„æ‰€éœ€å±æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼•å…¥ScaffoldGPTæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆ†å­æ”¯æ¶çš„æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰ï¼Œä¸“ä¸ºè¯ç‰©ä¼˜åŒ–è€Œè®¾è®¡ã€‚æˆ‘ä»¬çš„å·¥ä½œåŒ…æ‹¬ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯å“ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ•´åˆäº†é¢„è®­ç»ƒã€å¾®è°ƒå’Œè§£ç ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰ä¸€ä¸ªç‹¬ç‰¹è®¾è®¡çš„ä¸¤é˜¶æ®µå¢é‡è®­ç»ƒæ³•ï¼Œç”¨äºåœ¨åˆ†å­æ”¯æ¶ä¸Šé¢„è®­ç»ƒè¯å“ä¼˜åŒ–GPTï¼Œæé«˜æ€§èƒ½ã€‚ï¼ˆ3ï¼‰ä¸€ç§ä»¤ç‰Œçº§åˆ«çš„è§£ç ä¼˜åŒ–ç­–ç•¥TOP-Nï¼Œå®ƒé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒ&#x2F;å¾®è°ƒçš„GPTå®ç°å—æ§çš„ã€å¥–åŠ±å¼•å¯¼çš„ç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡é’ˆå¯¹COVIDå’Œç™Œç—‡åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°è¯æ˜ï¼ŒScaffoldGPTåœ¨è¯ç‰©ä¼˜åŒ–åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œå°¤å…¶æ“…é•¿ä¿ç•™åŸå§‹åŠŸèƒ½æ”¯æ¶å¹¶å¢å¼ºæ‰€éœ€å±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06891v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¯ç‰©ä¼˜åŒ–é¢ä¸´æ–°çš„æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å¿«é€Ÿå˜å¼‚çš„ç—…æ¯’èŒæ ªå’Œè€è¯ç™Œç»†èƒèƒŒæ™¯ä¸‹ä¿æŒè¯ç‰©çš„åŸæœ‰ä¼˜åŠ¿å±æ€§ï¼ŒåŒæ—¶å¢å¼ºæ‰€éœ€å±æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥ScaffoldGPTï¼Œä¸€ç§ç”¨äºè¯ç‰©ä¼˜åŒ–çš„æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç ”ç©¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼š1. ä¸‰é˜¶æ®µè¯ç‰©ä¼˜åŒ–æ–¹æ³•ï¼Œæ•´åˆé¢„è®­ç»ƒã€å¾®è°ƒå’Œè§£ç ä¼˜åŒ–ï¼›2. ç‹¬ç‰¹è®¾è®¡çš„ä¸¤é˜¶æ®µå¢é‡è®­ç»ƒæ–¹æ³•æ¥é¢„è®­ç»ƒè¯ç‰©ä¼˜åŒ–GPTï¼›3. ä¸€ç§tokençº§åˆ«çš„è§£ç ä¼˜åŒ–ç­–ç•¥TOP-Nï¼Œé€šè¿‡å¥–åŠ±å¼•å¯¼ä½¿ç”¨é¢„è®­ç»ƒæˆ–å¾®è°ƒåçš„GPTç”Ÿæˆå†…å®¹ã€‚é€šè¿‡å¯¹COVIDå’Œç™Œç—‡åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°ï¼Œè¯æ˜ScaffoldGPTåœ¨è¯ç‰©ä¼˜åŒ–åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ï¼ŒåŒæ—¶åœ¨ä¿æŒåŸå§‹åŠŸèƒ½éª¨æ¶å’Œå¢å¼ºæ‰€éœ€å±æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯ç‰©ä¼˜åŒ–é¢ä¸´æ–°çš„æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨ä¿æŒåŸæœ‰ä¼˜åŠ¿å±æ€§çš„åŒæ—¶å¢å¼ºæ‰€éœ€å±æ€§ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ScaffoldGPTï¼Œä¸€ç§æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰ï¼Œç”¨äºè§£å†³è¯ç‰©ä¼˜åŒ–æŒ‘æˆ˜ã€‚</li>
<li>ScaffoldGPTé‡‡ç”¨ä¸‰é˜¶æ®µè¯ç‰©ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒå’Œè§£ç ä¼˜åŒ–ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç‹¬ç‰¹çš„ä¸¤é˜¶æ®µå¢é‡è®­ç»ƒæ–¹æ³•æ¥é¢„è®­ç»ƒè¯ç‰©ä¼˜åŒ–GPTã€‚</li>
<li>TOP-Næ˜¯ä¸€ç§tokençº§åˆ«çš„è§£ç ä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥é€šè¿‡å¥–åŠ±å¼•å¯¼æ§åˆ¶ç”Ÿæˆå†…å®¹ã€‚</li>
<li>ScaffoldGPTåœ¨COVIDå’Œç™Œç—‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç«äº‰å¯¹æ‰‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe9dae3a50317fedc0aa7a47523b9aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff68f3b629529eea02d7b494c7b5137a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9622a0e405fc2a4437ce21c7de2d4c72.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
