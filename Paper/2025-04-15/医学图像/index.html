<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-15  GigaTok Scaling Visual Tokenizers to 3 Billion Parameters for   Autoregressive Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b169ee53107701ddb9675fc6dcae9521.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-15-更新"><a href="#2025-04-15-更新" class="headerlink" title="2025-04-15 更新"></a>2025-04-15 更新</h1><h2 id="GigaTok-Scaling-Visual-Tokenizers-to-3-Billion-Parameters-for-Autoregressive-Image-Generation"><a href="#GigaTok-Scaling-Visual-Tokenizers-to-3-Billion-Parameters-for-Autoregressive-Image-Generation" class="headerlink" title="GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for   Autoregressive Image Generation"></a>GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for   Autoregressive Image Generation</h2><p><strong>Authors:Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, Xihui Liu</strong></p>
<p>In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality – a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to $\bf{3 \space billion}$ parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality. </p>
<blockquote>
<p>在自回归（AR）图像生成中，视觉分词器将图像压缩成紧凑的离散潜在令牌，通过下一个令牌的预测实现对下游自回归模型进行高效的视觉生成训练。虽然扩大视觉分词器可以提高图像重建质量，但它往往会降低下游生成质量——这是现有文献中尚未充分解决的问题。为了解决这一问题，我们引入了GigaTok，这是一种在扩大视觉分词器的同时，可以同时提高图像重建、生成和表示学习的方法。我们发现潜在空间的复杂性增长是重建与生成困境背后的关键因素。为了缓解这一问题，我们提出了语义正则化，它能够将分词器特征与来自预训练视觉编码器的语义一致特征对齐。这种约束在扩大时防止了潜在空间复杂性过高，从而在重建和下游自回归生成方面都取得了持续的改进。基于语义正则化，我们探索了扩大分词器的三个关键实践：（1）使用一维分词器以提高可扩展性，（2）在扩大编码器和解码器时优先扩展解码器，（3）采用熵损失来稳定数十亿规模分词器的训练。通过扩展到$\bf{3 \space billion}$个参数，GigaTok在重建、下游AR生成和下游AR表示质量方面达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08736v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://silentview.github.io/GigaTok">https://silentview.github.io/GigaTok</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了在自回归图像生成中，视觉令牌器在压缩图像到离散潜在令牌时面临的挑战。随着令牌器规模的扩大，图像重建质量提高，但生成质量下降。为解决这一问题，本文提出了GigaTok方法，通过语义正则化对齐令牌器特征与预训练视觉编码器的语义一致特征，以缓解潜在空间的复杂性增长。在此基础上，探索了三个扩展令牌器的重要实践，包括使用1D令牌器提高可扩展性、优先扩展解码器，以及采用熵损失稳定大规模令牌器的训练。通过扩展到3亿参数，GigaTok在重建、下游自回归生成和下游自回归表示质量方面达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉令牌器在自回归图像生成中压缩图像到离散潜在令牌，提高训练效率。</li>
<li>扩大视觉令牌器规模虽能提高图像重建质量，但会导致下游生成质量下降。</li>
<li>GigaTok方法通过语义正则化解决这一挑战，改善图像重建和生成质量。</li>
<li>语义正则化对齐令牌器特征与预训练视觉编码器的语义一致特征，缓解潜在空间复杂性增长。</li>
<li>GigaTok探索了三个扩展令牌器的实践：使用1D令牌器、优先扩展解码器、采用熵损失稳定训练。</li>
<li>GigaTok扩展到3亿参数后，在重建、下游自回归生成和表示质量方面达到最先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08736">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c29634cfd231f5afde9a6055196eaff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e8315b3343ba1f6cbd085d5a98ade9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-335ad0dc26d23c7441599735eb2fcdf1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e851fe9b0d439aff4bcd589a3743e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5061ef996f6329e7a2e115de01e3c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-304f5370c0ddb7ad7ab461e8e9c44590.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="X2BR-High-Fidelity-3D-Bone-Reconstruction-from-a-Planar-X-Ray-Image-with-Hybrid-Neural-Implicit-Methods"><a href="#X2BR-High-Fidelity-3D-Bone-Reconstruction-from-a-Planar-X-Ray-Image-with-Hybrid-Neural-Implicit-Methods" class="headerlink" title="X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image   with Hybrid Neural Implicit Methods"></a>X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image   with Hybrid Neural Implicit Methods</h2><p><strong>Authors:Gokce Guven, H. Fatih Ugurdag, Hasan F. Ates</strong></p>
<p>Accurate 3D bone reconstruction from a single planar X-ray remains a challenge due to anatomical complexity and limited input data. We propose X2BR, a hybrid neural implicit framework that combines continuous volumetric reconstruction with template-guided non-rigid registration. The core network, X2B, employs a ConvNeXt-based encoder to extract spatial features from X-rays and predict high-fidelity 3D bone occupancy fields without relying on statistical shape models. To further refine anatomical accuracy, X2BR integrates a patient-specific template mesh, constructed using YOLOv9-based detection and the SKEL biomechanical skeleton model. The coarse reconstruction is aligned to the template using geodesic-based coherent point drift, enabling anatomically consistent 3D bone volumes. Experimental results on a clinical dataset show that X2B achieves the highest numerical accuracy, with an IoU of 0.952 and Chamfer-L1 distance of 0.005, outperforming recent baselines including X2V and D2IM-Net. Building on this, X2BR incorporates anatomical priors via YOLOv9-based bone detection and biomechanical template alignment, leading to reconstructions that, while slightly lower in IoU (0.875), offer superior anatomical realism, especially in rib curvature and vertebral alignment. This numerical accuracy vs. visual consistency trade-off between X2B and X2BR highlights the value of hybrid frameworks for clinically relevant 3D reconstructions. </p>
<blockquote>
<p>从单个平面X射线进行精确的3D骨骼重建仍然是一个挑战，主要是由于解剖结构的复杂性和输入数据的有限性。我们提出了X2BR，这是一种混合神经隐式框架，它将连续的体积重建与模板引导的非刚性注册结合起来。核心网络X2B采用基于ConvNeXt的编码器，从X射线中提取空间特征，并预测高保真3D骨骼占用场，而无需依赖统计形状模型。为了进一步提高解剖准确性，X2BR整合了针对患者的特定模板网格，该网格是使用YOLOv9检测器和SKEL生物力学骨骼模型构建的。粗重建与模板通过基于测地线的相干点漂移进行对齐，从而实现解剖上一致的3D骨骼体积。在临床数据集上的实验结果表明，X2B在数值上达到了最高精度，交并比（IoU）为0.952，Chamfer-L1距离为0.005，超过了最近的基线包括X2V和D2IM-Net。在此基础上，X2BR通过YOLOv9骨检测和生物力学模板对齐融入了解剖先验知识，虽然略微降低IoU（为0.875），但在解剖真实性方面提供了优势，尤其在肋骨弯曲和椎体对齐方面。X2B与X2BR之间的数值精度与视觉一致性的权衡突出了混合框架在临床相关3D重建中的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08675v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种混合神经网络隐式框架X2BR，结合连续体积重建和模板引导的非刚性配准，从单一的平面X射线实现精确的三维骨骼重建。核心网络X2B依靠ConvNeXt-based编码器提取X射线空间特征并预测高精度三维骨骼占用场，无需依赖统计形状模型。为进一步提高解剖精度，X2BR集成特定患者模板网格，通过YOLOv9检测和SKEL生物力学骨骼模型构建。粗重建与模板通过测地线相干点漂移对齐，生成解剖上一致的三维骨骼体积。实验结果显示，X2B在数值精度上达到最高水平，交并比（IoU）为0.952，Chamfer-L1距离为0.005，超越近期基线包括X2V和D2IM-Net。在此基础上，X2BR通过YOLOv9骨检测和生物力学模板对齐融入解剖先验知识，虽交并比略降至0.875，但在肋骨曲率和椎体对齐方面展现出优越的解剖现实性。这表明X2BR这种混合框架在具有临床相关性的三维重建中实现了数值精度与视觉一致性的权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种混合神经网络隐式框架X2BR，用于从单一平面X射线实现三维骨骼重建。</li>
<li>核心网络X2B能预测高精度三维骨骼占用场，不依赖统计形状模型。</li>
<li>通过集成特定患者模板网格和与模板对齐，提高了重建的解剖精度和一致性。</li>
<li>X2BR结合了YOLOv9骨检测和生物力学模板，以融入解剖先验知识。</li>
<li>X2B在数值精度上表现最佳，而X2BR在解剖现实性方面有所优势，尤其在肋骨曲率和椎体对齐方面。</li>
<li>X2BR框架在数值精度与视觉一致性之间达到了权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-842e8233db3f258b9829cc77ca1aea88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fa2cc817c1c5e388fdccf14500e1ffd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff557d76abdee7504421a81633c4a85c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bcc60922e8a5537c154b4fcc1bf23bb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Latent-Diffusion-Autoencoders-Toward-Efficient-and-Meaningful-Unsupervised-Representation-Learning-in-Medical-Imaging"><a href="#Latent-Diffusion-Autoencoders-Toward-Efficient-and-Meaningful-Unsupervised-Representation-Learning-in-Medical-Imaging" class="headerlink" title="Latent Diffusion Autoencoders: Toward Efficient and Meaningful   Unsupervised Representation Learning in Medical Imaging"></a>Latent Diffusion Autoencoders: Toward Efficient and Meaningful   Unsupervised Representation Learning in Medical Imaging</h2><p><strong>Authors:Gabriele Lozupone, Alessandro Bria, Francesco Fontanella, Frederick J. A. Meijer, Claudio De Stefano, Henkjan Huisman</strong></p>
<p>This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM &gt; 0.93, MSE &lt; 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at <a target="_blank" rel="noopener" href="https://github.com/GabrieleLozupone/LDAE">https://github.com/GabrieleLozupone/LDAE</a> </p>
<blockquote>
<p>本研究提出了潜在扩散自编码器（LDAE），这是一种基于扩散的新型编码器-解码器框架，可在医学成像中进行高效且有意义的无监督学习，重点关注使用ADNI数据库的大脑MRI对阿尔茨海默病（AD）进行个案研究。不同于在图像空间运行的常规扩散自编码器，LDAE在压缩的潜在表示中应用扩散过程，提高了计算效率，使3D医学成像表示学习变得可行。为了验证所提出的方法，我们探索了两个关键假设：（i）LDAE能够有效地捕获与AD和衰老相关的3D大脑MRI的有意义语义表示；（ii）LDAE在计算效率高的同时实现了高质量的图片生成和重建。实验结果支持了这两个假设：（i）线性探测评估显示AD的诊断性能令人鼓舞（ROC-AUC：90％，ACC：84％），年龄预测（MAE：4.1岁，RMSE：5.2岁）；（ii）学习到的语义表示能够实现属性操作，从而产生解剖上合理的修改；（iii）语义插值实验显示了强大的缺失扫描重建能力，对于6个月的间隔，SSIM为0.969（MSE：0.0019）。即使对于更长的间隔（24个月），该模型仍能保持稳健的性能（SSIM&gt; 0.93，MSE &lt;0.004），表明其能够捕捉时间进展趋势；（iv）与常规扩散自编码器相比，LDAE显著提高了推理吞吐量（快20倍），同时提高了重建质量。这些发现使LDAE成为医学成像应用程序中一个有前途的框架，并有可能成为医学图像分析的基础模型。代码可在<a target="_blank" rel="noopener" href="https://github.com/GabrieleLozupone/LDAE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GabrieleLozupone/LDAE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08635v1">PDF</a> 15 pages, 9 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>本研究提出了潜在扩散自编码器（LDAE）这一新的编码器-解码器扩散框架，用于医学成像中的高效和有意义无监督学习，以阿尔茨海默病（AD）为例，使用ADNI数据库的脑MR进行研究。LDAE在压缩的潜在表示中应用扩散过程，提高了计算效率，使三维医学成像表示学习变得可行。实验结果显示，LDAE在AD和年龄预测方面表现出良好诊断性能，并且实现了高质量图像生成和重建。此外，LDAE还能进行属性操作、解剖上合理的修改以及强大的重建缺失扫描。与常规扩散自编码器相比，LDAE提高了推理速度并增强了重建质量，为医学成像应用提供了有前途的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDAE是一种新的编码器-解码器扩散框架，用于医学成像中的无监督学习。</li>
<li>LDAE在压缩的潜在表示中应用扩散过程，提高了计算效率。</li>
<li>LDAE在AD和年龄预测方面表现出良好的诊断性能。</li>
<li>LDAE实现了高质量图像生成和重建，并可以进行属性操作。</li>
<li>LDAE能进行解剖上合理的修改和强大的重建缺失扫描。</li>
<li>LDAE与常规扩散自编码器相比，提高了推理速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e1407d392e91f7a9cec2f64eed97a2c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a09acc2cffd9c0b738287849cef3efdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db07bdbb674063d5b04f258ea4e0cd75.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Boosting-multi-demographic-federated-learning-for-chest-x-ray-analysis-using-general-purpose-self-supervised-representations"><a href="#Boosting-multi-demographic-federated-learning-for-chest-x-ray-analysis-using-general-purpose-self-supervised-representations" class="headerlink" title="Boosting multi-demographic federated learning for chest x-ray analysis   using general-purpose self-supervised representations"></a>Boosting multi-demographic federated learning for chest x-ray analysis   using general-purpose self-supervised representations</h2><p><strong>Authors:Mahshad Lotfinia, Arash Tayebiarasteh, Samaneh Samiei, Mehdi Joodaki, Soroosh Tayebi Arasteh</strong></p>
<p>Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n&#x3D;398,523 adult chest radiographs from diverse institutions across multiple countries and n&#x3D;9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P&lt;0.001) but degraded performance for larger datasets (P&lt;0.064) and pediatric cases (P&#x3D;0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P&#x3D;0.031) and most adult datasets (P&lt;0.008), except the largest dataset (P&#x3D;0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles. </p>
<blockquote>
<p>可靠的人工智能（AI）模型进行医学图像分析通常依赖于大量多样且有标签的数据集。联合学习（FL）提供了一种去中心化和保护隐私的训练方法，但在高度非独立同分布（non-IID）的环境中表现挣扎，在这种环境中，拥有更具代表性数据的机构可能会遇到性能下降的问题。此外，现有的大规模联合学习研究仅限于成人数据集，忽视了儿童数据带来的独特挑战，这引入了额外的非IID变化。为了解决这个问题，我们分析了来自多个国家的不同机构中的398,523张成人胸部放射图像和9,125张儿童图像，利用通用自监督图像表示的迁移学习来分类肺炎和无异常情况的病例。使用最先进的视觉转换器，我们发现联合学习只对较小的成人数据集的性能有所提升（P&lt;0.001），而对较大的数据集（P&lt;0.064）和儿童病例（P&#x3D;0.242）的性能却有所下降。然而，将联合学习与自监督权重相结合，显著提高了儿童病例（P&#x3D;0.031）和大多数成人数据集（P&lt;0.008）的结果，但未能改善最大数据集的结果（P&#x3D;0.052）。这些发现强调了易于部署的通用自监督图像表示在解决临床联合学习应用中的非IID挑战方面的潜力，并突显了其在改善患者成果和推进儿科医疗保健方面的前景，其中数据稀缺和变化性仍是持续存在的障碍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08584v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了联邦学习在医学图像分析中的应用，尤其是在处理不同来源的数据时的性能表现。研究发现，在小型成人数据集上，联邦学习能提高模型性能；但在大型数据集和儿科病例上，其性能有所下降。通过引入自监督图像表示技术，能够在很大程度上提升联邦学习在儿科病例和多数成人数据集上的表现。这为解决临床联邦学习中非独立同分布数据的挑战提供了潜力，并有望改善患者结果，特别是在数据稀缺和变化性较大的儿科医疗领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习在医学图像分析中面临非独立同分布数据的挑战，尤其是在大型数据集和儿科病例方面。</li>
<li>在小型成人数据集上，联邦学习能提高模型性能。但在大型数据集和儿科病例上，其性能可能下降。</li>
<li>自监督图像表示技术能显著提升联邦学习在儿科病例和多数成人数据集上的表现。</li>
<li>自监督图像表示技术有助于解决临床联邦学习中非独立同分布数据的挑战。</li>
<li>该技术有望改善患者结果，特别是在数据稀缺和变化性较大的儿科医疗领域。</li>
<li>研究表明，引入自监督权重能增强联邦学习的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08584">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5ea7dfbd20e830344d06441972f16073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ddb4642517e034182b0c95c1fae4dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea8a2cfc3558717d17cb78c54a83d779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b169ee53107701ddb9675fc6dcae9521.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Medical-Image-Classification"><a href="#A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Medical-Image-Classification" class="headerlink" title="A Hybrid Fully Convolutional CNN-Transformer Model for Inherently   Interpretable Medical Image Classification"></a>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently   Interpretable Medical Image Classification</h2><p><strong>Authors:Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens</strong></p>
<p>In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid CNN-ViT models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for medical image classification. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the model’s decision process. We evaluated our method on two medical image classification tasks using color fundus images. Our model not only achieves state-of-the-art predictive performance compared to both black-box and interpretable models but also provides class-specific sparse evidence maps in a single forward pass. The code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Expl-CNN-Transformer/">https://anonymous.4open.science/r/Expl-CNN-Transformer/</a>. </p>
<blockquote>
<p>在许多医学成像任务中，卷积神经网络（CNN）能够高效地分层提取局部特征。最近，使用自注意力机制的视觉转换器（ViT）越来越受欢迎，能够捕捉全局依赖性，但缺乏卷积的固有空间定位能力。因此，已经开发出了结合CNN和ViT的混合模型，以结合两种架构的优点。然而，这样的混合CNN-ViT模型难以解释，这阻碍了它们在医学成像中的应用。在这项工作中，我们介绍了一种可设计的混合全卷积CNN-Transformer架构，用于医学图像分类。与广泛使用的针对ViT的后期显著性方法不同，我们的方法生成忠实且定位准确的证据映射，直接反映模型的决策过程。我们使用彩色眼底图像对两个医学图像分类任务评估了我们的方法。我们的模型不仅与黑箱和可解释模型相比实现了最先进的预测性能，而且还在单次前向传递中提供了特定类别的稀疏证据映射。代码可在：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Expl-CNN-Transformer/%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/Expl-CNN-Transformer/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08481v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种专为解释性设计的混合全卷积CNN-Transformer架构，用于医学图像分类。该架构结合了卷积神经网络和视觉变压器的优点，生成忠实且定位的证据图，直接反映模型的决策过程。在医学图像分类任务中，该模型不仅实现了最先进的预测性能，与黑盒和可解释模型相比，还提供了单次前向传递的类特定稀疏证据图。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNNs和ViTs在医学成像中的优缺点：CNNs能高效地提取局部特征，而ViTs使用自注意力机制捕捉全局依赖性，但缺乏CNN的固有空间定位能力。</li>
<li>混合CNN-ViT模型的出现：为了结合两者的优点，已经开发出了混合CNN-ViT模型。</li>
<li>当前混合模型的可解释性问题：尽管混合CNN-ViT模型在性能上表现出色，但它们的解释性较差，这阻碍了它们在医学成像中的应用。</li>
<li>本文提出的解决方案：介绍了一种可解释的混合全卷积CNN-Transformer架构，该架构旨在直接反映模型的决策过程。</li>
<li>证据图的生成：该架构生成忠实且定位的证据图，这些证据图能展示模型如何结合局部和全局信息做出决策。</li>
<li>模型的性能：在医学图像分类任务中，该模型实现了最先进的预测性能，与现有模型相比具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1f282bc67da941991ec3209bd739138d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb9a22c72066410481647fe4c3a5bafa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d607e16c12055d3888c0625299a52735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b12d5764e9d5fc10fe57c1d102c01e48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083080bb0aaba4567bda0790928b68a7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Weak-lensing-analysis-of-A115-A2219-and-A2261-Detection-of-galaxy-groups-and-filaments-around-clusters"><a href="#Weak-lensing-analysis-of-A115-A2219-and-A2261-Detection-of-galaxy-groups-and-filaments-around-clusters" class="headerlink" title="Weak lensing analysis of A115, A2219 and A2261: Detection of galaxy   groups and filaments around clusters"></a>Weak lensing analysis of A115, A2219 and A2261: Detection of galaxy   groups and filaments around clusters</h2><p><strong>Authors:Anirban Dutta, John Peterson, Matteo Cianfaglione, Glenn Sembroski</strong></p>
<p>We present a weak lensing and multi-wavelength analysis of three galaxy clusters: A115, A2219, and A2261. Weak lensing is performed using shape measurements made in short 60s exposure images obtained using WIYN-ODI. Forced measurement is used to measure low Signal to Noise (SNR) sources in individual exposures. We find the weak lensing significance map recovers the galaxy clusters and most galaxy groups in the wide 40$’$ $\times$ 40$’$ field. Significant parts of the filamentary structures over this field, as indicated by the galaxy number density map, were also successfully recovered in lensing significance maps. We find the amount of structure recovery depends on both the depth and average seeing of the images. In particular, we detect a $&gt;$ 9 Mpc long structure that contains the cluster A2219. We compare our weak lensing maps with Chandra, XMM, and LOFAR observations and find that A115 and A2219 show clear signs of ongoing mergers. In particular, we find a significant separation of hot ICM and the weak lensing contours in A115. On the other hand, while A2261 appears relaxed, based on radio and X-ray analysis, we find that it is likely interacting with a structure 700 kpc SW of the main cluster. We also successfully recovered mass structures in two regions around A2261 indicated by diffuse X-ray emission in XMM images. </p>
<blockquote>
<p>我们对三个星系团A115、A2219和A2261进行了弱引力透镜和多波长分析。弱引力透镜分析是利用WIYN-ODI获取的60秒曝光图像中的形状测量来进行的。强制测量用于测量单个曝光中的低信噪比源。我们发现弱引力透镜显著性地图恢复了宽达40’×40’视野中的星系团和大多数星系团。由星系数密度图指示的此字段的大部分丝状结构，也在透镜显著性地图中成功恢复。我们发现结构恢复的数量取决于图像的深度和平均视宁度。尤其地，我们检测到一个包含星系团A2219的超过9Mpc长的结构。我们将弱引力透镜地图与Chandra、XMM和LOFAR观测结果进行比较，发现A115和A2219显示出正在进行中的合并迹象。特别是，我们发现A115中的热ICM和弱透镜轮廓有明显的分离。另一方面，虽然A2261在射电和X射线分析中看起来处于松弛状态，但我们发现它可能与主星系团西南700kpc处的结构相互作用。我们还成功恢复了XMM图像中围绕A2261的两个区域的弥散X射线发射所指示的质量结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08479v1">PDF</a> Accepted in PASP</p>
<p><strong>Summary</strong></p>
<p>本研究通过对A115、A2219和A2261三个星系团进行弱引力透镜和多波长的分析。利用WIYN-ODI获取的60秒曝光图像进行形状测量，成功恢复了大部分星系群结构。结果显示弱引力透镜显著图与星系数密度图吻合，尤其在发现超过9百万秒差距的结构包含A2219星系团。与Chandra、XMM和LOFAR观测对比，发现A115和A2219存在明显的合并迹象，而看似平静的A2261可能与附近结构有交互作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过对三个星系团A115、A2219和A2261的弱引力透镜和多波长分析，成功恢复了大部分星系群结构。</li>
<li>弱引力透镜显著图与星系数密度图相吻合，揭示了星系团及其周围的细丝状结构。</li>
<li>图像深度和平均视宁度影响结构恢复的程度。</li>
<li>检测到一个超过9百万秒差距的结构，包含A2219星系团。</li>
<li>与其他观测数据对比，发现A115和A2219正在经历合并。</li>
<li>A2261虽看似平静，但可能与附近的结构有交互作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08479">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ea75d46551299a22c0fc98e931b77ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d62bf5b041abb8bd75a4c88a0e942f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63ec355f61996cffdc6c2862a907438b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dada4625b9dd5acfa63553efafefafa9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-population-of-tidal-disruption-events-discovered-with-eROSITA"><a href="#The-population-of-tidal-disruption-events-discovered-with-eROSITA" class="headerlink" title="The population of tidal disruption events discovered with eROSITA"></a>The population of tidal disruption events discovered with eROSITA</h2><p><strong>Authors:Iuliia Grotova, Arne Rau, Pietro Baldini, Adelle J. Goodwin, Zhu Liu, Andrea Merloni, Mara Salvato, Gemma E. Anderson, Riccardo Arcodia, Johannes Buchner, Mirko Krumpe, Adam Malyali, Megan Masterson, James C. A. Miller-Jones, Kirpal Nandra, Raphael Shirley</strong></p>
<p>This paper presents a systematic study of X-ray-selected canonical tidal disruption events (TDEs) discovered in the western Galactic hemisphere of the first two eROSITA all-sky surveys (eRASS1 and eRASS2) performed between Dec 2019 and Dec 2020. We compiled a TDE sample from the catalog of eROSITA’s extragalactic transients and variables eRO-ExTra, which includes X-ray sources with a variability significance and fractional amplitude over four between eRASS1 and eRASS2, not associated with known AGNs. Each X-ray source is associated with an optical counterpart from the Legacy Survey DR10. Canonical TDEs were selected based on their X-ray light-curve properties (single flare or decline), soft X-ray spectra ($\Gamma&gt;3$), and the absence of archival X-ray variability and AGN signatures in their host photometry and spectroscopy. The sample includes 31 X-ray-selected TDE candidates with redshifts of $0.02&lt; z&lt;0.34$ and luminosities of $5.7 \times 10^{41}&lt;L_X&lt;5.3 \times 10^{44}$ erg&#x2F;s in the 0.2-6.0 keV rest frame, of which 30 are canonical TDEs and one is an off-nuclear TDE candidate. The derived X-ray luminosity function is best fit by a double power law with a luminosity break at $10^{44}$ erg&#x2F;s, corresponding to the Eddington-limiting prediction. This corresponds to a TDE volumetric rate of $ (2.3^{+1.2}_{-0.9})\times10^{-7},Mpc^{-3} yr^{-1}$ ($\approx1.2\times 10^{-5}$ events per galaxy per year). TDE host galaxies show a green-valley overdensity. In addition, 20%, 30%, and 15% of the sample exhibit flares in the optical, mid-infrared (mid-IR), or radio bands, respectively. We discuss the differences between X-ray, optical, and mid-IR TDE populations and the origins of multiwavelength flares in the context of the obscuring envelope and stream-stream collision models. Finally, we highlight TDE subpopulations that are not included in the canonical sample and should be explored in the future. </p>
<blockquote>
<p>本文系统地研究了在第一期和第二期eROSITA全天空调查（eRASS1和eRASS2）中，在西银道区域发现的通过X射线选定的典型潮汐撕裂事件（TDEs）。这些调查是在2019年12月至2020年12月之间进行的。我们从eROSITA的类星体外在事件和变量（ERO-ExTra）目录编译了一个TDE样本，其中包括在eRASS1和eRASS2之间四次调查中具有变率显著性和分数振幅的X射线源，这些源并未与已知的活动星系核（AGNs）相关联。每个X射线源都与Legacy Survey DR10的一个光学对应体相关联。典型TDE的选择基于其X射线光曲线特性（单次爆发或下降）、软X射线光谱（γ&gt;3），以及在其宿主的光度测量和光谱中缺乏归档的X射线变率和AGNs的迹象。样本包括31个通过X射线选定的TDE候选者，红移范围为$0.02&lt;z&lt;0.34$，在静止帧的0.2-6.0keV下，光度范围为$5.7\times 10^{41}&lt;Lx&lt;5.3\times 10^{44}$erg&#x2F;s，其中30个是典型TDEs，一个是离核TDE候选者。推导出的X射线光度函数最适合用双幂律拟合，光度中断在$10^{44}$erg&#x2F;s处，这与爱丁顿极限预测相对应。这对应于TDE的体积率为$(2.3^{+1.2}_{-0.9})\times 10^{-7},Mpc^{-3} yr^{-1}$（每年每个星系约有$1.2\times 10^{-5}$个事件）。TDE宿主星系显示出绿谷密度过高。此外，样本中有20％、30％和15％分别在光学、中红外或射频波段表现出爆发。我们讨论了X射线、光学和中红外TDE人群之间的差异以及多波长爆发的起源，这些起源涉及隐蔽包络和流碰撞模型。最后，我们强调了不包括在典型样本中的TDE亚群，未来应对其进行探索研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08424v1">PDF</a> 23 pages, 12 figures. Accepted for publication in A&amp;A</p>
<p><strong>摘要</strong><br>    本研究对首批两个eROSITA全天空调查（eRASS1和eRASS2）中发现的X射线选择的标准潮汐瓦解事件（TDEs）进行了系统研究。通过eROSITA的银河系外瞬态和变量源目录eRO-ExTra，我们筛选出了与已知活动星系核（AGNs）无关的X射线源，这些源在eRASS1和eRASS2之间的四次调查中表现出显著的变性和振幅变化。基于X射线光变曲线特性、软X射线光谱以及宿主的光学和光谱学中无归档的X射线变性和AGNs特征，我们选择了标准的TDEs。样本中包含31个X射线选择的TDE候选者，红移范围在$0.02&lt; z&lt;0.34$之间，在静止帧的0.2-6.0 keV内，光度为$5.7 \times 10^{41}&lt;L_X&lt;5.3 \times 10^{44}$ erg&#x2F;s。样本中大多数为标准的TDEs，一个为离核TDE候选者。推导出的X射线光度函数最适合用双幂律拟合，光度断裂在$10^{44}$ erg&#x2F;s处，与埃丁顿极限预测相符。这对应着每$Mpc^3$每年$ (2.3^{+1.2}_{-0.9})\times10^{-7}$的TDE体积率（即每年每星系约$1.2\times 10^{-5}$个事件）。TDE宿主星系显示出绿色山谷过密。此外，样本中有20%、30%、和15%的TDE在光学、中红外或射频波段表现出耀斑活动。我们讨论了X射线、光学和中红外TDE群体的差异以及多波长耀斑的起源，包括遮蔽包层和流-流碰撞模型。最后，我们强调了未来应探索的不包含在标准样本中的TDE亚群。</p>
<p><strong>关键发现</strong></p>
<ol>
<li>通过对eROSITA首次两次全天空调查中的TDEs进行系统研究，发现了31个X射线选择的TDE候选者。</li>
<li>推导出的X射线光度函数符合双幂律分布，光度断裂点与埃丁顿极限预测相符。</li>
<li>TDE宿主星系在绿色山谷区域有过密现象。</li>
<li>逾20%的样本在光学、中红外或射频波段展现出耀斑活动。</li>
<li>探讨了X射线、光学和中红外TDE群体的差异及多波长耀斑的起源。</li>
<li>发现并讨论了未被当前标准样本涵盖的TDE亚群，为未来研究提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08424">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c30d4e6cda67d8bb1c8090e0e08c2afe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b24e889c79c7d6397c343c6c7659ef3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6b27470da8504c94253f205e89d663c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ede9dba2b3e38ba18ca6eb8d1abe4284.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Modular-Edge-Device-Network-for-Surgery-Digitalization"><a href="#A-Modular-Edge-Device-Network-for-Surgery-Digitalization" class="headerlink" title="A Modular Edge Device Network for Surgery Digitalization"></a>A Modular Edge Device Network for Surgery Digitalization</h2><p><strong>Authors:Vincent Schorp, Frédéric Giraud, Gianluca Pargätzi, Michael Wäspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, Nicola A. Cavalcanti, John Garcia Henao, Nicholas Bünger, Dominique Cachin, Sebastiano Caprara, Philipp Fürnstahl, Fabio Carrillo</strong></p>
<p>Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition. </p>
<blockquote>
<p>未来的手术护理需要实时、集成的数据来推动决策的科学性和改善患者结果。手术室中对无缝、高效数据采集的迫切需求促使我们开发了一种模块化解决方案，以弥新兴的机器学习和介入医学之间的差距。我们引入了一种边缘设备网络，称为数据中心（DHs），它通过光纤和集中式网络交换机连接各种医疗传感器、成像系统和机器人工具。每个数据中心基于NVIDIA Jetson Orin NX构建，支持多个接口（HDMI、USB-C、以太网），并使用Isaac ROS框架和ROS2在Docker容器中封装设备特定驱动程序。一个集中的用户界面可实现简单的配置和实时监控，而Nvidia DGX计算机则提供最新颖的数据处理和存储功能。我们通过基于超声的3D解剖重建实验验证了我们的方法，该实验结合了医学影像、姿态跟踪和RGB-D数据采集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14049v3">PDF</a> Accepted for the Hamlyn Symposium, London, June 2025</p>
<p><strong>Summary</strong><br>     未来手术护理需求实时集成数据以推动决策并改善患者结果。为解决手术室中无缝高效数据采集的迫切需求，我们开发了一种模块化解决方案，该方案能够弥接新兴机器学习技术与介入医学之间的鸿沟。我们推出名为数据中心的边缘设备网络，通过光纤和中央网络交换机互联各种医疗传感器、成像系统和机器人工具。每个数据中心基于NVIDIA Jetson Orin NX构建，支持多个接口（HDMI、USB-C、以太网），并在Docker容器中使用Isaac ROS框架和ROS2封装设备特定驱动程序。集中式用户界面可实现简易配置和实时监控，而NVIDIA DGX计算机则提供先进的数据处理和存储功能。我们通过结合医学成像、姿态追踪和RGB-D数据采集的超声三维重建实验验证了我们的方法。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>未来手术护理需要实时集成数据以提高决策效率和改善患者结果。</li>
<li>开发了一种模块化解决方案，将新兴机器学习技术与介入医学相结合。</li>
<li>引入名为数据中心的边缘设备网络，可以互联医疗传感器、成像系统和机器人工具。</li>
<li>数据中心基于NVIDIA Jetson Orin NX构建，提供多种接口支持并在Docker容器中封装设备驱动程序。</li>
<li>集中式用户界面简化了配置和实时监控过程。</li>
<li>NVIDIA DGX计算机提供了先进的数据处理和存储功能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f7397683a3d8492f273e889ba8bdfbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc6c876e5d3e590b431ace977f9f560.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Latent-Drifting-in-Diffusion-Models-for-Counterfactual-Medical-Image-Synthesis"><a href="#Latent-Drifting-in-Diffusion-Models-for-Counterfactual-Medical-Image-Synthesis" class="headerlink" title="Latent Drifting in Diffusion Models for Counterfactual Medical Image   Synthesis"></a>Latent Drifting in Diffusion Models for Counterfactual Medical Image   Synthesis</h2><p><strong>Authors:Yousef Yeganeh, Azade Farshad, Ioannis Charisiadis, Marta Hasny, Martin Hartenberger, Björn Ommer, Nassir Navab, Ehsan Adeli</strong></p>
<p>Scaling by training on large datasets has been shown to enhance the quality and fidelity of image generation and manipulation with diffusion models; however, such large datasets are not always accessible in medical imaging due to cost and privacy issues, which contradicts one of the main applications of such models to produce synthetic samples where real data is scarce. Also, fine-tuning pre-trained general models has been a challenge due to the distribution shift between the medical domain and the pre-trained models. Here, we propose Latent Drift (LD) for diffusion models that can be adopted for any fine-tuning method to mitigate the issues faced by the distribution shift or employed in inference time as a condition. Latent Drifting enables diffusion models to be conditioned for medical images fitted for the complex task of counterfactual image generation, which is crucial to investigate how parameters such as gender, age, and adding or removing diseases in a patient would alter the medical images. We evaluate our method on three public longitudinal benchmark datasets of brain MRI and chest X-rays for counterfactual image generation. Our results demonstrate significant performance gains in various scenarios when combined with different fine-tuning schemes. </p>
<blockquote>
<p>通过在大规模数据集上进行训练，已经证明可以提高扩散模型在图像生成和操纵方面的质量和保真度。然而，在医学成像中，由于成本和隐私问题，并非总能获取到这样的大规模数据集，这与此类模型在真实数据稀缺的情况下产生合成样本的主要应用相矛盾。此外，由于医学领域与预训练模型之间的分布转移问题，对预训练的通用模型进行微调一直是一个挑战。在这里，我们为扩散模型提出了潜在漂移（LD）技术，该技术可以被任何微调方法所采用，以缓解由分布转移所面临的问题，或在推理时间作为条件被采用。潜在漂移技术使得扩散模型能够对医学图像进行适配，适应复杂的反事实图像生成任务，这对于研究参数如性别、年龄以及给患者添加或移除疾病后如何改变医学图像至关重要。我们在三个公开的纵向基准数据集上对我们的方法进行了评估，这些数据集包括用于反事实图像生成的脑MRI和胸部X射线。我们的结果表明，在不同的微调方案组合下，在各种场景中均实现了显著的性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20651v2">PDF</a> Accepted to CVPR 2025 (highlight)</p>
<p><strong>Summary</strong><br>     训练大规模数据集能提高扩散模型在图像生成和操纵方面的质量和保真度，但在医学成像领域，由于成本和隐私问题，并不总能获取到如此大规模的数据集。此外，由于医学领域与预训练模型之间的分布转移问题，微调预训练的通用模型也面临挑战。为此，我们提出了扩散模型的Latent Drift（LD）方法，可应用于任何微调方法来缓解分布转移问题或在推理时间作为条件使用。Latent Drifting使扩散模型能够进行医疗图像的条件适配，适用于复杂任务如反事实图像生成，这对于研究参数如性别、年龄以及增加或减少患者疾病等如何改变医疗图像至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过在大规模数据集上进行训练，可以提高图像生成和操纵的质量。</li>
<li>在医学成像领域，获取大规模数据集存在成本和隐私方面的挑战。</li>
<li>分布转移问题是将预训练模型应用于医学领域时面临的挑战之一。</li>
<li>提出了Latent Drift（LD）方法，可以缓解分布转移问题，并应用于任何微调方法。</li>
<li>Latent Drifting使扩散模型能够进行医疗图像的条件适配，适用于复杂任务如反事实图像生成。</li>
<li>反事实图像生成有助于研究参数变化如何影响医疗图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20651">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c1fa8632825f0f2a162bec1127b41719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e579183d9aa6bb3ac59e9a4b36aab8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9390063ffe61d61b6984c939327fda04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06fda529606e0f64f11c523c2526dd4c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-12bd037f52fa33fa3283c63a8f71c172.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-15  Generalized Multilingual Text-to-Speech Generation with Language-Aware   Style Adaptation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4c7675003f5763fde5e7dd7089fd5379.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-15  COP-GEN-Beta Unified Generative Modelling of COPernicus Imagery   Thumbnails
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18588k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
