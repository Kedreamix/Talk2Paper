<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9622a0e405fc2a4437ce21c7de2d4c72.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    43 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-15-更新"><a href="#2025-04-15-更新" class="headerlink" title="2025-04-15 更新"></a>2025-04-15 更新</h1><h2 id="DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation"><a href="#DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation" class="headerlink" title="DocAgent: A Multi-Agent System for Automated Code Documentation   Generation"></a>DocAgent: A Multi-Agent System for Automated Code Documentation   Generation</h2><p><strong>Authors:Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang</strong></p>
<p>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories. </p>
<blockquote>
<p>高质量的代码文档对软件开发至关重要，特别是在人工智能时代。然而，使用大型语言模型（LLM）自动生成文档仍然具有挑战性，因为现有方法通常会产生不完整、无帮助或事实错误的输出。我们引入了DocAgent，这是一个使用拓扑代码处理进行增量上下文构建的新型多智能体协作系统。专门的智能体（阅读器、搜索器、写入器、验证器、协调器）协同生成文档。我们还提出了一个多方面的评估框架，评估文档的完整性、帮助性和真实性。综合实验表明，DocAgent持续且显著地优于基线。我们的消融研究证实了拓扑处理顺序的重要作用。DocAgent为复杂和专有存储库中的可靠代码文档生成提供了稳健的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08725v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文强调高质量代码文档在软件开发中的重要性，特别是在人工智能时代。然而，使用大型语言模型（LLMs）自动生成文档仍然具有挑战性，因为现有方法常常产生不完整、无帮助或事实错误的输出。为此，本文提出了DocAgent，这是一种新型的多智能体协作系统，采用拓扑代码处理进行增量上下文构建。通过专门的智能体（阅读器、搜索器、编写器、验证器、协调器）协同生成文档。同时，提出了一种多方面的评估框架，评估文档的完整性、帮助性和真实性。实验表明，DocAgent在复杂和专有存储库中显著优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量代码文档在软件开发中的重要性，特别是在人工智能时代。</li>
<li>使用大型语言模型（LLMs）自动生成代码文档的挑战性，现有方法的不足。</li>
<li>DocAgent是一种多智能体协作系统，采用拓扑代码处理进行增量上下文构建。</li>
<li>DocAgent包括多个专门智能体：阅读器、搜索器、编写器、验证器和协调器。</li>
<li>提出了一个多面的评估框架，评估文档的完整性、帮助性和真实性。</li>
<li>实验表明，DocAgent在生成代码文档方面显著优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08725">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-84dea2119fbe38fca6011cfa93c867fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fd0534526e9f43e1ed4683ac785090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933e5dcbc145e1cb1361f03356dd2b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28680bc575a8c0f69c2ab5c8868cc5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cae1fe7424320522f3e73e14023dd673.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Genius-A-Generalizable-and-Purely-Unsupervised-Self-Training-Framework-For-Advanced-Reasoning"><a href="#Genius-A-Generalizable-and-Purely-Unsupervised-Self-Training-Framework-For-Advanced-Reasoning" class="headerlink" title="Genius: A Generalizable and Purely Unsupervised Self-Training Framework   For Advanced Reasoning"></a>Genius: A Generalizable and Purely Unsupervised Self-Training Framework   For Advanced Reasoning</h2><p><strong>Authors:Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, Zhiyong Wu</strong></p>
<p>Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/xufangzhi/Genius">https://github.com/xufangzhi/Genius</a>. </p>
<blockquote>
<p>推进LLM推理技能已经引起了广泛的关注。然而，当前的训练后技术很大程度上依赖于监督信号，如结果监督或辅助奖励模型，这面临着可扩展性和高标注成本的问题。这激励我们在不需要外部监督的情况下提高LLM的推理能力。我们引入了一个通用且纯粹的无监督自训练框架，名为Genius。无需外部辅助，Genius需要以逐步的方式寻找最佳响应序列并优化LLM。为了探索潜在的步骤并利用最佳的步骤，Genius引入了一种逐步前瞻性重新采样策略，通过模拟未来结果来采样并估算步骤值。此外，我们认识到无监督设置不可避免地会引发内在噪声和不确定性。为了提供稳健的优化，我们提出了一种优势校准优化（ACO）损失函数来缓解估计不一致的问题。结合这些技术，Genius是朝着无需监督的自我改进LLM推理方向迈出的初步一步，具有处理通用查询的能力，从而改变了推理扩展定律，利用大量可用的通用查询来进行革命性的进步。代码将在<a target="_blank" rel="noopener" href="https://github.com/xufangzhi/Genius%E4%B8%8A%E5%B8%83%E5%B0%8F%E3%80%82">https://github.com/xufangzhi/Genius上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08672v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>推进LLM推理技能引起了广泛关注。然而，当前的后训练技术严重依赖于监督信号，如结果监督或辅助奖励模型，这面临着可扩展性和高标注成本的问题。因此，我们提出了一种无需外部监督的通用纯自训练框架Genius，以提高LLM的推理能力。Genius通过逐步寻找最佳响应序列来优化LLM，无需外部辅助。为了探索潜在的步骤并利用最佳步骤，Genius引入了一种逐步预见重采样策略，通过模拟未来结果来采样并估算步骤值。此外，我们认识到无监督设置不可避免地会引发内在噪声和不确定性。为了进行稳健优化，我们提出了一种优势校准优化（ACO）损失函数，以减轻估计不一致的问题。结合这些技术，Genius为在没有监督的情况下自我改进LLM推理能力提供了先进的初步步骤，对于通用查询具有革命性的推理扩展定律。相关代码将发布在<a target="_blank" rel="noopener" href="https://github.com/xufangzhi/Genius%E3%80%82">https://github.com/xufangzhi/Genius。</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>当前LLM推理技能提升依赖于监督信号，存在可扩展性和高标注成本问题。</li>
<li>Genius框架是一种通用、无需外部监督的纯自训练方式，旨在提高LLM的推理能力。</li>
<li>Genius通过逐步寻找最佳响应序列进行优化，并引入逐步预见重采样策略来探索潜在步骤。</li>
<li>无监督设置引发内在噪声和不确定性，因此提出优势校准优化（ACO）损失函数进行稳健优化。</li>
<li>Genius对于在没有监督的情况下改进LLM推理能力具有重要意义，特别是在处理通用查询时。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5b5b1e1e0cea56d6566c1f7fa0ccd437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96abf154e05f9af418d911c46472477c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5bff4a3e511dc7139edc4fd39e61745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c47c01cceb83fb4e6bfc8c7183c9733.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Gen3DEval-Using-vLLMs-for-Automatic-Evaluation-of-Generated-3D-Objects"><a href="#Gen3DEval-Using-vLLMs-for-Automatic-Evaluation-of-Generated-3D-Objects" class="headerlink" title="Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects"></a>Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects</h2><p><strong>Authors:Shalini Maiti, Lourdes Agapito, Filippos Kokkinos</strong></p>
<p>Rapid advancements in text-to-3D generation require robust and scalable evaluation metrics that align closely with human judgment, a need unmet by current metrics such as PSNR and CLIP, which require ground-truth data or focus only on prompt fidelity. To address this, we introduce Gen3DEval, a novel evaluation framework that leverages vision large language models (vLLMs) specifically fine-tuned for 3D object quality assessment. Gen3DEval evaluates text fidelity, appearance, and surface quality by analyzing 3D surface normals, without requiring ground-truth comparisons, bridging the gap between automated metrics and user preferences. Compared to state-of-the-art task-agnostic models, Gen3DEval demonstrates superior performance in user-aligned evaluations, placing it as a comprehensive and accessible benchmark for future research on text-to-3D generation. The project page can be found here: \href{<a target="_blank" rel="noopener" href="https://shalini-maiti.github.io/gen3deval.github.io/%7D%7Bhttps://shalini-maiti.github.io/gen3deval.github.io/%7D">https://shalini-maiti.github.io/gen3deval.github.io/}{https://shalini-maiti.github.io/gen3deval.github.io/}</a>. </p>
<blockquote>
<p>随着文本到3D生成的快速发展，我们需要强大且可扩展的评估指标，这些指标需要紧密符合人类的判断，而当前诸如PSNR和CLIP等评价指标无法满足这一需求。这些指标要么需要真实数据，要么只关注提示的忠实度。为了解决这个问题，我们引入了Gen3DEval，这是一个新的评估框架，它利用专门用于3D对象质量评估的视觉大型语言模型（vLLMs）。Gen3DEval通过分析3D表面法线来评估文本的忠实度、外观和表面质量，无需真实数据的对比，在自动度量与用户偏好之间搭建了桥梁。与最先进的任务无关模型相比，Gen3DEval在用户对齐的评估中表现出卓越的性能，成为未来文本到3D生成研究全面且易于访问的基准测试。项目页面可在此找到：[<a target="_blank" rel="noopener" href="https://shalini-maiti.github.io/gen3deval.github.io/]">https://shalini-maiti.github.io/gen3deval.github.io/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08125v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>随着文本到3D生成的快速发展，现有的评估指标如PSNR和CLIP已无法满足需求。为此，我们推出Gen3DEval评估框架，利用特定微调用于评估3D物体质量的视觉大型语言模型（vLLMs）。Gen3DEval可分析无需真实对比的3D表面法线，评估文本忠实度、外观和表面质量，缩小了自动化指标与用户偏好之间的差距。相较于现有任务无关模型，Gen3DEval在用户评价中表现更佳，为未来文本到3D生成研究提供了全面且易于使用的基准。项目页面位于：<a target="_blank" rel="noopener" href="https://shalini-maiti.github.io/gen3deval.github.io/">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前文本到3D生成的评估指标存在局限性，需要更完善的评估方法。</li>
<li>Gen3DEval利用视觉大型语言模型（vLLMs）评估3D物体质量，为市场提供了新颖的评价框架。</li>
<li>Gen3DEval可分析3D表面法线，评估文本的忠实度、外观和表面质量。</li>
<li>Gen3DEval无需真实对比数据即可进行评估。</li>
<li>Gen3DEval缩小了自动化评估指标与用户偏好之间的差距。</li>
<li>Gen3DEval在用户评价中表现优于现有任务无关模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-665f48ab091aefe69ec9d1091ccba08a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751d5abeb828dfb9173de94ede867e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11b748edaccc1dc809131cea05a4e1fa.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepSeek-vs-o3-mini-How-Well-can-Reasoning-LLMs-Evaluate-MT-and-Summarization"><a href="#DeepSeek-vs-o3-mini-How-Well-can-Reasoning-LLMs-Evaluate-MT-and-Summarization" class="headerlink" title="DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?"></a>DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?</h2><p><strong>Authors:Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger</strong></p>
<p>Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use. </p>
<blockquote>
<p>具备推理功能的大型语言模型（LLMs）最近在复杂的逻辑和数学任务中表现出了令人印象深刻的性能，然而它们在评估自然语言生成方面的有效性仍未被探索。本研究系统地比较了基于推理的LLMs（DeepSeek-R1和OpenAI o3）与其在非推理任务中的对应模型在机器翻译（MT）和文本摘要（TS）评估任务中的表现。我们评估了三种架构类别中的八个模型，包括最先进的推理模型、它们的蒸馏变体（参数范围从8B到70B），以及等效的传统非推理LLMs。我们在WMT23和SummEval基准测试上的实验表明，推理能力的益处高度依赖于模型和任务：虽然OpenAI o3-mini模型显示出随着推理强度的增加而持续的性能改进，但DeepSeek-R1在其非推理变体上的表现较差，但在TS评估的某些方面除外。相关性分析表明，在o3-mini模型中，推理令牌使用量的增加与评估质量呈正相关。此外，我们的结果表明，在中等大小模型（32B）中蒸馏推理能力可以保持合理的性能，但在较小变体（8B）中会显著下降。这项工作首次全面评估了用于NLG评价的推理LLMs，并为其实践应用提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于自然语言理解和推理的大型语言模型（LLMs）在逻辑和数学任务上表现优异，但它们对于自然语言生成（NLG）的评价效果尚未被探索。本研究系统地比较了基于推理的LLMs（DeepSeek-R1和OpenAI o3）与非推理LLMs在机器翻译（MT）和文本摘要（TS）评价任务上的表现。实验结果显示，推理能力的优势在很大程度上取决于模型和任务。OpenAI o3-mini模型在增强推理强度后表现一致地更好，而DeepSeek-R1在非推理变体上表现较差，但在文本摘要评价的某些方面表现较好。此外，推理代币的使用与评估质量之间存在正相关关系。同时，研究还发现，在中等规模模型中保持推理能力的蒸馏表现良好，但在小型模型中性能会大幅下降。本研究首次全面评估了用于NLG评价的推理LLMs，并为其实践应用提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在逻辑和数学任务上的表现已经相当出色，但在自然语言生成（NLG）评价方面的效果尚未得到充分探索。</li>
<li>对比了基于推理的LLMs（如DeepSeek-R1和OpenAI o3）与非推理LLMs在机器翻译和文本摘要评价任务上的表现。</li>
<li>推理能力的优势在模型和任务上具有依赖性。OpenAI o3-mini模型在增强推理强度后表现更好，而DeepSeek-R1在某些文本摘要评价方面表现较好，但在总体上不如其非推理变体。</li>
<li>推理代币的使用与评估质量之间存在正相关关系，这意味着更多的推理代币可能意味着更高的评估准确性。</li>
<li>在中等规模模型中，通过蒸馏保留推理能力可以保持良好的性能。但在小型模型中，这种性能会大幅下降。</li>
<li>本研究首次全面评估了用于NLG评价的推理LLMs，为未来的研究提供了重要的参考依据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08120">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-30db7b845bd510c2decc93acf3460f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0bf230be48c6469d53ec115e4f6edcf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-Reasoning-LLMs-Enhance-Clinical-Document-Classification"><a href="#Can-Reasoning-LLMs-Enhance-Clinical-Document-Classification" class="headerlink" title="Can Reasoning LLMs Enhance Clinical Document Classification?"></a>Can Reasoning LLMs Enhance Clinical Document Classification?</h2><p><strong>Authors:Akram Mustafa, Usman Naseem, Mostafa Rahimi Azghadi</strong></p>
<p>Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications. </p>
<blockquote>
<p>临床文档分类对于将非结构化医疗文本转换为标准化的ICD-10诊断至关重要，但由于复杂的医疗语言、隐私约束和有限的有标注数据集，它面临着挑战。大型语言模型（LLM）为这个任务提供了提高准确性和效率的潜力。本研究评估了8种LLM的性能和一致性；四个推理模型（Qwen QWQ、Deepseek Reasoner、GPT o3 Mini、Gemini 2.0 Flash Thinking）和四个非推理模型（Llama 3.3、GPT 4o Mini、Gemini 2.0 Flash、Deepseek Chat）；使用MIMIC-IV数据集对临床出院摘要进行分类。使用cTAKES对临床叙述进行结构化处理，对模型进行了三次实验运行评估，以多数投票决定最终预测。结果表明，推理模型在准确性和F1分数方面优于非推理模型（准确率71%比68%，F1分数67%比60%），其中Gemini 2.0 Flash Thinking的准确率和F1分数最高（分别为75%和76%）。然而，非推理模型表现出更大的稳定性（一致性91%比84%）。不同ICD-10代码的性能有所不同，推理模型在复杂情况下表现出色，但在抽象类别中表现不佳。研究结果表明，需要在准确性和一致性之间进行权衡，提示采用混合方法可能最适用于临床编码。未来的研究应探索多标签分类、特定领域的微调以及集成方法，以提高模型在现实应用中的可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08040v1">PDF</a> 28 pages, 13 tables, 12 figures</p>
<p><strong>Summary</strong></p>
<p>该文研究了大型语言模型（LLMs）在医疗文本分类任务中的应用，特别是在将非结构化医疗文本转换为标准化的ICD-10诊断中的应用。实验评估了八种LLMs的性能和一致性，包括四种推理模型（Qwen QWQ等）和四种非推理模型（Llama 3.3等）。使用MIMIC-IV数据集和cTAKES工具进行临床叙述结构化处理，结果显示推理模型在准确性和F1得分上优于非推理模型，但非推理模型在稳定性方面表现更好。文章探讨了模型性能与ICD-10代码之间的变化，并提出了采用混合方法的优化方向。未来研究应关注多标签分类、特定领域的微调以及集成方法以提高模型在现实应用中的可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>临床文档分类是将非结构化医疗文本转换为ICD-10诊断的关键过程。</li>
<li>大型语言模型（LLMs）可以提高临床文档分类的准确性和效率。</li>
<li>推理模型在分类准确性上优于非推理模型，但非推理模型在稳定性方面表现更好。</li>
<li>Gemini 2.0 Flash Thinking模型在准确率和F1得分上表现最佳。</li>
<li>不同ICD-10代码下的模型性能有所差异，推理模型在处理复杂案例时表现出色，但在抽象类别上遇到困难。</li>
<li>存在准确性与一致性之间的权衡，提出采用混合方法来优化临床编码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-27ad85b10aa140d1cafa5b60eaab8169.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Holistic-Capability-Preservation-Towards-Compact-Yet-Comprehensive-Reasoning-Models"><a href="#Holistic-Capability-Preservation-Towards-Compact-Yet-Comprehensive-Reasoning-Models" class="headerlink" title="Holistic Capability Preservation: Towards Compact Yet Comprehensive   Reasoning Models"></a>Holistic Capability Preservation: Towards Compact Yet Comprehensive   Reasoning Models</h2><p><strong>Authors: Ling Team, Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang</strong></p>
<p>This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill’s reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at <a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a> </p>
<blockquote>
<p>本技术报告介绍了Ring-Lite-Distill，这是一个轻量级的推理模型，源于我们开源的混合专家（MoE）大型语言模型（LLM）Ling-Lite。这项研究表明，通过精心的高质量数据收集和巧妙的训练模式，紧凑的MoE模型Ling-Lite可以进一步训练，实现出色的推理能力，同时保持其参数高效的架构，仅有2.75亿个激活参数，建立了有效的轻量化推理架构。在构建此模型时，我们不仅仅专注于提高解决高难度数学问题的先进推理能力，而是致力于开发具有更全面能力覆盖范围的推理模型。我们的方法确保了不同难度级别的推理任务的覆盖，同时保留了一般能力，例如指令遵循、工具使用和知识保留。我们证明，Ring-Lite-Distill的推理能力与DeepSeek-R1-Distill-Qwen-7B相当，而其一般能力则显著超过了DeepSeek-R1-Distill-Qwen-7B。模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07158v2">PDF</a> Based on the further discussion of the working group, the current   version is deemed unsuitable for release. We are currently undertaking   further work that is expected to involve significant revisions, but this   process will require some additional time. We plan to proceed with the   release once these updates have been fully implemented</p>
<p><strong>Summary</strong></p>
<p>Ring-Lite-Distill模型是Mixture-of-Experts（MoE）的一种轻量级推理模型，基于开源的大型语言模型Ling-Lite。该模型通过高质量数据整合和创新训练模式，实现了高效的参数利用，拥有出色的推理能力，同时仅包含2.75亿激活参数。该模型不仅关注高难度数学问题的推理能力，还致力于发展具有全面能力的推理模型，确保覆盖不同难度的推理任务，并保留通用能力，如指令遵循、工具使用和知识保留。其推理能力与DeepSeek-R1-Distill-Qwen-7B相当，但一般能力显著超越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ring-Lite-Distill是一个基于Mixture-of-Experts的轻量级推理模型。</li>
<li>该模型由开源的大型语言模型Ling-Lite衍生而来。</li>
<li>通过高质量数据整合和创新训练模式，Ling-Lite进一步训练后展现出卓越的推理能力。</li>
<li>该模型仅包含2.75亿激活参数，实现了高效的参数利用。</li>
<li>Ring-Lite-Distill不仅关注高难度数学问题的推理，还具备全面的能力覆盖。</li>
<li>该模型能够应对不同难度的推理任务，同时保留通用能力，如指令遵循、工具使用和知识保留。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d54d371caf577f988b38227b5c15d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df81c4c0da0f66bbf4c6217e0376fab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d1788c5eaee4f28b7d5fbe8c389e635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6eefcbf9684755ca8ef8e2d00a1161b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50695317975e22051bfa8203d963dc0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks"><a href="#VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks" class="headerlink" title="VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks"></a>VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks</h2><p><strong>Authors:Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, Lin Yan</strong></p>
<p>We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of $\mathbf{60.4}$. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks. </p>
<blockquote>
<p>我们提出了VAPO，基于价值的增强近端策略优化框架（适用于推理模型）。这是一个针对基于价值的范式中的推理模型的新型框架。使用AIME 2024数据集进行基准测试，VAPO建立在Qwen 32B预训练模型上，达到了最先进的60.4分。在相同的实验设置下直接比较，VAPO的表现优于先前报道的DeepSeek-R1-Zero-Qwen-32B和DAPO的结果超过10分。VAPO的训练过程以稳定性和高效性而脱颖而出。它在短短5000步内达到了最先进的性能。此外，在多次独立运行中，没有发生训练崩溃，证明了其可靠性。本研究深入探讨了基于价值强化学习框架的长链思维（long-CoT）推理。我们确定了困扰基于价值方法的三个关键挑战：价值模型偏见、存在不同的序列长度以及奖励信号的稀疏性。通过系统设计，VAPO提供了一个综合解决方案，有效地缓解了这些挑战，能够在长链思维推理任务中提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05118v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>我们提出了针对推理模型的价值基础增强近端策略优化框架VAPO，该框架在价值基础上为推理模型定制。在AIME 2024数据集上进行了基准测试，VAPO建立在Qwen 32B预训练模型上，达到了最先进的60.4分。在相同的实验设置下，VAPO的表现优于DeepSeek-R1-Zero-Qwen-32B和DAPO超过10分。VAPO的训练过程以稳定性和高效性著称，它在短短5000步内达到了最先进的性能。此外，在多次独立运行中，没有出现训练崩溃，证明了其可靠性。本研究深入探讨了价值强化学习框架中的长链思维（long-CoT）推理，并指出了价值基础方法中的三个关键挑战：价值模型偏见、不同序列长度的存在和奖励信号的稀疏性。通过系统设计，VAPO提供了一个有效的综合解决方案来缓解这些挑战，从而在长链思维推理任务中实现了卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>VAPO是基于价值基础的增强近端策略优化框架，专为推理模型设计。</li>
<li>在AIME 2024数据集上，VAPO达到最先进的60.4分。</li>
<li>VAPO在相同实验设置下表现出色，优于其他模型。</li>
<li>VAPO训练过程稳定高效，可在5000步内达到先进性能。</li>
<li>在多次独立运行中，VAPO表现出极高的可靠性。</li>
<li>研究指出了价值基础方法中的三个关键挑战，包括价值模型偏见、不同序列长度的存在和奖励信号的稀疏性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0f3e83983b75dfb8a11dd597d9a879b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>大规模语言模型（LLMs）在推进科学知识和应对复杂挑战方面展现出了显著潜力。在这项工作中，我们介绍了OmniScience，这是一个针对通用科学的专业化大型推理模型，通过三个关键组件开发而成：（1）在精心筛选的科学文献语料库上进行领域自适应预训练；（2）在专门的数据集上进行指令调整，以指导模型执行特定领域的任务；（3）通过微调进行基于推理的知识蒸馏，以显著提高其生成语境相关和逻辑严谨回应的能力。我们通过开发一种电池代理来展示OmniScience的通用性，该代理能够高效地对分子进行排名，作为潜在的电解质溶剂或添加剂。综合评估表明，OmniScience在GPQA Diamond和特定领域的电池基准测试上与最先进的大型推理模型相竞争，同时在参数数量相似的所有公共推理和非推理模型中表现最佳。我们还通过消融实验进一步证明，领域自适应预训练和基于推理的知识蒸馏对于实现我们的性能水平至关重要，跨越各种基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在推动科学知识和应对复杂挑战方面展现出显著潜力。本研究介绍了一款针对通用科学的特殊大型推理模型OmniScience，其开发包括三个关键部分：对科学文献精心筛选的语料库进行领域自适应预训练、在专门数据集上进行指令调整以指导模型执行特定任务，以及通过微调进行基于推理的知识蒸馏，以显著提高其生成上下文相关和逻辑严谨响应的能力。OmniScience的通用性通过开发电池代理得以展示，该代理能够高效地排列分子作为潜在的电解质溶剂或添加剂。综合评估表明，OmniScience在GPQA Diamond和特定电池基准测试上与最新大型推理模型竞争，同时在参数数量相似的公共推理和非推理模型中表现最佳。通过消融实验进一步证明，领域自适应预训练和基于推理的知识蒸馏对于达到我们的性能水平至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在推进科学和应对复杂挑战中具有显著潜力。</li>
<li>OmniScience是一个针对通用科学的特殊大型推理模型。</li>
<li>OmniScience的开发包括领域自适应预训练、指令调整以及基于推理的知识蒸馏三个关键部分。</li>
<li>OmniScience展示了在电池代理开发中的通用性，能高效排列分子作为潜在电解质溶剂或添加剂。</li>
<li>综合评估显示，OmniScience在多个基准测试中具有竞争力。</li>
<li>消融实验证明领域自适应预训练和基于推理的知识蒸馏对OmniScience的性能至关重要。</li>
<li>OmniScience的开发和评估为大型语言模型在特定领域的应用提供了重要参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d6944cf303fea9494c3ca74eadb42e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning"><a href="#An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning" class="headerlink" title="An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning"></a>An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning</h2><p><strong>Authors:Navdeep Kaur, Lachlan McPheat, Alessandra Russo, Anthony G Cohn, Pranava Madhyastha</strong></p>
<p>In this paper, we examine the use of Conformal Language Modelling (CLM) alongside Answer Set Programming (ASP) to enhance the performance of standard open-weight LLMs on complex multi-step reasoning tasks. Using the StepGame dataset, which requires spatial reasoning, we apply CLM to generate sets of ASP programs from an LLM, providing statistical guarantees on the correctness of the outputs. Experimental results show that CLM significantly outperforms baseline models that use standard sampling methods, achieving substantial accuracy improvements across different levels of reasoning complexity. Additionally, the LLM-as-Judge metric enhances CLM’s performance, especially in assessing structurally and logically correct ASP outputs. However, calibrating CLM with diverse calibration sets did not improve generalizability for tasks requiring much longer reasoning steps, indicating limitations in handling more complex tasks. </p>
<blockquote>
<p>在这篇论文中，我们研究了将符合语言建模（CLM）与答案集编程（ASP）结合使用，以提高标准开放权重大型语言模型在复杂多步骤推理任务上的性能。我们利用需要空间推理的StepGame数据集，将CLM应用于从大型语言模型生成ASP程序集，为输出结果的正确性提供统计保证。实验结果表明，与传统的采样方法相比，CLM显著提高了基线模型的性能，在不同层次的推理复杂度上都实现了实质性的准确性提高。此外，大型语言模型作为评判者的指标提高了CLM的性能，特别是在评估结构上和逻辑上正确的ASP输出方面。然而，使用多种校准集对CLM进行校准并没有提高处理需要更多推理步骤的任务的通用性，这表明在处理更复杂的任务时存在局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了将Conformal语言建模（CLM）与Answer Set Programming（ASP）结合使用，以改善标准开放式大型语言模型（LLM）在复杂多步骤推理任务上的性能。文章使用StepGame数据集来要求空间推理能力，通过CLM生成ASP程序的集合，对输出的正确性提供统计保证。实验结果表明，CLM显著优于使用标准采样方法的基线模型，在不同层次的推理复杂度上实现了实质性的准确性提高。然而，对于需要更长时间推理步骤的任务，使用多种校准集校准CLM并没有提高通用性，这表明其在处理更复杂的任务方面存在局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Conformal语言建模（CLM）与Answer Set Programming（ASP）结合使用，增强了大型语言模型（LLM）在复杂多步骤推理任务上的性能。</li>
<li>使用StepGame数据集来测试模型的空间推理能力。</li>
<li>CLM可以生成ASP程序的集合，并为输出的正确性提供统计保证。</li>
<li>实验表明，CLM相较于基线模型在准确性上有显著提高。</li>
<li>LLM-as-Judge指标提高了CLM在评估ASP输出结构和逻辑正确性方面的性能。</li>
<li>使用多种校准集对CLM进行校准并未在提高需要更长时间推理步骤的任务的通用性方面显示出效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f87f0f4665100c6d9270c099f5c50cf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EmbodiedEval-Evaluate-Multimodal-LLMs-as-Embodied-Agents"><a href="#EmbodiedEval-Evaluate-Multimodal-LLMs-as-Embodied-Agents" class="headerlink" title="EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"></a>EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</h2><p><strong>Authors:Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at <a target="_blank" rel="noopener" href="https://github.com/thunlp/EmbodiedEval">https://github.com/thunlp/EmbodiedEval</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）已经取得了显著进展，为实体智能体提供了美好的未来前景。现有的评估MLLM的基准测试主要使用静态图像或视频，仅限于非交互式场景。同时，现有的实体人工智能基准测试是特定任务的，并不够多样化，不能充分评估MLLMs的实体能力。为了解决这一问题，我们提出了EmbodiedEval，这是一个全面且交互式的评估基准测试，适用于具有实体任务的MLLMs。EmbodiedEval在统一的模拟框架内拥有特定的标准程序包括通过精准的挑选和注释设计成的共涵盖导览、对象互动、社交互动等十二个主题的具有3D环境的任务的设定328项不同任务。该框架涵盖了广泛的现有实体人工智能任务，具有显著增强的多样性。任务分为五大类：导航、对象交互、社交交互、属性问答和空间问答，以评估智能体的不同能力。我们在EmbodiedEval上评估了最新的MLLMs，发现它们在实体任务上与人类水平存在明显差距。我们的分析揭示了现有MLLM在实体能力方面的局限性，为未来的开发提供了见解。我们公开所有评估数据和模拟框架<a target="_blank" rel="noopener" href="https://github.com/thunlp/EmbodiedEval%E3%80%82">https://github.com/thunlp/EmbodiedEval。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11858v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为EmbodiedEval的综合性交互式评估基准，用于对多模态大型语言模型（MLLMs）的实体能力进行评估。EmbodiedEval包含125个不同的3D场景中的328个独特任务，涵盖广泛的现有实体AI任务，具有显著增强的多样性，并为MLLMs量身定制了统一的模拟和评估框架。对现有MLLMs在EmbodiedEval上的评估表明，与人类水平相比，它们在实体任务上存在显著差距。这为未来MLLMs的发展提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在实体能力方面展现出显著进展。</li>
<li>当前评估基准主要利用静态图像或视频，限制在非交互式场景，无法充分评估MLLMs的实体能力。</li>
<li>EmbodiedEval是一个全面的交互式评估基准，旨在为MLLMs的实体能力进行评估。</li>
<li>EmbodiedEval包含328个独特任务，分布在125个不同的3D场景中，涵盖广泛的实体AI任务，并显著增强了多样性。</li>
<li>EmbodiedEval的任务分为五个类别：导航、物体交互、社交交互、属性问题回答和空间问题回答，以评估实体的不同能力。</li>
<li>对现有MLLMs在EmbodiedEval上的评估显示，它们在实体任务上的表现与人类水平存在显著差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11858">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-07336ceb162622cd249372c52c3c077c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b5faab522a2345ce0b57a5cda2797c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91b6bfa7a5b4d47937fca84099c7bd26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5650983f46a355fb8cc44ac6ac5dd00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9622a0e405fc2a4437ce21c7de2d4c72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec92c77c608b2de3e2246ff258e28b1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b93d3a2c16839c5eca1c782975d3bba.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ANSR-DT-An-Adaptive-Neuro-Symbolic-Learning-and-Reasoning-Framework-for-Digital-Twins"><a href="#ANSR-DT-An-Adaptive-Neuro-Symbolic-Learning-and-Reasoning-Framework-for-Digital-Twins" class="headerlink" title="ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for   Digital Twins"></a>ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for   Digital Twins</h2><p><strong>Authors:Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song</strong></p>
<p>In this paper, we propose an Adaptive Neuro-Symbolic Learning and Reasoning Framework for digital twin technology called &#96;&#96;ANSR-DT.” Digital twins in industrial environments often struggle with interpretability, real-time adaptation, and human input integration. Our approach addresses these challenges by combining CNN-LSTM dynamic event detection with reinforcement learning and symbolic reasoning to enable adaptive intelligence with interpretable decision processes. This integration enhances environmental understanding while promoting continuous learning, leading to more effective real-time decision-making in human-machine collaborative applications. We evaluated ANSR-DT on synthetic industrial data, observing significant improvements over traditional approaches, with up to 99.5% accuracy for dynamic pattern recognition. The framework demonstrated superior adaptability with extended reinforcement learning training, improving explained variance from 0.447 to 0.547. Future work aims at scaling to larger datasets to test rule management beyond the current 14 rules. Our open-source implementation promotes reproducibility and establishes a foundation for future research in adaptive, interpretable digital twins for industrial applications. </p>
<blockquote>
<p>本文提出了一种用于数字孪生技术的自适应神经符号学习与推理框架，名为“ANSR-DT”。工业环境中的数字孪生经常面临可解释性、实时自适应和人为输入集成方面的挑战。我们的方法通过结合CNN-LSTM动态事件检测与强化学习和符号推理来解决这些挑战，以实现具有可解释决策过程的自适应智能。这种集成增强了环境理解，同时促进了持续学习，从而在人机协作应用中实现了更有效的实时决策。我们在合成工业数据上评估了ANSR-DT，观察到与传统方法的显著改进，动态模式识别的准确率高达99.5%。该框架在扩展的强化学习训练中表现出出色的适应性，解释的方差从0.447提高到0.547。未来的工作旨在扩展到更大的数据集，以测试当前14条规则之外的规则管理。我们的开源实现促进了可重复性，并为未来在自适应、可解释的数字孪生工业应用方面的研究奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08561v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于数字孪生技术的自适应神经符号学习与推理框架研究，提出了名为“ANSR-DT”的框架。该框架解决了数字孪生在工业环境中面临的解释性、实时自适应和人类输入集成等挑战。通过结合CNN-LSTM动态事件检测、强化学习与符号推理，实现了具有可解释决策过程的自适应智能。在合成工业数据上的评估显示，与传统方法相比，ANSR-DT在动态模式识别方面实现了高达99.5%的准确率，并展现出卓越的适应性与更强的解释方差。未来工作将致力于扩大数据集规模，测试规则管理超越当前14条规则的应用场景。该开源实现促进了可重复性，并为未来研究自适应、可解释的工业应用数字孪生技术奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种名为“ANSR-DT”的自适应神经符号学习与推理框架，用于解决数字孪生技术在工业环境中的多重挑战。</li>
<li>结合CNN-LSTM动态事件检测与强化学习，增强了环境理解并促进了持续学习。</li>
<li>通过符号推理实现了可解释的决策过程，有助于提升实时决策效率在人机协作应用中。</li>
<li>在合成工业数据上的评估结果显示，ANSR-DT在动态模式识别上具有显著优势，准确率高达99.5%。</li>
<li>框架展现出卓越的适应性，通过扩展强化学习训练，解释方差得到了改善。</li>
<li>目前工作局限在于规则管理的数据集规模较小，未来研究将拓展至更大规模数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3fa4b77f049ca621f7643f878c86e071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe8c4fb4fb12644382663c097034ff06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f21722045e8010fd9a648d304a2734e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d54654987899b1e44aad9ef28aa8e61b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04c5e2b88b364667a0b7e6004077f691.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ff68f3b629529eea02d7b494c7b5137a.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-15  Steering CLIP's vision transformer with sparse autoencoders
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-13/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-461a70fadb968159e1382603dad2aba5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-13  An Adversarial Perspective on Machine Unlearning for AI Safety
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
