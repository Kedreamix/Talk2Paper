<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-04-15  Generalized Multilingual Text-to-Speech Generation with Language-Aware   Style Adaptation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e7134cc2dd1924582dd78f800e9375c6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    20 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-15-更新"><a href="#2025-04-15-更新" class="headerlink" title="2025-04-15 更新"></a>2025-04-15 更新</h1><h2 id="Generalized-Multilingual-Text-to-Speech-Generation-with-Language-Aware-Style-Adaptation"><a href="#Generalized-Multilingual-Text-to-Speech-Generation-with-Language-Aware-Style-Adaptation" class="headerlink" title="Generalized Multilingual Text-to-Speech Generation with Language-Aware   Style Adaptation"></a>Generalized Multilingual Text-to-Speech Generation with Language-Aware   Style Adaptation</h2><p><strong>Authors:Haowei Lou, Hye-young Paik, Sheng Li, Wen Hu, Lina Yao</strong></p>
<p>Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation. </p>
<blockquote>
<p>文本转语音（TTS）模型可以通过将音素转换为波形来生成多种自然语言、类似人类的语音。然而，由于音素词汇的差异以及跨语言的韵律和说话风格的差异，多语言TTS仍然具有挑战性。现有方法要么为每种语言训练单独模型（虽然计算资源消耗大但性能高），要么使用针对多种语言的统一模型（难以捕捉精细的语言特定风格变化）。在这项工作中，我们提出了LanStyleTTS，这是一个非自回归、语言感知的风格自适应TTS框架，它标准化了音素表示，并实现了跨语言的精细音素级风格控制。这种设计支持统一的多语言TTS模型，无需训练特定语言的模型即可产生准确且高质量的语音。我们将LanStyleTTS与几种最新的非自回归TTS架构进行集成，评估其性能。结果表明，在不同的模型主干上均实现了性能改进。此外，我们还研究了一系列声学特征表示，包括梅尔频谱和自动编码器衍生的潜在特征。我们的实验表明，潜在编码可以显著减小模型大小并降低计算成本，同时保持高质量语音生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08274v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种非自回归、语言感知的风格自适应文本转语音（TTS）框架——LanStyleTTS，用于标准化语音表现并支持跨语言的细粒度语音风格控制。该框架支持统一的多语言TTS模型，无需训练特定语言的模型即可产生准确、高质量的语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LanStyleTTS是一个非自回归、语言感知的TTS框架，可以生成自然、人类化的跨语言语音。</li>
<li>它通过转换语音为波形来支持多种语言。</li>
<li>LanStyleTTS解决了多语言TTS中的挑战，如语音词汇的差异和跨语言的韵律和说话风格的差异。</li>
<li>该框架提出了一个统一的多语言TTS模型，能够在无需训练特定语言模型的情况下，产生高质量语音。</li>
<li>LanStyleTTS通过标准化语音表现，实现了细粒度的语音风格控制。</li>
<li>实验表明，潜在编码可以显著减小模型大小并降低计算成本，同时保持高质量语音生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08274">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-12bd037f52fa33fa3283c63a8f71c172.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3c9e73525eea161bb72aa2776cb4961.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c9987ad0a2b2e5585084c543f1213be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ec5d28fa14aa7369d5040dbf71884a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39dd52695eb34eeecb3a1c65ce7fd519.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MathSpeech-Leveraging-Small-LMs-for-Accurate-Conversion-in-Mathematical-Speech-to-Formula"><a href="#MathSpeech-Leveraging-Small-LMs-for-Accurate-Conversion-in-Mathematical-Speech-to-Formula" class="headerlink" title="MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical   Speech-to-Formula"></a>MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical   Speech-to-Formula</h2><p><strong>Authors:Sieun Hyeon, Kyudan Jung, Jaehee Won, Nam-Joon Kim, Hyun Gon Ryu, Hyuk-Jae Lee, Jaeyoung Do</strong></p>
<p>In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Euler’s Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i $\textit{side}$ of x), instead of the concise $\LaTeX{}$ format (i.e., $ e^{ix} &#x3D; \cos(x) + i\sin(x) $), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured $\LaTeX{}$ representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates $\LaTeX{}$ generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for $\LaTeX{}$ translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-4o. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE&#x2F;BLEU scores compared to GPT-4o. </p>
<blockquote>
<p>在各种学术和专业场合，如数学讲座或研究报告会中，口头传达数学表达式是常见的需求。然而，在没有视觉辅助的情况下大声阅读数学表达式可能会显著阻碍理解，特别是对于听力受损或由于语言障碍依赖字幕的人来说。例如，当演讲者读出欧拉公式时，当前的自动语音识别（ASR）模型往往会生成冗长和易出错的文本描述（例如，“e的i乘以x次方等于x的余弦值加上i倍的x的正弦值”），而不是简洁的LaTeX格式（即，$ e^{ix} &#x3D; \cos(x) + i\sin(x) $）。这阻碍了清晰的理解和沟通。为了解决这一问题，我们引入了MathSpeech，这是一种新型管道，它将ASR模型与小语言模型（sLMs）集成在一起，以纠正数学表达式中的错误，并将口头表达式准确转换为结构化的LaTeX表示。在新的基于讲座录音的数据集上进行评估，MathSpeech展示了与领先的商业大型语言模型（LLMs）相当的LaTeX生成能力，同时利用仅包含1.2亿参数的微调小型语言模型。具体来说，在LaTeX翻译的CER、BLEU和ROUGE得分方面，MathSpeech相较于GPT-4o表现出了显著的优势。我们观察到CER从0.390降至0.298，并且相较于GPT-4o具有更高的ROUGE&#x2F;BLEU得分。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15655v3">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为MathSpeech的新技术，它通过结合ASR模型和较小的语言模型（sLMs）来提高数学表达式的语音识别精度和效率。它能准确地将口头表达的数学表达式转化为结构化的LaTeX格式，适用于数学讲座和研究报告等场景。在讲座录音衍生的新数据集上进行的评估显示，MathSpeech在LaTeX生成能力方面表现出与主流大型语言模型（LLMs）相当的竞争力，并且能利用仅包含12亿参数的精细调整的小型语言模型实现这一性能。相较于GPT-4o，MathSpeech的CER评分从原来的降低至更低的水平，而ROUGE和BLEU评分也显著提高了对于LaTeX的翻译质量。此外，它通过校正误差、促进理解的优势对于听力受损和语言障碍者特别有益。这一技术有助于提升学术和专业环境中的口头交流效率和准确性。总结不足百字：MathSpeech技术融合ASR模型和sLMs，将口头数学表达式准确转为LaTeX格式，显著提高理解力并改善沟通障碍问题。在测试集上性能优于GPT-4o。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MathSpeech技术结合了ASR模型和较小的语言模型（sLMs），用于处理口头表达的数学表达式。</li>
<li>该技术可将数学表达式准确转化为结构化的LaTeX格式，提升理解和沟通效率。</li>
<li>MathSpeech在LaTeX生成能力方面表现出显著优势，优于GPT-4o等传统模型。</li>
<li>该技术在听力受损和语言障碍人士的口头交流中具有潜在应用优势。其优越性体现在准确度改善，尤其在数学表达式识别方面效果突出。 </li>
<li>MathSpeech使用的语言模型具有精细调整的优势，参数仅为常规的十分之一不到（仅包含约十亿分之一的参数）。这降低了计算资源需求并可能促进更广泛的应用。 </li>
<li>MathSpeech技术在新的数据集上进行了评估，显示其在识别精确度上显著提高，特别是CER评分显著降低至低于现有模型水平。同时ROUGE和BLEU评分也显著提高，表明其在处理数学表达式的翻译质量方面有明显进步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3f060a30dc323257616b3fc782554b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e41436399815c63883bc6e6e6fa47aaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fe04d09c3d2e8627d2893fe5a32ba5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0acb571d174a59f80466c32bf42b741.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a9fdf00ac7682bfd03da42f5cea64eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-340f48b0bbd44afb8c12f4754fb5f3a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e57f6cd90fffb4d6e1e61f235d87a90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7134cc2dd1924582dd78f800e9375c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DiMoDif-Discourse-Modality-information-Differentiation-for-Audio-visual-Deepfake-Detection-and-Localization"><a href="#DiMoDif-Discourse-Modality-information-Differentiation-for-Audio-visual-Deepfake-Detection-and-Localization" class="headerlink" title="DiMoDif: Discourse Modality-information Differentiation for Audio-visual   Deepfake Detection and Localization"></a>DiMoDif: Discourse Modality-information Differentiation for Audio-visual   Deepfake Detection and Localization</h2><p><strong>Authors:Christos Koutlis, Symeon Papadopoulos</strong></p>
<p>Deepfake technology has rapidly advanced and poses significant threats to information integrity and trust in online multimedia. While significant progress has been made in detecting deepfakes, the simultaneous manipulation of audio and visual modalities, sometimes at small parts or in subtle ways, presents highly challenging detection scenarios. To address these challenges, we present DiMoDif, an audio-visual deepfake detection framework that leverages the inter-modality differences in machine perception of speech, based on the assumption that in real samples – in contrast to deepfakes – visual and audio signals coincide in terms of information. DiMoDif leverages features from deep networks that specialize in visual and audio speech recognition to spot frame-level cross-modal incongruities, and in that way to temporally localize the deepfake forgery. To this end, we devise a hierarchical cross-modal fusion network, integrating adaptive temporal alignment modules and a learned discrepancy mapping layer to explicitly model the subtle differences between visual and audio representations. Then, the detection model is optimized through a composite loss function accounting for frame-level detections and fake intervals localization. DiMoDif outperforms the state-of-the-art on the Deepfake Detection task by 30.5 AUC on the highly challenging AV-Deepfake1M, while it performs exceptionally on FakeAVCeleb and LAV-DF. On the Temporal Forgery Localization task, it outperforms the state-of-the-art by 47.88 <a href="mailto:&#x41;&#x50;&#64;&#x30;&#46;&#x37;&#x35;">&#x41;&#x50;&#64;&#x30;&#46;&#x37;&#x35;</a> on AV-Deepfake1M, and performs on-par on LAV-DF. Code available at <a target="_blank" rel="noopener" href="https://github.com/mever-team/dimodif">https://github.com/mever-team/dimodif</a>. </p>
<blockquote>
<p>深度伪造技术迅速发展和对在线多媒体的信息完整性和信任构成重大威胁。虽然深度伪造检测方面取得了重大进展，但同时对音频和视觉模式的操作，有时在小部分或以微妙的方式，呈现出极具挑战性的检测场景。为了解决这些挑战，我们提出了DiMoDif，一个音频视觉深度伪造检测框架，它利用机器感知语音的跨模态差异，基于真实样本（与深度伪造物相反）的视觉和音频信号在信息上是一致的。DiMoDif利用专门用于视觉和音频语音识别深度网络的特性，发现帧级跨模态不一致，以这种方式来暂时定位深度伪造篡改的位置。为此，我们设计了一个分层跨模态融合网络，集成了自适应时间对齐模块和学习差异映射层，以显式地模拟视觉和音频表示之间的细微差异。然后，通过考虑帧级检测和伪造间隔定位的复合损失函数来优化检测模型。DiMoDif在具有挑战性的AV-Deepfake1M上超越了最先进的技术，深度伪造检测任务的AUC提高了30.5%，在FakeAVCeleb和LAV-DF上表现尤为出色。在时间伪造定位任务上，它在AV-Deepfake1M上的<a href="mailto:&#65;&#80;&#x40;&#48;&#x2e;&#55;&#x35;">&#65;&#80;&#x40;&#48;&#x2e;&#55;&#x35;</a>超过了最先进的技术47.88%，并在LAV-DF上表现相当。代码可在<a target="_blank" rel="noopener" href="https://github.com/mever-team/dimodif%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mever-team/dimodif找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10193v2">PDF</a> </p>
<p><strong>摘要</strong><br>深度伪造技术迅速发展和对在线多媒体的信息完整性和信任构成严重威胁。尽管在检测深度伪造方面取得了重大进展，但对音频和视频模态的同时操纵，或以微小部分或微妙的方式操纵，呈现出了极具挑战性的检测场景。为了解决这些挑战，我们提出了DiMoDif，一个音频视觉深度伪造检测框架，它利用机器感知语音的跨模态差异，基于真实样本的假设——与深度伪造相比，视觉和音频信号在信息上是吻合的。DiMoDif利用深度网络中专门用于视觉和音频语音识别功能的特性，发现帧级跨模态不一致之处，以这种方式临时定位深度伪造篡改的位置。为此，我们设计了一个分层的跨模态融合网络，该网络集成了自适应时间对齐模块和学习到的差异映射层，以显式地模拟视觉和音频表示之间的细微差异。然后，通过考虑帧级检测和伪造间隔定位的复合损失函数来优化检测模型。DiMoDif在高度挑战的AV-Deepfake1M上，深度伪造检测任务的性能优于最新技术水平的30.5 AUC，在FakeAVCeleb和LAV-DF上表现尤为出色。在时序伪造定位任务上，它在AV-Deepfake1M上的表现优于最新技术水平的47.88 <a href="mailto:&#65;&#80;&#x40;&#x30;&#x2e;&#55;&#x54;">&#65;&#80;&#x40;&#x30;&#x2e;&#55;&#x54;</a>。代码可在<a target="_blank" rel="noopener" href="https://github.com/mever-team/dimodif%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mever-team/dimodif找到。</a></p>
<p><strong>关键见解</strong></p>
<p>1.深度伪造技术对信息完整性和在线多媒体信任构成重大威胁。<br>2.当前音频和视频篡造的检测面临巨大挑战，特别是同时对音频和视频模态的操纵。<br>3. DiMoDif是一个创新的音频视觉深度伪造检测框架，基于真实样本中视觉和音频信号的吻合信息来识别伪造内容。<br>4. DiMoDif利用深度网络中的跨模态差异来发现帧级的细微不一致之处，从而定位深度伪造篡改的位置。<br>5. DiMoDif采用分层跨模态融合网络设计，集成自适应时间对齐和差异映射层来模拟视觉和音频之间的差异。<br>6. DiMoDif在多个深度伪造检测任务上表现优于现有技术。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10193">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3e549f572d8d9fc2714c0d151cb98eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b470688fbc77243f62e6ce58bcb0c99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cbb6fa3f69738b45e1c99d7d6aa38ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6a2b28a75ebdd508ce8de995a22ac7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ebe6dd906ebce3209a64c2909fcba8a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unified-Static-and-Dynamic-Network-Efficient-Temporal-Filtering-for-Video-Grounding"><a href="#Unified-Static-and-Dynamic-Network-Efficient-Temporal-Filtering-for-Video-Grounding" class="headerlink" title="Unified Static and Dynamic Network: Efficient Temporal Filtering for   Video Grounding"></a>Unified Static and Dynamic Network: Efficient Temporal Filtering for   Video Grounding</h2><p><strong>Authors:Jingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, Meng Wang</strong></p>
<p>Inspired by the activity-silent and persistent activity mechanisms in human visual perception biology, we design a Unified Static and Dynamic Network (UniSDNet), to learn the semantic association between the video and text&#x2F;audio queries in a cross-modal environment for efficient video grounding. For static modeling, we devise a novel residual structure (ResMLP) to boost the global comprehensive interaction between the video segments and queries, achieving more effective semantic enhancement&#x2F;supplement. For dynamic modeling, we effectively exploit three characteristics of the persistent activity mechanism in our network design for a better video context comprehension. Specifically, we construct a diffusely connected video clip graph on the basis of 2D sparse temporal masking to reflect the “short-term effect” relationship. We innovatively consider the temporal distance and relevance as the joint “auxiliary evidence clues” and design a multi-kernel Temporal Gaussian Filter to expand the context clue into high-dimensional space, simulating the “complex visual perception”, and then conduct element level filtering convolution operations on neighbour clip nodes in message passing stage for finally generating and ranking the candidate proposals. Our UniSDNet is applicable to both Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely used datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new records at 38.88% R@1,<a href="mailto:&#x49;&#x6f;&#85;&#x40;&#x30;&#x2e;&#55;">&#x49;&#x6f;&#85;&#x40;&#x30;&#x2e;&#55;</a> on ActivityNet Captions and 40.26% R@1,<a href="mailto:&#x49;&#111;&#x55;&#64;&#x30;&#x2e;&#53;">&#x49;&#111;&#x55;&#64;&#x30;&#x2e;&#53;</a> on TACoS. To facilitate this field, we collect two new datasets (Charades-STA Speech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our UniSDNet is 1.56$\times$ faster than the strong multi-query benchmark. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/xian-sh/UniSDNet">https://github.com/xian-sh/UniSDNet</a>. </p>
<blockquote>
<p>受人类视觉感知生物学中的活动静默和持续活动机制的启发，我们设计了一个统一静态和动态网络（UniSDNet），用于在跨模态环境中学习视频和文本&#x2F;音频查询之间的语义关联，以实现高效视频定位。对于静态建模，我们设计了一种新型残差结构（ResMLP），以增强视频片段和查询之间的全局综合交互，实现更有效的语义增强&#x2F;补充。对于动态建模，我们有效地利用了网络中持续活动机制的三个特点，以更好地进行视频上下文理解。具体来说，我们在二维稀疏时间掩码的基础上构建了一个密集连接的视频剪辑图，以反映“短期效应”关系。我们创新地考虑时间距离和相关性作为联合的“辅助证据线索”，设计了一种多核时间高斯滤波器来将上下文线索扩展到高维空间，模拟“复杂视觉感知”，然后在消息传递阶段对相邻剪辑节点执行元素级滤波卷积操作，以生成和排序候选提案。我们的UniSDNet适用于自然语言视频定位（NLVG）和口语视频定位（SLVG）任务。UniSDNet在NLVG广泛使用的三个数据集以及SLVG的三个数据集上均达到了最先进的性能，例如在ActivityNet Captions上的R@1达到38.88%，<a href="mailto:&#73;&#x6f;&#85;&#64;&#48;&#x2e;&#55;">&#73;&#x6f;&#85;&#64;&#48;&#x2e;&#55;</a>的新纪录，以及在TACoS上的R@1达到40.26%，<a href="mailto:&#73;&#111;&#85;&#64;&#48;&#x2e;&#53;">&#73;&#111;&#85;&#64;&#48;&#x2e;&#53;</a>。为了促进该领域的发展，我们为SLVG任务收集了两个新数据集（Charades-STA Speech和TACoS Speech）。同时，我们的UniSDNet的推理速度比强大的多查询基准测试快1.56倍。代码可在：<a target="_blank" rel="noopener" href="https://github.com/xian-sh/UnisDNet%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xian-sh/UniSDNet访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.14174v2">PDF</a> Accepted to IEEE TPAMI 2025</p>
<p><strong>摘要</strong></p>
<p>本文借鉴人类视觉感知生物学中的活动和持续活动机制，设计了一种统一静态和动态网络（UniSDNet），用于学习视频与文本&#x2F;音频查询之间的语义关联，以实现高效视频定位。对于静态建模，我们提出了一种新型残差结构（ResMLP），以增强视频片段和查询之间的全局综合交互，实现更有效的语义增强&#x2F;补充。对于动态建模，我们有效地利用持续活动机制的三个特点来进行网络设计，以更好地理解视频上下文。具体来说，我们在2D稀疏时间掩码的基础上构建了一个密集连接的视频剪辑图，以反映“短期效应”关系。我们创新地考虑时间距离和相关性作为联合“辅助证据线索”，并设计了一种多核时间高斯滤波器来扩展上下文线索到高维空间，模拟“复杂视觉感知”，然后在消息传递阶段对邻居剪辑节点进行元素级滤波卷积操作，以生成和排序候选提案。我们的UniSDNet适用于自然语言视频定位（NLVG）和口语视频定位（SLVG）任务。UniSDNet在NLVG的三个常用数据集以及SLVG的三个数据集上实现了卓越的性能，例如在ActivityNet Captions上的R@1达到38.88%，<a href="mailto:&#x49;&#x6f;&#x55;&#64;&#x30;&#x2e;&#x37;">&#x49;&#x6f;&#x55;&#64;&#x30;&#x2e;&#x37;</a>，以及在TACoS上的R@1达到40.26%，<a href="mailto:&#x49;&#111;&#x55;&#x40;&#x30;&#46;&#53;">&#x49;&#111;&#x55;&#x40;&#x30;&#46;&#53;</a>。为了推动这一领域的发展，我们为SLVG任务收集了两个新数据集（Charades-STA Speech和TACoS Speech）。同时，UniSDNet的推理速度比强大的多查询基准测试快1.56倍。</p>
<p><strong>要点</strong></p>
<ol>
<li>借鉴人类视觉感知生物学中的活动和持续活动机制，设计UniSDNet网络进行视频与文本&#x2F;音频查询的语义关联学习。</li>
<li>静态建模采用新型残差结构（ResMLP）增强全局综合交互。</li>
<li>动态建模利用持续活动机制的三个特点进行网络设计，以更好地视频上下文理解。</li>
<li>构建密集连接的视频剪辑图，创新考虑时间距离和相关性作为辅助证据线索。</li>
<li>UniSDNet适用于NLVG和SLVG任务，并在多个数据集上实现卓越性能。</li>
<li>收集了两个新的数据集用于SLVG任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.14174">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ffffba00a846e8d562eb2c3dafa03c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59cc53528f38e0bb3d42724271024043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20bbd91685cd039659c55f1b2ad11ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edd9f62e678f4c856fc3e0c5bcfc2e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-653e211f893e469876b0aff041185c8b.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-04-15  On the Design of Diffusion-based Neural Speech Codecs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2607fb1e449d0c7278ee851114bcfe61.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-04-15  CLAP Isolating Content from Style through Contrastive Learning with   Augmented Prompts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
