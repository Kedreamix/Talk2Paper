<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-15-æ›´æ–°"><a href="#2025-04-15-æ›´æ–°" class="headerlink" title="2025-04-15 æ›´æ–°"></a>2025-04-15 æ›´æ–°</h1><h2 id="DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation"><a href="#DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation" class="headerlink" title="DocAgent: A Multi-Agent System for Automated Code Documentation   Generation"></a>DocAgent: A Multi-Agent System for Automated Code Documentation   Generation</h2><p><strong>Authors:Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang</strong></p>
<p>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories. </p>
<blockquote>
<p>é«˜è´¨é‡çš„ä»£ç æ–‡æ¡£å¯¹è½¯ä»¶å¼€å‘è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•ç»å¸¸äº§ç”Ÿä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚æˆ‘ä»¬å¼•å…¥äº†DocAgentï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé‡‡ç”¨æ‹“æ‰‘ä»£ç å¤„ç†æ¥è¿›è¡Œå¢é‡ä¸Šä¸‹æ–‡æ„å»ºã€‚ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼ˆé˜…è¯»å™¨ã€æœç´¢å™¨ã€ç¼–å†™å™¨ã€éªŒè¯å™¨ã€åè°ƒå™¨ï¼‰ååŒç”Ÿæˆæ–‡æ¡£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æ–‡æ¡£çš„å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDocAgentåœ¨ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºçš„é‡è¦ä½œç”¨ã€‚DocAgentä¸ºå¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­çš„å¯é ä»£ç æ–‡æ¡£ç”Ÿæˆæä¾›äº†ç¨³å¥çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08725v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œé«˜è´¨é‡ä»£ç æ–‡æ¡£å¯¹è½¯ä»¶å¼€å‘è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•å¾€å¾€äº§ç”Ÿä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚æˆ‘ä»¬æ¨å‡ºDocAgentï¼Œä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé‡‡ç”¨æ‹“æ‰‘ä»£ç å¤„ç†è¿›è¡Œå¢é‡ä¸Šä¸‹æ–‡æ„å»ºã€‚é€šè¿‡ä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“ï¼ˆé˜…è¯»å™¨ã€æœç´¢å™¨ã€å†™æ‰‹ã€éªŒè¯è€…ã€åè°ƒè€…ï¼‰ååŒç”Ÿæˆæ–‡æ¡£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDocAgentåœ¨ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºçš„é‡è¦ä½œç”¨ã€‚DocAgentä¸ºå¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­çš„å¯é ä»£ç æ–‡æ¡£ç”Ÿæˆæä¾›äº†ç¨³å¥çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜è´¨é‡ä»£ç æ–‡æ¡£åœ¨è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ã€‚</li>
<li>è‡ªåŠ¨ç”Ÿæˆä»£ç æ–‡æ¡£ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>DocAgentæ˜¯ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé‡‡ç”¨æ‹“æ‰‘ä»£ç å¤„ç†ç”Ÿæˆæ–‡æ¡£ã€‚</li>
<li>DocAgentåŒ…å«å¤šä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“ï¼šé˜…è¯»å™¨ã€æœç´¢å™¨ã€å†™æ‰‹ã€éªŒè¯è€…ã€åè°ƒè€…ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°ç”Ÿæˆçš„æ–‡æ¡£çš„å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>DocAgentåœ¨ç»¼åˆå®éªŒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºåœ¨DocAgentæ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84dea2119fbe38fca6011cfa93c867fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fd0534526e9f43e1ed4683ac785090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933e5dcbc145e1cb1361f03356dd2b34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28680bc575a8c0f69c2ab5c8868cc5f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cae1fe7424320522f3e73e14023dd673.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SWE-PolyBench-A-multi-language-benchmark-for-repository-level-evaluation-of-coding-agents"><a href="#SWE-PolyBench-A-multi-language-benchmark-for-repository-level-evaluation-of-coding-agents" class="headerlink" title="SWE-PolyBench: A multi-language benchmark for repository level   evaluation of coding agents"></a>SWE-PolyBench: A multi-language benchmark for repository level   evaluation of coding agents</h2><p><strong>Authors:Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buccholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, Laurent Callot</strong></p>
<p>Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/SWE-PolyBench">https://github.com/amazon-science/SWE-PolyBench</a> </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ä»£ç ç”Ÿæˆå™¨åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸­å¯¹å®ƒä»¬çš„æ€§èƒ½è¿›è¡Œè¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†SWE-PolyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹ä»£ç ç”Ÿæˆå™¨è¿›è¡Œä»“åº“çº§åˆ«çš„åŸºäºæ‰§è¡Œçš„è¯„ä»·ã€‚SWE-PolyBenchåŒ…å«æ¥è‡ª21ä¸ªä»“åº“çš„2110ä¸ªå®ä¾‹ï¼Œæ¶µç›–äº†Javaï¼ˆ165ä¸ªï¼‰ã€JavaScriptï¼ˆ1017ä¸ªï¼‰ã€TypeScriptï¼ˆ729ä¸ªï¼‰å’ŒPythonï¼ˆ199ä¸ªï¼‰çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬é”™è¯¯ä¿®å¤ã€åŠŸèƒ½æ·»åŠ å’Œä»£ç é‡æ„ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä»»åŠ¡å’Œä»“åº“åˆ†å±‚å­æ ·æœ¬ï¼ˆSWE-PolyBench500ï¼‰ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªè¯„ä¼°å·¥å…·ï¼Œå¯ä»¥å®ç°å…¨è‡ªåŠ¨è¯„ä¼°ã€‚ä¸ºäº†èƒ½å¤Ÿå¯¹ä»£ç ç”Ÿæˆå™¨è¿›è¡Œæ›´å…¨é¢çš„æ¯”è¾ƒï¼Œè¿™é¡¹å·¥ä½œè¿˜æå‡ºäº†åŸºäºè¯­æ³•æ ‘åˆ†æçš„æ–°æŒ‡æ ‡é›†ã€‚æˆ‘ä»¬åœ¨SWE-PolyBenchä¸Šè¯„ä¼°äº†é¢†å…ˆçš„å¼€æºä»£ç ç”Ÿæˆå™¨ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è¯­è¨€ã€ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦ç±»åˆ«æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰å„ä»£ç†åœ¨ä¸åŒè¯­è¨€ä¸­çš„è¡¨ç°ä¸å‡è¡¡ï¼Œåœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å›°éš¾ï¼Œè€Œåœ¨ç®€å•ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚SWE-PolyBenchçš„ç›®æ ‡æ˜¯æ¨åŠ¨å¼€å‘æ›´é€šç”¨ã€æ›´ç¨³å¥çš„AIç¼–ç åŠ©æ‰‹ï¼Œç”¨äºçœŸå®ä¸–ç•Œçš„è½¯ä»¶å·¥ç¨‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/amazon-science/SWE-PolyBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/amazon-science/SWE-PolyBenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08703v1">PDF</a> 20 pages, 6 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç¼–ç ä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†è·¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œç°å®åœºæ™¯è¯„ä¼°å…¶æ€§èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SWE-PolyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹ç¼–ç ä»£ç†è¿›è¡Œä»“åº“çº§çš„æ‰§è¡Œè¯„ä¼°ã€‚SWE-PolyBenchåŒ…å«æ¥è‡ª21ä¸ªä»“åº“çš„2110ä¸ªå®ä¾‹ï¼Œæ¶µç›–Javaã€JavaScriptã€TypeScriptå’ŒPythonçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬é”™è¯¯ä¿®å¤ã€åŠŸèƒ½æ·»åŠ å’Œä»£ç é‡æ„ã€‚æˆ‘ä»¬æä¾›äº†ä»»åŠ¡åˆ†å±‚å’Œä»“åº“åˆ†å±‚çš„å­é›†ï¼ˆSWE-PolyBench500ï¼‰ï¼Œå¹¶å‘å¸ƒäº†è¯„ä¼°å·¥å…·ï¼Œå¯ä»¥å®ç°å®Œå…¨è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚ä¸ºäº†æ›´å…¨é¢åœ°æ¯”è¾ƒç¼–ç ä»£ç†ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäºè¯­æ³•æ ‘åˆ†æçš„æ–°æŒ‡æ ‡é›†ã€‚æˆ‘ä»¬åœ¨SWE-PolyBenchä¸Šè¯„ä¼°äº†é¢†å…ˆçš„å¼€æºç¼–ç ä»£ç†ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è¯­è¨€ã€ä»»åŠ¡ç±»å‹å’Œå¤æ‚æ€§æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰ä»£ç†åœ¨ä¸åŒè¯­è¨€ä¹‹é—´çš„æ€§èƒ½ä¸å‡ï¼Œå¯¹å¤æ‚é—®é¢˜æ„Ÿåˆ°å›°éš¾ï¼Œåœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå¥½ã€‚SWE-PolyBenchæ—¨åœ¨æ¨åŠ¨å¼€å‘æ›´é€šç”¨å’Œç¨³å¥çš„AIç¼–ç¨‹åŠ©æ‰‹ï¼Œç”¨äºç°å®è½¯ä»¶å·¥ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç¼–ç ä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>è·¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œç°å®åœºæ™¯è¯„ä¼°ç¼–ç ä»£ç†æ€§èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>SWE-PolyBenchæ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºä»“åº“çº§çš„æ‰§è¡Œè¯„ä¼°ã€‚</li>
<li>SWE-PolyBenchåŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ï¼Œæ¶µç›–Javaã€JavaScriptã€TypeScriptå’ŒPythonã€‚</li>
<li>æä¾›äº†è‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·ï¼Œä¾¿äºè¯„ä¼°ç¼–ç ä»£ç†æ€§èƒ½ã€‚</li>
<li>å½“å‰ç¼–ç ä»£ç†åœ¨ä¸åŒè¯­è¨€ã€ä»»åŠ¡ç±»å‹å’Œå¤æ‚æ€§æ–¹é¢çš„æ€§èƒ½å­˜åœ¨å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2233545647eb40492497e0edf74ab1dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d3231822b18b5ffc11752e8d6afeae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-965a32ce7c5dd61107255472f6573e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc5f22efe3e57492b7454d3afcf96a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c75445987ff4e3d61bad871380cc85f3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SeaView-Software-Engineering-Agent-Visual-Interface-for-Enhanced-Workflow"><a href="#SeaView-Software-Engineering-Agent-Visual-Interface-for-Enhanced-Workflow" class="headerlink" title="SeaView: Software Engineering Agent Visual Interface for Enhanced   Workflow"></a>SeaView: Software Engineering Agent Visual Interface for Enhanced   Workflow</h2><p><strong>Authors:Timothy Bula, Saurabh Pujar, Luca Buratti, Mihaela Bornea, Avirup Sil</strong></p>
<p>Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (&gt;60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating â€œtrajectoriesâ€. Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaViewâ€™s novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment. </p>
<blockquote>
<p>åŸºäºè‡ªåŠ¨å›å½’LLMçš„è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»£ç†ï¼Œåœ¨æ­¤ç§°ä¸ºSWEä»£ç†ï¼Œåœ¨ç°å®ä¸–ç•Œçš„ç¼–ç¨‹æŒ‘æˆ˜ï¼ˆåŒ…æ‹¬GitHubé—®é¢˜è§£å†³æ–¹æ¡ˆï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§è¿›å±•ï¼ˆåœ¨SWE-Bench Verifiedä¸Šçš„å¾—åˆ†è¶…è¿‡60%ï¼‰ã€‚SWEä»£ç†é€šè¿‡ç»“åˆæ¨ç†ã€ç¯å¢ƒäº¤äº’å’Œè‡ªæˆ‘åæ€æ¥è§£å†³é—®é¢˜ï¼Œä»è€Œç”Ÿæˆâ€œè½¨è¿¹â€ã€‚SWEä»£ç†è½¨è¿¹çš„åˆ†æå¾ˆå›°éš¾ï¼Œä¸ä»…æ˜¯å› ä¸ºå®ƒä»¬è¶…è¿‡äº†LLMåºåˆ—é•¿åº¦ï¼ˆæœ‰æ—¶è¶…è¿‡128kï¼‰ï¼Œè€Œä¸”å› ä¸ºæ¶‰åŠåˆ°LLMå’Œä»£ç†ç®¡ç†çš„ç¯å¢ƒä¹‹é—´ç›¸å¯¹è¾ƒé•¿çš„äº¤äº’ã€‚åœ¨ä»£ç†å‡ºé”™çš„æƒ…å†µä¸‹ï¼Œå¾ˆéš¾è§£é‡Šã€å®šä½å’Œäº†è§£å…¶èŒƒå›´ã€‚åŒæ ·ï¼Œå¾ˆéš¾è·Ÿè¸ªå¤šæ¬¡è¿è¡Œæˆ–å®éªŒä¸­çš„æ”¹è¿›æˆ–å›å½’ã€‚è™½ç„¶å¾ˆå¤šç ”ç©¶å·²ç»æŠ•å…¥åˆ°è®©è¿™äº›SWEä»£ç†è¾¾åˆ°æœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œä½†å¾ˆå°‘å…³æ³¨åˆ›å»ºå·¥å…·æ¥å¸®åŠ©åˆ†æå’Œå¯è§†åŒ–ä»£ç†è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å·¥å…·ï¼Œç§°ä¸ºSeaViewï¼šè½¯ä»¶å·¥ç¨‹ä»£ç†å¯è§†åŒ–ç•Œé¢å¢å¼ºå·¥ä½œæµç¨‹ï¼Œå…¶æ„¿æ™¯æ˜¯å¸®åŠ©SWEä»£ç†ç ”ç©¶äººå‘˜å¯è§†åŒ–å¹¶æ£€æŸ¥ä»–ä»¬çš„å®éªŒã€‚SeaViewçš„æ–°é¢–æœºåˆ¶æœ‰åŠ©äºæ¯”è¾ƒå…·æœ‰ä¸åŒè¶…å‚æ•°æˆ–LLMçš„å®éªŒè¿è¡Œï¼Œå¹¶å¿«é€Ÿäº†è§£ä¸LLMæˆ–ç¯å¢ƒç›¸å…³çš„é—®é¢˜ã€‚æ ¹æ®æˆ‘ä»¬çš„ç”¨æˆ·ç ”ç©¶ï¼Œç»éªŒä¸°å¯Œçš„ç ”ç©¶äººå‘˜ä½¿ç”¨SeaViewæä¾›çš„ä¿¡æ¯éœ€è¦èŠ±è´¹10åˆ°30åˆ†é’Ÿçš„æ—¶é—´ï¼Œè€Œç»éªŒè¾ƒå°‘çš„ç ”ç©¶äººå‘˜åˆ™éœ€è¦èŠ±è´¹30åˆ†é’Ÿåˆ°1å°æ—¶çš„æ—¶é—´æ¥è¯Šæ–­ä»–ä»¬çš„å®éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08696v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼šåŸºäºè‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œç¼–ç æŒ‘æˆ˜æ–¹é¢å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œå¦‚GitHubé—®é¢˜è§£æã€‚SWEä»£ç†é€šè¿‡æ¨ç†ã€ç¯å¢ƒäº¤äº’å’Œè‡ªæˆ‘åæ€æ¥è§£å†³æ–°é—®é¢˜å¹¶ç”Ÿæˆâ€œè½¨è¿¹â€ã€‚åˆ†æSWEä»£ç†è½¨è¿¹å¾ˆå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè½¨è¿¹ä¸ä»…å¯èƒ½è¶…è¿‡LLMåºåˆ—é•¿åº¦ï¼ˆæœ‰æ—¶ç”šè‡³è¶…è¿‡128kï¼‰ï¼Œè€Œä¸”æ¶‰åŠLLMå’Œä»£ç†ç®¡ç†çš„ç¯å¢ƒä¹‹é—´ç›¸å¯¹é•¿æ—¶é—´çš„äº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å·¥å…·SeaViewï¼Œæ—¨åœ¨å¸®åŠ©SWEä»£ç†ç ”ç©¶äººå‘˜å¯è§†åŒ–å¹¶æ£€æŸ¥å…¶å®éªŒã€‚SeaViewå¯ä»¥å¿«é€Ÿæ¯”è¾ƒä¸åŒè¶…å‚æ•°æˆ–LLMçš„å®éªŒè¿è¡Œï¼Œå¹¶ç†è§£ç›¸å…³çš„LLMæˆ–ç¯å¢ƒé—®é¢˜ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œç»éªŒä¸°å¯Œçš„ç ”ç©¶äººå‘˜ä½¿ç”¨SeaViewæ”¶é›†ä¿¡æ¯çš„æ—¶é—´åœ¨10åˆ°30åˆ†é’Ÿä¹‹é—´ï¼Œè€Œç»éªŒè¾ƒå°‘çš„ç ”ç©¶äººå‘˜å¯èƒ½éœ€è¦èŠ±è´¹åŠå°æ—¶åˆ°ä¸€å°æ—¶æ¥è¯Šæ–­å®éªŒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºäºè‡ªå›å½’LLMçš„SWEä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œç¼–ç é—®é¢˜ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä¾‹å¦‚GitHubé—®é¢˜è§£æã€‚</li>
<li>SWä»£ç†ç”Ÿæˆè¢«ç§°ä¸ºâ€œè½¨è¿¹â€çš„è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œå…¶ä¸­åŒ…å«æ¨ç†ã€ç¯å¢ƒäº¤äº’å’Œè‡ªæˆ‘åæ€ç­‰å…ƒç´ ã€‚</li>
<li>åˆ†æSWEä»£ç†è½¨è¿¹å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬è½¨è¿¹é•¿åº¦è¶…è¿‡LLMåºåˆ—é•¿åº¦å’Œæ¶‰åŠé•¿æ—¶é—´çš„ç¯å¢ƒäº¤äº’ã€‚</li>
<li>æå‡ºçš„æ–°å‹å·¥å…·SeaViewæ—¨åœ¨å¸®åŠ©SWEä»£ç†ç ”ç©¶äººå‘˜å¯è§†åŒ–å¹¶æ£€æŸ¥å®éªŒï¼Œä¿ƒè¿›ç†è§£LLMæˆ–ç¯å¢ƒç›¸å…³çš„é—®é¢˜ã€‚</li>
<li>SeaViewæœ‰åŠ©äºæ¯”è¾ƒä¸åŒè¶…å‚æ•°æˆ–LLMçš„å®éªŒè¿è¡Œï¼Œå¿«é€Ÿè¯†åˆ«é—®é¢˜æ‰€åœ¨ã€‚</li>
<li>æ ¹æ®ç”¨æˆ·ç ”ç©¶ï¼Œä½¿ç”¨SeaViewæ”¶é›†ä¿¡æ¯çš„æ—¶é—´æ ¹æ®ç ”ç©¶äººå‘˜çš„ç»éªŒè€Œå¼‚ï¼Œç»éªŒä¸°å¯Œçš„ç ”ç©¶äººå‘˜å¯èƒ½éœ€è¦è¾ƒå°‘çš„æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94bc5b38eafcc5e4c43324a4dfae40bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d55739e6041b43aa0e30ea5ae4991f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de77ae138e941ece8ccedb0e1e097e2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df7252f83fc97b4d487475e3714a025d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6fdda5ae2a88ad0e9d0d956a9b6e22dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb2c4e7b3d70f30c35b55a404810cbf3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TP-RAG-Benchmarking-Retrieval-Augmented-Large-Language-Model-Agents-for-Spatiotemporal-Aware-Travel-Planning"><a href="#TP-RAG-Benchmarking-Retrieval-Augmented-Large-Language-Model-Agents-for-Spatiotemporal-Aware-Travel-Planning" class="headerlink" title="TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for   Spatiotemporal-Aware Travel Planning"></a>TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for   Spatiotemporal-Aware Travel Planning</h2><p><strong>Authors:Hang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, Hao Liu</strong></p>
<p>Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMsâ€™ intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ—…è¡Œè§„åˆ’æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œï¼Œåœ¨å¤„ç†å¾®å¦™çš„æ—¶ç©ºåˆç†æ€§æ–¹é¢ï¼Œå®ƒä»¬å¾€å¾€å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨åŸºæœ¬è®¡åˆ’çš„å¯è¡Œæ€§ä¸Šï¼Œå´å¿½è§†äº†è·¯çº¿æ•ˆç‡ã€å…´è¶£ç‚¹å¸å¼•åŠ›ä»¥åŠå®æ—¶é€‚åº”æ€§ç­‰å…³é”®æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸ºæ£€ç´¢å¢å¼ºã€æ—¶ç©ºæ„ŸçŸ¥æ—…è¡Œè§„åˆ’é‡èº«å®šåˆ¶çš„TP-RAGåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«2348ä¸ªçœŸå®æ—…è¡ŒæŸ¥è¯¢ã€85575ä¸ªç²¾ç»†æ ‡æ³¨çš„å…´è¶£ç‚¹å’Œ18784ä¸ªé«˜è´¨é‡æ—…è¡Œè½¨è¿¹å‚è€ƒï¼Œè¿™äº›å‚è€ƒæ¥æºäºåœ¨çº¿æ—…æ¸¸æ–‡æ¡£ï¼Œå¯å®ç°åŠ¨æ€å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§„åˆ’ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°é›†æˆå‚è€ƒè½¨è¿¹å¯ä»¥æ˜¾è‘—æé«˜æ—…è¡Œè®¡åˆ’çš„ç©ºé—´æ•ˆç‡å’Œå…´è¶£ç‚¹çš„åˆç†æ€§ï¼Œä½†ç”±äºå­˜åœ¨ç›¸äº’å†²çªçš„å¼•ç”¨å’Œå˜ˆæ‚çš„æ•°æ®ï¼Œä»å­˜åœ¨æ™®éæ€§å’Œç¨³å¥æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EvoRAGï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿå¼ºæœ‰åŠ›åœ°ååŒå¤šæ ·åŒ–çš„æ£€ç´¢è½¨è¿¹ä¸LLMçš„å†…åœ¨æ¨ç†ã€‚EvoRAGè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œä¸è‡ªä¸‹è€Œä¸Šå’Œæ£€ç´¢å¢å¼ºçš„åŸºå‡†ç›¸æ¯”ï¼Œæé«˜äº†æ—¶ç©ºåˆè§„æ€§å¹¶å‡å°‘äº†å¸¸è¯†è¿è§„è¡Œä¸ºã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†æ··åˆWebçŸ¥è¯†ä¸LLMé©±åŠ¨ä¼˜åŒ–çš„æ½œåŠ›ï¼Œä¸ºæ›´å¯é å’Œé€‚åº”æ€§æ›´å¼ºçš„æ—…è¡Œè§„åˆ’ä»£ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08694v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹æ£€ç´¢å¢å¼ºã€æ—¶ç©ºæ„ŸçŸ¥æ—…è¡Œè§„åˆ’çš„åŸºå‡†æµ‹è¯•TP-RAGã€‚è¯¥æ•°æ®é›†åŒ…å«çœŸå®æ—…è¡ŒæŸ¥è¯¢ã€ç²¾ç»†æ ‡æ³¨çš„POIå’Œé«˜è´¨é‡æ—…è¡Œè½¨è¿¹å‚è€ƒï¼Œä½¿åŠ¨æ€å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§„åˆ’æˆä¸ºå¯èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé›†æˆå‚è€ƒè½¨è¿¹å¯ä»¥æ˜¾è‘—æé«˜ç©ºé—´æ•ˆç‡å’ŒPOIåˆç†æ€§ï¼Œä½†ä»å­˜åœ¨æ™®éæ€§å’Œç¨³å¥æ€§é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è¿›åŒ–æ¡†æ¶EvoRAGï¼Œå®ƒæœ‰æ•ˆåœ°å°†å¤šç§æ£€ç´¢è½¨è¿¹ä¸LLMçš„å†…åœ¨æ¨ç†ç›¸ç»“åˆã€‚EvoRAGè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½æ°´å¹³ï¼Œæé«˜äº†æ—¶ç©ºä¸€è‡´æ€§å¹¶å‡å°‘äº†å¸¸è¯†æ€§è¿è§„è¡Œä¸ºã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†èåˆç½‘ç»œçŸ¥è¯†å’ŒLLMä¼˜åŒ–çš„æ½œåŠ›ï¼Œä¸ºæ—…è¡Œè§„åˆ’é¢†åŸŸæ›´å¯é ã€æ›´çµæ´»çš„æ™ºèƒ½åŠ©æ‰‹å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–æ—…è¡Œè§„åˆ’ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»éœ€è§£å†³æ—¶ç©ºç†æ€§çš„ç»†å¾®å·®å¼‚é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨åŸºæœ¬è®¡åˆ’çš„æœ‰æ•ˆæ€§ä¸Šï¼Œå¿½ç•¥äº†è·¯çº¿æ•ˆç‡ã€å…´è¶£ç‚¹å¸å¼•åŠ›å’Œå®æ—¶é€‚åº”æ€§ç­‰å…³é”®æ–¹é¢ã€‚</li>
<li>ä»‹ç»äº†é’ˆå¯¹æ£€ç´¢å¢å¼ºã€æ—¶ç©ºæ„ŸçŸ¥æ—…è¡Œè§„åˆ’çš„åŸºå‡†æµ‹è¯•TP-RAGï¼ŒåŒ…å«çœŸå®æ—…è¡ŒæŸ¥è¯¢å’Œç²¾ç»†æ ‡æ³¨çš„POIç­‰æ•°æ®ã€‚</li>
<li>é›†æˆå‚è€ƒè½¨è¿¹å¯æ˜¾è‘—æé«˜ç©ºé—´æ•ˆç‡å’ŒPOIåˆç†æ€§ï¼Œä½†å­˜åœ¨æ™®éæ€§å’Œç¨³å¥æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†è¿›åŒ–æ¡†æ¶EvoRAGï¼Œå°†å¤šç§æ£€ç´¢è½¨è¿¹ä¸LLMçš„å†…åœ¨æ¨ç†ç›¸ç»“åˆï¼Œè¾¾åˆ°å‰æ‰€æœªæœ‰çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>EvoRAGæé«˜äº†æ—¶ç©ºä¸€è‡´æ€§å¹¶å‡å°‘äº†å¸¸è¯†æ€§è¿è§„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bac2860dc03f9f129669bf226fe977d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e6f5d6f6556c07174a68b593fd81313.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69a0f63c0dc5a1a801bb660d86f1ec48.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Optimal-selection-of-the-most-informative-nodes-for-a-noisy-DeGroot-model-with-stubborn-agents"><a href="#Optimal-selection-of-the-most-informative-nodes-for-a-noisy-DeGroot-model-with-stubborn-agents" class="headerlink" title="Optimal selection of the most informative nodes for a noisy DeGroot   model with stubborn agents"></a>Optimal selection of the most informative nodes for a noisy DeGroot   model with stubborn agents</h2><p><strong>Authors:Roberta Raineri, Giacomo Como, Fabio Fagnani</strong></p>
<p>Finding the optimal subset of individuals to observe in order to obtain the best estimate of the average opinion of a society is a crucial problem in a wide range of applications, including policy-making, strategic business decisions, and the analysis of sociological trends. We consider the opinion vector X to be updated according to a DeGroot opinion dynamical model with stubborn agents, subject to perturbations from external random noise, which can be interpreted as transmission errors. The objective function of the optimization problem is the variance reduction achieved by observing the equilibrium opinions of a subset K of agents. We demonstrate that, under this specific setting, the objective function exhibits the property of submodularity. This allows us to effectively design a Greedy Algorithm to solve the problem, significantly reducing its computational complexity. Simple examples are provided to validate our results. </p>
<blockquote>
<p>å¯»æ‰¾æœ€ä¼˜ä¸ªä½“å­é›†è¿›è¡Œè§‚å¯Ÿï¼Œä»¥å¾—åˆ°ç¤¾ä¼šå¹³å‡æ„è§çš„æœ€ä½³ä¼°è®¡æ˜¯å¹¿æ³›åº”ç”¨ä¸­çš„å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬æ”¿ç­–åˆ¶å®šã€æˆ˜ç•¥ä¸šåŠ¡å†³ç­–å’Œç¤¾ä¼šè¶‹åŠ¿åˆ†æã€‚æˆ‘ä»¬è®¤ä¸ºæ„è§å‘é‡Xå°†æ ¹æ®å¸¦æœ‰é¡½å›ºä»£ç†çš„DeGrootæ„è§åŠ¨æ€æ¨¡å‹è¿›è¡Œæ›´æ–°ï¼Œå¹¶å—åˆ°å¤–éƒ¨éšæœºå™ªå£°çš„å¹²æ‰°ï¼Œå¯ä»¥è§£é‡Šä¸ºä¼ è¾“é”™è¯¯ã€‚ä¼˜åŒ–é—®é¢˜çš„ç›®æ ‡å‡½æ•°æ˜¯é€šè¿‡è§‚å¯Ÿä»£ç†å­é›†Kçš„å¹³è¡¡æ„è§å®ç°çš„æ–¹å·®å‡å°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨è¿™ç§ç‰¹å®šè®¾ç½®ä¸‹ï¼Œç›®æ ‡å‡½æ•°è¡¨ç°å‡ºå­æ¨¡å—åŒ–çš„å±æ€§ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°è®¾è®¡è´ªå©ªç®—æ³•æ¥è§£å†³é—®é¢˜ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚é€šè¿‡ç®€å•çš„ä¾‹å­éªŒè¯äº†æˆ‘ä»¬çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08622v1">PDF</a> </p>
<p><strong>Summary</strong><br>é’ˆå¯¹ä»ç¤¾ä¼šç¾¤ä½“ä¸­è§‚å¯Ÿç‰¹å®šä¸ªä½“ä»¥è·å¾—æœ€ä½³ç¾¤ä½“è§‚ç‚¹ä¼°ç®—çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºåŸºäºDeGrootè§‚ç‚¹åŠ¨æ€æ¨¡å‹ä½¿ç”¨é¡½å¼ºä¸ªä½“çš„è§‚ç‚¹æ›´æ–°æ–¹æ¡ˆï¼Œä¸”æ¨¡å‹å—å¤–éƒ¨éšæœºå™ªå£°æ‰°åŠ¨å½±å“ã€‚ç›®æ ‡å‡½æ•°æ˜¯é™ä½è§‚å¯Ÿéƒ¨åˆ†ä¸ªä½“æœ€ç»ˆè§‚ç‚¹æ‰€å¾—ç»“æœå¸¦æ¥çš„æ–¹å·®ã€‚ç ”ç©¶è¡¨æ˜è¯¥ç›®æ ‡å‡½æ•°å…·æœ‰å­æ¨¡æ€§ï¼Œå¯è®¾è®¡è´ªå¿ƒç®—æ³•æœ‰æ•ˆè§£å†³é—®é¢˜å¹¶æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ã€‚ç®€å•å®ä¾‹éªŒè¯äº†ç ”ç©¶ç»“æœçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é’ˆå¯¹ç¤¾ä¼šç¾¤ä½“è§‚ç‚¹ä¼°ç®—é—®é¢˜ï¼Œé‡‡ç”¨DeGrootè§‚ç‚¹åŠ¨æ€æ¨¡å‹æ›´æ–°è§‚ç‚¹ã€‚</li>
<li>æ¨¡å‹è€ƒè™‘å­˜åœ¨é¡½å¼ºä¸ªä½“å’Œå¤–éƒ¨éšæœºå™ªå£°æ‰°åŠ¨çš„å½±å“ã€‚</li>
<li>ç›®æ ‡å‡½æ•°æ—¨åœ¨é™ä½è§‚å¯Ÿç‰¹å®šä¸ªä½“æœ€ç»ˆè§‚ç‚¹æ‰€å¸¦æ¥çš„æ–¹å·®ã€‚</li>
<li>ç ”ç©¶å‘ç°ç›®æ ‡å‡½æ•°å…·æœ‰å­æ¨¡æ€§ï¼Œæœ‰åŠ©äºè®¾è®¡è´ªå¿ƒç®—æ³•è§£å†³ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>è´ªå¿ƒç®—æ³•èƒ½æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-05634cd6a181d2a6f036eb1f6b0135a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5da54e5b2d2ad51696d4db9ee6078284.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-408d09acc9fd945473853f1927ffb760.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a6dc1663545b0f0048b2fea7357201a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8afaab85712ef43d9e80836677cc30e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2af6d14c91f934dd54330ed084c7c236.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MooseAgent-A-LLM-Based-Multi-agent-Framework-for-Automating-Moose-Simulation"><a href="#MooseAgent-A-LLM-Based-Multi-agent-Framework-for-Automating-Moose-Simulation" class="headerlink" title="MooseAgent: A LLM Based Multi-agent Framework for Automating Moose   Simulation"></a>MooseAgent: A LLM Based Multi-agent Framework for Automating Moose   Simulation</h2><p><strong>Authors:Tao Zhang, Zhenhai Liu, Yong Xin, Yongjun Jiao</strong></p>
<p>The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at <a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent">https://github.com/taozhan18/MooseAgent</a> </p>
<blockquote>
<p>æœ‰é™å…ƒæ–¹æ³•ï¼ˆFEMï¼‰åœ¨å·¥ç¨‹å’Œç§‘å­¦è®¡ç®—ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶é¢„å¤„ç†ã€æ±‚è§£å™¨é…ç½®å’Œåå¤„ç†é˜¶æ®µå¾€å¾€è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬æ–‡é’ˆå¯¹å¤šç‰©ç†ä»¿çœŸæ¡†æ¶MOOSEï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¡†æ¶MooseAgentã€‚è¯¥æ¡†æ¶ç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä½¿ç”¨LLMsç†è§£ç”¨æˆ·æè¿°çš„è‡ªç„¶è¯­è¨€ä»¿çœŸè¦æ±‚ï¼Œå¹¶é‡‡ç”¨ä»»åŠ¡åˆ†è§£å’Œå¤šè½®è¿­ä»£éªŒè¯ç­–ç•¥è‡ªåŠ¨ç”ŸæˆMOOSEè¾“å…¥æ–‡ä»¶ã€‚ä¸ºäº†æé«˜ç²¾åº¦å¹¶å‡å°‘æ¨¡å‹å¹»è§‰ï¼Œè¯¥ç³»ç»Ÿæ„å»ºå¹¶åˆ©ç”¨äº†ä¸€ä¸ªåŒ…å«æ³¨é‡Šçš„MOOSEè¾“å…¥å¡ç‰‡å’ŒåŠŸèƒ½æ–‡æ¡£çš„å‘é‡æ•°æ®åº“ã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªå…¸å‹æ¡ˆä¾‹ä¸Šè¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬çƒ­ä¼ å¯¼ã€åŠ›å­¦ã€ç›¸åœºå’Œå¤šç‰©ç†è€¦åˆã€‚ç»“æœè¡¨æ˜ï¼ŒMooseAgentèƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šè‡ªåŠ¨åŒ–MOOSEä»¿çœŸè¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç›¸å¯¹ç®€å•çš„å•ç‰©ç†é—®é¢˜æ—¶æˆåŠŸç‡è¾ƒé«˜ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®æ˜¯æå‡ºäº†é’ˆå¯¹MOOSEçš„å¤šæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ¡†æ¶ï¼ŒéªŒè¯äº†å…¶åœ¨ç®€åŒ–æœ‰é™å…ƒä»¿çœŸè¿‡ç¨‹ã€é™ä½ç”¨æˆ·é—¨æ§›æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ™ºèƒ½æœ‰é™å…ƒä»¿çœŸè½¯ä»¶çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚æœ¬æ–‡æå‡ºçš„MooseAgentæ¡†æ¶çš„ä»£ç å·²å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/taozhan18/MooseAgent%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/taozhan18/MooseAgentä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08621v1">PDF</a> 7 pages, 2 Figs</p>
<p><strong>Summary</strong><br>å·¥ç¨‹å’Œç§‘å­¦è®¡ç®—ä¸­å¹¿æ³›åº”ç”¨çš„æœ‰é™å…ƒæ–¹æ³•ï¼ˆFEMï¼‰åœ¨é¢„å¤„ç†ã€æ±‚è§£å™¨é…ç½®å’Œåå¤„ç†é˜¶æ®µååˆ†è€—æ—¶å¹¶éœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºMooseAgentçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¡†æ¶ï¼Œç”¨äºå¤šç‰©ç†ä»¿çœŸæ¡†æ¶MOOSEã€‚è¯¥æ¡†æ¶ç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç†è§£ç”¨æˆ·æè¿°çš„ä»¿çœŸéœ€æ±‚å¹¶è‡ªåŠ¨ç”ŸæˆMOOSEè¾“å…¥æ–‡ä»¶ã€‚é€šè¿‡æ„å»ºå’Œåˆ©ç”¨å‘é‡æ•°æ®åº“æ¥æé«˜å‡†ç¡®æ€§å’Œå‡å°‘æ¨¡å‹å¹»è§‰ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMooseAgentåœ¨ä¸€å®šç¨‹åº¦ä¸Šèƒ½è‡ªåŠ¨åŒ–MOOSEä»¿çœŸè¿‡ç¨‹ï¼Œå°¤å…¶åœ¨å¤„ç†ç›¸å¯¹ç®€å•çš„å•ç‰©ç†é—®é¢˜æ—¶æˆåŠŸç‡è¾ƒé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MooseAgentæ˜¯ä¸€ä¸ªç”¨äºå¤šç‰©ç†ä»¿çœŸæ¡†æ¶MOOSEçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œç»“åˆäº†å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚</li>
<li>LLMsç”¨äºç†è§£ç”¨æˆ·æè¿°çš„ä»¿çœŸéœ€æ±‚ï¼Œå¹¶è‡ªåŠ¨ç”ŸæˆMOOSEè¾“å…¥æ–‡ä»¶ã€‚</li>
<li>MooseAgenté€šè¿‡æ„å»ºå’Œåˆ©ç”¨å‘é‡æ•°æ®åº“æ¥æé«˜å‡†ç¡®æ€§å’Œå‡å°‘æ¨¡å‹å¹»è§‰ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒMooseAgentèƒ½è‡ªåŠ¨åŒ–MOOSEä»¿çœŸè¿‡ç¨‹è‡³ä¸€å®šç¨‹åº¦ï¼Œå°¤å…¶å¤„ç†ç®€å•å•ç‰©ç†é—®é¢˜æ—¶æˆåŠŸç‡é«˜ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®æ˜¯æå‡ºä¸€ä¸ªç”¨äºMOOSEçš„å¤šæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæœ‰æ½œåŠ›ç®€åŒ–æœ‰é™å…ƒä»¿çœŸè¿‡ç¨‹å¹¶é™ä½ç”¨æˆ·é—¨æ§›ã€‚</li>
<li>MooseAgentæ¡†æ¶çš„ä»£ç å·²å¼€æºï¼Œå¯åœ¨å…¬å¼€å¹³å°è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7f785e2bbadd5f7b0a69d3b85ceb37a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-895778a4defe0d18c5accfc76dd27ee1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0e80dadc6623fd0ae96114217ef5154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50a9a88f863c4a7203b020812f710cb7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FMLGS-Fast-Multilevel-Language-Embedded-Gaussians-for-Part-level-Interactive-Agents"><a href="#FMLGS-Fast-Multilevel-Language-Embedded-Gaussians-for-Part-level-Interactive-Agents" class="headerlink" title="FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level   Interactive Agents"></a>FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level   Interactive Agents</h2><p><strong>Authors:Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie</strong></p>
<p>The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future. </p>
<blockquote>
<p>è¯­ä¹‰äº¤äº’è¾å°„åœºé•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯3Dç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚å®ç°åœºæ™¯ç†è§£å’Œæ“ä½œçš„åµŒå…¥å¼AIï¼‰çš„æœ‰å‰é€”çš„æ”¯æŸ±ã€‚ç„¶è€Œï¼Œç”±äºè¯­è¨€çš„ä¸æ˜ç¡®æ€§ä»¥åŠåœ¨æŸ¥è¯¢å¯¹è±¡ç»„ä»¶æ—¶çš„è´¨é‡ä¸‹é™ï¼Œå¤šç²’åº¦äº¤äº’ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FMLGSæ–¹æ³•ï¼Œå®ƒæ”¯æŒåœ¨3Dé«˜æ–¯æ··åˆï¼ˆ3DGSï¼‰ä¸­è¿›è¡Œéƒ¨åˆ†çº§åˆ«çš„å¼€æ”¾å¼è¯æ±‡æŸ¥è¯¢ã€‚æˆ‘ä»¬åŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰è®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆçš„æµç¨‹æ¥æ„å»ºå’ŒæŸ¥è¯¢å¯¹è±¡çº§å’Œéƒ¨åˆ†çº§çš„è¿è´¯è¯­ä¹‰ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è¯­ä¹‰åå·®ç­–ç•¥æ¥è§£å†³å¯¹è±¡éƒ¨åˆ†ä¹‹é—´è¯­è¨€æ¨¡ç³Šçš„é—®é¢˜ï¼Œé€šè¿‡æ’å€¼ç²¾ç»†ç›®æ ‡çš„è¯­ä¹‰ç‰¹å¾ä»¥è·å–æ›´ä¸°å¯Œä¿¡æ¯ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¯¹è±¡åŠå…¶å¯æè¿°çš„éƒ¨åˆ†ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•çš„æ¯”è¾ƒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å¯ä»¥æ›´å¥½åœ°å®šä½æŒ‡å®šçš„éƒ¨åˆ†çº§ç›®æ ‡ï¼Œè€Œä¸”åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è·å¾—äº†ç¬¬ä¸€åï¼Œå…¶ä¸­FMLGSæ˜¯LERFçš„98å€ã€LangSplatçš„4å€å’ŒLEGaussiansçš„2.5å€å¿«ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å°†FMLGSè¿›ä¸€æ­¥é›†æˆä½œä¸ºè™šæ‹Ÿä»£ç†ï¼Œå¯ä»¥äº¤äº’å¼åœ°æµè§ˆ3Dåœºæ™¯ã€å®šä½ç›®æ ‡å¹¶é€šè¿‡èŠå¤©ç•Œé¢å“åº”ç”¨æˆ·éœ€æ±‚ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„å·¥ä½œåœ¨æœªæ¥è¿›ä¸€æ­¥æ‰©å±•å’Œåº”ç”¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08581v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäº3Dé«˜æ–¯æ··åˆæŠ€æœ¯ï¼ˆ3DGSï¼‰çš„æ–¹æ³•FMLGSï¼Œç”¨äºæ”¯æŒåœ¨è¯­ä¹‰äº¤äº’è¾å°„åœºä¸­å®ç°å¯¹ç‰©ä½“çº§åˆ«çš„å¼€æ”¾è¯æ±‡æŸ¥è¯¢å’Œéƒ¨åˆ†çº§åˆ«çš„æŸ¥è¯¢ã€‚é€šè¿‡Segment Anything Model 2ï¼ˆSAM2ï¼‰æ„å»ºæŸ¥è¯¢ç®¡é“ï¼Œè§£å†³è¯­è¨€æ¨¡ç³Šé—®é¢˜ï¼Œé€šè¿‡è¯­ä¹‰åå·®ç­–ç•¥è§£å†³ç‰©ä½“éƒ¨åˆ†é—´çš„è¯­è¨€æ­§ä¹‰é—®é¢˜ã€‚è®­ç»ƒåï¼Œå¯ä½¿ç”¨è‡ªç„¶è¯­è¨€å¯¹ç‰©ä½“åŠå…¶æè¿°æ€§éƒ¨åˆ†è¿›è¡ŒæŸ¥è¯¢ã€‚ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•çš„æ¯”è¾ƒè¡¨æ˜ï¼ŒFMLGSä¸ä»…åœ¨å®šä½æŒ‡å®šéƒ¨åˆ†çº§åˆ«çš„ç›®æ ‡æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œä¸”åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½å–å¾—äº†ç¬¬ä¸€åçš„å¥½æˆç»©ã€‚æ­¤å¤–ï¼ŒFMLGSä½œä¸ºè™šæ‹Ÿä»£ç†çš„åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œèƒ½å¤Ÿäº¤äº’åœ°æµè§ˆä¸‰ç»´åœºæ™¯ã€å®šä½ç›®æ ‡å¹¶é€šè¿‡èŠå¤©ç•Œé¢å“åº”ç”¨æˆ·éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FMLGSæ–¹æ³•æ”¯æŒåœ¨è¯­ä¹‰äº¤äº’è¾å°„åœºä¸­è¿›è¡Œç‰©ä½“å’Œéƒ¨åˆ†çº§åˆ«çš„å¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚</li>
<li>é€šè¿‡SAM2æ¨¡å‹æ„å»ºæŸ¥è¯¢ç®¡é“ï¼Œå®ç°é«˜æ•ˆçš„å¯¹è±¡å’Œéƒ¨åˆ†çº§åˆ«è¯­ä¹‰æ„å»ºå’ŒæŸ¥è¯¢ã€‚</li>
<li>è¯­ä¹‰åå·®ç­–ç•¥è§£å†³ç‰©ä½“éƒ¨åˆ†é—´çš„è¯­è¨€æ­§ä¹‰é—®é¢˜ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç‰©ä½“åŠå…¶æè¿°æ€§éƒ¨åˆ†ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•æ¯”è¾ƒï¼ŒFMLGSåœ¨å®šä½æŒ‡å®šéƒ¨åˆ†çº§åˆ«çš„ç›®æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢å–å¾—é¢†å…ˆã€‚</li>
<li>FMLGSä½œä¸ºè™šæ‹Ÿä»£ç†çš„åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œèƒ½å¤Ÿäº¤äº’åœ°æµè§ˆä¸‰ç»´åœºæ™¯ã€å®šä½ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69268401c04593649b32fb72e9a7fddd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b454271594f96b845f4031bd4924014f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec6e72d259bdb7b4f13515d381a2f900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4a0e91a86ff8bea698bbf3564d1e787.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b54621024fb28c1fa507553d306d0c1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0d6f265a21571882d997c2842d4dff6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Embodied-Image-Captioning-Self-supervised-Learning-Agents-for-Spatially-Coherent-Image-Descriptions"><a href="#Embodied-Image-Captioning-Self-supervised-Learning-Agents-for-Spatially-Coherent-Image-Descriptions" class="headerlink" title="Embodied Image Captioning: Self-supervised Learning Agents for Spatially   Coherent Image Descriptions"></a>Embodied Image Captioning: Self-supervised Learning Agents for Spatially   Coherent Image Descriptions</h2><p><strong>Authors:Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo Natale</strong></p>
<p>We present a self-supervised method to improve an agentâ€™s abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œç”¨äºæé«˜ä»£ç†åœ¨ä¸»åŠ¨æ¢ç´¢é€šç”¨ç¯å¢ƒæ—¶æè¿°ä»»æ„å¯¹è±¡çš„èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºå½“å‰æ¨¡å‹ç”±äºä¸åŒçš„ç›¸æœºè§†è§’å’Œæ‚ä¹±å› ç´ è€Œéš¾ä»¥è·å¾—è¿è´¯çš„å›¾åƒæ ‡é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œå¯¹ç°æœ‰çš„æ ‡é¢˜æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡å…±è¯†æœºåˆ¶æé«˜è·¨è§†å›¾æ ‡é¢˜çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œä»£ç†æ¢ç´¢ç¯å¢ƒï¼Œæ”¶é›†å˜ˆæ‚çš„å›¾åƒ-æ ‡é¢˜å¯¹ã€‚ç„¶åï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å…±è¯†ä¸ºæ¯ä¸ªå¯¹è±¡å®ä¾‹æç‚¼ä¸€è‡´çš„ä¼ªæ ‡é¢˜ã€‚æœ€åï¼Œè¿™äº›ä¼ªæ ‡é¢˜ç”¨äºå¾®è°ƒç°æˆçš„æ ‡é¢˜æ¨¡å‹ï¼Œå¹¶åŠ å…¥å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬åˆ†æäº†æ ‡é¢˜æ¨¡å‹ã€æ¢ç´¢ç­–ç•¥ã€ä¼ªæ ‡ç­¾æ–¹æ³•å’Œå¾®è°ƒç­–ç•¥çš„ç»„åˆï¼Œåœ¨æˆ‘ä»¬æ‰‹åŠ¨æ ‡è®°çš„æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå¯ä»¥è®­ç»ƒä¸€ç§ç­–ç•¥æ¥æŒ–æ˜æ ·æœ¬ä¸­æ›´é«˜çš„åˆ†æ­§ä¸ç»å…¸åŸºçº¿ç›¸æ¯”ã€‚æˆ‘ä»¬çš„ä¼ªæ ‡é¢˜æ–¹æ³•ç»“åˆæ‰€æœ‰ç­–ç•¥ï¼Œä¸å…¶ä»–ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ ‡é¢˜çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ä»£ç å’Œæµ‹è¯•é›†æ³¨é‡Šå¯åœ¨<a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/%E6%89%BE%E5%88%B0%E3%80%82">https://hsp-iit.github.io/embodied-captioning/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08531v1">PDF</a> 11 pages, 8 figures, 5 tables, code and test set annotations   available at <a target="_blank" rel="noopener" href="https://hsp-iit.github.io/embodied-captioning/">https://hsp-iit.github.io/embodied-captioning/</a></p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„æ–¹æ³•ï¼Œç”¨äºæå‡ä»£ç†åœ¨é€šç”¨ç¯å¢ƒä¸­æè¿°ä»»æ„ç‰©ä½“çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶å¯¹ç°æœ‰çš„æè¿°æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡å…±è¯†æœºåˆ¶æé«˜å›¾åƒæè¿°çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œä»£ç†æ¢ç´¢ç¯å¢ƒï¼Œæ”¶é›†å˜ˆæ‚çš„å›¾åƒ-æè¿°å¯¹ã€‚æ¥ç€ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç‰©ä½“å®ä¾‹è¿›è¡Œä¸€è‡´æ€§ä¼ªæè¿°æç‚¼ã€‚æœ€åï¼Œè¿™äº›ä¼ªæè¿°ç”¨äºå¾®è°ƒå¸‚é¢ä¸Šçš„æè¿°æ¨¡å‹ï¼Œå¹¶åŠ å…¥å¯¹æ¯”å­¦ä¹ ã€‚è¯¥æ–¹æ³•åœ¨åˆ†æå„ç§ç»„åˆè¡¨ç°æ–¹é¢ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ”¿ç­–ã€ä¼ªæ ‡ç­¾æ–¹æ³•å’Œå¾®è°ƒç­–ç•¥æ–¹é¢ã€‚ä»£ç å’Œæµ‹è¯•é›†æ³¨é‡Šå¯åœ¨ç›¸å…³ç½‘ç«™æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§è‡ªæˆ‘ç›‘ç£æ–¹æ³•ï¼Œæå‡ä»£ç†æè¿°ä»»æ„ç‰©ä½“çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶å¾®è°ƒç°æœ‰æè¿°æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å…±è¯†æœºåˆ¶æé«˜æè¿°çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>æ”¶é›†å˜ˆæ‚çš„å›¾åƒ-æè¿°å¯¹ï¼Œè¿›è¡Œæ¢ç´¢ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æç‚¼ä¸€è‡´æ€§ä¼ªæè¿°ã€‚</li>
<li>ä¼ªæè¿°ç”¨äºå¾®è°ƒå¸‚é¢ä¸Šçš„æè¿°æ¨¡å‹ï¼Œå¹¶åŠ å…¥å¯¹æ¯”å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-32ee73d10a6c67b6e03a51af34e767c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a5136d3b534254bce7a55f0c6c267a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dca6b6e4a0b845d2ff18f67c0b643262.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-291bfe9746c2573272726ddc95826c02.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Task-Memory-Engine-TME-Enhancing-State-Awareness-for-Multi-Step-LLM-Agent-Tasks"><a href="#Task-Memory-Engine-TME-Enhancing-State-Awareness-for-Multi-Step-LLM-Agent-Tasks" class="headerlink" title="Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM   Agent Tasks"></a>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM   Agent Tasks</h2><p><strong>Authors:Ye Ye</strong></p>
<p>Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œå¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¡†æ¶æ— æ³•ç»´æŒå¯¹ä»»åŠ¡çŠ¶æ€çš„ç»“æ„åŒ–ç†è§£ï¼Œé€šå¸¸ä¾èµ–äºçº¿æ€§æç¤ºæ‹¼æ¥æˆ–æµ…å†…å­˜ç¼“å†²åŒºã€‚è¿™å¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€ç»å¸¸å‡ºç°å¹»è§‰å’Œé•¿æœŸè¿è´¯æ€§å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€ç»“æ„åŒ–çš„å†…å­˜æ¨¡å—ï¼Œå®ƒé€šè¿‡åˆ†å±‚çš„ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰æ¥è·Ÿè¸ªä»»åŠ¡æ‰§è¡Œæƒ…å†µã€‚æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”ä¸€ä¸ªä»»åŠ¡æ­¥éª¤ï¼Œå­˜å‚¨ç›¸å…³çš„è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æç¤ºåˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®æ´»åŠ¨èŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºï¼Œä»è€Œæ˜¾è‘—æé«˜æ‰§è¡Œä¸€è‡´æ€§ä¸Šä¸‹æ–‡å®šä½èƒ½åŠ›ã€‚é€šè¿‡å¤šæ­¥éª¤ä»£ç†ä»»åŠ¡çš„æ¡ˆä¾‹ç ”ç©¶å’Œå¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TMEåœ¨æé«˜ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œæ›´å¯è§£é‡Šçš„è¡Œä¸ºæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä¸”å®ç°å¼€é”€æå°ã€‚TMEçš„å®Œæ•´å®ç°å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/biubiotomato/TME-Agent%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/biubiotomato/TME-Agentè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08525v1">PDF</a> 14 pages, 5 figures. Preprint prepared for future submission.   Includes implementation and token-efficiency analysis. Code at   <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a></p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨ä½œå¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¡†æ¶æ— æ³•å¯¹ä»»åŠ¡çŠ¶æ€è¿›è¡Œç»“æ„åŒ–ç†è§£ï¼Œé€šå¸¸ä¾èµ–äºçº¿æ€§æç¤ºä¸²è”æˆ–æµ…ç¼“å†²åŒºï¼Œè¿™å¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€é¢‘ç¹å‡ºç°å¹»è§‰å’Œé•¿æœŸè¿è´¯æ€§è¾ƒå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰å’Œå±‚æ¬¡åŒ–çš„ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€ç»“æ„åŒ–çš„å†…å­˜æ¨¡å—ï¼Œç”¨äºè·Ÿè¸ªä»»åŠ¡æ‰§è¡Œã€‚æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”äºä¸€ä¸ªä»»åŠ¡æ­¥éª¤ï¼Œå­˜å‚¨ç›¸å…³çš„è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå½“å‰èŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºçš„æç¤ºåˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ‰§è¡Œä¸€è‡´æ€§ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ­¥éª¤ä»£ç†ä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TMEåœ¨æé«˜ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œè§£é‡Šæ€§è¡Œä¸ºæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸”å®ç°å¼€é”€æå°ã€‚å®Œæ•´çš„TMEå®ç°å¯ä»¥åœ¨XXXä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­çš„ä½¿ç”¨é€æ¸å¢åŠ ï¼Œä½†ä»é¢ä¸´ç»“æ„åŒ–ä»»åŠ¡çŠ¶æ€ç†è§£çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¡†æ¶é€šå¸¸ä¾èµ–çº¿æ€§æç¤ºä¸²è”å’Œæµ…ç¼“å†²åŒºï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šå’Œé•¿æœŸè¿è´¯æ€§è¾ƒå·®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰å’Œå±‚æ¬¡åŒ–çš„ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰ï¼Œä»¥ç»“æ„åŒ–æ–¹å¼è·Ÿè¸ªä»»åŠ¡æ‰§è¡Œã€‚</li>
<li>ä»»åŠ¡è®°å¿†æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨ä»»åŠ¡æ­¥éª¤çš„è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŠ¨æ€ç”ŸæˆLLMæç¤ºçš„æç¤ºåˆæˆæ–¹æ³•ï¼ŒåŸºäºå½“å‰çš„ä»»åŠ¡çŠ¶æ€ï¼Œæé«˜äº†æ‰§è¡Œä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒè¯æ˜ï¼ŒTMEåœ¨ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œè§£é‡Šæ€§è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>TMEçš„å®ç°å¼€é”€è¾ƒå°ï¼Œå®Œæ•´çš„å®ç°å¯åœ¨XXXä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acec3be0434fd047eae93d6b8bec6ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56cfc4f925f501a97f8aa2f5539a97f4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-AI-Scientist-v2-Workshop-Level-Automated-Scientific-Discovery-via-Agentic-Tree-Search"><a href="#The-AI-Scientist-v2-Workshop-Level-Automated-Scientific-Discovery-via-Agentic-Tree-Search" class="headerlink" title="The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via   Agentic Tree Search"></a>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via   Agentic Tree Search</h2><p><strong>Authors:Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha</strong></p>
<p>AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at <a target="_blank" rel="noopener" href="https://github.com/SakanaAI/AI-Scientist-v2">https://github.com/SakanaAI/AI-Scientist-v2</a> to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åœ¨æ”¹å˜ç§‘å­¦å‘ç°çš„æ–¹å¼ä¸­å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬ä»‹ç»äº†AIç§‘å­¦å®¶v2ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿäº§ç”Ÿç¬¬ä¸€ç¯‡å®Œå…¨ç”±AIç”Ÿæˆã€ç»è¿‡åŒè¡Œè¯„å®¡è®¤å¯çš„ç ”è®¨ä¼šè®ºæ–‡ã€‚è¯¥ç³»ç»Ÿå¯ä»¥è¿­ä»£åœ°æå‡ºç§‘å­¦å‡è®¾ï¼Œè®¾è®¡å¹¶æ‰§è¡Œå®éªŒï¼Œåˆ†æå’Œå¯è§†åŒ–æ•°æ®ï¼Œå¹¶è‡ªä¸»æ’°å†™ç§‘å­¦æ‰‹ç¨¿ã€‚ä¸å‰ä¸€ä»£äº§å“ï¼ˆv1ï¼ŒLuç­‰äººï¼ŒarXivï¼š2408.06292ï¼‰ç›¸æ¯”ï¼ŒAIç§‘å­¦å®¶v2æ¶ˆé™¤äº†å¯¹äººå·¥ç¼–ç æ¨¡æ¿çš„ä¾èµ–ï¼Œåœ¨ä¸åŒçš„æœºå™¨å­¦ä¹ é¢†åŸŸå®ç°äº†æœ‰æ•ˆçš„æ³›åŒ–ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ¸è¿›å¼æ™ºèƒ½æ ‘æœç´¢æ–¹æ³•ï¼Œç”±ä¸“èŒå®éªŒç®¡ç†æ™ºèƒ½ä½“è´Ÿè´£ç®¡ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹AIè¯„å®¡ç»„ä»¶è¿›è¡Œå¢å¼ºï¼Œé›†æˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åé¦ˆå¾ªç¯ï¼Œä»¥è¿­ä»£ä¼˜åŒ–å›¾è¡¨çš„å†…å®¹å’Œç¾å­¦ã€‚æˆ‘ä»¬é€šè¿‡å‘åŒè¡Œè¯„å®¡çš„ICLRç ”è®¨ä¼šæäº¤ä¸‰ä»½å®Œå…¨è‡ªä¸»çš„æ‰‹ç¨¿æ¥è¯„ä¼°AIç§‘å­¦å®¶v2çš„æ•ˆæœã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå…¶ä¸­ä¸€ç¯‡è®ºæ–‡å¾—åˆ†é«˜åˆ°è¶…è¿‡äº†å¹³å‡äººç±»æ¥å—é˜ˆå€¼ï¼Œæ ‡å¿—ç€ç¬¬ä¸€ç¯‡å®Œå…¨ç”±AIç”Ÿæˆå¹¶æˆåŠŸé€šè¿‡åŒè¡Œè¯„å®¡çš„è®ºæ–‡çš„è¯ç”Ÿã€‚è¿™ä¸€æˆå°±å‡¸æ˜¾äº†äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶å„ä¸ªæ–¹é¢çš„æ—¥ç›Šå¢é•¿çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¢„è®¡è‡ªä¸»ç§‘å­¦å‘ç°æŠ€æœ¯çš„è¿›ä¸€æ­¥è¿›æ­¥å°†å¯¹äººç±»çŸ¥è¯†äº§ç”Ÿæ·±è¿œå½±å“ï¼Œåœ¨ç ”ç©¶å’Œç”Ÿäº§åŠ›æ–¹é¢å®ç°å‰æ‰€æœªæœ‰çš„å¯æ‰©å±•æ€§ï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿç§‘å­¦çªç ´ï¼Œæå¤§åœ°é€ ç¦æ•´ä¸ªç¤¾ä¼šã€‚æˆ‘ä»¬å·²ç»å°†ä»£ç å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/SakanaAI/AI-Scientist-v2%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%BF%99%E9%A1%B9%E5%8F%98%E9%9D%A9%E6%8A%80%E6%9C%AF%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E8%AE%A8%E8%AE%BA%E4%BA%86%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9C%A8%E7%A7%91%E5%AD%A6%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%8C%E5%8C%85%E6%8B%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98%E3%80%82">https://github.com/SakanaAI/AI-Scientist-v2ï¼Œä»¥ä¿ƒè¿›è¿™é¡¹å˜é©æŠ€æœ¯çš„æœªæ¥å‘å±•ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ä¸­çš„ä½œç”¨ï¼ŒåŒ…æ‹¬äººå·¥æ™ºèƒ½çš„å®‰å…¨é—®é¢˜ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08066v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„ä½œç”¨æ—¥ç›Šé‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºAIç§‘å­¦å®¶v2ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯è‡ªåŠ¨ç”Ÿæˆç¬¬ä¸€ç¯‡è¢«åŒè¡Œè¯„å®¡æ¥å—çš„å·¥ä½œåŠè®ºæ–‡ã€‚è¯¥ç³»ç»Ÿå¯è¿­ä»£åœ°æå‡ºç§‘å­¦å‡è®¾ï¼Œè®¾è®¡å¹¶æ‰§è¡Œå®éªŒï¼Œåˆ†æå’Œå¯è§†åŒ–æ•°æ®ï¼Œå¹¶è‡ªä¸»æ’°å†™ç§‘å­¦è®ºæ–‡ã€‚ç›¸è¾ƒäºå…¶å‰èº«AIç§‘å­¦å®¶v1ï¼ŒAIç§‘å­¦å®¶v2æœ‰æ•ˆæ¶ˆé™¤äº†å¯¹äººç±»ç¼–å†™ä»£ç æ¨¡æ¿çš„ä¾èµ–ï¼Œå¯å¹¿æ³›åº”ç”¨äºä¸åŒçš„æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œå¹¶åˆ©ç”¨ä¸“ç”¨çš„å®éªŒç®¡ç†ä»£ç†é‡‡ç”¨æ–°å‹æ¸è¿›å¼ä»£ç†æ ‘æœç´¢æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¢å¼ºäº†AIè¯„å®¡ç»„ä»¶ï¼Œé›†æˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åé¦ˆå¾ªç¯ï¼Œä»¥è¿­ä»£æ–¹å¼å®Œå–„å›¾è¡¨çš„å†…å®¹å’Œç¾å­¦ã€‚é€šè¿‡å¯¹AIç§‘å­¦å®¶v2ç³»ç»Ÿçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæäº¤çš„ä¸‰ç¯‡å®Œå…¨è‡ªä¸»çš„è®ºæ–‡ä¸­æœ‰ä¸€ç¯‡è¾¾åˆ°äº†äººç±»å¹³å‡æ¥å—é˜ˆå€¼ä¹‹ä¸Šï¼Œæ ‡å¿—ç€é¦–ç¯‡å®Œå…¨ç”±AIç”Ÿæˆçš„è®ºæ–‡æˆåŠŸé€šè¿‡åŒè¡Œè¯„å®¡ã€‚è¿™è¯æ˜äº†AIåœ¨ç§‘ç ”å„æ–¹é¢çš„èƒ½åŠ›ä¸æ–­å¢é•¿ã€‚æˆ‘ä»¬é¢„æœŸè‡ªä¸»ç§‘å­¦å‘ç°æŠ€æœ¯çš„è¿›ä¸€æ­¥è¿›æ­¥å°†æå¤§åœ°å½±å“äººç±»çŸ¥è¯†ç”Ÿæˆï¼Œå®ç°å‰æ‰€æœªæœ‰çš„ç ”ç©¶ç”Ÿäº§åŠ›æ‰©å±•ï¼Œå¹¶åŠ é€Ÿç§‘å­¦çªç ´ï¼Œæå¤§åœ°é€ ç¦æ•´ä¸ªç¤¾ä¼šã€‚æˆ‘ä»¬å·²ç»å…¬å¼€äº†æºä»£ç ä»¥ä¿ƒè¿›è¿™é¡¹å˜é©æ€§æŠ€æœ¯çš„æœªæ¥å‘å±•ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿæ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ä¸­çš„è§’è‰²ï¼ŒåŒ…æ‹¬äººå·¥æ™ºèƒ½çš„å®‰å…¨é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„ä½œç”¨æ—¥ç›Šé‡è¦ï¼Œèƒ½å¤Ÿè‡ªä¸»å®Œæˆä»ç§‘å­¦å‡è®¾åˆ°è®ºæ–‡æ’°å†™çš„å…¨è¿‡ç¨‹ã€‚</li>
<li>AIç§‘å­¦å®¶v2ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå¹¶å‘è¡¨è¢«åŒè¡Œè¯„å®¡æ¥å—çš„å·¥ä½œåŠè®ºæ–‡ã€‚</li>
<li>AIç§‘å­¦å®¶v2ç›¸è¾ƒäºå‰ç‰ˆæœ¬ï¼Œå‡å°‘å¯¹äººç±»ç¼–å†™ä»£ç æ¨¡æ¿çš„ä¾èµ–ï¼Œå¹¶æœ‰æ•ˆè·¨è¶Šä¸åŒçš„æœºå™¨å­¦ä¹ é¢†åŸŸã€‚</li>
<li>é‡‡ç”¨äº†æ–°å‹çš„æ¸è¿›å¼ä»£ç†æ ‘æœç´¢æ–¹æ³•ç®¡ç†å®éªŒã€‚</li>
<li>AIè¯„å®¡ç»„ä»¶é€šè¿‡é›†æˆVLMåé¦ˆå¾ªç¯ï¼Œæå‡äº†å›¾è¡¨å†…å®¹å’Œç¾å­¦çš„è¿­ä»£å®Œå–„èƒ½åŠ›ã€‚</li>
<li>å…¶ä¸­ä¸€ç¯‡å®Œå…¨ç”±AIç”Ÿæˆçš„è®ºæ–‡æˆåŠŸè¾¾åˆ°å¹¶è¶…è¶Šäº†äººç±»å¹³å‡æ¥å—é˜ˆå€¼ï¼Œè¡¨æ˜AIçš„ç§‘å­¦ç ”ç©¶èƒ½åŠ›æ­£åœ¨å¢é•¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96b24aab82254eb282ce64f792f969d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f31c5c34401fd1f1fed25ff1d3ff94b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a12e5f6cfa9ab9492e24f43801a8bb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a35d84c80a0fd1580ed75369f95e4e99.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Agent-Based-Simulations-of-Online-Political-Discussions-A-Case-Study-on-Elections-in-Germany"><a href="#Agent-Based-Simulations-of-Online-Political-Discussions-A-Case-Study-on-Elections-in-Germany" class="headerlink" title="Agent-Based Simulations of Online Political Discussions: A Case Study on   Elections in Germany"></a>Agent-Based Simulations of Online Political Discussions: A Case Study on   Elections in Germany</h2><p><strong>Authors:Abdul Sittar, Simon MÃ¼nker, Fabio Sartori, Andreas Reitenbach, Achim Rettinger, Michael MÃ¤s, Alenka GuÄek, Marko Grobelnik</strong></p>
<p>User engagement on social media platforms is influenced by historical context, time constraints, and reward-driven interactions. This study presents an agent-based simulation approach that models user interactions, considering past conversation history, motivation, and resource constraints. Utilizing German Twitter data on political discourse, we fine-tune AI models to generate posts and replies, incorporating sentiment analysis, irony detection, and offensiveness classification. The simulation employs a myopic best-response model to govern agent behavior, accounting for decision-making based on expected rewards. Our results highlight the impact of historical context on AI-generated responses and demonstrate how engagement evolves under varying constraints. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°ä¸Šçš„ç”¨æˆ·å‚ä¸åº¦å—åˆ°å†å²èƒŒæ™¯ã€æ—¶é—´é™åˆ¶å’Œå¥–åŠ±é©±åŠ¨äº¤äº’çš„å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸»ä½“çš„æ¨¡æ‹Ÿæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è€ƒè™‘è¿‡å»çš„å¯¹è¯å†å²ã€åŠ¨æœºå’Œèµ„æºçº¦æŸæ¥æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ã€‚æˆ‘ä»¬åˆ©ç”¨å¾·å›½æ¨ç‰¹ä¸Šçš„æ”¿æ²»è¯è¯­æ•°æ®å¾®è°ƒäººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œä»¥ç”Ÿæˆå¸–å­å’Œå›å¤ï¼ŒåŒæ—¶èå…¥æƒ…æ„Ÿåˆ†æã€è®½åˆºæ£€æµ‹å’Œæ”»å‡»æ€§åˆ†ç±»ã€‚æ¨¡æ‹Ÿé‡‡ç”¨è¿‘è§†æœ€ä½³ååº”æ¨¡å‹æ¥æ§åˆ¶ä¸»ä½“è¡Œä¸ºï¼ŒåŸºäºé¢„æœŸå¥–åŠ±è¿›è¡Œå†³ç­–ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†å†å²èƒŒæ™¯å¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå›å¤çš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¸åŒçº¦æŸä¸‹å‚ä¸åº¦å¦‚ä½•å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24199v2">PDF</a> I forgot to take the consent from all other co authors and they want   to withdraw it</p>
<p><strong>Summary</strong><br>ç¤¾äº¤å¹³å°ä¸Šç”¨æˆ·å‚ä¸åº¦å—å†å²èƒŒæ™¯ã€æ—¶é—´é™åˆ¶å’Œå¥–åŠ±é©±åŠ¨äº¤äº’çš„å½±å“ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºä¸»ä½“çš„æ¨¡æ‹Ÿæ–¹æ³•ï¼Œè€ƒè™‘è¿‡å»çš„å¯¹è¯å†å²ã€åŠ¨æœºå’Œèµ„æºé™åˆ¶æ¥æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ã€‚åˆ©ç”¨å¾·å›½æ¨ç‰¹æ”¿æ²»è¯è¯­æ•°æ®ï¼Œæˆ‘ä»¬å¾®è°ƒäº†äººå·¥æ™ºèƒ½æ¨¡å‹ä»¥ç”Ÿæˆå¸–å­å’Œå›å¤ï¼Œç»“åˆæƒ…æ„Ÿåˆ†æã€è®½åˆºæ£€æµ‹å’Œæ”»å‡»æ€§åˆ†ç±»ã€‚æ¨¡æ‹Ÿé‡‡ç”¨è¿‘è§†æœ€ä¼˜ååº”æ¨¡å‹æ¥æ§åˆ¶ä¸»ä½“è¡Œä¸ºï¼ŒåŸºäºé¢„æœŸå¥–åŠ±è¿›è¡Œå†³ç­–ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†å†å²èƒŒæ™¯å¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå“åº”çš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†ä¸åŒçº¦æŸä¸‹å‚ä¸åº¦å¦‚ä½•å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”¨æˆ·å‚ä¸åº¦åœ¨ç¤¾äº¤å¹³å°ä¸Šå—å¤šç§å› ç´ å½±å“ï¼ŒåŒ…æ‹¬å†å²èƒŒæ™¯ã€æ—¶é—´é™åˆ¶å’Œå¥–åŠ±é©±åŠ¨äº¤äº’ã€‚</li>
<li>é€šè¿‡å¯¹è¿‡å»å¯¹è¯å†å²ã€åŠ¨æœºå’Œèµ„æºé™åˆ¶çš„è€ƒè™‘ï¼Œå¯ä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’è¡Œä¸ºã€‚</li>
<li>åˆ©ç”¨å¾·å›½æ¨ç‰¹æ”¿æ²»è¯è¯­æ•°æ®ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´åŠ çœŸå®å’Œå…·æœ‰è¯´æœåŠ›çš„å¸–å­å’Œå›å¤ã€‚</li>
<li>é€šè¿‡æƒ…æ„Ÿåˆ†æã€è®½åˆºæ£€æµ‹å’Œæ”»å‡»æ€§åˆ†ç±»ç­‰æ‰‹æ®µï¼Œå¯ä»¥æ›´æ·±å…¥åœ°ç†è§£ç”¨æˆ·çš„æƒ…æ„Ÿå’Œååº”ã€‚</li>
<li>ä¸»ä½“æ¨¡æ‹Ÿä¸­é‡‡ç”¨è¿‘è§†æœ€ä¼˜ååº”æ¨¡å‹æ¥æ¨¡æ‹Ÿä¸»ä½“çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŸºäºé¢„æœŸå¥–åŠ±æ¥æŒ‡å¯¼è¡Œä¸ºã€‚</li>
<li>å†å²èƒŒæ™¯å¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å“åº”å…·æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b05bebbbc4930db9bff8325c156a375b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83ad468a93db3cf81765a0b20d525e89.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Late-Breaking-Results-Breaking-Symmetry-Unconventional-Placement-of-Analog-Circuits-using-Multi-Level-Multi-Agent-Reinforcement-Learning"><a href="#Late-Breaking-Results-Breaking-Symmetry-Unconventional-Placement-of-Analog-Circuits-using-Multi-Level-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Late Breaking Results: Breaking Symmetry- Unconventional Placement of   Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning"></a>Late Breaking Results: Breaking Symmetry- Unconventional Placement of   Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Supriyo Maji, Linran Zhao, Souradip Poddar, David Z. Pan</strong></p>
<p>Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing. </p>
<blockquote>
<p>å¸ƒå±€ä¾èµ–æ•ˆåº”ï¼ˆLDEsï¼‰å¯¹æ¨¡æ‹Ÿç”µè·¯æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚ä¼ ç»Ÿä¸Šï¼Œè®¾è®¡è€…ä¾é å¯¹ç§°æ”¾ç½®ç”µè·¯å…ƒä»¶æ¥ç¼“è§£LDEså¼•èµ·çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ•ˆåº”çš„éçº¿æ€§æ€§è´¨ï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€è¾¾ä¸åˆ°é¢„æœŸæ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›®æ ‡é©±åŠ¨çš„å¤šå±‚æ¬¡å¤šæ™ºèƒ½ä½“Qå­¦ä¹ æ¡†æ¶ï¼Œä»¥æ¢ç´¢æ¨¡æ‹Ÿå¸ƒå±€çš„éä¼ ç»Ÿè®¾è®¡ç©ºé—´ï¼Œä¸ºä¼˜åŒ–æ¨¡æ‹Ÿç”µè·¯æ€§èƒ½å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ¯”å½“å‰æœ€å…ˆè¿›çš„å¸ƒå±€æŠ€æœ¯æ›´å¥½çš„å˜åŒ–æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨æ¨¡æ‹Ÿå¸ƒå±€è‡ªåŠ¨åŒ–ä¸­çš„é¦–æ¬¡åº”ç”¨ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸åŸºäºæ¨¡æ‹Ÿé€€ç«çš„éæœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œå¯¹æ¯”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22958v3">PDF</a> 2 pages, 3 figures, Proceedings of the 62nd ACM&#x2F;IEEE Design   Automation Conference (DAC), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¸ƒå±€ä¾èµ–æ•ˆåº”ï¼ˆLDEsï¼‰å¯¹æ¨¡æ‹Ÿç”µè·¯æ€§èƒ½çš„å½±å“ã€‚ä¼ ç»Ÿçš„ç”µè·¯è®¾è®¡é€šå¸¸é‡‡ç”¨å¯¹ç§°æ”¾ç½®ç»„ä»¶çš„æ–¹æ³•æ¥å‡è½»è¿™äº›å½±å“ï¼Œä½†ç”±äºè¿™äº›æ•ˆåº”çš„éçº¿æ€§æ€§è´¨ï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¸å¤Ÿæœ‰æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥ç›®æ ‡é©±åŠ¨çš„å¤šå±‚æ¬¡å¤šæ™ºèƒ½ä½“Qå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ¢ç´¢æ¨¡æ‹Ÿå¸ƒå±€çš„éä¼ ç»Ÿè®¾è®¡ç©ºé—´ï¼Œä¸ºä¼˜åŒ–æ¨¡æ‹Ÿç”µè·¯æ€§èƒ½å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚è¯¥æ–¹æ³•å®ç°äº†ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å¸ƒå±€æŠ€æœ¯çš„æ€§èƒ½å˜åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ¨¡æ‹Ÿå¸ƒå±€è‡ªåŠ¨åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡æ‹Ÿç”µè·¯å—åˆ°å¸ƒå±€ä¾èµ–æ•ˆåº”ï¼ˆLDEsï¼‰çš„å½±å“ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¯¹ç§°æ”¾ç½®ç”µè·¯ç»„ä»¶æ¥å‡è½»LDEsçš„å½±å“ï¼Œä½†æ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä»¥ç›®æ ‡é©±åŠ¨çš„å¤šå±‚æ¬¡å¤šæ™ºèƒ½ä½“Qå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„éä¼ ç»Ÿè®¾è®¡ç©ºé—´æ¢ç´¢ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å¸ƒå±€æŠ€æœ¯çš„æ€§èƒ½å˜åŒ–ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ¨¡æ‹Ÿå¸ƒå±€è‡ªåŠ¨åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸åŸºäºæ¨¡æ‹Ÿé€€ç«çš„éæœºå™¨å­¦ä¹ æ–¹æ³•æœ‰æ˜¾è‘—åŒºåˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-716160d38561be95ff25b651567b2209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d516abf16678441dc005596a8c914fd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c4af892bd97857f41cac508abe3a043.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CVE-Bench-A-Benchmark-for-AI-Agentsâ€™-Ability-to-Exploit-Real-World-Web-Application-Vulnerabilities"><a href="#CVE-Bench-A-Benchmark-for-AI-Agentsâ€™-Ability-to-Exploit-Real-World-Web-Application-Vulnerabilities" class="headerlink" title="CVE-Bench: A Benchmark for AI Agentsâ€™ Ability to Exploit Real-World Web   Application Vulnerabilities"></a>CVE-Bench: A Benchmark for AI Agentsâ€™ Ability to Exploit Real-World Web   Application Vulnerabilities</h2><p><strong>Authors:Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, Daniel Kang</strong></p>
<p>Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¶Šæ¥è¶Šèƒ½å¤Ÿè‡ªä¸»è¿›è¡Œç½‘ç»œæ”»å‡»ï¼Œå¯¹ç°æœ‰åº”ç”¨æ„æˆé‡å¤§å¨èƒã€‚è¿™ç§æ—¥ç›Šå¢é•¿çš„é£é™©å‡¸æ˜¾äº†ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼ˆbenchmarkï¼‰çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»¥è¯„ä¼°LLMä»£ç†åˆ©ç”¨Webåº”ç”¨ç¨‹åºæ¼æ´çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æ— æ³•è¾¾åˆ°è¦æ±‚ï¼Œå› ä¸ºå®ƒä»¬ä»…é™äºæŠ½è±¡çš„å¤ºæ——ç«èµ›æˆ–ç¼ºä¹å…¨é¢è¦†ç›–ã€‚æ„å»ºé’ˆå¯¹ç°å®ä¸–ç•Œæ¼æ´çš„åŸºå‡†æµ‹è¯•æ¶‰åŠä¸“ä¸šæŠ€æœ¯çš„æ¨¡ä»¿åˆ©ç”¨ä»¥åŠè¯„ä¼°ä¸å¯é¢„æµ‹å¨èƒçš„ç³»ç»Ÿæ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CVEåŸºå‡†æµ‹è¯•ï¼ˆCVE-Benchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå…³é”®ä¸¥é‡æ€§å¸¸è§æ¼æ´å’Œæš´éœ²çš„ç½‘ç»œå®‰å…¨åŸºå‡†æµ‹è¯•ã€‚åœ¨CVEåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ²™ç®±æ¡†æ¶ï¼Œä½¿LLMä»£ç†èƒ½å¤Ÿåœ¨æ¨¡æ‹ŸçœŸå®åœºæ™¯çš„æƒ…å†µä¸‹åˆ©ç”¨æ˜“å—æ”»å‡»çš„Webåº”ç”¨ç¨‹åºï¼ŒåŒæ—¶æœ‰æ•ˆåœ°è¯„ä¼°ä»–ä»¬çš„æ”»å‡»æ•ˆæœã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„ä»£ç†æ¡†æ¶å¯ä»¥è§£å†³é«˜è¾¾ç™¾åˆ†ä¹‹åä¸‰çš„æ¼æ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17332v3">PDF</a> 15 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å…·å¤‡è‡ªä¸»å¼€å±•ç½‘ç»œæ”»å‡»çš„èƒ½åŠ›ï¼Œå¯¹ç°æœ‰åº”ç”¨æ„æˆé‡å¤§å¨èƒã€‚å½“å‰ç¼ºä¹è¯„ä¼°LLMä»£ç†åˆ©ç”¨ç½‘é¡µåº”ç”¨æ¼æ´èƒ½åŠ›çš„ç°å®åŸºå‡†æµ‹è¯•ï¼Œè€Œç°æœ‰åŸºå‡†æµ‹è¯•å¤šä¸ºæŠ½è±¡çš„å¤ºæ——ç«èµ›æˆ–ç¼ºä¹å…¨é¢è¦†ç›–ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºCVE-Benchï¼Œä¸€ä¸ªåŸºäºå…³é”®ä¸¥é‡æ€§å¸¸è§æ¼æ´å’Œæš´éœ²ï¼ˆCVEï¼‰çš„ç°å®ç½‘ç»œå®‰å…¨åŸºå‡†æµ‹è¯•ã€‚CVE-Benchè®¾è®¡äº†ä¸€ä¸ªæ²™ç®±æ¡†æ¶ï¼Œä½¿LLMä»£ç†èƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿç°å®æ¡ä»¶çš„åœºæ™¯ä¸­åˆ©ç”¨è„†å¼±ç½‘é¡µåº”ç”¨ï¼ŒåŒæ—¶æœ‰æ•ˆè¯„ä¼°å…¶æ”»å‡»æ•ˆæœã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€æ–°ä»£ç†æ¡†æ¶å¯è§£å†³é«˜è¾¾13%çš„æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å…·å¤‡è‡ªä¸»ç½‘ç»œæ”»å‡»èƒ½åŠ›ï¼Œå¯¹ç°æœ‰åº”ç”¨æ„æˆå¨èƒã€‚</li>
<li>ç¼ºä¹è¯„ä¼°LLMä»£ç†åˆ©ç”¨ç½‘é¡µåº”ç”¨æ¼æ´èƒ½åŠ›çš„ç°å®åŸºå‡†æµ‹è¯•ã€‚</li>
<li>CVE-Benchæ˜¯ä¸€ä¸ªåŸºäºå…³é”®ä¸¥é‡æ€§å¸¸è§æ¼æ´å’Œæš´éœ²ï¼ˆCVEï¼‰çš„ç°å®ç½‘ç»œå®‰å…¨åŸºå‡†æµ‹è¯•ã€‚</li>
<li>CVE-Benchè®¾è®¡äº†ä¸€ä¸ªæ²™ç®±æ¡†æ¶ï¼Œæ¨¡æ‹Ÿç°å®æ¡ä»¶è¯„ä¼°LLMä»£ç†åˆ©ç”¨è„†å¼±ç½‘é¡µåº”ç”¨çš„èƒ½åŠ›ã€‚</li>
<li>æœ€æ–°ä»£ç†æ¡†æ¶å¯è§£å†³é«˜è¾¾13%çš„æ¼æ´ã€‚</li>
<li>CVE-Benchçš„å‡ºç°æ˜¯ä¸ºäº†è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„ä¸è¶³ï¼Œå¦‚æŠ½è±¡çš„å¤ºæ——ç«èµ›æˆ–ç¼ºä¹å…¨é¢è¦†ç›–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0064663bd910a7fae73ced3dec756c93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c212dfa8b7fb62a7aed512c3558b358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6011146f1c3dac1853736ced504f3bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4d2bb0c7f0c0b307f1546605a91149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd427d7da13bce46d43cfb750791cb93.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval"><a href="#A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval" class="headerlink" title="A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval"></a>A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval</h2><p><strong>Authors:Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li</strong></p>
<p>Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>. </p>
<blockquote>
<p>ä¿¡æ¯æŠ€æœ¯å·²ç»æ·±åˆ»æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚ç½‘ç»œä¸Šåˆ›å»ºã€å…±äº«å’Œä¼ æ’­çš„å¤§é‡å†…å®¹ä½¿å¾—è·å–ç›¸å…³ä¿¡æ¯çš„éš¾åº¦è¶Šæ¥è¶Šå¤§ã€‚åœ¨è¿‡å»çš„äºŒåå¹´ä¸­ï¼Œæ¨èç³»ç»Ÿå’Œæœç´¢ï¼ˆç»Ÿç§°ä¸ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰å·²ç»å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†è¶…è¶Šäººç±»çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºäº†ä¸€èˆ¬ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨æå‡æ¨èç³»ç»Ÿå’Œæœç´¢ç³»ç»Ÿæ–¹é¢çš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„åŠ¨æœºå’Œè§’è‰²ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†ç±»æ¡†æ¶æ¥è¯¦ç»†é˜è¿°ç°æœ‰ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨åº”å¯¹æ¨èå’Œæœç´¢æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†è§è§£ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿå›é¡¾å’Œåˆ†ç±»äº†è¿™äº›é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ç ”ç©¶ï¼Œä¸ºå¦‚ä½•åˆ©ç”¨è¿™ä¸€å…ˆè¿›çš„AIæŠ€æœ¯è¿›è¡Œä¿¡æ¯æ£€ç´¢æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚ä¸ºäº†ç†è§£ç°æœ‰çš„å·¥ä½œï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥åˆ—å‡ºäº†å…³äºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„æ¨èå’Œæœç´¢çš„ç°æœ‰è®ºæ–‡ï¼š<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search%E3%80%82">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Searchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05659v2">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¿¡æ¯æŠ€æœ¯æ·±åˆ»æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚éšç€ç½‘ä¸Šåˆ›å»ºã€å…±äº«å’Œä¼ æ’­çš„å†…å®¹å¤§é‡å¢åŠ ï¼Œè·å–ç›¸å…³ä¿¡æ¯å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚è¿‡å»äºŒåå¹´æ¥ï¼Œæ¨èç³»ç»Ÿå’Œæœç´¢ï¼ˆç»Ÿç§°ä¸ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰å·²ç»æ˜¾è‘—å‘å±•ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œåœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†è¶…è¶Šäººç±»çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºäº†ä¸€èˆ¬ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†LLMä»£ç†åœ¨å¢å¼ºæ¨èå’Œæœç´¢ç³»ç»Ÿæ–¹é¢çš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬è®¨è®ºäº†LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†ç±»æ¡†æ¶æ¥è¯¦ç»†é˜è¿°ç°æœ‰çš„ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMä»£ç†åœ¨åº”å¯¹æ¨èå’Œæœç´¢é¢†åŸŸçš„å½“å‰æŒ‘æˆ˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†è§è§£ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾å’Œåˆ†ç±»äº†LLMä»£ç†åœ¨è¿™äº›é¢†åŸŸçš„ç ”ç©¶ï¼Œä¸ºå¦‚ä½•åˆ©ç”¨è¿™ç§å…ˆè¿›çš„AIæŠ€æœ¯è¿›è¡Œä¿¡æ¯æ£€ç´¢æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æŠ€æœ¯æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ï¼Œä¿¡æ¯æ£€ç´¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ¨èç³»ç»Ÿå’Œæœç´¢åœ¨ä¿¡æ¯æ£€ç´¢ä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œå¹¶å·²æ˜¾è‘—å‘å±•ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶Šäººç±»ï¼Œå…·å¤‡ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
<li>LLMä»£ç†åœ¨å¢å¼ºæ¨èå’Œæœç´¢ç³»ç»Ÿæ–¹é¢å…·å¤‡å˜é©æ½œåŠ›ã€‚</li>
<li>LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²è¢«è®¨è®ºï¼Œå»ºç«‹äº†åˆ†ç±»æ¡†æ¶ä»¥é˜è¿°ç°æœ‰ç ”ç©¶ã€‚</li>
<li>LLMä»£ç†åœ¨åº”å¯¹æ¨èå’Œæœç´¢é¢†åŸŸçš„å½“å‰æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7582776c0e259c15939fabf692554e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9003206c7a510ca96f8efc0e6a109ce5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EmbodiedEval-Evaluate-Multimodal-LLMs-as-Embodied-Agents"><a href="#EmbodiedEval-Evaluate-Multimodal-LLMs-as-Embodied-Agents" class="headerlink" title="EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"></a>EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</h2><p><strong>Authors:Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at <a target="_blank" rel="noopener" href="https://github.com/thunlp/EmbodiedEval">https://github.com/thunlp/EmbodiedEval</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºå®ä½“ä»£ç†æä¾›äº†ç¾å¥½çš„æœªæ¥å‰æ™¯ã€‚ç°æœ‰çš„è¯„ä¼°MLLMçš„åŸºå‡†æµ‹è¯•ä¸»è¦åˆ©ç”¨é™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œå°†è¯„ä¼°é™åˆ¶åœ¨éäº¤äº’å¼åœºæ™¯ä¸­ã€‚åŒæ—¶ï¼Œç°æœ‰çš„å®ä½“AIåŸºå‡†æµ‹è¯•å…·æœ‰ç‰¹å®šçš„ä»»åŠ¡æ€§ï¼Œå¹¶ä¸”ä¸å¤Ÿå¤šæ ·åŒ–ï¼Œæ— æ³•å……åˆ†è¯„ä¼°MLLMsçš„å®ä½“èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EmbodiedEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”äº¤äº’å¼çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹å…·æœ‰å®ä½“ä»»åŠ¡çš„MLLMsè¿›è¡Œè¯„ä¼°ã€‚EmbodiedEvalåœ¨125ä¸ªå¤šæ ·åŒ–çš„3Dåœºæ™¯ä¸­è®¾æœ‰328ä¸ªç‹¬ç‰¹ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ç»è¿‡ä¸¥æ ¼ç­›é€‰å’Œæ ‡æ³¨ã€‚å®ƒæ¶µç›–äº†å¹¿æ³›çš„ç°æœ‰å®ä½“AIä»»åŠ¡ï¼Œå…·æœ‰æ˜¾è‘—å¢å¼ºçš„å¤šæ ·æ€§ï¼Œå…¨éƒ¨éƒ½åœ¨ç»Ÿä¸€çš„æ¨¡æ‹Ÿè¯„ä¼°æ¡†æ¶å†…ï¼Œè¯¥æ¡†æ¶ä¸“é—¨é’ˆå¯¹MLLMsè®¾è®¡ã€‚ä»»åŠ¡åˆ†ä¸ºäº”ä¸ªç±»åˆ«ï¼šå¯¼èˆªã€å¯¹è±¡äº¤äº’ã€ç¤¾ä¼šäº¤äº’ã€å±æ€§é—®ç­”å’Œç©ºé—´é—®ç­”ï¼Œä»¥è¯„ä¼°ä»£ç†çš„ä¸åŒèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨EmbodiedEvalä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„MLLMsï¼Œå‘ç°å®ƒä»¬åœ¨å®ä½“ä»»åŠ¡æ–¹é¢ä¸äººç±»æ°´å¹³å­˜åœ¨å¾ˆå¤§å·®è·ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ç°æœ‰MLLMåœ¨å®ä½“èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„å¼€å‘æä¾›äº†è§è§£ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/thunlp/EmbodiedEval%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%89%80%E6%9C%89%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E6%8B%9F%E6%A1%86%E6%9E%B6%E3%80%82">https://github.com/thunlp/EmbodiedEvalä¸Šå…¬å¼€äº†æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œæ¨¡æ‹Ÿæ¡†æ¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11858v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ™ºèƒ½ä½“é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´è¯„ä¼°éš¾é¢˜ã€‚ç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦ä½¿ç”¨é™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œå±€é™äºéäº’åŠ¨åœºæ™¯ã€‚é’ˆå¯¹æ­¤ï¼Œæœ¬æ–‡æå‡ºEmbodiedEvalï¼Œä¸€ä¸ªå…¨é¢äº’åŠ¨çš„è¯„ä»·åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMsçš„å®ä½“ä»»åŠ¡è¡¨ç°ã€‚EmbodiedEvalåŒ…å«125ä¸ªä¸åŒåœºæ™¯çš„328ä¸ªç‹¬ç‰¹ä»»åŠ¡ï¼Œæ¶µç›–å¹¿æ³›çš„ç°æœ‰å®ä½“AIä»»åŠ¡ï¼Œå¹¶åœ¨ç»Ÿä¸€çš„æ¨¡æ‹Ÿä¸è¯„ä¼°æ¡†æ¶ä¸‹å®šåˆ¶è¯„ä¼°MLLMsçš„ä¸åŒèƒ½åŠ›ã€‚ä»»åŠ¡åˆ†ä¸ºå¯¼èˆªã€ç‰©ä½“äº’åŠ¨ã€ç¤¾äº¤äº’åŠ¨ã€å±æ€§é—®ç­”åŠç©ºé—´é—®ç­”äº”å¤§ç±»ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰MLLMsåœ¨å®ä½“ä»»åŠ¡ä¸Šæ˜¾è‘—è½åäºäººç±»æ°´å¹³ï¼Œçªæ˜¾å…¶æœªæ¥å‘å±•æ–¹å‘ã€‚æˆ‘ä»¬å…¬å¼€æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œæ¨¡æ‹Ÿæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ™ºèƒ½ä½“é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦ä½¿ç”¨é™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œå±€é™äºéäº’åŠ¨åœºæ™¯ï¼Œæ— æ³•å……åˆ†è¯„ä¼°MLLMsçš„å®ä½“èƒ½åŠ›ã€‚</li>
<li>EmbodiedEvalæ˜¯ä¸€ä¸ªå…¨é¢äº’åŠ¨çš„è¯„ä»·åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMsçš„å®ä½“ä»»åŠ¡è¡¨ç°ï¼ŒåŒ…å«328ä¸ªç‹¬ç‰¹ä»»åŠ¡ï¼Œæ¶µç›–å¹¿æ³›çš„ç°æœ‰å®ä½“AIä»»åŠ¡ã€‚</li>
<li>EmbodiedEvalçš„ä»»åŠ¡åˆ†ä¸ºäº”å¤§ç±»ï¼šå¯¼èˆªã€ç‰©ä½“äº’åŠ¨ã€ç¤¾äº¤äº’åŠ¨ã€å±æ€§é—®ç­”åŠç©ºé—´é—®ç­”ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒèƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰MLLMsåœ¨å®ä½“ä»»åŠ¡ä¸Šæ˜¾è‘—è½åäºäººç±»æ°´å¹³ã€‚</li>
<li>å…¬å¼€æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œæ¨¡æ‹Ÿæ¡†æ¶ï¼Œä¾¿äºæœªæ¥ç ”ç©¶ä¸å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07336ceb162622cd249372c52c3c077c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b5faab522a2345ce0b57a5cda2797c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91b6bfa7a5b4d47937fca84099c7bd26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5650983f46a355fb8cc44ac6ac5dd00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9622a0e405fc2a4437ce21c7de2d4c72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec92c77c608b2de3e2246ff258e28b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b93d3a2c16839c5eca1c782975d3bba.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MrSteve-Instruction-Following-Agents-in-Minecraft-with-What-Where-When-Memory"><a href="#MrSteve-Instruction-Following-Agents-in-Minecraft-with-What-Where-When-Memory" class="headerlink" title="MrSteve: Instruction-Following Agents in Minecraft with What-Where-When   Memory"></a>MrSteve: Instruction-Following Agents in Minecraft with What-Where-When   Memory</h2><p><strong>Authors:Junyeong Park, Junmo Cho, Sungjin Ahn</strong></p>
<p>Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/mr-steve">https://sites.google.com/view/mr-steve</a>. </p>
<blockquote>
<p>åœ¨Minecraftç­‰ç¯å¢ƒä¸­ï¼Œé€šè¿‡é‡‡ç”¨LLMå¢å¼ºåˆ†å±‚æ–¹æ³•ï¼Œé€šç”¨å®ä½“äººå·¥æ™ºèƒ½çš„å‘å±•å–å¾—äº†é‡å¤§è¿›å±•ã€‚è™½ç„¶è¿™äº›ç»“åˆé«˜çº§è§„åˆ’å™¨å’Œä½çº§æ§åˆ¶å™¨çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºåå¤å¤±è´¥ï¼Œä½çº§æ§åˆ¶å™¨å¾€å¾€æˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè®¸å¤šä½çº§æ§åˆ¶å™¨å¤±è´¥çš„ä¸»è¦åŸå› æ˜¯ç¼ºä¹æƒ…å¢ƒè®°å¿†ç³»ç»Ÿã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MrSteveï¼ˆè®°å¿†å›å¿†å²è’‚å¤«ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä½çº§æ§åˆ¶å™¨ï¼Œé…å¤‡äº†äº‹ä»¶è®°å¿†ï¼ˆPEMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æƒ…å¢ƒè®°å¿†ï¼Œå¯ä»¥æ•è·äº‹ä»¶ä¸­çš„â€œä»€ä¹ˆâ€ã€â€œå“ªé‡Œâ€å’Œâ€œä½•æ—¶â€ä¿¡æ¯ã€‚è¿™ç›´æ¥è§£å†³äº†æµè¡Œçš„ä½çº§æ§åˆ¶å™¨å²è’‚å¤«-1çš„ä¸»è¦å±€é™æ€§ã€‚ä¸åŒäºä¾èµ–çŸ­æœŸè®°å¿†çš„ä¹‹å‰æ¨¡å‹ï¼ŒPEMèƒ½å¤Ÿç»„ç»‡åŸºäºç©ºé—´å’Œäº‹ä»¶çš„æ•°æ®ï¼Œåœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸­å®ç°é«˜æ•ˆå›å¿†å’Œå¯¼èˆªã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¢ç´¢ç­–ç•¥å’Œä»»åŠ¡è§£å†³æ¡†æ¶ï¼Œå…è®¸æ™ºèƒ½ä½“æ ¹æ®å›å¿†çš„äº‹ä»¶åœ¨æ¢ç´¢å’Œè§£å†³é—®é¢˜ä¹‹é—´äº¤æ›¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ä»»åŠ¡è§£å†³å’Œæ¢ç´¢æ•ˆç‡ã€‚æˆ‘ä»¬å°†åœ¨é¡¹ç›®é¡µé¢å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/mr-steve">https://sites.google.com/view/mr-steve</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06736v5">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨Minecraftç­‰ç¯å¢ƒä¸­ï¼Œé€šç”¨å®ä½“äººå·¥æ™ºèƒ½çš„å‘å±•å·²å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œé‡‡ç”¨äº†LLMå¢å¼ºåˆ†å±‚æ–¹æ³•ã€‚è™½ç„¶è¿™äº›æ–¹æ³•ç»“åˆäº†é«˜çº§è§„åˆ’å™¨å’Œä½çº§æ§åˆ¶å™¨æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä½çº§æ§åˆ¶å™¨ç»å¸¸æˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œä¸»è¦ç”±äºåå¤å¤±è´¥ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè®¸å¤šä½çº§æ§åˆ¶å™¨å¤±è´¥çš„ä¸»è¦åŸå› æ˜¯ç¼ºä¹æƒ…æ™¯è®°å¿†ç³»ç»Ÿã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MrSteveï¼ˆè®°å¿†å›å¿†å²è’‚å¤«ï¼‰ï¼Œä¸€ç§é…å¤‡åœºæ‰€äº‹ä»¶è®°å¿†ï¼ˆPEMï¼‰çš„æ–°å‹ä½çº§æ§åˆ¶å™¨ï¼Œè¿™æ˜¯ä¸€ç§æƒ…æ™¯è®°å¿†ï¼Œå¯ä»¥æ•æ‰äº‹ä»¶ä¸­çš„â€œä»€ä¹ˆâ€ã€â€œå“ªé‡Œâ€å’Œâ€œä½•æ—¶â€ä¿¡æ¯ã€‚è¿™ç›´æ¥è§£å†³äº†æµè¡Œçš„ä½çº§æ§åˆ¶å™¨Steve-1çš„ä¸»è¦å±€é™æ€§ã€‚ä¸ä¾èµ–çŸ­æœŸè®°å¿†çš„å…ˆå‰æ¨¡å‹ä¸åŒï¼ŒPEMç»„ç»‡ç©ºé—´å’Œäº‹ä»¶åŸºç¡€æ•°æ®ï¼Œåœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸­å®ç°é«˜æ•ˆå›å¿†å’Œå¯¼èˆªã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢ç´¢ç­–ç•¥å’Œä»»åŠ¡è§£å†³æ¡†æ¶ï¼Œå…è®¸æ™ºèƒ½ä½“æ ¹æ®å›å¿†çš„äº‹ä»¶åœ¨æ¢ç´¢å’Œè§£å†³é—®é¢˜ä¹‹é—´äº¤æ›¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”çš„ä»»åŠ¡è§£å†³å’Œæ¢ç´¢æ•ˆç‡ã€‚æˆ‘ä»¬å°†åœ¨é¡¹ç›®é¡µé¢ä¸Šå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/mr-steve">https://sites.google.com/view/mr-steve</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨å®ä½“AIåœ¨Minecraftç­‰ç¯å¢ƒä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œé‡‡ç”¨LLMå¢å¼ºåˆ†å±‚æ–¹æ³•ã€‚</li>
<li>ä½çº§æ§åˆ¶å™¨æˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œä¸»è¦ç”±äºåå¤å¤±è´¥ã€‚</li>
<li>ç°æœ‰ä½çº§æ§åˆ¶å™¨çš„ä¸»è¦å¤±è´¥åŸå› æ˜¯ç¼ºä¹æƒ…æ™¯è®°å¿†ç³»ç»Ÿã€‚</li>
<li>å¼•å…¥æ–°å‹ä½çº§æ§åˆ¶å™¨MrSteveï¼Œé…å¤‡åœºæ‰€äº‹ä»¶è®°å¿†ï¼ˆPEMï¼‰ã€‚</li>
<li>PEMèƒ½æ•æ‰äº‹ä»¶ä¸­çš„â€œä»€ä¹ˆâ€ã€â€œå“ªé‡Œâ€å’Œâ€œä½•æ—¶â€ä¿¡æ¯ï¼Œå®ç°é«˜æ•ˆå›å¿†å’Œå¯¼èˆªã€‚</li>
<li>MrSteveé€šè¿‡æ¢ç´¢ç­–ç•¥å’Œä»»åŠ¡è§£å†³æ¡†æ¶ï¼Œæ ¹æ®å›å¿†çš„äº‹ä»¶åœ¨æ¢ç´¢å’Œè§£å†³é—®é¢˜ä¹‹é—´äº¤æ›¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.06736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb3ba896d9be549ded9758f5b45a894f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee8149f5d8e0a06e3a39473cab46217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e608f308ba0c155e12964f679f625cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bff7944f759ba4baed343a61a852ea.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DSBench-How-Far-Are-Data-Science-Agents-from-Becoming-Data-Science-Experts"><a href="#DSBench-How-Far-Are-Data-Science-Agents-from-Becoming-Data-Science-Experts" class="headerlink" title="DSBench: How Far Are Data Science Agents from Becoming Data Science   Experts?"></a>DSBench: How Far Are Data Science Agents from Becoming Data Science   Experts?</h2><p><strong>Authors:Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu</strong></p>
<p>Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language&#x2F;vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¯­è¨€&#x2F;è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¼•å‘äº†æ„å»ºç”¨äºç‰¹å®šåº”ç”¨ï¼ˆå¦‚è´­ç‰©åŠ©ç†æˆ–AIè½¯ä»¶å·¥ç¨‹å¸ˆï¼‰çš„ä»£ç†äººçš„æœ€æ–°è¶‹åŠ¿ã€‚æœ€è¿‘ï¼Œè®¸å¤šæ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•å·²ç»è¢«æå‡ºæ¥ç ”ç©¶å®ƒä»¬åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œä¸çœŸå®ä¸–ç•Œçš„æ•°æ®ç§‘å­¦åº”ç”¨ç›¸æ¯”ï¼Œç°æœ‰çš„æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä»ç„¶æ˜¾å¾—ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬çš„è®¾ç½®è¿‡äºç®€åŒ–ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç”¨ç°å®ä»»åŠ¡è¯„ä¼°æ•°æ®ç§‘å­¦ä»£ç†äººã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•åŒ…æ‹¬æ¥è‡ªEloquenceå’ŒKaggleæ¯”èµ›çš„466ä¸ªæ•°æ®åˆ†æä»»åŠ¡å’Œ74ä¸ªæ•°æ®å»ºæ¨¡ä»»åŠ¡ã€‚DSBenché€šè¿‡åŒ…å«é•¿ä¸Šä¸‹æ–‡ã€å¤šæ¨¡å¼ä»»åŠ¡èƒŒæ™¯ã€å¤„ç†å¤§æ•°æ®æ–‡ä»¶å’Œå¤šè¡¨ç»“æ„è¿›è¡Œæ¨ç†å’Œç«¯åˆ°ç«¯æ•°æ®å»ºæ¨¡ä»»åŠ¡ï¼Œæä¾›äº†ä¸€ä¸ªç°å®ä¸»ä¹‰çš„è®¾ç½®ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„LLMsã€LVLMså’Œä»£ç†äººçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä»–ä»¬åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­éƒ½é‡åˆ°äº†å›°éš¾ï¼Œæœ€å¥½çš„ä»£ç†äººåªèƒ½è§£å†³34.12%çš„æ•°æ®åˆ†æä»»åŠ¡ï¼Œå¹¶å®ç°äº†34.74%çš„ç›¸å¯¹æ€§èƒ½å·®è·ï¼ˆRPGï¼‰ã€‚è¿™äº›å‘ç°å¼ºè°ƒéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›å’Œå‘å±•æ›´å®ç”¨ã€æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æ•°æ®ç§‘å­¦ä»£ç†äººã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07703v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¯­è¨€&#x2F;è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¹¶åº”ç”¨äºè´­ç‰©åŠ©ç†æˆ–AIè½¯ä»¶å·¥ç¨‹å¸ˆç­‰ç›®æ ‡åº”ç”¨ä¸­ã€‚ä¸ºè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç°å®æ•°æ®ç§‘å­¦åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œå¼•å…¥DSBenchç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«466ä¸ªæ•°æ®åˆ†æä»»åŠ¡å’Œ74ä¸ªæ•°æ®å»ºæ¨¡ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ï¼Œæœ€ä½³æ¨¡å‹ä»…å®Œæˆ34.12%çš„æ•°æ®åˆ†æä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºéœ€è¦è¿›ä¸€æ­¥æé«˜æ™ºèƒ½è‡ªä¸»æ•°æ®ç§‘å­¦ä»£ç†çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºè‰²çš„è¯­è¨€&#x2F;è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œè¢«åº”ç”¨äºå„ç§ç›®æ ‡åº”ç”¨ã€‚</li>
<li>ä¸ºè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç°å®æ•°æ®ç§‘å­¦åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œæå‡ºäº†DSBenchç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>DSBenchåŒ…å«ä¸°å¯Œçš„ä»»åŠ¡ç±»å‹ï¼Œæ¶µç›–æ•°æ®åˆ†æä¸å»ºæ¨¡çš„å¤šä¸ªæ–¹é¢ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æä¾›ç°å®ç¯å¢ƒè®¾ç½®ï¼Œæ¨¡æ‹Ÿäº†é•¿æ–‡æœ¬ç¯å¢ƒã€å¤šæ¨¡æ€ä»»åŠ¡èƒŒæ™¯ã€å¤§é‡æ•°æ®æ–‡ä»¶çš„æ¨ç†å’Œå¤šè¡¨ç»“æ„ç­‰åœºæ™¯ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæœ€ä½³æ¨¡å‹ä»…å®Œæˆçº¦ä¸‰åˆ†ä¹‹ä¸€çš„ä»»åŠ¡ã€‚</li>
<li>è¿™è¡¨æ˜éœ€è¦å¼€å‘æ›´å®ç”¨ã€æ™ºèƒ½å’Œè‡ªä¸»çš„æ¨¡å‹æ¥é€‚åº”ç°å®æ•°æ®ç§‘å­¦åº”ç”¨çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e58ace24ce85482bcf5351ac663f93f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404d7633d404300e6b7b9b7ffb12c72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41aa3f8525e355dfe1271d3e54351726.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c95881585ad1ed8897d4bb9c85df322e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d45256edfdd96399314a42f22eb83024.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02219cf9c7ba2490527c091977a8e2d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CoSQA-Pioneering-the-Multi-Choice-Code-Search-Benchmark-with-Test-Driven-Agents"><a href="#CoSQA-Pioneering-the-Multi-Choice-Code-Search-Benchmark-with-Test-Driven-Agents" class="headerlink" title="CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with   Test-Driven Agents"></a>CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with   Test-Driven Agents</h2><p><strong>Authors:Jing Gong, Yanghui Wu, Linxi Liang, Yanlin Wang, Jiachi Chen, Mingwei Liu, Zibin Zheng</strong></p>
<p>Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. Existing code search datasets face limitations: they rely on human annotators who assess code primarily through semantic understanding rather than functional verification, leading to potential inaccuracies and scalability issues. Additionally, current evaluation metrics often overlook the multi-choice nature of code search. This paper introduces CoSQA+, pairing high-quality queries from CoSQA with multiple suitable codes. We develop an automated pipeline featuring multiple model-based candidate selections and the novel test-driven agent annotation system. Among a single Large Language Model (LLM) annotator and Python expert annotators (without test-based verification), agents leverage test-based verification and achieve the highest accuracy of 92.0%. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance. We provide the code and data at <a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/CoSQA_Plus">https://github.com/DeepSoftwareAnalytics/CoSQA_Plus</a>. </p>
<blockquote>
<p>ä»£ç è¯­ä¹‰æœç´¢æ˜¯è½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸­æé«˜ç”Ÿäº§åŠ›çš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ï¼Œå®ƒæ—¨åœ¨æ£€ç´¢ä¸ç»™å®šè‡ªç„¶è¯­è¨€æŸ¥è¯¢åŒ¹é…çš„ä»£ç ã€‚ç°æœ‰çš„ä»£ç æœç´¢æ•°æ®é›†å­˜åœ¨å±€é™æ€§ï¼šå®ƒä»¬ä¸»è¦ä¾èµ–äºé€šè¿‡è¯­ä¹‰ç†è§£è€ŒéåŠŸèƒ½éªŒè¯æ¥è¯„ä¼°ä»£ç çš„äººä¸ºæ³¨é‡Šè€…ï¼Œè¿™å¯èƒ½å¯¼è‡´æ½œåœ¨çš„ä¸å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚æ­¤å¤–ï¼Œå½“å‰çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€å¿½è§†äº†ä»£ç æœç´¢çš„å¤šé€‰æ€§è´¨ã€‚æœ¬æ–‡ä»‹ç»äº†CoSQA+ï¼Œå®ƒå°†CoSQAçš„é«˜è´¨é‡æŸ¥è¯¢ä¸å¤šä¸ªåˆé€‚çš„ä»£ç é…å¯¹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œé€šè¿‡åŸºäºæ¨¡å‹çš„å€™é€‰é€‰æ‹©å’Œå¤šä»£ç†æ³¨é‡Šç³»ç»Ÿçš„æµ‹è¯•é©±åŠ¨ï¼Œå®ç°é«˜è´¨é‡çš„æ³¨é‡Šã€‚ä¸å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹æ³¨é‡Šå™¨å’ŒPythonä¸“å®¶æ³¨é‡Šå™¨ï¼ˆæ— æµ‹è¯•éªŒè¯ï¼‰ç›¸æ¯”ï¼Œé€šè¿‡æµ‹è¯•éªŒè¯çš„ä»£ç†è¾¾åˆ°äº†æœ€é«˜çš„92.0%å‡†ç¡®ç‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒCoSQA+çš„è´¨é‡ä¼˜äºCoSQAã€‚åœ¨CoSQA+æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/CoSQA_Plus%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/DeepSoftwareAnalytics/CoSQA_Plusæä¾›äº†ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11589v5">PDF</a> 15 pages, 5 figures, journal</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­ä¹‰ä»£ç æœç´¢çš„é‡è¦æ€§åŠå…¶åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨ã€‚ç°æœ‰ä»£ç æœç´¢æ•°æ®é›†å­˜åœ¨ä¾èµ–äººå·¥æ ‡æ³¨è€…è¿›è¡Œè¯­ä¹‰ç†è§£è¯„ä¼°è€ŒéåŠŸèƒ½éªŒè¯çš„é—®é¢˜ï¼Œå¯¼è‡´æ½œåœ¨çš„ä¸å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†CoSQA+æ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“å’Œæ¨¡å‹å€™é€‰è€…é€‰æ‹©ä»¥åŠæ–°å‹çš„æµ‹è¯•é©±åŠ¨ä»£ç†æ ‡æ³¨ç³»ç»Ÿï¼Œæé«˜äº†ä»£ç æœç´¢çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒCoSQA+ä¼˜äºCoSQAæ•°æ®é›†ï¼Œè®­ç»ƒå…¶ä¸Šçš„æ¨¡å‹æ€§èƒ½ä¹Ÿæœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­ä¹‰ä»£ç æœç´¢å¯¹äºæé«˜è½¯ä»¶å·¥ç¨‹çš„æ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ä»£ç æœç´¢æ•°æ®é›†å­˜åœ¨ä¾èµ–äººå·¥æ ‡æ³¨è€…è¿›è¡Œè¯­ä¹‰ç†è§£è¯„ä¼°çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸å‡†ç¡®å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>CoSQA+æ•°æ®é›†é€šè¿‡å¼•å…¥è‡ªåŠ¨åŒ–ç®¡é“å’Œæ¨¡å‹å€™é€‰è€…é€‰æ‹©æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>CoSQA+é‡‡ç”¨æ–°é¢–çš„æµ‹è¯•é©±åŠ¨ä»£ç†æ ‡æ³¨ç³»ç»Ÿï¼Œå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ˆ92.0%ï¼‰ã€‚</li>
<li>ä¸CoSQAç›¸æ¯”ï¼ŒCoSQA+æ•°æ®é›†å…·æœ‰æ›´é«˜çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨CoSQA+ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ”¹è¿›çš„æ€§èƒ½ã€‚</li>
<li>CoSQA+æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4085f89e2ed4f51013e44f470e0f32e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-981deafea60fb731bef82cab8ac88fb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df56e0e57887aed284ce369fabcdd7a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a11216015486dfd78c9deb8a219e4e4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b858b84d4b8a6ba293f91bbca028bb15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a51aa41ca447683dcca1fd1019bfc82f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MLLM-Tool-A-Multimodal-Large-Language-Model-For-Tool-Agent-Learning"><a href="#MLLM-Tool-A-Multimodal-Large-Language-Model-For-Tool-Agent-Learning" class="headerlink" title="MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"></a>MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</h2><p><strong>Authors:Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao</strong></p>
<p>Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMsâ€™ ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the usersâ€™ real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the modelâ€™s capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at <a target="_blank" rel="noopener" href="https://github.com/MLLM-Tool/MLLM-Tool">https://github.com/MLLM-Tool/MLLM-Tool</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„æƒŠäººè¡¨ç°ï¼Œå¼•å‘äº†äººä»¬å°†å…¶ä½œä¸ºä¸­å¤®æ§åˆ¶å™¨æ„å»ºä»£ç†ç³»ç»Ÿçš„æ¢ç´¢çƒ­æ½®ã€‚å¤šé¡¹ç ”ç©¶ä¸“æ³¨äºå°†LLMsä¸å¤–éƒ¨å·¥å…·ç›¸è¡”æ¥ï¼Œä»¥æ‰©å±•å…¶åº”ç”¨åœºæ™¯ã€‚ç„¶è€Œï¼Œå½“å‰LLMså¯¹å·¥å…·ä½¿ç”¨çš„æ„ŸçŸ¥èƒ½åŠ›ä»…é™äºå•ä¸ªæ–‡æœ¬æŸ¥è¯¢ï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹ç”¨æˆ·çœŸå®æ„å›¾ç†è§£çš„æ¨¡ç³Šæ€§ã€‚äººä»¬æœŸæœ›LLMsèƒ½å¤Ÿé€šè¿‡æ„ŸçŸ¥è§†è§‰æˆ–å¬è§‰åŸºç¡€çš„æŒ‡ä»¤æ¥æ¶ˆé™¤è¿™ç§æ¨¡ç³Šæ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MLLM-Toolç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¼€æºLLMså’Œå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œä½¿å¾—å­¦ä¹ åˆ°çš„LLMsèƒ½å¤Ÿæ„è¯†åˆ°å¤šæ¨¡æ€è¾“å…¥æŒ‡ä»¤ï¼Œç„¶åæ­£ç¡®åœ°é€‰æ‹©åŠŸèƒ½åŒ¹é…çš„å·¥å…·ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä»HuggingFaceæ”¶é›†äº†ä¸€ä¸ªä»¥å¤šæ¨¡æ€è¾“å…¥å·¥å…·ä¸ºç‰¹è‰²çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æ•°æ®é›†çš„å¦ä¸€ä¸ªé‡è¦ç‰¹ç‚¹æ˜¯ï¼Œç”±äºå­˜åœ¨ç›¸åŒåŠŸèƒ½å’ŒåŒä¹‰è¯åŠŸèƒ½ï¼Œå¯¹äºç›¸åŒçš„æŒ‡ä»¤ï¼Œå®ƒè¿˜åŒ…æ‹¬å¤šä¸ªæ½œåœ¨çš„é€‰æ‹©ï¼Œè¿™ä¸ºåŒä¸€æŸ¥è¯¢æä¾›äº†æ›´å¤šçš„æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MLLM-Toolèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€æŒ‡ä»¤æ¨èåˆé€‚çš„å·¥å…·ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MLLM-Tool/MLLM-Tool%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/MLLM-Tool/MLLM-Toolè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.10727v3">PDF</a> WACV 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¼•å‘äº†å¯¹å°†å…¶ä½œä¸ºä¸­å¤®æ§åˆ¶å™¨æ„å»ºä»£ç†ç³»ç»Ÿçš„æ¢ç´¢ã€‚å½“å‰LLMså¯¹å·¥å…·ä½¿ç”¨çš„æ„ŸçŸ¥èƒ½åŠ›ä»…é™äºæ–‡æœ¬æŸ¥è¯¢ï¼Œå¯èƒ½å¯¼è‡´å¯¹ç”¨æˆ·çœŸå®æ„å›¾çš„è¯¯è§£ã€‚æœ¬ç ”ç©¶æå‡ºMLLM-Toolç³»ç»Ÿï¼Œç»“åˆå¼€æºLLMså’Œå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œä½¿LLMsèƒ½å¤Ÿæ„ŸçŸ¥å¤šæ¨¡æ€è¾“å…¥æŒ‡ä»¤ï¼Œå¹¶æ­£ç¡®é€‰æ‹©åŠŸèƒ½åŒ¹é…çš„å·¥å…·ã€‚ä¸ºè¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œæˆ‘ä»¬æ”¶é›†äº†åŒ…å«å¤šæ¨¡æ€è¾“å…¥å·¥å…·çš„æ•°æ®é›†ï¼Œå¹¶å‘ç°MLLM-Toolèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€æŒ‡ä»¤æ¨èåˆé€‚çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå¼•å‘ä½œä¸ºä¸­å¤®æ§åˆ¶å™¨æ„å»ºä»£ç†ç³»ç»Ÿçš„æ¢ç´¢ã€‚</li>
<li>å½“å‰LLMså¯¹å·¥å…·ä½¿ç”¨çš„æ„ŸçŸ¥èƒ½åŠ›ä»…é™äºæ–‡æœ¬æŸ¥è¯¢ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·æ„å›¾çš„è¯¯è§£ã€‚</li>
<li>MLLM-Toolç³»ç»Ÿç»“åˆå¼€æºLLMså’Œå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œä½¿LLMsèƒ½å¤Ÿæ„ŸçŸ¥å¤šæ¨¡æ€è¾“å…¥æŒ‡ä»¤ã€‚</li>
<li>ä¸ºè¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œæ”¶é›†äº†ä¸€ä¸ªåŒ…å«å¤šæ¨¡æ€è¾“å…¥å·¥å…·çš„æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç›¸åŒæŒ‡ä»¤çš„å¤šä¸ªæ½œåœ¨é€‰æ‹©ï¼Œä¸ºç›¸åŒæŸ¥è¯¢æä¾›æ›´å¤šæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¡¨æ˜MLLM-Toolèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€æŒ‡ä»¤æ¨èåˆé€‚çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.10727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-816f189f1ebe3a5e1292b1e641318938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b78ac92924bd85f63f694b822c4804f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f374b3d20b800bb57bcb29d45a91f625.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b907c443f429003fe0e09a9abdd645.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e793967f88196778ded85405674f518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-128369f3df65bd30fd4b985490a07ffb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-15/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e934eb638d55bccac93b796bc47f8c4e.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  DRAFT-ing Architectural Design Decisions using LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ff68f3b629529eea02d7b494c7b5137a.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  Steering CLIP's vision transformer with sparse autoencoders
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
