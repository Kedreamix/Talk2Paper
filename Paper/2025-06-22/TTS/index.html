<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-06-22  TTSOps A Closed-Loop Corpus Optimization Framework for Training   Multi-Speaker TTS Models from Dark Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ae5ea89989805d02f0cf79d2c2aa9f32.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-22-更新"><a href="#2025-06-22-更新" class="headerlink" title="2025-06-22 更新"></a>2025-06-22 更新</h1><h2 id="TTSOps-A-Closed-Loop-Corpus-Optimization-Framework-for-Training-Multi-Speaker-TTS-Models-from-Dark-Data"><a href="#TTSOps-A-Closed-Loop-Corpus-Optimization-Framework-for-Training-Multi-Speaker-TTS-Models-from-Dark-Data" class="headerlink" title="TTSOps: A Closed-Loop Corpus Optimization Framework for Training   Multi-Speaker TTS Models from Dark Data"></a>TTSOps: A Closed-Loop Corpus Optimization Framework for Training   Multi-Speaker TTS Models from Dark Data</h2><p><strong>Authors:Kentaro Seki, Shinnosuke Takamichi, Takaaki Saeki, Hiroshi Saruwatari</strong></p>
<p>This paper presents TTSOps, a fully automated closed-loop framework for constructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated web-scale speech data, often referred to as &#96;&#96;dark data,’’ such as online videos. Conventional TTS training pipelines require well-curated corpora with high acoustic quality and accurate text-speech alignment, which severely limits scalability, speaker diversity, and real-world applicability. While recent studies have proposed acoustic-quality-based data selection techniques, they often overlook two critical aspects: (1) the inherent robustness of modern TTS models to noise, and (2) the potential contribution of perceptually low-quality yet informative samples. To address these issues, TTSOps introduces a data-centric training pipeline that integrates three core components: (1) automated data collection from dark data sources, (2) utterance-level dynamic selection of data cleansing methods based on training data quality, and (3) evaluation-in-the-loop data selection using automatically predicted mean opinion scores (MOS) to estimate each utterance’s impact on model performance. Furthermore, TTSOps jointly optimizes the corpus and the TTS model in a closed-loop framework by dynamically adapting both data selection and data cleansing processes to the characteristics of the target TTS model. Extensive experiments on Japanese YouTube data demonstrate that TTSOps outperforms conventional acoustic-quality-based baselines in both the naturalness and speaker diversity of synthesized speech. </p>
<blockquote>
<p>本文介绍了TTSOps，这是一个全自动的闭环框架，用于从嘈杂的、未整理的网页规模语音数据中构建多说话人文本到语音（TTS）系统，这种数据通常被称为“暗数据”，如在线视频。传统的TTS训练管道需要高质量的声音和准确的文本-语音对齐的精心整理语料库，这严重限制了可扩展性、说话人多样性和现实世界的适用性。虽然最近的研究已经提出了基于声音质量的数据选择技术，但他们往往忽略了两个关键方面：（1）现代TTS模型对噪声的固有鲁棒性；（2）感知质量较低但信息丰富的样本的潜在贡献。为了解决这些问题，TTSOps引入了一个以数据为中心的训练管道，该管道集成了三个核心组件：（1）从暗数据源自动收集数据，（2）基于训练数据质量的语句级动态选择数据清洗方法，（3）使用自动预测的平均意见分数（MOS）进行闭环数据选择，以估计每个语句对模型性能的影响。此外，TTSOps在一个闭环框架中联合优化语料库和TTS模型，通过动态适应数据选择和清洗过程来适应目标TTS模型的特征。在日语YouTube数据上的大量实验表明，TTSOps在自然度和说话人多样性方面超越了基于传统声音质量的基准线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了TTSOps，这是一个全自动的闭环框架，用于从嘈杂的、未整理的网页规模语音数据中构建多说话者文本到语音（TTS）系统。该框架解决了传统TTS训练管道在可扩展性、说话者多样性和现实世界应用方面的限制。TTSOps通过整合自动化数据采集、基于训练数据质量的动态选择数据清洗方法和基于自动预测平均意见分数（MOS）的闭环数据选择，形成了一个数据中心的训练管道。此外，TTSOps在一个闭环框架中联合优化语料库和TTS模型，动态适应数据选择和清洗过程以适应目标TTS模型的特性。在日语YouTube数据上的实验表明，TTSOps在自然性和说话者多样性方面优于传统的基于声音质量的基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSOps是一个全自动的闭环框架，用于构建多说话者文本到语音（TTS）系统。</li>
<li>它解决了传统TTS训练管道在可扩展性、说话者多样性和现实应用方面的限制。</li>
<li>TTSOps整合了自动化数据采集、动态选择数据清洗方法和基于自动预测平均意见分数（MOS）的闭环数据选择。</li>
<li>TTSOps考虑了现代TTS模型对噪声的固有鲁棒性和感知低质量但信息丰富的样本的潜在贡献。</li>
<li>框架中的动态适应机制允许根据目标TTS模型的特性调整数据选择和清洗过程。</li>
<li>实验表明，TTSOps在自然性和说话者多样性方面优于基于声音质量的传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15614">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ffdaff99a23d2b2c23d6d014b686ab15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15cbec87c7dc904033ad3a65f63f3908.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29e5d449514c7229a296f8b1d81186c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26c8f96aff7ec9ca38ab135f986fea2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7cc192e0a73e16242810fb4cf4be496.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a77f1ec8ea0d9e6981b18326f74f7165.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Investigation-of-Zero-shot-Text-to-Speech-Models-for-Enhancing-Short-Utterance-Speaker-Verification"><a href="#Investigation-of-Zero-shot-Text-to-Speech-Models-for-Enhancing-Short-Utterance-Speaker-Verification" class="headerlink" title="Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification"></a>Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification</h2><p><strong>Authors:Yiyang Zhao, Shuai Wang, Guangzhi Sun, Zehua Chen, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</strong></p>
<p>Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research. </p>
<blockquote>
<p>短时段语音说话者验证由于简短语音片段中的信息有限而面临重大挑战，这可能会损害准确性和可靠性。最近，零样本文本到语音（ZS-TTS）系统在保持说话者身份方面取得了很大的进展。在这项研究中，我们首次探索了ZS-TTS系统在测试时间数据增强（test-time data augmentation）在说话者验证中的应用。我们在VoxCeleb 1数据集上评估了三种最先进的预训练ZS-TTS系统，分别是NatureSpeech 3、CosyVoice和MaskGCT。我们的实验结果表明，结合真实和合成语音样本在所有时间段内导致相对等错误率（EER）降低了10%-16%，特别是对于短时段语音，而且无需对任何现有系统进行再训练。然而，我们的分析表明，较长的合成语音在降低EER方面并不产生与较长真实语音相同的效益。这些发现突显了使用ZS-TTS进行测试时间说话者验证的潜力和挑战，为未来的研究提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14226v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究首次探索了使用零样本文本到语音（ZS-TTS）系统进行测试时数据增强在说话人验证中的应用。实验结果显示，结合真实和合成语音样本相对于只用真实样本在短话语上的等错误率（EER）降低了10%-16%，并且不需要对现有系统进行任何重新训练。但分析表明，合成语音与真实语音在降低EER方面的效果并不等同。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZS-TTS系统在测试时间数据增强在说话人验证中具有潜力。</li>
<li>结合真实和合成语音样本可以提高说话人验证的准确性。</li>
<li>使用ZS-TTS系统可以在不重新训练现有系统的情况下提高性能。</li>
<li>短片段语音通过数据增强可获得显著的性能提升。</li>
<li>较长的合成语音在降低等错误率方面不如较长的真实语音有效。</li>
<li>在使用ZS-TTS系统时存在挑战和局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14226">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bf630bdfad1b39c38b5704284feaf182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0256bd1c03b59d535cb3b3a488529cf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47e17273cb8b2824416b560dc49e469e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40c944d44b370ec97fbe32fd18147d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5785249a9d34d296c0bd728b4543513.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EmoNews-A-Spoken-Dialogue-System-for-Expressive-News-Conversations"><a href="#EmoNews-A-Spoken-Dialogue-System-for-Expressive-News-Conversations" class="headerlink" title="EmoNews: A Spoken Dialogue System for Expressive News Conversations"></a>EmoNews: A Spoken Dialogue System for Expressive News Conversations</h2><p><strong>Authors:Ryuki Matsuura, Shikhar Bharadwaj, Jiarui Liu, Dhatchi Kunde Govindarajan</strong></p>
<p>We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1">https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1</a> </p>
<blockquote>
<p>我们开发了一个面向任务的口语对话系统（SDS），该系统基于上下文线索调节情感语音，以实现更具同理心的新闻对话。尽管情感文本到语音（TTS）技术取得了进展，但由于SDS和情感TTS研究的分隔性质，以及缺乏对社会目标的标准化评估指标，面向任务的情感SDS仍被较少探索。我们通过开发用于新闻对话的情感SDS来解决这些挑战，该系统利用基于大型语言模型的情感分析器来识别适当的情感，并利用PromptTTS合成适合上下文的情感语音。我们还为情感SDS提出了主观评价尺度，并判断所提出系统和基线系统的情感调节性能。实验表明，我们的情感SDS在情感调节和参与度方面优于基线系统。这些结果突显了语音情感对于更引人入胜的对话的关键作用。我们的所有源代码均已开源，可在<a target="_blank" rel="noopener" href="https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds&#x2F;sds1找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文开发了一种面向任务的情感对话系统（SDS），该系统基于上下文线索调节情感语音，以实现更具同理心的新闻对话。通过大型语言模型（LLM）的情感分析器识别适当情感，并利用PromptTTS合成符合上下文的情感语音。同时，本文提出了针对情感SDS的主观评价尺度，并评估了所提出系统和基线系统的情感调节性能。实验表明，在情感调节和参与度方面，所提出情感SDS优于基线系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开发了一种面向任务的情感对话系统（SDS），能够基于上下文调节情感语音，用于新闻对话。</li>
<li>利用大型语言模型（LLM）的情感分析器识别适当的情感。</li>
<li>使用PromptTTS技术合成与上下文相适应的情感语音。</li>
<li>提出了针对情感SDS的主观评价尺度。</li>
<li>所开发情感SDS在情感调节和参与度方面表现出优于基线系统的性能。</li>
<li>该研究有助于实现更具同理心的新闻对话。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b32d715486f43bd1f56b213087dfc8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-205a38672d1f24e498b2e0b817378d1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65cd3b88440a4865fd6b358f852c4c53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1234a218a5bb8cccf85aeed90bcfb37d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ZipVoice-Fast-and-High-Quality-Zero-Shot-Text-to-Speech-with-Flow-Matching"><a href="#ZipVoice-Fast-and-High-Quality-Zero-Shot-Text-to-Speech-with-Flow-Matching" class="headerlink" title="ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching"></a>ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching</h2><p><strong>Authors:Han Zhu, Wei Kang, Zengwei Yao, Liyong Guo, Fangjun Kuang, Zhaoqing Li, Weiji Zhuang, Long Lin, Daniel Povey</strong></p>
<p>Existing large-scale zero-shot text-to-speech (TTS) models deliver high speech quality but suffer from slow inference speeds due to massive parameters. To address this issue, this paper introduces ZipVoice, a high-quality flow-matching-based zero-shot TTS model with a compact model size and fast inference speed. Key designs include: 1) a Zipformer-based flow-matching decoder to maintain adequate modeling capabilities under constrained size; 2) Average upsampling-based initial speech-text alignment and Zipformer-based text encoder to improve speech intelligibility; 3) A flow distillation method to reduce sampling steps and eliminate the inference overhead associated with classifier-free guidance. Experiments on 100k hours multilingual datasets show that ZipVoice matches state-of-the-art models in speech quality, while being 3 times smaller and up to 30 times faster than a DiT-based flow-matching baseline. Codes, model checkpoints and demo samples are publicly available. </p>
<blockquote>
<p>现有的大规模零样本文本到语音（TTS）模型虽然能够提供高质量的语音，但由于参数众多，推理速度较慢。为解决这一问题，本文引入了ZipVoice，这是一个基于高质量流匹配的零样本TTS模型，具有模型体积小、推理速度快的特点。主要设计包括：1）基于Zipformer的流匹配解码器，在受限的模型大小下保持足够的建模能力；2）基于平均上采样的初始语音-文本对齐和基于Zipformer的文本编码器，以提高语音的可懂度；3）流蒸馏方法用于减少采样步骤，消除与无分类器引导相关的推理开销。在100k小时的多语种数据集上的实验表明，ZipVoice在语音质量方面与最新模型相匹配，同时比基于DiT的流匹配基准模型小3倍，速度快达30倍。代码、模型检查点和演示样本已公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于流匹配的零样本文本转语音（TTS）模型ZipVoice，具有模型体积小、推理速度快的特点。该模型通过引入Zipformer流匹配解码器、基于平均上采样的语音文本对齐方法和流蒸馏技术，实现了在有限模型大小下的高质量语音合成。实验结果表明，ZipVoice在语音质量方面与当前最佳模型相当，模型体积减小了3倍，推理速度提高了高达30倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZipVoice是一个基于流匹配的零样本TTS模型，解决了现有大规模模型参数庞大、推理速度慢的问题。</li>
<li>模型采用Zipformer流匹配解码器，在有限模型大小下保持足够的建模能力。</li>
<li>通过基于平均上采样的初始语音文本对齐和Zipformer文本编码器，提高了语音清晰度。</li>
<li>引入流蒸馏技术，减少采样步骤，消除与无分类引导相关的推理开销。</li>
<li>实验证明，ZipVoice在语音质量方面与最新技术相当。</li>
<li>ZipVoice模型体积较小，仅为其他模型的3倍大小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-699d4c4c638cfdab17d01c02eb4616fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b95442658d63b75a0261323235370b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4781955976ebd17ab6bf8e7b661543bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60581c5031091ac95e4d7fb0783be078.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a3ba3339606bea8eb72c2185f0f49e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-685098491ef52c668f41a230b28ae69c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-684363c637d82741c6c5d19470eab31e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd4ce1327c075b254a339d25ecef6d4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StreamMel-Real-Time-Zero-shot-Text-to-Speech-via-Interleaved-Continuous-Autoregressive-Modeling"><a href="#StreamMel-Real-Time-Zero-shot-Text-to-Speech-via-Interleaved-Continuous-Autoregressive-Modeling" class="headerlink" title="StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous   Autoregressive Modeling"></a>StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous   Autoregressive Modeling</h2><p><strong>Authors:Hui Wang, Yifan Yang, Shujie Liu, Jinyu Li, Lingwei Meng, Yanqing Liu, Jiaming Zhou, Haoqin Sun, Yan Lu, Yong Qin</strong></p>
<p>Recent advances in zero-shot text-to-speech (TTS) synthesis have achieved high-quality speech generation for unseen speakers, but most systems remain unsuitable for real-time applications because of their offline design. Current streaming TTS paradigms often rely on multi-stage pipelines and discrete representations, leading to increased computational cost and suboptimal system performance. In this work, we propose StreamMel, a pioneering single-stage streaming TTS framework that models continuous mel-spectrograms. By interleaving text tokens with acoustic frames, StreamMel enables low-latency, autoregressive synthesis while preserving high speaker similarity and naturalness. Experiments on LibriSpeech demonstrate that StreamMel outperforms existing streaming TTS baselines in both quality and latency. It even achieves performance comparable to offline systems while supporting efficient real-time generation, showcasing broad prospects for integration with real-time speech large language models. Audio samples are available at: <a target="_blank" rel="noopener" href="https://aka.ms/StreamMel">https://aka.ms/StreamMel</a>. </p>
<blockquote>
<p>近期零样本文本转语音（TTS）合成技术的进展为未见过的说话者实现了高质量语音生成，但大多数系统由于其离线设计仍不适合实时应用。当前的流式TTS范式通常依赖于多阶段管道和离散表示，导致计算成本增加和系统性能不佳。在这项工作中，我们提出了StreamMel，这是一个开创性的单阶段流式TTS框架，用于对连续的梅尔频谱进行建模。通过文本标记与声学帧的交错，StreamMel可实现低延迟的自动回归合成，同时保持高说话者相似性和自然度。在LibriSpeech上的实验表明，StreamMel在质量和延迟方面都优于现有的流式TTS基线。它甚至在支持高效实时生成的同时，实现了与离线系统相当的性能，展示了与实时语音大型语言模型集成的广阔前景。音频样本可通过以下网址获取：<a target="_blank" rel="noopener" href="https://aka.ms/StreamMel">https://aka.ms/StreamMel</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12570v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>最新零样本文本转语音（TTS）合成技术已实现了未见过的说话者的高质量语音生成，但由于其离线设计，大多数系统仍不适合实时应用。当前流式TTS范式通常依赖于多阶段管道和离散表示，导致计算成本增加和系统性能下降。在此研究中，我们提出了StreamMel，一个开创性的单阶段流式TTS框架，用于建模连续梅尔频谱图。通过将文本标记与声音帧交织，StreamMel可实现低延迟的自动回归合成，同时保持高说话者相似性和自然度。在LibriSpeech上的实验表明，StreamMel在质量和延迟方面都优于现有的流式TTS基线。它甚至实现了与离线系统相当的性能，同时支持高效的实时生成，显示出与实时语音大型语言模型结合的广阔前景。音频样本可在aka.ms&#x2F;StreamMel处获取。</p>
<p><strong>要点</strong></p>
<ol>
<li>最新零样本TTS技术实现了高质量语音生成。</li>
<li>大多数TTS系统为离线设计，不适合实时应用。</li>
<li>流式TTS范式通常计算成本高且性能下降。</li>
<li>StreamMel是一个单阶段流式TTS框架，用于建模连续梅尔频谱图。</li>
<li>StreamMel通过交织文本标记和声音帧实现低延迟的自动回归合成。</li>
<li>StreamMel在质量和延迟方面优于现有流式TTS系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12570">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9c0ea554cb063b5946ea387a09f6b0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d35413b56675cc00fae5866696c7494e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-978c6c3b337eae4abde2b48158212af3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3dc95a38645e362b825208de59505ec.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Phonikud-Hebrew-Grapheme-to-Phoneme-Conversion-for-Real-Time-Text-to-Speech"><a href="#Phonikud-Hebrew-Grapheme-to-Phoneme-Conversion-for-Real-Time-Text-to-Speech" class="headerlink" title="Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech"></a>Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech</h2><p><strong>Authors:Yakov Kolani, Maxim Melichov, Cobi Calev, Morris Alper</strong></p>
<p>Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the language’s orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P and as training data for TTS systems. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at <a target="_blank" rel="noopener" href="https://phonikud.github.io/">https://phonikud.github.io</a>. </p>
<blockquote>
<p>现代希伯来语的实时文本转语音（TTS）面临挑战，因为该语言的正字法复杂。现有解决方案忽略了关键的语音特征，例如即使添加元音标记也仍未明确指定的重音。为了解决这些限制，我们引入了Phonikud，这是一个轻量级的开源希伯来字母到音素（G2P）系统，可以输出完全指定的国际音标（IPA）转录。我们的方法通过轻量级适配器适应现有的标调模型，几乎不会增加额外的延迟。我们还提供了带有国际音标注释的ILSpeech希伯来语语音转录数据集，作为希伯来字母到音素的基准测试数据，并为TTS系统提供训练数据。我们的结果表明，与先前的方法相比，Phonikud的G2P转换更能准确地从希伯来语文本预测音素，并且这能够训练出有效的实时希伯来语TTS模型，在速度和准确性方面达到更好的权衡。我们在<a target="_blank" rel="noopener" href="https://phonikud.github.io发布我们的代码、数据和模型./">https://phonikud.github.io发布我们的代码、数据和模型。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12311v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://phonikud.github.io/">https://phonikud.github.io</a></p>
<p><strong>摘要</strong></p>
<p>针对现代希伯来语的实时文本转语音（TTS）存在挑战，因希伯来语的字形复杂，现有解决方案忽略了重要的语音特征，如即使添加元音符号也仍然未指定的音强。为解决这些局限性，我们引入了Phonikud，这是一个轻量级的开源希伯来字母到音素（G2P）系统，可输出完全指定的国际音标（IPA）转录。我们的方法通过轻量级适配器适应现有的标音模型，几乎不会增加额外的延迟。我们还贡献了带有IPA注释的ILSpeech希伯来语语音数据集，作为希伯来语G2P的基准测试和培训TTS系统的数据。结果表明，相较于前人的方法，Phonikud的G2P转换能更准确地从希伯来语文本预测音素，并且能训练出有效、实时的希伯来语TTS模型，在速度和准确性方面都有更优的权衡。我们的代码、数据和模型已发布在<a target="_blank" rel="noopener" href="https://phonikud.github.io上./">https://phonikud.github.io上。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>希伯来语实时文本转语音（TTS）面临挑战，因字形复杂和缺乏足够的语音特征。</li>
<li>Phonikud是一个轻量级的希伯来语字母到音素（G2P）系统，能输出完全指定的国际音标（IPA）转录。</li>
<li>Phonikud通过轻量级适配器改进了现有标音模型，几乎不增加额外延迟。</li>
<li>ILSpeech数据集为希伯来语G2P提供了基准测试，并作为TTS系统的训练数据。</li>
<li>Phonikud的G2P转换能更准确地预测希伯来语中的音素。</li>
<li>Phonikud使训练实时希伯来语TTS模型成为可能，且在速度和准确性方面有更优的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12311">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cc74671b3405ea7bc787ebdd349ac9e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6653056e94fb5ed6017a351853ddbb54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04da13a1c1dd7de622c2b50e670a7fe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9ddfa45e00791381e8aac15aa28aeb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="S2ST-Omni-An-Efficient-and-Scalable-Multilingual-Speech-to-Speech-Translation-Framework-via-Seamlessly-Speech-Text-Alignment-and-Streaming-Speech-Decoder"><a href="#S2ST-Omni-An-Efficient-and-Scalable-Multilingual-Speech-to-Speech-Translation-Framework-via-Seamlessly-Speech-Text-Alignment-and-Streaming-Speech-Decoder" class="headerlink" title="S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamlessly Speech-Text Alignment and Streaming   Speech Decoder"></a>S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamlessly Speech-Text Alignment and Streaming   Speech Decoder</h2><p><strong>Authors:Yu Pan, Yuguang Yang, Yanni Hu, Jianhao Ye, Xiang Zhang, Hongbin Zhou, Lei Ma, Jianjun Zhao</strong></p>
<p>Multilingual speech-to-speech translation (S2ST) aims to directly convert spoken utterances from multiple source languages into fluent and intelligible speech in a target language. Despite recent progress, several critical challenges persist: 1) achieving high-quality and low-latency S2ST remains a significant obstacle; 2) most existing S2ST methods rely heavily on large-scale parallel speech corpora, which are difficult and resource-intensive to obtain. To tackle these challenges, we introduce S2ST-Omni, a novel, efficient, and scalable framework tailored for multilingual speech-to-speech translation. To enable high-quality S2TT while mitigating reliance on large-scale parallel speech corpora, we leverage powerful pretrained models: Whisper for robust audio understanding and Qwen 3.0 for advanced text comprehension. A lightweight speech adapter is introduced to bridge the modality gap between speech and text representations, facilitating effective utilization of pretrained multimodal knowledge. To ensure both translation accuracy and real-time responsiveness, we adopt a streaming speech decoder in the TTS stage, which generates the target speech in an autoregressive manner. Extensive experiments conducted on the CVSS benchmark demonstrate that S2ST-Omni consistently surpasses several state-of-the-art S2ST baselines in translation quality, highlighting its effectiveness and superiority. </p>
<blockquote>
<p>多语种语音到语音翻译（S2ST）旨在将多种源语言的口头表达直接翻译成目标语言中的流畅和可理解的语音。尽管最近有进展，但仍存在几个关键挑战：1）实现高质量、低延迟的S2ST仍然是一个重大障碍；2）大多数现有的S2ST方法严重依赖于大规模并行语音语料库，这些语料库的获取困难和资源密集。为了应对这些挑战，我们引入了S2ST-Omni，这是一个新颖、高效、可扩展的多语种语音到语音翻译框架。为了实现高质量的S2TT，同时减少大规模并行语音语料库的依赖，我们利用强大的预训练模型：Whisper进行稳健的音频理解，Qwen 3.0进行高级文本理解。我们引入了一个轻量级的语音适配器来弥合语音和文本表示之间的模态差距，促进预训练多模态知识的有效利用。为了确保翻译准确性和实时响应性，我们在TTS阶段采用流式语音解码器，以自回归的方式生成目标语音。在CVSS基准测试上进行的大量实验表明，S2ST-Omni在翻译质量上始终超越了几种最先进的S2ST基准测试，凸显了其有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11160v2">PDF</a> Working in progress</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对多语言语音到语音翻译（S2ST）的挑战，提出了一种新的高效、可扩展的框架S2ST-Omni。该框架利用预训练模型实现高质量S2ST，减少对大规模平行语音语料库的依赖。通过引入轻量级语音适配器，有效融合语音和文本表示，并利用预训练的多模态知识。采用流式语音解码器，确保翻译准确性和实时响应性。在CVSS基准测试上的实验表明，S2ST-Omni在翻译质量上超越了多个最新S2ST基线，凸显其有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多语言语音到语音翻译（S2ST）的目标是直接将多种源语言的口语表达转化为目标语言的流畅、可理解的语音。</li>
<li>现有S2ST方法面临高质量、低延迟翻译以及依赖大规模平行语音语料库的挑战。</li>
<li>S2ST-Omni框架利用预训练模型实现高质量S2ST，同时减少大规模平行语音语料库的依赖。</li>
<li>引入轻量级语音适配器，有效融合语音和文本表示，利用预训练的多模态知识。</li>
<li>采用流式语音解码器，确保翻译准确性和实时响应性。</li>
<li>S2ST-Omni在CVSS基准测试上的表现超越了多个最新S2ST基线，凸显其有效性和优越性。</li>
<li>S2ST-Omni框架对于多语言语音翻译具有重要的实际应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3cc2cddc18be94463e136ebbdd5006dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf22ab268bbbf84bd98efbdd95a7184e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62d98ddcbe56060abd1c14891c176992.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection"><a href="#EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection" class="headerlink" title="EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection"></a>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection</h2><p><strong>Authors:Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer</strong></p>
<p>The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. </p>
<blockquote>
<p>文本转语音和音频生成模型的进步为评估人工智能系统的情感理解能力提供了强大的基准。现有的语音情感识别（SER）数据集在情感精细度、隐私担忧或依赖于表现表现方面存在局限性。本文介绍了用于语音情感检测的EmoNet-Voice新资源，其中包括EmoNet-Voice Big大规模预训练数据集（包含超过4500小时的语音数据，涵盖11种声音、40种情感和四种语言），以及带有专家注释的新型基准数据集EmoNet-Voice Bench。EmoNet-Voice旨在针对一个情感分类精细度高达40个类别的光谱评估SER模型，不同级别的强度。借助最先进的语音生成技术，我们精心制作了模拟演员扮演场景合成音频片段，旨在唤起特定情感。重要的是，我们通过心理学专家进行了严格验证，他们赋予了感知强度标签。这种合成的方法可保护隐私，允许加入现有数据集中通常缺少的敏感情感状态。最后，我们推出了Empathic Insight Voice模型，该模型在语音情感识别方面树立了新的标准，与人类专家的认可度很高。我们在当前模型景观中的评估展现了有价值的发现，例如愤怒等高唤起情感比集中等低唤起状态更容易检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09827v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的语音情感检测资源——EmoNet-Voice。它包括两个主要部分：大规模的预训练数据集EmoNet-Voice Big和新型基准数据集EmoNet-Voice Bench。EmoNet-Voice旨在评估语音情感识别模型在精细粒度的情感类别上的表现，并引入了合成音频片段和心理学专家验证的流程。其采用先进的语音生成技术，模拟演员在不同场景下的情感表现。此外，还介绍了Empathic Insight Voice模型，该模型在语音情感识别方面设定了新的标准，并展示了一些有趣的发现，如愤怒等高唤起情感更容易被检测，而低唤起状态如专注则较难。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新型的语音情感检测资源——EmoNet-Voice，用于评估AI系统的情感理解能力。</li>
<li>包含了大规模的预训练数据集EmoNet-Voice Big，涵盖多种语言、情感和声音。</li>
<li>提供了新型基准数据集EmoNet-Voice Bench，具有人类专家注释，用于精细粒度的语音情感识别。</li>
<li>通过合成音频片段模拟演员的情感表现，解决了现有数据集在情感粒度、隐私关注或表演依赖方面的问题。</li>
<li>引入了心理学专家验证流程，确保情感标注的准确性。</li>
<li>采用了隐私保护的合成方法，能够包含现有数据集中常缺失的敏感情感状态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5826b7a29f41fe24f9d3145cda5eeda7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89e5c3e69192ee6e28a9481fc91b6d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f3ad4742d3f56ecf7b1adf9445f1db9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead12f776165c52d3bd1fc04d82aeaba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LEP3-A-High-Luminosity-e-e-Higgs-and-ElectroweakFactory-in-the-LHC-Tunnel"><a href="#LEP3-A-High-Luminosity-e-e-Higgs-and-ElectroweakFactory-in-the-LHC-Tunnel" class="headerlink" title="LEP3: A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel"></a>LEP3: A High-Luminosity e+e- Higgs and ElectroweakFactory in the LHC   Tunnel</h2><p><strong>Authors:C. Anastopoulos, R. Assmann, A. Ball, O. Bruning, O. Buchmueller, T. Camporesi, P. Collier, J Dainton, G. Davies, J. R. Ellis, B. Goddard, L. Gouskos, M. Klute, M. Koratzinos, G. Landsberg, K. Long, L. Malgeri, F. Maltoni, F. Moortgat, C. Mariotti, S. Myers, J. A. Osborne, M. Pierini, D. R. Tovey, D. Treille, T. S. Virdee, N. Wardle, M. Zanetti</strong></p>
<p>As stated in the 2019 European Strategy for Particle Physics (ESPP), it is of the utmost importance that the HL-LHC upgrade of the accelerator and the experiments be successfully completed in a timely manner. All necessary efforts should be devoted to achieving this goal. We also recall two of the principal recommendations of the 2019 ESPP for future accelerator initiatives, namely that 1) An electron-positron Higgs factory is the highest priority for the next collider (Rec. c). 2) Europe, together with its international partners, should investigate the technical and financial feasibility of a future hadron collider at CERN with a centre-of-mass energy of at least 100 TeV and with an electron-positron Higgs and electroweak factory as a possible first stage (Rec. e). A major objective in particle physics is always to operate an accelerator that allows a leap of an order of magnitude in the constituent centre-of-mass energy with respect to the previous one. We support FCC-ee and FCC-hh as the preferred option for CERN future, as it addresses both of the above recommendations.   The guidance for the 2025 ESPP requests, in addition to the preferred option, the inclusion of &#96;&#96;prioritised alternatives to be pursued if the chosen preferred option turns out not to be feasible or competitive’’. Proposed alternatives to the preferred FCC option include linear, muon colliders and LHeC accelerators. In response to this request we propose reusing the existing LHC tunnel for an electron-positron collider, called LEP3, as a back-up alternative if the FCC cannot proceed. LEP3 leverages much of the R&amp;D conducted for FCC-ee, offers high-precision studies of Z, W, and Higgs bosons below the tt threshold, and offers potential physics performance comparable or superior to other fallback options at a lower cost while supporting continued R&amp;D towards a next-generation energy frontier machine. </p>
<blockquote>
<p>如2019年欧洲粒子物理策略（ESPP）所述，及时成功完成HL-LHC加速器及实验的升级至关重要。我们应该付出一切努力来实现这个目标。我们还回顾了2019年ESPP关于未来加速器倡议的两个主要建议，即：1) 电子正电子希格斯工厂是下一个对撞机的首要选择（建议c）。2) 欧洲应与其国际合作伙伴共同研究在CERN建造一个至少具有100TeV质心能量的未来强子对撞机的技术和财务可行性，并将电子正电子希格斯工厂和电弱工厂作为可能的第一阶段（建议e）。粒子物理学的一个主要目标始终是运行一种加速器，这种加速器能够使质心能量相对于前一个实现数量级的飞跃。我们支持FCC-ee和FCC-hh作为CERN未来的首选方案，因为它涵盖了上述两个建议。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00541v2">PDF</a> 11 pages, 3 tables</p>
<p><strong>Summary</strong><br>     欧洲粒子物理战略强调HL-LHC加速器及其实验的升级至关重要，需及时完成。主要推荐未来加速器计划为电子正负粒子希格斯工厂及在至少百TeV级的大型强子对撞机上进行的技术和财务可行性研究。支持FCC作为CERN的优选方案。同时考虑备选方案，如线性加速器、μ子对撞机和LHeC加速器等。并提出若FCC无法实施则利用现有LHC隧道建立LEP3作为备用选项。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HL-LHC加速器及其实验的升级至关重要，需努力完成以达成未来粒子物理研究目标。</li>
<li>电子正负粒子希格斯工厂是未来加速器的首要计划。</li>
<li>欧洲与其国际伙伴正在研究在CERN建立一个至少百TeV级的大型强子对撞机的技术和财务可行性。</li>
<li>FCC被视作CERN的首选方案，它结合了以上两项建议的目标。</li>
<li>提出了替代方案如线性加速器、μ子对撞机和LHeC加速器等以应对可能的不确定性。</li>
<li>如果首选方案无法实现，可以考虑利用现有的LHC隧道建立一个电子正负粒子对撞机LEP3作为备选方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-65da9c8f38f5ed7455883653392fdc71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-caaac85e3381ec883503d3682f5a4da2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae5ea89989805d02f0cf79d2c2aa9f32.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="On-the-Feasibility-of-Fully-AI-automated-Vishing-Attacks"><a href="#On-the-Feasibility-of-Fully-AI-automated-Vishing-Attacks" class="headerlink" title="On the Feasibility of Fully AI-automated Vishing Attacks"></a>On the Feasibility of Fully AI-automated Vishing Attacks</h2><p><strong>Authors:João Figueiredo, Afonso Carvalho, Daniel Castro, Daniel Gonçalves, Nuno Santos</strong></p>
<p>A vishing attack is a form of social engineering where attackers use phone calls to deceive individuals into disclosing sensitive information, such as personal data, financial information, or security credentials. Attackers exploit the perceived urgency and authenticity of voice communication to manipulate victims, often posing as legitimate entities like banks or tech support. Vishing is a particularly serious threat as it bypasses security controls designed to protect information. In this work, we study the potential for vishing attacks to escalate with the advent of AI. In theory, AI-powered software bots may have the ability to automate these attacks by initiating conversations with potential victims via phone calls and deceiving them into disclosing sensitive information. To validate this thesis, we introduce ViKing, an AI-powered vishing system developed using publicly available AI technology. It relies on a Large Language Model (LLM) as its core cognitive processor to steer conversations with victims, complemented by a pipeline of speech-to-text and text-to-speech modules that facilitate audio-text conversion in phone calls. Through a controlled social experiment involving 240 participants, we discovered that ViKing has successfully persuaded many participants to reveal sensitive information, even those who had been explicitly warned about the risk of vishing campaigns. Interactions with ViKing’s bots were generally considered realistic. From these findings, we conclude that tools like ViKing may already be accessible to potential malicious actors, while also serving as an invaluable resource for cyber awareness programs. </p>
<blockquote>
<p>钓鱼攻击是一种社会工程学手段，攻击者通过拨打电话欺骗个人泄露敏感信息，如个人数据、财务信息或安全凭据。攻击者利用语音通信的紧迫感和真实性来操纵受害者，经常伪装成合法实体，如银行或技术支持。钓鱼攻击是一种特别严重的威胁，因为它绕过了为保护信息而设计的安全控制。在这项工作中，我们研究了随着人工智能的出现，钓鱼攻击恶化的潜力。理论上，人工智能软件机器人有能力通过拨打与潜在受害者进行对话的电话自动执行这些攻击，并欺骗他们泄露敏感信息。为了验证这一观点，我们介绍了使用公开可用的人工智能技术开发的名为ViKing的人工智能钓鱼系统。它依赖于大型语言模型作为其核心的对话处理器来引导与受害者的对话，辅以语音到文本和文本到语音的模块管道，便于电话中的音频文本转换。通过一项涉及240名参与者的受控社会实验，我们发现ViKing已成功说服许多参与者透露敏感信息，即使那些已经明确警告过面临钓鱼攻击风险的人。与ViKing机器人的互动通常被认为是真实的。根据这些发现，我们得出结论：像ViKing这样的工具可能已经被潜在的恶意行为者所掌握，同时也为网络安全意识计划提供了宝贵的资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13793v2">PDF</a> To appear in AsiaCCS 2025</p>
<p><strong>摘要</strong></p>
<p>这篇文本探讨了AI的兴起对于网络攻击产生的影响，特别是基于AI技术的维希攻击。文本中详细介绍了维希攻击的形式和原理，以及AI如何被用于自动化此类攻击。通过引入名为ViKing的AI维希系统，展示了AI技术在网络攻击中的应用。实验结果显示，该系统的确能够欺骗参与者并获取敏感信息。这表明这类工具可能会被潜在的网络犯罪分子所利用，并成为一种极具威胁的网络攻击手段。但也可以将其作为网络安全意识培训的资源。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>维希攻击是一种利用电话通话欺骗个人信息的社交工程形式。攻击者会伪装成合法实体以获取个人数据、财务信息或安全凭据等敏感信息。这种攻击方式对保护信息的传统安全措施具有强大的规避能力。</li>
<li>AI的发展给这些攻击带来了更大的潜力，通过使用AI软件机器人发起自动化电话对话来实现更高的攻击效率。而AI技术在模拟真实对话方面表现出较高的准确性，增加了此类攻击的欺骗性。例如，ViKing系统利用大型语言模型作为核心认知处理器来引导与受害者的对话，同时通过语音到文本和文本到语音模块进行音频文本转换，使其电话通话更逼真。这一技术被广泛运用可能导致安全威胁的升级。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ced65d3ce6435f36a9787756bf32ec87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4a9c0c2be624e32c7c3997ebdb6729.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cc5d658d3532f020dd903f92a371c53.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-06-22  Improving Dialogue Discourse Parsing through Discourse-aware Utterance   Clarification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-22  Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
