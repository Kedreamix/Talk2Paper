<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ee45cedcd6353bdb3f75d59119e8916e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="Exploiting-Music-Source-Separation-for-Automatic-Lyrics-Transcription-with-Whisper"><a href="#Exploiting-Music-Source-Separation-for-Automatic-Lyrics-Transcription-with-Whisper" class="headerlink" title="Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper"></a>Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper</h2><p><strong>Authors:Jaza Syed, Ivan Meresman Higgs, OndÅ™ej CÃ­fka, Mark Sandler</strong></p>
<p>Automatic lyrics transcription (ALT) remains a challenging task in the field of music information retrieval, despite great advances in automatic speech recognition (ASR) brought about by transformer-based architectures in recent years. One of the major challenges in ALT is the high amplitude of interfering audio signals relative to conventional ASR due to musical accompaniment. Recent advances in music source separation have enabled automatic extraction of high-quality separated vocals, which could potentially improve ALT performance. However, the effect of source separation has not been systematically investigated in order to establish best practices for its use. This work examines the impact of source separation on ALT using Whisper, a state-of-the-art open source ASR model. We evaluate Whisperâ€™s performance on original audio, separated vocals, and vocal stems across short-form and long-form transcription tasks. For short-form, we suggest a concatenation method that results in a consistent reduction in Word Error Rate (WER). For long-form, we propose an algorithm using source separation as a vocal activity detector to derive segment boundaries, which results in a consistent reduction in WER relative to Whisperâ€™s native long-form algorithm. Our approach achieves state-of-the-art results for an open source system on the Jam-ALT long-form ALT benchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the first dataset of long-form lyric transcripts following the Jam-ALT guidelines for which vocal stems are publicly available. </p>
<blockquote>
<p>è‡ªåŠ¨æ­Œè¯è½¬å½•ï¼ˆALTï¼‰ä»æ˜¯éŸ³ä¹ä¿¡æ¯æ£€ç´¢é¢†åŸŸä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡è¿‘å¹´æ¥åŸºäºè½¬æ¢å™¨çš„æ¶æ„åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œä½†ALTä»ç„¶é¢ä¸´è®¸å¤šå›°éš¾ã€‚ALTçš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯éŸ³ä¹ä¼´å¥äº§ç”Ÿçš„å¹²æ‰°éŸ³é¢‘ä¿¡å·å¹…åº¦è¾ƒé«˜ï¼Œè¿™ä½¿å¾—ç›¸å¯¹äºä¼ ç»ŸASRæ¥è¯´æ›´å…·æŒ‘æˆ˜æ€§ã€‚éŸ³ä¹æºåˆ†ç¦»æ–¹é¢çš„æœ€æ–°è¿›å±•ä½¿å¾—èƒ½å¤Ÿè‡ªåŠ¨æå–é«˜è´¨é‡çš„åˆ†å¼€çš„å£°éŸ³ï¼Œè¿™å¯èƒ½ä¼šæé«˜ALTçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸ºäº†å»ºç«‹æœ€ä½³å®è·µï¼Œæºåˆ†ç¦»çš„å½±å“å°šæœªè¿›è¡Œç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚è¿™é¡¹å·¥ä½œä½¿ç”¨Whisperï¼ˆä¸€ç§å…ˆè¿›çš„å¼€æºASRæ¨¡å‹ï¼‰æ¥æ£€æŸ¥æºåˆ†ç¦»å¯¹ALTçš„å½±å“ã€‚æˆ‘ä»¬è¯„ä¼°äº†Whisperåœ¨åŸå§‹éŸ³é¢‘ã€åˆ†ç¦»åçš„å£°éŸ³ã€ä»¥åŠè¯­éŸ³èŒä¸Šçš„æ€§èƒ½ï¼Œæ¶µç›–äº†çŸ­æœŸå’Œé•¿æœŸè½¬å½•ä»»åŠ¡ã€‚å¯¹äºçŸ­æœŸä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‹¼æ¥æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¼è‡´å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æŒç»­ä¸‹é™ã€‚å¯¹äºé•¿æœŸä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æºåˆ†ç¦»ä½œä¸ºè¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨æ¥æ¨å¯¼æ®µè¾¹ç•Œçš„ç®—æ³•ï¼Œç›¸å¯¹äºWhisperçš„æœ¬åœ°é•¿æœŸç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨WERæ–¹é¢å®ç°äº†æŒç»­çš„é™ä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨éµå¾ªJam-ALTæŒ‡å¯¼æ–¹é’ˆçš„é•¿æœŸALTåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¼€æ”¾æºä»£ç ç³»ç»Ÿçš„æœ€æ–°æˆæœæ°´å¹³ï¼Œæ— éœ€è¿›è¡Œä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ç¬¬ä¸€ä¸ªéµå¾ªJam-ALTæŒ‡å¯¼æ–¹é’ˆçš„é•¿æœŸæ­Œè¯è½¬å½•æ•°æ®é›†MUSDB-ALTï¼Œè¯¥æ•°æ®é›†æä¾›çš„æ­Œè¯å½•éŸ³å‡ä¸ºå…¬å¼€å¯ç”¨çš„è¯­éŸ³èŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15514v1">PDF</a> Accepted at 2025 ICME Workshop AI for Music</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å°½ç®¡è¿‘å¹´æ¥åŸºäºè½¬æ¢å™¨çš„æ¶æ„åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†è‡ªåŠ¨æ­Œè¯è½¬å½•ï¼ˆALTï¼‰ä»æ˜¯éŸ³ä¹ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ALTçš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯éŸ³ä¹ä¼´å¥äº§ç”Ÿçš„å¹²æ‰°éŸ³é¢‘ä¿¡å·å¹…åº¦è¾ƒé«˜ï¼Œä¸å¸¸è§„ASRç›¸æ¯”ï¼Œè¿™å¯¹ALTæå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚éŸ³ä¹æºåˆ†ç¦»çš„è¿‘æœŸè¿›å±•ä½¿å¾—èƒ½å¤Ÿè‡ªåŠ¨æå–é«˜è´¨é‡çš„åˆ†ç¦»äººå£°ï¼Œè¿™å¯èƒ½ä¼šæé«˜ALTçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæºåˆ†ç¦»çš„å½±å“å°šæœªè¿›è¡Œç³»ç»Ÿæ€§çš„ç ”ç©¶ï¼Œä»¥å»ºç«‹å…¶æœ€ä½³å®è·µã€‚æœ¬ç ”ç©¶ä½¿ç”¨Whisperè¿™ä¸€å…ˆè¿›çš„å¼€æºASRæ¨¡å‹ï¼Œæ¢è®¨äº†æºåˆ†ç¦»å¯¹ALTçš„å½±å“ã€‚æˆ‘ä»¬è¯„ä¼°äº†Whisperåœ¨åŸå§‹éŸ³é¢‘ã€åˆ†ç¦»çš„äººå£°å’Œè¯­éŸ³ç‰‡æ®µä¸Šçš„æ€§èƒ½ï¼Œæ¶µç›–äº†çŸ­å½¢å¼å’Œé•¿å½¢å¼çš„è½¬å½•ä»»åŠ¡ã€‚å¯¹äºçŸ­å½¢å¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‹¼æ¥æ–¹æ³•ï¼Œå¯¼è‡´è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æŒç»­é™ä½ã€‚å¯¹äºé•¿å½¢å¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æºåˆ†ç¦»ä½œä¸ºè¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨æ¥æ¨å¯¼æ®µè½è¾¹ç•Œçš„ç®—æ³•ï¼Œä¸Whisperçš„å†…ç½®é•¿å½¢å¼ç®—æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•å¯¼è‡´WERæŒç»­é™ä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨éµå¾ªJam-ALTé•¿å½¢å¼ALTåŸºå‡†çš„MUSDB-ALTæ•°æ®é›†ä¸Šå®ç°äº†å¼€æºç³»ç»Ÿçš„æœ€æ–°ç»“æœï¼Œæ— éœ€è¿›è¡Œä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªåŠ¨æ­Œè¯è½¬å½•ï¼ˆALTï¼‰æ˜¯éŸ³ä¹ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯éŸ³ä¹ä¼´å¥äº§ç”Ÿçš„é«˜å¹…åº¦å¹²æ‰°éŸ³é¢‘ä¿¡å·ã€‚</li>
<li>è¿‘æœŸéŸ³ä¹æºåˆ†ç¦»çš„è¿›å±•ä¸ºALTçš„æ”¹è¿›æä¾›äº†å¯èƒ½ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æå–é«˜è´¨é‡çš„åˆ†ç¦»äººå£°ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨Whisperè¿™ä¸€å…ˆè¿›çš„å¼€æºASRæ¨¡å‹ï¼Œæ¢è®¨äº†æºåˆ†ç¦»å¯¹ALTçš„å½±å“ã€‚</li>
<li>å¯¹äºçŸ­å½¢å¼çš„è½¬å½•ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ‹¼æ¥æ–¹æ³•ï¼Œèƒ½å¤Ÿé™ä½è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>å¯¹äºé•¿å½¢å¼çš„è½¬å½•ä»»åŠ¡ï¼Œä½¿ç”¨æºåˆ†ç¦»ä½œä¸ºè¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨æ¥æ¨å¯¼æ®µè½è¾¹ç•Œçš„ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆé™ä½WERã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åœ¨éµå¾ªJam-ALTé•¿å½¢å¼ALTåŸºå‡†çš„MUSDB-ALTæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒã€‚</li>
<li>ç ”ç©¶è¿˜å‘å¸ƒäº†éµå¾ªJam-ALTæŒ‡å—çš„é•¿å½¢å¼æ­Œè¯è½¬å½•æ•°æ®é›†MUSDB-ALTï¼Œå…¶ä¸­äººå£°ç´ æå¯å…¬å¼€è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76116cbee6fa7a63dd1d76946d4f263a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee45cedcd6353bdb3f75d59119e8916e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3175e0ee94136779af73bf1c39a25f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe22069528e0f74b50ef218223d94ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8134f427ba5c3baee5a6cf86b9a400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af420a3ab285d10f4722703b38f41d3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd17ef34c9cb53ccaa0b1943233bd73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b910a62a5aec036e66bc74abfc939b0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Evaluation-of-Deep-Learning-Models-for-Speech-Enhancement-in-Real-World-Noisy-Environments"><a href="#A-Comparative-Evaluation-of-Deep-Learning-Models-for-Speech-Enhancement-in-Real-World-Noisy-Environments" class="headerlink" title="A Comparative Evaluation of Deep Learning Models for Speech Enhancement   in Real-World Noisy Environments"></a>A Comparative Evaluation of Deep Learning Models for Speech Enhancement   in Real-World Noisy Environments</h2><p><strong>Authors:Md Jahangir Alam Khondkar, Ajan Ahmed, Masudul Haider Imtiaz, Stephanie Schuckers</strong></p>
<p>Speech enhancement, particularly denoising, is vital in improving the intelligibility and quality of speech signals for real-world applications, especially in noisy environments. While prior research has introduced various deep learning models for this purpose, many struggle to balance noise suppression, perceptual quality, and speaker-specific feature preservation, leaving a critical research gap in their comparative performance evaluation. This study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and U-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These models were chosen due to their relevance in the literature and code accessibility. The evaluation reveals that U-Net achieves high noise suppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and +364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality, attaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it well-suited for applications prioritizing natural and intelligible speech. Wave-U-Net balances these attributes with improvements in speaker-specific feature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and +27.38% on VPQAD. This research indicates how advanced methods can optimize trade-offs between noise suppression, perceptual quality, and speaker recognition. The findings may contribute to advancing voice biometrics, forensic audio analysis, telecommunication, and speaker verification in challenging acoustic conditions. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼Œå°¤å…¶æ˜¯å»å™ªï¼Œåœ¨æ”¹å–„çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è¯­éŸ³ä¿¡å·çš„æ¸…æ™°åº¦å’Œè´¨é‡æ–¹é¢è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»å¼•å…¥äº†å„ç§æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä½†è®¸å¤šæ¨¡å‹åœ¨å¹³è¡¡å™ªå£°æŠ‘åˆ¶ã€æ„ŸçŸ¥è´¨é‡å’Œè¯´è¯äººç‰¹å®šç‰¹å¾ä¿ç•™æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½¿å¾—å®ƒä»¬åœ¨æ€§èƒ½æ¯”è¾ƒè¯„ä¼°æ–¹é¢å­˜åœ¨å…³é”®çš„ç ”ç©¶ç©ºç™½ã€‚æœ¬ç ”ç©¶å¯¹Wave-U-Netã€CMGANå’ŒU-Netä¸‰ç§å‰æ²¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨çš„æ•°æ®é›†åŒ…æ‹¬SpEARã€VPQADå’ŒClarksonæ•°æ®é›†ç­‰å¤šæ ·åŒ–æ•°æ®é›†ã€‚è¿™äº›æ¨¡å‹ä¹‹æ‰€ä»¥è¢«é€‰ä¸­ï¼Œæ˜¯å› ä¸ºå®ƒä»¬åœ¨æ–‡çŒ®ä¸­çš„ç›¸å…³æ€§å’Œä»£ç çš„å¯è®¿é—®æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒU-Netåœ¨SpEARæ•°æ®é›†ä¸Šå®ç°äº†+71.96%ã€åœ¨VPQADæ•°æ®é›†ä¸Šå®ç°äº†+64.83%ã€åœ¨Clarksonæ•°æ®é›†ä¸Šå®ç°äº†+364.2%çš„ä¿¡å™ªæ¯”æé«˜ï¼Œå–å¾—äº†å¾ˆé«˜çš„å™ªå£°æŠ‘åˆ¶æ•ˆæœã€‚CMGANåœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°çªå‡ºï¼Œåœ¨SpEARå’ŒVPQADä¸Šåˆ†åˆ«è·å¾—äº†4.04å’Œ1.46çš„PESQé«˜åˆ†ï¼Œä½¿å…¶æˆä¸ºä¼˜å…ˆè€ƒè™‘è‡ªç„¶å’Œå¯ç†è§£è¯­éŸ³çš„åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚Wave-U-Netåœ¨è¯´è¯äººç‰¹å®šç‰¹å¾ä¿ç•™æ–¹é¢æœ‰æ‰€æé«˜ï¼Œå¹³è¡¡äº†è¿™äº›ç‰¹ç‚¹ï¼Œè¿™åœ¨SpEARå’ŒVPQADçš„VeriSpeakå¾—åˆ†å¢é•¿+10.84%å’Œ+27.38%ä¸­å¾—åˆ°äº†è¯æ˜ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œå…ˆè¿›çš„æ–¹æ³•å¦‚ä½•ä¼˜åŒ–å™ªå£°æŠ‘åˆ¶ã€æ„ŸçŸ¥è´¨é‡å’Œè¯´è¯äººè¯†åˆ«ä¹‹é—´çš„æƒè¡¡ã€‚è¿™äº›å‘ç°å¯èƒ½æœ‰åŠ©äºæ¨åŠ¨è¯­éŸ³ç”Ÿç‰©è¯†åˆ«ã€éŸ³é¢‘åˆ†æé‰´å®šã€ç”µä¿¡å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦æ¡ä»¶ä¸‹çš„è¯´è¯äººéªŒè¯çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸‰ç§å…ˆè¿›çš„è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼ˆWave-U-Netã€CMGANå’ŒU-Netï¼‰åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚U-Netåœ¨å™ªå£°æŠ‘åˆ¶æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œç‰¹åˆ«æ˜¯åœ¨SpEARã€VPQADå’ŒClarksonæ•°æ®é›†ä¸Šçš„ä¿¡å·å™ªå£°æ¯”ï¼ˆSNRï¼‰æå‡æ˜¾è‘—ã€‚CMGANåœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œé€‚åˆéœ€è¦è‡ªç„¶å’Œå¯ç†è§£è¯­éŸ³çš„åº”ç”¨ã€‚Wave-U-Netåˆ™åœ¨å¹³è¡¡å™ªå£°æŠ‘åˆ¶ã€æ„ŸçŸ¥è´¨é‡å’Œè¯´è¯äººç‰¹å¾ä¿ç•™æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚ç ”ç©¶å¯¹äºä¼˜åŒ–è¿™äº›æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºåœ¨å™ªå£°ç¯å¢ƒä¸­å¯¹æ”¹å–„è¯­éŸ³ä¿¡å·çš„æ¸…æ™°åº¦å’Œè´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>U-Netåœ¨å™ªå£°æŠ‘åˆ¶æ–¹é¢è¡¨ç°çªå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„SNRæå‡æ˜¾è‘—ã€‚</li>
<li>CMGANåœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æœ€ä½³ï¼Œé€‚ç”¨äºéœ€è¦è‡ªç„¶å’Œå¯ç†è§£è¯­éŸ³çš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>Wave-U-Netåœ¨å¹³è¡¡å™ªå£°æŠ‘åˆ¶ã€æ„ŸçŸ¥è´¨é‡å’Œè¯´è¯äººç‰¹å¾ä¿ç•™æ–¹é¢å…·å¤‡ä¼˜åŠ¿ã€‚</li>
<li>è¯´è¯äººç‰¹å¾ä¿ç•™å¯¹äºè¯­éŸ³ç”Ÿç‰©è¯†åˆ«ã€è¯­éŸ³éªŒè¯ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºæ·±å…¥äº†è§£ä¸åŒæ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-718cd9af1d414398cb3bbec416541c32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ca0de073f71656e88c2b989e8005ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f99deaec6f819b5be47523f0db94d770.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f2f6a99fb8fd0adebede96f8d216bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c255786f5c63e2c0eaefff0fe5ca9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31fb8a1d4257e26eea842eff00a512db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3eb533005527d15716ad1e6ad10a6204.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Variational-Framework-for-Improving-Naturalness-in-Generative-Spoken-Language-Models"><a href="#A-Variational-Framework-for-Improving-Naturalness-in-Generative-Spoken-Language-Models" class="headerlink" title="A Variational Framework for Improving Naturalness in Generative Spoken   Language Models"></a>A Variational Framework for Improving Naturalness in Generative Spoken   Language Models</h2><p><strong>Authors:Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky</strong></p>
<p>The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at <a target="_blank" rel="noopener" href="https://github.com/b04901014/vae-gslm">https://github.com/b04901014/vae-gslm</a>. </p>
<blockquote>
<p>æ–‡æœ¬å¤„ç†ä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆåŠŸæ¿€å‘äº†å…¶åœ¨è¯­éŸ³å»ºæ¨¡ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³æ˜¯è¿ç»­ä¸”å¤æ‚çš„ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œç¦»æ•£åŒ–ä»¥è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ã€‚ä»è‡ªç›‘ç£æ¨¡å‹æ´¾ç”Ÿçš„è¯­éŸ³ä»¤ç‰Œï¼ˆç§°ä¸ºè¯­ä¹‰ä»¤ç‰Œï¼‰ä¸»è¦å…³æ³¨è¯­éŸ³çš„è¯­è¨€æ–¹é¢ï¼Œä½†å¿½ç•¥äº†éŸµå¾‹ä¿¡æ¯ã€‚å› æ­¤ï¼Œåœ¨è¿™äº›ä»¤ç‰Œä¸Šè®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆçš„è¯­éŸ³ä¼šé™ä½è‡ªç„¶åº¦ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å‘è¯­ä¹‰ä»¤ç‰Œæ·»åŠ éŸ³è°ƒç‰¹å¾æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå•ç‹¬çš„éŸ³è°ƒä¸èƒ½å®Œå…¨ä»£è¡¨å‰¯è¯­è¨€å±æ€§çš„èŒƒå›´ï¼Œå¹¶ä¸”é€‰æ‹©æ­£ç¡®çš„ç‰¹å¾éœ€è¦ä»”ç»†çš„æ‰‹å·¥è®¾è®¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å˜åˆ†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ç¼–ç è¿™äº›è¿ç»­çš„è¯­éŸ³å±æ€§ä»¥å¢å¼ºè¯­ä¹‰ä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†æ‰‹åŠ¨æå–å’Œé€‰æ‹©å‰¯è¯­è¨€ç‰¹å¾çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œå®ƒæ ¹æ®äººç±»è¯„ä¼°è€…äº§ç”Ÿäº†æ›´å—æ¬¢è¿çš„è¯­éŸ³å»¶ç»­ã€‚ä»£ç ã€æ ·æœ¬å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/b04901014/vae-gslm%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/b04901014/vae-gslmè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14767v1">PDF</a> International Conference on Machine Learning (ICML) 2025</p>
<p><strong>æ€»ç»“</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬å¤„ç†æ–¹é¢çš„æˆåŠŸï¼Œäººä»¬å°è¯•å°†å…¶åº”ç”¨äºè¯­éŸ³å»ºæ¨¡ã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³çš„è¿ç»­æ€§å’Œå¤æ‚æ€§ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œç¦»æ•£åŒ–ä»¥è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ã€‚ä»è‡ªç›‘ç£æ¨¡å‹æ´¾ç”Ÿçš„è¯­éŸ³ä»¤ç‰Œï¼ˆç§°ä¸ºè¯­ä¹‰ä»¤ç‰Œï¼‰ä¸»è¦å…³æ³¨è¯­éŸ³çš„è¯­è¨€æ–¹é¢ï¼Œä½†å¿½ç•¥äº†éŸµå¾‹ä¿¡æ¯ã€‚å› æ­¤ï¼Œåœ¨è¿™äº›ä»¤ç‰Œä¸Šè®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆçš„è¯­éŸ³è‡ªç„¶åº¦é™ä½ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å‘è¯­ä¹‰ä»¤ç‰Œæ·»åŠ éŸ³é«˜ç‰¹å¾æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼ŒéŸ³é«˜æ— æ³•å®Œå…¨ä»£è¡¨å¤šç§å‰¯è¯­è¨€å±æ€§ï¼Œé€‰æ‹©åˆé€‚çš„ç‰¹å¾éœ€è¦ç²¾å¿ƒæ‰‹å·¥è®¾è®¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å˜åˆ†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨å­¦ä¹ ç¼–ç è¿™äº›è¿ç»­çš„è¯­éŸ³å±æ€§ä»¥å¢å¼ºè¯­ä¹‰ä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†æ‰‹åŠ¨æå–å’Œé€‰æ‹©å‰¯è¯­è¨€ç‰¹å¾çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæ ¹æ®äººç±»è¯„åˆ†è€…çš„è¯„ä¼°ï¼Œå®ƒäº§ç”Ÿäº†æ›´å—æ¬¢è¿çš„è¯­éŸ³è¿ç»­å†…å®¹ã€‚ç›¸å…³ä»£ç ã€æ ·æœ¬å’Œæ¨¡å‹å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/b04901014/vae-gslm">https://github.com/b04901014/vae-gslm</a> è·å¾—ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬å¤„ç†ä¸­çš„æˆåŠŸæ¨åŠ¨äº†å…¶åœ¨è¯­éŸ³å»ºæ¨¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>è¯­éŸ³ç”±äºå…¶è¿ç»­æ€§å’Œå¤æ‚æ€§ï¼Œå¸¸å¸¸éœ€è¦ç¦»æ•£åŒ–ä»¥è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸­å¸¸ç”¨çš„è¯­éŸ³ä»¤ç‰Œå¿½ç•¥äº†éŸµå¾‹ä¿¡æ¯ï¼Œå½±å“ç”Ÿæˆçš„è¯­éŸ³çš„è‡ªç„¶åº¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å‘è¯­ä¹‰ä»¤ç‰Œæ·»åŠ éŸ³é«˜ç‰¹å¾æ¥è§£å†³è¯­éŸ³è‡ªç„¶åº¦é—®é¢˜ã€‚ä½†éŸ³é«˜ä¸è¶³ä»¥ä»£è¡¨æ‰€æœ‰å‰¯è¯­è¨€å±æ€§ä¸”ç‰¹å¾é€‰æ‹©éœ€è¦ç²¾ç»†çš„æ‰‹å·¥è®¾è®¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å˜åˆ†æ–¹æ³•ï¼Œè‡ªåŠ¨å­¦ä¹ ç¼–ç è¿ç»­çš„è¯­éŸ³å±æ€§ä»¥å¢å¼ºè¯­ä¹‰ä»¤ç‰Œã€‚è¿™ä¸€æ–¹æ³•æ— éœ€æ‰‹åŠ¨æå–å’Œé€‰æ‹©å‰¯è¯­è¨€ç‰¹å¾ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è¯­éŸ³å†…å®¹æ›´å—æ¬¢è¿ï¼Œæ ¹æ®äººç±»è¯„åˆ†è€…çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eb6e1d595694baa1c25187dab0b74700.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa19d6b7e3e1b1aa58805a0cdcdb8d9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd19fdd04901fd07488e5dd78bfb7e42.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SyncTalk-High-Fidelity-and-Efficient-Synchronized-Talking-Heads-Synthesis-Using-Gaussian-Splatting"><a href="#SyncTalk-High-Fidelity-and-Efficient-Synchronized-Talking-Heads-Synthesis-Using-Gaussian-Splatting" class="headerlink" title="SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads   Synthesis Using Gaussian Splatting"></a>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads   Synthesis Using Gaussian Splatting</h2><p><strong>Authors:Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, Zhaoxin Fan</strong></p>
<p>Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the â€˜â€™devilâ€™â€™ in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: <a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk++">https://ziqiaopeng.github.io/synctalk++</a>. </p>
<blockquote>
<p>åœ¨åˆæˆé€¼çœŸçš„è¯­éŸ³é©±åŠ¨å¼è¯´è¯äººå¤´è§†é¢‘æ—¶ï¼Œå®ç°é«˜åŒæ­¥ç‡æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸€ä¸ªæ ©æ ©å¦‚ç”Ÿçš„è¯´è¯äººå¤´éƒ¨éœ€è¦åŒæ­¥åè°ƒä¸»ä½“èº«ä»½ã€å˜´å”‡åŠ¨ä½œã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿æ€ã€‚ç¼ºä¹è¿™äº›åŒæ­¥æ˜¯ä¸€ä¸ªåŸºæœ¬ç¼ºé™·ï¼Œä¼šå¯¼è‡´ä¸çœŸå®çš„ç»“æœã€‚ä¸ºäº†è§£å†³åŒæ­¥è¿™ä¸€å…³é”®é—®é¢˜ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºåˆ›å»ºé€¼çœŸè¯´è¯å¤´éƒ¨çš„â€œé­”é¬¼â€ï¼Œå¹¶å¼•å…¥äº†SyncTalk++ã€‚å®ƒå…·å¤‡åŠ¨æ€è‚–åƒæ¸²æŸ“å™¨å’Œé«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼Œç¡®ä¿ä¸»ä½“èº«ä»½çš„ä¸€è‡´ä¿ç•™ï¼Œä»¥åŠé¢éƒ¨åŒæ­¥æ§åˆ¶å™¨ï¼Œè¯¥æ§åˆ¶å™¨ä½¿å˜´å”‡åŠ¨ä½œä¸è¯­éŸ³å¯¹é½ï¼ŒåŒæ—¶åˆ›æ–°åœ°ä½¿ç”¨3Dé¢éƒ¨æ··åˆå½¢çŠ¶æ¨¡å‹æ¥é‡å»ºå‡†ç¡®çš„é¢éƒ¨è¡¨æƒ…ã€‚ä¸ºäº†ç¡®ä¿è‡ªç„¶çš„å¤´éƒ¨åŠ¨ä½œï¼Œæˆ‘ä»¬æå‡ºäº†å¤´éƒ¨åŒæ­¥ç¨³å®šå™¨ï¼Œå®ƒä¼˜åŒ–äº†å¤´éƒ¨å§¿æ€ï¼Œæé«˜äº†ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒSyncTalk++é€šè¿‡èå…¥è¡¨æƒ…ç”Ÿæˆå™¨å’Œèº¯å¹²æ¢å¤å™¨ï¼Œå¢å¼ºäº†å¤„ç†ç¦»ç¾¤å€¼ï¼ˆOODï¼‰éŸ³é¢‘çš„ç¨³å¥æ€§ï¼Œè¿™ä¸¤ä¸ªç»„ä»¶å¯ç”Ÿæˆä¸è¯­éŸ³ç›¸åŒ¹é…çš„è¡¨æƒ…å’Œæ— ç¼çš„èº¯å¹²åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†è·¨å¸§çš„è§†è§‰ç»†èŠ‚çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œå“è´¨ï¼Œè¾¾åˆ°äº†æ¯ç§’101å¸§ã€‚å¤§é‡çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒSyncTalk++åœ¨åŒæ­¥å’Œé€¼çœŸåº¦æ–¹é¢è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æˆ‘ä»¬æ¨èè§‚çœ‹è¡¥å……è§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk++.html">https://ziqiaopeng.github.io/synctalk++.html</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14742v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SyncTalk++æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç”¨äºåˆæˆé«˜åº¦åŒæ­¥çš„ã€ä»¥è¯­éŸ³é©±åŠ¨çš„çœŸå®è°ˆè¯è§†é¢‘ã€‚å®ƒåŒ…å«åŠ¨æ€è‚–åƒæ¸²æŸ“å™¨ã€é¢éƒ¨åŒæ­¥æ§åˆ¶å™¨å’Œå¤´éƒ¨åŒæ­¥ç¨³å®šå™¨ç­‰å¤šä¸ªç»„ä»¶ï¼Œç¡®ä¿ä¸»ä½“èº«ä»½ã€å˜´å”‡åŠ¨ä½œã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿æ€çš„åŒæ­¥åè°ƒã€‚æ­¤å¤–ï¼ŒSyncTalk++è¿˜æé«˜äº†å¯¹ç¦»ç¾¤éŸ³é¢‘çš„ç¨³å¥æ€§ï¼Œå¹¶é€šè¿‡è¡¨è¾¾ç”Ÿæˆå™¨å’Œèº¯å¹²æ¢å¤å™¨ç­‰æŠ€æœ¯ï¼Œç”Ÿæˆä¸è¯­éŸ³åŒ¹é…çš„é¢éƒ¨è¡¨æƒ…å’Œæ— ç¼çš„èº¯å¹²åŒºåŸŸã€‚SyncTalk++æé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œè§†é¢‘è´¨é‡ï¼Œåœ¨åŒæ­¥å’Œé€¼çœŸåº¦æ–¹é¢è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ã€‚å»ºè®®è§‚çœ‹è¡¥å……è§†é¢‘ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncTalk++æŠ€æœ¯æ—¨åœ¨åˆæˆé«˜åº¦åŒæ­¥çš„ã€ä»¥è¯­éŸ³é©±åŠ¨çš„è°ˆè¯è§†é¢‘ï¼Œç¡®ä¿ä¸»ä½“èº«ä»½ã€å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…çš„åŒæ­¥åè°ƒã€‚</li>
<li>åŒ…å«åŠ¨æ€è‚–åƒæ¸²æŸ“å™¨ï¼Œé‡‡ç”¨é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ä»¥ç¡®ä¿ä¸€è‡´çš„ä¸»ä½“èº«ä»½ä¿ç•™ã€‚</li>
<li>å¼•å…¥é¢éƒ¨åŒæ­¥æ§åˆ¶å™¨ï¼Œå°†å˜´å”‡åŠ¨ä½œä¸è¯­éŸ³å¯¹é½ï¼Œå¹¶ä½¿ç”¨3Dé¢éƒ¨blendshapeæ¨¡å‹é‡å»ºå‡†ç¡®çš„é¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>æå‡ºå¤´éƒ¨åŒæ­¥ç¨³å®šå™¨ï¼Œä¼˜åŒ–å¤´éƒ¨å§¿æ€ä»¥å®ç°æ›´å¤§çš„ç¨³å®šæ€§ã€‚</li>
<li>SyncTalk++é€šè¿‡è¡¨è¾¾ç”Ÿæˆå™¨å’Œèº¯å¹²æ¢å¤å™¨ç­‰æŠ€æœ¯æé«˜äº†å¯¹ç¦»ç¾¤éŸ³é¢‘çš„ç¨³å¥æ€§ï¼Œå¹¶ç”Ÿæˆä¸è¯­éŸ³åŒ¹é…çš„é¢éƒ¨è¡¨æƒ…å’Œæ— ç¼çš„èº¯å¹²åŒºåŸŸã€‚</li>
<li>SyncTalk++å¯ä»¥ç»´æŒè·¨å¸§çš„è§†è§‰ç»†èŠ‚çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ï¼Œå¹¶æé«˜æ¸²æŸ“é€Ÿåº¦å’Œè§†é¢‘è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-15c66b633743e877bdd3801913d82173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037c4b1b81c911d27bb468c64ce77801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c30f4d6defa9eea186e6506bc2cf0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff62d47e48bac53ae737a1a6efbdfb97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a0490a95147fedee062bab74ef0f2b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Investigation-of-Zero-shot-Text-to-Speech-Models-for-Enhancing-Short-Utterance-Speaker-Verification"><a href="#Investigation-of-Zero-shot-Text-to-Speech-Models-for-Enhancing-Short-Utterance-Speaker-Verification" class="headerlink" title="Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification"></a>Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification</h2><p><strong>Authors:Yiyang Zhao, Shuai Wang, Guangzhi Sun, Zehua Chen, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</strong></p>
<p>Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research. </p>
<blockquote>
<p>çŸ­æ—¶é•¿è¯´è¯äººéªŒè¯é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œç”±äºç®€çŸ­è¯­éŸ³ç‰‡æ®µä¸­çš„ä¿¡æ¯æœ‰é™ï¼Œè¿™å¯èƒ½ä¼šé™ä½å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æœ€è¿‘ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆZS-TTSï¼‰ç³»ç»Ÿåœ¨ä¿æŒè¯´è¯äººèº«ä»½æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†ZS-TTSç³»ç»Ÿåœ¨æµ‹è¯•æ—¶é—´æ•°æ®å¢å¼ºåœ¨è¯´è¯äººéªŒè¯ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨VoxCeleb 1æ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸‰ç§æœ€å…ˆè¿›çš„é¢„è®­ç»ƒZS-TTSç³»ç»Ÿï¼ŒåŒ…æ‹¬NatureSpeech 3ã€CosyVoiceå’ŒMaskGCTã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆçœŸå®å’Œåˆæˆè¯­éŸ³æ ·æœ¬åœ¨æ‰€æœ‰æ—¶é•¿ä¸Šå¯¼è‡´ç›¸å¯¹ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰é™ä½äº†10%-16%ï¼Œå¯¹äºçŸ­æ—¶é•¿è¯­éŸ³çš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œä¸”æ— éœ€å¯¹ä»»ä½•ç°æœ‰ç³»ç»Ÿè¿›è¡Œå†è®­ç»ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¾ƒé•¿çš„åˆæˆè¯­éŸ³åœ¨é™ä½EERæ–¹é¢å¹¶æ²¡æœ‰å¸¦æ¥ä¸è¾ƒé•¿çœŸå®è¯­éŸ³ç›¸åŒçš„ç›Šå¤„ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä½¿ç”¨ZS-TTSè¿›è¡Œæµ‹è¯•æ—¶é—´è¯´è¯äººéªŒè¯çš„æ½œåŠ›å’ŒæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆZS-TTSï¼‰ç³»ç»Ÿåœ¨æµ‹è¯•æ—¶é—´æ•°æ®å¢å¼ºä¸­çš„é¦–æ¬¡åº”ç”¨åœ¨çŸ­è®²è¯éªŒè¯ä¸­çš„åº”ç”¨ã€‚åœ¨VoxCeleb 1æ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸‰ä¸ªæœ€æ–°çš„é¢„è®­ç»ƒZS-TTSç³»ç»Ÿï¼ˆNatureSpeech 3ã€CosyVoiceå’ŒMaskGCTï¼‰ã€‚ç»“åˆçœŸå®å’Œåˆæˆè¯­éŸ³æ ·æœ¬åï¼Œç›¸å¯¹ç­‰è¯¯ç‡ï¼ˆEERï¼‰åœ¨æ‰€æœ‰æ—¶é•¿ä¸Šé™ä½äº†10%-16%ï¼Œç‰¹åˆ«æ˜¯çŸ­è¯­çš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œä¸”æ— éœ€å¯¹ç°æœ‰ç³»ç»Ÿè¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œåˆ†æè¡¨æ˜ï¼Œè¾ƒé•¿çš„åˆæˆè¯­éŸ³åœ¨é™ä½EERæ–¹é¢å¹¶æœªäº§ç”Ÿä¸è¾ƒé•¿çœŸå®è¯­éŸ³ç›¸åŒçš„å¥½å¤„ã€‚è¯¥ç ”ç©¶çªå‡ºäº†ä½¿ç”¨ZS-TTSåœ¨æµ‹è¯•æ—¶é—´è¯´è¯äººéªŒè¯ä¸­çš„æ½œåŠ›å’ŒæŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ­è¯­è¯´è¯äººéªŒè¯é¢ä¸´ç”±äºä¿¡æ¯æœ‰é™è€Œå¯¼è‡´çš„å‡†ç¡®æ€§å’Œå¯é æ€§é—®é¢˜ã€‚</li>
<li>é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆZS-TTSï¼‰ç³»ç»Ÿåœ¨ä¿ç•™è¯´è¯äººèº«ä»½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†å°†ZS-TTSç³»ç»Ÿåº”ç”¨äºæµ‹è¯•æ—¶é—´æ•°æ®å¢å¼ºåœ¨è¯´è¯äººéªŒè¯ä¸­çš„æ½œåŠ›ã€‚</li>
<li>åœ¨VoxCeleb 1æ•°æ®é›†ä¸Šè¯„ä¼°çš„ä¸‰ä¸ªé¢„è®­ç»ƒZS-TTSç³»ç»Ÿæ˜¾ç¤ºï¼Œç»“åˆçœŸå®å’Œåˆæˆè¯­éŸ³æ ·æœ¬å¯ä»¥é™ä½ç­‰è¯¯ç‡ï¼ˆEERï¼‰ã€‚</li>
<li>åˆæˆè¯­éŸ³åœ¨çŸ­è®²è¯éªŒè¯ä¸­çš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œä¸”æ— éœ€å¯¹ç°æœ‰ç³»ç»Ÿè¿›è¡Œé‡æ–°è®­ç»ƒã€‚</li>
<li>è¾ƒé•¿çš„åˆæˆè¯­éŸ³åœ¨é™ä½EERæ–¹é¢å¹¶æœªäº§ç”Ÿä¸è¾ƒé•¿çœŸå®è¯­éŸ³ç›¸åŒçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf630bdfad1b39c38b5704284feaf182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0256bd1c03b59d535cb3b3a488529cf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47e17273cb8b2824416b560dc49e469e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40c944d44b370ec97fbe32fd18147d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5785249a9d34d296c0bd728b4543513.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SpeechRefiner-Towards-Perceptual-Quality-Refinement-for-Front-End-Algorithms"><a href="#SpeechRefiner-Towards-Perceptual-Quality-Refinement-for-Front-End-Algorithms" class="headerlink" title="SpeechRefiner: Towards Perceptual Quality Refinement for Front-End   Algorithms"></a>SpeechRefiner: Towards Perceptual Quality Refinement for Front-End   Algorithms</h2><p><strong>Authors:Sirui Li, Shuai Wang, Zhijun Liu, Zhongjie Jiang, Yannan Wang, Haizhou Li</strong></p>
<p>Speech pre-processing techniques such as denoising, de-reverberation, and separation, are commonly employed as front-ends for various downstream speech processing tasks. However, these methods can sometimes be inadequate, resulting in residual noise or the introduction of new artifacts. Such deficiencies are typically not captured by metrics like SI-SNR but are noticeable to human listeners. To address this, we introduce SpeechRefiner, a post-processing tool that utilizes Conditional Flow Matching (CFM) to improve the perceptual quality of speech. In this study, we benchmark SpeechRefiner against recent task-specific refinement methods and evaluate its performance within our internal processing pipeline, which integrates multiple front-end algorithms. Experiments show that SpeechRefiner exhibits strong generalization across diverse impairment sources, significantly enhancing speech perceptual quality. Audio demos can be found at <a target="_blank" rel="noopener" href="https://speechrefiner.github.io/SpeechRefiner/">https://speechrefiner.github.io/SpeechRefiner/</a>. </p>
<blockquote>
<p>è¯­éŸ³é¢„å¤„ç†æŠ€æœ¯ï¼Œå¦‚é™å™ªã€å»æ··å“å’Œåˆ†ç¦»ç­‰ï¼Œé€šå¸¸è¢«ç”¨ä½œå„ç§ä¸‹æ¸¸è¯­éŸ³å¤„ç†ä»»åŠ¡çš„å‰ç«¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœ‰æ—¶å¯èƒ½ä¸å¤Ÿå……åˆ†ï¼Œå¯¼è‡´æ®‹ç•™å™ªå£°æˆ–å¼•å…¥æ–°çš„å¤±çœŸã€‚è¿™äº›ç¼ºé™·é€šå¸¸ä¸ä¼šè¢«SI-SNRç­‰æŒ‡æ ‡æ‰€æ•è·ï¼Œä½†å¯¹äºäººç±»å¬ä¼—æ¥è¯´æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRefinerï¼Œè¿™æ˜¯ä¸€ç§åå¤„ç†å·¥å…·ï¼Œå®ƒåˆ©ç”¨æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰æ¥æé«˜è¯­éŸ³çš„æ„ŸçŸ¥è´¨é‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†SpeechRefinerä¸æœ€æ–°çš„ç‰¹å®šä»»åŠ¡ç»†åŒ–æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„é›†æˆå¤šä¸ªå‰ç«¯ç®—æ³•çš„å†…éƒ¨å¤„ç†ç®¡é“ä¸­è¯„ä¼°äº†å…¶æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSpeechRefineråœ¨ä¸åŒæ¥æºçš„ç¼ºé™·ä¸Šå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†è¯­éŸ³æ„ŸçŸ¥è´¨é‡ã€‚éŸ³é¢‘æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://speechrefiner.github.io/SpeechRefiner/%E6%89%BE%E5%88%B0%E3%80%82">https://speechrefiner.github.io/SpeechRefiner/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13709v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong>ï¼šä¸ºæé«˜è¯­éŸ³æ„ŸçŸ¥è´¨é‡ï¼Œæ¨å‡ºäº†ä¸€ç§åä¸ºSpeechRefinerçš„åå¤„ç†å·¥å…·ï¼Œé‡‡ç”¨æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰æŠ€æœ¯ï¼Œé’ˆå¯¹è¯­éŸ³é¢„å¤„ç†æŠ€æœ¯å¦‚é™å™ªã€å»æ··å“å’Œåˆ†ç¦»ç­‰å­˜åœ¨çš„ç¼ºé™·è¿›è¡Œæ”¹å–„ã€‚ç›¸è¾ƒäºè¿‘æœŸç‰¹å®šçš„ä»»åŠ¡ä¼˜åŒ–æ–¹æ³•ï¼ŒSpeechRefineråœ¨å†…éƒ¨å¤„ç†ç®¡é“ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åº”å¯¹å¤šç§ä¸åŒçš„æŸä¼¤æ¥æºï¼Œæ˜¾è‘—æå‡è¯­éŸ³æ„ŸçŸ¥è´¨é‡ã€‚è¯¦æƒ…å¯è®¿é—®<a target="_blank" rel="noopener" href="https://speechrefiner.github.io/SpeechRefiner/">é“¾æ¥</a>äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³é¢„å¤„ç†æŠ€æœ¯å¦‚é™å™ªã€å»æ··å“å’Œåˆ†ç¦»ç­‰æ˜¯ä¸‹æ¸¸è¯­éŸ³å¤„ç†ä»»åŠ¡çš„å‰ç«¯ï¼Œä½†å­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½äº§ç”Ÿæ®‹ä½™å™ªå£°æˆ–æ–°å¼•å…¥çš„ä¼ªè¿¹ã€‚</li>
<li>SpeechRefineræ˜¯ä¸€ç§åå¤„ç†å·¥å…·ï¼Œé‡‡ç”¨æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰æŠ€æœ¯æ”¹å–„è¯­éŸ³æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>SpeechRefinerç›¸è¾ƒäºè¿‘æœŸç‰¹å®šçš„ä»»åŠ¡ä¼˜åŒ–æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SpeechRefinerèƒ½æœ‰æ•ˆåº”å¯¹å¤šç§ä¸åŒçš„è¯­éŸ³æŸä¼¤æ¥æºã€‚</li>
<li>SpeechRefinerèƒ½æ˜¾è‘—æå‡è¯­éŸ³æ„ŸçŸ¥è´¨é‡ï¼Œå…¶æ•ˆæœå¯é€šè¿‡éŸ³é¢‘æ¼”ç¤ºè¿›è¡Œå±•ç¤ºã€‚</li>
<li>SpeechRefinerå·¥å…·çš„å†…éƒ¨å¤„ç†ç®¡é“é›†æˆäº†å¤šç§å‰ç«¯ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d3c15e194272cc3905112f6c2227694.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-659f9c86a44e4ad269b33433bffc369e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f588b44fdcf58117cfb4a3c426c37b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d2b2fcbe075f4b82e53bf59a19514b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems"><a href="#Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems" class="headerlink" title="Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems"></a>Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems</h2><p><strong>Authors:Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran</strong></p>
<p>This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER&#x2F;CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬ä¸ºMLC-SLMæŒ‘æˆ˜2025è®¾è®¡çš„ç³»ç»Ÿï¼Œä¸»è¦ä¾§é‡äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œè¯­è¨€å»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»è¿‡å¾®è°ƒåçš„Whisper-large-v3ç¼–ç å™¨ã€é«˜æ•ˆçš„æŠ•å½±ä»ªæ¶æ„å’Œå„ç§è§£ç å™¨é…ç½®ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥ä¼˜åŒ–ç¼–ç å™¨ã€æŠ•å½±ä»ªå’ŒLLMç»„ä»¶ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä½¿ç”¨ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹Gemma3-12Bå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹³å‡å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰&#x2F;å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º16.63%ï¼Œä½¿ç”¨Qwen2.5-7Bæ—¶çš„æ€§èƒ½ä¸º18.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13596v1">PDF</a> Technical report for Interspeech 2025 MLC-SLM Challenge</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹MLC-SLMæŒ‘æˆ˜2025çš„ç³»ç»Ÿï¼Œé‡ç‚¹ç ”ç©¶å¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œè¯­è¨€å»ºæ¨¡ã€‚æœ¬ç³»ç»Ÿç»“åˆäº†å¾®è°ƒè¿‡çš„Whisper-large-v3ç¼–ç å™¨ã€é«˜æ•ˆæŠ•å½±æ¶æ„ä»¥åŠå¤šç§è§£ç å™¨é…ç½®ã€‚é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥ä¼˜åŒ–ç¼–ç å™¨ã€æŠ•å½±å™¨å’Œå¤§æ¨¡å‹ç»„ä»¶ã€‚ç³»ç»Ÿåœ¨ä»…ä½¿ç”¨è§£ç å™¨è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œä½¿ç”¨Gemma3-12Bæ¨¡å‹æ—¶çš„ç§æœ‰æµ‹è¯•å¹³å‡WER&#x2F;CERç»“æœä¸º16.63%ï¼Œä½¿ç”¨Qwen2.5-7Bæ¨¡å‹æ—¶ä¸º18.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿé’ˆå¯¹MLC-SLMæŒ‘æˆ˜ï¼Œç ”ç©¶å¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œè¯­è¨€å»ºæ¨¡ã€‚</li>
<li>ç»“åˆå¾®è°ƒè¿‡çš„ç¼–ç å™¨ã€é«˜æ•ˆæŠ•å½±æ¶æ„å’Œå¤šç§è§£ç å™¨é…ç½®ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ä¼˜åŒ–ç³»ç»Ÿç»„ä»¶ã€‚</li>
<li>ç³»ç»Ÿä½¿ç”¨è§£ç å™¨è¯­è¨€æ¨¡å‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä½¿ç”¨Gemma3-12Bæ¨¡å‹æ—¶ï¼Œç§æœ‰æµ‹è¯•å¹³å‡WER&#x2F;CERç»“æœä¸º16.63%ã€‚</li>
<li>ä½¿ç”¨Qwen2.5-7Bæ¨¡å‹æ—¶ï¼Œæ€§èƒ½è¡¨ç°ç•¥æœ‰ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67c7f29208e5db1d3189c55444a8a5a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57bccb9c9e9ac3d118a4f499a1755bb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b1454b2becbe3a864b2539edd863f8e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BUT-System-for-the-MLC-SLM-Challenge"><a href="#BUT-System-for-the-MLC-SLM-Challenge" class="headerlink" title="BUT System for the MLC-SLM Challenge"></a>BUT System for the MLC-SLM Challenge</h2><p><strong>Authors:Alexander Polok, Jiangyu Han, Dominik Klement, Samuele Cornell, Jan ÄŒernockÃ½, LukÃ¡Å¡ Burget</strong></p>
<p>We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW â€“ a diarization-conditioned variant of Whisper â€“ with DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate both systems in out-of-domain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whisperâ€™s multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves a micro-average tcpWER&#x2F;CER of 16.75% and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training data â€“ such as missing speech segments and incorrect silence annotations â€“ which can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆDiCoWï¼ˆä¸€ç§åŸºäºWhisperçš„è¯´è¯äººåˆ†å‰²æ¡ä»¶å˜ä½“ï¼‰å’ŒDiariZenï¼ˆä¸€ç§åŸºäºPyannoteçš„è¯´è¯äººåˆ†å‰²ç®¡é“ï¼‰çš„åŒäººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æ²¡æœ‰è¿›è¡Œä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è·¨é¢†åŸŸï¼ˆOODï¼‰å¤šè¯­è¨€åœºæ™¯ä¸­å¯¹ä¸¤ä¸ªç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒDiariZenå§‹ç»ˆä¼˜äºåŸºå‡†Pyannoteè¯´è¯äººåˆ†å‰²æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡DiCoWæ˜¯é’ˆå¯¹ç›®æ ‡è¯´è¯äººASRçš„è‹±è¯­å”¯ä¸€æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä½†å®ƒä»å…·æœ‰å¼ºå¤§çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œè¡¨æ˜ç¼–ç å™¨ä¿®æ”¹ä¿ç•™äº†Whisperçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚ç„¶åæˆ‘ä»¬å¯¹DiCoWå’ŒDiariZenè¿›è¡Œäº†MLC-SLMæŒ‘æˆ˜æ•°æ®çš„å¾®è°ƒã€‚ç»è¿‡å¾®è°ƒåï¼ŒDiariZenç»§ç»­è¶…è¶Šç»è¿‡å¾®è°ƒPyannoteåŸºå‡†çº¿ï¼Œè€ŒDiCoWä»åŸŸé€‚åº”ä¸­çœ‹åˆ°äº†è¿›ä¸€æ­¥çš„æ”¶ç›Šã€‚æˆ‘ä»¬çš„æœ€ç»ˆç³»ç»Ÿè¾¾åˆ°äº†16.75%çš„å¾®è§‚å¹³å‡tcpWER&#x2F;CERï¼Œå¹¶åœ¨MLC-SLMæŒ‘æˆ˜çš„ç¬¬äºŒé¡¹ä»»åŠ¡ä¸­è·å¾—äº†ç¬¬äºŒåã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°äº†è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨è‹¥å¹²æ ‡ç­¾ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¦‚ç¼ºå¤±è¯­éŸ³ç‰‡æ®µå’Œé”™è¯¯çš„é™éŸ³æ³¨é‡Šç­‰ï¼Œè¿™äº›é—®é¢˜å¯èƒ½ä¼šé˜»ç¢è¯´è¯äººåˆ†å‰²çš„å¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€äº›ç®€å•çš„ç¼“è§£ç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜å¹¶æé«˜ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆDiCoWå’ŒDiariZençš„ä¸¤äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚åœ¨è·¨é¢†åŸŸå¤šè¯­è¨€åœºæ™¯ä¸‹ï¼ŒDiariZenè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºåŸºçº¿Pyannoteæ¨¡å‹æœ‰æ›´å¥½çš„è¡¨ç°ã€‚DiCoWåœ¨ä¸éœ€è¦é’ˆå¯¹ç›®æ ‡è¯´è¯è€…ASRè¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒå¼ºå¤§çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚åœ¨MLC-SLMæŒ‘æˆ˜æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒåï¼ŒDiariZenç»§ç»­ä¼˜äºç»è¿‡è°ƒæ•´çš„Pyannoteæ¨¡å‹ï¼Œè€ŒDiCoWä»é¢†åŸŸé€‚åº”ä¸­è·å¾—äº†è¿›ä¸€æ­¥çš„æå‡ã€‚æœ€ç»ˆç³»ç»Ÿå–å¾—äº†ç¬¬äºŒåçš„æˆç»©ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æŒ‡å‡ºäº†è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„ä¸€äº›æ ‡æ³¨ä¸ä¸€è‡´é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç®€å•çš„ç¼“è§£ç­–ç•¥æ¥æé«˜ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç»“åˆDiCoWå’ŒDiariZençš„ä¸¤äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚</li>
<li>DiariZenåœ¨è·¨é¢†åŸŸå¤šè¯­è¨€åœºæ™¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºåŸºçº¿Pyannoteæ¨¡å‹ã€‚</li>
<li>DiCoWå…·æœ‰å¼ºå¤§çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œåœ¨ä¸éœ€è¦é’ˆå¯¹ç›®æ ‡è¯´è¯è€…ASRè¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ä»èƒ½è‰¯å¥½è¿è¡Œã€‚</li>
<li>ç»è¿‡åœ¨MLC-SLMæŒ‘æˆ˜æ•°æ®ä¸Šçš„å¾®è°ƒï¼ŒDiariZenç»§ç»­ä¼˜äºPyannoteæ¨¡å‹ï¼Œè€ŒDiCoWè·å¾—äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚</li>
<li>æœ€ç»ˆç³»ç»Ÿå–å¾—äº†ç¬¬äºŒåçš„å¥½æˆç»©ã€‚</li>
<li>æ–‡ä¸­æŒ‡å‡ºäº†è®­ç»ƒæ•°æ®å­˜åœ¨çš„æ ‡æ³¨ä¸ä¸€è‡´é—®é¢˜ï¼Œå¦‚ç¼ºå¤±è¯­éŸ³æ®µå’Œé”™è¯¯çš„é™éŸ³æ³¨é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d961b0ed84d90b59a8840fc58c5ec1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0751c4778b7ae3e6b4ea2e41d7ed4de3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e8b187c05f575212dad0760b4de00b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-333edc45a39f4c88949319aaf5c1c8ba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR"><a href="#Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR" class="headerlink" title="Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR"></a>Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR</h2><p><strong>Authors:Yizhou Peng, Hexin Liu, Eng Siong Chng</strong></p>
<p>This paper introduces the integration of language-specific bi-directional context into a speech large language model (SLLM) to improve multilingual continuous conversational automatic speech recognition (ASR). We propose a character-level contextual masking strategy during training, which randomly removes portions of the context to enhance robustness and better emulate the flawed transcriptions that may occur during inference. For decoding, a two-stage pipeline is utilized: initial isolated segment decoding followed by context-aware re-decoding using neighboring hypotheses. Evaluated on the 1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM) corpus covering eleven languages, our method achieves an 18% relative improvement compared to a strong baseline, outperforming even the model trained on 6000 hours of data for the MLC-SLM competition. These results underscore the significant benefit of incorporating contextual information in multilingual continuous conversational ASR. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å°†ç‰¹å®šè¯­è¨€çš„åŒå‘ä¸Šä¸‹æ–‡é›†æˆåˆ°è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ä¸­ï¼Œä»¥æé«˜è·¨è¯­è¨€è¿ç»­å¯¹è¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å­—ç¬¦çº§ä¸Šä¸‹æ–‡æ©ç ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºç§»é™¤éƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§å¹¶æ›´å¥½åœ°æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„é”™è¯¯è½¬å½•ã€‚åœ¨è§£ç æ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆæ˜¯åˆæ­¥çš„ç‹¬ç«‹åˆ†æ®µè§£ç ï¼Œç„¶åæ˜¯ä½¿ç”¨ç›¸é‚»å‡è®¾è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‡æ–°è§£ç ã€‚åœ¨æ¶µç›–åä¸€ç§è¯­è¨€çš„1500å°æ—¶å¤šè¯­è¨€å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼ˆMLC-SLMï¼‰è¯­æ–™åº“ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œå®ç°äº†18%çš„ç›¸å¯¹æ”¹è¿›ï¼Œç”šè‡³è¶…è¶Šäº†ä¸ºMLC-SLMç«èµ›è®­ç»ƒçš„6000å°æ—¶æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨å¤šè¯­è¨€è¿ç»­å¯¹è¯ASRä¸­åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡å¤§ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13396v1">PDF</a> Submitted to Interspeech 2025 MLC-SLM workshop as a Research Paper</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†è¯­è¨€ç‰¹å®šåŒå‘ä¸Šä¸‹æ–‡é›†æˆåˆ°è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ä¸­çš„æ–¹æ³•ï¼Œä»¥æ”¹è¿›å¤šè¯­ç§è¿ç»­å¯¹è¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­—ç¬¦çº§ä¸Šä¸‹æ–‡æ©ç ç­–ç•¥ï¼Œéšæœºç§»é™¤éƒ¨åˆ†ä¸Šä¸‹æ–‡ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶æ›´å¥½åœ°æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„é”™è¯¯è½¬å½•ã€‚è§£ç é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šå…ˆè¿›è¡Œåˆæ­¥çš„ç‹¬ç«‹åˆ†æ®µè§£ç ï¼Œç„¶åä½¿ç”¨ç›¸é‚»å‡è®¾è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†è§£ç ã€‚åœ¨æ¶µç›–11ç§è¯­è¨€çš„Multilingual Conversational Speech and Language Modelï¼ˆMLC-SLMï¼‰è¯­æ–™åº“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºè¡¨ç°è‰¯å¥½çš„åŸºçº¿æ¨¡å‹å®ç°äº†ç›¸å¯¹æ”¹è¿›ç‡18%ï¼Œç”šè‡³åœ¨MLC-SLMç«èµ›ä¸­è¡¨ç°ä¼˜äºç»è¿‡6000å°æ—¶æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åœ¨å¤šè¯­ç§è¿ç»­å¯¹è¯ASRä¸­èå…¥ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼•å…¥è¯­è¨€ç‰¹å®šåŒå‘ä¸Šä¸‹æ–‡åˆ°è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ—¨åœ¨æ”¹è¿›å¤šè¯­ç§è¿ç»­å¯¹è¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å­—ç¬¦çº§ä¸Šä¸‹æ–‡æ©ç ç­–ç•¥ï¼Œéšæœºç§»é™¤éƒ¨åˆ†ä¸Šä¸‹æ–‡ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„é”™è¯¯è½¬å½•ï¼Œæé«˜æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè§£ç æµç¨‹ï¼šåˆæ­¥ç‹¬ç«‹åˆ†æ®µè§£ç åï¼Œè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†è§£ç ã€‚</li>
<li>åœ¨MLC-SLMè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—çš„ç›¸å¯¹æ”¹è¿›ç‡ã€‚</li>
<li>ä¸ç»è¿‡æ›´å¤šæ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè¯­ç§è¿ç»­å¯¹è¯ASRä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-70f64c60e4bba32d746f5eb09692a8d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a8daa33f63b024de5dc584119aca715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad174257bce8b6b417541d9d214da64e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334378040cf240dde107c32dc4077074.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f310ab047a401019abe0923b9cbadf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab894683560554c8e9dfd01b5ca6e9b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025"><a href="#NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025" class="headerlink" title="NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025"></a>NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025</h2><p><strong>Authors:Yizhou Peng, Bin Wang, Yi-Wen Chao, Ziyang Ma, Haoyang Zhang, Hexin Liu, Xie Chen, Eng Siong Chng</strong></p>
<p>This report details the NTU Speechlab system developed for the Interspeech 2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge (Task I), where we achieved 5th place. We present comprehensive analyses of our multilingual automatic speech recognition system, highlighting key advancements in model architecture, data selection, and training strategies. In particular, language-specific prompts and model averaging techniques were instrumental in boosting system performance across diverse languages. Compared to the initial baseline system, our final model reduced the average Mix Error Rate from 20.2% to 10.6%, representing an absolute improvement of 9.6% (a relative improvement of 48%) on the evaluation set. Our results demonstrate the effectiveness of our approach and offer practical insights for future Speech Large Language Models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†ä¸ºInterspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼ˆMLC-SLMï¼‰æŒ‘æˆ˜èµ›ï¼ˆä»»åŠ¡ä¸€ï¼‰å¼€å‘çš„NTU Speechlabç³»ç»Ÿï¼Œæˆ‘ä»¬åœ¨æ¯”èµ›ä¸­å–å¾—äº†ç¬¬äº”åã€‚æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œé‡ç‚¹ä»‹ç»äº†æ¨¡å‹æ¶æ„ã€æ•°æ®é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥æ–¹é¢çš„å…³é”®è¿›å±•ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯­è¨€ç‰¹å®šçš„æç¤ºå’Œæ¨¡å‹å¹³å‡æŠ€æœ¯å¯¹æé«˜ç³»ç»Ÿåœ¨å¤šç§è¯­è¨€ä¸­çš„æ€§èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚ä¸åˆå§‹åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹å°†å¹³å‡æ··åˆé”™è¯¯ç‡ä»20.2%é™ä½åˆ°10.6%ï¼Œåœ¨è¯„ä¼°é›†ä¸Šå®ç°äº†ç»å¯¹æ”¹è¿›9.6%ï¼ˆç›¸å¯¹æ”¹è¿›48%ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹æä¾›äº†å®é™…è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13339v1">PDF</a> Submitted to Interspeech 2025 MLC-SLM challenge (5th place). System   report</p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€ä»½å…³äºNTU Speechlabç³»ç»Ÿä¸ºInterspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼ˆMLC-SLMï¼‰æŒ‘æˆ˜ä»»åŠ¡è®¾è®¡çš„è¯¦ç»†æŠ¥å‘Šã€‚æœ¬æŠ¥å‘Šå…¨é¢åˆ†æäº†æˆ‘ä»¬çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„ä¸»è¦è¿›æ­¥ï¼Œå¦‚æ¨¡å‹æ¶æ„ã€æ•°æ®é€‰æ‹©åŠè®­ç»ƒç­–ç•¥ç­‰ã€‚ä¸åˆå§‹åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œæœ€ç»ˆæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå°†å¹³å‡æ··åˆé”™è¯¯ç‡é™ä½äº†è¿‘ä¸€åŠï¼Œè¿™ä½“ç°äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®ƒä¸ºæœªæ¥å¤§å‹è¯­è¨€æ¨¡å‹çš„æ„å»ºæä¾›äº†å®ç”¨çš„è§è§£ã€‚è¯¥ç³»ç»ŸæˆåŠŸåœ¨æ¯”èµ›ä¸­å–å¾—äº†ç¬¬äº”åçš„å¥½æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬å†…å®¹çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>NTU Speechlabç³»ç»Ÿé’ˆå¯¹Interspeech 2025çš„å¤šè¯­ç§å¯¹è¯è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹æŒ‘æˆ˜è¿›è¡Œäº†ç³»ç»Ÿè®¾è®¡ã€‚</li>
<li>ç³»ç»Ÿåˆ†ææ¶µç›–äº†æ¨¡å‹æ¶æ„ã€æ•°æ®é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥ç­‰é‡è¦æ–¹é¢ã€‚</li>
<li>è¯­è¨€ç‰¹å®šæç¤ºå’Œæ¨¡å‹å¹³å‡æŠ€æœ¯å¯¹äºæé«˜å¤šè¯­ç§ç³»ç»Ÿçš„æ€§èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>ä¸åˆå§‹åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œæœ€ç»ˆæ¨¡å‹çš„å¹³å‡æ··åˆé”™è¯¯ç‡é™ä½äº†9.6%ï¼Œç›¸å¯¹æ”¹å–„ç‡ä¸º48%ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨æ¯”èµ›ä¸­å–å¾—äº†ç¬¬äº”åçš„å¥½æˆç»©ã€‚</li>
<li>æ­¤æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸ºæœªæ¥çš„è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºæä¾›äº†å®è·µæŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a73e8debaa3054eeedc8dbf0a7f8915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ecc8b2b371ccb62d563a94527a98f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a18d47d7e6084887dd915f1515a91f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a8bac7ecc1887d23adeb1ac8ec6f313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bbab8169e4838e32d244cc729207270.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d61636e3923645158f9918dc9dfe00a3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Seewoâ€™s-Submission-to-MLC-SLM-Lessons-learned-from-Speech-Reasoning-Language-Models"><a href="#Seewoâ€™s-Submission-to-MLC-SLM-Lessons-learned-from-Speech-Reasoning-Language-Models" class="headerlink" title="Seewoâ€™s Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models"></a>Seewoâ€™s Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models</h2><p><strong>Authors:Bo Li, Chengben Xu, Wufeng Zhang</strong></p>
<p>This paper presents Seewoâ€™s systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER&#x2F;CER of 11.57% for Track 1 and a tcpWER&#x2F;tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Seewoåœ¨å¤šå…ƒè¯­è¨€å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜ï¼ˆMLC-SLMï¼‰çš„ä¸¤ä¸ªèµ›é“ä¸­çš„ç³»ç»Ÿï¼Œæ¶µç›–äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¸¦æœ‰ASRçš„è¯´è¯äººæ—¥è®°åŒ–ï¼ˆSD-ASRï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œè¯¥æµç¨‹åœ¨è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­æ˜¾å¼å¢å¼ºæ¨ç†å’Œè‡ªæˆ‘æ ¡æ­£åŠŸèƒ½ï¼Œä»¥ç”¨äºASRã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¯¾ç¨‹å­¦ä¹ ä»¥é€æ­¥è·å–èƒ½åŠ›ã€æ€ç»´é“¾æ•°æ®å¢å¼ºä»¥ä¿ƒè¿›ä¸­é—´åæ€ï¼Œä»¥åŠå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ï¼Œé€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–è¿›ä¸€æ­¥æ”¹è¿›è‡ªæˆ‘æ ¡æ­£ã€‚è¯¥æ–¹æ³•åœ¨å®˜æ–¹æŒ‘æˆ˜åŸºçº¿çš„åŸºç¡€ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ã€‚åœ¨è¯„ä¼°é›†ä¸Šï¼Œæˆ‘ä»¬æœ€å¥½çš„ç³»ç»Ÿåœ¨ç¬¬1èµ›é“ä¸Šè¾¾åˆ°äº†11.57%çš„WER&#x2F;CERï¼Œåœ¨ç¬¬2èµ›é“ä¸Šè¾¾åˆ°äº†17.67%çš„tcpWER&#x2F;tcpCERã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¯æ˜äº†æ¯ä¸ªç»„ä»¶åœ¨æŒ‘æˆ˜çº¦æŸä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13300v3">PDF</a> </p>
<p><strong>Summary</strong>:<br>æœ¬æ–‡ä»‹ç»äº†Seewoåœ¨Multilingual Conversational Speech Language Model Challengeï¼ˆMLC-SLMï¼‰ä¸­çš„ä¸¤ä¸ªèµ›é“ï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¸¦æœ‰ASRçš„è¯´è¯äººåˆ†å—åŒ–ï¼ˆSD-ASRï¼‰ï¼‰çš„ç³»ç»Ÿã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æé«˜è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†å’Œè‡ªçº é”™èƒ½åŠ›ã€‚ç»“åˆè¯¾ç¨‹å­¦ä¹ å®ç°æ¸è¿›èƒ½åŠ›è·å–ï¼Œåˆ©ç”¨Chain-of-Thoughtæ•°æ®è¿›è¡Œå¢å¼ºä»¥æ¨åŠ¨ä¸­é—´é˜¶æ®µçš„åæ€ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›ä¸€æ­¥é€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–è¿›è¡Œè‡ªçº é”™ã€‚æ­¤æ–¹æ³•ç›¸è¾ƒäºå®˜æ–¹æŒ‘æˆ˜åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ï¼Œæœ€ä½³ç³»ç»Ÿåœ¨è¯„ä¼°é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°Track 1çš„WER&#x2F;CERä¸º11.57%ï¼ŒTrack 2çš„tcpWER&#x2F;tcpCERä¸º17.67%ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¯æ˜äº†åœ¨æŒ‘æˆ˜çº¦æŸä¸‹æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Seewoæå‡ºçš„å¤šé˜¶æ®µè®­ç»ƒç®¡é“ç”¨äºå¢å¼ºè¯­éŸ³è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œè‡ªçº é”™èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è¯¾ç¨‹å­¦ä¹ å®ç°æ¸è¿›èƒ½åŠ›è·å–ã€‚</li>
<li>é‡‡ç”¨Chain-of-Thoughtæ•°æ®å¢å¼ºæ¨åŠ¨ä¸­é—´é˜¶æ®µåæ€ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç”¨äºä¼˜åŒ–è‡ªçº é”™èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜ä¸­å®ç°äº†æ˜¾è‘—æˆæœï¼Œè¾ƒå®˜æ–¹åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>æœ€ä½³ç³»ç»Ÿè¡¨ç°ä¼˜å¼‚ï¼ŒTrack 1çš„WER&#x2F;CERä¸º11.57%ï¼ŒTrack 2çš„tcpWER&#x2F;tcpCERä¸º17.67%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4699484f27576657f63c1b56aa36d56f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fec32241e658ccd300ca24eac739ee88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5858e1fab9c34b9b9d8c086975b8b30e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c05416f6933a7297c49f6794654285a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2fc3b4c606bb924858746a864f0df6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420e87a51ff6e652b7bdbf25534b5ce2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="I-2-S-TFCKD-Intra-Inter-Set-Knowledge-Distillation-with-Time-Frequency-Calibration-for-Speech-Enhancement"><a href="#I-2-S-TFCKD-Intra-Inter-Set-Knowledge-Distillation-with-Time-Frequency-Calibration-for-Speech-Enhancement" class="headerlink" title="I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency   Calibration for Speech Enhancement"></a>I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency   Calibration for Speech Enhancement</h2><p><strong>Authors:Jiaming Cheng, Ruiyu Liang, Chao Xu, Ye Ni, Wei Zhou, BjÃ¶rn W. Schuller, Xiaoshuai Hao</strong></p>
<p>In recent years, complexity compression of neural network (NN)-based speech enhancement (SE) models has gradually attracted the attention of researchers, especially in scenarios with limited hardware resources or strict latency requirements. The main difficulties and challenges lie in achieving a balance between complexity and performance according to the characteristics of the task. In this paper, we propose an intra-inter set knowledge distillation (KD) framework with time-frequency calibration (I$^2$S-TFCKD) for SE. Different from previous distillation strategies for SE, the proposed framework fully utilizes the time-frequency differential information of speech while promoting global knowledge flow. Firstly, we propose a multi-layer interactive distillation based on dual-stream time-frequency cross-calibration, which calculates the teacher-student similarity calibration weights in the time and frequency domains respectively and performs cross-weighting, thus enabling refined allocation of distillation contributions across different layers according to speech characteristics. Secondly, we construct a collaborative distillation paradigm for intra-set and inter-set correlations. Within a correlated set, multi-layer teacher-student features are pairwise matched for calibrated distillation. Subsequently, we generate representative features from each correlated set through residual fusion to form the fused feature set that enables inter-set knowledge interaction. The proposed distillation strategy is applied to the dual-path dilated convolutional recurrent network (DPDCRN) that ranked first in the SE track of the L3DAS23 challenge. Objective evaluations demonstrate that the proposed KD strategy consistently and effectively improves the performance of the low-complexity student model and outperforms other distillation schemes. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç¥ç»ç½‘ç»œï¼ˆNNï¼‰åŸºäºçš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹çš„å¤æ‚åº¦å‹ç¼©é€æ¸å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬ä»¶èµ„æºæœ‰é™æˆ–å»¶è¿Ÿè¦æ±‚ä¸¥æ ¼çš„æƒ…å†µä¸‹ã€‚ä¸»è¦çš„å›°éš¾å’ŒæŒ‘æˆ˜åœ¨äºæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹åœ¨å¤æ‚åº¦å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´é¢‘ç‡æ ¡å‡†çš„ç»„å†…ç»„é—´çŸ¥è¯†è’¸é¦ï¼ˆI$^2$S-TFCKDï¼‰æ¡†æ¶ï¼Œç”¨äºè¯­éŸ³å¢å¼ºã€‚ä¸åŒäºä»¥å‰çš„SEçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å……åˆ†åˆ©ç”¨äº†è¯­éŸ³çš„æ—¶é—´é¢‘ç‡å·®å¼‚ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ƒè¿›äº†å…¨å±€çŸ¥è¯†æµã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŒæµæ—¶é—´é¢‘ç‡äº¤å‰æ ¡å‡†çš„å¤šå±‚äº¤äº’è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«åœ¨æ—¶é—´å’Œé¢‘ç‡åŸŸè®¡ç®—æ•™å¸ˆå­¦ç”Ÿç›¸ä¼¼åº¦æ ¡å‡†æƒé‡ï¼Œå¹¶æ‰§è¡Œäº¤å‰åŠ æƒï¼Œä»è€Œèƒ½å¤Ÿæ ¹æ®è¯­éŸ³ç‰¹å¾åœ¨ä¸åŒçš„å±‚ä¹‹é—´ç²¾ç»†åˆ†é…è’¸é¦è´¡çŒ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ„å»ºäº†ç»„å†…å’Œç»„é—´ç›¸å…³æ€§çš„åä½œè’¸é¦æ¨¡å¼ã€‚åœ¨ç›¸å…³é›†åˆå†…ï¼Œå¤šå±‚æ•™å¸ˆå­¦ç”Ÿç‰¹å¾æ˜¯æˆå¯¹åŒ¹é…çš„ï¼Œè¿›è¡Œæ ¡å‡†è’¸é¦ã€‚éšåï¼Œæˆ‘ä»¬é€šè¿‡æ®‹å·®èåˆç”Ÿæˆæ¯ä¸ªç›¸å…³é›†åˆçš„ä»£è¡¨ç‰¹å¾ï¼Œå½¢æˆèåˆç‰¹å¾é›†ï¼Œä»è€Œå®ç°ç»„é—´çŸ¥è¯†äº¤äº’ã€‚æ‰€æå‡ºçš„çŸ¥è¯†è’¸é¦ç­–ç•¥åº”ç”¨äºåŒè·¯å¾„è†¨èƒ€å·ç§¯å¾ªç¯ç½‘ç»œï¼ˆDPDCRNï¼‰ï¼Œåœ¨L3DAS23æŒ‘æˆ˜çš„SEèµ›é“ä¸­æ’åç¬¬ä¸€ã€‚å®¢è§‚è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„çŸ¥è¯†è’¸é¦ç­–ç•¥æŒç»­æœ‰æ•ˆåœ°æé«˜äº†ä½å¤æ‚åº¦å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºå…¶ä»–è’¸é¦æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13127v1">PDF</a> submitted to IEEE Transactions on Neural Networks and Learning   Systems</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºç¥ç»ç½‘ç»œï¼ˆNNï¼‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹çš„å¤æ‚åº¦å‹ç¼©é€æ¸å¼•èµ·ç ”ç©¶äººå‘˜çš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬ä»¶èµ„æºæœ‰é™æˆ–å»¶è¿Ÿè¦æ±‚ä¸¥æ ¼çš„æƒ…å†µä¸‹ã€‚ä¸»è¦éš¾ç‚¹å’ŒæŒ‘æˆ˜åœ¨äºæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹åœ¨å¤æ‚åº¦å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºSEçš„åŸºäºæ—¶é—´é¢‘ç‡æ ¡å‡†çš„å†…å¤–é›†åˆçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ¡†æ¶ï¼ˆI$^2$S-TFCKDï¼‰ã€‚ä¸åŒäºä»¥å‰çš„SEè’¸é¦ç­–ç•¥ï¼Œè¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨è¯­éŸ³çš„æ—¶é—´é¢‘ç‡å·®å¼‚ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ƒè¿›å…¨å±€çŸ¥è¯†æµã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŒæµæ—¶é—´é¢‘ç‡äº¤å‰æ ¡å‡†çš„å¤šå±‚äº¤äº’è’¸é¦ï¼Œåˆ†åˆ«è®¡ç®—æ—¶é—´å’Œé¢‘ç‡åŸŸçš„æ•™å¸ˆå­¦ç”Ÿç›¸ä¼¼åº¦æ ¡å‡†æƒé‡ï¼Œå¹¶æ‰§è¡Œäº¤å‰åŠ æƒï¼Œä»è€Œæ ¹æ®è¯­éŸ³ç‰¹ç‚¹åœ¨ä¸åŒå±‚ä¸Šç²¾ç»†åˆ†é…è’¸é¦è´¡çŒ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ„å»ºäº†ç”¨äºå†…éƒ¨å’Œå¤–éƒ¨é›†åˆå…³ç³»çš„ååŒè’¸é¦èŒƒå¼ã€‚åœ¨ç›¸å…³é›†åˆå†…ï¼Œå¯¹å¤šå±‚æ•™å¸ˆå­¦ç”Ÿç‰¹å¾è¿›è¡Œé…å¯¹æ ¡å‡†è’¸é¦ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡æ®‹å·®èåˆç”Ÿæˆå„ç›¸å…³é›†åˆçš„ç‰¹å¾è¡¨ç¤ºï¼Œå½¢æˆèåˆç‰¹å¾é›†ï¼Œå®ç°é›†åˆé—´çš„çŸ¥è¯†äº¤äº’ã€‚æ‰€æå‡ºçš„è’¸é¦ç­–ç•¥åº”ç”¨äºåŒè·¯å¾„è†¨èƒ€å·ç§¯å¾ªç¯ç½‘ç»œï¼ˆDPDCRNï¼‰ï¼Œåœ¨L3DAS23æŒ‘æˆ˜çš„SEèµ›é“ä¸­æ’åç¬¬ä¸€ã€‚å®¢è§‚è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºKDç­–ç•¥æŒç»­æœ‰æ•ˆåœ°æé«˜äº†ä½å¤æ‚åº¦å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½å¹¶è¶…è¶Šäº†å…¶ä»–è’¸é¦æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œçš„è¯­éŸ³å¢å¼ºæ¨¡å‹å¤æ‚åº¦å‹ç¼©æ˜¯è¿‘å¹´æ¥çš„ç ”ç©¶çƒ­ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬ä»¶èµ„æºå—é™æˆ–å»¶è¿Ÿè¦æ±‚ä¸¥æ ¼çš„åœºæ™¯ä¸‹ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†è’¸é¦æ¡†æ¶I$^2$S-TFCKDï¼Œç”¨äºSEæ¨¡å‹ï¼Œè¯¥æ¡†æ¶èƒ½å……åˆ†åˆ©ç”¨è¯­éŸ³çš„æ—¶é—´é¢‘ç‡å·®å¼‚ä¿¡æ¯ã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨å¤šå±‚äº¤äº’è’¸é¦æ–¹æ³•ï¼ŒåŸºäºåŒæµæ—¶é—´é¢‘ç‡äº¤å‰æ ¡å‡†ï¼Œæ ¹æ®è¯­éŸ³ç‰¹ç‚¹åœ¨ä¸åŒå±‚ä¸Šåˆ†é…è’¸é¦è´¡çŒ®ã€‚</li>
<li>è®ºæ–‡æ„å»ºäº†å†…å¤–é›†åˆå…³ç³»çš„ååŒè’¸é¦èŒƒå¼ï¼Œå®ç°äº†é›†åˆé—´çš„çŸ¥è¯†äº¤äº’ã€‚</li>
<li>æå‡ºçš„è’¸é¦ç­–ç•¥è¢«åº”ç”¨äºDPDCRNæ¨¡å‹ï¼Œå¹¶åœ¨L3DAS23æŒ‘æˆ˜çš„SEèµ›é“ä¸­å–å¾—äº†ç¬¬ä¸€åã€‚</li>
<li>å®¢è§‚è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥è’¸é¦ç­–ç•¥æé«˜äº†ä½å¤æ‚åº¦å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½å¹¶è¶…è¶Šäº†å…¶ä»–è’¸é¦æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b376d5d90d55b25232cf44c5a172f4de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f23ec14b3090a4d7ee519d5d69b1494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61d03227d24a13906e2d3714a6e709c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83e7264d24da36bbf7a97f750e00178f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c98a1559c05e8836692b665767323b29.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SSLAM-Enhancing-Self-Supervised-Models-with-Audio-Mixtures-for-Polyphonic-Soundscapes"><a href="#SSLAM-Enhancing-Self-Supervised-Models-with-Audio-Mixtures-for-Polyphonic-Soundscapes" class="headerlink" title="SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for   Polyphonic Soundscapes"></a>SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for   Polyphonic Soundscapes</h2><p><strong>Authors:Tony Alex, Sara Ahmed, Armin Mustafa, Muhammad Awais, Philip JB Jackson</strong></p>
<p>Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the SSL pre-training has sufficiently equipped them to handle real-world audio. However, a critical question remains: how well do these models actually perform in real-world conditions, where audio is typically polyphonic and complex, involving multiple overlapping sound sources? Current audio SSL methods are often benchmarked on datasets predominantly featuring monophonic audio, such as environmental sounds, and speech. As a result, the ability of SSL models to generalize to polyphonic audio, a common characteristic in natural scenarios, remains underexplored. This limitation raises concerns about the practical robustness of SSL models in more realistic audio settings. To address this gap, we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel direction in audio SSL research, designed to improve, designed to improve the modelâ€™s ability to learn from polyphonic data while maintaining strong performance on monophonic data. We thoroughly evaluate SSLAM on standard audio SSL benchmark datasets which are predominantly monophonic and conduct a comprehensive comparative analysis against SOTA methods using a range of high-quality, publicly available polyphonic datasets. SSLAM not only improves model performance on polyphonic audio, but also maintains or exceeds performance on standard audio SSL benchmarks. Notably, it achieves up to a 3.9% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision (mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear evaluation and fine-tuning regimes with performance improvements of up to 9.1% (mAP). </p>
<blockquote>
<p>è‡ªç›‘ç£é¢„è®­ç»ƒéŸ³é¢‘ç½‘ç»œå·²åœ¨çœŸå®ä¸–ç•Œç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚è¿™äº›ç½‘ç»œé€šå¸¸å¤„äºå†»ç»“çŠ¶æ€ï¼Œå‡è®¾SSLé¢„è®­ç»ƒå·²ç»ä½¿ä»–ä»¬è¶³ä»¥å¤„ç†çœŸå®ä¸–ç•Œçš„éŸ³é¢‘ã€‚ä½†æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šè¿™äº›æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„è¡¨ç°å¦‚ä½•ï¼Œé‚£é‡Œçš„éŸ³é¢‘é€šå¸¸æ˜¯å¤šéŸ³çš„å’Œå¤æ‚çš„ï¼Œæ¶‰åŠå¤šä¸ªé‡å çš„å£°éŸ³æºï¼Ÿå½“å‰çš„éŸ³é¢‘SSLæ–¹æ³•é€šå¸¸ä¸»è¦åœ¨ä»¥å•éŸ³éŸ³é¢‘ä¸ºä¸»çš„æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¦‚ç¯å¢ƒå£°å’Œè¯­éŸ³ã€‚å› æ­¤ï¼ŒSSLæ¨¡å‹æ³›åŒ–åˆ°å¤šéŸ³éŸ³é¢‘çš„èƒ½åŠ›ï¼Œè¿™åœ¨è‡ªç„¶åœºæ™¯ä¸­æ˜¯ä¸€ä¸ªå¸¸è§ç‰¹å¾ï¼Œä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚è¿™ä¸€å±€é™æ€§å¼•å‘äº†äººä»¬å¯¹SSLæ¨¡å‹åœ¨æ›´ç°å®çš„éŸ³é¢‘è®¾ç½®ä¸­çš„å®é™…ç¨³å¥æ€§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªç›‘ç£å­¦ä¹ ä»æ··åˆéŸ³é¢‘ï¼ˆSSLAMï¼‰ï¼Œè¿™æ˜¯éŸ³é¢‘SSLç ”ç©¶çš„ä¸€ä¸ªæ–°æ–¹å‘ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹ä»å¤šéŸ³æ•°æ®å­¦ä¹ èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿æŒå¯¹å•éŸ³æ•°æ®çš„å¼ºåŠ²è¡¨ç°ã€‚æˆ‘ä»¬å¯¹SSLAMåœ¨æ ‡å‡†çš„éŸ³é¢‘SSLåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦æ˜¯å•éŸ³çš„ï¼Œå¹¶ä½¿ç”¨ä¸€ç³»åˆ—é«˜è´¨é‡ã€å…¬å¼€å¯ç”¨çš„å¤šéŸ³æ•°æ®é›†ä¸æœ€æ–°æ–¹æ³•è¿›è¡Œäº†ç»¼åˆæ¯”è¾ƒåˆ†æã€‚SSLAMä¸ä»…æé«˜äº†æ¨¡å‹åœ¨å¤šéŸ³éŸ³é¢‘ä¸Šçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨æ ‡å‡†éŸ³é¢‘SSLåŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†æ€§èƒ½æˆ–æœ‰æ‰€æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨AudioSet-2Mï¼ˆAS-2Mï¼‰ä¸Šå®ç°äº†é«˜è¾¾3.9%çš„æ”¹è¿›ï¼Œè¾¾åˆ°å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ä¸º50.2ã€‚å¯¹äºå¤šéŸ³æ•°æ®é›†ï¼ŒSSLAMåœ¨çº¿æ€§è¯„ä¼°å’Œå¾®è°ƒæ–¹æ¡ˆä¸­éƒ½è¾¾åˆ°äº†æ–°çš„æœ€ä½³çŠ¶æ€ï¼Œæ€§èƒ½æé«˜äº†é«˜è¾¾9.1%ï¼ˆmAPï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12222v1">PDF</a> Accepted at ICLR 2025. Code and pre-trained models are available at   \url{<a target="_blank" rel="noopener" href="https://github.com/ta012/SSLAM%7D">https://github.com/ta012/SSLAM}</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ç®€è¦ä»‹ç»äº†è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„éŸ³é¢‘ç½‘ç»œåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚å¤šå˜ç°å®ç¯å¢ƒä¸­çš„éŸ³é¢‘æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚æ–‡ç« å¼ºè°ƒäº†åœ¨ç°å®æ¡ä»¶ä¸‹ï¼ŒéŸ³é¢‘é€šå¸¸æ˜¯å¤šéŸ³è°ƒå’Œå¤æ‚çš„ï¼Œæ¶‰åŠå¤šä¸ªé‡å çš„å£°éŸ³æºã€‚ç°æœ‰çš„éŸ³é¢‘SSLæ–¹æ³•ä¸»è¦åœ¨ä»¥å•éŸ³éŸ³é¢‘ä¸ºä¸»çš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¦‚ç¯å¢ƒå£°å’Œè¯­éŸ³ã€‚å› æ­¤ï¼ŒSSLæ¨¡å‹æ³›åŒ–åˆ°å¤šéŸ³éŸ³é¢‘çš„èƒ½åŠ›â€”â€”è‡ªç„¶åœºæ™¯ä¸­çš„å¸¸è§ç‰¹å¾ï¼Œä»å¾…æ¢ç´¢ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†ä»éŸ³é¢‘æ··åˆç‰©çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLAMï¼‰ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹ä»å¤šéŸ³æ•°æ®ä¸­å­¦ä¹ çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹å•éŸ³æ•°æ®çš„å¼ºåŠ²è¡¨ç°ã€‚é€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæ˜¾ç¤ºSSLAMä¸ä»…æé«˜äº†æ¨¡å‹åœ¨å¤šéŸ³éŸ³é¢‘ä¸Šçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨æ ‡å‡†éŸ³é¢‘SSLåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿå¾—ä»¥ç»´æŒæˆ–æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„éŸ³é¢‘ç½‘ç»œåœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­å—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å¤šå˜çš„ç°å®éŸ³é¢‘æ¡ä»¶ä¸‹è¡¨ç°å—é™ã€‚</li>
<li>éŸ³é¢‘é€šå¸¸æ˜¯å¤šéŸ³è°ƒå’Œå¤æ‚çš„ï¼Œæ¶‰åŠå¤šä¸ªé‡å çš„å£°éŸ³æºã€‚</li>
<li>SSLæ¨¡å‹åœ¨æ³›åŒ–åˆ°å¤šéŸ³éŸ³é¢‘æ–¹é¢çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚</li>
<li>SSLAMçš„å¼•å…¥æ—¨åœ¨æé«˜æ¨¡å‹ä»å¤šéŸ³æ•°æ®ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>SSLAMåœ¨æ ‡å‡†éŸ³é¢‘SSLåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜ç§€ï¼Œå¹¶åœ¨å¤šéŸ³éŸ³é¢‘ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c45df64d79826ea5dddd02fdd9b9f6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8d8e03def4d8b2c92c1ddbc4c6ff946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f474da472a334f67138e35f79252027.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection"><a href="#Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection" class="headerlink" title="Evaluating Logit-Based GOP Scores for Mispronunciation Detection"></a>Evaluating Logit-Based GOP Scores for Mispronunciation Detection</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment. </p>
<blockquote>
<p>å‘éŸ³è¯„ä¼°ä¾èµ–äºå‘éŸ³è´¨é‡ï¼ˆGOPï¼‰åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°ä¼ ç»Ÿä¸Šç”±åŸºäºsoftmaxçš„åéªŒæ¦‚ç‡å¾—å‡ºã€‚ç„¶è€Œï¼ŒåéªŒæ¦‚ç‡å¯èƒ½ä¼šé­å—è¿‡åº¦è‡ªä¿¡å’Œä¸å‡†ç¡®çš„éŸ³ç´ åˆ†ç¦»é—®é¢˜çš„å½±å“ï¼Œä»è€Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶å¯¹æ¯”äº†åŸºäºå¯¹æ•°ä¼¼ç„¶ç‡çš„GOPåˆ†æ•°ä¸åŸºäºæ¦‚ç‡çš„GOPåˆ†æ•°åœ¨å‘éŸ³é”™è¯¯æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨ç”±è·å…°è¯­å’Œæ±‰è¯­ä½¿ç”¨è€…è¯´çš„ä¸¤ä¸ªè‹±è¯­äºŒçº§è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯„ä¼°äº†åˆ†ç±»æ€§èƒ½ä¸äººç±»è¯„åˆ†çš„ç›¸å…³æ€§ã€‚åŸºäºå¯¹æ•°ä¼¼ç„¶ç‡çš„æ–¹æ³•åœ¨åˆ†ç±»æ–¹é¢ä¼˜äºåŸºäºæ¦‚ç‡çš„GOPæ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºæ•°æ®é›†çš„ç‰¹å¾ã€‚æœ€å¤§å¯¹æ•°ä¼¼ç„¶ç‡GOPä¸äººç±»æ„ŸçŸ¥æœ€ä¸ºå»åˆï¼Œè€Œç»“åˆä¸åŒçš„GOPåˆ†æ•°å¯ä»¥å¹³è¡¡æ¦‚ç‡å’Œå¯¹æ•°ä¼¼ç„¶ç‡ç‰¹å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆä¸ç¡®å®šæ€§å»ºæ¨¡å’ŒéŸ³ç´ ç‰¹å®šæƒé‡çš„æ··åˆGOPæ–¹æ³•å¯ä»¥æ”¹å–„å‘éŸ³è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12067v1">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å‘éŸ³è¯„ä¼°ä¸­çš„è¯­éŸ³æ¸…æ™°åº¦è¯„åˆ†é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºå¯¹æ•°å‡ ç‡ï¼ˆlogitï¼‰çš„è¯­éŸ³æ¸…æ™°åº¦è¯„åˆ†åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¼˜äºåŸºäºæ¦‚ç‡çš„è¯„åˆ†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é’ˆå¯¹è·å…°è¯­å’Œè‹±è¯­äºŒè¯­è€…çš„è‹±è¯­è¯­éŸ³æ•°æ®é›†ä¸Šã€‚æœ€å¤§å¯¹æ•°å‡ ç‡è¯­éŸ³æ¸…æ™°åº¦è¯„åˆ†ä¸äººç±»æ„ŸçŸ¥æœ€ä¸ºä¸€è‡´ï¼Œè€Œç»“åˆä¸åŒè¯­éŸ³æ¸…æ™°åº¦è¯„åˆ†çš„æ··åˆæ–¹æ³•åˆ™å¹³è¡¡äº†æ¦‚ç‡å’Œå¯¹æ•°å‡ ç‡ç‰¹å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆä¸ç¡®å®šæ€§å»ºæ¨¡å’ŒéŸ³ç´ ç‰¹å®šæƒé‡çš„æ··åˆè¯­éŸ³æ¸…æ™°åº¦è¯„åˆ†æ–¹æ³•èƒ½æé«˜å‘éŸ³è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘éŸ³è¯„ä¼°ä¾èµ–äºè¯­éŸ³æ¸…æ™°åº¦ï¼ˆGOPï¼‰è¯„åˆ†ã€‚</li>
<li>ä¼ ç»Ÿä¸Šï¼ŒGOPè¯„åˆ†åŸºäºsoftmaxç”Ÿæˆçš„åéªŒæ¦‚ç‡ï¼Œä½†å­˜åœ¨è¿‡åº¦è‡ªä¿¡åŠéŸ³ç´ åˆ†ç¦»ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”äº†åŸºäºå¯¹æ•°å‡ ç‡å’ŒåŸºäºæ¦‚ç‡çš„GOPè¯„åˆ†æ–¹æ³•è¿›è¡Œå‘éŸ³è¯¯è¯»æ£€æµ‹ã€‚</li>
<li>åœ¨é’ˆå¯¹è·å…°è¯­å’Œè‹±è¯­äºŒè¯­è€…çš„è‹±è¯­è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚</li>
<li>å¯¹æ•°å‡ ç‡æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯æœ€å¤§å¯¹æ•°å‡ ç‡ï¼‰åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¼˜äºåŸºäºæ¦‚ç‡çš„GOPæ–¹æ³•ï¼Œä¸äººç±»æ„ŸçŸ¥ä¸€è‡´æ€§æ›´é«˜ã€‚</li>
<li>æ··åˆæ–¹æ³•ç»“åˆäº†æ¦‚ç‡å’Œå¯¹æ•°å‡ ç‡ç‰¹å¾ï¼Œè¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f4e551e2f7763d08071870c0c0a2cc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8501f81f5987ff7174983f35cb9f857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3afdf19f5433f9ce302c70aeaaed2c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f22f7604e892e9ad251a4c4528b2c56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ca5b3087af3ce6abd1d10cc99db7210.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CMT-LLM-Contextual-Multi-Talker-ASR-Utilizing-Large-Language-Models"><a href="#CMT-LLM-Contextual-Multi-Talker-ASR-Utilizing-Large-Language-Models" class="headerlink" title="CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models"></a>CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models</h2><p><strong>Authors:Jiajun He, Naoki Sawada, Koichi Miyazaki, Tomoki Toda</strong></p>
<p>In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLMâ€™s prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios. </p>
<blockquote>
<p>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¿…é¡»å¤„ç†æ¥è‡ªå¤šä¸ªå‘è¨€è€…çš„é‡å è¯­éŸ³ï¼Œå¹¶è¯†åˆ«è¯¸å¦‚æŠ€æœ¯æœ¯è¯­ä¹‹ç±»çš„ç½•è§è¯æ±‡ã€‚ä¼ ç»Ÿæ–¹æ³•åˆ†åˆ«å¤„ç†å¤šå‘è¨€äººASRå’Œä¸Šä¸‹æ–‡åå·®ï¼Œè¿™åœ¨å¤æ‚åœºæ™¯ä¸­é™åˆ¶äº†æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†å¤šå‘è¨€äººé‡å è¯­éŸ³è¯†åˆ«å’Œä¸Šä¸‹æ–‡åå·®ç»“åˆæˆä¸€é¡¹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ASRæ–¹æ³•é›†æˆäº†é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé‡‡ç”¨ä¼˜åŒ–çš„å¾®è°ƒç­–ç•¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•ï¼Œæœ‰æ•ˆåœ°ä»å¤§é‡åå·®åˆ—è¡¨ä¸­è¯†åˆ«å‡ºç›¸å…³çš„ç½•è§è¯æ±‡ï¼Œå¹¶å°†å…¶çº³å…¥LLMçš„æç¤ºè¾“å…¥ï¼Œä»è€Œæé«˜ç½•è§è¯æ±‡çš„è¯†åˆ«ç‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨LibriMixä¸Šå–å¾—äº†7.9%çš„WERï¼ˆè¯é”™è¯¯ç‡ï¼‰ï¼Œåœ¨AMI SDMä¸Šå–å¾—äº†32.9%çš„WERï¼Œå½“åå·®å¤§å°ä¸º1000æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºåœ¨å¤æ‚è¯­éŸ³åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡åå·®æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12059v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æå‡ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç»“åˆäº†å¤šè¯´è¯äººé‡å è¯­éŸ³è¯†åˆ«å’Œä¸Šä¸‹æ–‡åå·®çº æ­£ï¼Œä»¥æé«˜å¤æ‚åœºæ™¯ä¸­çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ä¼˜åŒ–çš„å¾®è°ƒç­–ç•¥ï¼ŒåŒæ—¶å¼•å…¥ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•ä»¥æé«˜å¯¹ç½•è§è¯çš„è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LibriMixå’ŒAMI SDMä¸Šåˆ†åˆ«å®ç°äº†è¯é”™è¯¯ç‡ä¸º7.9%å’Œ32.9%çš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ•´åˆå¤šè¯´è¯äººé‡å è¯­éŸ³è¯†åˆ«å’Œä¸Šä¸‹æ–‡åå·®çº æ­£ã€‚</li>
<li>é›†æˆé¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–å¾®è°ƒç­–ç•¥ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å¼•å…¥ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•ï¼Œä»å¤§é‡åå·®åˆ—è¡¨ä¸­æœ‰æ•ˆè¯†åˆ«ç›¸å…³ç½•è§è¯æ±‡ã€‚</li>
<li>æ–¹æ³•åœ¨LibriMixå’ŒAMI SDMå¤æ‚è¯­éŸ³åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿä¸Šä¸‹æ–‡åå·®çº æ­£æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
<li>åœ¨LibriMixä¸Šå®ç°è¯é”™è¯¯ç‡ä¸º7.9%ï¼Œåœ¨AMI SDMä¸Šå®ç°è¯é”™è¯¯ç‡ä¸º32.9%ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæé«˜è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å®ç”¨æ€§å’Œæ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4df9e3cca248b1115e40afd117fc22b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc32ff949dd3c9dc27e4027694c0a77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819cb347f62ccab7d819d562ec0c17d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3024a82e19a9899f894d37e20335fca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c133ef7c553d19ace274657a1f93c45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31acfcf6296b26ed84845bc28d3917cd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection"><a href="#EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection" class="headerlink" title="EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection"></a>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection</h2><p><strong>Authors:Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, SÃ¶ren Auer</strong></p>
<p>The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³å’ŒéŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥è¦æ±‚å¯¹AIç³»ç»Ÿçš„æƒ…æ„Ÿç†è§£èƒ½åŠ›è¿›è¡Œç¨³å¥çš„åŸºå‡†æµ‹è¯•ã€‚å½“å‰è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ•°æ®é›†åœ¨æƒ…æ„Ÿç²’åº¦ã€éšç§æ‹…å¿§æˆ–ä¾èµ–è¡¨æ¼”è¡¨ç°ç­‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†EmoNet-Voiceï¼Œä¸€ä¸ªç”¨äºè¯­éŸ³æƒ…æ„Ÿæ£€æµ‹çš„æ–°èµ„æºï¼ŒåŒ…æ‹¬EmoNet-Voice Bigï¼Œä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼ˆè·¨è¶Š11ä¸ªå£°éŸ³ã€40ç§æƒ…æ„Ÿå’Œ4ç§è¯­è¨€ï¼ŒåŒ…å«è¶…è¿‡4500å°æ—¶çš„è¯­éŸ³ï¼‰ï¼Œä»¥åŠEmoNet-Voice Benchï¼Œä¸€ä¸ªå¸¦æœ‰ä¸“å®¶æ³¨é‡Šçš„æ–°å‹åŸºå‡†æ•°æ®é›†ã€‚EmoNet-Voiceæ—¨åœ¨è¯„ä¼°SERæ¨¡å‹åœ¨40ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ç²¾ç»†ç²’åº¦å…‰è°±ä¸Šçš„è¡¨ç°ï¼Œè¿™äº›æƒ…æ„Ÿç±»åˆ«çš„å¼ºåº¦ä¸åŒã€‚æˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­éŸ³ç”ŸæˆæŠ€æœ¯ï¼Œç²¾å¿ƒåˆ¶ä½œäº†æ¨¡æ‹Ÿæ¼”å‘˜è¡¨ç°ç‰¹å®šæƒ…æ„Ÿåœºæ™¯çš„åˆæˆéŸ³é¢‘ç‰‡æ®µã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬é‚€è¯·äº†å¿ƒç†å­¦ä¸“å®¶è¿›è¡Œäº†ä¸¥æ ¼çš„éªŒè¯ï¼Œä»–ä»¬åˆ†é…äº†æ„ŸçŸ¥å¼ºåº¦æ ‡ç­¾ã€‚è¿™ç§åˆæˆçš„ã€ä¿æŠ¤éšç§çš„æ–¹æ³•å¯ä»¥åŒ…å«ç°æœ‰æ•°æ®é›†ä¸­é€šå¸¸ä¸å­˜åœ¨çš„æ•æ„Ÿæƒ…æ„ŸçŠ¶æ€ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†Empathic Insight Voiceæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œä¸äººç±»ä¸“å®¶çš„å…±è¯†é«˜åº¦ä¸€è‡´ã€‚æˆ‘ä»¬åœ¨å½“å‰æ¨¡å‹æ™¯è§‚ä¸­çš„è¯„ä¼°å±•ç°äº†æœ‰ä»·å€¼çš„å‘ç°ï¼Œä¾‹å¦‚é«˜å”¤é†’æƒ…ç»ªï¼ˆå¦‚æ„¤æ€’ï¼‰æ¯”ä½å”¤é†’çŠ¶æ€ï¼ˆå¦‚ä¸“æ³¨ï¼‰æ›´å®¹æ˜“æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09827v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›†EmoNet-Voiceï¼ŒåŒ…æ‹¬ç”¨äºé¢„è®­ç»ƒçš„EmoNet-Voice Bigå’Œä½œä¸ºåŸºå‡†æµ‹è¯•é›†çš„EmoNet-Voice Benchã€‚è¯¥æ•°æ®é›†å…·æœ‰40ç§æƒ…æ„Ÿç±»åˆ«ï¼Œä¸åŒå¼ºåº¦æ°´å¹³ï¼Œå¹¶ç”±è¯­éŸ³ç”Ÿæˆæ¨¡å‹æ¨¡æ‹ŸçœŸå®åœºæ™¯ç”ŸæˆéŸ³é¢‘ç‰‡æ®µã€‚é€šè¿‡å¿ƒç†å­¦ä¸“å®¶è¿›è¡Œä¸¥æ ¼çš„éªŒè¯ï¼Œå¹¶èµ‹äºˆæƒ…æ„Ÿå¼ºåº¦æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Empathic Insight Voiceæ¨¡å‹ï¼Œåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹é¢ä¸äººç±»ä¸“å®¶é«˜åº¦ä¸€è‡´ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé«˜å”¤é†’æƒ…ç»ªå¦‚æ„¤æ€’æ¯”ä½å”¤é†’çŠ¶æ€å¦‚ä¸“æ³¨æ›´å®¹æ˜“æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†æ–°å‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›†EmoNet-Voiceï¼ŒåŒ…å«å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†EmoNet-Voice Bigå’ŒåŸºå‡†æµ‹è¯•é›†EmoNet-Voice Benchã€‚</li>
<li>EmoNet-Voiceæ•°æ®é›†å…·æœ‰40ç§æƒ…æ„Ÿç±»åˆ«ï¼Œæ¶µç›–ä¸åŒå¼ºåº¦æ°´å¹³ï¼Œå¯è¯„ä¼°æ¨¡å‹çš„ç²¾ç»†æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„è¯­éŸ³ç”ŸæˆæŠ€æœ¯ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ç”ŸæˆéŸ³é¢‘ç‰‡æ®µï¼Œä»¥å¢å¼ºæ•°æ®é›†çš„å®ç”¨æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>é€šè¿‡å¿ƒç†å­¦ä¸“å®¶è¿›è¡Œä¸¥æ ¼çš„éªŒè¯ï¼Œå¹¶èµ‹äºˆæƒ…æ„Ÿå¼ºåº¦æ ‡ç­¾ï¼Œæé«˜æ•°æ®é›†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>å¼•å…¥äº†Empathic Insight Voiceæ¨¡å‹ï¼Œåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹é¢ä¸äººç±»ä¸“å®¶é«˜åº¦ä¸€è‡´ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œé«˜å”¤é†’æƒ…ç»ªç›¸å¯¹å®¹æ˜“æ£€æµ‹ï¼Œè€Œä½å”¤é†’çŠ¶æ€å¦‚ä¸“æ³¨ç­‰æƒ…æ„Ÿçš„è¯†åˆ«æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5826b7a29f41fe24f9d3145cda5eeda7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e5c3e69192ee6e28a9481fc91b6d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f3ad4742d3f56ecf7b1adf9445f1db9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ead12f776165c52d3bd1fc04d82aeaba.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLM-and-Self-Supervised-Training-Models-for-Speech-Recognition-in-Chinese-Dialects-A-Comparative-Analysis"><a href="#Leveraging-LLM-and-Self-Supervised-Training-Models-for-Speech-Recognition-in-Chinese-Dialects-A-Comparative-Analysis" class="headerlink" title="Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis"></a>Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis</h2><p><strong>Authors:Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei</strong></p>
<p>Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre-training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research </p>
<blockquote>
<p>å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“å·²ç»æ˜¾è‘—æé«˜äº†è¯­éŸ³è¯†åˆ«æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç›¸å¯¹ç¨€ç¼ºï¼Œä¸­æ–‡å£éŸ³å’Œæ–¹è¨€ä»ç„¶æ˜¯å¤§å¤šæ•°è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„è‡ªç›‘ç£å­¦ä¹ è¿›å±•è¡¨æ˜ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“åˆï¼Œå¯ä»¥åœ¨ä½èµ„æºæƒ…å†µä¸‹æœ‰æ•ˆæé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç ”ç©¶è¿™ç§èŒƒå¼åœ¨ä¸­æ–‡æ–¹è¨€ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨30ä¸‡å°æ—¶çš„æ— æ ‡ç­¾æ–¹è¨€å’Œå¸¦å£éŸ³è¯­éŸ³æ•°æ®ä¸Šé¢„è®­ç»ƒäº†ä¸€ä¸ªData2vec2æ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ä¸ª4ä¸‡å°æ—¶çš„ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œå¯¹é½è®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†åœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œå„ç§æŠ•å½±ä»ªå’ŒLLMå¯¹æ™®é€šè¯ã€æ–¹è¨€å’Œå¸¦å£éŸ³è¯­éŸ³è¯†åˆ«æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ–¹è¨€æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³ç»“æœï¼ŒåŒ…æ‹¬Kespeechã€‚æˆ‘ä»¬å°†å¼€æºæˆ‘ä»¬çš„å·¥ä½œï¼Œä»¥ä¿ƒè¿›å¯å¤åˆ¶çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21138v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“æ˜¾è‘—æé«˜äº†è¯­éŸ³è¯†åˆ«æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç›¸å¯¹ç¨€ç¼ºï¼Œä¸­æ–‡å£éŸ³å’Œæ–¹è¨€ä»æ˜¯å¤§å¤šæ•°è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æŒ‘æˆ˜ã€‚æœ€è¿‘è‡ªç›‘ç£å­¦ä¹ çš„è¿›å±•æ˜¾ç¤ºï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆï¼Œå¯æœ‰æ•ˆæé«˜ä½èµ„æºåœºæ™¯ä¸­çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶è¿™ä¸€æ¨¡å¼åœ¨ä¸­æ–‡æ–¹è¨€ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨30ä¸‡å°æ—¶çš„æ— æ ‡ç­¾æ–¹è¨€å’Œå£éŸ³è¯­éŸ³æ•°æ®ä¸Šé¢„è®­ç»ƒäº†Data2vec2æ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ä¸ª4ä¸‡å°æ—¶çš„ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œå¯¹é½è®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†è¯¥æ¨¡å¼ä¸‹å„ç§æŠ•å½±å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ™®é€šè¯ã€æ–¹è¨€å’Œå£éŸ³è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ–¹è¨€æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒ…æ‹¬Kespeechæ•°æ®é›†ã€‚æˆ‘ä»¬å°†å¼€æºæˆ‘ä»¬çš„å·¥ä½œä»¥ä¿ƒè¿›å¯å¤ç°ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“å¢å¼ºäº†è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸­æ–‡å£éŸ³å’Œæ–¹è¨€ä»æ˜¯è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆåœ¨ä½èµ„æºåœºæ™¯ä¸­çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶è€…ä½¿ç”¨Data2vec2æ¨¡å‹åœ¨30ä¸‡å°æ—¶çš„æ— æ ‡ç­¾æ–¹è¨€å’Œå£éŸ³è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>å¯¹é½è®­ç»ƒåœ¨4ä¸‡å°æ—¶çš„ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è€…ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒæŠ•å½±å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24312001676eb086354716bd668bb4b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-013d37df0e3f2cb6fe97a767a5f31427.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c83d5dfb5fb3af0f0b09f6cda952ff6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1339c33604d6c4e21e782d07e92a7a65.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ArrayDPS-Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior"><a href="#ArrayDPS-Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior" class="headerlink" title="ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior"></a>ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior</h2><p><strong>Authors:Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury</strong></p>
<p>Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a>. </p>
<blockquote>
<p>ç›²è¯­éŸ³åˆ†ç¦»ï¼ˆBSSï¼‰æ—¨åœ¨ä»éº¦å…‹é£é˜µåˆ—è®°å½•çš„éŸ³é¢‘æ··åˆç‰©ä¸­åˆ†ç¦»å‡ºå¤šä¸ªè¯­éŸ³æºã€‚è¿™ä¸ªé—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç›²åé—®é¢˜ï¼Œå³éº¦å…‹é£é˜µåˆ—çš„å‡ ä½•å½¢çŠ¶ã€æˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰å’Œè¯­éŸ³æºéƒ½æ˜¯æœªçŸ¥çš„ã€‚æˆ‘ä»¬æå‡ºArrayDPSä»¥æ— ç›‘ç£ã€é˜µåˆ—æ— å…³å’Œç”Ÿæˆçš„æ–¹å¼è§£å†³BSSé—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³å»ºç«‹åœ¨æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰çš„åŸºç¡€ä¸Šï¼Œä½†ä¸DPSä¸åŒçš„æ˜¯ï¼ŒDPSçš„å¯èƒ½æ€§æ˜¯æ˜ç¡®çš„ï¼ŒArrayDPSå¿…é¡»é€šè¿‡åˆ¶å®šä¸€ä¸ªå•ç‹¬çš„ä¼˜åŒ–é—®é¢˜æ¥è¿‘ä¼¼å¯èƒ½æ€§ã€‚ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆè¿‘ä¼¼äºæˆ¿é—´å£°å­¦ä»¥åŠéº¦å…‹é£ä¹‹é—´çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ã€‚è¿™äº›è¿‘ä¼¼å€¼ï¼Œç»“åˆæ‰©æ•£å…ˆéªŒï¼Œåœ¨ArrayDPSé‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£ï¼Œå¹¶æœ€ç»ˆäº§ç”Ÿåˆ†ç¦»çš„è¯­éŸ³æºã€‚æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªç®€å•çš„å•è¯´è¯äººè¯­éŸ³æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œä»¥åŠéº¦å…‹é£è®°å½•çš„æ··åˆç‰©ï¼›ä¸éœ€è¦éº¦å…‹é£é˜µåˆ—çš„ä¿¡æ¯ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒArrayDPSåœ¨SDRæ–¹é¢ä¼˜äºæ‰€æœ‰åŸºçº¿æ— ç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶ä¸æœ‰ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚éŸ³é¢‘æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/%E3%80%82">https://arraydps.github.io/ArrayDPSDemo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05657v3">PDF</a> Paper Accepted at ICML2025 Demo:   <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/ArrayDPS/ArrayDPS">https://github.com/ArrayDPS/ArrayDPS</a></p>
<p><strong>Summary</strong><br>ç›²è¯­éŸ³åˆ†ç¦»ï¼ˆBSSï¼‰æ˜¯ä»éº¦å…‹é£é˜µåˆ—å½•åˆ¶çš„éŸ³é¢‘æ··åˆä¸­åˆ†ç¦»å¤šä¸ªè¯­éŸ³æºçš„é—®é¢˜ã€‚æå‡ºArrayDPSä»¥æ— ç›‘ç£ã€é˜µåˆ—æ— å…³å’Œç”Ÿæˆçš„æ–¹å¼è§£å†³BSSé—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³å»ºç«‹åœ¨æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰çš„åŸºç¡€ä¸Šï¼Œä½†ArrayDPSå¿…é¡»é€šè¿‡å•ç‹¬çš„ä¼˜åŒ–é—®é¢˜æ¥è¿‘ä¼¼å¯èƒ½æ€§ã€‚è§£å†³æ–¹æ¡ˆè¿‘ä¼¼æˆ¿é—´å£°å­¦åŠéº¦å…‹é£ä¹‹é—´çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ã€‚è¿™äº›è¿‘ä¼¼ä¸æ‰©æ•£å…ˆéªŒç›¸ç»“åˆï¼Œåœ¨ArrayDPSé‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£ï¼Œæœ€ç»ˆäº§ç”Ÿåˆ†ç¦»çš„è¯­éŸ³æºã€‚åªéœ€ç®€å•çš„å•è¯´è¯äººè¯­éŸ³æ‰©æ•£æ¨¡å‹å…ˆéªŒå’Œéº¦å…‹é£å½•åˆ¶çš„æ··éŸ³ï¼Œæ— éœ€çŸ¥é“éº¦å…‹é£é˜µåˆ—ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›²è¯­éŸ³åˆ†ç¦»ï¼ˆBSSï¼‰æ˜¯ä»éŸ³é¢‘æ··åˆä¸­åˆ†ç¦»å¤šä¸ªæœªçŸ¥è¯­éŸ³æºçš„é—®é¢˜ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ArrayDPSæ˜¯ä¸€ç§è§£å†³BSSé—®é¢˜çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨æ— ç›‘ç£ã€é˜µåˆ—æ— å…³å’Œç”Ÿæˆçš„æ–¹å¼ã€‚</li>
<li>ArrayDPSå»ºç«‹åœ¨æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰çš„åŸºç¡€ä¸Šï¼Œä½†å¿…é¡»é€šè¿‡å•ç‹¬çš„ä¼˜åŒ–é—®é¢˜æ¥è¿‘ä¼¼å¯èƒ½æ€§ã€‚</li>
<li>ArrayDPSçš„è§£å†³æ–¹æ¡ˆå¯ä»¥è¿‘ä¼¼æˆ¿é—´å£°å­¦å’Œéº¦å…‹é£ä¹‹é—´çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•åªéœ€å•è¯´è¯äººè¯­éŸ³æ‰©æ•£æ¨¡å‹å…ˆéªŒå’Œæ··éŸ³ï¼Œæ— éœ€çŸ¥é“éº¦å…‹é£é˜µåˆ—çš„å…·ä½“ä¿¡æ¯ã€‚</li>
<li>è¯„ä»·ç»“æœæ˜¾ç¤ºï¼ŒArrayDPSåœ¨æ— ç›‘ç£æ–¹æ³•ä¸­çš„è¡¨ç°ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨SDRæ–¹é¢ä¸ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96eb1634841b5ffd7b5293fe60717f11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91e144398c5b5e2a2baa6c0e71473a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc750cae64fd3a5dd2196958422f64f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e43afdd9ce694caecba76520109e6e7e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions"><a href="#QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions" class="headerlink" title="QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions"></a>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang</strong></p>
<p>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥è¿›è¡Œè¯­éŸ³è´¨é‡è¯„ä¼°çš„æ–°è§†è§’ï¼Œæä¾›æ¯”ä¼ ç»Ÿæ•°å­—è¯„åˆ†æ–¹æ³•æ›´ä¸°å¯Œã€æ›´ç»†å¾®çš„è§è§£ã€‚è‡ªç„¶è¯­è¨€åé¦ˆæä¾›æŒ‡å¯¼æ€§çš„å»ºè®®å’Œè¯¦ç»†çš„è¯„ä»·ï¼Œä½†ç°æœ‰çš„æ•°æ®é›†ç¼ºä¹è¿™ç§æ–¹æ³•çš„å…¨é¢æ³¨é‡Šã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†QualiSpeechæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä½å±‚æ¬¡è¯­éŸ³è´¨é‡è¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–äº†1 1ä¸ªå…³é”®æ–¹é¢å’ŒåŒ…å«æ¨ç†å’Œä¸Šä¸‹æ–‡æ´å¯Ÿçš„è‡ªç„¶è¯­è¨€è¯„è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†QualiSpeechåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä½å±‚æ¬¡è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„å¬è§‰LLMèƒ½å¤Ÿå¯é åœ°æè¿°å™ªå£°å’Œå¤±çœŸçš„ç»†èŠ‚ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å®ƒä»¬çš„ç±»å‹å’Œæ—¶é—´ç‰¹å¾ã€‚ç»“æœè¿›ä¸€æ­¥çªæ˜¾äº†ç»“åˆæ¨ç†æ¥æé«˜è´¨é‡è¯„ä¼°å‡†ç¡®æ€§å’Œå¯é æ€§çš„æ½œåŠ›ã€‚è¯¥æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeechå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20290v3">PDF</a> 22 pages, 10 figures</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æ¢ç´¢äº†åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œè¯­éŸ³è´¨é‡è¯„ä¼°çš„æ–°è§†è§’ï¼Œæä¾›æ¯”ä¼ ç»Ÿæ•°å­—è¯„åˆ†æ–¹æ³•æ›´ä¸°å¯Œã€æ›´ç»†å¾®çš„è§è§£ã€‚ä¸ºè§£å†³ç°æœ‰æ•°æ®é›†ç¼ºä¹å…¨é¢æ³¨é‡Šçš„é—®é¢˜ï¼Œæå‡ºäº†QualiSpeechæ•°æ®é›†å’ŒQualiSpeechåŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å«11ä¸ªå…³é”®æ–¹é¢çš„ä½çº§åˆ«è¯­éŸ³è´¨é‡è¯„ä¼°ï¼Œä»¥åŠåŒ…å«æ¨ç†å’Œä¸Šä¸‹æ–‡æ´å¯Ÿçš„è‡ªç„¶è¯­è¨€è¯„è®ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥å¯é åœ°æè¿°å™ªå£°å’Œå¤±çœŸï¼Œæœ‰æ•ˆè¯†åˆ«å®ƒä»¬çš„ç±»å‹å’Œæ—¶é—´ç‰¹å¾ã€‚è¯¥æ•°æ®é›†å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeechã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­éŸ³è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æä¾›ä¸°å¯Œçš„è§è§£ã€‚</li>
<li>è‡ªç„¶è¯­è¨€åé¦ˆæä¾›äº†è¯¦ç»†è¯„ä»·å’ŒæŒ‡å¯¼æ€§å»ºè®®ã€‚</li>
<li>å­˜åœ¨ç¼ºä¹å…¨é¢æ³¨é‡Šçš„ç°æœ‰æ•°æ®é›†ï¼Œä¸ºæ­¤å¼•å…¥äº†QualiSpeechæ•°æ®é›†ã€‚</li>
<li>QualiSpeechæ•°æ®é›†åŒ…å«ä½çº§åˆ«è¯­éŸ³è´¨é‡è¯„ä¼°çš„å¤šä¸ªå…³é”®æ–¹é¢å’Œè¯¦ç»†çš„è‡ªç„¶è¯­è¨€è¯„è®ºã€‚</li>
<li>QualiSpeechåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½çº§åˆ«è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å‡†ç¡®æè¿°è¯­éŸ³ä¸­çš„å™ªå£°å’Œå¤±çœŸç±»å‹åŠå…¶æ—¶é—´ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd02b561eaa9fde550b85f371259d93e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-324aa14684013b8b470bd43a3c761fd3" align="middle">
<img src="https://pic1.zhimg.com/v2-7ca3df0bd79ad7aa72716e77df206cce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-735f70115a2acb80ba97e25ff412b5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dfa45425e00bb2f0b3a3bca9015140c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MTLM-Incorporating-Bidirectional-Text-Information-to-Enhance-Language-Model-Training-in-Speech-Recognition-Systems"><a href="#MTLM-Incorporating-Bidirectional-Text-Information-to-Enhance-Language-Model-Training-in-Speech-Recognition-Systems" class="headerlink" title="MTLM: Incorporating Bidirectional Text Information to Enhance Language   Model Training in Speech Recognition Systems"></a>MTLM: Incorporating Bidirectional Text Information to Enhance Language   Model Training in Speech Recognition Systems</h2><p><strong>Authors:Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai, Huizhi Liang</strong></p>
<p>Automatic speech recognition (ASR) systems normally consist of an acoustic model (AM) and a language model (LM). The acoustic model estimates the probability distribution of text given the input speech, while the language model calibrates this distribution toward a specific knowledge domain to produce the final transcription. Traditional ASR-specific LMs are typically trained in a unidirectional (left-to-right) manner to align with autoregressive decoding. However, this restricts the model from leveraging the right-side context during training, limiting its representational capacity. In this work, we propose MTLM, a novel training paradigm that unifies unidirectional and bidirectional manners through 3 training objectives: ULM, BMLM, and UMLM. This approach enhances the LMâ€™s ability to capture richer linguistic patterns from both left and right contexts while preserving compatibility with standard ASR autoregressive decoding methods. As a result, the MTLM model not only enhances the ASR systemâ€™s performance but also support multiple decoding strategies, including shallow fusion, unidirectional&#x2F;bidirectional n-best rescoring. Experiments on the LibriSpeech dataset show that MTLM consistently outperforms unidirectional training across multiple decoding strategies, highlighting its effectiveness and flexibility in ASR applications. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿé€šå¸¸ç”±å£°å­¦æ¨¡å‹ï¼ˆAMï¼‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ç»„æˆã€‚å£°å­¦æ¨¡å‹ä¼°è®¡ç»™å®šè¾“å…¥è¯­éŸ³çš„æ–‡æœ¬æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œè¯­è¨€æ¨¡å‹åˆ™å¯¹è¿™ä¸ªåˆ†å¸ƒè¿›è¡Œæ ¡å‡†ï¼Œä»¥é¢å‘ç‰¹å®šçš„çŸ¥è¯†é¢†åŸŸï¼Œä»è€Œäº§ç”Ÿæœ€ç»ˆçš„è½¬å½•ã€‚ä¼ ç»Ÿçš„ASRä¸“ç”¨LMé€šå¸¸é‡‡ç”¨å•å‘ï¼ˆä»å·¦åˆ°å³ï¼‰çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¬¦åˆè‡ªå›å½’è§£ç ã€‚ç„¶è€Œï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨å³ä¾§ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†å…¶è¡¨å¾èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MTLMï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ä¸‰ä¸ªè®­ç»ƒç›®æ ‡ï¼šULMã€BMLMå’ŒUMLMï¼Œç»Ÿä¸€äº†å•å‘å’ŒåŒå‘æ–¹å¼ã€‚è¿™ç§æ–¹æ³•æé«˜äº†LMä»å·¦å³ä¸Šä¸‹æ–‡ä¸­æ•è·æ›´ä¸°å¯Œè¯­è¨€æ¨¡å¼çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸æ ‡å‡†ASRè‡ªå›å½’è§£ç æ–¹æ³•çš„å…¼å®¹æ€§ã€‚å› æ­¤ï¼ŒMTLMæ¨¡å‹ä¸ä»…æé«˜äº†ASRç³»ç»Ÿçš„æ€§èƒ½ï¼Œè¿˜æ”¯æŒå¤šç§è§£ç ç­–ç•¥ï¼ŒåŒ…æ‹¬æµ…èåˆã€å•å‘&#x2F;åŒå‘n-besté‡è¯„åˆ†ã€‚åœ¨LibriSpeechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMTLMåœ¨å¤šç§è§£ç ç­–ç•¥ä¸Šå§‹ç»ˆä¼˜äºå•å‘è®­ç»ƒï¼Œçªå‡ºäº†å…¶åœ¨ASRåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10058v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è®­ç»ƒæ–°æ–¹æ³•â€”â€”MTLMã€‚è¯¥æ–¹æ³•ç»“åˆäº†å•å‘å’ŒåŒå‘è®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡ULMã€BMLMå’ŒUMLMä¸‰ä¸ªè®­ç»ƒç›®æ ‡ï¼Œæé«˜äº†è¯­è¨€æ¨¡å‹æ•æ‰å·¦å³è¯­å¢ƒä¸­æ›´ä¸°å¯Œè¯­è¨€æ¨¡å¼çš„èƒ½åŠ›ï¼ŒåŒæ—¶å…¼å®¹æ ‡å‡†ASRè‡ªå›å½’è§£ç æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒMTLMæ¨¡å‹åœ¨å¤šä¸ªè§£ç ç­–ç•¥ä¸‹å‡ä¼˜äºå•å‘è®­ç»ƒï¼Œæœ‰æ•ˆæé«˜ASRç³»ç»Ÿæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿé€šå¸¸ç”±å£°å­¦æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ç»„æˆï¼Œå‰è€…ä¼°è®¡æ–‡æœ¬çš„æ¦‚ç‡åˆ†å¸ƒï¼Œåè€…å°†æ­¤åˆ†å¸ƒæ ¡å‡†åˆ°ç‰¹å®šçŸ¥è¯†åŸŸä»¥äº§ç”Ÿæœ€ç»ˆè½¬å½•ã€‚</li>
<li>ä¼ ç»ŸASRç‰¹å®šçš„è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥å•å‘ï¼ˆä»å·¦åˆ°å³ï¼‰æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä¸è‡ªå›å½’è§£ç ç›¸åŒ¹é…ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å³ä¾§è¯­å¢ƒçš„åˆ©ç”¨ã€‚</li>
<li>MTLMæ˜¯ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†å•å‘å’ŒåŒå‘æ–¹å¼ï¼Œé€šè¿‡ULMã€BMLMå’ŒUMLMä¸‰ä¸ªè®­ç»ƒç›®æ ‡ï¼Œæé«˜è¯­è¨€æ¨¡å‹æ•æ‰å·¦å³è¯­å¢ƒä¸­è¯­è¨€æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
<li>MTLMå¢å¼ºäº†ASRç³»ç»Ÿçš„æ€§èƒ½ï¼Œæ”¯æŒå¤šç§è§£ç ç­–ç•¥ï¼ŒåŒ…æ‹¬æµ…èåˆã€å•å‘&#x2F;åŒå‘n-besté‡è¯„åˆ†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨LibriSpeechæ•°æ®é›†ä¸Šï¼ŒMTLMåœ¨å¤šä¸ªè§£ç ç­–ç•¥ä¸‹çš„è¡¨ç°å‡ä¼˜äºå•å‘è®­ç»ƒã€‚</li>
<li>MTLMæ–¹æ³•æ—¢æœ‰æ•ˆåˆçµæ´»ï¼Œå¯åº”ç”¨äºä¸åŒçš„ASRåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-341d3f3f745ad381664d5f021e70be6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bcf44cdc1ceee62111bd347725b9e20.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c00f94d61873b744a72e97f748e23fd.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Public Acceptance of Cybernetic Avatars in the service sector Evidence   from a Large-Scale Survey in Dubai
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0b3ebdc4821c4c59d09a146b320df3e3.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
