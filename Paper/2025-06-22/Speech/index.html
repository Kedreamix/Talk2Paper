<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-22  Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ee45cedcd6353bdb3f75d59119e8916e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-22-更新"><a href="#2025-06-22-更新" class="headerlink" title="2025-06-22 更新"></a>2025-06-22 更新</h1><h2 id="Exploiting-Music-Source-Separation-for-Automatic-Lyrics-Transcription-with-Whisper"><a href="#Exploiting-Music-Source-Separation-for-Automatic-Lyrics-Transcription-with-Whisper" class="headerlink" title="Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper"></a>Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper</h2><p><strong>Authors:Jaza Syed, Ivan Meresman Higgs, Ondřej Cífka, Mark Sandler</strong></p>
<p>Automatic lyrics transcription (ALT) remains a challenging task in the field of music information retrieval, despite great advances in automatic speech recognition (ASR) brought about by transformer-based architectures in recent years. One of the major challenges in ALT is the high amplitude of interfering audio signals relative to conventional ASR due to musical accompaniment. Recent advances in music source separation have enabled automatic extraction of high-quality separated vocals, which could potentially improve ALT performance. However, the effect of source separation has not been systematically investigated in order to establish best practices for its use. This work examines the impact of source separation on ALT using Whisper, a state-of-the-art open source ASR model. We evaluate Whisper’s performance on original audio, separated vocals, and vocal stems across short-form and long-form transcription tasks. For short-form, we suggest a concatenation method that results in a consistent reduction in Word Error Rate (WER). For long-form, we propose an algorithm using source separation as a vocal activity detector to derive segment boundaries, which results in a consistent reduction in WER relative to Whisper’s native long-form algorithm. Our approach achieves state-of-the-art results for an open source system on the Jam-ALT long-form ALT benchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the first dataset of long-form lyric transcripts following the Jam-ALT guidelines for which vocal stems are publicly available. </p>
<blockquote>
<p>自动歌词转录（ALT）仍是音乐信息检索领域中的一个具有挑战性的任务。尽管近年来基于转换器的架构在自动语音识别（ASR）方面取得了巨大进展，但ALT仍然面临许多困难。ALT的主要挑战之一是音乐伴奏产生的干扰音频信号幅度较高，这使得相对于传统ASR来说更具挑战性。音乐源分离方面的最新进展使得能够自动提取高质量的分开的声音，这可能会提高ALT的性能。然而，为了建立最佳实践，源分离的影响尚未进行系统性的研究。这项工作使用Whisper（一种先进的开源ASR模型）来检查源分离对ALT的影响。我们评估了Whisper在原始音频、分离后的声音、以及语音茎上的性能，涵盖了短期和长期转录任务。对于短期任务，我们提出了一种拼接方法，该方法导致单词错误率（WER）持续下降。对于长期任务，我们提出了一种使用源分离作为语音活动检测器来推导段边界的算法，相对于Whisper的本地长期算法，该算法在WER方面实现了持续的降低。我们的方法在遵循Jam-ALT指导方针的长期ALT基准测试上达到了开放源代码系统的最新成果水平，无需进行任何训练或微调。我们还发布了第一个遵循Jam-ALT指导方针的长期歌词转录数据集MUSDB-ALT，该数据集提供的歌词录音均为公开可用的语音茎。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15514v1">PDF</a> Accepted at 2025 ICME Workshop AI for Music</p>
<p><strong>摘要</strong></p>
<p>尽管近年来基于转换器的架构在自动语音识别（ASR）方面取得了巨大进步，但自动歌词转录（ALT）仍是音乐信息检索领域的一项具有挑战性的任务。ALT的主要挑战之一是音乐伴奏产生的干扰音频信号幅度较高，与常规ASR相比，这对ALT提出了更高的要求。音乐源分离的近期进展使得能够自动提取高质量的分离人声，这可能会提高ALT的性能。然而，源分离的影响尚未进行系统性的研究，以建立其最佳实践。本研究使用Whisper这一先进的开源ASR模型，探讨了源分离对ALT的影响。我们评估了Whisper在原始音频、分离的人声和语音片段上的性能，涵盖了短形式和长形式的转录任务。对于短形式，我们提出了一种拼接方法，导致词错误率（WER）持续降低。对于长形式，我们提出了一种使用源分离作为语音活动检测器来推导段落边界的算法，与Whisper的内置长形式算法相比，该算法导致WER持续降低。我们的方法在遵循Jam-ALT长形式ALT基准的MUSDB-ALT数据集上实现了开源系统的最新结果，无需进行任何训练或微调。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>自动歌词转录（ALT）是音乐信息检索领域的一项具有挑战性的任务，主要挑战之一是音乐伴奏产生的高幅度干扰音频信号。</li>
<li>近期音乐源分离的进展为ALT的改进提供了可能，能够自动提取高质量的分离人声。</li>
<li>本研究使用Whisper这一先进的开源ASR模型，探讨了源分离对ALT的影响。</li>
<li>对于短形式的转录任务，提出了一种拼接方法，能够降低词错误率（WER）。</li>
<li>对于长形式的转录任务，使用源分离作为语音活动检测器来推导段落边界的算法能够有效降低WER。</li>
<li>提出的方法在遵循Jam-ALT长形式ALT基准的MUSDB-ALT数据集上实现了最新结果，且无需任何训练或微调。</li>
<li>研究还发布了遵循Jam-ALT指南的长形式歌词转录数据集MUSDB-ALT，其中人声素材可公开获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76116cbee6fa7a63dd1d76946d4f263a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee45cedcd6353bdb3f75d59119e8916e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3175e0ee94136779af73bf1c39a25f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe22069528e0f74b50ef218223d94ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8134f427ba5c3baee5a6cf86b9a400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af420a3ab285d10f4722703b38f41d3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd17ef34c9cb53ccaa0b1943233bd73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b910a62a5aec036e66bc74abfc939b0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Evaluation-of-Deep-Learning-Models-for-Speech-Enhancement-in-Real-World-Noisy-Environments"><a href="#A-Comparative-Evaluation-of-Deep-Learning-Models-for-Speech-Enhancement-in-Real-World-Noisy-Environments" class="headerlink" title="A Comparative Evaluation of Deep Learning Models for Speech Enhancement   in Real-World Noisy Environments"></a>A Comparative Evaluation of Deep Learning Models for Speech Enhancement   in Real-World Noisy Environments</h2><p><strong>Authors:Md Jahangir Alam Khondkar, Ajan Ahmed, Masudul Haider Imtiaz, Stephanie Schuckers</strong></p>
<p>Speech enhancement, particularly denoising, is vital in improving the intelligibility and quality of speech signals for real-world applications, especially in noisy environments. While prior research has introduced various deep learning models for this purpose, many struggle to balance noise suppression, perceptual quality, and speaker-specific feature preservation, leaving a critical research gap in their comparative performance evaluation. This study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and U-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These models were chosen due to their relevance in the literature and code accessibility. The evaluation reveals that U-Net achieves high noise suppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and +364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality, attaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it well-suited for applications prioritizing natural and intelligible speech. Wave-U-Net balances these attributes with improvements in speaker-specific feature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and +27.38% on VPQAD. This research indicates how advanced methods can optimize trade-offs between noise suppression, perceptual quality, and speaker recognition. The findings may contribute to advancing voice biometrics, forensic audio analysis, telecommunication, and speaker verification in challenging acoustic conditions. </p>
<blockquote>
<p>语音增强，尤其是去噪，在改善真实世界应用中语音信号的清晰度和质量方面至关重要，特别是在嘈杂的环境中。虽然之前的研究已经引入了各种深度学习模型来实现这一目标，但许多模型在平衡噪声抑制、感知质量和说话人特定特征保留方面存在困难，使得它们在性能比较评估方面存在关键的研究空白。本研究对Wave-U-Net、CMGAN和U-Net三种前沿模型进行了评估，使用的数据集包括SpEAR、VPQAD和Clarkson数据集等多样化数据集。这些模型之所以被选中，是因为它们在文献中的相关性和代码的可访问性。评估结果表明，U-Net在SpEAR数据集上实现了+71.96%、在VPQAD数据集上实现了+64.83%、在Clarkson数据集上实现了+364.2%的信噪比提高，取得了很高的噪声抑制效果。CMGAN在感知质量方面表现突出，在SpEAR和VPQAD上分别获得了4.04和1.46的PESQ高分，使其成为优先考虑自然和可理解语音的应用的理想选择。Wave-U-Net在说话人特定特征保留方面有所提高，平衡了这些特点，这在SpEAR和VPQAD的VeriSpeak得分增长+10.84%和+27.38%中得到了证明。这项研究表明，先进的方法如何优化噪声抑制、感知质量和说话人识别之间的权衡。这些发现可能有助于推动语音生物识别、音频分析鉴定、电信和具有挑战性的声学条件下的说话人验证的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了三种先进的语音增强模型（Wave-U-Net、CMGAN和U-Net）在噪声环境下的性能表现。U-Net在噪声抑制方面表现优秀，特别是在SpEAR、VPQAD和Clarkson数据集上的信号噪声比（SNR）提升显著。CMGAN在感知质量方面表现最佳，适合需要自然和可理解语音的应用。Wave-U-Net则在平衡噪声抑制、感知质量和说话人特征保留方面表现出优势。研究对于优化这些模型在噪声环境下的性能具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音增强在噪声环境中对改善语音信号的清晰度和质量至关重要。</li>
<li>U-Net在噪声抑制方面表现突出，特别是在多个数据集上的SNR提升显著。</li>
<li>CMGAN在感知质量方面最佳，适用于需要自然和可理解语音的应用场景。</li>
<li>Wave-U-Net在平衡噪声抑制、感知质量和说话人特征保留方面具备优势。</li>
<li>说话人特征保留对于语音生物识别、语音验证等应用至关重要。</li>
<li>研究结果有助于深入了解不同模型在噪声环境下的性能差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15000">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-718cd9af1d414398cb3bbec416541c32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ca0de073f71656e88c2b989e8005ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f99deaec6f819b5be47523f0db94d770.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f2f6a99fb8fd0adebede96f8d216bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c255786f5c63e2c0eaefff0fe5ca9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31fb8a1d4257e26eea842eff00a512db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3eb533005527d15716ad1e6ad10a6204.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Variational-Framework-for-Improving-Naturalness-in-Generative-Spoken-Language-Models"><a href="#A-Variational-Framework-for-Improving-Naturalness-in-Generative-Spoken-Language-Models" class="headerlink" title="A Variational Framework for Improving Naturalness in Generative Spoken   Language Models"></a>A Variational Framework for Improving Naturalness in Generative Spoken   Language Models</h2><p><strong>Authors:Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky</strong></p>
<p>The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at <a target="_blank" rel="noopener" href="https://github.com/b04901014/vae-gslm">https://github.com/b04901014/vae-gslm</a>. </p>
<blockquote>
<p>文本处理中大型语言模型的成功激发了其在语音建模中的应用。然而，由于语音是连续且复杂的，通常需要进行离散化以进行自回归建模。从自监督模型派生的语音令牌（称为语义令牌）主要关注语音的语言方面，但忽略了韵律信息。因此，在这些令牌上训练的模型生成的语音会降低自然度。现有方法试图通过向语义令牌添加音调特征来解决这个问题。然而，单独的音调不能完全代表副语言属性的范围，并且选择正确的特征需要仔细的手工设计。为了克服这一问题，我们提出了一种端到端的变分方法，该方法能够自动学习编码这些连续的语音属性以增强语义令牌。我们的方法消除了手动提取和选择副语言特征的需求。此外，它根据人类评估者产生了更受欢迎的语音延续。代码、样本和模型可在<a target="_blank" rel="noopener" href="https://github.com/b04901014/vae-gslm%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/b04901014/vae-gslm获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14767v1">PDF</a> International Conference on Machine Learning (ICML) 2025</p>
<p><strong>总结</strong><br>基于大型语言模型在文本处理方面的成功，人们尝试将其应用于语音建模。然而，由于语音的连续性和复杂性，通常需要进行离散化以进行自回归建模。从自监督模型派生的语音令牌（称为语义令牌）主要关注语音的语言方面，但忽略了韵律信息。因此，在这些令牌上训练的模型生成的语音自然度降低。现有方法试图通过向语义令牌添加音高特征来解决这一问题。然而，音高无法完全代表多种副语言属性，选择合适的特征需要精心手工设计。为了克服这一问题，我们提出了一种端到端的变分方法，该方法可自动学习编码这些连续的语音属性以增强语义令牌。我们的方法消除了手动提取和选择副语言特征的需求。此外，根据人类评分者的评估，它产生了更受欢迎的语音连续内容。相关代码、样本和模型可在 <a target="_blank" rel="noopener" href="https://github.com/b04901014/vae-gslm">https://github.com/b04901014/vae-gslm</a> 获得。</p>
<p><strong>要点</strong></p>
<ol>
<li>大型语言模型在文本处理中的成功推动了其在语音建模中的应用。</li>
<li>语音由于其连续性和复杂性，常常需要离散化以进行自回归建模。</li>
<li>现有研究中常用的语音令牌忽略了韵律信息，影响生成的语音的自然度。</li>
<li>现有方法试图通过向语义令牌添加音高特征来解决语音自然度问题。但音高不足以代表所有副语言属性且特征选择需要精细的手工设计。</li>
<li>提出了一种端到端的变分方法，自动学习编码连续的语音属性以增强语义令牌。这一方法无需手动提取和选择副语言特征。</li>
<li>与现有方法相比，该方法生成的语音内容更受欢迎，根据人类评分者的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-eb6e1d595694baa1c25187dab0b74700.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa19d6b7e3e1b1aa58805a0cdcdb8d9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd19fdd04901fd07488e5dd78bfb7e42.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SyncTalk-High-Fidelity-and-Efficient-Synchronized-Talking-Heads-Synthesis-Using-Gaussian-Splatting"><a href="#SyncTalk-High-Fidelity-and-Efficient-Synchronized-Talking-Heads-Synthesis-Using-Gaussian-Splatting" class="headerlink" title="SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads   Synthesis Using Gaussian Splatting"></a>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads   Synthesis Using Gaussian Splatting</h2><p><strong>Authors:Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, Zhaoxin Fan</strong></p>
<p>Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ‘’devil’’ in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: <a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk++">https://ziqiaopeng.github.io/synctalk++</a>. </p>
<blockquote>
<p>在合成逼真的语音驱动式说话人头视频时，实现高同步率是一个巨大的挑战。一个栩栩如生的说话人头部需要同步协调主体身份、嘴唇动作、面部表情和头部姿态。缺乏这些同步是一个基本缺陷，会导致不真实的结果。为了解决同步这一关键问题，我们将其视为创建逼真说话头部的“魔鬼”，并引入了SyncTalk++。它具备动态肖像渲染器和高斯拼贴技术，确保主体身份的一致保留，以及面部同步控制器，该控制器使嘴唇动作与语音对齐，同时创新地使用3D面部混合形状模型来重建准确的面部表情。为了确保自然的头部动作，我们提出了头部同步稳定器，它优化了头部姿态，提高了稳定性。此外，SyncTalk++通过融入表情生成器和躯干恢复器，增强了处理离群值（OOD）音频的稳健性，这两个组件可生成与语音相匹配的表情和无缝的躯干区域。我们的方法保持了跨帧的视觉细节的一致性和连续性，并显著提高了渲染速度和品质，达到了每秒101帧。大量的实验和用户研究表明，SyncTalk++在同步和逼真度方面超过了最新技术方法。我们推荐观看补充视频：<a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk++.html">https://ziqiaopeng.github.io/synctalk++.html</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14742v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了SyncTalk++技术，该技术用于合成高度同步的、以语音驱动的真实谈话视频。它包含动态肖像渲染器、面部同步控制器和头部同步稳定器等多个组件，确保主体身份、嘴唇动作、面部表情和头部姿态的同步协调。此外，SyncTalk++还提高了对离群音频的稳健性，并通过表达生成器和躯干恢复器等技术，生成与语音匹配的面部表情和无缝的躯干区域。SyncTalk++提高了渲染速度和视频质量，在同步和逼真度方面超过了现有技术。建议观看补充视频以获取更多详细信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncTalk++技术旨在合成高度同步的、以语音驱动的谈话视频，确保主体身份、嘴唇动作和面部表情的同步协调。</li>
<li>包含动态肖像渲染器，采用高斯涂抹技术以确保一致的主体身份保留。</li>
<li>引入面部同步控制器，将嘴唇动作与语音对齐，并使用3D面部blendshape模型重建准确的面部表情。</li>
<li>提出头部同步稳定器，优化头部姿态以实现更大的稳定性。</li>
<li>SyncTalk++通过表达生成器和躯干恢复器等技术提高了对离群音频的稳健性，并生成与语音匹配的面部表情和无缝的躯干区域。</li>
<li>SyncTalk++可以维持跨帧的视觉细节的一致性和连续性，并提高渲染速度和视频质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14742">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-15c66b633743e877bdd3801913d82173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037c4b1b81c911d27bb468c64ce77801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c30f4d6defa9eea186e6506bc2cf0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff62d47e48bac53ae737a1a6efbdfb97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a0490a95147fedee062bab74ef0f2b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Investigation-of-Zero-shot-Text-to-Speech-Models-for-Enhancing-Short-Utterance-Speaker-Verification"><a href="#Investigation-of-Zero-shot-Text-to-Speech-Models-for-Enhancing-Short-Utterance-Speaker-Verification" class="headerlink" title="Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification"></a>Investigation of Zero-shot Text-to-Speech Models for Enhancing   Short-Utterance Speaker Verification</h2><p><strong>Authors:Yiyang Zhao, Shuai Wang, Guangzhi Sun, Zehua Chen, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</strong></p>
<p>Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research. </p>
<blockquote>
<p>短时长说话人验证面临着巨大的挑战，由于简短语音片段中的信息有限，这可能会降低准确性和可靠性。最近，零样本文本到语音（ZS-TTS）系统在保持说话人身份方面取得了重大进展。在这项研究中，我们首次探索了ZS-TTS系统在测试时间数据增强在说话人验证中的应用。我们在VoxCeleb 1数据集上评估了三种最先进的预训练ZS-TTS系统，包括NatureSpeech 3、CosyVoice和MaskGCT。我们的实验结果表明，结合真实和合成语音样本在所有时长上导致相对等错误率（EER）降低了10%-16%，对于短时长语音的改进尤为显著，且无需对任何现有系统进行再训练。然而，我们的分析表明，较长的合成语音在降低EER方面并没有带来与较长真实语音相同的益处。这些发现突显了使用ZS-TTS进行测试时间说话人验证的潜力和挑战，为未来的研究提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了零样本文本到语音（ZS-TTS）系统在测试时间数据增强中的首次应用在短讲话验证中的应用。在VoxCeleb 1数据集上评估了三个最新的预训练ZS-TTS系统（NatureSpeech 3、CosyVoice和MaskGCT）。结合真实和合成语音样本后，相对等误率（EER）在所有时长上降低了10%-16%，特别是短语的改进尤为显著，且无需对现有系统进行任何重新训练。然而，分析表明，较长的合成语音在降低EER方面并未产生与较长真实语音相同的好处。该研究突出了使用ZS-TTS在测试时间说话人验证中的潜力和挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>短语说话人验证面临由于信息有限而导致的准确性和可靠性问题。</li>
<li>零样本文本到语音（ZS-TTS）系统在保留说话人身份方面取得了显著进步。</li>
<li>研究首次探索了将ZS-TTS系统应用于测试时间数据增强在说话人验证中的潜力。</li>
<li>在VoxCeleb 1数据集上评估的三个预训练ZS-TTS系统显示，结合真实和合成语音样本可以降低等误率（EER）。</li>
<li>合成语音在短讲话验证中的改进尤为显著，且无需对现有系统进行重新训练。</li>
<li>较长的合成语音在降低EER方面并未产生与较长真实语音相同的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14226">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bf630bdfad1b39c38b5704284feaf182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0256bd1c03b59d535cb3b3a488529cf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47e17273cb8b2824416b560dc49e469e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40c944d44b370ec97fbe32fd18147d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5785249a9d34d296c0bd728b4543513.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SpeechRefiner-Towards-Perceptual-Quality-Refinement-for-Front-End-Algorithms"><a href="#SpeechRefiner-Towards-Perceptual-Quality-Refinement-for-Front-End-Algorithms" class="headerlink" title="SpeechRefiner: Towards Perceptual Quality Refinement for Front-End   Algorithms"></a>SpeechRefiner: Towards Perceptual Quality Refinement for Front-End   Algorithms</h2><p><strong>Authors:Sirui Li, Shuai Wang, Zhijun Liu, Zhongjie Jiang, Yannan Wang, Haizhou Li</strong></p>
<p>Speech pre-processing techniques such as denoising, de-reverberation, and separation, are commonly employed as front-ends for various downstream speech processing tasks. However, these methods can sometimes be inadequate, resulting in residual noise or the introduction of new artifacts. Such deficiencies are typically not captured by metrics like SI-SNR but are noticeable to human listeners. To address this, we introduce SpeechRefiner, a post-processing tool that utilizes Conditional Flow Matching (CFM) to improve the perceptual quality of speech. In this study, we benchmark SpeechRefiner against recent task-specific refinement methods and evaluate its performance within our internal processing pipeline, which integrates multiple front-end algorithms. Experiments show that SpeechRefiner exhibits strong generalization across diverse impairment sources, significantly enhancing speech perceptual quality. Audio demos can be found at <a target="_blank" rel="noopener" href="https://speechrefiner.github.io/SpeechRefiner/">https://speechrefiner.github.io/SpeechRefiner/</a>. </p>
<blockquote>
<p>语音预处理技术，如降噪、去混响和分离等，通常被用作各种下游语音处理任务的前端。然而，这些方法有时可能不够充分，导致残留噪声或引入新的失真。这些缺陷通常不会被SI-SNR等指标所捕获，但对于人类听众来说是显而易见的。为了解决这一问题，我们引入了SpeechRefiner，这是一种后处理工具，它利用条件流匹配（CFM）来提高语音的感知质量。在这项研究中，我们将SpeechRefiner与最新的特定任务细化方法进行了基准测试，并在我们的集成多个前端算法的内部处理管道中评估了其性能。实验表明，SpeechRefiner在不同来源的缺陷上具有强大的泛化能力，显著提高了语音感知质量。音频演示可在<a target="_blank" rel="noopener" href="https://speechrefiner.github.io/SpeechRefiner/%E6%89%BE%E5%88%B0%E3%80%82">https://speechrefiner.github.io/SpeechRefiner/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13709v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong>：为提高语音感知质量，推出了一种名为SpeechRefiner的后处理工具，采用条件流匹配（CFM）技术，针对语音预处理技术如降噪、去混响和分离等存在的缺陷进行改善。相较于近期特定的任务优化方法，SpeechRefiner在内部处理管道中表现出强大的泛化能力，能应对多种不同的损伤来源，显著提升语音感知质量。详情可访问<a target="_blank" rel="noopener" href="https://speechrefiner.github.io/SpeechRefiner/">链接</a>了解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>语音预处理技术如降噪、去混响和分离等是下游语音处理任务的前端，但存在不足，可能产生残余噪声或新引入的伪迹。</li>
<li>SpeechRefiner是一种后处理工具，采用条件流匹配（CFM）技术改善语音感知质量。</li>
<li>SpeechRefiner相较于近期特定的任务优化方法表现出强大的泛化能力。</li>
<li>SpeechRefiner能有效应对多种不同的语音损伤来源。</li>
<li>SpeechRefiner能显著提升语音感知质量，其效果可通过音频演示进行展示。</li>
<li>SpeechRefiner工具的内部处理管道集成了多种前端算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13709">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d3c15e194272cc3905112f6c2227694.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-659f9c86a44e4ad269b33433bffc369e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f588b44fdcf58117cfb4a3c426c37b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d2b2fcbe075f4b82e53bf59a19514b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems"><a href="#Qwen-vs-Gemma-Integration-with-Whisper-A-Comparative-Study-in-Multilingual-SpeechLLM-Systems" class="headerlink" title="Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems"></a>Qwen vs. Gemma Integration with Whisper: A Comparative Study in   Multilingual SpeechLLM Systems</h2><p><strong>Authors:Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran</strong></p>
<p>This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER&#x2F;CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model. </p>
<blockquote>
<p>本文介绍了我们为MLC-SLM挑战2025设计的系统，主要侧重于使用大型语言模型（LLM）进行多语种语音识别和语言建模。我们的方法结合了经过微调后的Whisper-large-v3编码器、高效的投影仪架构和各种解码器配置。我们采用三阶段训练方法，逐步优化编码器、投影仪和LLM组件。我们的系统使用仅解码器语言模型Gemma3-12B取得了具有竞争力的性能表现，平均单词错误率（WER）&#x2F;字符错误率（CER）为16.63%，使用Qwen2.5-7B时的性能为18.6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13596v1">PDF</a> Technical report for Interspeech 2025 MLC-SLM Challenge</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对MLC-SLM挑战2025的系统，重点研究多语种语音识别和语言建模。本系统结合了微调过的Whisper-large-v3编码器、高效投影架构以及多种解码器配置。采用三阶段训练方法，逐步优化编码器、投影器和大模型组件。系统在仅使用解码器语言模型的情况下取得了具有竞争力的性能表现，使用Gemma3-12B模型时的私有测试平均WER&#x2F;CER结果为16.63%，使用Qwen2.5-7B模型时为18.6%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>系统针对MLC-SLM挑战，研究多语种语音识别和语言建模。</li>
<li>结合微调过的编码器、高效投影架构和多种解码器配置。</li>
<li>采用三阶段训练方法优化系统组件。</li>
<li>系统使用解码器语言模型取得了具有竞争力的性能表现。</li>
<li>使用Gemma3-12B模型时，私有测试平均WER&#x2F;CER结果为16.63%。</li>
<li>使用Qwen2.5-7B模型时，性能表现略有不同。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67c7f29208e5db1d3189c55444a8a5a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57bccb9c9e9ac3d118a4f499a1755bb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b1454b2becbe3a864b2539edd863f8e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BUT-System-for-the-MLC-SLM-Challenge"><a href="#BUT-System-for-the-MLC-SLM-Challenge" class="headerlink" title="BUT System for the MLC-SLM Challenge"></a>BUT System for the MLC-SLM Challenge</h2><p><strong>Authors:Alexander Polok, Jiangyu Han, Dominik Klement, Samuele Cornell, Jan Černocký, Lukáš Burget</strong></p>
<p>We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW – a diarization-conditioned variant of Whisper – with DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate both systems in out-of-domain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whisper’s multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves a micro-average tcpWER&#x2F;CER of 16.75% and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training data – such as missing speech segments and incorrect silence annotations – which can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness. </p>
<blockquote>
<p>我们提出了一种结合DiCoW（一种基于Whisper的说话人分割条件变体）和DiariZen（一种基于Pyannote的说话人分割管道）的双人自动语音识别（ASR）系统。我们首先在没有进行任何微调的情况下，在跨领域（OOD）多语言场景中对两个系统进行了评估。在这种情况下，DiariZen始终优于基准Pyannote说话人分割模型，显示出强大的泛化能力。尽管DiCoW是针对目标说话人ASR的英语唯一数据进行微调，但它仍具有强大的多语言能力，表明编码器修改保留了Whisper的多语言能力。然后我们对DiCoW和DiariZen进行了MLC-SLM挑战数据的微调。经过微调后，DiariZen继续超越经过微调Pyannote基准线，而DiCoW从域适应中看到了进一步的收益。我们的最终系统达到了16.75%的微观平均tcpWER&#x2F;CER，并在MLC-SLM挑战的第二项任务中获得了第二名。最后，我们发现了训练数据中存在若干标签不一致的问题，如缺失语音片段和错误的静音注释等，这些问题可能会阻碍说话人分割的微调。我们提出了一些简单的缓解策略来解决这些问题并提高系统的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合DiCoW和DiariZen的两人自动语音识别（ASR）系统。在跨领域多语言场景下，DiariZen表现出强大的泛化能力，相较于基线Pyannote模型有更好的表现。DiCoW在不需要针对目标说话者ASR进行微调的情况下，仍能保持强大的多语言能力。在MLC-SLM挑战数据上进行微调后，DiariZen继续优于经过调整的Pyannote模型，而DiCoW从领域适应中获得了进一步的提升。最终系统取得了第二名的成绩。同时，文章还指出了训练数据中存在的一些标注不一致问题，并提出了简单的缓解策略来提高系统的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了结合DiCoW和DiariZen的两人自动语音识别（ASR）系统。</li>
<li>DiariZen在跨领域多语言场景下表现出强大的泛化能力，优于基线Pyannote模型。</li>
<li>DiCoW具有强大的多语言能力，在不需要针对目标说话者ASR进行微调的情况下仍能良好运行。</li>
<li>经过在MLC-SLM挑战数据上的微调，DiariZen继续优于Pyannote模型，而DiCoW获得了进一步的性能提升。</li>
<li>最终系统取得了第二名的好成绩。</li>
<li>文中指出了训练数据存在的标注不一致问题，如缺失语音段和错误的静音注释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3d961b0ed84d90b59a8840fc58c5ec1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0751c4778b7ae3e6b4ea2e41d7ed4de3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e8b187c05f575212dad0760b4de00b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-333edc45a39f4c88949319aaf5c1c8ba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR"><a href="#Bi-directional-Context-Enhanced-Speech-Large-Language-Models-for-Multilingual-Conversational-ASR" class="headerlink" title="Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR"></a>Bi-directional Context-Enhanced Speech Large Language Models for   Multilingual Conversational ASR</h2><p><strong>Authors:Yizhou Peng, Hexin Liu, Eng Siong Chng</strong></p>
<p>This paper introduces the integration of language-specific bi-directional context into a speech large language model (SLLM) to improve multilingual continuous conversational automatic speech recognition (ASR). We propose a character-level contextual masking strategy during training, which randomly removes portions of the context to enhance robustness and better emulate the flawed transcriptions that may occur during inference. For decoding, a two-stage pipeline is utilized: initial isolated segment decoding followed by context-aware re-decoding using neighboring hypotheses. Evaluated on the 1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM) corpus covering eleven languages, our method achieves an 18% relative improvement compared to a strong baseline, outperforming even the model trained on 6000 hours of data for the MLC-SLM competition. These results underscore the significant benefit of incorporating contextual information in multilingual continuous conversational ASR. </p>
<blockquote>
<p>本文介绍了将特定语言的双向上下文集成到语音大语言模型（SLLM）中，以提高跨语言连续对话自动语音识别（ASR）的效果。我们提出了一种字符级上下文掩码策略，在训练过程中随机移除部分上下文，以增强模型的稳健性并更好地模拟推理过程中可能出现的错误转录。在解码方面，我们采用了两阶段流程：首先是初步的独立分段解码，然后是使用相邻假设进行上下文感知的重新解码。在涵盖十一种语言的1500小时多语言对话语音和语言模型（MLC-SLM）语料库上进行了评估，我们的方法与强大的基线相比，实现了18%的相对改进，甚至超越了为MLC-SLM竞赛训练的6000小时模型的性能。这些结果强调了在多语言连续对话ASR中加入上下文信息的重大益处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13396v1">PDF</a> Submitted to Interspeech 2025 MLC-SLM workshop as a Research Paper</p>
<p><strong>总结</strong><br>    本文介绍了一种将语言特定双向上下文集成到语音大语言模型（SLLM）中的方法，以改进多语种连续对话自动语音识别（ASR）。通过训练过程中的字符级上下文掩码策略，随机移除部分上下文以增强模型的稳健性，并更好地模拟推理过程中可能出现的错误转录。解码采用两阶段流程：先进行初步的独立分段解码，然后使用相邻假设进行上下文感知的再解码。在涵盖11种语言的Multilingual Conversational Speech and Language Model（MLC-SLM）语料库上进行评估，该方法相对于表现良好的基线模型实现了相对改进率18%，甚至在MLC-SLM竞赛中表现优于经过6000小时数据训练的模型。这些结果突显了在多语种连续对话ASR中融入上下文信息的显著优势。</p>
<p><strong>要点</strong></p>
<ol>
<li>引入语言特定双向上下文到语音大语言模型中，旨在改进多语种连续对话自动语音识别（ASR）。</li>
<li>训练过程中采用字符级上下文掩码策略，随机移除部分上下文以增强模型的稳健性。</li>
<li>通过模拟推理过程中可能出现的错误转录，提高模型的实用性。</li>
<li>采用两阶段解码流程：初步独立分段解码后，进行上下文感知的再解码。</li>
<li>在MLC-SLM语料库上的实验表明，该方法相对于基线模型有显著的相对改进率。</li>
<li>与经过更多数据训练的模型相比，该方法在多语种连续对话ASR任务上表现出优异的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-70f64c60e4bba32d746f5eb09692a8d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a8daa33f63b024de5dc584119aca715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad174257bce8b6b417541d9d214da64e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334378040cf240dde107c32dc4077074.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f310ab047a401019abe0923b9cbadf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab894683560554c8e9dfd01b5ca6e9b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025"><a href="#NTU-Speechlab-LLM-Based-Multilingual-ASR-System-for-Interspeech-MLC-SLM-Challenge-2025" class="headerlink" title="NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025"></a>NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM   Challenge 2025</h2><p><strong>Authors:Yizhou Peng, Bin Wang, Yi-Wen Chao, Ziyang Ma, Haoyang Zhang, Hexin Liu, Xie Chen, Eng Siong Chng</strong></p>
<p>This report details the NTU Speechlab system developed for the Interspeech 2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge (Task I), where we achieved 5th place. We present comprehensive analyses of our multilingual automatic speech recognition system, highlighting key advancements in model architecture, data selection, and training strategies. In particular, language-specific prompts and model averaging techniques were instrumental in boosting system performance across diverse languages. Compared to the initial baseline system, our final model reduced the average Mix Error Rate from 20.2% to 10.6%, representing an absolute improvement of 9.6% (a relative improvement of 48%) on the evaluation set. Our results demonstrate the effectiveness of our approach and offer practical insights for future Speech Large Language Models. </p>
<blockquote>
<p>本报告详细介绍了为Interspeech 2025多语种对话语音和语言模型（MLC-SLM）挑战赛（任务一）开发的NTU Speechlab系统，我们在比赛中取得了第五名。我们对我们的多语种自动语音识别系统进行了全面的分析，重点介绍了模型架构、数据选择和训练策略方面的关键进展。特别是，语言特定的提示和模型平均技术对提高系统在多种语言中的性能起到了关键作用。与初始基线系统相比，我们的最终模型将平均混合错误率从20.2%降低到10.6%，在评估集上实现了绝对改进9.6%（相对改进48%）。我们的结果证明了我们的方法的有效性，并为未来的语音大语言模型提供了实际见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13339v1">PDF</a> Submitted to Interspeech 2025 MLC-SLM challenge (5th place). System   report</p>
<p><strong>Summary</strong><br>这是一份关于NTU Speechlab系统为Interspeech 2025多语种对话语音和语言模型（MLC-SLM）挑战任务设计的详细报告。本报告全面分析了我们的多语种自动语音识别系统的主要进步，如模型架构、数据选择及训练策略等。与初始基线系统相比，最终模型在测试集上将平均混合错误率降低了近一半，这体现了该方法的有效性。它为未来大型语言模型的构建提供了实用的见解。该系统成功在比赛中取得了第五名的好成绩。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于文本内容的七个关键见解：</p>
<ol>
<li>NTU Speechlab系统针对Interspeech 2025的多语种对话语音和语言模型挑战进行了系统设计。</li>
<li>系统分析涵盖了模型架构、数据选择和训练策略等重要方面。</li>
<li>语言特定提示和模型平均技术对于提高多语种系统的性能起到了关键作用。</li>
<li>与初始基线系统相比，最终模型的平均混合错误率降低了9.6%，相对改善率为48%。</li>
<li>该系统在比赛中取得了第五名的好成绩。</li>
<li>此方法的有效性为未来的语音大型语言模型构建提供了实践指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a73e8debaa3054eeedc8dbf0a7f8915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ecc8b2b371ccb62d563a94527a98f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a18d47d7e6084887dd915f1515a91f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a8bac7ecc1887d23adeb1ac8ec6f313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bbab8169e4838e32d244cc729207270.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d61636e3923645158f9918dc9dfe00a3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Seewo’s-Submission-to-MLC-SLM-Lessons-learned-from-Speech-Reasoning-Language-Models"><a href="#Seewo’s-Submission-to-MLC-SLM-Lessons-learned-from-Speech-Reasoning-Language-Models" class="headerlink" title="Seewo’s Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models"></a>Seewo’s Submission to MLC-SLM: Lessons learned from Speech Reasoning   Language Models</h2><p><strong>Authors:Bo Li, Chengben Xu, Wufeng Zhang</strong></p>
<p>This paper presents Seewo’s systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER&#x2F;CER of 11.57% for Track 1 and a tcpWER&#x2F;tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints. </p>
<blockquote>
<p>本文介绍了Seewo在多元语言对话语音语言模型挑战（MLC-SLM）的两个赛道中的系统，涵盖了自动语音识别（ASR）和带有ASR的说话人日记化（SD-ASR）。我们引入了一个多阶段训练流程，该流程在语音语言模型中显式增强推理和自我校正功能，以用于ASR。我们的方法结合了课程学习以逐步获取能力、思维链数据增强以促进中间反思，以及可验证奖励强化学习（RLVR），通过奖励驱动优化进一步改进自我校正。该方法在官方挑战基线的基础上取得了重大改进。在评估集上，我们最好的系统在第1赛道上达到了11.57%的WER&#x2F;CER，在第2赛道上达到了17.67%的tcpWER&#x2F;tcpCER。全面的消融研究证明了每个组件在挑战约束下的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13300v3">PDF</a> </p>
<p><strong>Summary</strong>:<br>本文介绍了Seewo在Multilingual Conversational Speech Language Model Challenge（MLC-SLM）中的两个赛道（自动语音识别（ASR）和带有ASR的说话人分块化（SD-ASR））的系统。文章提出了一种多阶段训练管道，旨在提高语音语言模型中的推理和自纠错能力。结合课程学习实现渐进能力获取，利用Chain-of-Thought数据进行增强以推动中间阶段的反思，并通过强化学习与可验证奖励（RLVR）进一步通过奖励驱动优化进行自纠错。此方法相较于官方挑战基线有显著改善，最佳系统在评估集上的表现达到Track 1的WER&#x2F;CER为11.57%，Track 2的tcpWER&#x2F;tcpCER为17.67%。全面的消融研究证明了在挑战约束下每个组件的有效性。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Seewo提出的多阶段训练管道用于增强语音语言模型的推理和自纠错能力。</li>
<li>通过课程学习实现渐进能力获取。</li>
<li>采用Chain-of-Thought数据增强推动中间阶段反思。</li>
<li>强化学习与可验证奖励（RLVR）用于优化自纠错能力。</li>
<li>该方法在多语种对话语音语言模型挑战中实现了显著成果，较官方基线有显著改善。</li>
<li>最佳系统表现优异，Track 1的WER&#x2F;CER为11.57%，Track 2的tcpWER&#x2F;tcpCER为17.67%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13300">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4699484f27576657f63c1b56aa36d56f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fec32241e658ccd300ca24eac739ee88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5858e1fab9c34b9b9d8c086975b8b30e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c05416f6933a7297c49f6794654285a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2fc3b4c606bb924858746a864f0df6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420e87a51ff6e652b7bdbf25534b5ce2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="I-2-S-TFCKD-Intra-Inter-Set-Knowledge-Distillation-with-Time-Frequency-Calibration-for-Speech-Enhancement"><a href="#I-2-S-TFCKD-Intra-Inter-Set-Knowledge-Distillation-with-Time-Frequency-Calibration-for-Speech-Enhancement" class="headerlink" title="I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency   Calibration for Speech Enhancement"></a>I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency   Calibration for Speech Enhancement</h2><p><strong>Authors:Jiaming Cheng, Ruiyu Liang, Chao Xu, Ye Ni, Wei Zhou, Björn W. Schuller, Xiaoshuai Hao</strong></p>
<p>In recent years, complexity compression of neural network (NN)-based speech enhancement (SE) models has gradually attracted the attention of researchers, especially in scenarios with limited hardware resources or strict latency requirements. The main difficulties and challenges lie in achieving a balance between complexity and performance according to the characteristics of the task. In this paper, we propose an intra-inter set knowledge distillation (KD) framework with time-frequency calibration (I$^2$S-TFCKD) for SE. Different from previous distillation strategies for SE, the proposed framework fully utilizes the time-frequency differential information of speech while promoting global knowledge flow. Firstly, we propose a multi-layer interactive distillation based on dual-stream time-frequency cross-calibration, which calculates the teacher-student similarity calibration weights in the time and frequency domains respectively and performs cross-weighting, thus enabling refined allocation of distillation contributions across different layers according to speech characteristics. Secondly, we construct a collaborative distillation paradigm for intra-set and inter-set correlations. Within a correlated set, multi-layer teacher-student features are pairwise matched for calibrated distillation. Subsequently, we generate representative features from each correlated set through residual fusion to form the fused feature set that enables inter-set knowledge interaction. The proposed distillation strategy is applied to the dual-path dilated convolutional recurrent network (DPDCRN) that ranked first in the SE track of the L3DAS23 challenge. Objective evaluations demonstrate that the proposed KD strategy consistently and effectively improves the performance of the low-complexity student model and outperforms other distillation schemes. </p>
<blockquote>
<p>近年来，神经网络（NN）基于的语音增强（SE）模型的复杂度压缩逐渐引起了研究人员的关注，特别是在硬件资源有限或延迟要求严格的情况下。主要的困难和挑战在于根据任务特点在复杂度和性能之间取得平衡。针对这一问题，本文提出了一种基于时间频率校准的组内组间知识蒸馏（I$^2$S-TFCKD）框架，用于语音增强。不同于以前的SE知识蒸馏策略，所提出的框架充分利用了语音的时间频率差异信息，同时促进了全局知识流。首先，我们提出了一种基于双流时间频率交叉校准的多层交互蒸馏方法，该方法分别在时间和频率域计算教师学生相似度校准权重，并执行交叉加权，从而能够根据语音特征在不同的层之间精细分配蒸馏贡献。其次，我们构建了组内和组间相关性的协作蒸馏模式。在相关集合内，多层教师学生特征是成对匹配的，进行校准蒸馏。随后，我们通过残差融合生成每个相关集合的代表特征，形成融合特征集，从而实现组间知识交互。所提出的知识蒸馏策略应用于双路径膨胀卷积循环网络（DPDCRN），在L3DAS23挑战的SE赛道中排名第一。客观评估表明，所提出的知识蒸馏策略持续有效地提高了低复杂度学生模型的性能，并优于其他蒸馏方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13127v1">PDF</a> submitted to IEEE Transactions on Neural Networks and Learning   Systems</p>
<p><strong>摘要</strong></p>
<p>近年来，基于神经网络（NN）的语音增强（SE）模型的复杂度压缩逐渐引起研究人员的关注，特别是在硬件资源有限或延迟要求严格的情况下。主要难点和挑战在于根据任务特点在复杂度和性能之间取得平衡。本文提出了一种用于SE的基于时间频率校准的内外集合知识蒸馏（KD）框架（I$^2$S-TFCKD）。不同于以前的SE蒸馏策略，该框架充分利用语音的时间频率差异信息，同时促进全局知识流。首先，我们提出了一种基于双流时间频率交叉校准的多层交互蒸馏，分别计算时间和频率域的教师学生相似度校准权重，并执行交叉加权，从而根据语音特点在不同层上精细分配蒸馏贡献。其次，我们构建了用于内部和外部集合关系的协同蒸馏范式。在相关集合内，对多层教师学生特征进行配对校准蒸馏。然后我们通过残差融合生成各相关集合的特征表示，形成融合特征集，实现集合间的知识交互。所提出的蒸馏策略应用于双路径膨胀卷积循环网络（DPDCRN），在L3DAS23挑战的SE赛道中排名第一。客观评估表明，所提出KD策略持续有效地提高了低复杂度学生模型的性能并超越了其他蒸馏方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>神经网络的语音增强模型复杂度压缩是近年来的研究热点，特别是在硬件资源受限或延迟要求严格的场景下。</li>
<li>论文提出了一种新的知识蒸馏框架I$^2$S-TFCKD，用于SE模型，该框架能充分利用语音的时间频率差异信息。</li>
<li>论文采用多层交互蒸馏方法，基于双流时间频率交叉校准，根据语音特点在不同层上分配蒸馏贡献。</li>
<li>论文构建了内外集合关系的协同蒸馏范式，实现了集合间的知识交互。</li>
<li>提出的蒸馏策略被应用于DPDCRN模型，并在L3DAS23挑战的SE赛道中取得了第一名。</li>
<li>客观评估显示，该蒸馏策略提高了低复杂度学生模型的性能并超越了其他蒸馏方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b376d5d90d55b25232cf44c5a172f4de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f23ec14b3090a4d7ee519d5d69b1494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61d03227d24a13906e2d3714a6e709c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83e7264d24da36bbf7a97f750e00178f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c98a1559c05e8836692b665767323b29.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SSLAM-Enhancing-Self-Supervised-Models-with-Audio-Mixtures-for-Polyphonic-Soundscapes"><a href="#SSLAM-Enhancing-Self-Supervised-Models-with-Audio-Mixtures-for-Polyphonic-Soundscapes" class="headerlink" title="SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for   Polyphonic Soundscapes"></a>SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for   Polyphonic Soundscapes</h2><p><strong>Authors:Tony Alex, Sara Ahmed, Armin Mustafa, Muhammad Awais, Philip JB Jackson</strong></p>
<p>Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the SSL pre-training has sufficiently equipped them to handle real-world audio. However, a critical question remains: how well do these models actually perform in real-world conditions, where audio is typically polyphonic and complex, involving multiple overlapping sound sources? Current audio SSL methods are often benchmarked on datasets predominantly featuring monophonic audio, such as environmental sounds, and speech. As a result, the ability of SSL models to generalize to polyphonic audio, a common characteristic in natural scenarios, remains underexplored. This limitation raises concerns about the practical robustness of SSL models in more realistic audio settings. To address this gap, we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel direction in audio SSL research, designed to improve, designed to improve the model’s ability to learn from polyphonic data while maintaining strong performance on monophonic data. We thoroughly evaluate SSLAM on standard audio SSL benchmark datasets which are predominantly monophonic and conduct a comprehensive comparative analysis against SOTA methods using a range of high-quality, publicly available polyphonic datasets. SSLAM not only improves model performance on polyphonic audio, but also maintains or exceeds performance on standard audio SSL benchmarks. Notably, it achieves up to a 3.9% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision (mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear evaluation and fine-tuning regimes with performance improvements of up to 9.1% (mAP). </p>
<blockquote>
<p>自监督预训练音频网络已在真实世界系统中得到广泛应用，特别是在多模态大型语言模型中。这些网络通常处于冻结状态，假设SSL预训练已经使他们足以处理真实世界的音频。但有一个关键问题仍然存在：这些模型在真实世界条件下的表现如何，那里的音频通常是多音的和复杂的，涉及多个重叠的声音源？当前的音频SSL方法通常主要在以单音音频为主的数据集上进行基准测试，如环境声和语音。因此，SSL模型泛化到多音音频的能力，这在自然场景中是一个常见特征，仍然被探索得不够。这一局限性引发了人们对SSL模型在更现实的音频设置中的实际稳健性的担忧。为了解决这一差距，我们引入了自监督学习从混合音频（SSLAM），这是音频SSL研究的一个新方向，旨在提高模型从多音数据学习能力的同时，保持对单音数据的强劲表现。我们对SSLAM在标准的音频SSL基准数据集上进行了全面评估，这些数据集主要是单音的，并使用一系列高质量、公开可用的多音数据集与最新方法进行了综合比较分析。SSLAM不仅提高了模型在多音音频上的性能，而且在标准音频SSL基准测试中保持了性能或有所提高。值得注意的是，它在AudioSet-2M（AS-2M）上实现了高达3.9%的改进，达到平均精度（mAP）为50.2。对于多音数据集，SSLAM在线性评估和微调方案中都达到了新的最佳状态，性能提高了高达9.1%（mAP）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12222v1">PDF</a> Accepted at ICLR 2025. Code and pre-trained models are available at   \url{<a target="_blank" rel="noopener" href="https://github.com/ta012/SSLAM%7D">https://github.com/ta012/SSLAM}</a></p>
<p><strong>Summary</strong></p>
<p>本摘要简要介绍了自我监督预训练的音频网络在多模态大型语言模型中的广泛应用，并指出了它们在处理复杂多变现实环境中的音频时存在的局限性。文章强调了在现实条件下，音频通常是多音调和复杂的，涉及多个重叠的声音源。现有的音频SSL方法主要在以单音音频为主的数据集上进行评估，如环境声和语音。因此，SSL模型泛化到多音音频的能力——自然场景中的常见特征，仍待探索。为解决这个问题，研究引入了从音频混合物的自我监督学习（SSLAM），旨在提高模型从多音数据中学习的能力，同时保持对单音数据的强劲表现。通过严格的评估，显示SSLAM不仅提高了模型在多音音频上的性能，而且在标准音频SSL基准数据集上的表现也得以维持或提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自我监督预训练的音频网络在多模态语言模型中受到广泛应用。</li>
<li>现有方法在复杂多变的现实音频条件下表现受限。</li>
<li>音频通常是多音调和复杂的，涉及多个重叠的声音源。</li>
<li>SSL模型在泛化到多音音频方面的能力仍待探索。</li>
<li>SSLAM的引入旨在提高模型从多音数据中学习的能力。</li>
<li>SSLAM在标准音频SSL基准数据集上的表现优秀，并在多音音频上的性能有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12222">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c45df64d79826ea5dddd02fdd9b9f6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8d8e03def4d8b2c92c1ddbc4c6ff946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f474da472a334f67138e35f79252027.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection"><a href="#Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection" class="headerlink" title="Evaluating Logit-Based GOP Scores for Mispronunciation Detection"></a>Evaluating Logit-Based GOP Scores for Mispronunciation Detection</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment. </p>
<blockquote>
<p>发音评估依赖于发音质量（GOP）分数，这些分数传统上由基于softmax的后验概率得出。然而，后验概率可能会遭受过度自信和不准确的音素分离问题的影响，从而限制了其有效性。本研究对比了基于对数似然率的GOP分数与基于概率的GOP分数在发音错误检测中的应用。我们在由荷兰语和汉语使用者说的两个英语二级语音数据集上进行了实验，评估了分类性能与人类评分的相关性。基于对数似然率的方法在分类方面优于基于概率的GOP方法，但其有效性取决于数据集的特征。最大对数似然率GOP与人类感知最为吻合，而结合不同的GOP分数可以平衡概率和对数似然率特征。研究结果表明，结合不确定性建模和音素特定权重的混合GOP方法可以改善发音评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12067v1">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了发音评估中的语音清晰度评分问题。研究发现，基于对数几率（logit）的语音清晰度评分在分类性能上优于基于概率的评分方法，特别是在针对荷兰语和英语二语者的英语语音数据集上。最大对数几率语音清晰度评分与人类感知最为一致，而结合不同语音清晰度评分的混合方法则平衡了概率和对数几率特征。研究结果表明，结合不确定性建模和音素特定权重的混合语音清晰度评分方法能提高发音评估的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>发音评估依赖于语音清晰度（GOP）评分。</li>
<li>传统上，GOP评分基于softmax生成的后验概率，但存在过度自信及音素分离不佳的问题。</li>
<li>对比了基于对数几率和基于概率的GOP评分方法进行发音误读检测。</li>
<li>在针对荷兰语和英语二语者的英语语音数据集上进行了实验。</li>
<li>对数几率方法（特别是最大对数几率）在分类性能上优于基于概率的GOP方法，与人类感知一致性更高。</li>
<li>混合方法结合了概率和对数几率特征，表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1f4e551e2f7763d08071870c0c0a2cc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8501f81f5987ff7174983f35cb9f857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3afdf19f5433f9ce302c70aeaaed2c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f22f7604e892e9ad251a4c4528b2c56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ca5b3087af3ce6abd1d10cc99db7210.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CMT-LLM-Contextual-Multi-Talker-ASR-Utilizing-Large-Language-Models"><a href="#CMT-LLM-Contextual-Multi-Talker-ASR-Utilizing-Large-Language-Models" class="headerlink" title="CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models"></a>CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models</h2><p><strong>Authors:Jiajun He, Naoki Sawada, Koichi Miyazaki, Tomoki Toda</strong></p>
<p>In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM’s prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios. </p>
<blockquote>
<p>在实际应用中，自动语音识别（ASR）系统必须处理来自多个发言者的重叠语音，并识别诸如技术术语之类的罕见词汇。传统方法分别处理多发言人ASR和上下文偏差，这在复杂场景中限制了性能。我们提出了一个统一框架，将多发言人重叠语音识别和上下文偏差结合成一项任务。我们的ASR方法集成了预训练的语音编码器和大型语言模型（LLM），采用优化的微调策略。我们还引入了一种两阶段过滤算法，有效地从大量偏差列表中识别出相关的罕见词汇，并将其纳入LLM的提示输入，从而提高罕见词汇的识别率。实验表明，我们的方法在LibriMix上取得了7.9%的WER（词错误率），在AMI SDM上取得了32.9%的WER，当偏差大小为1000时，我们的方法表现出在复杂语音场景中的有效性，并优于传统的上下文偏差方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12059v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong>：本文提出一个统一的框架，结合了多说话人重叠语音识别和上下文偏差纠正，以提高复杂场景中的语音识别性能。通过集成预训练的语音编码器和大型语言模型，并采用优化的微调策略，同时引入两阶段过滤算法以提高对罕见词的识别能力。实验表明，该方法在LibriMix和AMI SDM上分别实现了词错误率为7.9%和32.9%的优异表现。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出一个统一框架，整合多说话人重叠语音识别和上下文偏差纠正。</li>
<li>集成预训练的语音编码器和大型语言模型，优化微调策略以提高性能。</li>
<li>引入两阶段过滤算法，从大量偏差列表中有效识别相关罕见词汇。</li>
<li>方法在LibriMix和AMI SDM复杂语音场景下表现出色。</li>
<li>实验结果显示，与传统上下文偏差纠正方法相比，该方法具有优越性。</li>
<li>在LibriMix上实现词错误率为7.9%，在AMI SDM上实现词错误率为32.9%。</li>
<li>该方法对于提高语音识别系统在复杂环境中的实用性和性能具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12059">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4df9e3cca248b1115e40afd117fc22b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc32ff949dd3c9dc27e4027694c0a77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819cb347f62ccab7d819d562ec0c17d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3024a82e19a9899f894d37e20335fca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c133ef7c553d19ace274657a1f93c45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31acfcf6296b26ed84845bc28d3917cd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection"><a href="#EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection" class="headerlink" title="EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection"></a>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection</h2><p><strong>Authors:Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer</strong></p>
<p>The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. </p>
<blockquote>
<p>文本转语音和音频生成模型的进步要求对AI系统的情感理解能力进行稳健的基准测试。当前语音情感识别（SER）数据集在情感粒度、隐私担忧或依赖表演表现等方面存在局限性。本文介绍了EmoNet-Voice，一个用于语音情感检测的新资源，包括EmoNet-Voice Big，一个大规模预训练数据集（跨越11个声音、40种情感和4种语言，包含超过4500小时的语音），以及EmoNet-Voice Bench，一个带有专家注释的新型基准数据集。EmoNet-Voice旨在评估SER模型在40个情感类别的精细粒度光谱上的表现，这些情感类别的强度不同。我们利用最先进的语音生成技术，精心制作了模拟演员表现特定情感场景的合成音频片段。关键的是，我们邀请了心理学专家进行了严格的验证，他们分配了感知强度标签。这种合成的、保护隐私的方法可以包含现有数据集中通常不存在的敏感情感状态。最后，我们引入了Empathic Insight Voice模型，该模型在语音情感识别方面树立了新的标准，与人类专家的共识高度一致。我们在当前模型景观中的评估展现了有价值的发现，例如高唤醒情绪（如愤怒）比低唤醒状态（如专注）更容易检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09827v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个新型的语音情感识别数据集EmoNet-Voice，包括用于预训练的EmoNet-Voice Big和作为基准测试集的EmoNet-Voice Bench。该数据集具有40种情感类别，不同强度水平，并由语音生成模型模拟真实场景生成音频片段。通过心理学专家进行严格的验证，并赋予情感强度标签。此外，还引入了Empathic Insight Voice模型，在语音情感识别方面与人类专家高度一致。研究结果表明，高唤醒情绪如愤怒比低唤醒状态如专注更容易检测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了新型的语音情感识别数据集EmoNet-Voice，包含大规模预训练数据集EmoNet-Voice Big和基准测试集EmoNet-Voice Bench。</li>
<li>EmoNet-Voice数据集具有40种情感类别，涵盖不同强度水平，可评估模型的精细情感识别能力。</li>
<li>利用先进的语音生成技术，模拟真实场景生成音频片段，以增强数据集的实用性和真实性。</li>
<li>通过心理学专家进行严格的验证，并赋予情感强度标签，提高数据集的准确性和可靠性。</li>
<li>引入了Empathic Insight Voice模型，在语音情感识别方面与人类专家高度一致。</li>
<li>评估结果显示，高唤醒情绪相对容易检测，而低唤醒状态如专注等情感的识别更具挑战性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5826b7a29f41fe24f9d3145cda5eeda7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e5c3e69192ee6e28a9481fc91b6d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f3ad4742d3f56ecf7b1adf9445f1db9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ead12f776165c52d3bd1fc04d82aeaba.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLM-and-Self-Supervised-Training-Models-for-Speech-Recognition-in-Chinese-Dialects-A-Comparative-Analysis"><a href="#Leveraging-LLM-and-Self-Supervised-Training-Models-for-Speech-Recognition-in-Chinese-Dialects-A-Comparative-Analysis" class="headerlink" title="Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis"></a>Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis</h2><p><strong>Authors:Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei</strong></p>
<p>Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre-training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research </p>
<blockquote>
<p>大规模训练语料库已经显著提高了语音识别模型性能。然而，由于数据相对稀缺，中文口音和方言仍然是大多数语音识别模型的挑战。最近的自监督学习进展表明，自监督预训练与大型语言模型（LLM）的结合，可以在低资源情况下有效提高语音识别性能。我们的目标是研究这种范式在中文方言中的有效性。具体来说，我们在30万小时的无标签方言和带口音语音数据上预训练了一个Data2vec2模型，并在一个4万小时的监督数据集上进行对齐训练。然后，我们系统地研究了在这种范式下，各种投影仪和LLM对普通话、方言和带口音语音识别性能的影响。我们的方法在多个方言数据集上达到了最佳结果，包括Kespeech。我们将开源我们的工作，以促进可复制的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21138v2">PDF</a> </p>
<p><strong>Summary</strong>：大规模训练语料库显著提高了语音识别模型性能。然而，由于数据相对稀缺，中文口音和方言仍是大多数语音识别模型的挑战。最近自监督学习的进展显示，自监督预训练与大型语言模型的结合，可有效提高低资源场景中的语音识别性能。本研究旨在探究这一模式在中文方言中的有效性。具体来说，我们在30万小时的无标签方言和口音语音数据上预训练了Data2vec2模型，并在一个4万小时的监督数据集上进行对齐训练。然后，我们系统地研究了该模式下各种投影器和大型语言模型对普通话、方言和口音语音识别的性能影响。我们的方法在多个方言数据集上达到了最新技术水平，包括Kespeech数据集。我们将开源我们的工作以促进可复现研究。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大规模训练语料库增强了语音识别模型的性能。</li>
<li>中文口音和方言仍是语音识别模型的一个挑战。</li>
<li>自监督预训练与大型语言模型的结合在低资源场景中的语音识别性能提升显著。</li>
<li>研究者使用Data2vec2模型在30万小时的无标签方言和口音语音数据上进行预训练。</li>
<li>对齐训练在4万小时的监督数据集上进行，以提高模型性能。</li>
<li>研究者系统地研究了不同投影器和大型语言模型对语音识别的性能影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21138">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-24312001676eb086354716bd668bb4b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-013d37df0e3f2cb6fe97a767a5f31427.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c83d5dfb5fb3af0f0b09f6cda952ff6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1339c33604d6c4e21e782d07e92a7a65.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ArrayDPS-Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior"><a href="#ArrayDPS-Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior" class="headerlink" title="ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior"></a>ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior</h2><p><strong>Authors:Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury</strong></p>
<p>Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a>. </p>
<blockquote>
<p>盲语音分离（BSS）旨在从麦克风阵列记录的音频混合物中分离出多个语音源。这个问题具有挑战性，因为它是一个盲反问题，即麦克风阵列的几何形状、房间冲击响应（RIR）和语音源都是未知的。我们提出ArrayDPS以无监督、阵列无关和生成的方式解决BSS问题。其核心思想建立在扩散后采样（DPS）的基础上，但与DPS不同的是，DPS的可能性是明确的，ArrayDPS必须通过制定一个单独的优化问题来近似可能性。优化的解决方案近似于房间声学以及麦克风之间的相对传递函数。这些近似值，结合扩散先验，在ArrayDPS采样过程中进行迭代，并最终产生分离的语音源。我们只需要一个简单的单说话人语音扩散模型作为先验，以及麦克风记录的混合物；不需要麦克风阵列的信息。评估结果表明，ArrayDPS在SDR方面优于所有基线无监督方法，同时与有监督方法相当。音频演示请访问：<a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/%E3%80%82">https://arraydps.github.io/ArrayDPSDemo/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05657v3">PDF</a> Paper Accepted at ICML2025 Demo:   <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/ArrayDPS/ArrayDPS">https://github.com/ArrayDPS/ArrayDPS</a></p>
<p><strong>Summary</strong><br>盲语音分离（BSS）是从麦克风阵列录制的音频混合中分离多个语音源的问题。提出ArrayDPS以无监督、阵列无关和生成的方式解决BSS问题。其核心思想建立在扩散后采样（DPS）的基础上，但ArrayDPS必须通过单独的优化问题来近似可能性。解决方案近似房间声学及麦克风之间的相对传递函数。这些近似与扩散先验相结合，在ArrayDPS采样过程中进行迭代，最终产生分离的语音源。只需简单的单说话人语音扩散模型先验和麦克风录制的混音，无需知道麦克风阵列信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>盲语音分离（BSS）是从音频混合中分离多个未知语音源的问题，具有挑战性。</li>
<li>ArrayDPS是一种解决BSS问题的新方法，采用无监督、阵列无关和生成的方式。</li>
<li>ArrayDPS建立在扩散后采样（DPS）的基础上，但必须通过单独的优化问题来近似可能性。</li>
<li>ArrayDPS的解决方案可以近似房间声学和麦克风之间的相对传递函数。</li>
<li>该方法只需单说话人语音扩散模型先验和混音，无需知道麦克风阵列的具体信息。</li>
<li>评价结果显示，ArrayDPS在无监督方法中的表现优于所有基线方法，并且在SDR方面与监督方法相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96eb1634841b5ffd7b5293fe60717f11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91e144398c5b5e2a2baa6c0e71473a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc750cae64fd3a5dd2196958422f64f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e43afdd9ce694caecba76520109e6e7e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions"><a href="#QualiSpeech-A-Speech-Quality-Assessment-Dataset-with-Natural-Language-Reasoning-and-Descriptions" class="headerlink" title="QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions"></a>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language   Reasoning and Descriptions</h2><p><strong>Authors:Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang</strong></p>
<p>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech</a>. </p>
<blockquote>
<p>本文探索了一种利用自然语言描述来进行语音质量评估的新视角，提供比传统数字评分方法更丰富、更细微的见解。自然语言反馈提供指导性的建议和详细的评价，但现有的数据集缺乏这种方法的全面注释。为了弥补这一差距，我们推出了QualiSpeech数据集，这是一个全面的低层次语音质量评估数据集，涵盖了1 1个关键方面和包含推理和上下文洞察的自然语言评论。此外，我们提出了QualiSpeech基准测试，以评估听觉大型语言模型（LLM）的低层次语音理解能力。实验结果表明，微调后的听觉LLM能够可靠地描述噪声和失真的细节，有效地识别它们的类型和时间特征。结果进一步突显了结合推理来提高质量评估准确性和可靠性的潜力。该数据集将在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20290v3">PDF</a> 22 pages, 10 figures</p>
<p><strong>Summary</strong>：<br>本文探索了利用自然语言描述进行语音质量评估的新视角，提供比传统数字评分方法更丰富、更细微的见解。为解决现有数据集缺乏全面注释的问题，提出了QualiSpeech数据集和QualiSpeech基准测试。该数据集包含11个关键方面的低级别语音质量评估，以及包含推理和上下文洞察的自然语言评论。实验结果表明，微调后的听觉大型语言模型（LLMs）可以可靠地描述噪声和失真，有效识别它们的类型和时间特征。该数据集将发布在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/QualiSpeech%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/QualiSpeech。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>论文提出了一种新的语音质量评估方法，利用自然语言描述提供丰富的见解。</li>
<li>自然语言反馈提供了详细评价和指导性建议。</li>
<li>存在缺乏全面注释的现有数据集，为此引入了QualiSpeech数据集。</li>
<li>QualiSpeech数据集包含低级别语音质量评估的多个关键方面和详细的自然语言评论。</li>
<li>QualiSpeech基准测试用于评估听觉大型语言模型的低级别语音理解能力。</li>
<li>实验结果表明，微调后的听觉大型语言模型可以准确描述语音中的噪声和失真类型及其时间特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fd02b561eaa9fde550b85f371259d93e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-324aa14684013b8b470bd43a3c761fd3" align="middle">
<img src="https://pic1.zhimg.com/v2-7ca3df0bd79ad7aa72716e77df206cce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-735f70115a2acb80ba97e25ff412b5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dfa45425e00bb2f0b3a3bca9015140c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MTLM-Incorporating-Bidirectional-Text-Information-to-Enhance-Language-Model-Training-in-Speech-Recognition-Systems"><a href="#MTLM-Incorporating-Bidirectional-Text-Information-to-Enhance-Language-Model-Training-in-Speech-Recognition-Systems" class="headerlink" title="MTLM: Incorporating Bidirectional Text Information to Enhance Language   Model Training in Speech Recognition Systems"></a>MTLM: Incorporating Bidirectional Text Information to Enhance Language   Model Training in Speech Recognition Systems</h2><p><strong>Authors:Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai, Huizhi Liang</strong></p>
<p>Automatic speech recognition (ASR) systems normally consist of an acoustic model (AM) and a language model (LM). The acoustic model estimates the probability distribution of text given the input speech, while the language model calibrates this distribution toward a specific knowledge domain to produce the final transcription. Traditional ASR-specific LMs are typically trained in a unidirectional (left-to-right) manner to align with autoregressive decoding. However, this restricts the model from leveraging the right-side context during training, limiting its representational capacity. In this work, we propose MTLM, a novel training paradigm that unifies unidirectional and bidirectional manners through 3 training objectives: ULM, BMLM, and UMLM. This approach enhances the LM’s ability to capture richer linguistic patterns from both left and right contexts while preserving compatibility with standard ASR autoregressive decoding methods. As a result, the MTLM model not only enhances the ASR system’s performance but also support multiple decoding strategies, including shallow fusion, unidirectional&#x2F;bidirectional n-best rescoring. Experiments on the LibriSpeech dataset show that MTLM consistently outperforms unidirectional training across multiple decoding strategies, highlighting its effectiveness and flexibility in ASR applications. </p>
<blockquote>
<p>自动语音识别（ASR）系统通常由声学模型（AM）和语言模型（LM）组成。声学模型估计给定输入语音的文本概率分布，而语言模型则对这个分布进行校准，以面向特定的知识领域，从而产生最终的转录。传统的ASR专用LM通常采用单向（从左到右）的方式进行训练，以符合自回归解码。然而，这限制了模型在训练过程中利用右侧上下文的能力，从而限制了其表征能力。在本文中，我们提出了MTLM，这是一种新的训练范式，通过三个训练目标：ULM、BMLM和UMLM，统一了单向和双向方式。这种方法提高了LM从左右上下文中捕获更丰富语言模式的能力，同时保留了与标准ASR自回归解码方法的兼容性。因此，MTLM模型不仅提高了ASR系统的性能，还支持多种解码策略，包括浅融合、单向&#x2F;双向n-best重评分。在LibriSpeech数据集上的实验表明，MTLM在多种解码策略上始终优于单向训练，突出了其在ASR应用中的有效性和灵活性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10058v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了自动语音识别（ASR）系统中的语言模型（LM）训练新方法——MTLM。该方法结合了单向和双向训练方式，通过ULM、BMLM和UMLM三个训练目标，提高了语言模型捕捉左右语境中更丰富语言模式的能力，同时兼容标准ASR自回归解码方法。实验表明，MTLM模型在多个解码策略下均优于单向训练，有效提高ASR系统性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASR系统通常由声学模型和语言模型组成，前者估计文本的概率分布，后者将此分布校准到特定知识域以产生最终转录。</li>
<li>传统ASR特定的语言模型通常以单向（从左到右）方式进行训练，与自回归解码相匹配，这限制了模型在训练过程中对右侧语境的利用。</li>
<li>MTLM是一种新的训练范式，结合了单向和双向方式，通过ULM、BMLM和UMLM三个训练目标，提高语言模型捕捉左右语境中语言模式的能力。</li>
<li>MTLM增强了ASR系统的性能，支持多种解码策略，包括浅融合、单向&#x2F;双向n-best重评分。</li>
<li>实验表明，在LibriSpeech数据集上，MTLM在多个解码策略下的表现均优于单向训练。</li>
<li>MTLM方法既有效又灵活，可应用于不同的ASR场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10058">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-341d3f3f745ad381664d5f021e70be6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bcf44cdc1ceee62111bd347725b9e20.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c00f94d61873b744a72e97f748e23fd.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-06-22  Public Acceptance of Cybernetic Avatars in the service sector Evidence   from a Large-Scale Survey in Dubai
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0b3ebdc4821c4c59d09a146b320df3e3.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-22  Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
