<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  PhantomHunter Detecting Unseen Privately-Tuned LLM-Generated Text via   Family-Aware Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-fce97b4f6def7d65ec1be062205a3d29.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="PhantomHunter-Detecting-Unseen-Privately-Tuned-LLM-Generated-Text-via-Family-Aware-Learning"><a href="#PhantomHunter-Detecting-Unseen-Privately-Tuned-LLM-Generated-Text-via-Family-Aware-Learning" class="headerlink" title="PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via   Family-Aware Learning"></a>PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via   Family-Aware Learning</h2><p><strong>Authors:Yuhui Shi, Yehan Yang, Qiang Sheng, Hao Mi, Beizhe Hu, Chaoxi Xu, Juan Cao</strong></p>
<p>With the popularity of large language models (LLMs), undesirable societal problems like misinformation production and academic misconduct have been more severe, making LLM-generated text detection now of unprecedented importance. Although existing methods have made remarkable progress, a new challenge posed by text from privately tuned LLMs remains underexplored. Users could easily possess private LLMs by fine-tuning an open-source one with private corpora, resulting in a significant performance drop of existing detectors in practice. To address this issue, we propose PhantomHunter, an LLM-generated text detector specialized for detecting text from unseen, privately-tuned LLMs. Its family-aware learning framework captures family-level traits shared across the base models and their derivatives, instead of memorizing individual characteristics. Experiments on data from LLaMA, Gemma, and Mistral families show its superiority over 7 baselines and 3 industrial services, with F1 scores of over 96%. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠï¼Œè¯¸å¦‚å‡ä¿¡æ¯ä¼ æ’­å’Œå­¦æœ¯ä¸ç«¯ç­‰ä¸å—æ¬¢è¿çš„ç¤¾ä¼šé—®é¢˜å˜å¾—æ›´åŠ ä¸¥é‡ï¼Œè¿™ä½¿å¾—å¯¹LLMç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹å˜å¾—è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±ç§æœ‰å¾®è°ƒLLMäº§ç”Ÿçš„æ–‡æœ¬æ‰€å¸¦æ¥çš„æ–°æŒ‘æˆ˜ä»ç„¶è¢«å¿½è§†ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ä½¿ç”¨ç§æœ‰è¯­æ–™åº“å¯¹å¼€æºLLMè¿›è¡Œå¾®è°ƒæ¥è½»æ¾æ‹¥æœ‰ç§æœ‰LLMï¼Œè¿™å¯¼è‡´ç°æœ‰æ£€æµ‹å™¨åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PhantomHunterï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºæ£€æµ‹æ¥è‡ªæœªè§è¿‡çš„ç§æœ‰å¾®è°ƒLLMçš„æ–‡æœ¬çš„æ£€æµ‹å™¨ã€‚å…¶å®¶æ—æ„ŸçŸ¥å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿæ•è·åŸºç¡€æ¨¡å‹åŠå…¶è¡ç”Ÿç‰©ä¹‹é—´çš„å®¶æ—çº§ç‰¹å¾ï¼Œè€Œä¸æ˜¯è®°å¿†å•ä¸ªç‰¹å¾ã€‚åœ¨LLaMAã€Gemmaå’ŒMistralå®¶æ—çš„æ•°æ®ä¸Šè¿›è¡Œå®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¼˜äº7ä¸ªåŸºå‡†æµ‹è¯•å’Œ3ä¸ªå·¥ä¸šæœåŠ¡ï¼ŒF1å¾—åˆ†è¶…è¿‡9.æ­¤æ–¹æ¡ˆç›¸è¾ƒäºä¹‹å‰æ–¹æ¡ˆèƒ½å¤Ÿæ›´å¥½æ£€æµ‹å‡ºå†…éƒ¨å˜åŒ–å¯¼è‡´æ¼æ´ç­‰é—®é¢˜ã€å¤§å¤§æå‡ç²¾åº¦ç‡å’Œå¯é ç¨‹åº¦ä¸ºä¸šç•Œå¸¦æ¥æ›´åŠ å®‰å…¨é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15683v1">PDF</a> 17 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠï¼Œäº§ç”Ÿçš„è¯¸å¦‚å‡æ–°é—»å’Œå­¦æœ¯ä¸ç«¯ç­‰ç¤¾ä¼šæ€§é—®é¢˜æ„ˆå‘ä¸¥é‡ï¼Œä½¿å¾—æ£€æµ‹LLMç”Ÿæˆçš„æ–‡æœ¬å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•è™½æœ‰æ‰€è¿›å±•ï¼Œä½†å¯¹ä½¿ç”¨ç§æœ‰è¯­æ–™å¾®è°ƒå¾—åˆ°çš„LLMç”Ÿæˆçš„æ–‡æœ¬æ£€æµ‹ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºPhantomHunteræ–¹æ³•ï¼Œä¸“é—¨ç”¨äºæ£€æµ‹è¿™ç±»æ–‡æœ¬çš„LLMç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨ã€‚å®ƒé€šè¿‡å®¶æ—çº§å­¦ä¹ æ¡†æ¶æ•æ‰åŸºç¡€æ¨¡å‹åŠå…¶è¡ç”Ÿç‰ˆæœ¬ä¹‹é—´çš„å®¶æ—çº§ç‰¹å¾ï¼Œè€Œéæ­»è®°ç¡¬èƒŒä¸ªåˆ«ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–ä¸ƒå¤§åŸºçº¿æ¨¡å‹åŠä¸‰å¤§å·¥ä¸šæœåŠ¡ï¼ŒPhantomHunteråœ¨LLaMAã€Gemmaå’ŒMistralç­‰å®¶æ—æ•°æ®ä¸Šçš„è¡¨ç°æ›´èƒœä¸€ç­¹ï¼ŒF1åˆ†æ•°è¶…è¿‡96%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™®åŠå¸¦æ¥çš„ç¤¾ä¼šé—®é¢˜ï¼Œå¦‚å‡æ–°é—»å’Œå­¦æœ¯ä¸ç«¯è¡Œä¸ºï¼Œä½¿æ£€æµ‹LLMç”Ÿæˆçš„æ–‡æœ¬è‡³å…³é‡è¦ã€‚</li>
<li>å°½ç®¡ç°æœ‰æ£€æµ‹æ–¹æ³•å–å¾—è¿›å±•ï¼Œä½†é’ˆå¯¹ä½¿ç”¨ç§æœ‰è¯­æ–™å¾®è°ƒå¾—åˆ°çš„LLMç”Ÿæˆçš„æ–‡æœ¬æ£€æµ‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>PhantomHunteræ˜¯ä¸€ç§ä¸“é—¨ç”¨äºæ£€æµ‹LLMç”Ÿæˆæ–‡æœ¬çš„å…¨æ–°æ–¹æ³•ã€‚</li>
<li>PhantomHunteré‡‡ç”¨å®¶æ—çº§å­¦ä¹ æ¡†æ¶ï¼Œæ•æ‰åŸºç¡€æ¨¡å‹åŠå…¶è¡ç”Ÿç‰ˆæœ¬ä¹‹é—´çš„å®¶æ—çº§ç‰¹å¾ã€‚</li>
<li>PhantomHunterèƒ½å¤Ÿæ£€æµ‹æœªç»è®­ç»ƒçš„ç§æœ‰LLMç”Ÿæˆçš„æ–‡æœ¬ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒPhantomHunterç›¸è¾ƒäºå…¶ä»–æ¨¡å‹åŠå·¥ä¸šæœåŠ¡è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-475484b1ae07f0f6c0f5f818c8d46482.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70b2d0770a25784297ca9c22948395cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-060d4f76e218358d2de0e7ef6d778569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606de17368691f8d60da00ff5484489d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c36dee8d1439d30be3b0aa805faeea8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SwarmAgentic-Towards-Fully-Automated-Agentic-System-Generation-via-Swarm-Intelligence"><a href="#SwarmAgentic-Towards-Fully-Automated-Agentic-System-Generation-via-Swarm-Intelligence" class="headerlink" title="SwarmAgentic: Towards Fully Automated Agentic System Generation via   Swarm Intelligence"></a>SwarmAgentic: Towards Fully Automated Agentic System Generation via   Swarm Intelligence</h2><p><strong>Authors:Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp</strong></p>
<p>The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at <a target="_blank" rel="noopener" href="https://yaoz720.github.io/SwarmAgentic/">https://yaoz720.github.io/SwarmAgentic/</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å†³ç­–ã€åè°ƒå’Œä»»åŠ¡æ‰§è¡Œä¸­çš„æ™ºèƒ½ä½“ç³»ä»£ç†çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ™ºèƒ½ä½“ç³»ç”Ÿæˆæ¡†æ¶ç¼ºä¹å®Œå…¨çš„è‡ªä¸»æ€§ï¼Œç¼ºå°‘ä»å¤´å¼€å§‹ç”Ÿæˆä»£ç†ã€è‡ªæˆ‘ä¼˜åŒ–ä»£ç†åŠŸèƒ½å’Œåä½œçš„èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†SwarmAgenticï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ™ºèƒ½ä½“ç³»ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä»å¤´å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ç³»ï¼Œå¹¶é€šè¿‡è¯­è¨€é©±åŠ¨çš„æ¢ç´¢æ¥è”åˆä¼˜åŒ–ç›¸äº’ä¾èµ–çš„ä»£ç†åŠŸèƒ½å’Œåä½œã€‚ä¸ºäº†åœ¨ç³»ç»Ÿçº§åˆ«ç»“æ„ä¸Šå®ç°æœ‰æ•ˆçš„æœç´¢ï¼ŒSwarmAgenticç»´æŠ¤äº†ä¸€ç»„å€™é€‰ç³»ç»Ÿï¼Œå¹¶é€šè¿‡åé¦ˆæŒ‡å¯¼çš„æ›´æ–°æ¥è¿›åŒ–å®ƒä»¬ï¼Œè¿™å¾—ç›Šäºç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰çš„å¯å‘ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠé«˜çº§è§„åˆ’ã€ç³»ç»Ÿçº§åè°ƒå’Œåˆ›é€ æ€§æ¨ç†çš„å…­ä¸ªçœŸå®ä¸–ç•Œã€å¼€æ”¾æ€§å’Œæ¢ç´¢æ€§ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ä»…ç»™å‡ºä»»åŠ¡æè¿°å’Œç›®æ ‡å‡½æ•°ï¼ŒSwarmAgenticåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸­ç›¸å¯¹äºADASå®ç°äº†+261.8%çš„ç›¸å¯¹æ”¹è¿›ï¼Œè¿™å‡¸æ˜¾äº†å…¨è‡ªåŠ¨åœ¨ç»“æ„æ€§æ— çº¦æŸä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æ¡†æ¶æ ‡å¿—ç€æœç€å¯æ‰©å±•å’Œè‡ªä¸»çš„æ™ºèƒ½ä½“ç³»è®¾è®¡è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œå®ƒå°†ç¾¤ä½“æ™ºèƒ½ä¸å…¨è‡ªåŠ¨å¤šæ™ºèƒ½ä½“ç³»ç”Ÿæˆç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://yaoz720.github.io/SwarmAgentic/%E4%B8%8A%E3%80%82">https://yaoz720.github.io/SwarmAgentic/ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15672v1">PDF</a> 41 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å†³ç­–ã€åè°ƒå’Œä»»åŠ¡æ‰§è¡Œæ–¹é¢çš„agenticç³»ç»Ÿè¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„agenticç³»ç»Ÿç”Ÿæˆæ¡†æ¶ç¼ºä¹å®Œå…¨è‡ªä¸»æ€§ï¼Œç¼ºå°‘ä»å¤´å¼€å§‹ç”Ÿæˆagentã€è‡ªæˆ‘ä¼˜åŒ–agentåŠŸèƒ½ä»¥åŠåä½œèƒ½åŠ›ï¼Œé™åˆ¶äº†é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSwarmAgenticæ¡†æ¶ï¼Œå®ç°agenticç³»ç»Ÿçš„å…¨è‡ªåŠ¨ç”Ÿæˆã€‚å®ƒé€šè¿‡è¯­è¨€é©±åŠ¨çš„æ¢ç´¢ï¼Œä»å¤´æ„å»ºagenticç³»ç»Ÿï¼Œå¹¶è”åˆä¼˜åŒ–agentåŠŸèƒ½å’Œåä½œèƒ½åŠ›ã€‚SwarmAgenticé€šè¿‡ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•çš„çµæ„Ÿï¼Œç»´æŠ¤å€™é€‰ç³»ç»Ÿç§ç¾¤å¹¶é€šè¿‡åé¦ˆæŒ‡å¯¼çš„æ›´æ–°è¿›è¡Œæ¼”åŒ–ã€‚åœ¨æ¶‰åŠé«˜çº§è§„åˆ’ã€ç³»ç»Ÿçº§åè°ƒå’Œåˆ›é€ æ€§æ¨ç†çš„å…­ä¸ªç°å®ä¸–ç•Œçš„å¼€æ”¾å’Œæ¢ç´¢æ€§ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…ç»™å‡ºä»»åŠ¡æè¿°å’Œç›®æ ‡å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºADASåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†+261.8%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ­¤æ¡†æ¶æ ‡å¿—ç€åœ¨å¯ä¼¸ç¼©å’Œè‡ªä¸»agenticç³»ç»Ÿè®¾è®¡æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå°†ç¾¤ä½“æ™ºèƒ½ä¸å…¨è‡ªåŠ¨å¤šagentç³»ç»Ÿç”Ÿæˆç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://yaoz720.github.io/SwarmAgentic/">ç½‘ç«™é“¾æ¥</a>ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†å†³ç­–ã€åè°ƒå’Œä»»åŠ¡æ‰§è¡Œä¸­agenticç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>å½“å‰agenticç³»ç»Ÿç”Ÿæˆæ¡†æ¶ç¼ºä¹å®Œå…¨è‡ªä¸»æ€§ï¼Œé™åˆ¶äº†é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>SwarmAgenticæ¡†æ¶å®ç°agenticç³»ç»Ÿçš„å…¨è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…æ‹¬ä»å¤´æ„å»ºã€åŠŸèƒ½ä¼˜åŒ–å’Œåä½œèƒ½åŠ›ã€‚</li>
<li>SwarmAgenticæ¡†æ¶å—åˆ°ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•çš„å¯å‘ï¼Œé€šè¿‡ç»´æŠ¤å€™é€‰ç³»ç»Ÿç§ç¾¤å¹¶é€šè¿‡åé¦ˆè¿›è¡Œæ›´æ–°æ¼”åŒ–ã€‚</li>
<li>SwarmAgenticæ¡†æ¶åœ¨å¤šä¸ªç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç›¸å¯¹äºADASåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ­¤æ¡†æ¶æ˜¯æœç€å¯ä¼¸ç¼©å’Œè‡ªä¸»agenticç³»ç»Ÿè®¾è®¡çš„é‡è¦ä¸€æ­¥ï¼Œç»“åˆäº†ç¾¤ä½“æ™ºèƒ½å’Œå…¨è‡ªåŠ¨å¤šagentç³»ç»Ÿç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38cb83ecf2dab629bd291eb0553fb40b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4eeb17b0426971b1844bf62cbdcbb479.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-029bf913f94ef2640518a1c433d03f91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88e11a142c092dd6438cb6e904520942.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PhishDebate-An-LLM-Based-Multi-Agent-Framework-for-Phishing-Website-Detection"><a href="#PhishDebate-An-LLM-Based-Multi-Agent-Framework-for-Phishing-Website-Detection" class="headerlink" title="PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website   Detection"></a>PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website   Detection</h2><p><strong>Authors:Wenhao Li, Selvakumar Manickam, Yung-wey Chong, Shankar Karuppayah</strong></p>
<p>Phishing websites continue to pose a significant cybersecurity threat, often leveraging deceptive structures, brand impersonation, and social engineering tactics to evade detection. While recent advances in large language models (LLMs) have enabled improved phishing detection through contextual understanding, most existing approaches rely on single-agent classification facing the risks of hallucination and lack interpretability or robustness. To address these limitations, we propose PhishDebate, a modular multi-agent LLM-based debate framework for phishing website detection. PhishDebate employs four specialized agents to independently analyze different textual aspects of a webpageâ€“URL structure, HTML composition, semantic content, and brand impersonationâ€“under the coordination of a Moderator and a final Judge. Through structured debate and divergent thinking, the framework delivers more accurate and interpretable decisions. Extensive evaluations on commercial LLMs demonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate (TPR) on a real-world phishing dataset, and outperforms single-agent and Chain of Thought (CoT) baselines. Additionally, its modular design allows agent-level configurability, enabling adaptation to varying resource and application requirements. </p>
<blockquote>
<p>é’“é±¼ç½‘ç«™ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç½‘ç»œå®‰å…¨å¨èƒï¼Œå®ƒä»¬ç»å¸¸åˆ©ç”¨æ¬ºéª—æ€§ç»“æ„ã€å“ç‰Œä¼ªè£…å’Œç¤¾ä¼šå·¥ç¨‹ç­–ç•¥æ¥èº²é¿æ£€æµ‹ã€‚å°½ç®¡æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡ç†è§£æ¥æ”¹å–„é’“é±¼ç½‘ç«™æ£€æµ‹ï¼Œä½†ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä»ç„¶ä¾èµ–äºå•ä»£ç†åˆ†ç±»ï¼Œé¢ä¸´ç€å‡ºç°å¹»è§‰å’Œç¼ºä¹å¯è§£é‡Šæ€§æˆ–ç¨³å¥æ€§çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PhishDebateï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šä»£ç†è¾©è®ºçš„æ¨¡å—åŒ–é’“é±¼ç½‘ç«™æ£€æµ‹æ¡†æ¶ã€‚PhishDebateé‡‡ç”¨å››ä¸ªä¸“ä¸šä»£ç†æ¥ç‹¬ç«‹åˆ†æç½‘é¡µçš„å››ä¸ªä¸åŒæ–‡æœ¬æ–¹é¢â€”â€”URLç»“æ„ã€HTMLç»„æˆã€è¯­ä¹‰å†…å®¹å’Œå“ç‰Œä¼ªè£…ï¼Œå¹¶åœ¨åè°ƒå‘˜çš„åè°ƒå’Œä¸€ä¸ªæœ€ç»ˆåˆ¤å†³è€…çš„åˆ¤å†³ä¸‹è¿è¡Œã€‚é€šè¿‡ç»“æ„åŒ–çš„è¾©è®ºå’Œå‘æ•£æ€ç»´ï¼Œè¯¥æ¡†æ¶èƒ½åšå‡ºæ›´å‡†ç¡®ã€æ›´å¯è§£é‡Šçš„å†³å®šã€‚åœ¨å•†ä¸šLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPhishDebateåœ¨çœŸå®ä¸–ç•Œçš„é’“é±¼æ•°æ®é›†ä¸Šè¾¾åˆ°äº†98.2%çš„å¬å›ç‡å’Œ98.2%çš„çœŸæ­£é˜³æ€§ç‡ï¼ˆTPRï¼‰ï¼Œå¹¶ä¼˜äºå•ä»£ç†å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡å…è®¸ä»£ç†çº§åˆ«çš„é…ç½®ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„èµ„æºå’Œåº”ç”¨éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15656v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ä¸ºæ£€æµ‹é’“é±¼ç½‘ç«™æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚PhishDebateæ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“è®¾è®¡ï¼Œé€šè¿‡å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ç‹¬ç«‹åˆ†æç½‘é¡µçš„URLç»“æ„ã€HTMLç»„æˆã€è¯­ä¹‰å†…å®¹å’Œå“ç‰Œæ¨¡ä»¿ç­‰ä¸åŒæ–¹é¢ï¼Œå¹¶åœ¨åè°ƒå‘˜å’Œæœ€ç»ˆè£åˆ¤çš„ç›‘ç£ä¸‹è¿›è¡Œç»“æ„åŒ–è¾©è®ºå’Œå‘æ•£æ€ç»´ï¼Œä»¥æé«˜å†³ç­–å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚åœ¨ç°å®ä¸–ç•Œé’“é±¼ç½‘ç«™æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPhishDebateå–å¾—äº†98.2%çš„å¬å›ç‡å’ŒçœŸæ­£é˜³æ€§ç‡ï¼ˆTPRï¼‰ï¼Œä¼˜äºå•æ™ºèƒ½ä½“å’Œé“¾æ€ç»´ï¼ˆCoTï¼‰åŸºçº¿ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡è¿˜å…è®¸æ™ºèƒ½ä½“çº§åˆ«çš„é…ç½®ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„èµ„æºå’Œåº”ç”¨éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Phishingç½‘ç«™ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç½‘ç»œå®‰å…¨å¨èƒï¼Œå®ƒä»¬ä½¿ç”¨æ¬ºéª—æ€§ç»“æ„ã€å“ç‰Œæ¨¡ä»¿å’Œç¤¾ä¼šå·¥ç¨‹ç­–ç•¥æ¥èº²é¿æ£€æµ‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•å·²ç»æ”¹å–„äº†é€šè¿‡ä¸Šä¸‹æ–‡ç†è§£è¿›è¡Œé’“é±¼ç½‘ç«™æ£€æµ‹çš„èƒ½åŠ›ã€‚</li>
<li>PhishDebateæ˜¯ä¸€ä¸ªåŸºäºLLMçš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºæ›´å‡†ç¡®åœ°æ£€æµ‹é’“é±¼ç½‘ç«™ã€‚</li>
<li>PhishDebateé€šè¿‡å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ç‹¬ç«‹åˆ†æç½‘é¡µçš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬URLç»“æ„ã€HTMLç»„æˆã€è¯­ä¹‰å†…å®¹å’Œå“ç‰Œæ¨¡ä»¿ã€‚</li>
<li>PhishDebateé€šè¿‡ç»“æ„åŒ–è¾©è®ºå’Œå‘æ•£æ€ç»´æé«˜å†³ç­–çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œé’“é±¼ç½‘ç«™æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒPhishDebateçš„å¬å›ç‡å’ŒçœŸæ­£é˜³æ€§ç‡å‡è¾¾åˆ°98.2%ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acb091eaef921bf977419eeb31d6e9b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e3277797b2db9ff45f573d489de3ac6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-705cef6a86ff3776c07875522ca16a18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d1f92cd048c55c5f6e9adebc30f3dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d631248cddec5c773f9286a7eebc3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c601620dcb82eb6d03a5e443647e69d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18406afc67f6d564f7be05fbc386d04c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="deepSURF-Detecting-Memory-Safety-Vulnerabilities-in-Rust-Through-Fuzzing-LLM-Augmented-Harnesses"><a href="#deepSURF-Detecting-Memory-Safety-Vulnerabilities-in-Rust-Through-Fuzzing-LLM-Augmented-Harnesses" class="headerlink" title="deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through   Fuzzing LLM-Augmented Harnesses"></a>deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through   Fuzzing LLM-Augmented Harnesses</h2><p><strong>Authors:Georgios Androutsopoulos, Antonio Bianchi</strong></p>
<p>Although Rust ensures memory safety by default, it also permits the use of unsafe code, which can introduce memory safety vulnerabilities if misused. Unfortunately, existing tools for detecting memory bugs in Rust typically exhibit limited detection capabilities, inadequately handle Rust-specific types, or rely heavily on manual intervention.   To address these limitations, we present deepSURF, a tool that integrates static analysis with Large Language Model (LLM)-guided fuzzing harness generation to effectively identify memory safety vulnerabilities in Rust libraries, specifically targeting unsafe code. deepSURF introduces a novel approach for handling generics by substituting them with custom types and generating tailored implementations for the required traits, enabling the fuzzer to simulate user-defined behaviors within the fuzzed library. Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically, facilitating exploration of complex API interactions and significantly increasing the likelihood of exposing memory safety vulnerabilities. We evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20 known memory safety bugs and uncovering 6 previously unknown vulnerabilities, demonstrating clear improvements over state-of-the-art tools. </p>
<blockquote>
<p>å°½ç®¡Rusté»˜è®¤ç¡®ä¿å†…å­˜å®‰å…¨ï¼Œä½†å®ƒä¹Ÿå…è®¸ä½¿ç”¨ä¸å®‰å…¨çš„ä»£ç ï¼Œå¦‚æœè¯¯ç”¨ï¼Œå¯èƒ½ä¼šå¼•å…¥å†…å­˜å®‰å…¨æ¼æ´ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç›®å‰ç”¨äºæ£€æµ‹Rustä¸­å†…å­˜é”™è¯¯çš„å·¥å…·é€šå¸¸æ£€æµ‹èƒ½åŠ›æœ‰é™ã€ä¸èƒ½å¾ˆå¥½åœ°å¤„ç†Rustç‰¹å®šç±»å‹ï¼Œæˆ–è€…ä¸¥é‡ä¾èµ–äººå·¥å¹²é¢„ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†deepSURFå·¥å…·ï¼Œå®ƒç»“åˆäº†é™æ€åˆ†æä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼çš„æ¨¡ç³Šæµ‹è¯•æ¡†æ¶ç”ŸæˆæŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«Ruståº“ä¸­çš„å†…å­˜å®‰å…¨æ¼æ´ï¼Œä¸»è¦é’ˆå¯¹ä¸å®‰å…¨ä»£ç ã€‚deepSURFå¼•å…¥äº†ä¸€ç§å¤„ç†é€šç”¨ç±»å‹çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†å…¶æ›¿æ¢ä¸ºè‡ªå®šä¹‰ç±»å‹å¹¶ä¸ºæ‰€éœ€ç‰¹æ€§ç”Ÿæˆé‡èº«å®šåˆ¶çš„å®ç°ï¼Œä½¿æ¨¡ç³Šæµ‹è¯•å™¨èƒ½å¤Ÿåœ¨æ¨¡ç³Šåº“ä¸­æ¨¡æ‹Ÿç”¨æˆ·å®šä¹‰çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒdeepSURFè¿˜åˆ©ç”¨LLMåŠ¨æ€å¢å¼ºæ¨¡ç³Šæµ‹è¯•æ¡†æ¶ï¼Œä¿ƒè¿›å¤æ‚APIäº¤äº’çš„æ¢ç´¢ï¼Œå¹¶æ˜¾è‘—æé«˜æš´éœ²å†…å­˜å®‰å…¨æ¼æ´çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬åœ¨27ä¸ªçœŸå®çš„Ruståº“ä¸­è¯„ä¼°äº†deepSURFçš„æ•ˆæœï¼Œå®ƒæˆåŠŸé‡æ–°å‘ç°äº†20ä¸ªå·²çŸ¥çš„å†…å­˜å®‰å…¨æ¼æ´ï¼Œå¹¶æ­ç¤ºäº†6ä¸ªä»¥å‰æœªçŸ¥çš„æ¼æ´ï¼Œæ˜¾ç¤ºå‡ºå…¶å¯¹ç°æœ‰å·¥å…·çš„æ˜æ˜¾æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Rustè¯­è¨€è™½é»˜è®¤ç¡®ä¿å†…å­˜å®‰å…¨ï¼Œä½†å…è®¸ä½¿ç”¨å¯èƒ½å¯¼è‡´å†…å­˜å®‰å…¨æ¼æ´çš„ä¸å®‰å…¨ä»£ç ã€‚ç°æœ‰çš„Rustå†…å­˜é”™è¯¯æ£€æµ‹å·¥å…·å­˜åœ¨å±€é™æ€§ï¼Œå¦‚æ£€æµ‹èƒ½åŠ›æœ‰é™ã€å¤„ç†Rustç‰¹å®šç±»å‹ä¸è¶³æˆ–è¿‡äºä¾èµ–äººå·¥å¹²é¢„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†deepSURFå·¥å…·ï¼Œå®ƒç»“åˆäº†é™æ€åˆ†æä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼çš„åŠ¨æ€æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯ï¼Œæ›´æœ‰æ•ˆåœ°è¯†åˆ«Ruståº“ä¸­çš„å†…å­˜å®‰å…¨æ¼æ´ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸å®‰å…¨ä»£ç ã€‚deepSURFé€šè¿‡æ›¿æ¢æ³›å‹å¹¶ç”Ÿæˆç‰¹å®šå®ç°æ¥åˆ›æ–°åœ°å¤„ç†æ³›å‹é—®é¢˜ï¼Œä½¿æ¨¡ç³Šæµ‹è¯•èƒ½å¤Ÿæ¨¡æ‹Ÿåº“ä¸­çš„ç”¨æˆ·è‡ªå®šä¹‰è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒdeepSURFåˆ©ç”¨LLMåŠ¨æ€å¢å¼ºæ¨¡ç³Šæµ‹è¯•å·¥å…·ï¼Œä¾¿äºæ¢ç´¢å¤æ‚çš„APIäº¤äº’ï¼Œæ˜¾è‘—æé«˜æš´éœ²å†…å­˜å®‰å…¨æ¼æ´çš„å¯èƒ½æ€§ã€‚åœ¨27ä¸ªçœŸå®ä¸–ç•Œçš„Rust cratesä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒdeepSURFæˆåŠŸé‡æ–°å‘ç°äº†20ä¸ªå·²çŸ¥çš„å†…å­˜å®‰å…¨æ¼æ´å¹¶å‘ç°äº†6ä¸ªä»¥å‰æœªçŸ¥çš„æ¼æ´ï¼Œæ˜¾ç¤ºå‡ºå¯¹æœ€æ–°æŠ€æœ¯çš„æ˜æ˜¾æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rustå…è®¸ä½¿ç”¨ä¸å®‰å…¨ä»£ç ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥å†…å­˜å®‰å…¨æ¼æ´ã€‚</li>
<li>ç°æœ‰çš„Rustå†…å­˜é”™è¯¯æ£€æµ‹å·¥å…·å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>deepSURFå·¥å…·ç»“åˆé™æ€åˆ†æä¸LLMå¼•å¯¼çš„åŠ¨æ€æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯ï¼Œé’ˆå¯¹Ruståº“ä¸­çš„å†…å­˜å®‰å…¨æ¼æ´è¿›è¡Œæœ‰æ•ˆè¯†åˆ«ã€‚</li>
<li>deepSURFé€šè¿‡å¤„ç†æ³›å‹é—®é¢˜åˆ›æ–°åœ°å¢å¼ºäº†æ¨¡ç³Šæµ‹è¯•çš„æ•ˆæœã€‚</li>
<li>LLMè¢«ç”¨äºåŠ¨æ€å¢å¼ºæ¨¡ç³Šæµ‹è¯•å·¥å…·ï¼Œæé«˜æ¢ç´¢å¤æ‚APIäº¤äº’å’Œæš´éœ²å†…å­˜å®‰å…¨æ¼æ´çš„å¯èƒ½æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®Rust cratesä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒdeepSURFæˆåŠŸå‘ç°äº†å¤šä¸ªå·²çŸ¥å’ŒæœªçŸ¥çš„å†…å­˜å®‰å…¨æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb5a482ef4b5697d2d908c9ae147fd11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff437c76285174096c38c1886a484e8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bf6faaafaf57f0a052aa100c3a8b5b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b82c0b6a70dd69009aa16b8c9d0cd2a6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Compositional-Architecture-of-Regret-in-Large-Language-Models"><a href="#The-Compositional-Architecture-of-Regret-in-Large-Language-Models" class="headerlink" title="The Compositional Architecture of Regret in Large Language Models"></a>The Compositional Architecture of Regret in Large Language Models</h2><p><strong>Authors:Xiangxiang Cui, Shu Yang, Tianjin Huang, Wanyu Lin, Lijie Hu, Di Wang</strong></p>
<p>Regret in Large Language Models refers to their explicit regret expression when presented with evidence contradicting their previously generated misinformation. Studying the regret mechanism is crucial for enhancing model reliability and helps in revealing how cognition is coded in neural networks. To understand this mechanism, we need to first identify regret expressions in model outputs, then analyze their internal representation. This analysis requires examining the modelâ€™s hidden states, where information processing occurs at the neuron level. However, this faces three key challenges: (1) the absence of specialized datasets capturing regret expressions, (2) the lack of metrics to find the optimal regret representation layer, and (3) the lack of metrics for identifying and analyzing regret neurons. Addressing these limitations, we propose: (1) a workflow for constructing a comprehensive regret dataset through strategically designed prompting scenarios, (2) the Supervised Compression-Decoupling Index (S-CDI) metric to identify optimal regret representation layers, and (3) the Regret Dominance Score (RDS) metric to identify regret neurons and the Group Impact Coefficient (GIC) to analyze activation patterns. Our experimental results successfully identified the optimal regret representation layer using the S-CDI metric, which significantly enhanced performance in probe classification experiments. Additionally, we discovered an M-shaped decoupling pattern across model layers, revealing how information processing alternates between coupling and decoupling phases. Through the RDS metric, we categorized neurons into three distinct functional groups: regret neurons, non-regret neurons, and dual neurons. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é—æ†¾æŒ‡çš„æ˜¯åœ¨é¢å¯¹ä¸å…¶å…ˆå‰ç”Ÿæˆçš„é”™è¯¯ä¿¡æ¯ç›¸çŸ›ç›¾çš„è¯æ®æ—¶ï¼Œå®ƒä»¬è¡¨ç°å‡ºçš„æ˜ç¡®é—æ†¾çš„è¡¨è¾¾ã€‚ç ”ç©¶åæ‚”æœºåˆ¶å¯¹äºæé«˜æ¨¡å‹å¯é æ€§å¹¶æ­ç¤ºç¥ç»ç½‘ç»œä¸­çš„è®¤çŸ¥ç¼–ç æ–¹å¼è‡³å…³é‡è¦ã€‚ä¸ºäº†ç†è§£è¿™ä¸€æœºåˆ¶ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è¯†åˆ«æ¨¡å‹è¾“å‡ºä¸­çš„é—æ†¾è¡¨è¾¾ï¼Œç„¶ååˆ†æå®ƒä»¬çš„å†…éƒ¨è¡¨ç¤ºã€‚è¿™ç§åˆ†æéœ€è¦æ£€æŸ¥æ¨¡å‹çš„éšè—çŠ¶æ€ï¼Œå³ä¿¡æ¯å¤„ç†å‘ç”Ÿåœ¨ç¥ç»å…ƒå±‚é¢ã€‚ç„¶è€Œï¼Œè¿™é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç¼ºä¹æ•æ‰é—æ†¾è¡¨è¾¾çš„ä¸“ç”¨æ•°æ®é›†ï¼Œï¼ˆ2ï¼‰ç¼ºä¹æ‰¾åˆ°æœ€ä½³åæ‚”è¡¨ç¤ºå±‚çš„æŒ‡æ ‡ï¼Œï¼ˆ3ï¼‰ç¼ºä¹è¯†åˆ«å’Œåˆ†æåæ‚”ç¥ç»å…ƒçš„æŒ‡æ ‡ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºï¼šï¼ˆ1ï¼‰é€šè¿‡æˆ˜ç•¥è®¾è®¡çš„æç¤ºåœºæ™¯æ„å»ºå…¨é¢çš„é—æ†¾æ•°æ®é›†çš„æµç¨‹ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨ç›‘ç£å‹ç¼©-è§£è€¦æŒ‡æ•°ï¼ˆS-CDIï¼‰æŒ‡æ ‡æ¥è¯†åˆ«æœ€ä½³åæ‚”è¡¨ç¤ºå±‚ï¼Œï¼ˆ3ï¼‰ä½¿ç”¨åæ‚”æ”¯é…åˆ†æ•°ï¼ˆRDSï¼‰æŒ‡æ ‡æ¥è¯†åˆ«åæ‚”ç¥ç»å…ƒï¼Œå¹¶ä½¿ç”¨ç»„å½±å“ç³»æ•°ï¼ˆGICï¼‰æ¥åˆ†ææ¿€æ´»æ¨¡å¼ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæˆåŠŸåœ°ä½¿ç”¨S-CDIæŒ‡æ ‡ç¡®å®šäº†æœ€ä½³åæ‚”è¡¨ç¤ºå±‚ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ¢é’ˆåˆ†ç±»å®éªŒçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹å±‚ä¹‹é—´å­˜åœ¨Må‹è§£è€¦æ¨¡å¼ï¼Œæ­ç¤ºäº†ä¿¡æ¯å¤„ç†å¦‚ä½•åœ¨è€¦åˆå’Œè§£è€¦é˜¶æ®µä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚é€šè¿‡RDSæŒ‡æ ‡ï¼Œæˆ‘ä»¬å°†ç¥ç»å…ƒåˆ†ä¸ºä¸‰ç§ä¸åŒçš„åŠŸèƒ½ç»„ï¼šåæ‚”ç¥ç»å…ƒã€éåæ‚”ç¥ç»å…ƒå’ŒåŒç¥ç»å…ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15617v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„åæ‚”è¡¨è¾¾ç ”ç©¶å¯¹äºæé«˜æ¨¡å‹å¯é æ€§å’Œæ­ç¤ºç¥ç»ç½‘ç»œä¸­çš„è®¤çŸ¥ç¼–ç æœºåˆ¶è‡³å…³é‡è¦ã€‚ä¸ºäº†ç†è§£åæ‚”æœºåˆ¶ï¼Œéœ€è¦é¦–å…ˆåœ¨æ¨¡å‹è¾“å‡ºä¸­è¯†åˆ«åæ‚”è¡¨è¾¾ï¼Œç„¶ååˆ†æå®ƒä»¬çš„å†…éƒ¨è¡¨ç¤ºã€‚è¿™é¡¹åˆ†æè¦æ±‚æ£€æŸ¥æ¨¡å‹çš„éšè—çŠ¶æ€ï¼Œå³åœ¨ç¥ç»å…ƒå±‚é¢å‘ç”Ÿä¿¡æ¯å¤„ç†çš„åœ°æ–¹ã€‚ç„¶è€Œï¼Œè¿™é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹æ•æ‰åæ‚”è¡¨è¾¾çš„ä¸“é—¨æ•°æ®é›†ã€ç¼ºä¹æ‰¾åˆ°æœ€ä½³åæ‚”è¡¨ç¤ºå±‚çš„æŒ‡æ ‡ã€ä»¥åŠç¼ºä¹è¯†åˆ«å’Œåˆ†æåæ‚”ç¥ç»å…ƒçš„æŒ‡æ ‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†æ„å»ºåæ‚”æ•°æ®é›†çš„æµç¨‹ã€ç›‘ç£å‹ç¼©è§£è€¦æŒ‡æ•°ï¼ˆS-CDIï¼‰æŒ‡æ ‡æ¥è¯†åˆ«æœ€ä½³åæ‚”è¡¨ç¤ºå±‚ï¼Œä»¥åŠåæ‚”æ”¯é…åˆ†æ•°ï¼ˆRDSï¼‰å’Œé›†å›¢å½±å“ç³»æ•°ï¼ˆGICï¼‰æŒ‡æ ‡æ¥è¯†åˆ«å’Œåˆ†æåæ‚”ç¥ç»å…ƒã€‚å®éªŒæˆåŠŸä½¿ç”¨S-CDIæŒ‡æ ‡ç¡®å®šäº†æœ€ä½³åæ‚”è¡¨ç¤ºå±‚ï¼Œå¹¶æ˜¾è‘—æé«˜æ¢æµ‹åˆ†ç±»å®éªŒçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå‘ç°äº†æ¨¡å‹å±‚ä¹‹é—´çš„Må‹è§£è€¦æ¨¡å¼ï¼Œæ­ç¤ºäº†ä¿¡æ¯å¤„ç†å¦‚ä½•åœ¨è€¦åˆå’Œè§£è€¦é˜¶æ®µä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚é€šè¿‡RDSæŒ‡æ ‡ï¼Œå°†ç¥ç»å…ƒåˆ†ä¸ºä¸‰ç§ä¸åŒçš„åŠŸèƒ½ç»„åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„åæ‚”è¡¨è¾¾ç ”ç©¶å¯¹äºå¢å¼ºæ¨¡å‹å¯é æ€§å’Œç†è§£ç¥ç»ç½‘ç»œè®¤çŸ¥ç¼–ç è‡³å…³é‡è¦ã€‚</li>
<li>éœ€è¦è¯†åˆ«æ¨¡å‹è¾“å‡ºä¸­çš„åæ‚”è¡¨è¾¾å¹¶åˆ†æå…¶å†…éƒ¨è¡¨ç¤ºï¼Œæ¶‰åŠæ£€æŸ¥æ¨¡å‹çš„éšè—çŠ¶æ€ã€‚</li>
<li>é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ä¸“é—¨æ•°æ®é›†ã€ç¼ºä¹è¯†åˆ«æœ€ä½³åæ‚”è¡¨ç¤ºå±‚çš„æŒ‡æ ‡ã€ä»¥åŠç¼ºä¹åæ‚”ç¥ç»å…ƒçš„è¯†åˆ«å’Œåˆ†ææŒ‡æ ‡ã€‚</li>
<li>æå‡ºæ„å»ºåæ‚”æ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡æˆ˜ç•¥æ€§è®¾è®¡æç¤ºåœºæ™¯æ¥å®ç°ã€‚</li>
<li>å¼•å…¥S-CDIæŒ‡æ ‡æ¥è¯†åˆ«æœ€ä½³åæ‚”è¡¨ç¤ºå±‚ï¼ŒæˆåŠŸåº”ç”¨äºå®éªŒå¹¶æ˜¾è‘—æé«˜æ¢æµ‹åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>å‘ç°æ¨¡å‹å±‚ä¹‹é—´çš„Må‹è§£è€¦æ¨¡å¼ï¼Œæ˜¾ç¤ºä¿¡æ¯å¤„ç†åœ¨è€¦åˆä¸è§£è€¦é˜¶æ®µä¹‹é—´çš„äº¤æ›¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c32a639f40a6df25a452b0b522122c01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2daf2759d5fc685bbf527c1d94d64691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d79c9b08c0080b81749746808157576.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LoX-Low-Rank-Extrapolation-Robustifies-LLM-Safety-Against-Fine-tuning"><a href="#LoX-Low-Rank-Extrapolation-Robustifies-LLM-Safety-Against-Fine-tuning" class="headerlink" title="LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning"></a>LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</h2><p><strong>Authors:Gabrel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong</strong></p>
<p>Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the modelâ€™s adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com&#x2F;VITA-Group&#x2F;LoX. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å·²æˆä¸ºä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†é‡å¤§çš„å®‰å…¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å›ç­”æœ‰å®³äºç¤¾ä¼šçš„é—®é¢˜æ—¶ã€‚å°½ç®¡äººä»¬é€šè¿‡æ¨¡å‹å¯¹é½æ¥æé«˜æ¨¡å‹å®‰å…¨æ€§çš„åŠªåŠ›å·¨å¤§ï¼Œä½†éšåçš„å¾®è°ƒä»ç„¶å¯èƒ½ç ´åå¯¹é½æ¨¡å‹çš„å®‰å…¨ä¿æŠ¤ï¼Œå³ä½¿é¢å¤–çš„è®­ç»ƒæ•°æ®çœ‹ä¼¼è‰¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯è¡¨æ˜ï¼Œè¿™ç§è„†å¼±æ€§æºäºLLMå‚æ•°ä¸­å®‰å…¨å…³é”®ä½é˜¶å­ç©ºé—´å¯¹å¾®è°ƒçš„æ•æ„Ÿæ€§ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºä½é˜¶å¤–æ¨ï¼ˆLoXï¼‰ï¼Œé€šè¿‡å¤–æ¨å¯¹é½LLMçš„å®‰å…¨å­ç©ºé—´æ¥æé«˜å®‰å…¨ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯å®äº†LoXçš„æœ‰æ•ˆæ€§ï¼Œåœ¨åº”å¯¹è‰¯æ€§æˆ–æ¶æ„å¾®è°ƒæ”»å‡»æ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„é€‚åº”æ€§ã€‚ä¾‹å¦‚ï¼ŒLoXå¯¼è‡´é¢å¯¹è‰¯æ€§æˆ–æ¶æ„å¾®è°ƒæ”»å‡»æ—¶çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ç»å¯¹é™ä½äº†11%è‡³54%ã€‚é€šè¿‡è°ƒæŸ¥ASRå‚æ•°æ™¯è§‚ï¼Œæˆ‘ä»¬å°†LoXçš„æˆåŠŸå½’å› äºå¤–æ¨å°†LLMå‚æ•°ç§»åŠ¨åˆ°æ›´å¹³å¦çš„åŒºåŸŸï¼Œä»è€Œå‡å°‘å¯¹æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚ä»£ç å¯åœ¨github.com&#x2F;VITA-Group&#x2F;LoXæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15606v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®åº”ç”¨ä¸­çš„æ™®åŠå¸¦æ¥äº†å®‰å…¨æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å¯¹ç¤¾ä¼šæœ‰å®³é—®é¢˜çš„å›åº”ã€‚å°½ç®¡æœ‰åŠªåŠ›é€šè¿‡å¯¹é½æ”¹å–„æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä½†éšåçš„å¾®è°ƒä»å¯èƒ½ç ´åå…¶å®‰å…¨ä¿æŠ¤ï¼Œå³ä½¿é¢å¤–çš„è®­ç»ƒæ•°æ®çœ‹ä¼¼æ— å®³ã€‚æœ¬æ–‡å®è¯è¡¨æ˜ï¼Œè¿™ç§è„†å¼±æ€§æºäºLLMå‚æ•°ä¸­çš„å®‰å…¨å…³é”®ä½é˜¶å­ç©ºé—´å¯¹ç»†å¾®è°ƒæ•´çš„æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ— éœ€è®­ç»ƒçš„æ–¹æ³•â€”â€”ä½é˜¶å¤–æ¨ï¼ˆLoXï¼‰ï¼Œé€šè¿‡å¤–æ¨å¯¹é½LLMçš„å®‰å…¨å­ç©ºé—´æ¥æé«˜å…¶å®‰å…¨ç¨³å¥æ€§ã€‚å®éªŒè¯å®ï¼ŒLoXåœ¨åº”å¯¹è‰¯æ€§åŠæ¶æ„å¾®è°ƒæ”»å‡»æ—¶æ•ˆæœæ˜¾è‘—ï¼Œæ—¢æé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œåˆä¿æŒäº†å…¶é€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œé¢å¯¹è‰¯æ€§æˆ–æ¶æ„å¾®è°ƒæ”»å‡»æ—¶ï¼ŒLoXçš„ç»å¯¹é™ä½æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¾¾åˆ°11%~54%ã€‚è°ƒæŸ¥å‘ç°ï¼ŒLoXçš„æˆåŠŸå½’å› äºå…¶èƒ½å°†LLMå‚æ•°ç§»åŠ¨åˆ°è¾ƒå¹³å¦çš„åŒºåŸŸï¼Œä»è€Œé™ä½å¯¹æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³github.com&#x2F;VITA-Group&#x2F;LoXä¾›æŸ¥é˜…å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®åº”ç”¨ä¸­å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹ç¤¾ä¼šæœ‰å®³é—®é¢˜æ—¶å¯èƒ½å­˜åœ¨é£é™©ã€‚</li>
<li>å¯¹é½æ¨¡å‹çš„å®‰å…¨ä¿æŠ¤å¯èƒ½å› åç»­çš„å¾®è°ƒè€Œå¤±æ•ˆï¼Œå³ä½¿è®­ç»ƒæ•°æ®çœ‹ä¼¼æ— å®³ã€‚</li>
<li>LLMçš„è„†å¼±æ€§æºäºå…¶å‚æ•°ä¸­çš„å®‰å…¨å…³é”®ä½é˜¶å­ç©ºé—´å¯¹ç»†å¾®è°ƒæ•´çš„æ•æ„Ÿæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ— éœ€è®­ç»ƒçš„æ–¹æ³•â€”â€”ä½é˜¶å¤–æ¨ï¼ˆLoXï¼‰ï¼Œé€šè¿‡å¤–æ¨å®‰å…¨å­ç©ºé—´æé«˜LLMçš„å®‰å…¨ç¨³å¥æ€§ã€‚</li>
<li>LoXåœ¨åº”å¯¹è‰¯æ€§åŠæ¶æ„å¾®è°ƒæ”»å‡»æ—¶æ˜¾è‘—æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒå…¶é€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>LoXå°†LLMå‚æ•°ç§»åŠ¨åˆ°è¾ƒå¹³å¦çš„åŒºåŸŸï¼Œé™ä½å¯¹æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œè¿™æ˜¯å…¶æˆåŠŸçš„å…³é”®åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bbfab8c2235ef018efff4ef16d0e293.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-972ea3baff8a06c9d47ebda4605eed16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2d1afc7cb5457b8a916033e74fa6995.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RePCS-Diagnosing-Data-Memorization-in-LLM-Powered-Retrieval-Augmented-Generation"><a href="#RePCS-Diagnosing-Data-Memorization-in-LLM-Powered-Retrieval-Augmented-Generation" class="headerlink" title="RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented   Generation"></a>RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented   Generation</h2><p><strong>Authors:Le Vu Anh, Nguyen Viet Anh, Mehmet Dik, Luong Van Nghia</strong></p>
<p>Retrieval-augmented generation (RAG) has become a common strategy for updating large language model (LLM) responses with current, external information. However, models may still rely on memorized training data, bypass the retrieved evidence, and produce contaminated outputs. We introduce Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects such behavior without requiring model access or retraining. RePCS compares two inference paths: (i) a parametric path using only the query, and (ii) a retrieval-augmented path using both the query and retrieved context by computing the Kullback-Leibler (KL) divergence between their output distributions. A low divergence suggests that the retrieved context had minimal impact, indicating potential memorization. This procedure is model-agnostic, requires no gradient or internal state access, and adds only a single additional forward pass. We further derive PAC-style guarantees that link the KL threshold to user-defined false positive and false negative rates. On the Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result outperforms the strongest prior method by 6.5 percentage points while keeping latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight, black-box safeguard to verify whether a RAG system meaningfully leverages retrieval, making it especially valuable in safety-critical applications. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºä¸€ç§å¸¸è§ç­–ç•¥ï¼Œç”¨äºåˆ©ç”¨å½“å‰å¤–éƒ¨ä¿¡æ¯æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“åº”ã€‚ç„¶è€Œï¼Œæ¨¡å‹ä»ç„¶å¯èƒ½ä¾èµ–è®°å¿†è®­ç»ƒæ•°æ®ï¼Œç»•è¿‡æ£€ç´¢åˆ°çš„è¯æ®ï¼Œå¹¶äº§ç”Ÿå—æ±¡æŸ“çš„è¾“å‡ºã€‚æˆ‘ä»¬å¼•å…¥äº†æ£€ç´¢è·¯å¾„æ±¡æŸ“è¯„åˆ†ï¼ˆRePCSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¯Šæ–­æ–¹æ³•ï¼Œæ— éœ€è®¿é—®æ¨¡å‹æˆ–é‡æ–°è®­ç»ƒå³å¯æ£€æµ‹æ­¤ç±»è¡Œä¸ºã€‚RePCSæ¯”è¾ƒäº†ä¸¤ä¸ªæ¨ç†è·¯å¾„ï¼šï¼ˆiï¼‰ä»…ä½¿ç”¨æŸ¥è¯¢çš„å‚æ•°è·¯å¾„ï¼Œï¼ˆiiï¼‰ä½¿ç”¨æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ£€ç´¢å¢å¼ºè·¯å¾„ï¼Œé€šè¿‡è®¡ç®—å®ƒä»¬è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦ã€‚è¾ƒä½çš„æ•£åº¦è¡¨æ˜æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å½±å“å¾ˆå°ï¼Œè¡¨æ˜å¯èƒ½å­˜åœ¨è®°å¿†ã€‚æ­¤è¿‡ç¨‹ä¸æ¨¡å‹æ— å…³ï¼Œæ— éœ€è®¿é—®æ¢¯åº¦æˆ–å†…éƒ¨çŠ¶æ€ï¼Œå¹¶ä¸”åªéœ€è¿›è¡Œä¸€æ¬¡é¢å¤–çš„å‰å‘ä¼ é€’ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¨å¯¼å‡ºä¸KLé˜ˆå€¼ä¸ç”¨æˆ·å®šä¹‰çš„è¯¯æŠ¥ç‡å’Œè¯¯æŠ¥ç‡ç›¸å…³è”çš„PACé£æ ¼ä¿è¯ã€‚åœ¨Prompt-WNQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRePCSè¾¾åˆ°äº†ROC-AUCçš„0.918ã€‚æ­¤ç»“æœä¼˜äºå…ˆå‰æœ€å¼ºæ–¹æ³•ï¼Œåœ¨NVIDIA T4 GPUä¸Šå»¶è¿Ÿå¼€é”€ä½äº4.7%ã€‚RePCSæä¾›äº†ä¸€ä¸ªè½»é‡çº§ã€é»‘ç®±çš„å®‰å…¨ä¿éšœæ¥éªŒè¯RAGç³»ç»Ÿæ˜¯å¦æœ‰æ•ˆåœ°åˆ©ç”¨äº†æ£€ç´¢åŠŸèƒ½ï¼Œä½¿å…¶åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å°¤å…¶æœ‰ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15513v1">PDF</a> 11 pages, 7 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>RAGç­–ç•¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¸¸è§ï¼Œä½†æ¨¡å‹å¯èƒ½ä¾èµ–è®°å¿†åŒ–è®­ç»ƒæ•°æ®å¹¶äº§ç”Ÿæ±¡æŸ“è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥Retrieval-Path Contamination Scoringï¼ˆRePCSï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒä»…ä½¿ç”¨æŸ¥è¯¢çš„å‚æ•°å­—å¾„å’Œä½¿ç”¨æŸ¥è¯¢å’Œæ£€ç´¢ä¸Šä¸‹æ–‡çš„æ£€ç´¢å¢å¼ºè·¯å¾„ä¹‹é—´çš„è¾“å‡ºåˆ†å¸ƒæ¥è®¡ç®—Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦ï¼Œä»¥æ£€æµ‹æ¨¡å‹è¡Œä¸ºã€‚ä½æ•£åº¦è¡¨ç¤ºæ£€ç´¢ä¸Šä¸‹æ–‡å½±å“è¾ƒå°ï¼Œå¯èƒ½å­˜åœ¨è®°å¿†åŒ–ã€‚è¯¥æ–¹æ³•æ¨¡å‹æ— å…³ï¼Œæ— éœ€æ¢¯åº¦æˆ–å†…éƒ¨çŠ¶æ€è®¿é—®ï¼Œåªéœ€é¢å¤–çš„å‰å‘ä¼ é€’ã€‚åœ¨Prompt-WNQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRePCSè¾¾åˆ°ROC-AUC 0.918ï¼Œè¾ƒä¹‹å‰æœ€ä½³æ–¹æ³•æé«˜6.5ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶åœ¨NVIDIA T4 GPUä¸Šçš„å»¶è¿Ÿå¼€é”€ä½äº4.7%ã€‚RePCSä¸ºRAGç³»ç»Ÿæ˜¯å¦æœ‰æ•ˆåˆ©ç”¨æ£€ç´¢æä¾›äº†è½»é‡çº§ã€é»‘ç®±ä¿éšœï¼Œå°¤å…¶åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ä»·å€¼æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯LLMä¸­å¸¸è§çš„ç­–ç•¥ï¼Œä½†å­˜åœ¨æ¨¡å‹ä¾èµ–è®°å¿†åŒ–è®­ç»ƒæ•°æ®å’Œäº§ç”Ÿæ±¡æŸ“è¾“å‡ºçš„é—®é¢˜ã€‚</li>
<li>Retrieval-Path Contamination Scoringï¼ˆRePCSï¼‰æ–¹æ³•é€šè¿‡æ¯”è¾ƒä¸åŒæ¨ç†è·¯å¾„çš„è¾“å‡ºåˆ†å¸ƒæ¥æ£€æµ‹æ¨¡å‹è¡Œä¸ºã€‚</li>
<li>RePCSæ–¹æ³•é€šè¿‡è®¡ç®—Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦æ¥è¯„ä¼°æ£€ç´¢ä¸Šä¸‹æ–‡çš„å½±å“ï¼Œä½æ•£åº¦å¯èƒ½è¡¨ç¤ºå­˜åœ¨è®°å¿†åŒ–ã€‚</li>
<li>RePCSæ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€æ¢¯åº¦æˆ–å†…éƒ¨çŠ¶æ€è®¿é—®ï¼Œä»…éœ€è¦é¢å¤–çš„å‰å‘ä¼ é€’ã€‚</li>
<li>RePCSåœ¨Prompt-WNQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>RePCSçš„å»¶è¿Ÿå¼€é”€è¾ƒä½ï¼Œå¯¹å®‰å…¨å…³é”®åº”ç”¨å…·æœ‰æ˜¾è‘—ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce23268a9cba0fefeaa95cbdd1f7a995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da5b84b2df16b818e087c2086590e127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06d3d6a613e82f9c9f6ce849f9e0af68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d545d7d09fd91e6cb983a74740d2ccc1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Optimizing-Web-Based-AI-Query-Retrieval-with-GPT-Integration-in-LangChain-A-CoT-Enhanced-Prompt-Engineering-Approach"><a href="#Optimizing-Web-Based-AI-Query-Retrieval-with-GPT-Integration-in-LangChain-A-CoT-Enhanced-Prompt-Engineering-Approach" class="headerlink" title="Optimizing Web-Based AI Query Retrieval with GPT Integration in   LangChain A CoT-Enhanced Prompt Engineering Approach"></a>Optimizing Web-Based AI Query Retrieval with GPT Integration in   LangChain A CoT-Enhanced Prompt Engineering Approach</h2><p><strong>Authors:Wenqi Guan, Yang Fang</strong></p>
<p>Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each studentâ€™s needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºè¿œç¨‹å­¦ä¹ å­¦ç”Ÿç­‰æ–¹é¢å¸¦æ¥äº†æ·±åˆ»çš„å˜åŒ–ã€‚ç›®å‰è¿œç¨‹å­¦ä¹ èµ„æºæ£€ç´¢åœ¨è¯­å¢ƒæ„ä¹‰æ·±åº¦ä¸Šç¼ºä¹å…¨é¢çš„å¤æ‚å­¦ç”ŸæŸ¥è¯¢ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGPTæ¨¡å‹é›†æˆåœ¨LangChainæ¡†æ¶å†…æ”¹è¿›è¿œç¨‹å­¦ä¹ æ£€ç´¢çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä»¥æ›´åŠ ç›´è§‚å’Œé«˜æ•ˆçš„æ–¹å¼é€šè¿‡è®¤çŸ¥æ¨ç†å’Œæç¤ºå·¥ç¨‹å®ç°è¿™ä¸€ç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶ä¾§é‡äºæé«˜æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼Œä»¥è¿”å›æœ€é€‚åˆæ¯ä¸ªå­¦ç”Ÿéœ€æ±‚çš„å…¨é¢ä¸”è¯­å¢ƒä¸°å¯Œçš„è§£é‡Šå’Œèµ„æºã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸å…¸å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŠ¥å‘Šäº†ç”¨æˆ·æ»¡æ„åº¦å’Œå­¦ä¹ æˆæœçš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15512v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºè¿œç¨‹å­¦ä¹ å­¦ç”Ÿç­‰æ–¹é¢å¸¦æ¥äº†æ·±åˆ»å˜é©ã€‚å½“å‰è¿œç¨‹å­¦ä¹ èµ„æºæ£€ç´¢ç¼ºä¹æ·±åº¦è¯­å¢ƒæ„ä¹‰ï¼Œæ— æ³•æä¾›å…³äºå¤æ‚å­¦ç”ŸæŸ¥è¯¢çš„å…¨é¢ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å°†GPTæ¨¡å‹é›†æˆåˆ°LangChainæ¡†æ¶ä¸­ï¼Œå¢å¼ºè¿œç¨‹å­¦ä¹ èµ„æºæ£€ç´¢çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬è¿ç”¨è®¤çŸ¥æ¨ç†å’Œæç¤ºå·¥ç¨‹ä½¿ç³»ç»Ÿæ›´ç›´è§‚ã€æ›´å…·ç”Ÿäº§åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶ç€é‡æé«˜æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼Œä»¥è¿”å›æœ€ç¬¦åˆæ¯ä¸ªå­¦ç”Ÿéœ€æ±‚çš„å…¨é¢ä¸”ä¸°å¯Œè¯­å¢ƒçš„è§£é‡Šå’Œèµ„æºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºå…¸å‹çš„LLMçš„æ•ˆæœï¼Œå¹¶æŠ¥å‘Šäº†æé«˜çš„ç”¨æˆ·æ»¡æ„åº¦å’Œå­¦ä¹ æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºè¿œç¨‹æ•™è‚²å¸¦æ¥äº†å˜é©ã€‚</li>
<li>å½“å‰è¿œç¨‹å­¦ä¹ èµ„æºæ£€ç´¢ç¼ºä¹æ·±åº¦è¯­å¢ƒç†è§£ã€‚</li>
<li>æè®®åœ¨LangChainæ¡†æ¶å†…é›†æˆGPTæ¨¡å‹ä»¥å¢å¼ºè¿œç¨‹å­¦ä¹ èµ„æºæ£€ç´¢ã€‚</li>
<li>é€šè¿‡è®¤çŸ¥æ¨ç†å’Œæç¤ºå·¥ç¨‹ä½¿ç³»ç»Ÿæ›´ç›´è§‚å’Œé«˜æ•ˆã€‚</li>
<li>æ¡†æ¶å¼ºè°ƒæé«˜æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ç”¨æˆ·æ»¡æ„åº¦å’Œå­¦ä¹ æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ae2bcd5430dec1e678a612cc1899afeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be30d3b57d62bfb4a11d67b523a42e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8886d187aef424cd5e747baf67935590.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebc99c277eb4de7b2c01d958d7ffadb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Context-Informed-Grounding-Supervision"><a href="#Context-Informed-Grounding-Supervision" class="headerlink" title="Context-Informed Grounding Supervision"></a>Context-Informed Grounding Supervision</h2><p><strong>Authors:Hyunji Lee, Seunghyun Yoon, Yunjae Won, Hanseok Oh, Geewook Kim, Trung Bui, Franck Dernoncourt, Elias Stengel-Eskin, Mohit Bansal, Minjoon Seo</strong></p>
<p>Large language models (LLMs) are often supplemented with external knowledge to provide information not encoded in their parameters or to reduce hallucination. In such cases, we expect the model to generate responses by grounding its response in the provided external context. However, prior work has shown that simply appending context at inference time does not ensure grounded generation. To address this, we propose Context-INformed Grounding Supervision (CINGS), a post-training supervision in which the model is trained with relevant context prepended to the response, while computing the loss only over the response tokens and masking out the context. Our experiments demonstrate that models trained with CINGS exhibit stronger grounding in both textual and visual domains compared to standard instruction-tuned models. In the text domain, CINGS outperforms other training methods across 11 information-seeking datasets and is complementary to inference-time grounding techniques. In the vision-language domain, replacing a vision-language modelâ€™s LLM backbone with a CINGS-trained model reduces hallucinations across four benchmarks and maintains factual consistency throughout the generated response. This improved grounding comes without degradation in general downstream performance. Finally, we analyze the mechanism underlying the enhanced grounding in CINGS and find that it induces a shift in the modelâ€™s prior knowledge and behavior, implicitly encouraging greater reliance on the external context. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ä¼šå€ŸåŠ©å¤–éƒ¨çŸ¥è¯†æ¥æä¾›æœªåœ¨å…¶å‚æ•°ä¸­ç¼–ç çš„ä¿¡æ¯æˆ–å‡å°‘å¹»è§‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿåœ¨æä¾›çš„å¤–éƒ¨ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆå“åº”ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼Œä»…åœ¨æ¨ç†æ—¶é—´è¿½åŠ ä¸Šä¸‹æ–‡å¹¶ä¸èƒ½ç¡®ä¿ç”Ÿæˆæœ‰æ ¹æ®çš„å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Context-INformed Grounding Supervisionï¼ˆCINGSï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åœ¨è®­ç»ƒåè¿›è¡Œçš„ç›‘ç£æ–¹å¼ï¼Œåœ¨è¿™ç§æ–¹å¼ä¸­ï¼Œæ¨¡å‹ä¼šä½¿ç”¨ä¸å“åº”ç›¸å…³çš„ä¸Šä¸‹æ–‡è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨è®¡ç®—æŸå¤±æ—¶ä»…å¯¹å“åº”æ ‡è®°è¿›è¡Œè®¡ç®—å¹¶å±è”½ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡CINGSè®­ç»ƒçš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰é¢†åŸŸä¸­çš„å®šä½èƒ½åŠ›æ›´å¼ºï¼Œç›¸è¾ƒäºæ ‡å‡†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹è¡¨ç°æ›´ä¼˜ã€‚åœ¨æ–‡æœ¬é¢†åŸŸï¼ŒCINGSåœ¨11ä¸ªä¿¡æ¯æœç´¢æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸æ¨ç†æ—¶é—´å®šä½æŠ€æœ¯äº’è¡¥ã€‚åœ¨è§†è§‰è¯­è¨€é¢†åŸŸï¼Œç”¨CINGSè®­ç»ƒè¿‡çš„æ¨¡å‹æ›¿æ¢è§†è§‰è¯­è¨€æ¨¡å‹çš„LLMä¸»å¹²ï¼Œèƒ½å¤Ÿåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å°‘å¹»è§‰ï¼Œå¹¶åœ¨æ•´ä¸ªç”Ÿæˆå“åº”ä¸­ä¿æŒäº‹å®ä¸€è‡´æ€§ã€‚è¿™ç§æ”¹è¿›çš„å®šä½èƒ½åŠ›å¹¶ä¸ä¼šé™ä½é€šç”¨ä¸‹æ¸¸æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†CINGSä¸­å¢å¼ºå®šä½èƒ½åŠ›çš„æœºåˆ¶ï¼Œå‘ç°å®ƒä¼šå¼•èµ·æ¨¡å‹å…ˆéªŒçŸ¥è¯†å’Œè¡Œä¸ºçš„å˜åŒ–ï¼Œéšå«åœ°é¼“åŠ±æ›´å¤šåœ°ä¾èµ–å¤–éƒ¨ä¸Šä¸‹æ–‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15480v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>LLMé€šå¸¸ä¸å¤–éƒ¨çŸ¥è¯†ç»“åˆä»¥æä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯æˆ–å‡å°‘å¹»æƒ³å›åº”ã€‚ä½†ä»…å°†ä¸Šä¸‹æ–‡é™„åŠ åœ¨æ¨ç†é˜¶æ®µæ— æ³•ä¿è¯å›åº”çš„å¯é æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºContext-INformed Grounding Supervisionï¼ˆCINGSï¼‰çš„åæœŸè®­ç»ƒç›‘ç£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒæ¨¡å‹æ—¶å°†ç›¸å…³ä¸Šä¸‹æ–‡æ·»åŠ åˆ°å›åº”å‰ï¼Œå¹¶åœ¨è®¡ç®—æŸå¤±æ—¶ä»…è€ƒè™‘å›åº”ä»¤ç‰Œè€Œå¿½ç•¥ä¸Šä¸‹æ–‡ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨CINGSè®­ç»ƒçš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰é¢†åŸŸå‡è¡¨ç°å‡ºæ›´å¼ºçš„ä¾èµ–æ€§ï¼Œç›¸å¯¹äºä¼ ç»ŸæŒ‡ä»¤è®­ç»ƒæ¨¡å‹æ›´å¼ºã€‚åœ¨æ–‡æœ¬é¢†åŸŸï¼ŒCINGSè¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•å¹¶åœ¨è¶…è¿‡çš„ä¿¡æ¯å¯»æ±‚æ•°æ®é›†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸æ¨ç†é˜¶æ®µçš„æ¥åœ°æŠ€æœ¯äº’è¡¥ã€‚åœ¨è§†è§‰è¯­è¨€é¢†åŸŸï¼Œä½¿ç”¨CINGSè®­ç»ƒçš„æ¨¡å‹æ›¿æ¢è§†è§‰è¯­è¨€æ¨¡å‹çš„LLMä¸»å¹²ï¼Œå‡å°‘äº†å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹»æƒ³å›åº”å¹¶ä¿æŒå›åº”çš„å®¢è§‚æ€§ã€‚è¿™ç§æ”¹è¿›æ— éœ€ç‰ºç‰²æ•´ä½“æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†CINGSå¢å¼ºæ¥åœ°æ€§çš„æœºåˆ¶ï¼Œå‘ç°å®ƒæ”¹å˜äº†æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œè¡Œä¸ºï¼Œé—´æ¥é¼“åŠ±æ›´å¤šåœ°ä¾èµ–å¤–éƒ¨ä¸Šä¸‹æ–‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMä¸å¤–éƒ¨çŸ¥è¯†ç»“åˆå¯ä»¥æé«˜å›åº”çš„ä¿¡æ¯ä¸°å¯Œæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ç®€å•çš„åœ¨æ¨ç†é˜¶æ®µæ·»åŠ ä¸Šä¸‹æ–‡ä¸èƒ½ä¿è¯å›åº”çš„å¯é æ€§ã€‚</li>
<li>CINGSæ˜¯ä¸€ç§æœ‰æ•ˆçš„åæœŸè®­ç»ƒç›‘ç£æ–¹æ³•ï¼Œèƒ½æé«˜æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰é¢†åŸŸçš„æ¥åœ°æ€§èƒ½ã€‚</li>
<li>åœ¨æ–‡æœ¬é¢†åŸŸï¼ŒCINGSè¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•å¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•æœ‰æ•ˆã€‚</li>
<li>åœ¨è§†è§‰è¯­è¨€é¢†åŸŸï¼Œä½¿ç”¨CINGSè®­ç»ƒçš„æ¨¡å‹å‡å°‘å¹»æƒ³å›åº”å¹¶ä¿æŒå›åº”çš„å®¢è§‚æ€§ï¼Œä¸å½±å“æ•´ä½“æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61af960b1a5451a363e4e5227c16c12a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b488a686faaa1e17b350f69c12200440.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fce97b4f6def7d65ec1be062205a3d29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d412bf29f039068f16ba8997a9a249de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55ae1fb338abca79f55215491db89c33.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ClimateChat-Designing-Data-and-Methods-for-Instruction-Tuning-LLMs-to-Answer-Climate-Change-Queries"><a href="#ClimateChat-Designing-Data-and-Methods-for-Instruction-Tuning-LLMs-to-Answer-Climate-Change-Queries" class="headerlink" title="ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to   Answer Climate Change Queries"></a>ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to   Answer Climate Change Queries</h2><p><strong>Authors:Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai</strong></p>
<p>As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs. </p>
<blockquote>
<p>éšç€å…¨çƒæ°”å€™å˜åŒ–é—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œå¯¹æ°”å€™ç§‘å­¦ç ”ç©¶çš„éœ€æ±‚æŒç»­å¢é•¿ã€‚ä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºä»£è¡¨çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å·²è¢«å¹¿æ³›åº”ç”¨äºæ°”å€™å˜åŒ–ç›¸å…³ç ”ç©¶ï¼Œä¸ºå†³ç­–è€…å’Œå…¬ä¼—æä¾›é‡è¦çš„ä¿¡æ¯æ”¯æŒã€‚ä¸€äº›ç ”ç©¶é€šè¿‡æ„å»ºä¸æ°”å€™å˜åŒ–ç›¸å…³çš„æŒ‡ä»¤æ•°æ®å¹¶å¯¹LLMè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹åœ¨ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶åœ¨é«˜æ•ˆç”Ÿæˆå¤§é‡é«˜ç²¾åº¦æ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œè¿™é™åˆ¶äº†æ°”å€™å˜åŒ–LLMçš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ„å»ºæŒ‡ä»¤æ•°æ®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æ–‡æ¡£ä¸­çš„äº‹å®å’ŒèƒŒæ™¯çŸ¥è¯†ç”ŸæˆæŒ‡ä»¤ï¼Œå¹¶é€šè¿‡ç½‘ç»œçˆ¬è™«å’Œç§å­æŒ‡ä»¤æ”¶é›†å¢å¼ºæŒ‡ä»¤æ•°æ®çš„å¤šæ ·æ€§ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºClimateChat-Corpusçš„æ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºLLMï¼Œä»è€Œäº§ç”Ÿäº†åä¸ºClimateChatçš„LLMã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒClimateChatåœ¨æ°”å€™å˜åŒ–é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸åŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤æ•°æ®å¯¹LLMæ€§èƒ½çš„å½±å“ï¼Œè¯æ˜äº†å…¶é€‚åº”å„ç§æ°”å€™å˜åŒ–ç§‘å­¦å‘ç°ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¼ºè°ƒäº†é€‰æ‹©é€‚å½“çš„åŸºç¡€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®å’Œè®­ç»ƒé’ˆå¯¹æ°”å€™å˜åŒ–çš„LLMæä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’Œå®è¯æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13796v1">PDF</a> ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables</p>
<p><strong>Summary</strong><br>     éšç€å…¨çƒæ°”å€™å˜åŒ–é—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œæ°”å€™ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶éœ€æ±‚ä¸æ–­å¢é•¿ã€‚è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºä»£è¡¨ï¼Œå·²å¹¿æ³›åº”ç”¨äºæ°”å€™å˜åŒ–ç›¸å…³ç ”ç©¶ï¼Œä¸ºå†³ç­–è€…å’Œå…¬ä¼—æä¾›å¿…è¦çš„ä¿¡æ¯æ”¯æŒã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ„å»ºæŒ‡ä»¤æ•°æ®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ–‡æ¡£ä¸­çš„äº‹å®å’ŒèƒŒæ™¯çŸ¥è¯†ç”ŸæˆæŒ‡ä»¤ï¼Œå¹¶é€šè¿‡ç½‘ç»œçˆ¬è™«å’Œç§å­æŒ‡ä»¤æ”¶é›†å¢å¼ºæŒ‡ä»¤æ•°æ®çš„å¤šæ ·æ€§ã€‚ä½¿ç”¨æ­¤æ–¹æ³•æ„å»ºäº†åä¸ºClimateChat-Corpusçš„æ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºLLMï¼Œç”Ÿæˆäº†åä¸ºClimateChatçš„LLMã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒClimateChatåœ¨æ°”å€™å˜åŒ–é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜è¯„ä¼°äº†ä¸åŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤æ•°æ®å¯¹LLMæ€§èƒ½çš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†å…¶é€‚åº”å„ç§æ°”å€™å˜åŒ–ç§‘å­¦å‘ç°ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¼ºè°ƒäº†é€‰æ‹©é€‚å½“çš„åŸºç¡€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ˆå°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹LLMï¼‰åœ¨æ°”å€™å˜åŒ–ç ”ç©¶ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å½“å‰ç ”ç©¶åœ¨é«˜æ•ˆç”Ÿæˆå¤§é‡é«˜ç²¾åº¦æ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ„å»ºæ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¹¶æ„å»ºäº†ClimateChat-Corpusæ•°æ®é›†ã€‚</li>
<li>ä½¿ç”¨ClimateChat-Corpusæ•°æ®é›†è®­ç»ƒçš„LLMï¼ˆClimateChatï¼‰åœ¨æ°”å€™å˜åŒ–é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>é€‰æ‹©é€‚å½“çš„åŸºç¡€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´å¯¹LLMçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸åŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤æ•°æ®å¯¹LLMçš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†å…¶é€‚åº”å¤šç§æ°”å€™å˜åŒ–ç§‘å­¦å‘ç°ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-651db0cc8a2436665423744a9c4863f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d027a5f8e59d1fd74c1dcd691abdda5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d22af11bd97690cd272c72f6a7702c0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fec7fd224a4f52eee0306fff09266c72.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DDiT-Dynamic-Resource-Allocation-for-Diffusion-Transformer-Model-Serving"><a href="#DDiT-Dynamic-Resource-Allocation-for-Diffusion-Transformer-Model-Serving" class="headerlink" title="DDiT: Dynamic Resource Allocation for Diffusion Transformer Model   Serving"></a>DDiT: Dynamic Resource Allocation for Diffusion Transformer Model   Serving</h2><p><strong>Authors:Heyang Huang, Cunchen Hu, Jiaqi Zhu, Ziyuan Gao, Liangliang Xu, Yizhou Shan, Yungang Bao, Sun Ninghui, Tianwei Zhang, Sa Wang</strong></p>
<p>The Text-to-Video (T2V) model aims to generate dynamic and expressive videos from textual prompts. The generation pipeline typically involves multiple modules, such as language encoder, Diffusion Transformer (DiT), and Variational Autoencoders (VAE). Existing serving systems often rely on monolithic model deployment, while overlooking the distinct characteristics of each module, leading to inefficient GPU utilization. In addition, DiT exhibits varying performance gains across different resolutions and degrees of parallelism, and significant optimization potential remains unexplored. To address these problems, we present DDiT, a flexible system that integrates both inter-phase and intra-phase optimizations. DDiT focuses on two key metrics: optimal degree of parallelism, which prevents excessive parallelism for specific resolutions, and starvation time, which quantifies the sacrifice of each request. To this end, DDiT introduces a decoupled control mechanism to minimize the computational inefficiency caused by imbalances in the degree of parallelism between the DiT and VAE phases. It also designs a greedy resource allocation algorithm with a novel scheduling mechanism that operates at the single-step granularity, enabling dynamic and timely resource scaling. Our evaluation on the T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets reveals that DDiT significantly outperforms state-of-the-art baselines by up to 1.44x in p99 latency and 1.43x in average latency. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹æ—¨åœ¨ä»æ–‡æœ¬æç¤ºç”ŸæˆåŠ¨æ€å’Œè¡¨è¾¾æ€§è§†é¢‘ã€‚ç”Ÿæˆæµç¨‹é€šå¸¸æ¶‰åŠå¤šä¸ªæ¨¡å—ï¼Œå¦‚è¯­è¨€ç¼–ç å™¨ã€æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚ç°æœ‰çš„æœåŠ¡ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå•ä¸€æ¨¡å‹éƒ¨ç½²ï¼Œè€Œå¿½è§†æ¯ä¸ªæ¨¡å—çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼ŒDiTåœ¨ä¸åŒåˆ†è¾¨ç‡å’Œå¹¶è¡Œåº¦ä¸Šè¡¨ç°å‡ºä¸åŒçš„æ€§èƒ½æå‡ï¼Œä»æœ‰å¾ˆå¤§çš„ä¼˜åŒ–æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DDiTï¼Œä¸€ä¸ªèåˆäº†è·¨é˜¶æ®µå’Œå†…éƒ¨é˜¶æ®µä¼˜åŒ–çš„çµæ´»ç³»ç»Ÿã€‚DDiTä¸“æ³¨äºä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼šæœ€ä½³å¹¶è¡Œåº¦ï¼Œé˜²æ­¢ç‰¹å®šåˆ†è¾¨ç‡çš„è¿‡åº¦å¹¶è¡Œï¼›ä»¥åŠé¥¥é¥¿æ—¶é—´ï¼Œé‡åŒ–æ¯ä¸ªè¯·æ±‚çš„ç‰ºç‰²ã€‚ä¸ºæ­¤ï¼ŒDDiTå¼•å…¥äº†è§£è€¦æ§åˆ¶æœºåˆ¶ï¼Œä»¥æœ€å°åŒ–DiTå’ŒVAEé˜¶æ®µå¹¶è¡Œåº¦ä¸å¹³è¡¡é€ æˆçš„è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚å®ƒè¿˜è®¾è®¡äº†ä¸€ç§å¸¦æœ‰æ–°å‹è°ƒåº¦æœºåˆ¶çš„è´ªå©ªèµ„æºåˆ†é…ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨å•æ­¥ç²’åº¦ä¸Šè¿è¡Œï¼Œèƒ½å¤Ÿå®ç°åŠ¨æ€å’ŒåŠæ—¶çš„èµ„æºæ‰©å±•ã€‚æˆ‘ä»¬åœ¨T5ç¼–ç å™¨ã€OpenSora SDDiTå’ŒOpenSora VAEæ¨¡å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDDiTåœ¨p99å»¶è¿Ÿå’Œå¹³å‡å»¶è¿Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œæœ€é«˜å¯è¾¾1.44å€å’Œ1.43å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13497v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹çš„ç”Ÿæˆç®¡é“ï¼Œæå‡ºäº†DDiTç³»ç»Ÿï¼Œå®ç°äº†è·¨é˜¶æ®µå’Œè·¨é˜¶æ®µå†…çš„ä¼˜åŒ–ã€‚é€šè¿‡çµæ´»çš„å¹¶è¡Œåº¦æ§åˆ¶å’Œèµ„æºåˆ†é…ç®—æ³•ï¼Œæé«˜äº†GPUåˆ©ç”¨ç‡å’Œæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>T2Væ¨¡å‹æ—¨åœ¨ä»æ–‡æœ¬æç¤ºç”ŸæˆåŠ¨æ€å’Œè¡¨è¾¾æ€§çš„è§†é¢‘ã€‚</li>
<li>ç°æœ‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå•ä¸€æ¨¡å‹éƒ¨ç½²ï¼Œå¿½ç•¥äº†å„ä¸ªæ¨¡å—çš„ç‰¹æ€§ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä½ä¸‹ã€‚</li>
<li>DDiTç³»ç»Ÿæ˜¯ä¸€ä¸ªçµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰ç³»ç»Ÿçš„ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡è·¨é˜¶æ®µå’Œé˜¶æ®µå†…çš„ä¼˜åŒ–æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>DDiTä¸“æ³¨äºä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼šæœ€ä½³å¹¶è¡Œåº¦å’Œé¥¥é¥¿æ—¶é—´ï¼Œæ—¨åœ¨é‡åŒ–æ¯ä¸ªè¯·æ±‚çš„ç‰ºç‰²ã€‚</li>
<li>DDiTå¼•å…¥äº†è§£è€¦æ§åˆ¶æœºåˆ¶ï¼Œä»¥æœ€å°åŒ–DiTå’ŒVAEé˜¶æ®µå¹¶è¡Œåº¦ä¸å¹³è¡¡å¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>DDiTè®¾è®¡äº†ä¸€ç§è´ªå©ªçš„èµ„æºåˆ†é…ç®—æ³•ï¼Œå…·æœ‰æ–°é¢–çš„è°ƒåº¦æœºåˆ¶ï¼Œä»¥å•æ­¥ç²’åº¦æ“ä½œï¼Œå®ç°åŠ¨æ€å’ŒåŠæ—¶çš„èµ„æºç¼©æ”¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eaedf31f3f7816c304782be6d0ec2200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dd2baa648395cedb77f35b8bebd3049.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e26ff96dec495e3c00e9d957f054a5d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9518aab716ac3b4b616cfd762569488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b86cffb52f13aa00af5934787bbd418.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SeqPE-Transformer-with-Sequential-Position-Encoding"><a href="#SeqPE-Transformer-with-Sequential-Position-Encoding" class="headerlink" title="SeqPE: Transformer with Sequential Position Encoding"></a>SeqPE: Transformer with Sequential Position Encoding</h2><p><strong>Authors:Huayang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe</strong></p>
<p>Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPEâ€™s embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracyâ€“particularly under context length extrapolationâ€“but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at <a target="_blank" rel="noopener" href="https://github.com/ghrua/seqpe">https://github.com/ghrua/seqpe</a>. </p>
<blockquote>
<p>ç”±äºTransformerä¸­çš„è‡ªæ³¨æ„åŠ›å±‚åœ¨è®¾è®¡ä¸Šæ˜¯æ’åˆ—ä¸å˜çš„ï¼Œå¿…é¡»æ˜¾å¼åœ°çº³å…¥ä½ç½®ç¼–ç ä»¥å®ç°ç©ºé—´ç†è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿå¯å­¦ä¹ ä½ç½®åµŒå…¥ï¼ˆPEï¼‰ä¸­ä½¿ç”¨çš„å›ºå®šå¤§å°æŸ¥æ‰¾è¡¨é™åˆ¶äº†é¢„è®­ç»ƒåºåˆ—é•¿åº¦ä¹‹å¤–çš„æ¨æ–­èƒ½åŠ›ã€‚è™½ç„¶ä¸“å®¶è®¾è®¡çš„æ–¹æ³•ï¼ˆå¦‚ALiBiå’ŒRoPEï¼‰ç¼“è§£äº†è¿™ä¸€é™åˆ¶ï¼Œä½†å®ƒä»¬åœ¨é€‚åº”æ–°æ¨¡æ€æ—¶éœ€è¦è¿›è¡Œå¤§é‡ä¿®æ”¹ï¼Œè¿™å‡¸æ˜¾äº†é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§çš„åŸºæœ¬æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SeqPEï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”å¯å®Œå…¨å­¦ä¹ çš„ä½ç½®ç¼–ç æ¡†æ¶ï¼Œå®ƒå°†æ¯ä¸ªnç»´ä½ç½®ç´¢å¼•è¡¨ç¤ºä¸ºä¸€ä¸ªç¬¦å·åºåˆ—ï¼Œå¹¶é‡‡ç”¨ä¸€ä¸ªè½»é‡çº§é¡ºåºä½ç½®ç¼–ç å™¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ å…¶åµŒå…¥ã€‚ä¸ºäº†è§„èŒƒSeqPEçš„åµŒå…¥ç©ºé—´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§äº’è¡¥çš„ç›®æ ‡ï¼šä¸€ç§å¯¹æ¯”ç›®æ ‡ï¼Œå®ƒå°†åµŒå…¥è·ç¦»ä¸é¢„å®šä¹‰çš„ä½ç½®è·ç¦»å‡½æ•°å¯¹é½ï¼›ä»¥åŠä¸€ç§çŸ¥è¯†è’¸é¦æŸå¤±ï¼Œå®ƒå°†ç¦»ç¾¤ä½ç½®åµŒå…¥é”šå®šåˆ°åœ¨åˆ†å¸ƒå†…çš„æ•™å¸ˆè¡¨ç¤ºï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜å¤–æ¨æ€§èƒ½ã€‚åœ¨è¯­è¨€å»ºæ¨¡ã€é•¿ä¸Šä¸‹æ–‡é—®ç­”å’Œ2Då›¾åƒåˆ†ç±»æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒSeqPEä¸ä»…åœ¨å›°æƒ‘åº¦ã€ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰å’Œå‡†ç¡®æ€§æ–¹é¢è¶…è¶Šäº†å¼ºå¤§çš„åŸºå‡†çº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¤–æ¨æ–¹é¢è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œè€Œä¸”èƒ½å¤Ÿæ— ç¼åœ°æ¨å¹¿åˆ°å¤šç»´è¾“å…¥ï¼Œæ— éœ€æ‰‹åŠ¨é‡æ–°è®¾è®¡æ¶æ„ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ghrua/seqpe">https://github.com/ghrua/seqpe</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13277v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å˜æ¢å™¨ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚åœ¨è®¾è®¡ä¸Šæ˜¯æ’åˆ—ä¸å˜çš„ï¼Œå› æ­¤éœ€è¦æ˜¾å¼åœ°èå…¥ä½ç½®ç¼–ç ä»¥å®ç°ç©ºé—´ç†è§£ã€‚ä¼ ç»Ÿçš„å­¦ä¹ å‹ä½ç½®åµŒå…¥ï¼ˆPEï¼‰ä½¿ç”¨çš„å›ºå®šå¤§å°æŸ¥æ‰¾è¡¨é™åˆ¶äº†è¶…è¶Šé¢„è®­ç»ƒåºåˆ—é•¿åº¦çš„æ¨æ¼”èƒ½åŠ›ã€‚SeqPEæ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”å®Œå…¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ¡†æ¶ï¼Œå°†æ¯ä¸ªnç»´ä½ç½®ç´¢å¼•è¡¨ç¤ºä¸ºç¬¦å·åºåˆ—ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§é¡ºåºä½ç½®ç¼–ç å™¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ å…¶åµŒå…¥ã€‚é€šè¿‡å¼•å…¥ä¸¤ç§äº’è¡¥çš„ç›®æ ‡â€”â€”å¯¹æ¯”ç›®æ ‡å’ŒçŸ¥è¯†è’¸é¦æŸå¤±â€”â€”æ¥è§„èŒƒSeqPEçš„åµŒå…¥ç©ºé—´ï¼Œå¯¹é½åµŒå…¥è·ç¦»ä¸é¢„å®šä¹‰çš„ä½ç½®è·ç¦»å‡½æ•°ï¼Œå¹¶å°†éåˆ†å¸ƒä½ç½®åµŒå…¥é”šå®šåˆ°åˆ†å¸ƒå†…çš„æ•™å¸ˆè¡¨ç¤ºï¼Œä»è€Œæé«˜å¤–æ¨æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSeqPEä¸ä»…åœ¨å„ç§ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¤–æ¨æ–¹é¢ï¼Œè€Œä¸”èƒ½å¤Ÿæ— ç¼æ¨å¹¿åˆ°å¤šç»´è¾“å…¥ï¼Œæ— éœ€æ‰‹åŠ¨é‡æ–°è®¾è®¡æ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªæ³¨æ„åŠ›å±‚åœ¨Transformerä¸­æ˜¯æ’åˆ—ä¸å˜çš„ï¼Œéœ€è¦ä½ç½®ç¼–ç æ¥å®ç°ç©ºé—´ç†è§£ã€‚</li>
<li>ä¼ ç»Ÿå­¦ä¹ å‹ä½ç½®åµŒå…¥ä½¿ç”¨çš„å›ºå®šå¤§å°æŸ¥æ‰¾è¡¨é™åˆ¶äº†æ¨æ¼”èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¶…è¿‡é¢„è®­ç»ƒåºåˆ—é•¿åº¦çš„æƒ…å†µæ—¶ã€‚</li>
<li>SeqPEæ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”å®Œå…¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ¡†æ¶ï¼Œå°†ä½ç½®è¡¨ç¤ºä¸ºç¬¦å·åºåˆ—å¹¶ä½¿ç”¨è½»é‡çº§ç¼–ç å™¨å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯¹æ¯”ç›®æ ‡å’ŒçŸ¥è¯†è’¸é¦æŸå¤±æ¥è§„èŒƒSeqPEçš„åµŒå…¥ç©ºé—´ã€‚</li>
<li>SeqPEåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¤–æ¨æ–¹é¢ã€‚</li>
<li>SeqPEèƒ½å¤Ÿæ— ç¼æ¨å¹¿åˆ°å¤šç»´è¾“å…¥ï¼Œæ— éœ€æ‰‹åŠ¨é‡æ–°è®¾è®¡æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-238aa79426343abadef12d563cbede1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cb5474cfa65fda242e7ba916c139878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bad2e513b23003bc6e2afd83e064818.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80dfecb04b6af525f9d9a22be07ec1d2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Survey-on-Continual-Learning-in-Generative-Models"><a href="#A-Comprehensive-Survey-on-Continual-Learning-in-Generative-Models" class="headerlink" title="A Comprehensive Survey on Continual Learning in Generative Models"></a>A Comprehensive Survey on Continual Learning in Generative Models</h2><p><strong>Authors:Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu</strong></p>
<p>The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models remain fundamentally constrained by catastrophic forgetting - a persistent challenge where adapting to new tasks typically leads to significant degradation in performance on previously learned tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative models in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative models, including large language models, multimodal large language models, vision language action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, offering deeper insights into the field. The project page of this paper is available at <a target="_blank" rel="noopener" href="https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models">https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—ç°ä»£AIç³»ç»Ÿèƒ½å¤Ÿç†è§£å¹¶äº§ç”Ÿé«˜åº¦å¤æ‚çš„å†…å®¹ï¼Œç”šè‡³åœ¨ç‰¹å®šé¢†åŸŸè¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å—åˆ°ç¾éš¾æ€§é—å¿˜çš„æ ¹æœ¬æ€§çº¦æŸâ€”â€”è¿™æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œé€‚åº”æ–°ä»»åŠ¡é€šå¸¸ä¼šå¯¼è‡´ä¹‹å‰åœ¨ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å®é™…é™åˆ¶ï¼Œå·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥æé«˜ç”Ÿæˆæ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸»æµç”Ÿæˆæ¨¡å‹çš„æŒç»­å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€è¡Œä¸ºæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚ä»äººç±»å¤§è„‘çš„è®°å¿†æœºåˆ¶ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬å°†è¿™äº›æ–¹æ³•ç³»ç»Ÿåœ°åˆ†ä¸ºä¸‰ç±»ï¼šåŸºäºæ¶æ„çš„æ–¹æ³•ã€åŸºäºæ­£åˆ™åŒ–çš„æ–¹æ³•å’ŒåŸºäºé‡æ¼”çš„æ–¹æ³•ï¼ŒåŒæ—¶é˜è¿°äº†å®ƒä»¬çš„åŸºç¡€æ–¹æ³•å’ŒåŠ¨æœºã€‚è¿›ä¸€æ­¥åˆ†æäº†ä¸åŒç”Ÿæˆæ¨¡å‹çš„æŒç»­å­¦ä¹ è®¾ç½®ï¼ŒåŒ…æ‹¬è®­ç»ƒç›®æ ‡ã€åŸºå‡†æµ‹è¯•å’Œæ ¸å¿ƒéª¨å¹²ï¼Œä¸ºè¿™ä¸€é¢†åŸŸæä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚è¯¥è®ºæ–‡çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Modelsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13045v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç°ä»£AIç³»ç»Ÿå·²ç»èƒ½å¤Ÿç†è§£å¹¶ç”Ÿæˆé«˜åº¦å¤æ‚çš„å†…å®¹ï¼Œç”šè‡³åœ¨ç‰¹å®šé¢†åŸŸè¾¾åˆ°äº†äººç±»æ°´å¹³çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å—åˆ°ç¾éš¾æ€§é—å¿˜è¿™ä¸€åŸºæœ¬çº¦æŸçš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œäººä»¬æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥æé«˜ç”Ÿæˆæ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†ä¸»æµç”Ÿæˆæ¨¡å‹çš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚ä»äººç±»å¤§è„‘çš„è®°å¿†æœºåˆ¶ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬å°†è¿™äº›æ–¹æ³•ç³»ç»Ÿåœ°åˆ†ä¸ºä¸‰ç±»ï¼šåŸºäºæ¶æ„çš„ã€åŸºäºæ­£åˆ™åŒ–çš„å’ŒåŸºäºå¤ä¹ çš„æ–¹æ³•ï¼Œå¹¶é˜æ˜äº†å®ƒä»¬çš„æ–¹æ³•å’ŒåŠ¨æœºã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†ä¸åŒç”Ÿæˆæ¨¡å‹çš„æŒç»­å­¦ä¹ è®¾ç½®ï¼ŒåŒ…æ‹¬è®­ç»ƒç›®æ ‡ã€åŸºå‡†æµ‹è¯•å’Œæ ¸å¿ƒæ¶æ„ï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›äº†æ·±å…¥è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¿«é€Ÿå‘å±•ï¼Œä½¿å¾—ç°ä»£AIç³»ç»Ÿåœ¨ç†è§£å’Œç”Ÿæˆå†…å®¹æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œè¾¾åˆ°äººç±»æ°´å¹³çš„è¡¨ç°ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¾éš¾æ€§é—å¿˜ï¼Œå³é€‚åº”æ–°ä»»åŠ¡æ—¶ä¼šå¯¼è‡´å¯¹å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†å¤šç§æŒç»­å­¦ä¹ æ–¹æ³•æ¥æé«˜ç”Ÿæˆæ¨¡å‹çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†ä¸»æµç”Ÿæˆæ¨¡å‹çš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„æŒç»­å­¦ä¹ ã€‚</li>
<li>æŒç»­å­¦ä¹ æ–¹æ³•åˆ†ä¸ºä¸‰ç±»ï¼šåŸºäºæ¶æ„çš„ã€åŸºäºæ­£åˆ™åŒ–çš„å’ŒåŸºäºå¤ä¹ çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æ·±å…¥åˆ†æäº†ä¸åŒç”Ÿæˆæ¨¡å‹çš„æŒç»­å­¦ä¹ è®¾ç½®ï¼ŒåŒ…æ‹¬è®­ç»ƒç›®æ ‡ã€åŸºå‡†æµ‹è¯•å’Œæ ¸å¿ƒæ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-019fd40638a7048137128264ebfbec4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fca3155cdd747bcf105ae13b5ac20b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb79acbce62808ca195fb4a56b3a3c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f2d4e2cbda0c666ad8a20b6f4c6e18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6c28206af3b72f55f22eed70a125e69.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Automatic-Expert-Discovery-in-LLM-Upcycling-via-Sparse-Interpolated-Mixture-of-Experts"><a href="#Automatic-Expert-Discovery-in-LLM-Upcycling-via-Sparse-Interpolated-Mixture-of-Experts" class="headerlink" title="Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated   Mixture-of-Experts"></a>Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated   Mixture-of-Experts</h2><p><strong>Authors:Shengzhuang Chen, Ying Wei, Jonathan Richard Schwarz</strong></p>
<p>We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning, an end-to-end algorithm designed to fine-tune a dense pre-trained Large Language Model (LLM) into a MoE-style model that possesses capabilities in multiple specialized domains. During instruction-tuning, SIMoE automatically identifies multiple specialized experts under a specified sparsity constraint, with each expert representing a structurally sparse subset of the seed LLMâ€™s parameters that correspond to domain-specific knowledge within the data. SIMoE simultaneously learns an input-dependent expert merging strategy via a router network, leveraging rich cross-expert knowledge for superior downstream generalization that surpasses existing baselines. Empirically, SIMoE consistently achieves state-of-the-art performance on common instruction-tuning benchmarks while maintaining an optimal performance-compute trade-off compared to all baselines. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ç¨€ç–æ’å€¼æ··åˆä¸“å®¶ï¼ˆSparse Interpolated Mixture-of-Expertsï¼Œç®€ç§°SIMoEï¼‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„ç®—æ³•ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„å¯†é›†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒä¸ºå…·å¤‡å¤šä¸ªä¸“ä¸šé¢†åŸŸèƒ½åŠ›çš„æ··åˆä¸“å®¶æ¨¡å‹ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒSIMoEåœ¨æŒ‡å®šçš„ç¨€ç–æ€§çº¦æŸä¸‹è‡ªåŠ¨è¯†åˆ«å¤šä¸ªä¸“ä¸šé¢†åŸŸçš„ä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶ä»£è¡¨ç§å­LLMå‚æ•°çš„ç»“æ„ç¨€ç–å­é›†ï¼Œè¿™äº›å‚æ•°å¯¹åº”äºæ•°æ®ä¸­çš„ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚SIMoEé€šè¿‡è·¯ç”±å™¨ç½‘ç»œåŒæ—¶å­¦ä¹ ä¸€ç§è¾“å…¥ä¾èµ–çš„ä¸“å®¶åˆå¹¶ç­–ç•¥ï¼Œåˆ©ç”¨ä¸°å¯Œçš„è·¨ä¸“å®¶çŸ¥è¯†å®ç°å‡ºè‰²çš„ä¸‹æ¸¸æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶Šç°æœ‰åŸºçº¿ã€‚ç»éªŒä¸Šï¼ŒSIMoEåœ¨å¸¸è§çš„æŒ‡ä»¤å¾®è°ƒåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶ä¸æ‰€æœ‰åŸºçº¿ç›¸æ¯”ä¿æŒäº†æœ€ä½³çš„æ€§èƒ½è®¡ç®—æŠ˜è¡·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12597v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>SIMoEæ˜¯ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒçš„åˆ›æ–°ç®—æ³•ï¼Œæ—¨åœ¨å°†å…¶è½¬åŒ–ä¸ºå…·å¤‡å¤šä¸ªä¸“ä¸šé¢†åŸŸèƒ½åŠ›çš„æ··åˆä¸“å®¶æ¨¡å‹ã€‚è¯¥ç®—æ³•é€šè¿‡æŒ‡ä»¤å¾®è°ƒè‡ªåŠ¨è¯†åˆ«å¤šä¸ªä¸“ä¸šä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶ä»£è¡¨ç§å­LLMå‚æ•°çš„ç»“æ„ç¨€ç–å­é›†ï¼Œå¯¹åº”äºæ•°æ®ä¸­çš„ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚SIMoEåŒæ—¶å­¦ä¹ ä¸€ç§åŸºäºè¾“å…¥çš„ä¸“å®¶åˆå¹¶ç­–ç•¥ï¼Œå¹¶é€šè¿‡è·¯ç”±å™¨ç½‘ç»œåˆ©ç”¨ä¸°å¯Œçš„è·¨ä¸“å®¶çŸ¥è¯†å®ç°å‡ºè‰²çš„ä¸‹æ¸¸æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶Šç°æœ‰åŸºå‡†æµ‹è¯•æ°´å¹³ã€‚æ­¤ç®—æ³•åœ¨æŒ‡ä»¤å¾®è°ƒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨æ€§èƒ½ä¸è®¡ç®—ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SIMoEæ˜¯ä¸€ç§ç”¨äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„ç®—æ³•ï¼Œæ—¨åœ¨å°†å…¶è½¬åŒ–ä¸ºæ··åˆä¸“å®¶æ¨¡å‹ã€‚</li>
<li>è¯¥ç®—æ³•é€šè¿‡æŒ‡ä»¤å¾®è°ƒè‡ªåŠ¨è¯†åˆ«å¤šä¸ªä¸“ä¸šé¢†åŸŸçš„ä¸“å®¶ã€‚</li>
<li>æ¯ä¸ªä¸“å®¶ä»£è¡¨ç§å­LLMå‚æ•°çš„ç»“æ„ç¨€ç–å­é›†ï¼Œå¯¹åº”ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>SIMoEå­¦ä¹ åŸºäºè¾“å…¥çš„ä¸“å®¶åˆå¹¶ç­–ç•¥ï¼Œé€šè¿‡è·¯ç”±å™¨ç½‘ç»œå®ç°è·¨ä¸“å®¶çŸ¥è¯†çš„åˆ©ç”¨ã€‚</li>
<li>SIMoEåœ¨æŒ‡ä»¤å¾®è°ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¶…è¿‡ç°æœ‰åŸºå‡†æ°´å¹³ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†è‰¯å¥½çš„æ€§èƒ½ä¸è®¡ç®—ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00b384cfdaf3a24e009bf3462c63d309.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-627993c75b2b2d2bf369e219aa24ab9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c9249111776752826b9331322e85068.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention"><a href="#On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention" class="headerlink" title="On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention"></a>On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention</h2><p><strong>Authors:Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella</strong></p>
<p>Large language models (LLMs) excel at capturing global token dependencies via self-attention but face prohibitive compute and memory costs on lengthy inputs. While sub-quadratic methods (e.g., linear attention) can reduce these costs, they often degrade accuracy due to overemphasizing recent tokens. In this work, we first propose dual-state linear attention (DSLA), a novel design that maintains two specialized hidden states-one for preserving historical context and one for tracking recency-thereby mitigating the short-range bias typical of linear-attention architectures. To further balance efficiency and accuracy under dynamic workload conditions, we introduce DSLA-Serve, an online adaptive distillation framework that progressively replaces Transformer layers with DSLA layers at inference time, guided by a sensitivity-based layer ordering. DSLA-Serve uses a chained fine-tuning strategy to ensure that each newly converted DSLA layer remains consistent with previously replaced layers, preserving the overall quality. Extensive evaluations on commonsense reasoning, long-context QA, and text summarization demonstrate that DSLA-Serve yields 2.3x faster inference than Llama2-7B and 3.0x faster than the hybrid Zamba-7B, while retaining comparable performance across downstream tasks. Our ablation studies show that DSLAâ€™s dual states capture both global and local dependencies, addressing the historical-token underrepresentation seen in prior linear attentions. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve">https://github.com/utnslab/DSLA-Serve</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿é€šè¿‡è‡ªæ³¨æ„åŠ›æ•æ‰å…¨å±€ä»¤ç‰Œä¾èµ–å…³ç³»ï¼Œä½†åœ¨å¤„ç†é•¿è¾“å…¥æ—¶é¢ä¸´ç€è®¡ç®—é‡å¤§å’Œå†…å­˜æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚è™½ç„¶æ¬¡äºŒæ¬¡æ–¹æ³•ï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›ï¼‰å¯ä»¥é™ä½è¿™äº›æˆæœ¬ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå› ä¸ºè¿‡åº¦å¼ºè°ƒæœ€è¿‘çš„ä»¤ç‰Œè€Œé™ä½å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºåŒçŠ¶æ€çº¿æ€§æ³¨æ„åŠ›ï¼ˆDSLAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®¾è®¡ï¼Œèƒ½å¤Ÿç»´æŒä¸¤ä¸ªä¸“é—¨åŒ–çš„éšè—çŠ¶æ€ï¼Œä¸€ä¸ªç”¨äºä¿ç•™å†å²ä¸Šä¸‹æ–‡ï¼Œå¦ä¸€ä¸ªç”¨äºè·Ÿè¸ªæœ€æ–°ä¿¡æ¯ï¼Œä»è€Œå‡è½»çº¿æ€§æ³¨æ„åŠ›æ¶æ„æ‰€å›ºæœ‰çš„çŸ­ç¨‹åå·®ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åŠ¨æ€å·¥ä½œè´Ÿè½½æ¡ä»¶ä¸‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSLA-Serveï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¥åŸºäºæ•æ„Ÿæ€§çš„å±‚åºä¸ºæŒ‡å¯¼ï¼Œé€æ­¥æ›¿æ¢Transformerå±‚ä¸­çš„DSLAå±‚ã€‚DSLA-Serveé‡‡ç”¨é“¾å¼å¾®è°ƒç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªæ–°è½¬æ¢çš„DSLAå±‚ä¸ä¹‹å‰æ›¿æ¢çš„å±‚ä¿æŒä¸€è‡´ï¼Œä»è€Œä¿æŒæ•´ä½“è´¨é‡ã€‚åœ¨å¸¸è¯†æ¨ç†ã€é•¿æ–‡æœ¬é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDSLA-Serveç›¸è¾ƒäºLlama2-7Bæ¨¡å‹æ¨ç†é€Ÿåº¦æé«˜äº†2.3å€ï¼Œç›¸è¾ƒäºæ··åˆæ¨¡å‹Zamba-7Bæé«˜äº†3.0å€ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¿æŒäº†ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDSLAçš„åŒçŠ¶æ€æ•æ‰å…¨å±€å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œè§£å†³äº†å…ˆå‰çº¿æ€§æ³¨æ„åŠ›åœ¨å†å²ä»¤ç‰Œè¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/utnslab/DSLA-Serveè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09316v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŒæ€çº¿æ€§æ³¨æ„åŠ›ï¼ˆDSLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾“å…¥æ—¶é¢ä¸´çš„è®¡ç®—ä¸å†…å­˜æˆæœ¬é—®é¢˜ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªä¸“é—¨åŒ–çš„éšè—çŠ¶æ€ï¼Œä¸€ä¸ªç”¨äºä¿ç•™å†å²ä¸Šä¸‹æ–‡ï¼Œä¸€ä¸ªç”¨äºè·Ÿè¸ªè¿‘æœŸä¿¡æ¯ï¼Œä»¥ç¼“è§£çº¿æ€§æ³¨æ„åŠ›æ¶æ„çš„çŸ­ç¨‹åè§ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†åœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶DSLA-Serveï¼Œæ ¹æ®åŠ¨æ€å·¥ä½œè´Ÿè½½æ¡ä»¶ä¸‹çš„æ•æ„Ÿæ€§ï¼Œé€æ­¥åœ¨æ¨ç†æ—¶é—´æ›¿æ¢Transformerå±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒDSLA-Serveåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æ¯”Llama2-7Bå¿«2.3å€ï¼Œæ¯”æ··åˆZamba-7Bå¿«3.0å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSLAæ¨¡å‹é€šè¿‡å¼•å…¥ä¸¤ä¸ªéšè—çŠ¶æ€ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿è¾“å…¥æ—¶çš„è®¡ç®—ä¸å†…å­˜æŒ‘æˆ˜ã€‚</li>
<li>åŒæ€è®¾è®¡èƒ½å¤ŸåŒæ—¶æ•æ‰å…¨å±€å’Œå±€éƒ¨ä¾èµ–ï¼Œæ”¹å–„å…ˆå‰çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„å†å²ä»¤ç‰Œæ¬ è¡¨å¾é—®é¢˜ã€‚</li>
<li>DSLA-Serveæ¡†æ¶èƒ½åœ¨æ¨ç†æ—¶é—´é€æ­¥æ›¿æ¢Transformerå±‚ï¼Œé€šè¿‡åœ¨çº¿è‡ªé€‚åº”è’¸é¦æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒDSLA-Serveåœ¨å¸¸è¯†æ¨ç†ã€é•¿æ–‡æœ¬é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>DSLA-Serveåœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¹¿æ³›çš„è¯„ä¼°è¯æ˜äº†DSLAå’ŒDSLA-Serveçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8ee9906c06c2fa0c086a22873f03473e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60c67094e0bcd3f621e31431b29f3ded.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90de5cd9ba73d7e8bc828d401a22efc8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning"><a href="#Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning" class="headerlink" title="Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning"></a>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning</h2><p><strong>Authors:Haozhen Zhang, Tao Feng, Jiaxuan You</strong></p>
<p>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave â€œthinkâ€ actions (internal deliberation) with â€œrouteâ€ actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management. </p>
<blockquote>
<p>å¤šæ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ¶Œç°ä¿ƒè¿›äº†LLMè·¯ç”±å™¨çš„å‘å±•ï¼Œè¿™äº›è·¯ç”±å™¨å°†ç”¨æˆ·æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMè·¯ç”±å™¨é€šå¸¸æ‰§è¡Œå•ä¸€è½®æ¬¡ã€ä¸€å¯¹ä¸€çš„æ˜ å°„ï¼ˆå³ï¼Œå°†æ¯ä¸ªæŸ¥è¯¢å­¤ç«‹åœ°åˆ†é…ç»™ä¸€ä¸ªæ¨¡å‹ï¼‰ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™äº›å¤æ‚ä»»åŠ¡éœ€è¦å¤šä¸ªLLMçš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶Router-R1ï¼Œå°†å¤šLLMè·¯ç”±å’Œèšåˆåˆ¶å®šä¸ºé¡ºåºå†³ç­–è¿‡ç¨‹ã€‚Router-R1å°†è·¯ç”±å™¨æœ¬èº«å®ä¾‹åŒ–ä¸ºä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„LLMï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ä¸â€œæ€è€ƒâ€è¡ŒåŠ¨ï¼ˆå†…éƒ¨æƒè¡¡ï¼‰å’Œâ€œè·¯ç”±â€è¡ŒåŠ¨ï¼ˆåŠ¨æ€æ¨¡å‹è°ƒç”¨ï¼‰äº¤ç»‡ï¼Œå¹¶å°†æ¯ä¸ªå“åº”é›†æˆåˆ°å…¶ä¸æ–­å‘å±•çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†ä¿ƒè¿›å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºè§„åˆ™çš„è½»é‡çº§å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±å’Œä¸€ç§ç”¨äºä¼˜åŒ–æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´å¹³è¡¡çš„æ–°æˆæœ¬å¥–åŠ±ï¼Œå¼€è¾Ÿäº†é€šè¿‡RLæé«˜æ€§èƒ½ä¸æˆæœ¬æƒè¡¡çš„é€”å¾„ã€‚Router-R1ä»…ä¾èµ–äºç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼Œå¦‚å®šä»·ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ¨¡å‹é€‰æ‹©ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRouter-R1ä¼˜äºå‡ ä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¿æŒç¨³å¥çš„æ¨å¹¿å’Œæˆæœ¬ç®¡ç†çš„åŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09033v2">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a>. Models   and Datasets are available at   <a target="_blank" rel="noopener" href="https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03">https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è·¯ç”±è®¾è®¡ä¸ä¼˜åŒ–ã€‚ç°æœ‰LLMè·¯ç”±å™¨å­˜åœ¨å•ä¸€æ˜ å°„çš„å±€é™æ€§ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šä¸ªLLMçš„äº’è¡¥ä¼˜åŠ¿å¤„ç†å¤æ‚ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMè·¯ç”±å™¨æ¡†æ¶Router-R1ï¼Œè¯¥æ¡†æ¶å¯å°†å¤šLLMè·¯ç”±å’Œèšåˆè¡¨è¿°ä¸ºåºè´¯å†³ç­–è¿‡ç¨‹ã€‚Router-R1é€šè¿‡å†…éƒ¨æ¨ç†ä¸åŠ¨æ€æ¨¡å‹è°ƒç”¨çš„äº¤æ›¿æ‰§è¡Œï¼Œå®ç°äº†å¯¹å¤šä¸ªLLMçš„é«˜æ•ˆè·¯ç”±å’Œèšåˆã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„è§„åˆ™å¥–åŠ±æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRouter-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæˆæœ¬ç®¡ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMè·¯ç”±å™¨çš„å¿«é€Ÿå‡ºç°è§£å†³äº†åˆ†é…ç”¨æˆ·æŸ¥è¯¢åˆ°æœ€åˆé€‚çš„æ¨¡å‹çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰LLMè·¯ç”±å™¨é€šå¸¸é‡‡ç”¨å•ä¸€æ˜ å°„æ–¹å¼ï¼Œé™åˆ¶äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>Router-R1åŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶è®¾è®¡ï¼Œå®ç°äº†å¤šLLMçš„è·¯ç”±å’Œèšåˆçš„åºè´¯å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>Router-R1é€šè¿‡å†…éƒ¨æ¨ç†ä¸åŠ¨æ€æ¨¡å‹è°ƒç”¨çš„äº¤æ›¿æ‰§è¡Œè¿›è¡Œå†³ç­–ã€‚</li>
<li>Router-R1é‡‡ç”¨è½»é‡çº§è§„åˆ™å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–æ€§èƒ½å’Œæˆæœ¬çš„å¹³è¡¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºRouter-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæˆæœ¬ç®¡ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e19a4192b4e8fd2cfc5e4866033a41c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bafe07447f0b6bfb6ba0025348a119c8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation"><a href="#Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation" class="headerlink" title="Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation"></a>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation</h2><p><strong>Authors:Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean Oâ€™Brien, Vasu Sharma</strong></p>
<p>We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original modelâ€™s strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIPâ€™s original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIPâ€™s zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md">https://anonymous.4open.science/r/DCLIP-B772/README.md</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Distill CLIPï¼ˆDCLIPï¼‰ï¼Œå®ƒæ˜¯CLIPæ¨¡å‹çš„ä¸€ç§ç²¾ç»†è°ƒæ•´å˜ä½“ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çš„å¼ºå¤§é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚CLIPæ¨¡å‹é€šå¸¸å—åˆ°å›ºå®šå›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™ä¸Šä¸‹æ–‡çš„é™åˆ¶ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶åœ¨éœ€è¦ç²¾ç»†è·¨æ¨¡æ€ç†è§£çš„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆå­¦ç”Ÿè’¸é¦æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå…¶ä¸­è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆç»è¿‡å¾®è°ƒä»¥é€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡çš„æ··åˆæŸå¤±æ¥æŒ‡å¯¼è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚å°½ç®¡ä»…ä½¿ç”¨ä»MSCOCOã€Flickr30kå’ŒConceptual Captionsç²¾é€‰çš„çº¦67,500ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»…å CLIPåŸå§‹æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPçš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„çº¦94%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ä¸“ä¸šåŒ–ä¸é€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/DCLIP-B772/README.mdæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21549v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹çš„ä¸€ä¸ªç²¾ç»†è°ƒæ•´å˜ä½“Distill CLIPï¼ˆDCLIPï¼‰è¢«æå‡ºï¼Œå®ƒåœ¨ä¿æŒåŸå§‹æ¨¡å‹å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›çš„åŒæ—¶ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢èƒ½åŠ›ã€‚CLIPæ¨¡å‹é€šå¸¸å—é™äºå›ºå®šçš„å›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™çš„ä¸Šä¸‹æ–‡ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶åœ¨éœ€è¦ç²¾ç»†ç²’åº¦è·¨æ¨¡æ€ç†è§£çš„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆå­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå…¶ä¸­è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆç»è¿‡å¾®è°ƒï¼Œé€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡æ··åˆæŸå¤±å¼•å¯¼è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒï¼Œè¯¥æ··åˆæŸå¤±ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡ã€‚å°½ç®¡ä»…åœ¨MSCOCOã€Flickr30kå’ŒConceptual Captionsç­‰æ•°æ®é›†ä¸­ç²¾é€‰çš„çº¦67ï¼Œ500ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ˜¯CLIPåŸå§‹æ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†ï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPå¤§çº¦94%çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ä¸“ä¸šåŒ–ä¸é€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Distill CLIPï¼ˆDCLIPï¼‰æ˜¯CLIPæ¨¡å‹çš„ä¸€ç§æ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ã€‚</li>
<li>CLIPæ¨¡å‹é¢ä¸´å›ºå®šå›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™ä¸Šä¸‹æ–‡çš„é™åˆ¶ï¼Œå¯èƒ½å½±å“å…¶åœ¨è·¨æ¨¡æ€ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>DCLIPé€šè¿‡å…ƒæ•™å¸ˆå­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³è¿™äº›é—®é¢˜ï¼Œåˆ©ç”¨è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆå¾®è°ƒäº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚</li>
<li>DCLIPé€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡ï¼Œä½¿ç”¨æ··åˆæŸå¤±æ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>DCLIPåœ¨ä»…ä½¿ç”¨å°éƒ¨åˆ†æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æ€§èƒ½ï¼Œå¹¶ä¿ç•™äº†å¤§éƒ¨åˆ†CLIPçš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>DCLIPè§£å†³äº†ä»»åŠ¡ç‰¹æ®ŠåŒ–å’Œé€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e80a5d32c25c7430a1ae8a67f49d58f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64fc51ed96f60f5dfbe42773329d3a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e680fe1ed2b3c97ea2db253829e51a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0d957fd4470925c627a4aaf95ccd620.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Policy-Optimization"><a href="#J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Policy-Optimization" class="headerlink" title="J4R: Learning to Judge with Equivalent Initial State Group Relative   Policy Optimization"></a>J4R: Learning to Judge with Equivalent Initial State Group Relative   Policy Optimization</h2><p><strong>Authors:Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</strong></p>
<p>To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•æ­¥ä¼çš„åŠ å¿«ï¼Œæ¨¡å‹è¾“å‡ºè¯„ä¼°å·²ä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ï¼Œå…¶ä¸­LLMè‡ªèº«è¢«ç”¨äºè¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹çš„è¾“å‡ºã€‚LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆè¯„ä¼°å™¨ï¼Œæ“…é•¿è¯„ä¼°ç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼Œå¦‚èŠå¤©è´¨é‡ï¼Œä½†åœ¨éœ€è¦æ¨ç†çš„å¤æ‚é¢†åŸŸä¸­ï¼Œæ¨¡å‹å›åº”åŒ…å«æ›´å¤šå®è´¨æ€§å’Œæœ‰æŒ‘æˆ˜æ€§çš„å†…å®¹æ—¶ä¼šæ„Ÿåˆ°å›°éš¾ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰è¯„å§”çš„ç¼ºé™·ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒè¯„å§”ã€‚æˆ‘ä»¬åšå‡ºäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒè¯„å§”ï¼Œä½¿å…¶å¯¹æ›´å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­å‡ºç°çš„å®šä½åå·®å…·æœ‰é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ReasoningJudgeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ä»¥åœ¨å¤šæ ·åŒ–æ¨ç†ç¯å¢ƒä¸­è¯„ä¼°è¯„å§”çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä»¥å‰çš„å·¥ä½œæœªæ¶‰åŠçš„å†…å®¹ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬ä½¿ç”¨EIS-GRPOè®­ç»ƒäº†Reasoning Judgeï¼ˆJ4Rï¼‰ï¼Œä¸€ä¸ª7Bçš„è¯„å§”ï¼Œå…¶æ€§èƒ½ä¼˜äºGPT-4oå’Œä¸‹ä¸€ä¸ªæœ€å¥½çš„å°å‹è¯„å§”6.7%å’Œ9%ï¼Œåœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ä¸æ›´å¤§çš„GRPOè®­ç»ƒè¯„å§”ç›¸åŒ¹é…æˆ–æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13346v3">PDF</a> 25 pages, 4 figures, 6 tables. Updated with code and benchmark</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ä¿ƒè¿›äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°ä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°çš„è½¬å˜ï¼Œç°åœ¨LLMè‡ªèº«è¢«ç”¨æ¥è¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹è¾“å‡ºã€‚ç ”ç©¶æå‡ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒLLMä½œä¸ºè¯„å§”çš„ç­–ç•¥ï¼Œå¹¶åšå‡ºäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šæå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œä½¿è¯„å§”åœ¨å¤æ‚çš„è¯„ä¼°ç¯å¢ƒä¸­æ›´åŠ ç¨³å¥ï¼›å¼•å…¥äº†ReasoningJudgeBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°åœ¨å¤šæ ·åŒ–æ¨ç†ç¯å¢ƒä¸‹çš„è¯„å§”æ€§èƒ½ï¼›è®­ç»ƒäº†ä½¿ç”¨EIS-GRPOçš„Judge for Reasoningï¼ˆJ4Rï¼‰æ¨¡å‹ï¼Œåœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†GPT-4oå’Œå…¶ä»–å°å‹è¯„å§”ï¼ŒåŒ¹é…ç”šè‡³è¶…è¶Šäº†å¤§å‹GRPOè®­ç»ƒè¯„å§”åœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å‘å±•ä¿ƒè¿›äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°çš„è½¬å˜ï¼Œä»äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ï¼Œä½¿ç”¨LLMè‡ªèº«è¿›è¡Œå…¶ä»–æ¨¡å‹çš„è¾“å‡ºè¯„ä¼°ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«ç”¨äºè®­ç»ƒLLMä½œä¸ºè¯„å§”ï¼Œä»¥æ”¹å–„ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œå¢å¼ºè¯„å§”åœ¨å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥äº†ReasoningJudgeBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°åœ¨å¤šæ ·åŒ–æ¨ç†ç¯å¢ƒä¸‹çš„è¯„å§”æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒäº†Judge for Reasoningï¼ˆJ4Rï¼‰æ¨¡å‹ï¼Œä½¿ç”¨EIS-GRPOç®—æ³•ï¼Œæ€§èƒ½ä¼˜äºGPT-4oå’Œå…¶ä»–å°å‹è¯„å§”ã€‚</li>
<li>J4Ræ¨¡å‹åœ¨æ€§èƒ½ä¸ŠåŒ¹é…ç”šè‡³è¶…è¶Šäº†å¤§å‹GRPOè®­ç»ƒè¯„å§”åœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ã€‚</li>
<li>è‡ªåŠ¨è¯„ä»·æ˜¯LLMé¢†åŸŸçš„é‡è¦å‘å±•æ–¹å‘ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ¨¡å‹è¾“å‡ºçš„è¯„ä»·æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2381be16a29f1d7bbbdac73f6e758eca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd2e79f44555ed40cdff875cd770a026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb972d15aa503412e3b0a4847f7f9899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8b703b210ce54431ca9c70043f728f3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Fractured-Chain-of-Thought-Reasoning"><a href="#Fractured-Chain-of-Thought-Reasoning" class="headerlink" title="Fractured Chain-of-Thought Reasoning"></a>Fractured Chain-of-Thought Reasoning</h2><p><strong>Authors:Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong</strong></p>
<p>Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/BaohaoLiao/frac-cot">https://github.com/BaohaoLiao/frac-cot</a>. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯é€šè¿‡åˆ©ç”¨æ¨ç†æ—¶çš„é¢å¤–è®¡ç®—åŠªåŠ›ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ ·åœ°ï¼Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåŠå…¶æ‰©å±•ç‰ˆæœ¬é•¿æ€ç»´é“¾ï¼ˆLong CoTï¼‰é€šè¿‡ç”Ÿæˆä¸°å¯Œçš„ä¸­é—´æ¨ç†è½¨è¿¹æé«˜äº†å‡†ç¡®æ€§ã€‚ä½†è¿™äº›æ–¹æ³•äº§ç”Ÿäº†å·¨å¤§çš„ä»¤ç‰Œæˆæœ¬ï¼Œé˜»ç¢äº†å…¶åœ¨å»¶è¿Ÿæ•æ„Ÿç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†æˆªæ–­æ€ç»´é“¾ï¼ˆTruncated CoTï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†å®Œæˆä¹‹å‰åœæ­¢æ¨ç†å¹¶ç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå¾€å¾€èƒ½ä¸å®Œæ•´çš„CoTé‡‡æ ·ç›¸åŒ¹æ•Œï¼ŒåŒæ—¶ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡å¤§å¤§å‡å°‘ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–­è£‚é‡‡æ ·ï¼ˆFractured Samplingï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„æ¨ç†æ—¶é—´ç­–ç•¥ï¼Œæ²¿ç€ä¸‰ä¸ªæ­£äº¤è½´åœ¨å®Œæ•´çš„CoTå’Œä»…è§£å†³æ–¹æ¡ˆé‡‡æ ·ä¹‹é—´è¿›è¡Œæ’å€¼ï¼šï¼ˆ1ï¼‰æ¨ç†è½¨è¿¹çš„æ•°é‡ï¼Œï¼ˆ2ï¼‰æ¯æ¡è½¨è¿¹çš„æœ€ç»ˆè§£å†³æ–¹æ¡ˆæ•°é‡ï¼Œï¼ˆ3ï¼‰æ¨ç†è½¨è¿¹è¢«æˆªæ–­çš„æ·±åº¦ã€‚é€šè¿‡äº”ä¸ªä¸åŒçš„æ¨ç†åŸºå‡†æµ‹è¯•å’Œå¤šç§æ¨¡å‹è§„æ¨¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ–­è£‚é‡‡æ ·åœ¨å‡†ç¡®æ€§æˆæœ¬æƒè¡¡æ–¹é¢å§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼Œåœ¨Pass@kä¸ä»¤ç‰Œé¢„ç®—æ–¹é¢å®ç°äº†é™¡å³­çš„å¯¹æ•°çº¿æ€§ç¼©æ”¾æ”¶ç›Šã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å¦‚ä½•åœ¨è¿™äº›ç»´åº¦ä¸Šåˆ†é…è®¡ç®—ä»¥æœ€å¤§åŒ–æ€§èƒ½ï¼Œä¸ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„LLMæ¨ç†é“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BaohaoLiao/frac-cot%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BaohaoLiao/frac-cotæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12992v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ä½¿ç”¨çš„æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—åŠªåŠ›æ¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æ–‡ç« è¿˜æ¢è®¨äº†Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºåŠå…¶æ‰©å±•Long CoTåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œä¹Ÿå¸¦æ¥äº†å¤§é‡çš„ä»¤ç‰Œæˆæœ¬ï¼Œé˜»ç¢äº†å…¶åœ¨å»¶è¿Ÿæ•æ„Ÿåœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œé€šè¿‡æˆªæ–­CoTï¼Œåœ¨æ¨ç†å®Œæˆå‰åœæ­¢å¹¶ç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå¯ä»¥åŒ¹é…å®Œæ•´çš„CoTé‡‡æ ·ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘ä»¤ç‰Œä½¿ç”¨é‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡å¼•å…¥äº†Fractured Samplingè¿™ä¸€ç»Ÿä¸€çš„æ¨ç†æ—¶é—´ç­–ç•¥ï¼Œé€šè¿‡åœ¨ä¸‰ä¸ªæ­£äº¤è½´ä¸Šæ’å€¼å®Œæ•´çš„CoTå’Œè§£å†³æ–¹æ¡ˆé‡‡æ ·ï¼Œå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§æˆæœ¬æƒè¡¡ã€‚å®éªŒè¯æ˜ï¼ŒFractured Samplingåœ¨äº”ä¸ªä¸åŒçš„æ¨ç†åŸºå‡†æµ‹è¯•å’Œå¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šå‡å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯æ— éœ€é‡æ–°è®­ç»ƒå³å¯æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå’ŒLong CoTèƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†å¸¦æ¥è¾ƒé«˜çš„ä»¤ç‰Œæˆæœ¬ã€‚</li>
<li>æˆªæ–­CoTèƒ½åœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘ä»¤ç‰Œä½¿ç”¨ã€‚</li>
<li>Fractured Samplingæ˜¯ä¸€ç§ç»Ÿä¸€çš„æ¨ç†æ—¶é—´ç­–ç•¥ï¼Œé€šè¿‡è°ƒæ•´ä¸‰ä¸ªæ­£äº¤è½´ä¸Šçš„å‚æ•°å®ç°æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>Fractured Samplingåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è°ƒæ•´è®¡ç®—åˆ†é…ï¼Œå¯ä»¥å®ç°æœ€ä½³çš„æ€§èƒ½-æˆæœ¬æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97e0d4b25f023fc8ec980bc7954942dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21beba6ce8e3fe2450148bd5a2d289bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-194a1e84adf5383eb30a290340c3ff29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71402f61fb1611b8e9f7a21d284ebae6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯ä¸­ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿï¼Œéœ€è¦å¤§é‡çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†éå¸¸å›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ¨¡å‹â€”â€”Ophoraï¼Œå®ƒå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºäº†æ„å»ºOphoraï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™äº‹çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16ä¸‡å¯¹è§†é¢‘æŒ‡ä»¤å¯¹ï¼Œå³Ophora-160Kæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»¥ä»T2Væ¨¡å‹é¢„è®­ç»ƒçš„è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸­è½¬ç§»ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ï¼ŒåŸºäºOphora-160Kæ•°æ®é›†è¿›è¡Œéšç§ä¿æŠ¤çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å¯¹è§†é¢‘è´¨é‡çš„å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Ophoraåœ¨æ‰§è¡Œçœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mar-cry/Ophoraè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v5">PDF</a> Early accepted in MICCAI25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOphoraçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºè§£å†³é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ•°æ®éš¾ä»¥æ”¶é›†çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å¤§å‹æ•°æ®é›†Ophora-160Kï¼Œå¹¶æå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®æ•´ç†æµç¨‹å’Œæ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒOphoraèƒ½å¤Ÿç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œå¹¶èµ‹èƒ½çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.Ophoraæ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œè§£å†³äº†é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ•°æ®éš¾ä»¥æ”¶é›†çš„é—®é¢˜ã€‚</p>
<p>2.ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å¤§å‹æ•°æ®é›†Ophora-160Kï¼Œç”¨äºè®­ç»ƒå’Œå‘å±•Ophoraæ¨¡å‹ã€‚</p>
<p>3.æå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œå°†å™äº‹æ€§çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºé«˜è´¨é‡æ•°æ®é›†ã€‚</p>
<p>4.æ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆç”¨äºä»é¢„è®­ç»ƒçš„T2Væ¨¡å‹ä¸­è½¬ç§»æ—¶ç©ºçŸ¥è¯†ï¼Œå®ç°åŸºäºéšç§ä¿æŠ¤çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚</p>
<p>5.å®éªŒè¯æ˜ï¼ŒOphoraèƒ½å¤Ÿç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚</p>
<p>6.Ophoraæ¨¡å‹åœ¨çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºèµ‹èƒ½ä½œç”¨ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-01d68ed0db47728debc97d7e73ca13b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ca6fc71c7ae3f197d22d5d6cbcb4ad4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c58459f75ee68db9190fdd9a9cd2ec5.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cebd040a0fd5cefd5a58fe9d9506828a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Embodied Web Agents Bridging Physical-Digital Realms for Integrated   Agent Intelligence
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8a197ccf26b0d6a33db627ad0545d04b.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  FrontendBench A Benchmark for Evaluating LLMs on Front-End Development   via Automatic Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
