<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  FrontendBench A Benchmark for Evaluating LLMs on Front-End Development   via Automatic Evaluation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8a197ccf26b0d6a33db627ad0545d04b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="FrontendBench-A-Benchmark-for-Evaluating-LLMs-on-Front-End-Development-via-Automatic-Evaluation"><a href="#FrontendBench-A-Benchmark-for-Evaluating-LLMs-on-Front-End-Development-via-Automatic-Evaluation" class="headerlink" title="FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development   via Automatic Evaluation"></a>FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development   via Automatic Evaluation</h2><p><strong>Authors:Hongda Zhu, Yiwen Zhang, Bing Zhao, Jingzhe Ding, Siyao Liu, Tong Liu, Dandan Wang, Yanan Liu, Zhaojian Li</strong></p>
<p>Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å‰ç«¯ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨è¯¸å¤šå…³é”®å±€é™æ€§ï¼šè®¸å¤šä»»åŠ¡è¿‡äºç®€å•ï¼Œæµ‹è¯•ç”¨ä¾‹å¾€å¾€ç¼ºä¹ä¸¥è°¨æ€§ï¼Œä¸”ç¼ºä¹ç«¯åˆ°ç«¯çš„éªŒè¯ã€‚è¿™äº›é—®é¢˜é˜»ç¢äº†æ¨¡å‹æ€§èƒ½çš„å‡†ç¡®è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FrontendBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±äººç±»å’ŒLLMå…±åŒå¼€å‘çš„åŸºå‡†æµ‹è¯•ã€‚FrontendBenchæ ¹æ®ä»£ç åŠŸèƒ½åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶èå…¥äº¤äº’å¼æµ‹è¯•åœºæ™¯ï¼Œèƒ½å¤Ÿæ›´å…¨é¢ã€æ›´å®ç”¨åœ°è¯„ä¼°å‰ç«¯ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«148ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤º-æµ‹è¯•ç”¨ä¾‹å¯¹ï¼Œæ¶µç›–äº”ä¸ªçº§åˆ«çš„ç½‘é¡µç»„ä»¶ï¼Œä»åŸºæœ¬çš„UIå…ƒç´ åˆ°å¤æ‚çš„äº¤äº’åŠŸèƒ½ã€‚æ¯ä¸ªä»»åŠ¡éƒ½åæ˜ äº†ç°å®å‰ç«¯å¼€å‘çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ²™ç®±ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆçš„ä»£ç ï¼Œå¹¶ä½¿ç”¨é¢„è®¾çš„æµ‹è¯•è„šæœ¬æ¥è¯„ä¼°ç»“æœã€‚è¯¥æ¡†æ¶ä¸ä¸“å®¶äººå·¥è¯„ä¼°çš„å¥‘åˆç‡è¾¾åˆ°9.54%ï¼Œæ˜¾ç¤ºå‡ºé«˜åº¦çš„å¯é æ€§ã€‚æˆ‘ä»¬åœ¨FrontendBenchä¸Šè¯„ä¼°äº†å‡ æ¬¾å‰æ²¿çš„LLMï¼Œå¹¶è§‚å¯Ÿåˆ°åœ¨å¤„ç†ç°å®ä¸–ç•Œå‰ç«¯ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†FrontendBenchä½œä¸ºä¸€ä¸ªå¯é ä¸”å¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ï¼Œæ”¯æŒå¤šæ¨¡å¼ä¸€è‡´æ€§çš„è¯„ä¼°ï¼Œå¹¶ä¸ºæœªæ¥å‰ç«¯ä»£ç ç”Ÿæˆç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13832v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‰ç«¯ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°å·¥å…·å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FrontendBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹å…±åŒå¼€å‘çš„è¯„ä¼°å·¥å…·ã€‚FrontendBenchåŸºäºä»£ç åŠŸèƒ½åˆ†ç±»ä»»åŠ¡ï¼Œèå…¥äº’åŠ¨æµ‹è¯•åœºæ™¯ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°å‰ç«¯ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œåœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆä»£ç å¹¶æŒ‰é¢„è®¾æµ‹è¯•è„šæœ¬è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬åœ¨FrontendBenchä¸Šè¯„ä¼°äº†å¤šæ¬¾æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨å¤„ç†çœŸå®å‰ç«¯ä»»åŠ¡æ—¶çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚è¿™è¡¨æ˜FrontendBenchæ˜¯ä¸€ä¸ªå¯é ä¸”å¯æ‰©å±•çš„è¯„ä¼°å·¥å…·ï¼Œä¸ºæœªæ¥å‰ç«¯ä»£ç ç”Ÿæˆç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‰ç«¯ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰è¯„ä¼°å·¥å…·å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä»»åŠ¡è¿‡äºç®€å•ã€æµ‹è¯•æ¡ˆä¾‹ç¼ºä¹ä¸¥è°¨æ€§ã€ç¼ºä¹ç«¯åˆ°ç«¯éªŒè¯ã€‚</li>
<li>FrontendBenchæ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°å·¥å…·ï¼Œç”±äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹å…±åŒå¼€å‘ã€‚</li>
<li>FrontendBenchåŸºäºä»£ç åŠŸèƒ½åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶èå…¥äº’åŠ¨æµ‹è¯•åœºæ™¯ï¼Œæä¾›æ›´å…¨é¢çš„è¯„ä¼°ã€‚</li>
<li>å»ºç«‹äº†è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œåœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆä»£ç å¹¶æŒ‰é¢„è®¾æµ‹è¯•è„šæœ¬è¿›è¡Œè¯„ä¼°ï¼Œä¸“å®¶è¯„ä»·ç‡é«˜è¾¾90.54%ã€‚</li>
<li>åœ¨FrontendBenchä¸Šè¯„ä¼°çš„å¤šæ¬¾æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†çœŸå®å‰ç«¯ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f07e30ae21524526bc2025b9c146ad5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3dc7ec208e261112f102c9560fa84e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c652fbeca3476a75fd115acf8d4e8e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b27d702d52e1cd591314079063ffd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8731ac84dc9e3056db2a2a6452dbc24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61d06185589acb0bfdc411448c8d8f88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1309e98689a1affe14692eeeb056afbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e4737e86f011e08a0400f1d1474ff13.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Cognitive-Reasoners-Modular-Reasoning-with-Brain-Like-Specialization"><a href="#Mixture-of-Cognitive-Reasoners-Modular-Reasoning-with-Brain-Like-Specialization" class="headerlink" title="Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like   Specialization"></a>Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like   Specialization</h2><p><strong>Authors:Badr AlKhamissi, C. NicolÃ² De Sabbata, Zeming Chen, Martin Schrimpf, Antoine Bosselut</strong></p>
<p>Human intelligence emerges from the interaction of specialized brain networks, each dedicated to distinct cognitive functions such as language processing, logical reasoning, social understanding, and memory retrieval. Inspired by this biological observation, we introduce the Mixture of Cognitive Reasoners (MiCRo) architecture and training paradigm: a modular transformer-based language model with a training curriculum that encourages the emergence of functional specialization among different modules. Inspired by studies in neuroscience, we partition the layers of a pretrained transformer model into four expert modules, each corresponding to a well-studied cognitive brain network. Our Brain-Like model has three key benefits over the state of the art: First, the specialized experts are highly interpretable and functionally critical, where removing a module significantly impairs performance on domain-relevant benchmarks. Second, our model outperforms comparable baselines that lack specialization on seven reasoning benchmarks. And third, the modelâ€™s behavior can be steered at inference time by selectively emphasizing certain expert modules (e.g., favoring social over logical reasoning), enabling fine-grained control over the style of its response. Our findings suggest that biologically inspired inductive biases involved in human cognition lead to significant modeling gains in interpretability, performance, and controllability. </p>
<blockquote>
<p>äººç±»æ™ºæ…§æºäºå„ç§ä¸“ä¸šè„‘ç½‘ç»œçš„äº¤äº’ä½œç”¨ï¼Œæ¯ä¸ªç½‘ç»œéƒ½ä¸“æ³¨äºä¸åŒçš„è®¤çŸ¥åŠŸèƒ½ï¼Œå¦‚è¯­è¨€å¤„ç†ã€é€»è¾‘æ¨ç†ã€ç¤¾ä¼šç†è§£å’Œè®°å¿†æ£€ç´¢ã€‚å—è¿™ä¸€ç”Ÿç‰©å­¦è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†è®¤çŸ¥æ¨ç†å™¨æ··åˆç‰©ï¼ˆMiCRoï¼‰æ¶æ„å’Œè®­ç»ƒèŒƒå¼ï¼šä¸€ç§åŸºäºæ¨¡å—åŒ–è½¬æ¢å™¨æ¶æ„çš„è¯­è¨€æ¨¡å‹ï¼Œå…¶è®­ç»ƒè¯¾ç¨‹é¼“åŠ±ä¸åŒæ¨¡å—ä¹‹é—´åŠŸèƒ½ä¸“ä¸šåŒ–çš„å‡ºç°ã€‚å—ç¥ç»ç§‘å­¦ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒè½¬æ¢å™¨æ¨¡å‹çš„å±‚åˆ’åˆ†ä¸ºå››ä¸ªä¸“å®¶æ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—å¯¹åº”äºä¸€ä¸ªç»è¿‡æ·±å…¥ç ”ç©¶çš„è®¤çŸ¥è„‘ç½‘ç»œã€‚æˆ‘ä»¬çš„è„‘çŠ¶æ¨¡å‹ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯å…·æœ‰ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼šé¦–å…ˆï¼Œä¸“ä¸šä¸“å®¶é«˜åº¦å¯è§£é‡Šå¹¶ä¸”åœ¨åŠŸèƒ½ä¸Šè‡³å…³é‡è¦ï¼Œç§»é™¤ä¸€ä¸ªæ¨¡å—ä¼šæ˜¾è‘—æŸå®³å…¶åœ¨ç›¸å…³é¢†åŸŸçš„åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç¼ºä¹ä¸“ä¸šæ€§çš„åŒç±»åŸºå‡†æµ‹è¯•ã€‚ç¬¬ä¸‰ï¼Œåœ¨æ¨ç†æ—¶é—´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æœ‰é€‰æ‹©åœ°å¼ºè°ƒæŸäº›ä¸“å®¶æ¨¡å—ï¼ˆä¾‹å¦‚ï¼Œåå¥½ç¤¾ä¼šè€Œéé€»è¾‘æ¨ç†ï¼‰æ¥å¼•å¯¼æ¨¡å‹çš„è¡Œä¸ºï¼Œå®ç°å¯¹å“åº”é£æ ¼çš„ç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¶‰åŠäººç±»è®¤çŸ¥çš„ç”Ÿç‰©å¯å‘å½’çº³åè§åœ¨è§£é‡Šæ€§ã€æ€§èƒ½å’Œå¯æ§æ€§æ–¹é¢å¸¦æ¥äº†æ˜¾è‘—çš„å»ºæ¨¡æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13331v1">PDF</a> Preprint. Code, data, and models available at   $\href{<a target="_blank" rel="noopener" href="https://bkhmsi.github.io/mixture-of-cog-reasoners%7D%7B/text%7Bthis">https://bkhmsi.github.io/mixture-of-cog-reasoners}{\text{this</a> https   URL.}}$</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å—äººç±»å¤§è„‘ç¥ç»ç§‘å­¦çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ··åˆè®¤çŸ¥æ¨ç†å™¨ï¼ˆMiCRoï¼‰æ¶æ„å’ŒåŸ¹è®­èŒƒå¼ã€‚è¯¥æ¶æ„åŸºäºæ¨¡å—åŒ–å˜å‹å™¨è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è®­ç»ƒè¯¾ç¨‹é¼“åŠ±ä¸åŒæ¨¡å—çš„åŠŸèƒ½ä¸“ä¸šåŒ–ï¼Œä»¥æ¨¡æ‹Ÿäººç±»å¤§è„‘ä¸åŒè®¤çŸ¥ç½‘ç»œçš„åŠŸèƒ½ã€‚è¯¥æ¨¡å‹å…·æœ‰é«˜åº¦çš„å¯è§£é‡Šæ€§å’ŒåŠŸèƒ½æ€§ï¼Œè¶…è¿‡ç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œèƒ½å¤Ÿåœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†æ—¶é€šè¿‡å¼ºè°ƒç‰¹å®šä¸“å®¶æ¨¡å—å®ç°ç²¾ç»†æ§åˆ¶å“åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»æ™ºèƒ½æºäºä¸“é—¨åŒ–å¤§è„‘ç½‘ç»œçš„äº¤äº’ï¼Œæ¯ä¸ªç½‘ç»œè´Ÿè´£ä¸åŒçš„è®¤çŸ¥åŠŸèƒ½ã€‚</li>
<li>å—æ­¤å¯å‘ï¼Œæå‡ºäº†MiCRoæ¶æ„å’ŒåŸ¹è®­èŒƒå¼ï¼Œæ˜¯ä¸€ç§æ¨¡å—åŒ–å˜å‹å™¨è¯­è¨€æ¨¡å‹ã€‚</li>
<li>MiCRoæ¨¡å‹å°†é¢„è®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹å±‚åˆ’åˆ†ä¸ºå››ä¸ªä¸“å®¶æ¨¡å—ï¼Œå¯¹åº”è®¤çŸ¥å¤§è„‘ç½‘ç»œã€‚</li>
<li>å»é™¤ä»»ä½•ä¸€ä¸ªä¸“å®¶æ¨¡å—ä¼šå¯¹é¢†åŸŸç›¸å…³åŸºå‡†æµ‹è¯•çš„æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œæ˜¾ç¤ºå‡ºé«˜åº¦å¯è§£é‡Šæ€§å’ŒåŠŸèƒ½æ€§ã€‚</li>
<li>MiCRoæ¨¡å‹åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¼˜äºæ— ä¸“ä¸šåŒ–çš„åŸºå‡†æ¨¡å‹ã€‚</li>
<li>åœ¨æ¨ç†æ—¶ï¼Œå¯ä»¥é€šè¿‡å¼ºè°ƒæŸäº›ä¸“å®¶æ¨¡å—æ¥ç²¾ç»†æ§åˆ¶æ¨¡å‹çš„å“åº”é£æ ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b2568702f3f9d9ae3b5464b7c185a8da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d737512340a56e8fa48751d348083a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b83993f5709526a0e3d1c1c9ba9a6e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51acb331459e6fccb6f254068519ce03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6e24b419bab95ddf89ca52d22bcb4c5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Metis-RISE-RL-Incentivizes-and-SFT-Enhances-Multimodal-Reasoning-Model-Learning"><a href="#Metis-RISE-RL-Incentivizes-and-SFT-Enhances-Multimodal-Reasoning-Model-Learning" class="headerlink" title="Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model   Learning"></a>Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model   Learning</h2><p><strong>Authors:Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma</strong></p>
<p>Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the modelâ€™s exploratory capacity and face suboptimal convergence. In this work, we introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and \textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the modelâ€™s latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è§è¯äº†é«˜çº§æ¨ç†æ¨¡å¼çš„è“¬å‹ƒå‘å±•ï¼Œè¿™äº›æ¨¡å¼æ­£è¢«é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨ä¸è¶³ï¼šä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•å¯èƒ½ä¼šé¢ä¸´æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œæ— æ³•æ¿€æ´»å®Œå…¨ç¼ºå¤±çš„æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œè€Œä¼ ç»Ÿæµç¨‹åœ¨å¼ºåŒ–å­¦ä¹ ä¹‹å‰å…ˆè¿›è¡Œä¸€æ¬¡å†·å¯åŠ¨çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå¯èƒ½ä¼šé™åˆ¶æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¹¶å¯¼è‡´æ¬¡ä¼˜æ”¶æ•›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­¦ä¹ çš„<strong>Metis-RISEï¼ˆRLæ¿€åŠ±å¹¶å¢å¼ºSFTï¼‰</strong>ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒMetis-RISEç‹¬ç‰¹åœ°çœç•¥äº†åˆå§‹SFTé˜¶æ®µï¼Œè€Œæ˜¯é¦–å…ˆè¿›è¡ŒRLé˜¶æ®µï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨Group Relative Policy Optimizationçš„å˜ä½“ï¼‰æ¥æ¿€åŠ±å’Œæ¿€æ´»æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚éšåï¼Œæœ‰é’ˆå¯¹æ€§çš„SFTé˜¶æ®µè§£å†³äº†RLæœŸé—´è¯†åˆ«çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰\emph{è½¨è¿¹é‡‡æ ·æ•ˆç‡ä½ä¸‹}ï¼Œé’ˆå¯¹æ¨¡å‹æ‹¥æœ‰ä½†åº”ç”¨ä¸æ­£ç¡®çš„æ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨RLæ¨¡å‹æœ¬èº«çš„è‡ªæˆ‘è’¸é¦æ¨ç†è½¨è¿¹æ¥è§£å†³æ­¤é—®é¢˜ï¼›ï¼ˆ2ï¼‰\emph{åŸºæœ¬èƒ½åŠ›ç¼ºå¤±}ï¼Œæˆ‘ä»¬é€šè¿‡æ³¨å…¥ä¸“å®¶å¢å¼ºçš„çŸ¥è¯†æ¥è§£å†³æ¨¡å‹å®Œå…¨å¤±è´¥çš„æƒ…å†µã€‚è¿™æ˜¯RLæ¿€åŠ±åè·ŸéšSFTå¢å¼ºçš„æˆ˜ç•¥åº”ç”¨ï¼Œå½¢æˆäº†Metis-RISEçš„æ ¸å¿ƒï¼Œå¯¼è‡´æˆ‘ä»¬å¼€å‘å‡ºä¸¤ä¸ªç‰ˆæœ¬çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆå‚æ•°åˆ†åˆ«ä¸º7Bå’Œ72Bï¼‰ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ¨ç†æ’è¡Œæ¦œä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨åŒç±»è§„æ¨¡çš„æ¨¡å‹ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶ä¸­72Bç‰ˆæœ¬æ€»ä½“æ’åç¬¬å››ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13056v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/MM-Thinking/Metis-RISE">https://github.com/MM-Thinking/Metis-RISE</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›é˜¶æ¨åŠ¨äº†å…ˆè¿›æ¨ç†æ¨¡å¼çš„å‘å±•ï¼Œå¹¶èåˆè‡³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢ä¸´æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œç¼ºå¤±æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œè€Œåˆå§‹çš„å†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå¯èƒ½é™åˆ¶æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¹¶å¯¼è‡´æ¬¡ä¼˜æ”¶æ•›ã€‚æœ¬ç ”ç©¶æå‡ºMetis-RISEæ–¹æ³•ï¼Œé€šè¿‡RLæ¿€åŠ±å¹¶æ¿€æ´»æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ï¼Œç„¶åæœ‰é’ˆå¯¹æ€§çš„SFTé˜¶æ®µè§£å†³RLä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ä»»åŠ¡ä¸­æ¨¡å‹æ­£ç¡®ä½†åº”ç”¨ä¸ä¸€è‡´çš„æ¨ç†è½¨è¿¹é‡‡æ ·æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œé€šè¿‡è‡ªæˆ‘è’¸é¦çš„æ¨ç†è½¨è¿¹è§£å†³ï¼›2ï¼‰æ ¹æœ¬èƒ½åŠ›ç¼ºå¤±é—®é¢˜ï¼Œé€šè¿‡æ³¨å…¥ä¸“å®¶å¢å¼ºçŸ¥è¯†è§£å†³ã€‚è¿™ç§ç­–ç•¥æ€§åº”ç”¨RLæ¿€åŠ±å’ŒSFTå¢å¼ºå½¢æˆMetis-RISEçš„æ ¸å¿ƒï¼Œç ”å‘å‡ºä¸¤æ¬¾MLLMï¼ˆ7Bå’Œ72Bå‚æ•°ç‰ˆæœ¬ï¼‰ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ¨ç†æ’è¡Œæ¦œä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸¤æ¬¾æ¨¡å‹åœ¨åŒç±»è§„æ¨¡æ¨¡å‹ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼Œå…¶ä¸­72Bç‰ˆæœ¬æ€»ä½“æ’åç¬¬å››ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›é˜¶æ¨åŠ¨äº†å…ˆè¿›æ¨ç†æ¨¡å¼çš„å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç»“åˆå¤šæ¨¡æ€æ¨ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨ä¸è¶³ï¼Œå¦‚æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œèƒ½åŠ›ç¼ºå¤±é—®é¢˜ã€‚</li>
<li>Metis-RISEæ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±å¹¶æ¿€æ´»æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Metis-RISEé‡‡ç”¨å…ˆRLåæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µï¼Œè§£å†³RLä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>RLé˜¶æ®µçš„æŒ‘æˆ˜åŒ…æ‹¬æ ·æœ¬é‡‡é›†æ•ˆç‡ä¸é«˜å’Œèƒ½åŠ›ç¼ºå¤±é—®é¢˜ï¼Œé€šè¿‡è‡ªæˆ‘è’¸é¦çš„æ¨ç†è½¨è¿¹å’Œä¸“å®¶å¢å¼ºçŸ¥è¯†è§£å†³ã€‚</li>
<li>åœ¨OpenCompasså¤šæ¨¡æ€æ¨ç†æ’è¡Œæ¦œä¸Šï¼ŒMetis-RISEç ”å‘çš„æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1017705887743c1b64dfa23433b506f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba74c31a8e0adbba98889d02b06436bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44eecadb7158b7584037a96dea1051f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20b08e08dd8b5259b78ba555c9332861.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SoundMind-RL-Incentivized-Logic-Reasoning-for-Audio-Language-Models"><a href="#SoundMind-RL-Incentivized-Logic-Reasoning-for-Audio-Language-Models" class="headerlink" title="SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models"></a>SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models</h2><p><strong>Authors:Xingjian Diao, Chunhui Zhang, Keyi Kong, Weiyi Wu, Chiyu Ma, Zhongyu Ouyang, Peijun Qing, Soroush Vosoughi, Jiang Gui</strong></p>
<p>While large language models have shown reasoning capabilities, their application to the audio modality, particularly in large audio-language models (ALMs), remains significantly underdeveloped. Addressing this gap requires a systematic approach, involving a capable base model, high-quality reasoning-oriented audio data, and effective training algorithms. In this study, we present a comprehensive solution: we introduce the Audio Logical Reasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples specifically designed for complex reasoning tasks. Building on this resource, we propose SoundMind, a rule-based reinforcement learning (RL) algorithm tailored to endow ALMs with deep bimodal reasoning abilities. By training Qwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves state-of-the-art performance in audio logical reasoning. This work highlights the impact of combining high-quality, reasoning-focused datasets with specialized RL techniques, advancing the frontier of auditory intelligence in language models. Our code and the proposed dataset are available at <a target="_blank" rel="noopener" href="https://github.com/xid32/SoundMind">https://github.com/xid32/SoundMind</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»å±•ç°å‡ºæ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨éŸ³é¢‘æ¨¡æ€çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰ä¸­çš„åº”ç”¨ï¼Œä»ç„¶æ˜¾è‘—ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œéœ€è¦ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªèƒ½åŠ›å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ã€é«˜è´¨é‡çš„æ¨ç†å¯¼å‘éŸ³é¢‘æ•°æ®ä»¥åŠæœ‰æ•ˆçš„è®­ç»ƒç®—æ³•ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼šæˆ‘ä»¬ä»‹ç»äº†éŸ³é¢‘é€»è¾‘æ¨ç†ï¼ˆALRï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸“é—¨ç”¨äºå¤æ‚æ¨ç†ä»»åŠ¡çš„6446ä¸ªæ–‡æœ¬-éŸ³é¢‘æ³¨é‡Šæ ·æœ¬ã€‚åŸºäºè¿™ä¸€èµ„æºï¼Œæˆ‘ä»¬æå‡ºäº†SoundMindï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ï¼Œæ—¨åœ¨èµ‹äºˆALMsæ·±åº¦åŒæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç”¨SoundMindåœ¨ALRæ•°æ®é›†ä¸Šè®­ç»ƒQwen2.5-Omni-7Bï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³é¢‘é€»è¾‘æ¨ç†æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ç»“åˆé«˜è´¨é‡ã€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ•°æ®é›†ä¸ä¸“é—¨çš„RLæŠ€æœ¯çš„å½±å“ï¼Œæ¨åŠ¨äº†è¯­è¨€æ¨¡å‹ä¸­å¬è§‰æ™ºèƒ½çš„è¾¹ç•Œã€‚æˆ‘ä»¬çš„ä»£ç å’Œæè®®çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xid32/SoundMind">https://github.com/xid32/SoundMind</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12935v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘æ¨¡æ€ä¸Šçš„æ¨ç†èƒ½åŠ›æœ‰å¾…å‘å±•ï¼Œæœ¬ç ”ç©¶ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æå‡ºäº†ä¸€ç§å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸“é—¨ç”¨äºå¤æ‚æ¨ç†ä»»åŠ¡çš„éŸ³é¢‘é€»è¾‘æ¨ç†ï¼ˆALRï¼‰æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•SoundMindï¼Œæ—¨åœ¨èµ‹äºˆå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰æ·±åº¦åŒæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹Qwen2.5-Omni-7Båœ¨ALRæ•°æ®é›†ä¸Šä½¿ç”¨SoundMindç®—æ³•ï¼Œå®ç°äº†éŸ³é¢‘é€»è¾‘æ¨ç†é¢†åŸŸçš„æœ€ä½³æ€§èƒ½ã€‚æœ¬ç ”ç©¶å‡¸æ˜¾äº†é«˜è´¨é‡æ¨ç†æ•°æ®é›†ä¸ä¸“é¡¹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„ç»“åˆå¯¹æ¨åŠ¨è¯­è¨€æ¨¡å‹å¬è§‰æ™ºèƒ½å‘å±•çš„é‡è¦æ€§ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘æ¨¡æ€çš„æ¨ç†èƒ½åŠ›å°šå¤„äºå‘å±•é˜¶æ®µã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†é’ˆå¯¹å¤æ‚æ¨ç†ä»»åŠ¡çš„éŸ³é¢‘é€»è¾‘æ¨ç†ï¼ˆALRï¼‰æ•°æ®é›†ã€‚</li>
<li>SoundMindç®—æ³•æ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åœ¨ALRæ•°æ®é›†ä¸Šè®­ç»ƒQwen2.5-Omni-7Bæ¨¡å‹ï¼Œå¹¶åº”ç”¨SoundMindç®—æ³•ï¼Œå–å¾—äº†éŸ³é¢‘é€»è¾‘æ¨ç†é¢†åŸŸçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶çš„é‡è¦æ€§åœ¨äºå‡¸æ˜¾äº†é«˜è´¨é‡æ¨ç†æ•°æ®é›†ä¸ä¸“é¡¹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ç»“åˆçš„é‡è¦æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æ¨åŠ¨äº†è¯­è¨€æ¨¡å‹åœ¨å¬è§‰æ™ºèƒ½æ–¹é¢çš„å‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-845127c666089f61561510aaaa4b1a58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e8922cb45bbb773355c0341ffc1cea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26af681a78bc33dbaac59a53d7cbe06d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea46349930062a4cdc3da35ebccfca8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fcb51e63f6b7436db98d40e2b376b24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7512b0c2cffa550badf2eccfb3df21c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CAPO-Reinforcing-Consistent-Reasoning-in-Medical-Decision-Making"><a href="#CAPO-Reinforcing-Consistent-Reasoning-in-Medical-Decision-Making" class="headerlink" title="CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making"></a>CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making</h2><p><strong>Authors:Songtao Jiang, Yuan Wang, Ruizhe Chen, Yan Zhang, Ruilin Luo, Bohan Lei, Sibo Song, Yang Feng, Jimeng Sun, Jian Wu, Zuozhu Liu</strong></p>
<p>In medical visual question answering (Med-VQA), achieving accurate responses relies on three critical steps: precise perception of medical imaging data, logical reasoning grounded in visual input and textual questions, and coherent answer derivation from the reasoning process. Recent advances in general vision-language models (VLMs) show that large-scale reinforcement learning (RL) could significantly enhance both reasoning capabilities and overall model performance. However, their application in medical domains is hindered by two fundamental challenges: 1) misalignment between perceptual understanding and reasoning stages, and 2) inconsistency between reasoning pathways and answer generation, both compounded by the scarcity of high-quality medical datasets for effective large-scale RL. In this paper, we first introduce Med-Zero-17K, a curated dataset for pure RL-based training, encompassing over 30 medical image modalities and 24 clinical tasks. Moreover, we propose a novel large-scale RL framework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), which integrates rewards to ensure fidelity between perception and reasoning, consistency in reasoning-to-answer derivation, and rule-based accuracy for final responses. Extensive experiments on both in-domain and out-of-domain scenarios demonstrate the superiority of our method over strong VLM baselines, showcasing strong generalization capability to 3D Med-VQA benchmarks and R1-like training paradigms. </p>
<blockquote>
<p>åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰ä¸­ï¼Œè·å¾—å‡†ç¡®ç­”æ¡ˆçš„å…³é”®åœ¨äºä¸‰ä¸ªæ­¥éª¤ï¼šå¯¹åŒ»ç–—æˆåƒæ•°æ®çš„ç²¾ç¡®æ„ŸçŸ¥ã€åŸºäºè§†è§‰è¾“å…¥å’Œæ–‡æœ¬é—®é¢˜çš„é€»è¾‘æ¨ç†ã€ä»¥åŠä»æ¨ç†è¿‡ç¨‹ä¸­å¾—å‡ºçš„è¿è´¯ç­”æ¡ˆã€‚é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›å’Œæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å—åˆ°ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜çš„å½±å“ï¼š1ï¼‰æ„ŸçŸ¥ç†è§£å’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„ä¸åŒ¹é…ï¼Œä»¥åŠ2ï¼‰æ¨ç†é€”å¾„å’Œç­”æ¡ˆç”Ÿæˆä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œè¿™ä¸¤ä¸ªæŒ‘æˆ˜éƒ½å› ç¼ºä¹ç”¨äºæœ‰æ•ˆå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ çš„é«˜è´¨é‡åŒ»ç–—æ•°æ®é›†è€ŒåŠ å‰§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†Med-Zero-17Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçº¯RLçš„è®­åˆ™è®­ç»ƒçš„æ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡30ç§åŒ»å­¦å›¾åƒæ¨¡æ€å’Œ24é¡¹ä¸´åºŠä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºMed-VLMçš„å¤§å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºä¸€è‡´æ€§æ„ŸçŸ¥åå¥½ä¼˜åŒ–ï¼ˆCAPOï¼‰ï¼Œå®ƒé›†æˆäº†å¥–åŠ±ä»¥ç¡®ä¿æ„ŸçŸ¥å’Œæ¨ç†ä¹‹é—´çš„å¿ å®åº¦ã€æ¨ç†åˆ°ç­”æ¡ˆæ¨å¯¼çš„ä¸€è‡´æ€§ä»¥åŠåŸºäºè§„åˆ™çš„æœ€ç»ˆå“åº”å‡†ç¡®æ€§ã€‚é’ˆå¯¹åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•ç›¸è¾ƒäºå¼ºå¤§çš„VLMåŸºå‡†çº¿çš„ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†å¯¹3D Med-VQAåŸºå‡†æµ‹è¯•å’ŒR1ç±»ä¼¼è®­ç»ƒæ¨¡å¼çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12849v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åœ¨åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰ä¸­ï¼Œå‡†ç¡®å›åº”ä¾èµ–äºä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šç²¾ç¡®æ„ŸçŸ¥åŒ»å­¦æˆåƒæ•°æ®ã€åŸºäºè§†è§‰è¾“å…¥å’Œæ–‡æœ¬é—®é¢˜çš„é€»è¾‘æ¨ç†ã€ä»¥åŠä»æ¨ç†è¿‡ç¨‹ä¸­å¾—å‡ºçš„è¿è´¯ç­”æ¡ˆã€‚æœ€è¿‘é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥æ˜¾ç¤ºï¼Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›å’Œæ¨¡å‹æ•´ä½“æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨å—åˆ°ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜çš„å½±å“ï¼š1ï¼‰æ„ŸçŸ¥ç†è§£ä¸æ¨ç†é˜¶æ®µä¹‹é—´çš„ä¸åŒ¹é…ï¼›2ï¼‰æ¨ç†é€”å¾„ä¸ç­”æ¡ˆç”Ÿæˆä¹‹é—´ä¸ä¸€è‡´ï¼Œè¿™ä¸¤è€…éƒ½å› é«˜è´¨é‡åŒ»å­¦æ•°æ®é›†ç¼ºä¹ï¼Œå¯¼è‡´æ— æ³•è¿›è¡Œå¤§è§„æ¨¡çš„æœ‰æ•ˆRLã€‚æœ¬æ–‡é¦–å…ˆä»‹ç»äº†ä¸“ä¸ºçº¯RLè®­ç»ƒè®¾è®¡çš„Med-Zero-17Kæ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡30ç§åŒ»å­¦å›¾åƒæ¨¡æ€å’Œ24é¡¹ä¸´åºŠä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤§å‹RLæ¡†æ¶â€”â€”ä¸€è‡´æ€§æ„è¯†åå¥½ä¼˜åŒ–ï¼ˆCAPOï¼‰ï¼Œç”¨äºæ•´åˆå¥–åŠ±ï¼Œä»¥ç¡®ä¿æ„ŸçŸ¥å’Œæ¨ç†ä¹‹é—´çš„å¿ å®åº¦ã€æ¨ç†åˆ°ç­”æ¡ˆçš„è¿è´¯æ€§ï¼Œä»¥åŠæœ€ç»ˆå“åº”çš„è§„åˆ™å‡†ç¡®æ€§ã€‚åœ¨åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå¼ºå¤§çš„VLMåŸºå‡†æµ‹è¯•ï¼Œå±•ç°å‡ºå¯¹3D Med-VQAåŸºå‡†æµ‹è¯•å’ŒR1ç±»ä¼¼è®­ç»ƒèŒƒå¼çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Med-VQAçš„å‡†ç¡®å“åº”ä¾èµ–äºç²¾ç¡®æ„ŸçŸ¥åŒ»å­¦æˆåƒæ•°æ®ã€é€»è¾‘æ¨ç†åŠè¿è´¯ç­”æ¡ˆçš„æ¨å¯¼ã€‚</li>
<li>é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜æ¨ç†èƒ½åŠ›å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åŒ»å­¦é¢†åŸŸåº”ç”¨RLé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ„ŸçŸ¥ä¸æ¨ç†é˜¶æ®µçš„ä¸åŒ¹é…ä»¥åŠæ¨ç†ä¸ç­”æ¡ˆç”Ÿæˆçš„ä¸ä¸€è‡´ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡åŒ»å­¦æ•°æ®é›†æ˜¯è¿›è¡Œæœ‰æ•ˆå¤§è§„æ¨¡RLçš„ä¸»è¦éšœç¢ã€‚</li>
<li>ä»‹ç»äº†Med-Zero-17Kæ•°æ®é›†ï¼Œä¸“ä¸ºçº¯RLè®­ç»ƒè®¾è®¡ï¼Œæ¶µç›–å¤šç§åŒ»å­¦å›¾åƒæ¨¡æ€å’Œä¸´åºŠä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†å¤§å‹RLæ¡†æ¶CAPOï¼Œç¡®ä¿æ„ŸçŸ¥ä¸æ¨ç†ã€æ¨ç†åˆ°ç­”æ¡ˆçš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05b7953bcd29cd2445f2b068e8e60052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a977771b783bc5b88ea6c1d97b7cac17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2730e45b55a37fa00cd2adfd2533f5c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-728e82ff83a27c0263759eed69af461d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af8ec8e045581e0cb37d10119ced5be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d50201bb9971db5e0a287dd31f5f11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-286e761a94029752d03d057a00550572.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ConsistencyChecker-Tree-based-Evaluation-of-LLM-Generalization-Capabilities"><a href="#ConsistencyChecker-Tree-based-Evaluation-of-LLM-Generalization-Capabilities" class="headerlink" title="ConsistencyChecker: Tree-based Evaluation of LLM Generalization   Capabilities"></a>ConsistencyChecker: Tree-based Evaluation of LLM Generalization   Capabilities</h2><p><strong>Authors:Zhaochen Hong, Haofei Yu, Jiaxuan You</strong></p>
<p>Evaluating consistency in large language models (LLMs) is crucial for ensuring reliability, particularly in complex, multi-step interactions between humans and LLMs. Traditional self-consistency methods often miss subtle semantic changes in natural language and functional shifts in code or equations, which can accumulate over multiple transformations. To address this, we propose ConsistencyChecker, a tree-based evaluation framework designed to measure consistency through sequences of reversible transformations, including machine translation tasks and AI-assisted programming tasks. In our framework, nodes represent distinct text states, while edges correspond to pairs of inverse operations. Dynamic and LLM-generated benchmarks ensure a fair assessment of the modelâ€™s generalization ability and eliminate benchmark leakage. Consistency is quantified based on similarity across different depths of the transformation tree. Experiments on eight models from various families and sizes show that ConsistencyChecker can distinguish the performance of different models. Notably, our consistency scores-computed entirely without using WMT paired data-correlate strongly (r &gt; 0.7) with WMT 2024 auto-ranking, demonstrating the validity of our benchmark-free approach. Our implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/consistencychecker">https://github.com/ulab-uiuc/consistencychecker</a>. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€è‡´æ€§å¯¹äºç¡®ä¿å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººç±»å’ŒLLMä¹‹é—´å¤æ‚çš„å¤šæ­¥äº¤äº’ä¸­ã€‚ä¼ ç»Ÿçš„è‡ªæ´½æ€§æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è‡ªç„¶è¯­è¨€ä¸­çš„ç»†å¾®è¯­ä¹‰å˜åŒ–å’Œä»£ç æˆ–å…¬å¼ä¸­çš„åŠŸèƒ½è½¬æ¢ï¼Œè¿™äº›ç»†å¾®çš„å˜åŒ–ä¼šåœ¨å¤šæ¬¡è½¬æ¢ä¸­ç´¯ç§¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConsistencyCheckerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ ‘çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å¯é€†è½¬æ¢æ¥è¡¡é‡ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ä»»åŠ¡å’ŒAIè¾…åŠ©ç¼–ç¨‹ä»»åŠ¡ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼ŒèŠ‚ç‚¹ä»£è¡¨ä¸åŒçš„æ–‡æœ¬çŠ¶æ€ï¼Œè€Œè¾¹å¯¹åº”ä¸€å¯¹é€†å‘æ“ä½œã€‚åŠ¨æ€å’ŒLLMç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ç¡®ä¿äº†å…¬å¹³åœ°è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ¶ˆé™¤äº†åŸºå‡†æµ‹è¯•æ³„æ¼ã€‚ä¸€è‡´æ€§æ˜¯æ ¹æ®è½¬æ¢æ ‘ä¸åŒæ·±åº¦çš„ç›¸ä¼¼æ€§æ¥é‡åŒ–çš„ã€‚å¯¹æ¥è‡ªä¸åŒå®¶æ—å’Œè§„æ¨¡çš„å…«ä¸ªæ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼ŒConsistencyCheckerå¯ä»¥åŒºåˆ†ä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ä¸€è‡´æ€§è¯„åˆ†å®Œå…¨æœªä½¿ç”¨WMTé…å¯¹æ•°æ®è®¡ç®—ï¼Œä¸WMT 2024è‡ªåŠ¨æ’åæœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ˆr &gt; 0.7ï¼‰ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ— åŸºå‡†æµ‹è¯•æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/consistencychecker%E3%80%82">https://github.com/ulab-uiuc/consistencycheckerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12376v2">PDF</a> Accepted at ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°ä¸€è‡´æ€§å¯¹äºç¡®ä¿å…¶åœ¨äººç±»ä¸LLMä¹‹é—´å¤æ‚ã€å¤šæ­¥éª¤äº¤äº’ä¸­çš„å¯é æ€§è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•å¸¸å¸¸å¿½ç•¥è‡ªç„¶è¯­è¨€ä¸­çš„ç»†å¾®è¯­ä¹‰å˜åŒ–å’Œä»£ç æˆ–æ–¹ç¨‹å¼ä¸­çš„åŠŸèƒ½è½¬å˜ï¼Œè¿™äº›è½¬å˜å¯èƒ½ä¼šåœ¨å¤šæ¬¡è½¬æ¢ä¸­ç´¯ç§¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConsistencyCheckerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ ‘çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å¯é€†è½¬æ¢æ¥è¡¡é‡ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ä»»åŠ¡å’ŒAIè¾…åŠ©ç¼–ç¨‹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸­çš„èŠ‚ç‚¹ä»£è¡¨ä¸åŒçš„æ–‡æœ¬çŠ¶æ€ï¼Œè¾¹å¯¹åº”ä¸€å¯¹é€†å‘æ“ä½œã€‚åŠ¨æ€å’ŒLLMç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ç¡®ä¿å¯¹æ¨¡å‹çš„é€šç”¨èƒ½åŠ›è¿›è¡Œå…¬å¹³è¯„ä¼°ï¼Œå¹¶æ¶ˆé™¤åŸºå‡†æ³„éœ²ã€‚ä¸€è‡´æ€§æ˜¯æ ¹æ®è½¬æ¢æ ‘ä¸åŒæ·±åº¦çš„ç›¸ä¼¼æ€§é‡åŒ–çš„ã€‚åœ¨å¤šä¸ªæ¨¡å‹å’Œå¤§å°å®¶æ—çš„å…«ç§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒConsistencyCheckerå¯ä»¥åŒºåˆ†ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ä¸€è‡´æ€§è¯„åˆ†ï¼ˆå®Œå…¨æœªä½¿ç”¨WMTé…å¯¹æ•°æ®è®¡ç®—ï¼‰ä¸WMT 2024è‡ªåŠ¨æ’åå…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ˆr &gt; 0.7ï¼‰ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ— åŸºå‡†æµ‹è¯•æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€è‡´æ€§è¯„ä»·å¯¹äºç¡®ä¿å…¶åœ¨å¤æ‚äº¤äº’ä¸­çš„å¯é æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æ•æ‰è¯­ä¹‰å’ŒåŠŸèƒ½çš„ç»†å¾®å˜åŒ–ã€‚</li>
<li>ConsistencyCheckeræ¡†æ¶è¢«æå‡ºï¼Œé€šè¿‡å¯é€†è½¬æ¢åºåˆ—æ¥è¡¡é‡ä¸€è‡´æ€§ã€‚</li>
<li>æ¡†æ¶ä¸­çš„èŠ‚ç‚¹å’Œè¾¹åˆ†åˆ«ä»£è¡¨ä¸åŒçš„æ–‡æœ¬çŠ¶æ€å’Œé€†å‘æ“ä½œå¯¹ã€‚</li>
<li>åŠ¨æ€å’ŒLLMç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ç¡®ä¿å…¬å¹³è¯„ä¼°æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œå¹¶æ¶ˆé™¤åŸºå‡†æ³„éœ²ã€‚</li>
<li>ä¸€è‡´æ€§æ˜¯æ ¹æ®è½¬æ¢æ ‘ä¸åŒæ·±åº¦çš„ç›¸ä¼¼æ€§è¿›è¡Œé‡åŒ–çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27a865f8e6570470b6d1ee670c347ce2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f218e13ea354aba559071440614fac14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97669298649463285da4eab74484ac6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cd9058b15406ea2452711d75b4721b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-227d1fcb2ad5f19990f98074aede6ca2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bhatt-Conjectures-On-Necessary-But-Not-Sufficient-Benchmark-Tautology-for-Human-Like-Reasoning"><a href="#Bhatt-Conjectures-On-Necessary-But-Not-Sufficient-Benchmark-Tautology-for-Human-Like-Reasoning" class="headerlink" title="Bhatt Conjectures: On Necessary-But-Not-Sufficient Benchmark Tautology   for Human Like Reasoning"></a>Bhatt Conjectures: On Necessary-But-Not-Sufficient Benchmark Tautology   for Human Like Reasoning</h2><p><strong>Authors:Manish Bhatt</strong></p>
<p>The Bhatt Conjectures framework introduces rigorous, hierarchical benchmarks for evaluating AI reasoning and understanding, moving beyond pattern matching to assess representation invariance, robustness, and metacognitive self-awareness. The agentreasoning-sdk demonstrates practical implementation, revealing that current AI models struggle with complex reasoning tasks and highlighting the need for advanced evaluation protocols to distinguish genuine cognitive abilities from statistical inference. This comprehensive AI evaluation methodology establishes necessary-but-not-sufficient benchmark conditions for advancing artificial general intelligence research while maintaining academic search engine optimization standards through strategic keyword density optimization, technical terminology consistency, and cross-modal evaluation protocols.   <a target="_blank" rel="noopener" href="https://github.com/mbhatt1/agentreasoning-sdk">https://github.com/mbhatt1/agentreasoning-sdk</a> </p>
<blockquote>
<p>BhattçŒœæƒ³æ¡†æ¶å¼•å…¥äº†ä¸¥æ ¼çš„å±‚æ¬¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½çš„æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚å®ƒè¶…è¶Šäº†æ¨¡å¼åŒ¹é…ï¼Œè¯„ä¼°è¡¨ç¤ºä¸å˜æ€§ã€ç¨³å¥æ€§å’Œå…ƒè®¤çŸ¥è‡ªæˆ‘æ„è¯†ã€‚agentreasoning-sdkå±•ç¤ºäº†å®é™…å®æ–½æƒ…å†µï¼Œè¡¨æ˜å½“å‰çš„äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åº”å¯¹å¤æ‚çš„æ¨ç†ä»»åŠ¡æ—¶é‡åˆ°äº†å›°éš¾ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦é«˜çº§è¯„ä¼°åè®®æ¥åŒºåˆ†çœŸæ­£çš„è®¤çŸ¥èƒ½åŠ›å’Œç»Ÿè®¡æ¨æ–­ã€‚è¿™ç§å…¨é¢çš„AIè¯„ä¼°æ–¹æ³•ä¸ºæ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ç ”ç©¶å»ºç«‹äº†å¿…è¦ä½†ä¸å……åˆ†çš„åŸºå‡†æ¡ä»¶ï¼ŒåŒæ—¶é€šè¿‡æˆ˜ç•¥å…³é”®è¯å¯†åº¦ä¼˜åŒ–ã€æŠ€æœ¯æœ¯è¯­ä¸€è‡´æ€§å’Œè·¨æ¨¡æ€è¯„ä¼°åè®®æ¥ç»´æŒå­¦æœ¯æœç´¢å¼•æ“ä¼˜åŒ–æ ‡å‡†ã€‚è¯¦æƒ…è¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/mbhatt1/agentreasoning-sdk%E3%80%82%EF%BC%88%E5%BD%AD%E6%B2%AC%E6%AD%A5%E5%BD%B%E4%B9%8B%E5%8F%98%E6%98%AF%E5%9B%A0%E5%AE%9A%E8%BF%9E%E6%95%85%E6%A0%B7%E7%A8%8B%E5%9B%A0%E4%B8%BA%E4%BB%8B%E5%8C%BA%E4%B8%AD%E8%BF%9B%E4%BA%86GitHub%E4%BB%93%E5%BA%93%EF%BC%89">https://github.com/mbhatt1/agentreasoning-sdkã€‚ï¼ˆæ­¤å¤„æä¾›é“¾æ¥æ˜¯å› ä¸ºåŸæ–‡ä¸­æåˆ°äº†GitHubä»“åº“ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11423v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºBhattçŒœæƒ³æ¡†æ¶ï¼Œå¼•å…¥ä¸¥æ ¼çš„å±‚æ¬¡åŒ–åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°AIæ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚ä¸ä»…è¯„ä¼°æ¨¡å¼åŒ¹é…ï¼Œè¿˜è¯„ä¼°è¡¨ç¤ºä¸å˜æ€§ã€ç¨³å¥æ€§å’Œå…ƒè®¤çŸ¥è‡ªæˆ‘æ„è¯†ã€‚agentreasoning-sdkçš„å®é™…åº”ç”¨è¡¨æ˜ï¼Œå½“å‰AIæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ï¼Œå‡¸æ˜¾äº†éœ€è¦å…ˆè¿›çš„è¯„ä¼°åè®®æ¥åŒºåˆ†çœŸæ­£çš„è®¤çŸ¥èƒ½åŠ›å’Œç»Ÿè®¡æ¨æ–­ã€‚æ­¤å…¨é¢çš„AIè¯„ä¼°æ–¹æ³•è®ºå»ºç«‹äº†å¿…è¦çš„ä½†éå……åˆ†çš„åŸºå‡†æ¡ä»¶ï¼Œä»¥ä¿ƒè¿›äººå·¥æ™ºèƒ½é€šç”¨ç ”ç©¶çš„è¿›æ­¥ï¼ŒåŒæ—¶é€šè¿‡å…³é”®è¯å¯†åº¦ä¼˜åŒ–ã€æŠ€æœ¯æœ¯è¯­ä¸€è‡´æ€§å’Œè·¨æ¨¡æ€è¯„ä¼°åè®®æ¥ç»´æŒå­¦æœ¯æœç´¢å¼•æ“ä¼˜åŒ–æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bhatt Conjecturesæ¡†æ¶å¼•å…¥å±‚æ¬¡åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°AIçš„æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ä»…å…³æ³¨æ¨¡å¼åŒ¹é…ï¼Œè¿˜å…³æ³¨è¡¨ç¤ºä¸å˜æ€§ã€ç¨³å¥æ€§å’Œå…ƒè®¤çŸ¥è‡ªæˆ‘æ„è¯†ã€‚</li>
<li>agentreasoning-sdkçš„å®é™…åº”ç”¨æ­ç¤ºäº†å½“å‰AIæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦å…ˆè¿›çš„è¯„ä¼°åè®®æ¥åŒºåˆ†AIçš„çœŸæ­£è®¤çŸ¥èƒ½åŠ›å’Œç»Ÿè®¡æ¨æ–­ã€‚</li>
<li>å…¨é¢çš„AIè¯„ä¼°æ–¹æ³•è®ºä¸ºæ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨ç ”ç©¶æä¾›äº†å¿…è¦çš„ä½†éå……åˆ†çš„åŸºå‡†æ¡ä»¶ã€‚</li>
<li>é€šè¿‡å…³é”®è¯å¯†åº¦ä¼˜åŒ–ã€æŠ€æœ¯æœ¯è¯­ä¸€è‡´æ€§å’Œè·¨æ¨¡æ€è¯„ä¼°åè®®æ¥ç»´æŒå­¦æœ¯æœç´¢å¼•æ“ä¼˜åŒ–æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f661478ce69a5d8f0486227dbca5a997.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45879e6e9c9716df0716b84825153162.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50a984d685d591ca2bf687294331f747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cd518d01bfc4d11d35833643a03cf61.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-Reasoning-to-Code-GRPO-Optimization-for-Underrepresented-Languages"><a href="#From-Reasoning-to-Code-GRPO-Optimization-for-Underrepresented-Languages" class="headerlink" title="From Reasoning to Code: GRPO Optimization for Underrepresented Languages"></a>From Reasoning to Code: GRPO Optimization for Underrepresented Languages</h2><p><strong>Authors:Federico Pennino, Bianca Raimondi, Massimo Rondelli, Andrea Gurioli, Maurizio Gabbrielli</strong></p>
<p>Generating accurate and executable code using large language models (LLMs) is challenging for languages with limited public training data compared to popular languages such as Python. This paper introduces a generalizable approach that uses small-scale code versions of the Qwen 2.5 model combined with Group Relative Policy Optimization (GRPO) to enable effective code generation through explicit reasoning steps, which is particularly beneficial for languages with smaller source code databases. Using Prolog as a representative use case â€“ given its limited online presence â€“ the initial model faced challenges in generating executable code. After some training steps, the model successfully produces logically consistent and syntactically accurate code by directly integrating reasoning-driven feedback into the reinforcement learning loop. Experimental evaluations using mathematical logic problem benchmarks illustrate significant improvements in reasoning quality, code accuracy, and logical correctness, underscoring the potential of this approach to benefit a wide range of programming languages lacking extensive training resources. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå‡†ç¡®ä¸”å¯æ‰§è¡Œçš„ä»£ç å¯¹äºæœ‰é™å…¬å…±è®­ç»ƒæ•°æ®çš„è¯­è¨€æ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä¸Pythonç­‰æµè¡Œè¯­è¨€ç›¸æ¯”ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è§„æ¨¡è¾ƒå°çš„Qwen 2.5æ¨¡å‹ç‰ˆæœ¬ï¼Œç»“åˆç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡æ˜ç¡®çš„æ¨ç†æ­¥éª¤å®ç°æœ‰æ•ˆçš„ä»£ç ç”Ÿæˆï¼Œè¿™å¯¹äºæºä»£ç æ•°æ®åº“è¾ƒå°çš„è¯­è¨€å°¤å…¶æœ‰ç›Šã€‚ä»¥Prologä½œä¸ºå…¸å‹çš„ç”¨ä¾‹â€”â€”è€ƒè™‘åˆ°å…¶æœ‰é™çš„åœ¨çº¿å­˜åœ¨â€”â€”åˆå§‹æ¨¡å‹åœ¨ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ç»è¿‡ä¸€äº›è®­ç»ƒæ­¥éª¤åï¼Œè¯¥æ¨¡å‹é€šè¿‡ç›´æ¥å°†æ¨ç†é©±åŠ¨åé¦ˆæ•´åˆåˆ°å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ï¼ŒæˆåŠŸç”Ÿæˆäº†é€»è¾‘ä¸€è‡´ä¸”è¯­æ³•å‡†ç¡®çš„ä»£ç ã€‚ä½¿ç”¨æ•°å­¦é€»è¾‘é—®é¢˜åŸºå‡†æµ‹è¯•è¿›è¡Œçš„å®éªŒè¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå…¶åœ¨æ¨ç†è´¨é‡ã€ä»£ç å‡†ç¡®æ€§å’Œé€»è¾‘æ­£ç¡®æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ï¼Œè¿™çªæ˜¾äº†è¿™ç§æ–¹æ³•åœ¨ç¼ºä¹å¹¿æ³›è®­ç»ƒèµ„æºçš„å¤šç§ç¼–ç¨‹è¯­è¨€ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11027v2">PDF</a> Preprint. Under review</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚Prologï¼‰ä¸­ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå‡†ç¡®ä¸”å¯æ‰§è¡Œçš„ä»£ç æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€šç”¨æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å°å‹ä»£ç ç‰ˆæœ¬çš„Qwen 2.5æ¨¡å‹å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡æ˜ç¡®çš„æ¨ç†æ­¥éª¤å®ç°æœ‰æ•ˆçš„ä»£ç ç”Ÿæˆã€‚é€šè¿‡è®­ç»ƒæ­¥éª¤ï¼Œæ¨¡å‹èƒ½å¤ŸæˆåŠŸç”Ÿæˆé€»è¾‘ä¸€è‡´ä¸”è¯­æ³•å‡†ç¡®çš„ä»£ç ï¼Œç›´æ¥å°†æ¨ç†é©±åŠ¨åé¦ˆé›†æˆåˆ°å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é€»è¾‘æ¨ç†è´¨é‡ã€ä»£ç å‡†ç¡®æ€§å’Œé€»è¾‘æ­£ç¡®æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä¸ºç¼ºä¹å¤§é‡è®­ç»ƒèµ„æºçš„ç¼–ç¨‹è¯­è¨€æä¾›äº†æ½œåœ¨çš„ç›Šå¤„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå…·æœ‰æœ‰é™å…¬å¼€è®­ç»ƒæ•°æ®çš„è¯­è¨€çš„å¯æ‰§è¡Œä»£ç æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå°å‹ä»£ç ç‰ˆæœ¬çš„Qwen 2.5æ¨¡å‹å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„é€šç”¨æ–¹æ³•ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®çš„æ¨ç†æ­¥éª¤å®ç°æœ‰æ•ˆä»£ç ç”Ÿæˆï¼Œå¯¹æºä»£ç æ•°æ®åº“è¾ƒå°çš„è¯­è¨€ç‰¹åˆ«æœ‰ç›Šã€‚</li>
<li>ä½¿ç”¨Prologä½œä¸ºæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨ç”Ÿæˆé€»è¾‘ä¸€è‡´ä¸”è¯­æ³•å‡†ç¡®çš„ä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç›´æ¥å°†æ¨ç†é©±åŠ¨åé¦ˆé›†æˆåˆ°å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ï¼Œæé«˜äº†ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é€»è¾‘æ¨ç†è´¨é‡ã€ä»£ç å‡†ç¡®æ€§å’Œé€»è¾‘æ­£ç¡®æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-890d2abe2796d9207f6fc8902073b9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-963a007d977ef666f829911b4245da4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f209bbb22043f78a02249b149af54d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7fc5cf677703f841d9a1775696458dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-549ebbb8c8d741141f512e9d8c61290d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39a5b913215a273c7856712ed50bc307.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation"><a href="#Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation" class="headerlink" title="Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation"></a>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation</h2><p><strong>Authors:Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xingang Wang</strong></p>
<p>Recent advances in large language models, especially in natural language understanding and reasoning, have opened new possibilities for text-to-motion generation. Although existing approaches have made notable progress in semantic alignment and motion synthesis, they often rely on end-to-end mapping strategies that fail to capture deep linguistic structures and logical reasoning. Consequently, generated motions tend to lack controllability, consistency, and diversity. To address these limitations, we propose Motion-R1, a unified motion-language modeling framework that integrates a Chain-of-Thought mechanism. By explicitly decomposing complex textual instructions into logically structured action paths, Motion-R1 provides high-level semantic guidance for motion generation, significantly enhancing the modelâ€™s ability to interpret and execute multi-step, long-horizon, and compositionally rich commands. To train our model, we adopt Group Relative Policy Optimization, a reinforcement learning algorithm designed for large models, which leverages motion quality feedback to optimize reasoning chains and motion synthesis jointly. Extensive experiments across multiple benchmark datasets demonstrate that Motion-R1 achieves competitive or superior performance compared to state-of-the-art methods, particularly in scenarios requiring nuanced semantic understanding and long-term temporal coherence. The code, model and data will be publicly available. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ä¸ºæ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç«¯åˆ°ç«¯çš„æ˜ å°„ç­–ç•¥ï¼Œæ— æ³•æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘æ¨ç†ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„è¿åŠ¨å¾€å¾€ç¼ºä¹å¯æ§æ€§ã€ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Motion-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¿åŠ¨è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ€ç»´é“¾æœºåˆ¶ã€‚é€šè¿‡å°†å¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤æ˜ç¡®åœ°åˆ†è§£ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ï¼ŒMotion-R1ä¸ºè¿åŠ¨ç”Ÿæˆæä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹è§£é‡Šå’Œæ‰§è¡Œå¤šæ­¥éª¤ã€é•¿æœŸå’Œç»„åˆä¸°å¯ŒæŒ‡ä»¤çš„èƒ½åŠ›ã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é’ˆå¯¹å¤§å‹æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥ç®—æ³•åˆ©ç”¨è¿åŠ¨è´¨é‡åé¦ˆæ¥ä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMotion-R1ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ï¼Œå…¶æ€§èƒ½å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10353v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ä¸ºæ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Motion-R1æ¡†æ¶ï¼Œå®ƒç»“åˆäº†Chain-of-Thoughtæœºåˆ¶ï¼Œå°†å¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤åˆ†è§£ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ï¼Œä¸ºè¿åŠ¨ç”Ÿæˆæä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–è®­ç»ƒï¼Œå®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨å¤šä»»åŠ¡åŸºå‡†æ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚Motion-R1çš„æºä»£ç ã€æ¨¡å‹å’Œæ•°éƒ½å°†å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆé¢†åŸŸå±•ç°å‡ºæ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘ã€‚</li>
<li>Motion-R1æ¡†æ¶ç»“åˆäº†Chain-of-Thoughtæœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆåˆ†è§£å¤æ‚æ–‡æœ¬æŒ‡ä»¤ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ã€‚</li>
<li>Motion-R1æ¡†æ¶æé«˜äº†è¿åŠ¨ç”Ÿæˆçš„å¯æ§æ€§ã€ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•Group Relative Policy Optimizationå¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–è®­ç»ƒã€‚</li>
<li>Motion-R1åœ¨å¤šä»»åŠ¡åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-164c03c0a06ee1f9fb8274e13d02a9b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc821b3a22a5c622efa4c17a2912ae49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc9c1fc1631b69fbbe56d8f72b8ba81.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Intra-Trajectory-Consistency-for-Reward-Modeling"><a href="#Intra-Trajectory-Consistency-for-Reward-Modeling" class="headerlink" title="Intra-Trajectory Consistency for Reward Modeling"></a>Intra-Trajectory Consistency for Reward Modeling</h2><p><strong>Authors:Chaoyang Zhou, Shunyu Liu, Zengmao Wang, Di Wang, Rong-Cheng Tu, Bo Du, Dacheng Tao</strong></p>
<p>Reward models are critical for improving large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) or inference-time verification. Current reward modeling typically relies on scores of overall responses to learn the outcome rewards for the responses. However, since the response-level scores are coarse-grained supervision signals, the reward model struggles to identify the specific components within a response trajectory that truly correlate with the scores, leading to poor generalization on unseen responses. In this paper, we propose to leverage generation probabilities to establish reward consistency between processes in the response trajectory, which allows the response-level supervisory signal to propagate across processes, thereby providing additional fine-grained signals for reward learning. Building on analysis under the Bayesian framework, we develop an intra-trajectory consistency regularization to enforce that adjacent processes with higher next-token generation probability maintain more consistent rewards. We apply the proposed regularization to the advanced outcome reward model, improving its performance on RewardBench. Besides, we show that the reward model trained with the proposed regularization induces better DPO-aligned policies and achieves better best-of-N (BON) inference-time verification results. Our code is provided in <a target="_blank" rel="noopener" href="https://github.com/chaoyang101/ICRM">https://github.com/chaoyang101/ICRM</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹å¯¹äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æˆ–æ¨ç†æ—¶é—´éªŒè¯ä¸­ã€‚å½“å‰çš„å¥–åŠ±å»ºæ¨¡é€šå¸¸ä¾èµ–äºæ•´ä½“å“åº”çš„åˆ†æ•°æ¥å­¦ä¹ å“åº”çš„ç»“æœå¥–åŠ±ã€‚ç„¶è€Œï¼Œç”±äºå“åº”çº§åˆ«çš„åˆ†æ•°æ˜¯ç²—ç²’åº¦çš„ç›‘ç£ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹éš¾ä»¥è¯†åˆ«å“åº”è½¨è¿¹å†…çœŸæ­£ä¸åˆ†æ•°ç›¸å…³çš„ç‰¹å®šç»„ä»¶ï¼Œå¯¼è‡´åœ¨æœªè§è¿‡çš„å“åº”ä¸Šæ³›åŒ–æ€§èƒ½å·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ç”Ÿæˆæ¦‚ç‡æ¥å»ºç«‹å“åº”è½¨è¿¹ä¸­è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ï¼Œè¿™å…è®¸å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·åœ¨è¿‡ç¨‹ä¹‹é—´ä¼ æ’­ï¼Œä»è€Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç»†ç²’åº¦ä¿¡å·ã€‚æˆ‘ä»¬åœ¨è´å¶æ–¯æ¡†æ¶çš„åˆ†æåŸºç¡€ä¸Šï¼Œå¼€å‘äº†ä¸€ç§è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥å¼ºåˆ¶å…·æœ‰æ›´é«˜ä¸‹ä¸€ä¸ªä»¤ç‰Œç”Ÿæˆæ¦‚ç‡çš„ç›¸é‚»è¿‡ç¨‹ä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ­£åˆ™åŒ–åº”ç”¨äºé«˜çº§ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œåœ¨RewardBenchä¸Šæé«˜äº†å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨æ‰€æå‡ºæ­£åˆ™åŒ–è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹èƒ½è¯±å‘æ›´å¥½çš„DPOå¯¹é½ç­–ç•¥ï¼Œå¹¶åœ¨æœ€ä½³Nï¼ˆBONï¼‰æ¨ç†æ—¶é—´éªŒè¯ä¸­å®ç°æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/chaoyang101/ICRM%E3%80%82">https://github.com/chaoyang101/ICRMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09096v3">PDF</a> Under review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨ç”Ÿæˆæ¦‚ç‡æ¥å»ºç«‹å“åº”è½¨è¿¹ä¸­è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ï¼Œä»¥è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¹è¿›ä¸­çš„ä¸è¶³ã€‚é€šè¿‡å¼•å…¥å“åº”è½¨è¿¹å†…çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œä½¿ç›¸é‚»è¿‡ç¨‹å…·æœ‰æ›´é«˜çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œç”Ÿæˆæ¦‚ç‡ï¼Œä»è€Œä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚è¯¥æ­£åˆ™åŒ–åº”ç”¨äºé«˜çº§ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œæé«˜äº†å…¶åœ¨RewardBenchä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¥–åŠ±æ¨¡å‹å¯¹æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æˆ–æ¨ç†æ—¶é—´éªŒè¯ä¸­ã€‚</li>
<li>å½“å‰å¥–åŠ±å»ºæ¨¡é€šå¸¸ä¾èµ–äºæ•´ä½“å“åº”çš„åˆ†æ•°æ¥å­¦ä¹ ç»“æœå¥–åŠ±ï¼Œä½†è¿™ç§æ–¹å¼éš¾ä»¥è¯†åˆ«ä¸åˆ†æ•°çœŸæ­£ç›¸å…³çš„å“åº”è½¨è¿¹ä¸­çš„ç‰¹å®šç»„ä»¶ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆæ¦‚ç‡æ¥å»ºç«‹å“åº”è½¨è¿¹ä¸­è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ï¼Œä»¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>å“åº”çº§ç›‘ç£ä¿¡å·å¯ä»¥é€šè¿‡è¿‡ç¨‹ä¼ æ’­ï¼Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç»†ç²’åº¦ä¿¡å·ã€‚</li>
<li>å¼•å…¥åŸºäºè´å¶æ–¯æ¡†æ¶çš„åˆ†æå’Œå“åº”è½¨è¿¹å†…çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œä»¥ç¡®ä¿ç›¸é‚»è¿‡ç¨‹å…·æœ‰æ›´é«˜ç”Ÿæˆæ¦‚ç‡çš„åŒæ—¶ä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚</li>
<li>åº”ç”¨è¯¥æ­£åˆ™åŒ–è‡³é«˜çº§ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œæé«˜åœ¨RewardBenchä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab0dc3f906a839c6ad4988b6d705f766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d5a4b439c9d8381fda7bf76a50a02f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a197ccf26b0d6a33db627ad0545d04b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FlagEvalMM-A-Flexible-Framework-for-Comprehensive-Multimodal-Model-Evaluation"><a href="#FlagEvalMM-A-Flexible-Framework-for-Comprehensive-Multimodal-Model-Evaluation" class="headerlink" title="FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model   Evaluation"></a>FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model   Evaluation</h2><p><strong>Authors:Zheqi He, Yesheng Liu, Jing-shu Zheng, Xuejing Li, Jin-Ge Yao, Bowen Qin, Richeng Xuan, Xi Yang</strong></p>
<p>We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image&#x2F;video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at<a target="_blank" rel="noopener" href="https://github.com/flageval-baai/FlagEvalMM">https://github.com/flageval-baai/FlagEvalMM</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºFlagEvalMMï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¤šç§æ¨¡æ€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¦‚è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆå’Œå›¾åƒæ–‡æœ¬æ£€ç´¢ç­‰ã€‚æˆ‘ä»¬é€šè¿‡ç‹¬ç«‹çš„è¯„ä¼°æœåŠ¡å°†æ¨¡å‹æ¨ç†ä¸è¯„ä¼°åˆ†å¼€ï¼Œä»è€Œå®ç°çµæ´»çš„èµ„æºé…ç½®å’Œæ–°ä»»åŠ¡åŠæ— ç¼é›†æˆæ¨¡å‹çš„ä¾¿æ·æ€§ã€‚æ­¤å¤–ï¼ŒFlagEvalMMåˆ©ç”¨å…ˆè¿›çš„æ¨ç†åŠ é€Ÿå·¥å…·ï¼ˆå¦‚vLLMã€SGLangï¼‰å’Œå¼‚æ­¥æ•°æ®åŠ è½½æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜è¯„ä¼°æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlagEvalMMèƒ½å‡†ç¡®é«˜æ•ˆåœ°æ´å¯Ÿæ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œæˆä¸ºæ¨åŠ¨å¤šæ¨¡æ€ç ”ç©¶çš„é‡è¦å·¥å…·ã€‚è¯¥æ¡†æ¶å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/flageval-baai/FlagEvalMM">https://github.com/flageval-baai/FlagEvalMM</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09081v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æ¨å‡ºäº†FlagEvalMMï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°è·¨å¤šç§è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„å¤šåª’ä½“æ¨¡å‹ï¼Œå¦‚è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆå’Œå›¾åƒæ–‡æœ¬æ£€ç´¢ã€‚å®ƒé€šè¿‡ç‹¬ç«‹çš„è¯„ä¼°æœåŠ¡å°†æ¨¡å‹æ¨ç†ä¸è¯„ä¼°è§£è€¦ï¼Œä»è€Œå®ç°çµæ´»çš„èµ„æºåˆ†é…å’Œæ–°ä»»åŠ¡åŠæ¨¡å‹çš„æ— ç¼é›†æˆã€‚æ­¤å¤–ï¼ŒFlagEvalMMåˆ©ç”¨å…ˆè¿›çš„æ¨ç†åŠ é€Ÿå·¥å…·ï¼ˆå¦‚vLLMã€SGLangï¼‰å’Œå¼‚æ­¥æ•°æ®åŠ è½½ï¼Œæ˜¾è‘—æé«˜è¯„ä¼°æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlagEvalMMèƒ½å‡†ç¡®é«˜æ•ˆåœ°æ´å¯Ÿæ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œæ˜¯æ¨åŠ¨å¤šåª’ä½“ç ”ç©¶çš„æœ‰åŠ›å·¥å…·ã€‚è¯¥æ¡†æ¶å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/flageval-baai/FlagEvalMM%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/flageval-baai/FlagEvalMMè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlagEvalMMæ˜¯ä¸€ä¸ªå¼€æºçš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå…¨é¢è¯„ä¼°å¤šåª’ä½“æ¨¡å‹ã€‚</li>
<li>å®ƒæ”¯æŒå¤šç§è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆå’Œå›¾åƒæ–‡æœ¬æ£€ç´¢ã€‚</li>
<li>é€šè¿‡ç‹¬ç«‹çš„è¯„ä¼°æœåŠ¡ï¼Œæ¨¡å‹æ¨ç†ä¸è¯„ä¼°è¢«è§£è€¦ï¼Œå®ç°çµæ´»èµ„æºåˆ†é…åŠæ–°ä»»åŠ¡æ¨¡å‹çš„é›†æˆã€‚</li>
<li>FlagEvalMMåˆ©ç”¨æ¨ç†åŠ é€Ÿå·¥å…·å’Œå¼‚æ­¥æ•°æ®åŠ è½½æå‡è¯„ä¼°æ•ˆç‡ã€‚</li>
<li>æ¡†æ¶èƒ½å‡†ç¡®é«˜æ•ˆåœ°ä¸ºæ¨¡å‹æä¾›ä¼˜ç¼ºç‚¹æ´å¯Ÿã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¯æ¨è¿›å¤šåª’ä½“ç ”ç©¶çš„é‡è¦å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6cad23cf48556ada014674b0d1cf3c53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f9bd8909c534a6cc6dca8fa6078da4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf3778f74a33189c0dec61421a1eae0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd49a0cf309b078bbe7d102e1362e2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7672ad0a86f6e7c4e7f0958aaacdcfd0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BIS-Reasoning-1-0-The-First-Large-Scale-Japanese-Benchmark-for-Belief-Inconsistent-Syllogistic-Reasoning"><a href="#BIS-Reasoning-1-0-The-First-Large-Scale-Japanese-Benchmark-for-Belief-Inconsistent-Syllogistic-Reasoning" class="headerlink" title="BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for   Belief-Inconsistent Syllogistic Reasoning"></a>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for   Belief-Inconsistent Syllogistic Reasoning</h2><p><strong>Authors:Ha-Thanh Nguyen, Chaoran Liu, Koichi Takeda, Yusuke Miyao, Pontus Stenetorp, Qianying Liu, Su Myat Noe, Hideyuki Tachibana, Sadao Kurohashi</strong></p>
<p>We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†BIS Reasoning 1.0ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡æ—¥è¯­é›†åˆæ¨ç†æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¿¡å¿µä¸ä¸€è‡´æ¨ç†ã€‚ä¸åŒäºå…ˆå‰å…³æ³¨é€šç”¨æˆ–ä¿¡å¿µä¸€è‡´çš„æ¨ç†çš„NeuBAROCOå’ŒJFLDæ•°æ®é›†ï¼ŒBIS Reasoning 1.0å¼•å…¥äº†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ’æ¯”æ¨ç†ï¼Œä»¥æ­ç¤ºè®­ç»ƒåœ¨äººç±»è¯­æ–™åº“ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†åè§ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPTæ¨¡å‹ã€Claudeæ¨¡å‹å’Œé¢†å…ˆçš„æ—¥æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒGPT-4oçš„å‡†ç¡®ç‡è¾¾åˆ°äº†79.54%ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µå†²çªçš„è¾“å…¥æ—¶çš„å…³é”®å¼±ç‚¹ã€‚è¿™äº›å‘ç°åœ¨æ³•å¾‹ã€åŒ»ç–—ä¿å¥å’Œç§‘å­¦æ–‡çŒ®ç­‰é«˜é£é™©é¢†åŸŸéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹æ—¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼ŒçœŸç›¸å¿…é¡»è¶…è¶Šç›´è§‰ä¿¡å¿µï¼Œä»¥ç¡®ä¿å®Œæ•´æ€§å’Œå®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06955v2">PDF</a> This version includes an updated literature review, added   acknowledgements, and a revised author list</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¿¡å¿µä¸ä¸€è‡´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼±ç‚¹ã€‚ä¸ºæ­¤ï¼Œæ¨å‡ºäº†BIS Reasoning 1.0æ•°æ®é›†ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ—¥è¯­ä¸‹çš„é€»è¾‘æ¨ç†è§£é¢˜æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMåœ¨ä¿¡å¿µä¸ä¸€è‡´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†å¼•å…¥é€»è¾‘ä¸Šåˆç†ä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é¢˜ï¼Œä»¥æ­ç¤ºè®­ç»ƒæœ‰ç´ LLMçš„æ¨ç†åè§ã€‚GPT-4oç­‰å…ˆè¿›æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º79.54%ï¼Œè¡¨ç°å‡ºå¤„ç†é€»è¾‘åˆç†ä½†ä¿¡å¿µå†²çªçš„è¾“å…¥æ—¶çš„å…³é”®å¼±ç‚¹ã€‚è¿™äº›å‘ç°å¯¹åœ¨éœ€è¦ç¡®ä¿çœŸå®æ€§å’Œå®‰å…¨æ€§çš„é«˜é£é™©é¢†åŸŸéƒ¨ç½²LLMå…·æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BIS Reasoning 1.0æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ—¥è¯­ä¸‹çš„é€»è¾‘æ¨ç†è§£é¢˜æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨ä¿¡å¿µä¸ä¸€è‡´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ•°æ®é›†å¼•å…¥é€»è¾‘ä¸Šåˆç†ä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é¢˜ï¼Œä»¥æ­ç¤ºLLMçš„æ¨ç†åè§ã€‚</li>
<li>å…ˆè¿›æ¨¡å‹å¦‚GPT-4oåœ¨è¯¥æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æœ‰å¾…æé«˜ã€‚</li>
<li>LLMåœ¨å¤„ç†é€»è¾‘åˆç†ä½†ä¿¡å¿µå†²çªçš„è¾“å…¥æ—¶å­˜åœ¨å…³é”®å¼±ç‚¹ã€‚</li>
<li>BIS Reasoning 1.0çš„å‘ç°å¯¹äºéƒ¨ç½²LLMåœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—å’Œç§‘å­¦æ–‡çŒ®ï¼‰å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>çœŸå®æ€§å’Œå®‰å…¨æ€§åœ¨è¿™äº›é¢†åŸŸè‡³å…³é‡è¦ï¼Œéœ€è¦å…‹æœLLMçš„å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8a76e9f72790fbda8d3296b793dfdf63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b543c9d7889aa49212733805d2e64b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60709b7a434fc511677607cf40483f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c93fbbc6c9d5db6b7f19d9d4902dadf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bfbc449c02ec2664c2d35648488671f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SPARQ-Synthetic-Problem-Generation-for-Reasoning-via-Quality-Diversity-Algorithms"><a href="#SPARQ-Synthetic-Problem-Generation-for-Reasoning-via-Quality-Diversity-Algorithms" class="headerlink" title="SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity   Algorithms"></a>SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity   Algorithms</h2><p><strong>Authors:Alex Havrilla, Edward Hughes, Mikayel Samvelyan, Jacob Abernethy</strong></p>
<p>Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problemâ€™s solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆå·²æˆä¸ºæé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•è¦ä¹ˆæ˜¯å°†å…ˆè¿›çš„å¤§å‹æ¨¡å‹è’¸é¦ä¸ºå°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œè¦ä¹ˆæ˜¯ä½¿ç”¨è‡ªç„¶çœŸå®çš„é—®é¢˜é™ˆè¿°æ¥ä¿è¯é—®é¢˜é™ˆè¿°çš„è´¨é‡ã€‚è¿™é™åˆ¶äº†è¿™äº›æ–¹æ³•åœ¨æ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„é—®é¢˜é¢†åŸŸä¸­çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SPARQï¼šä¸€ç§é€šè¿‡è´¨é‡å¤šæ ·æ€§ç®—æ³•è¿›è¡Œæ¨ç†çš„åˆæˆé—®é¢˜ç”Ÿæˆæ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§ä»…ä½¿ç”¨å•ä¸ªæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°å­¦é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆå¯¹çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¡¡é‡é—®é¢˜çš„è§£å†³ç‡ï¼ˆä½œä¸ºé—®é¢˜éš¾åº¦çš„ä»£ç†ï¼‰æ¥å®ç°ã€‚ä»åŒ…å«7.5Kæ ·æœ¬çš„ç§å­æ•°æ®é›†å¼€å§‹ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡2000ä¸‡å¯¹æ–°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡éš¾åº¦è¿‡æ»¤ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œç„¶ååœ¨å¯¹ç»“æœè¿›è¡Œå¾®è°ƒçš„åŒä¸€å®¶æ¨¡å‹ä¸Šï¼Œç›¸å¯¹æ¨¡å‹æ€§èƒ½æé«˜äº†é«˜è¾¾24%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å‰¥ç¦»ç ”ç©¶ï¼Œæ¢è®¨äº†åˆæˆæ•°æ®çš„æ•°é‡ã€è´¨é‡å’Œå¤šæ ·æ€§å¯¹æ¨¡å‹æ³›åŒ–çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»¥é—®é¢˜éš¾åº¦è¡¡é‡çš„é«˜è´¨é‡æœ‰åŠ©äºæ›´å¥½çš„å†…éƒ¨åˆ†å¸ƒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè™½ç„¶ç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆæ•°æ®å¹¶ä¸å¼ºçƒˆæœ‰åˆ©äºå†…éƒ¨åˆ†å¸ƒæ€§èƒ½ï¼Œä½†å¯¹æ›´å¤šæ ·åŒ–æ•°æ®çš„è¿‡æ»¤æœ‰åŠ©äºæ›´ç¨³å¥çš„OODæ³›åŒ–ã€‚æˆ‘ä»¬è¿˜è¯å®äº†åˆæˆç”Ÿæˆé—®é¢˜ä¸­å­˜åœ¨æ¨¡å‹å’Œæ•°æ®çš„è§„æ¨¡å®šå¾‹ï¼Œè¿™å¯¹ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–å…·æœ‰ç§¯æå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06499v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨ä¸‹çš„åˆæˆæ•°æ®ç”Ÿæˆå·²æˆä¸ºæé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•è¦ä¹ˆæ˜¯å°†å¤§å‹å…ˆè¿›æ¨¡å‹è’¸é¦æˆå°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œè¦ä¹ˆæ˜¯ä½¿ç”¨è‡ªç„¶çœŸå®çš„é—®é¢˜é™ˆè¿°æ¥ä¿è¯é—®é¢˜é™ˆè¿°çš„è´¨é‡ã€‚è¿™é™åˆ¶äº†è¿™äº›æ–¹æ³•åœ¨æ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„é—®é¢˜é¢†åŸŸä¸­çš„å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SPARQæ–¹æ³•ï¼šé€šè¿‡è´¨é‡å¤šæ ·æ€§ç®—æ³•è¿›è¡Œæ¨ç†çš„åˆæˆé—®é¢˜ç”Ÿæˆã€‚SPARQä½¿ç”¨å•ä¸€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°å­¦é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆå¯¹ï¼Œé€šè¿‡æµ‹é‡é—®é¢˜çš„è§£é¢˜ç‡ï¼ˆä½œä¸ºé—®é¢˜éš¾åº¦çš„ä»£ç†ï¼‰æ¥å®ç°ã€‚æˆ‘ä»¬ä»åŒ…å«7.5Kæ ·æœ¬çš„ç§å­æ•°æ®é›†å¼€å§‹ï¼Œç”Ÿæˆäº†è¶…è¿‡2åƒä¸‡ä¸ªæ–°çš„é—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡éš¾åº¦è¿‡æ»¤ç”Ÿæˆçš„æ•°æ®å’Œåœ¨æ­¤åŸºç¡€ä¸Šå¾®è°ƒåŒä¸€æ¨¡å‹ï¼Œå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½é«˜è¾¾24%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†åˆæˆæ•°æ®æ•°é‡ã€è´¨é‡å’Œå¤šæ ·æ€§å¯¹æ¨¡å‹æ³›åŒ–çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»¥éš¾åº¦è¡¡é‡çš„é«˜è´¨é‡æœ‰åŠ©äºæ›´å¥½çš„å†…éƒ¨åˆ†å¸ƒæ€§èƒ½ã€‚åŒæ—¶ï¼Œè™½ç„¶ç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆæ•°æ®å¹¶ä¸å¼ºçƒˆæœ‰åˆ©äºå†…éƒ¨åˆ†å¸ƒæ€§èƒ½ï¼Œä½†ç­›é€‰å‡ºæ›´å¤šæ ·åŒ–çš„æ•°æ®æœ‰åŠ©äºæ›´ç¨³å¥çš„OODæ³›åŒ–ã€‚æˆ‘ä»¬è¿˜è¯å®äº†åˆæˆé—®é¢˜çš„æ¨¡å‹å’Œæ•°æ®çš„æ‰©å±•æ€§è§„å¾‹ç¡®å®å­˜åœ¨ï¼Œè¿™å¯¹ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–æœ‰ç§¯æå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆæ•°æ®ç”Ÿæˆèƒ½å¤Ÿæé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤§å¤šæ•°æ–¹æ³•é¢ä¸´éš¾ä»¥æ‰©å±•è‡³æ›´å¤æ‚ã€å¤šæ ·åŒ–é—®é¢˜é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>SPARQæ–¹æ³•é€šè¿‡è´¨é‡å¤šæ ·æ€§ç®—æ³•ç”Ÿæˆåˆæˆæ•°å­¦é—®é¢˜åŠè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä½¿ç”¨å•ä¸€æ¨¡å‹ç”Ÿæˆè¶…è¿‡2åƒä¸‡ä¸ªé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆå¯¹ã€‚</li>
<li>é€šè¿‡éš¾åº¦è¿‡æ»¤ç”Ÿæˆæ•°æ®å¹¶å¾®è°ƒæ¨¡å‹ï¼Œå¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é«˜è´¨é‡æ•°æ®æœ‰åŠ©äºæ¨¡å‹åœ¨å†…éƒ¨åˆ†å¸ƒä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c25ec2b7d067d315f452dfa0929d63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-587bdcef093af6cb2798ecf3d6f53d9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1eaab322d73d6074372613340bd9438.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ea36be4a3ac09084ad64ef9ed81faf4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AgentCPM-GUI-Building-Mobile-Use-Agents-with-Reinforcement-Fine-Tuning"><a href="#AgentCPM-GUI-Building-Mobile-Use-Agents-with-Reinforcement-Fine-Tuning" class="headerlink" title="AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning"></a>AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning</h2><p><strong>Authors:Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming Liu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, Maosong Sun</strong></p>
<p>The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9%$ Type-Match and $91.3%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†äººçš„è¿›å±•ä¸ºé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–ä»»åŠ¡å¼€å¯äº†æ–°çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§»åŠ¨ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½äº¤äº’å¯ä»¥æå¤§åœ°æé«˜å¯ç”¨æ€§ã€‚ç„¶è€Œï¼Œæ­¤ç±»ä»£ç†äººçš„å®é™…éƒ¨ç½²ä»é¢ä¸´ä¸€äº›å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„è®­ç»ƒæ•°æ®é€šå¸¸å˜ˆæ‚ä¸”ç¼ºä¹è¯­ä¹‰å¤šæ ·æ€§ï¼Œè¿™é˜»ç¢äº†ç²¾ç¡®æ¥åœ°å’Œè§„åˆ’çš„å­¦ä¹ ã€‚ä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šå¯¹è§è¿‡çš„ç•Œé¢æ¨¡å¼è¿‡äºé€‚åº”ï¼Œæ— æ³•åœ¨é™Œç”Ÿåœºæ™¯ä¸­æ¨å¹¿ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ—©æœŸçš„å·¥ä½œéƒ½é›†ä¸­åœ¨è‹±è¯­ç•Œé¢ä¸Šï¼Œè€Œå¿½è§†äº†éè‹±è¯­åº”ç”¨ç¨‹åºçš„æ—¥ç›Šå¤šæ ·æ€§ï¼Œå¦‚ä¸­æ–‡ç§»åŠ¨ç”Ÿæ€ç³»ç»Ÿä¸­çš„åº”ç”¨ç¨‹åºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AgentCPM-GUIï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºç¨³å¥é«˜æ•ˆçš„è®¾å¤‡ç«¯GUIäº¤äº’è€Œæ„å»ºçš„8Bå‚æ•°GUIä»£ç†ã€‚æˆ‘ä»¬çš„è®­ç»ƒç®¡é“åŒ…æ‹¬å¢å¼ºæ„ŸçŸ¥çš„æ¥åœ°æ„ŸçŸ¥é¢„è®­ç»ƒã€åœ¨é«˜è´¨é‡ä¸­æ–‡å’Œè‹±æ–‡è½¨è¿¹ä¸Šçš„ç›‘ç£å¾®è°ƒä»¥æ¨¡ä»¿äººç±»è¡ŒåŠ¨ã€ä½¿ç”¨GRPOè¿›è¡Œå¼ºåŒ–å¾®è°ƒä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç´§å‡‘çš„åŠ¨ä½œç©ºé—´ï¼Œä»¥å‡å°‘è¾“å‡ºé•¿åº¦å¹¶æ”¯æŒåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„ä½å»¶è¿Ÿæ‰§è¡Œã€‚AgentCPM-GUIåœ¨äº”ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªåä¸ºCAGUIçš„æ–°ä¸­æ–‡GUIåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†96.9%çš„ç±»å‹åŒ¹é…åº¦å’Œ91.3%çš„ç²¾ç¡®åŒ¹é…åº¦ã€‚ä¸ºäº†ä¿ƒè¿›å¯å¤åˆ¶æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æ‰€æœ‰ä»£ç ã€æ¨¡å‹æ£€æŸ¥ç‚¹å’Œè¯„ä¼°æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01391v2">PDF</a> Updated results in Table 2 and Table 3; The project is available at   <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/AgentCPM-GUI">https://github.com/OpenBMB/AgentCPM-GUI</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§»åŠ¨ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œå…¶å®è·µéƒ¨ç½²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒæ•°æ®å™ªå£°å’Œè¯­ä¹‰å¤šæ ·æ€§ä¸è¶³ã€æ¨¡å‹å¯¹ç•Œé¢æ¨¡å¼çš„è¿‡åº¦æ‹Ÿåˆä»¥åŠç¼ºä¹æ³›åŒ–èƒ½åŠ›ç­‰é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AgentCPM-GUIï¼Œä¸€ä¸ªä¸ºç¨³å¥å’Œé«˜æ•ˆçš„è®¾å¤‡ç«¯GUIäº¤äº’è€Œæ„å»ºçš„8Bå‚æ•°GUIä»£ç†ã€‚å…¶è®­ç»ƒç®¡é“åŒ…æ‹¬å¢å¼ºæ„ŸçŸ¥çš„æ¥åœ°é¢„è®­ç»ƒã€åœ¨é«˜è´¨é‡ä¸­è‹±æ–‡è½¨è¿¹ä¸Šçš„ç›‘ç£å¾®è°ƒä»¥æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œä»¥åŠä½¿ç”¨GRPOè¿›è¡Œå¼ºåŒ–å¾®è°ƒä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªç´§å‡‘çš„åŠ¨ä½œç©ºé—´ï¼Œä»¥å‡å°‘è¾“å‡ºé•¿åº¦å¹¶æ”¯æŒåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„ä½å»¶è¿Ÿæ‰§è¡Œã€‚AgentCPM-GUIåœ¨äº”ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªåä¸ºCAGUIçš„æ–°ä¸­æ–‡GUIåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°äº†96.9%çš„ç±»å‹åŒ¹é…å’Œ91.3%çš„ç²¾ç¡®åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨GUIè‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶åœ¨ç§»åŠ¨ç¯å¢ƒä¸‹ã€‚</li>
<li>å½“å‰å®è·µéƒ¨ç½²é¢ä¸´è®­ç»ƒæ•°æ®å™ªå£°ã€è¯­ä¹‰å¤šæ ·æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚</li>
<li>AgentCPM-GUIé€šè¿‡æ¥åœ°é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–å¾®è°ƒç­‰æ–¹æ³•åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>ç´§å‡‘çš„åŠ¨ä½œç©ºé—´æ”¯æŒç§»åŠ¨è®¾å¤‡çš„ä½å»¶è¿Ÿæ‰§è¡Œã€‚</li>
<li>AgentCPM-GUIåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–°çš„ä¸­æ–‡GUIåŸºå‡†æµ‹è¯•CAGUIã€‚</li>
<li>ç ”ç©¶äººå‘˜å…¬å¼€äº†ä»£ç ã€æ¨¡å‹æ£€æŸ¥ç‚¹å’Œè¯„ä¼°æ•°æ®ä»¥ä¿ƒè¿›ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d06484877d26fc68dad8f65965dca89c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c47b7cbe943ef4a581ad265d3e029ec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcfb9d048b07036eb312a37c6e84c230.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="REAL-Prover-Retrieval-Augmented-Lean-Prover-for-Mathematical-Reasoning"><a href="#REAL-Prover-Retrieval-Augmented-Lean-Prover-for-Mathematical-Reasoning" class="headerlink" title="REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning"></a>REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</h2><p><strong>Authors:Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong</strong></p>
<p>Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64). </p>
<blockquote>
<p>å¦‚ä»Šï¼Œå½¢å¼åŒ–å®šç†è¯æ˜å™¨åœ¨é«˜ä¸­å’Œç«èµ›çº§åˆ«çš„æ•°å­¦æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†å¾ˆå°‘æœ‰èƒ½å¤Ÿæ¨å¹¿åˆ°æ›´é«˜çº§æ•°å­¦çš„è¯æ˜å™¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†REAL-Proverï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼€æºé€æ­¥å®šç†è¯æ˜å™¨ï¼Œé€‚ç”¨äºLean 4ï¼Œæ—¨åœ¨çªç ´è¿™ä¸€ç•Œé™ã€‚è¯¥è¯æ˜å™¨åŸºäºæˆ‘ä»¬ç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆREAL-Prover-v1ï¼‰å¹¶ä¸æ£€ç´¢ç³»ç»Ÿï¼ˆLeansearch-PSï¼‰é›†æˆï¼Œåœ¨è§£å†³å¤§å­¦çº§åˆ«æ•°å­¦é—®é¢˜æ–¹é¢çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä¸ºäº†è®­ç»ƒREAL-Prover-v1ï¼Œæˆ‘ä»¬å¼€å‘äº†HERALD-AFï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®æå–ç®¡é“ï¼Œå¯å°†è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜è½¬æ¢ä¸ºæ­£å¼è¯­å¥ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„å¼€æºLean 4äº¤äº’å¼ç¯å¢ƒï¼ˆJixia-interactiveï¼‰ï¼Œä»¥ä¿ƒè¿›åˆæˆæ•°æ®æ”¶é›†ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œä»…ä½¿ç”¨ç›‘ç£å¾®è°ƒè¯æ˜å™¨åœ¨ProofNetæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ŒæˆåŠŸç‡ä¸º23.7%ï¼ˆPass@64ï¼‰ï¼Œä¸æœ€æ–°ï¼ˆSOTAï¼‰æ¨¡å‹ç›¸å½“ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†FATE-Mï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºä»£æ•°é—®é¢˜çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬çš„è¯æ˜å™¨åœ¨é‚£é‡Œè¾¾åˆ°äº†SOTAçš„æˆåŠŸç‡56.7%ï¼ˆPass@64ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20613v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€æ¬¾åä¸ºREAL-Proverçš„æ–°å‹å¼€æºé€æ­¥å®šç†è¯æ˜å™¨ï¼Œå®ƒåŸºäºç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿè§£å†³å¤§å­¦çº§åˆ«çš„æ•°å­¦é—®é¢˜ã€‚é€šè¿‡å¼€å‘HERALD-AFæ•°æ®æå–ç®¡é“å’Œè‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜è½¬åŒ–ä¸ºæ­£å¼è¯­å¥çš„Jixia-interactiveäº¤äº’ç¯å¢ƒï¼Œè®­ç»ƒäº†REAL-Prover-v1ã€‚åœ¨ProofNetæ•°æ®é›†ä¸Šï¼ŒREAL-Proverå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ŒæˆåŠŸç‡ä¸º23.7%ï¼ˆPass@64ï¼‰ï¼Œå¹¶ä¸”åœ¨ä¸“æ³¨äºä»£æ•°é—®é¢˜çš„æ–°åŸºå‡†æµ‹è¯•FATE-Mä¸Šå–å¾—äº†æœ€é«˜æˆåŠŸç‡56.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REAL-Proveræ˜¯ä¸€ä¸ªé’ˆå¯¹Lean 4çš„æ–°å‹å¼€æºé€æ­¥å®šç†è¯æ˜å™¨ï¼Œæ—¨åœ¨è§£å†³æ›´é«˜çº§çš„æ•°å­¦é—®é¢˜ã€‚</li>
<li>REAL-ProveråŸºäºç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆREAL-Prover-v1ï¼‰å’Œé›†æˆæ£€ç´¢ç³»ç»Ÿï¼ˆLeansearch-PSï¼‰ã€‚</li>
<li>HERALD-AFæ•°æ®æå–ç®¡é“èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜è½¬åŒ–ä¸ºæ­£å¼è¯­å¥ï¼Œç”¨äºè®­ç»ƒREAL-Prover-v1ã€‚</li>
<li>Jixia-interactiveæ˜¯ä¸€ä¸ªæ–°å¼€çš„äº¤äº’å¼ç¯å¢ƒï¼Œç”¨äºä¿ƒè¿›åˆæˆæ•°æ®çš„æ”¶é›†ï¼Œè¾…åŠ©REAL-Proverçš„å¼€å‘ã€‚</li>
<li>REAL-Proveråœ¨ProofNetæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ŒæˆåŠŸç‡ä¸º23.7%ã€‚</li>
<li>FATE-Mæ˜¯ä¸€ä¸ªæ–°çš„ä¸“æ³¨äºä»£æ•°é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼ŒREAL-Proveråœ¨æ­¤æµ‹è¯•ä¸Šçš„æˆåŠŸç‡è¾¾åˆ°äº†56.7%ï¼Œè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfd669546f55cea0fd83c34f42d13569.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5104331159589baf182909b864e7dd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec138ed1b2b89a27e6366f99f3ae6182.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GreekBarBench-A-Challenging-Benchmark-for-Free-Text-Legal-Reasoning-and-Citations"><a href="#GreekBarBench-A-Challenging-Benchmark-for-Free-Text-Legal-Reasoning-and-Citations" class="headerlink" title="GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and   Citations"></a>GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and   Citations</h2><p><strong>Authors:Odysseas S. Chlapanis, Dimitrios Galanis, Nikolaos Aletras, Ion Androutsopoulos</strong></p>
<p>We introduce GreekBarBench, a benchmark that evaluates LLMs on legal questions across five different legal areas from the Greek Bar exams, requiring citations to statutory articles and case facts. To tackle the challenges of free-text evaluation, we propose a three-dimensional scoring system combined with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to assess the correlation between LLM-judges and human expert evaluations, revealing that simple, span-based rubrics improve their alignment. Our systematic evaluation of 13 proprietary and open-weight LLMs shows that even though the best models outperform average expert scores, they fall short of the 95th percentile of experts. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GreekBarBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¸Œè…Šå¾‹å¸ˆè€ƒè¯•ä¸­çš„äº”ä¸ªä¸åŒæ³•å¾‹é¢†åŸŸçš„æ³•å¾‹é—®é¢˜ã€‚è¿™éœ€è¦å¼•ç”¨æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹äº‹å®ã€‚ä¸ºäº†åº”å¯¹æ–‡æœ¬è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰ç»´è¯„åˆ†ç³»ç»Ÿï¼Œç»“åˆä»¥LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå…ƒè¯„ä¼°åŸºå‡†ï¼Œä»¥è¯„ä¼°LLMæ³•å®˜å’Œäººç±»ä¸“å®¶è¯„ä»·ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè¡¨æ˜ç®€å•çš„åŸºäºè·¨åº¦çš„æ–¹æ³•å¯ä»¥æ”¹å–„å…¶å¯¹é½ã€‚æˆ‘ä»¬å¯¹13ä¸ªä¸“æœ‰å’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡æœ€ä½³æ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†å¹³å‡ä¸“å®¶å¾—åˆ†ï¼Œä½†å®ƒä»¬ä»ä½äºä¸“å®¶å¾—åˆ†çš„ç¬¬95ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17267v2">PDF</a> 19 pages, 17 figures, submitted to May ARR</p>
<p><strong>Summary</strong></p>
<p>å¸Œè…ŠBarBenchåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œæ¶‰åŠå¸Œè…Šå¾‹å¸ˆè€ƒè¯•ä¸­çš„äº”ä¸ªä¸åŒæ³•å¾‹é¢†åŸŸï¼Œè¦æ±‚å¼•ç”¨æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹äº‹å®ã€‚ä¸ºè§£å†³è‡ªç”±æ–‡æœ¬è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰ç»´è¯„åˆ†ç³»ç»Ÿï¼Œç»“åˆLLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå…ƒè¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°LLMæ³•å®˜ä¸äººç±»ä¸“å®¶è¯„ä¼°ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‘ç°ç®€å•çš„åŸºäºèŒƒå›´çš„è¯„åˆ†è§„åˆ™æœ‰åŠ©äºæé«˜ä¸€è‡´æ€§ã€‚å¯¹13ä¸ªä¸“æœ‰å’Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡æœ€ä½³æ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†å¹³å‡ä¸“å®¶å¾—åˆ†ï¼Œä½†ä»æœªè¾¾åˆ°ä¸“å®¶å¾—åˆ†çš„ç¬¬95ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸Œè…ŠBarBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ³•å¾‹é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æµ‹è¯•æ¶µç›–å¸Œè…Šå¾‹å¸ˆè€ƒè¯•ä¸­çš„äº”ä¸ªä¸åŒæ³•å¾‹é¢†åŸŸï¼Œè¦æ±‚å¼•ç”¨æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹äº‹å®ã€‚</li>
<li>é‡‡ç”¨ä¸‰ç»´è¯„åˆ†ç³»ç»Ÿä¸LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•æ¥è§£å†³è‡ªç”±æ–‡æœ¬è¯„ä¼°çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼€å‘äº†å…ƒè¯„ä¼°åŸºå‡†æµ‹è¯•ä»¥è¯„ä¼°LLMæ³•å®˜ä¸äººç±»ä¸“å®¶è¯„ä¼°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li>
<li>ç®€å•çš„åŸºäºèŒƒå›´çš„è¯„åˆ†è§„åˆ™èƒ½å¤Ÿæé«˜LLMæ³•å®˜ä¸äººç±»è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†13ä¸ªLLMsçš„è¡¨ç°ï¼Œå‘ç°æœ€ä½³æ¨¡å‹çš„è¡¨ç°è™½è¶…è¿‡å¹³å‡ä¸“å®¶å¾—åˆ†ï¼Œä½†æœªè¾¾ä¸“å®¶å¾—åˆ†çš„ç¬¬95ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d1c47d9eac29e6f450c4a7fc7e731ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-730c80753418a16fb212bb841fd6dc7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae9b59682fd4c9626e9c79880ab1a804.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4157ac03dbcddb406104754858ba19d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b11c567455b8481f9f5bd8bfd1c5202.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Fractured-Chain-of-Thought-Reasoning"><a href="#Fractured-Chain-of-Thought-Reasoning" class="headerlink" title="Fractured Chain-of-Thought Reasoning"></a>Fractured Chain-of-Thought Reasoning</h2><p><strong>Authors:Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong</strong></p>
<p>Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/BaohaoLiao/frac-cot">https://github.com/BaohaoLiao/frac-cot</a>. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯é€šè¿‡åˆ©ç”¨æ¨ç†æ—¶çš„é¢å¤–è®¡ç®—åŠªåŠ›ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ ·ï¼Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåŠå…¶æ‰©å±•â€”â€”é•¿æ€ç»´é“¾ï¼ˆLong CoTï¼‰ï¼Œé€šè¿‡ç”Ÿæˆä¸°å¯Œçš„ä¸­é—´æ¨ç†è½¨è¿¹æ¥æé«˜å‡†ç¡®æ€§ï¼Œä½†è¿™äº›æ–¹æ³•äº§ç”Ÿäº†å·¨å¤§çš„ä»¤ç‰Œæˆæœ¬ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨å»¶è¿Ÿæ•æ„Ÿç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†æˆªæ–­æ€ç»´é“¾ï¼ˆtruncated CoTï¼‰ï¼Œå®ƒåœ¨å®Œæˆæ¨ç†ä¹‹å‰åœæ­¢æ¨ç†ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œä¸ä½¿ç”¨å¤§é‡ä»¤ç‰Œçš„å®Œæ•´æ€ç»´é“¾é‡‡æ ·ç›¸æ¯”æ•ˆæœå¾€å¾€ä¸ç›¸ä¸Šä¸‹ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†è£‚é‡‡æ ·ï¼ˆFractured Samplingï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„æ¨ç†æ—¶é—´ç­–ç•¥ï¼Œåœ¨ä¸‰ä¸ªæ­£äº¤è½´ä¸Šå¯¹å®Œæ•´æ€ç»´é“¾å’Œä»…è§£å†³æ–¹æ¡ˆé‡‡æ ·è¿›è¡Œæ’å€¼ï¼šï¼ˆ1ï¼‰æ¨ç†è½¨è¿¹çš„æ•°é‡ï¼Œï¼ˆ2ï¼‰æ¯æ¡è½¨è¿¹çš„æœ€ç»ˆè§£å†³æ–¹æ¡ˆæ•°é‡ï¼Œï¼ˆ3ï¼‰æ¨ç†è½¨è¿¹è¢«æˆªæ–­çš„æ·±åº¦ã€‚é€šè¿‡äº”é¡¹ä¸åŒçš„æ¨ç†åŸºå‡†æµ‹è¯•å’Œå¤šç§æ¨¡å‹è§„æ¨¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ†è£‚é‡‡æ ·åœ¨å‡†ç¡®æ€§æˆæœ¬æƒè¡¡æ–¹é¢è¡¨ç°å“è¶Šï¼Œåœ¨Pass@kä¸ä»¤ç‰Œé¢„ç®—æ–¹é¢å®ç°äº†é™¡å³­çš„å¯¹æ•°çº¿æ€§ç¼©æ”¾æ”¶ç›Šã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å¦‚ä½•åœ¨è¿™äº›ç»´åº¦ä¸Šåˆ†é…è®¡ç®—ä»¥æœ€å¤§åŒ–æ€§èƒ½ï¼Œä¸ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„LLMæ¨ç†é“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BaohaoLiao/frac-cot%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BaohaoLiao/frac-cotæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12992v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä¸­ä»‹ç»äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•é€šè¿‡é‡‡ç”¨æˆªæ–­å¼é“¾å¼æ€ç»´ï¼ˆTruncated CoTï¼‰å’Œåˆ†è£‚é‡‡æ ·ï¼ˆFractured Samplingï¼‰ç­–ç•¥æ¥ä¼˜åŒ–æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨åˆç†æˆªæ–­é“¾å¼æ€ç»´åï¼Œå¯ç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå¹¶è¾¾åˆ°å…¨é“¾å¼æ€ç»´é‡‡æ ·çš„å‡†ç¡®åº¦ã€‚è€Œåˆ†è£‚é‡‡æ ·æ˜¯ä¸€ç§åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ ¹æ®ä¸åŒå‚æ•°é€‰æ‹©çš„ä¸åŒç­–ç•¥ï¼ŒåŒ…æ‹¬æ¨ç†è½¨è¿¹æ•°é‡ã€æ¯ä¸ªè½¨è¿¹çš„æœ€ç»ˆè§£å†³æ–¹æ¡ˆæ•°é‡å’Œæˆªæ–­æ¨ç†æ·±åº¦çš„å¹³è¡¡ã€‚åœ¨äº”ä¸ªä¸åŒçš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œåˆ†è£‚é‡‡æ ·åœ¨å‡†ç¡®åº¦ä¸æˆæœ¬ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œå®ç°äº†åœ¨Pass@kä¸Šçš„å¯¹æ•°çº¿æ€§ç¼©æ”¾å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯åˆ©ç”¨é¢å¤–çš„è®¡ç®—èµ„æºã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åŠå…¶æ‰©å±•é•¿é“¾å¼æ€ç»´ï¼ˆLong CoTï¼‰é€šè¿‡ç”Ÿæˆä¸°å¯Œçš„ä¸­é—´æ¨ç†è½¨è¿¹æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†å¢åŠ äº†ä»¤ç‰Œæˆæœ¬ï¼Œä¸é€‚ç”¨äºå»¶è¿Ÿæ•æ„Ÿçš„ç¯å¢ƒã€‚</li>
<li>æˆªæ–­å¼é“¾å¼æ€ç»´ï¼ˆTruncated CoTï¼‰åœ¨å‡å°‘ä»¤ç‰Œä½¿ç”¨çš„åŒæ—¶ï¼Œèƒ½å¤ŸåŒ¹é…å…¨é“¾å¼æ€ç»´é‡‡æ ·çš„å‡†ç¡®åº¦ã€‚</li>
<li>åˆ†è£‚é‡‡æ ·æ˜¯ä¸€ç§ç»Ÿä¸€çš„æ¨ç†æ—¶é—´ç­–ç•¥ï¼Œé€šè¿‡å¹³è¡¡æ¨ç†è½¨è¿¹æ•°é‡ã€æ¯ä¸ªè½¨è¿¹çš„æœ€ç»ˆè§£å†³æ–¹æ¡ˆæ•°é‡å’Œæˆªæ–­æ¨ç†çš„æ·±åº¦ï¼Œå®ç°å‡†ç¡®æ€§ä¸æˆæœ¬ä¹‹é—´çš„ä¼˜åŒ–ã€‚</li>
<li>åˆ†è£‚é‡‡æ ·åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†åœ¨Pass@kä¸Šçš„å¯¹æ•°çº¿æ€§ç¼©æ”¾å¢ç›Šã€‚</li>
<li>é€šè¿‡å®éªŒåˆ†æï¼Œç¡®å®šäº†å¦‚ä½•åœ¨è¿™äº›ç»´åº¦ä¸Šåˆ†é…è®¡ç®—ä»¥æœ€å¤§åŒ–æ€§èƒ½çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97e0d4b25f023fc8ec980bc7954942dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21beba6ce8e3fe2450148bd5a2d289bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-194a1e84adf5383eb30a290340c3ff29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71402f61fb1611b8e9f7a21d284ebae6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Vision-Language-Action-Models-in-Procedurally-Generated-Open-Ended-Action-Environments"><a href="#Benchmarking-Vision-Language-Action-Models-in-Procedurally-Generated-Open-Ended-Action-Environments" class="headerlink" title="Benchmarking Vision, Language, &amp; Action Models in Procedurally   Generated, Open Ended Action Environments"></a>Benchmarking Vision, Language, &amp; Action Models in Procedurally   Generated, Open Ended Action Environments</h2><p><strong>Authors:Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Harshvardhan Sikka, Paul Pu Liang</strong></p>
<p>Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLMs and VLAs - including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST - on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity; (2) VLAs generally outperforms other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering. We release our benchmark, evaluation framework, and findings to enable the assessment of future VLA models and identify critical areas for improvement in their application to out-of-distribution digital tasks. </p>
<blockquote>
<p>è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œï¼Œæœç€é€šç”¨æœºå™¨äººç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚ç„¶è€Œï¼Œå¯¹è¿™äº›æ¨¡å‹çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨‹åºå¤–åˆ†å¸ƒï¼ˆOODï¼‰ç¯å¢ƒä¸­é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„è¯„ä¼°ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MultiNet v0.2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å’Œåˆ†ææœ€å…ˆè¿›çš„VLMå’ŒVLAæ³›åŒ–æ€§èƒ½çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPT-4oã€GPT-4.1ã€OpenVLAã€Pi0 Baseå’ŒPi0 FASTç­‰æ¨¡å‹åœ¨å„ç§ProcgenåŸºå‡†æµ‹è¯•ä¸­çš„å¤šæ ·åŒ–ç¨‹åºä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬æ³›åŒ–åˆ°OODä»»åŠ¡ä¸Šéƒ½å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œæ€§èƒ½å—åˆ°åŠ¨ä½œè¡¨å¾å’Œä»»åŠ¡å¤æ‚æ€§ç­‰å› ç´ çš„å½±å“ï¼›ï¼ˆ2ï¼‰ç”±äºVLAçš„ç¨³å¥æ¶æ„è®¾è®¡ï¼Œå®ƒä»¬é€šå¸¸è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼›ï¼ˆ3ï¼‰å½“å¾—åˆ°é€‚å½“çš„çº¦æŸæ—¶ï¼ŒVLMå˜ä½“è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œè¿™çªå‡ºäº†æ¨¡å‹æ€§èƒ½å¯¹ç²¾ç¡®æç¤ºå·¥ç¨‹çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ã€è¯„ä¼°æ¡†æ¶å’Œå‘ç°ï¼Œä»¥ä¾¿å¯¹å°†æ¥çš„VLAæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç¡®å®šå…¶åœ¨å¤„ç†ç¦»ç¾¤æ•°å­—ä»»åŠ¡æ—¶çš„å…³é”®æ”¹è¿›é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05540v2">PDF</a> 16 pages, 26 figures</p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œæ–¹é¢çš„æ•´åˆä¸­ï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æ˜¯æœç€é€šç”¨æœºå™¨äººç³»ç»Ÿè¿ˆè¿›çš„é‡è¦ä¸€æ­¥ã€‚ç„¶è€Œï¼Œå¯¹æ­¤ç±»æ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨‹åºå¤–çš„åˆ†å¸ƒç¯å¢ƒä¸­çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è¯„ä¼°ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†MultiNet v0.2ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œåˆ†ææœ€å‰æ²¿çš„VLMså’ŒVLAsåœ¨å¤šæ ·åŒ–ç¨‹åºä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚åˆ†ææ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼šVLAæ¨¡å‹åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼›ä¸åŒæ¨¡å‹çš„è¡¨ç°å—åŠ¨ä½œè¡¨å¾å’Œä»»åŠ¡å¤æ‚æ€§ç­‰å› ç´ çš„å½±å“ï¼›é€‚å½“çº¦æŸVLMå˜ä½“å¯æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å‘å¸ƒåŸºå‡†æµ‹è¯•ã€è¯„ä¼°æ¡†æ¶å’Œå‘ç°ï¼Œä»¥ä¾¿è¯„ä¼°æœªæ¥çš„VLAæ¨¡å‹ï¼Œå¹¶ç¡®å®šå…¶åœ¨å¤„ç†å¤–éƒ¨åˆ†å¸ƒæ•°å­—ä»»åŠ¡æ—¶çš„å…³é”®æ”¹è¿›é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹é›†æˆè§†è§‰æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œï¼Œæ˜¯é€šç”¨æœºå™¨äººç³»ç»Ÿçš„é‡è¦è¿›æ­¥ã€‚</li>
<li>ç›®å‰å¯¹VLAæ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨‹åºå¤–çš„åˆ†å¸ƒç¯å¢ƒä¸­çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚</li>
<li>MultiNet v0.2æ˜¯ä¸€ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æœ€å‰æ²¿çš„VLMså’ŒVLAsåœ¨å¤šæ ·åŒ–ç¨‹åºä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>VLAæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–æ½œåŠ›ï¼Œä½†å­˜åœ¨åŠ¨ä½œè¡¨å¾å’Œä»»åŠ¡å¤æ‚æ€§ç­‰å› ç´ çš„å½±å“ã€‚</li>
<li>VLMå˜ä½“çš„æ€§èƒ½å¯ä»¥é€šè¿‡é€‚å½“çš„çº¦æŸæ¥æ˜¾è‘—æé«˜ã€‚</li>
<li>é‡Šæ”¾åŸºå‡†æµ‹è¯•ã€è¯„ä¼°æ¡†æ¶å’Œå‘ç°ï¼Œä»¥ä¾¿æœªæ¥å¯¹VLAæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4b289ae7ef7df6d33ffb9e9b1edfa6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c521c491789be966d58a2733dfa69c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3674b3631ee0013dee1a8771049fb158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f4febcc48286ed90fecc41b2ae209d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMsâ€™ abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMsâ€™ abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git">https://github.com/zli12321/VideoHallu.git</a>. </p>
<blockquote>
<p>è§†é¢‘åˆæˆç”Ÿæˆå› å…¶çœŸå®æ„Ÿå’Œå¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ï¼Œä½†ä»å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„æ¼æ´ã€‚è¿™å¼ºè°ƒäº†éœ€è¦å¯é çš„å¼‚å¸¸æ£€æµ‹å™¨ï¼Œè¿™äº›æ£€æµ‹å™¨éœ€è¦ç†è§£è¿™äº›åŸç†ï¼Œå¹¶èƒ½å¤ŸæŠµæŠ—å¹»è§‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoHalluæ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªç”±è¶…è¿‡3000ä¸ªé—®ç­”å¯¹æ„æˆçš„åŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè§†é¢‘å†…å®¹è¯„ä¼°ã€‚è¿™äº›é—®ç­”å¯¹åŸºäºåˆæˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Veo2ã€Soraå’ŒKlingï¼‰æ„å»ºï¼ŒåŒæ—¶ç»“åˆä¸“å®¶è®¾è®¡çš„åç›´è§‰é—®ç­”ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥æ˜æ˜¾ä½†å¯¹äººç±»æ¥è¯´å¸¸å¸¸å› è¯­è¨€å…ˆéªŒçŸ¥è¯†è€Œäº§ç”Ÿå¹»è§‰çš„å¼‚å¸¸ç°è±¡ä¸Šçš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ã€‚VideoHallué€šè¿‡æ¶µç›–å¯¹é½ã€ä¸€è‡´æ€§ã€å¸¸è¯†å’Œç‰©ç†å­¦çš„ä¾‹å­æ¥è¯„ä¼°MLLMsçš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬GPT-4oã€Gemini-2.5-Proã€Qwen2.5-VLåœ¨å†…çš„å…ˆè¿›MLLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è§‚å¯Ÿåˆ°è¿™äº›æ¨¡å‹åœ¨è¯¸å¦‚MVBenchå’ŒMovieChatç­‰çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘çš„åŸºäºç‰©ç†å’Œå¸¸è¯†çš„æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä½¿ç”¨ç»„åˆè§†é¢‘é—®ç­”å’Œåç›´è§‰å¸¸è¯†ä¸ç‰©ç†æ¨ç†çš„æ•°æ®é›†è¿›è¡Œè¯¾ç¨‹å­¦ä¹ çš„åè®­ç»ƒï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•å¯ä»¥æ”¹å–„MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè¯æ˜æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒå¯¹æé«˜å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git">https://github.com/zli12321/VideoHallu.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹åˆæˆè§†é¢‘ç”Ÿæˆé¢†åŸŸå­˜åœ¨çš„ç°å®é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºVideoHalluåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡3000ä¸ªè§†é¢‘é—®ç­”å¯¹ã€‚è¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äººç±»æ˜æ˜¾æ„ŸçŸ¥ä½†å¸¸å› è¯­è¨€å…ˆéªŒè€Œäº§ç”Ÿå¹»è§‰çš„å¼‚å¸¸ç°è±¡çš„ç†è§£ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘ä¸­çš„åŸºæœ¬ç‰©ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚é€šè¿‡é’ˆå¯¹è§†é¢‘é—®ç­”ä¸å…·æœ‰æŒ‘æˆ˜æ€§çš„å¸¸è¯†å’Œç‰©ç†æ¨ç†æ•°æ®é›†è¿›è¡Œåè®­ç»ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆæˆè§†é¢‘ç”Ÿæˆå¼•å‘å¯¹çœŸå®æ€§å’Œå¹¿æ³›åº”ç”¨çš„å…´è¶£ï¼Œä½†ä»å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ã€‚</li>
<li>æ¨å‡ºVideoHalluåŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹Ÿäººç±»æ˜æ˜¾æ„ŸçŸ¥ä½†å¸¸å› è¯­è¨€å…ˆéªŒè€Œäº§ç”Ÿå¹»è§‰çš„å¼‚å¸¸ç°è±¡ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘ä¸­çš„ç‰©ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨Group Relative Policy Optimization (GRPO)å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£ã€‚</li>
<li>VideoHalluæä¾›äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå¯è¡¡é‡æ¨¡å‹åœ¨ç†è§£åˆæˆè§†é¢‘ä¸­çš„å¼‚å¸¸ç°è±¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å…¬å¼€å¯ç”¨ä»£ç ä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›æä¾›äº†æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0733746e310ac120e88dee7a0bb4e33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c0e565c33c36de724192494cf59dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd791c7ebf56cd412d8edaf50d4d8b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c07288c94070b78a2e6183c618ed95a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4f30a27d12edd17a705e7e98a27eba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eba42bf105f2f69700259d4c77c4224.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SOPBench-Evaluating-Language-Agents-at-Following-Standard-Operating-Procedures-and-Constraints"><a href="#SOPBench-Evaluating-Language-Agents-at-Following-Standard-Operating-Procedures-and-Constraints" class="headerlink" title="SOPBench: Evaluating Language Agents at Following Standard Operating   Procedures and Constraints"></a>SOPBench: Evaluating Language Agents at Following Standard Operating   Procedures and Constraints</h2><p><strong>Authors:Zekun Li, Shinda Huang, Jiangtian Wang, Nathan Zhang, Antonis Antoniades, Wenyue Hua, Kaijie Zhu, Sirui Zeng, Chi Wang, William Yang Wang, Xifeng Yan</strong></p>
<p>As language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline SOPBench with: (1) executable environments containing 167 tools&#x2F;functions across seven customer service domains with service-specific SOPs and rule-based verifiers, (2) an automated test generation framework producing over 900 verified test cases, and (3) an automated evaluation framework to rigorously assess agent adherence from multiple dimensions. Our approach transforms each service-specific SOP code program into a directed graph of executable functions and requires agents to call these functions based on natural language SOP descriptions. The original code serves as oracle rule-based verifiers to assess compliance, reducing reliance on manual annotations and LLM-based evaluations. We evaluate 18 leading models, and results show the task is challenging even for top-tier models (like GPT-4o, Claude-3.7-Sonnet), with variances across domains. Reasoning models like o4-mini-high show superiority while other powerful models perform less effectively (pass rates of 30%-50%), and small models (7B, 8B) perform significantly worse. Additionally, language agents can be easily jailbroken to overlook SOPs and constraints. Code, data, and over 24k agent trajectories are released at <a target="_blank" rel="noopener" href="https://github.com/Leezekun/SOPBench">https://github.com/Leezekun/SOPBench</a>. </p>
<blockquote>
<p>éšç€è¯­è¨€ä»£ç†äººåœ¨å…³é”®ä»»åŠ¡ä¸Šçš„è‡ªåŠ¨åŒ–ç¨‹åº¦è¶Šæ¥è¶Šé«˜ï¼Œä»–ä»¬åœ¨è¡ŒåŠ¨å’Œå·¥å…·è°ƒç”¨æ—¶éµå¾ªç‰¹å®šé¢†åŸŸçš„æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ã€æ”¿ç­–å’Œçº¦æŸçš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ï¼Œä½†è¿™æ–¹é¢ä»ç„¶æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“SOPBenchï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å¯æ‰§è¡Œç¯å¢ƒï¼ŒåŒ…å«ä¸ƒä¸ªå®¢æˆ·æœåŠ¡é¢†åŸŸçš„167ä¸ªå·¥å…·&#x2F;åŠŸèƒ½ï¼Œä»¥åŠé’ˆå¯¹ç‰¹å®šæœåŠ¡çš„SOPså’ŒåŸºäºè§„åˆ™çš„éªŒè¯å™¨ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆè¶…è¿‡900ä¸ªç»è¿‡éªŒè¯çš„æµ‹è¯•ç”¨ä¾‹ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦ä¸¥æ ¼è¯„ä¼°ä»£ç†äººéµå¾ªæƒ…å†µã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¯é¡¹ç‰¹å®šæœåŠ¡çš„SOPä»£ç ç¨‹åºè½¬æ¢ä¸ºå¯æ‰§è¡Œå‡½æ•°çš„å®šå‘å›¾ï¼Œå¹¶è¦æ±‚ä»£ç†äººæ ¹æ®è‡ªç„¶è¯­è¨€SOPæè¿°è°ƒç”¨è¿™äº›å‡½æ•°ã€‚åŸå§‹ä»£ç ä½œä¸ºåŸºäºè§„åˆ™çš„éªŒè¯å™¨æ¥è¯„ä¼°åˆè§„æ€§ï¼Œå‡å°‘äº†å¯¹æ‰‹åŠ¨æ³¨é‡Šå’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°çš„ä¾èµ–ã€‚æˆ‘ä»¬è¯„ä¼°äº†18æ¬¾é¢†å…ˆæ¨¡å‹ï¼Œç»“æœè¡¨æ˜ï¼Œå³ä½¿å¯¹äºé¡¶å°–æ¨¡å‹ï¼ˆå¦‚GPT-4oã€Claude-3.7-Sonnetï¼‰ï¼Œæ­¤ä»»åŠ¡ä¹Ÿæå…·æŒ‘æˆ˜æ€§ï¼Œä¸åŒé¢†åŸŸä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œåƒo4-mini-highè¿™æ ·çš„æ¨ç†æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§ï¼Œè€Œå…¶ä»–å¼ºå¤§æ¨¡å‹çš„æ•ˆæœè¾ƒå·®ï¼ˆé€šè¿‡ç‡ä¸º30%-50%ï¼‰ï¼Œå°å‹æ¨¡å‹ï¼ˆ7Bã€8Bï¼‰çš„è¡¨ç°åˆ™æ›´å·®ã€‚æ­¤å¤–ï¼Œè¯­è¨€ä»£ç†äººå¾ˆå®¹æ˜“è¢«ç ´è§£è€Œå¿½è§†SOPså’Œçº¦æŸã€‚ä»£ç ã€æ•°æ®å’Œè¶…è¿‡24kçš„ä»£ç†äººè½¨è¿¹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Leezekun/SOPBench%E3%80%82">https://github.com/Leezekun/SOPBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08669v2">PDF</a> Code, data, and over 24k agent trajectories are released at   <a target="_blank" rel="noopener" href="https://github.com/Leezekun/SOPBench">https://github.com/Leezekun/SOPBench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨è¯­è¨€ä»£ç†åœ¨éµå¾ªé¢†åŸŸç‰¹å®šæ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ã€æ”¿ç­–å’Œçº¦æŸæ–¹é¢çš„èƒ½åŠ›çš„é‡è¦æ€§åŠå…¶è¢«å¿½è§†çš„ç°è±¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹SOPBenchï¼ŒåŒ…æ‹¬å¯æ‰§è¡Œç¯å¢ƒã€è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆæ¡†æ¶å’Œè¯„ä¼°æ¡†æ¶ã€‚SOPBenchèƒ½å°†ç‰¹å®šæœåŠ¡çš„SOPä»£ç è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠŸèƒ½çš„å¯¼å‘å›¾ï¼Œå¹¶è¦æ±‚ä»£ç†æ ¹æ®è‡ªç„¶è¯­è¨€SOPæè¿°è°ƒç”¨è¿™äº›åŠŸèƒ½ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯é¡¶å°–æ¨¡å‹ï¼ˆå¦‚GPT-4oã€Claude-3.7-Sonnetï¼‰åœ¨è¯¥ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°æœ‰æŒ‘æˆ˜ï¼Œä¸åŒé¢†åŸŸé—´å­˜åœ¨å·®å¼‚ã€‚æ¨ç†æ¨¡å‹å¦‚o4-mini-highè¡¨ç°ä¼˜è¶Šï¼Œè€Œå…¶ä»–å¼ºå¤§æ¨¡å‹æ•ˆæœè¾ƒå·®ï¼ˆé€šè¿‡ç‡30%-50%ï¼‰ï¼Œå°å‹æ¨¡å‹è¡¨ç°æ›´å·®ã€‚æ­¤å¤–ï¼Œè¯­è¨€ä»£ç†å®¹æ˜“è¢«ç ´è§£ï¼Œå¿½è§†SOPså’Œçº¦æŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€ä»£ç†åœ¨è‡ªåŠ¨åŒ–å…³é”®ä»»åŠ¡æ—¶ï¼Œéµå¾ªç‰¹å®šé¢†åŸŸçš„æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ã€æ”¿ç­–å’Œçº¦æŸçš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>SOPBenchæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ï¼ŒåŒ…å«å¯æ‰§è¡Œç¯å¢ƒã€è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆæ¡†æ¶å’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€ä»£ç†çš„éµå¾ªèƒ½åŠ›ã€‚</li>
<li>SOPBenchèƒ½å°†æœåŠ¡ç‰¹å®šSOPä»£ç è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠŸèƒ½çš„å¯¼å‘å›¾ï¼Œå¹¶åŸºäºè‡ªç„¶è¯­è¨€SOPæè¿°è¦æ±‚ä»£ç†è¿›è¡Œè°ƒç”¨ã€‚</li>
<li>é¡¶å°–æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°æœ‰æŒ‘æˆ˜ï¼Œä¸åŒé¢†åŸŸé—´å­˜åœ¨å·®å¼‚ï¼Œæ¨ç†æ¨¡å‹çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>è¯­è¨€ä»£ç†å­˜åœ¨å®¹æ˜“å¿½è§†SOPså’Œçº¦æŸçš„é—®é¢˜ï¼Œå­˜åœ¨è¢«â€œç ´è§£â€çš„é£é™©ã€‚</li>
<li>SOPBenchçš„ç›¸å…³ä»£ç ã€æ•°æ®å’Œä»£ç†è½¨è¿¹å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾¿å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88f4d5f5b696befae445423b2c2bb725.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22317fa421bb1c523832e2ef90fbb5c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-094a917ff79d0ed5699de7f51a20b4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9107c8496139cdfcc68e67156689992b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2341aec4af7e32efc41a536eb3f08a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf6aa9bcbe33ad7125233f5202c4af43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52f12a611d8cef6ab24a78cde887bec0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fce97b4f6def7d65ec1be062205a3d29.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  PhantomHunter Detecting Unseen Privately-Tuned LLM-Generated Text via   Family-Aware Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-21/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04665052a4701a654af5574696cb6530.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-21  Exploring Timeline Control for Facial Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
