<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  OpenPath Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6be101dced6ae52c25293095f5eb1857.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="OpenPath-Open-Set-Active-Learning-for-Pathology-Image-Classification-via-Pre-trained-Vision-Language-Models"><a href="#OpenPath-Open-Set-Active-Learning-for-Pathology-Image-Classification-via-Pre-trained-Vision-Language-Models" class="headerlink" title="OpenPath: Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models"></a>OpenPath: Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models</h2><p><strong>Authors:Lanfeng Zhong, Xin Liao, Shichuan Zhang, Shaoting Zhang, Guotai Wang</strong></p>
<p>Pathology image classification plays a crucial role in accurate medical diagnosis and treatment planning. Training high-performance models for this task typically requires large-scale annotated datasets, which are both expensive and time-consuming to acquire. Active Learning (AL) offers a solution by iteratively selecting the most informative samples for annotation, thereby reducing the labeling effort. However, most AL methods are designed under the assumption of a closed-set scenario, where all the unannotated images belong to target classes. In real-world clinical environments, the unlabeled pool often contains a substantial amount of Out-Of-Distribution (OOD) data, leading to low efficiency of annotation in traditional AL methods. Furthermore, most existing AL methods start with random selection in the first query round, leading to a significant waste of labeling costs in open-set scenarios. To address these challenges, we propose OpenPath, a novel open-set active learning approach for pathological image classification leveraging a pre-trained Vision-Language Model (VLM). In the first query, we propose task-specific prompts that combine target and relevant non-target class prompts to effectively select In-Distribution (ID) and informative samples from the unlabeled pool. In subsequent queries, Diverse Informative ID Sampling (DIS) that includes Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic Sampling (EGSS) is proposed to ensure both purity and informativeness in a query, avoiding the selection of OOD samples. Experiments on two public pathology image datasets show that OpenPath significantly enhances the modelâ€™s performance due to its high purity of selected samples, and outperforms several state-of-the-art open-set AL methods. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/OpenPath%7D%7Bhttps://github.com/HiLab-git/OpenPath%7D">https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}</a>.. </p>
<blockquote>
<p>ç—…ç†å­¦å›¾åƒåˆ†ç±»åœ¨å‡†ç¡®çš„åŒ»å­¦è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­å‘æŒ¥è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºæ­¤ä»»åŠ¡è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹é€šå¸¸éœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®çš„è·å–æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰é€šè¿‡è¿­ä»£é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œä»è€Œå‡å°‘äº†æ ‡æ³¨å·¥ä½œé‡ï¼Œä¸ºæ­¤æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ALæ–¹æ³•çš„è®¾è®¡æ˜¯åŸºäºå°é—­é›†åœºæ™¯çš„å‡è®¾ï¼Œå³æ‰€æœ‰æœªæ ‡æ³¨çš„å›¾åƒéƒ½å±äºç›®æ ‡ç±»åˆ«ã€‚åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œæœªæ ‡æ³¨æ± ä¸­ç»å¸¸åŒ…å«å¤§é‡è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çš„æ•°æ®ï¼Œå¯¼è‡´ä¼ ç»ŸALæ–¹æ³•çš„æ ‡æ³¨æ•ˆç‡é™ä½ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰çš„ALæ–¹æ³•åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢æ—¶é‡‡ç”¨éšæœºé€‰æ‹©çš„æ–¹å¼ï¼Œè¿™åœ¨å¼€æ”¾é›†åœºæ™¯ä¸­é€ æˆäº†æ˜¾è‘—çš„æ ‡æ³¨æˆæœ¬æµªè´¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OpenPathï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒå¥½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç—…ç†å­¦å›¾åƒåˆ†ç±»çš„æ–°å‹å¼€æ”¾é›†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ã€‚åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡ç‰¹å®šçš„æç¤ºï¼Œè¿™äº›æç¤ºç»“åˆäº†ç›®æ ‡å’Œç›¸å…³çš„éç›®æ ‡ç±»åˆ«æç¤ºï¼Œä»¥æœ‰æ•ˆåœ°ä»æœªæ ‡æ³¨çš„æ± ä¸­é€‰å–In-Distributionï¼ˆIDï¼‰å’Œå…·æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚åœ¨éšåçš„æŸ¥è¯¢ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒ…å«åŸºäºåŸå‹IDå€™é€‰é€‰æ‹©çš„å¤šæ ·åŒ–ä¿¡æ¯IDé‡‡æ ·å’Œç†µå¼•å¯¼éšæœºé‡‡æ ·çš„æ–¹æ³•ï¼ˆDISï¼‰ï¼Œä»¥ç¡®ä¿æŸ¥è¯¢çš„çº¯å‡€åº¦å’Œä¿¡æ¯é‡ï¼Œé¿å…é€‰æ‹©OODæ ·æœ¬ã€‚åœ¨ä¸¤ä¸ªå…¬å…±ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç”±äºæ‰€é€‰æ ·æœ¬çš„é«˜çº¯å‡€åº¦ï¼ŒOpenPathæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºå‡ ç§å…ˆè¿›çš„å¼€æ”¾é›†ALæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/OpenPath%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HiLab-git/OpenPathä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15318v1">PDF</a> MICCAI 2025 early accept</p>
<p><strong>Summary</strong><br>     ç—…ç†å›¾åƒåˆ†ç±»åœ¨å‡†ç¡®çš„åŒ»å­¦è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ä¸ºäº†è§£å†³å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„é«˜æˆæœ¬å’Œè€—æ—¶çš„éœ€æ±‚ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°å‹å¼€æ”¾é›†æ´»æ€§å­¦ä¹ ï¼ˆOpenPathï¼‰æ–¹æ³•ï¼Œä»¥æé«˜ç—…ç†å›¾åƒåˆ†ç±»çš„æ•ˆç‡ã€‚å®ƒé€šè¿‡ç»“åˆç›®æ ‡ç±»å’Œç›¸å…³éç›®æ ‡ç±»çš„ä»»åŠ¡ç‰¹å®šæç¤ºæ¥é€‰å–å…·æœ‰ä»£è¡¨æ€§æ ·æœ¬ã€‚è¯¥æ–¹æ³•æ€§èƒ½æ˜¾è‘—ï¼Œåœ¨å…¬å¼€ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„å¼€æ”¾é›†æ´»æ€§å­¦ä¹ æ–¹æ³•ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å›¾åƒåˆ†ç±»åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ´»æ€§å­¦ä¹ æ–¹æ³•é¢ä¸´è·å–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„é«˜æˆæœ¬å’Œè€—æ—¶é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¼€æ”¾é›†æ´»æ€§å­¦ä¹ æ–¹æ³•ï¼ˆOpenPathï¼‰ã€‚</li>
<li>åœ¨é¦–æ¬¡æŸ¥è¯¢æ—¶é‡‡ç”¨ä»»åŠ¡ç‰¹å®šæç¤ºæ¥é€‰æ‹©æ ·æœ¬ã€‚</li>
<li>åç»­æŸ¥è¯¢é‡‡ç”¨å¤šæ ·ä¿¡æ¯å†…éƒ¨é‡‡æ ·ï¼ˆDISï¼‰ï¼Œç¡®ä¿æ‰€é€‰æ ·æœ¬çš„çº¯åº¦å’Œä¿¡æ¯é‡ï¼Œé¿å…é€‰æ‹©å‡ºç•Œæ ·æœ¬ã€‚</li>
<li>åœ¨å…¬å¼€ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOpenPathæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edd9d4d0809e719940dd340fc1595f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b047c3148cb95a814d4cf9bf03b12f31.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vision-Transformers-for-End-to-End-Quark-Gluon-Jet-Classification-from-Calorimeter-Images"><a href="#Vision-Transformers-for-End-to-End-Quark-Gluon-Jet-Classification-from-Calorimeter-Images" class="headerlink" title="Vision Transformers for End-to-End Quark-Gluon Jet Classification from   Calorimeter Images"></a>Vision Transformers for End-to-End Quark-Gluon Jet Classification from   Calorimeter Images</h2><p><strong>Authors:Md Abrar Jahin, Shahriar Soudeep, Arian Rahman Aditta, M. F. Mridha, Nafiz Fahad, Md. Jakir Hossen</strong></p>
<p>Distinguishing between quark- and gluon-initiated jets is a critical and challenging task in high-energy physics, pivotal for improving new physics searches and precision measurements at the Large Hadron Collider. While deep learning, particularly Convolutional Neural Networks (CNNs), has advanced jet tagging using image-based representations, the potential of Vision Transformer (ViT) architectures, renowned for modeling global contextual information, remains largely underexplored for direct calorimeter image analysis, especially under realistic detector and pileup conditions. This paper presents a systematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet classification using simulated 2012 CMS Open Data. We construct multi-channel jet-view images from detector-level energy deposits (ECAL, HCAL) and reconstructed tracks, enabling an end-to-end learning approach. Our comprehensive benchmarking demonstrates that ViT-based models, notably ViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN baselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of capturing long-range spatial correlations within jet substructure. This work establishes the first systematic framework and robust performance baselines for applying ViT architectures to calorimeter image-based jet classification using public collider data, alongside a structured dataset suitable for further deep learning research in this domain. </p>
<blockquote>
<p>åœ¨é«˜èƒ½ç‰©ç†å­¦ä¸­ï¼ŒåŒºåˆ†å¤¸å…‹å’Œèƒ¶å­å¼•å‘çš„å–·æµæ˜¯ä¸€é¡¹å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¯¹äºæé«˜å¤§å‹å¼ºå­å¯¹æ’æœºçš„æ–°ç‰©ç†æœç´¢å’Œç²¾å¯†æµ‹é‡è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œå·²ç»é€šè¿‡åŸºäºå›¾åƒçš„è¡¨ç°å½¢å¼æ¨åŠ¨äº†å°„æµæ ‡è®°çš„å‘å±•ã€‚ç„¶è€Œï¼Œå¯¹äºç›´æ¥é‡çƒ­ä»ªå›¾åƒåˆ†æï¼Œå°¤å…¶æ˜¯ç°å®æ¢æµ‹å™¨æ¡ä»¶å’Œå †ç§¯æƒ…å†µä¸‹çš„åˆ†æï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼ŒViTï¼‰æ¶æ„çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œåè€…ä»¥å…¶å¯¹å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å»ºæ¨¡è€Œé—»åã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†ViTå’ŒViT-CNNæ··åˆæ¨¡å‹åœ¨æ¨¡æ‹Ÿçš„å¤¸å…‹-èƒ¶å­å°„æµåˆ†ç±»ä¸­ä½¿ç”¨2012å¹´CMSå…¬å¼€æ•°æ®çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä»æ¢æµ‹å™¨çº§åˆ«çš„èƒ½é‡æ²‰ç§¯ï¼ˆECALã€HCALï¼‰å’Œé‡å»ºè½¨è¿¹æ„å»ºäº†å¤šé€šé“å°„æµè§†å›¾å›¾åƒï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»¼åˆåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒåŸºäºViTçš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ViT+MaxViTå’ŒViT+ConvNeXtæ··åˆæ¨¡å‹ï¼Œåœ¨F1å¾—åˆ†ã€ROC-AUCå’Œå‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆä¼˜äºå·²å»ºç«‹çš„CNNåŸºå‡†æµ‹è¯•ï¼Œè¿™çªæ˜¾äº†æ•è·å°„æµå­ç»“æ„ä¸­é•¿ç¨‹ç©ºé—´å…³è”çš„ä¼˜åŠ¿ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä½¿ç”¨å…¬å…±å¯¹æ’æœºæ•°æ®è¿›è¡ŒåŸºäºé‡çƒ­ä»ªå›¾åƒçš„å°„æµåˆ†ç±»åº”ç”¨ViTæ¶æ„çš„ç¬¬ä¸€ä¸ªç³»ç»Ÿæ¡†æ¶å’Œç¨³å¥çš„æ€§èƒ½åŸºå‡†ï¼Œä»¥åŠé€‚åˆåœ¨æ­¤é¢†åŸŸè¿›è¡Œæ·±åº¦å­¦ä¹ ç ”ç©¶çš„ç»“æ„åŒ–æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14934v1">PDF</a> Accepted in Third International Workshop on Generalizing from Limited   Resources in the Open World Workshop at International Joint Conference on   Artificial Intelligence (IJCAI) 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æ¢è®¨äº†åˆ©ç”¨Vision Transformerï¼ˆViTï¼‰æ¶æ„å¯¹é«˜èƒ½ç‰©ç†ä¸­çš„å¤¸å…‹å’Œèƒ¶å­æ¿€å‘çš„å–·å°„è¿›è¡Œåˆ†ç±»çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶ä½¿ç”¨æ¨¡æ‹Ÿçš„CMS Open Dataæ„å»ºå¤šé€šé“å–·å°„å›¾åƒï¼Œå¹¶é‡‡ç”¨ViTå’ŒViT-CNNæ··åˆæ¨¡å‹è¿›è¡Œå–·å°„åˆ†ç±»ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºViTçš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ViT+MaxViTå’ŒViT+ConvNeXtæ··åˆæ¨¡å‹ï¼Œåœ¨F1å¾—åˆ†ã€ROC-AUCå’Œå‡†ç¡®åº¦æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„CNNåŸºå‡†æ¨¡å‹ï¼Œè¿™å¾—ç›Šäºå…¶æ•æ‰å–·å°„å­ç»“æ„ä¸­é•¿ç¨‹ç©ºé—´å…³è”çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å…¬å…±ç¢°æ’æ•°æ®ä¸Šåº”ç”¨ViTæ¶æ„è¿›è¡ŒåŸºäºé‡èƒ½å™¨å›¾åƒçš„å–·å°„åˆ†ç±»å»ºç«‹äº†ç³»ç»Ÿçš„æ¡†æ¶å’Œç¨³å¥çš„æ€§èƒ½åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerï¼ˆViTï¼‰æ¶æ„åœ¨é«˜èƒ½ç‰©ç†ä¸­çš„å¤¸å…‹å’Œèƒ¶å­å¼•å‘çš„å–·å°„åˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨å¤šé€šé“å–·å°„å›¾åƒï¼ŒåŒ…å«æ¢æµ‹å™¨çº§èƒ½é‡æ²‰ç§¯å’Œé‡å»ºè½¨è¿¹ä¿¡æ¯ã€‚</li>
<li>ViTå’ŒViT-CNNæ··åˆæ¨¡å‹åœ¨å–·å°„åˆ†ç±»é—®é¢˜ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºViTçš„æ¨¡å‹èƒ½å¤Ÿæ•æ‰å–·å°„å­ç»“æ„ä¸­çš„é•¿ç¨‹ç©ºé—´å…³è”ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºä½¿ç”¨å…¬å…±ç¢°æ’æ•°æ®åº”ç”¨ViTæ¶æ„è¿›è¡ŒåŸºäºé‡èƒ½å™¨å›¾åƒçš„å–·å°„åˆ†ç±»æä¾›äº†ç³»ç»Ÿçš„æ¡†æ¶å’Œæ€§èƒ½åŸºå‡†ã€‚</li>
<li>ç ”ç©¶ç»“æœå±•ç¤ºäº†ViTæ¶æ„åœ¨æ”¹å–„æ–°ç‰©ç†æœç´¢å’Œå¤§å‹å¼ºå­å¯¹æ’æœºçš„ç²¾å¯†æµ‹é‡æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76a3dd4bb22a220d11ea47ef833de761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80245fbdca96417dc196dd6c722e16b8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SceneAware-Scene-Constrained-Pedestrian-Trajectory-Prediction-with-LLM-Guided-Walkability"><a href="#SceneAware-Scene-Constrained-Pedestrian-Trajectory-Prediction-with-LLM-Guided-Walkability" class="headerlink" title="SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with   LLM-Guided Walkability"></a>SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with   LLM-Guided Walkability</h2><p><strong>Authors:Juho Bai, Inwook Shim</strong></p>
<p>Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer<del>(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models</del>(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH&#x2F;UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/juho127/SceneAware">https://github.com/juho127/SceneAware</a>. </p>
<blockquote>
<p>è¡Œäººè½¨è¿¹çš„ç²¾ç¡®é¢„æµ‹å¯¹äºæœºå™¨äººå’Œç›‘æ§ç³»ç»Ÿåº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¡Œäººä¹‹é—´çš„ç¤¾ä¼šäº¤äº’ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†ä¸°å¯Œçš„ç¯å¢ƒèƒŒæ™¯ï¼Œç¯å¢ƒèƒŒæ™¯å¯¹äººç±»è¿åŠ¨æ¨¡å¼æœ‰é‡å¤§å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SceneAwareï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜¾å¼èå…¥åœºæ™¯ç†è§£ä»¥å¢å¼ºè½¨è¿¹é¢„æµ‹ç²¾åº¦çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨Vision Transformerï¼ˆViTï¼‰åœºæ™¯ç¼–ç å™¨å¤„ç†æ¥è‡ªé™æ€åœºæ™¯å›¾åƒçš„ç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”ŸæˆäºŒè¿›åˆ¶å¯æ­¥è¡Œæ€§æ©ç ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†å¯é€šè¡Œå’Œé™åˆ¶åŒºåŸŸã€‚æˆ‘ä»¬å°†åŸºäºTransformerçš„è½¨è¿¹ç¼–ç å™¨å’ŒåŸºäºViTçš„åœºæ™¯ç¼–ç å™¨ç›¸ç»“åˆï¼Œæ•æ‰æ—¶é—´åŠ¨æ€å’Œç©ºé—´çº¦æŸã€‚è¯¥æ¡†æ¶é›†æˆäº†ç¢°æ’æƒ©ç½šæœºåˆ¶ï¼Œé˜»æ­¢é¢„æµ‹è½¨è¿¹è¿åç‰©ç†è¾¹ç•Œï¼Œç¡®ä¿é¢„æµ‹ç»“æœç¬¦åˆç‰©ç†è§„å¾‹ã€‚SceneAwareæœ‰ç¡®å®šæ€§å’Œéšæœºæ€§ä¸¤ç§å®ç°æ–¹å¼ã€‚åœ¨ETH&#x2F;UCYåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œè¾ƒä¹‹å‰æ¨¡å‹æœ‰50%ä»¥ä¸Šçš„æ”¹è¿›ã€‚æˆ‘ä»¬åŸºäºä¸åŒè½¨è¿¹ç±»åˆ«çš„åˆ†æè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§ç±»å‹çš„è¡Œäººè¿åŠ¨ä¸­è¡¨ç°ä¸€è‡´ã€‚è¿™å¼ºè°ƒäº†ä½¿ç”¨æ˜ç¡®åœºæ™¯ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„åœºæ™¯æ„ŸçŸ¥æ–¹æ³•åœ¨ç”Ÿæˆå‡†ç¡®ä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„é¢„æµ‹æ–¹é¢æ—¢æœ‰æ•ˆåˆå¯é ã€‚ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/juho127/SceneAware%E3%80%82">https://github.com/juho127/SceneAwareã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14144v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¡Œäººè½¨è¿¹é¢„æµ‹å¯¹äºæœºå™¨äººå’Œç›‘æ§ç³»ç»Ÿåº”ç”¨è‡³å…³é‡è¦ã€‚å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨è¡Œäººé—´çš„ç¤¾äº¤äº’åŠ¨ï¼Œå´å¿½ç•¥äº†ç¯å¢ƒå¯¹è¿åŠ¨æ¨¡å¼çš„é‡è¦å½±å“ã€‚æœ¬æ–‡æå‡ºSceneAwareæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åœºæ™¯ç†è§£æŠ€æœ¯æé«˜è½¨è¿¹é¢„æµ‹å‡†ç¡®æ€§ã€‚åˆ©ç”¨Vision Transformerå¤„ç†é™æ€åœºæ™¯å›¾åƒçš„ç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯æ­¥è¡Œæ€§æ©è†œã€‚ç»“åˆTransformerçš„è½¨è¿¹ç¼–ç å™¨å’ŒViTçš„åœºæ™¯ç¼–ç å™¨ï¼Œæ•æ‰æ—¶ç©ºçº¦æŸå’ŒåŠ¨æ€ä¿¡æ¯ã€‚æ¡†æ¶åŒ…å«ç¢°æ’æƒ©ç½šæœºåˆ¶ï¼Œç¡®ä¿é¢„æµ‹è½¨è¿¹ç¬¦åˆç‰©ç†è¾¹ç•Œã€‚SceneAwareæ¡†æ¶åœ¨ETH&#x2F;UCYåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¨¡å‹åœ¨å„ç§ç±»å‹çš„è¡Œäººè¿åŠ¨ä¸Šè¡¨ç°ç¨³å®šã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¡Œäººè½¨è¿¹é¢„æµ‹åœ¨æœºå™¨äººå’Œç›‘æ§ç³»ç»Ÿä¸­æœ‰é‡è¦åº”ç”¨ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¤¾äº¤äº’åŠ¨ï¼Œä½†ç¯å¢ƒå¯¹è¡Œäººè¿åŠ¨æ¨¡å¼å½±å“å·¨å¤§ã€‚</li>
<li>SceneAwareæ¡†æ¶å¼•å…¥åœºæ™¯ç†è§£æŠ€æœ¯ï¼Œæé«˜è½¨è¿¹é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨Vision Transformerå¤„ç†é™æ€åœºæ™¯å›¾åƒï¼Œæ•æ‰ç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯æ­¥è¡Œæ€§æ©è†œï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ç»“åˆTransformerå’ŒViTæŠ€æœ¯ï¼Œæ•æ‰æ—¶ç©ºçº¦æŸå’ŒåŠ¨æ€ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c736c9d7a699385f705ebb8a3d8f9f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8791238d763b7590a3c815fc55b8b24b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60e6726d654c671e2a0da593c06b1cf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed666363ef9410fdb785ddb38380faab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be101dced6ae52c25293095f5eb1857.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ViT-NeBLa-A-Hybrid-Vision-Transformer-and-Neural-Beer-Lambert-Framework-for-Single-View-3D-Reconstruction-of-Oral-Anatomy-from-Panoramic-Radiographs"><a href="#ViT-NeBLa-A-Hybrid-Vision-Transformer-and-Neural-Beer-Lambert-Framework-for-Single-View-3D-Reconstruction-of-Oral-Anatomy-from-Panoramic-Radiographs" class="headerlink" title="ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework   for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs"></a>ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework   for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs</h2><p><strong>Authors:Bikram Keshari Parida, Anusree P. Sunilkumar, Abhijit Sen, Wonsang You</strong></p>
<p>Dental diagnosis relies on two primary imaging modalities: panoramic radiographs (PX) providing 2D oral cavity representations, and Cone-Beam Computed Tomography (CBCT) offering detailed 3D anatomical information. While PX images are cost-effective and accessible, their lack of depth information limits diagnostic accuracy. CBCT addresses this but presents drawbacks including higher costs, increased radiation exposure, and limited accessibility. Existing reconstruction models further complicate the process by requiring CBCT flattening or prior dental arch information, often unavailable clinically. We introduce ViT-NeBLa, a vision transformer-based Neural Beer-Lambert model enabling accurate 3D reconstruction directly from single PX. Our key innovations include: (1) enhancing the NeBLa framework with Vision Transformers for improved reconstruction capabilities without requiring CBCT flattening or prior dental arch information, (2) implementing a novel horseshoe-shaped point sampling strategy with non-intersecting rays that eliminates intermediate density aggregation required by existing models due to intersecting rays, reducing sampling point computations by $52 %$, (3) replacing CNN-based U-Net with a hybrid ViT-CNN architecture for superior global and local feature extraction, and (4) implementing learnable hash positional encoding for better higher-dimensional representation of 3D sample points compared to existing Fourier-based dense positional encoding. Experiments demonstrate that ViT-NeBLa significantly outperforms prior state-of-the-art methods both quantitatively and qualitatively, offering a cost-effective, radiation-efficient alternative for enhanced dental diagnostics. </p>
<blockquote>
<p>ç‰™ç§‘è¯Šæ–­ä¾èµ–äºä¸¤ç§ä¸»è¦çš„æˆåƒæ¨¡å¼ï¼šå…¨æ™¯Xå°„çº¿ï¼ˆPXï¼‰æä¾›äºŒç»´å£è…”è…”å®¤çš„è¡¨ç¤ºï¼Œè€Œé”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰æä¾›è¯¦ç»†çš„3Dè§£å‰–ä¿¡æ¯ã€‚è™½ç„¶PXå›¾åƒæˆæœ¬ä½ä¸”æ˜“äºè·å–ï¼Œä½†å®ƒä»¬ç¼ºä¹æ·±åº¦ä¿¡æ¯ï¼Œé™åˆ¶äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚CBCTè§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼ŒåŒ…æ‹¬æˆæœ¬è¾ƒé«˜ã€è¾å°„æš´éœ²å¢åŠ ä»¥åŠè·å–éš¾åº¦è¾ƒé«˜ã€‚ç°æœ‰çš„é‡å»ºæ¨¡å‹éœ€è¦è¿›ä¸€æ­¥å¤æ‚åŒ–æµç¨‹ï¼Œéœ€è¦CBCTå±•å¹³æˆ–å…ˆå‰çš„ç‰™é½¿æ’åˆ—ä¿¡æ¯ï¼Œè¿™åœ¨ä¸´åºŠä¸Šå¾€å¾€æ— æ³•è·å¾—ã€‚æˆ‘ä»¬å¼•å…¥äº†ViT-NeBLaï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§†è§‰å˜å‹å™¨çš„ç¥ç»ç½‘ç»œBeer-Lambertæ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ä»å•ä¸ªPXè¿›è¡Œå‡†ç¡®çš„3Dé‡å»ºã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä½¿ç”¨è§†è§‰å˜å‹å™¨å¢å¼ºNeBLaæ¡†æ¶ï¼Œæé«˜äº†é‡å»ºèƒ½åŠ›ï¼Œæ— éœ€CBCTå±•å¹³æˆ–å…ˆå‰çš„ç‰™é½¿æ’åˆ—ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰å®æ–½äº†ä¸€ç§æ–°é¢–çš„é©¬è¹„å½¢ç‚¹é‡‡æ ·ç­–ç•¥ï¼Œé‡‡ç”¨éäº¤å‰å°„çº¿ï¼Œæ¶ˆé™¤äº†ç°æœ‰æ¨¡å‹å› äº¤å‰å°„çº¿æ‰€éœ€çš„ä¸­ä»‹å¯†åº¦èšé›†ï¼Œå‡å°‘äº†é‡‡æ ·ç‚¹è®¡ç®—é‡çš„52%ï¼›ï¼ˆ3ï¼‰ç”¨æ··åˆViT-CNNæ¶æ„æ›¿æ¢åŸºäºCNNçš„U-Netï¼Œä»¥è¿›è¡Œæ›´å‡ºè‰²çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾æå–ï¼›ï¼ˆ4ï¼‰å®æ–½å¯å­¦ä¹ çš„å“ˆå¸Œä½ç½®ç¼–ç ï¼Œä¸ç°æœ‰çš„åŸºäºå‚…ç«‹å¶çš„å¯†é›†ä½ç½®ç¼–ç ç›¸æ¯”ï¼Œç”¨äºæ›´å¥½åœ°è¡¨ç¤º3Dæ ·æœ¬ç‚¹çš„é«˜ç»´ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒViT-NeBLaåœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸ºå¢å¼ºç‰™ç§‘è¯Šæ–­æä¾›äº†æˆæœ¬ä½ã€è¾å°„æ•ˆç‡é«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13195v1">PDF</a> 10 figures, 19 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè§†è§‰è½¬æ¢å™¨çš„ç¥ç»ç½‘ç»œBeer-Lambertæ¨¡å‹ViT-NeBLaï¼Œå¯ç›´æ¥ä»å…¨æ™¯Xå°„çº¿å›¾åƒè¿›è¡Œå‡†ç¡®çš„3Dé‡å»ºã€‚æ¨¡å‹åˆ›æ–°åŒ…æ‹¬å¢å¼ºNeBLaæ¡†æ¶ä¸è§†è§‰è½¬æ¢å™¨çš„ç»“åˆï¼Œå®ç°æ— éœ€CBCTå±•å¹³æˆ–é¢„å…ˆç‰™ç§‘å¼“å½¢ä¿¡æ¯çš„é‡å»ºèƒ½åŠ›ï¼›å®æ–½æ–°å‹é©¬è¹„å½¢ç‚¹é‡‡æ ·ç­–ç•¥ï¼Œæ¶ˆé™¤ç°æœ‰æ¨¡å‹å› å°„çº¿ç›¸äº¤æ‰€éœ€çš„ä¸­é—´å¯†åº¦èšé›†ï¼Œå‡å°‘é‡‡æ ·ç‚¹è®¡ç®—ï¼›ä½¿ç”¨æ··åˆViT-CNNæ¶æ„æ›¿ä»£CNN-based U-Netï¼Œå®ç°æ›´ä¼˜è¶Šçš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾æå–ï¼›ä»¥åŠå®æ–½å¯å­¦ä¹ çš„å“ˆå¸Œä½ç½®ç¼–ç ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤º3Dæ ·æœ¬ç‚¹çš„é«˜ç»´ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒViT-NeBLaåœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†ä¸€ä¸ªæˆæœ¬æ•ˆç›Šé«˜ã€è¾å°„æ•ˆç‡é«˜çš„å¢å¼ºç‰™ç§‘è¯Šæ–­çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViT-NeBLaæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰è½¬æ¢å™¨çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºä»å…¨æ™¯Xå°„çº¿å›¾åƒè¿›è¡Œ3Dé‡å»ºã€‚</li>
<li>è¯¥æ¨¡å‹å¢å¼ºäº†NeBLaæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨æé«˜é‡å»ºç²¾åº¦ï¼Œæ— éœ€CBCTå±•å¹³æˆ–é¢„å…ˆçš„ç‰™ç§‘å¼“å½¢ä¿¡æ¯ã€‚</li>
<li>å®æ–½æ–°å‹ç‚¹é‡‡æ ·ç­–ç•¥ï¼Œæ¶ˆé™¤ä¸­é—´å¯†åº¦èšé›†ï¼Œå‡å°‘é‡‡æ ·ç‚¹è®¡ç®—ã€‚</li>
<li>ä½¿ç”¨æ··åˆViT-CNNæ¶æ„æå–å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œä¼˜äºä¼ ç»Ÿçš„CNN-based U-Netã€‚</li>
<li>å¼•å…¥äº†å¯å­¦ä¹ çš„å“ˆå¸Œä½ç½®ç¼–ç ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°è¡¨ç¤º3Dæ ·æœ¬ç‚¹çš„é«˜ç»´ä¿¡æ¯ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºViT-NeBLaåœ¨ç‰™ç§‘è¯Šæ–­çš„3Dé‡å»ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æ›´å‡†ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5296e93a7c03c1390764082c1c823302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-501234a3991957e52254e319a2a88d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d46afffc93dd1febbc52bb31e01df4dc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Combining-Self-attention-and-Dilation-Convolutional-for-Semantic-Segmentation-of-Coal-Maceral-Groups"><a href="#Combining-Self-attention-and-Dilation-Convolutional-for-Semantic-Segmentation-of-Coal-Maceral-Groups" class="headerlink" title="Combining Self-attention and Dilation Convolutional for Semantic   Segmentation of Coal Maceral Groups"></a>Combining Self-attention and Dilation Convolutional for Semantic   Segmentation of Coal Maceral Groups</h2><p><strong>Authors:Zhenghao Xi, Zhengnan Lv, Yang Zheng, Xiang Liu, Zhuang Yu, Junran Chen, Jing Hu, Yaqi Liu</strong></p>
<p>The segmentation of coal maceral groups can be described as a semantic segmentation process of coal maceral group images, which is of great significance for studying the chemical properties of coal. Generally, existing semantic segmentation models of coal maceral groups use the method of stacking parameters to achieve higher accuracy. It leads to increased computational requirements and impacts model training efficiency. At the same time, due to the professionalism and diversity of coal maceral group images sampling, obtaining the number of samples for model training requires a long time and professional personnel operation. To address these issues, We have innovatively developed an IoT-based DA-VIT parallel network model. By utilizing this model, we can continuously broaden the dataset through IoT and achieving sustained improvement in the accuracy of coal maceral groups segmentation. Besides, we decouple the parallel network from the backbone network to ensure the normal using of the backbone network during model data updates. Secondly, DCSA mechanism of DA-VIT is introduced to enhance the local feature information of coal microscopic images. This DCSA can decompose the large kernels of convolutional attention into multiple scales and reduce 81.18% of parameters.Finally, we performed the contrast experiment and ablation experiment between DA-VIT and state-of-the-art methods at lots of evaluation metrics. Experimental results show that DA-VIT-Base achieves 92.14% pixel accuracy and 63.18% mIoU. Params and FLOPs of DA-VIT-Tiny are 4.95M and 8.99G, respectively. All of the evaluation metrics of the proposed DA-VIT are better than other state-of-the-art methods. </p>
<blockquote>
<p>ç…¤çš„æ˜¾å¾®ç»„åˆ†åˆ†å‰²å¯ä»¥æè¿°ä¸ºç…¤æ˜¾å¾®ç»„åˆ†å›¾åƒçš„è¯­ä¹‰åˆ†å‰²è¿‡ç¨‹ï¼Œè¿™å¯¹ç ”ç©¶ç…¤çš„åŒ–å­¦æ€§è´¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç›®å‰ï¼Œç°æœ‰çš„ç…¤æ˜¾å¾®ç»„åˆ†è¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸é‡‡ç”¨å †å å‚æ•°çš„æ–¹æ³•æ¥æé«˜ç²¾åº¦ï¼Œè¿™å¯¼è‡´äº†è®¡ç®—éœ€æ±‚çš„å¢åŠ å’Œæ¨¡å‹è®­ç»ƒæ•ˆç‡çš„å½±å“ã€‚åŒæ—¶ï¼Œç”±äºç…¤æ˜¾å¾®ç»„åˆ†å›¾åƒé‡‡æ ·çš„ä¸“ä¸šæ€§å’Œå¤šæ ·æ€§ï¼Œè·å–ç”¨äºæ¨¡å‹è®­ç»ƒçš„æ ·æœ¬æ•°é‡éœ€è¦å¾ˆé•¿æ—¶é—´å’Œä¸“ä¸šäººå‘˜çš„æ“ä½œã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°å¼€å‘äº†ä¸€ä¸ªåŸºäºç‰©è”ç½‘çš„DA-VITå¹¶è¡Œç½‘ç»œæ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‰©è”ç½‘æŒç»­æ‰©å¤§æ•°æ®é›†ï¼Œå®ç°ç…¤æ˜¾å¾®ç»„åˆ†åˆ†å‰²ç²¾åº¦çš„æŒç»­æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¹¶è¡Œç½‘ç»œä¸éª¨å¹²ç½‘ç»œè§£è€¦ï¼Œä»¥ç¡®ä¿åœ¨æ¨¡å‹æ•°æ®æ›´æ–°æœŸé—´éª¨å¹²ç½‘ç»œçš„æ­£å¸¸ä½¿ç”¨ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†DA-VITçš„DCSAæœºåˆ¶ï¼Œä»¥å¢å¼ºç…¤å¾®è§‚å›¾åƒçš„å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚DCSAå¯ä»¥å°†å·ç§¯æ³¨æ„åŠ›çš„å¤§å†…æ ¸åˆ†è§£æˆå¤šä¸ªå°ºåº¦ï¼Œå¹¶å‡å°‘81.18%çš„å‚æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è®¸å¤šè¯„ä¼°æŒ‡æ ‡ä¹‹é—´å¯¹DA-VITå’Œæœ€æ–°æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”å®éªŒå’Œæ¶ˆèå®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDA-VIT-Baseçš„åƒç´ ç²¾åº¦è¾¾åˆ°92.14%ï¼ŒmIoUè¾¾åˆ°63.18%ã€‚DA-VIT-Tinyçš„å‚æ•°å’ŒFLOPsåˆ†åˆ«ä¸º4.95Må’Œ8.99Gã€‚æ‰€æå‡ºDA-VITçš„æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12712v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç…¤ç‚­æ˜¾å¾®å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„ç…¤å²©ç»„åˆ†åˆ†å‰²å…·æœ‰ç ”ç©¶ç…¤åŒ–å­¦æ€§è´¨çš„é‡è¦æ„ä¹‰ã€‚ç°æœ‰æ–¹æ³•å¸¸é‡‡ç”¨å †å å‚æ•°æé«˜ç²¾åº¦ï¼Œä½†è®¡ç®—é‡å¤§ä¸”å½±å“æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚ä¸ºè§£å†³æ ·æœ¬é‡‡é›†çš„ä¸“ä¸šæ€§å’Œå¤šæ ·æ€§é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›æ–°æ€§åœ°æ„å»ºäº†åŸºäºç‰©è”ç½‘çš„DA-VITå¹¶è¡Œç½‘ç»œæ¨¡å‹ï¼Œä»¥æ‰©å¤§æ•°æ®é›†å¹¶æŒç»­æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é€šè¿‡è§£è€¦å¹¶è¡Œç½‘ç»œç¡®ä¿åœ¨æ•°æ®æ›´æ–°æ—¶ä¸»å¹²ç½‘ç»œçš„æ­£å¸¸ä½¿ç”¨ï¼Œå¼•å…¥DA-VITçš„DCSAæœºåˆ¶æå‡ç…¤æ˜¾å¾®å›¾åƒå±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDA-VIT-Baseçš„åƒç´ ç²¾åº¦è¾¾åˆ°92.14%ï¼ŒmIoUä¸º63.18%ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç…¤ç‚­æ˜¾å¾®å›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ˜¯ç ”ç©¶ç…¤åŒ–å­¦æ€§è´¨çš„é‡è¦æ‰‹æ®µã€‚</li>
<li>ç°æœ‰è¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šè¿‡å †å å‚æ•°æé«˜ç²¾åº¦ï¼Œä½†è®¡ç®—é‡å¤§ä¸”å½±å“æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºç‰©è”ç½‘çš„DA-VITå¹¶è¡Œç½‘ç»œæ¨¡å‹ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½æŒç»­æ‰©å¤§æ•°æ®é›†å¹¶æé«˜ç…¤ç‚­ç»„åˆ†åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>DA-VITçš„DCSAæœºåˆ¶å¢å¼ºäº†ç…¤æ˜¾å¾®å›¾åƒçš„å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>DA-VIT-Baseçš„åƒç´ ç²¾åº¦è¾¾åˆ°92.14%ï¼ŒmIoUä¸º63.18%ã€‚</li>
<li>DA-VITæ¨¡å‹çš„è¡¨ç°ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e75817d38c5e5547aaa80629fd47bc8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c6c2c45344c5e7e3505b6abf7c9efeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-02cd0fd9b358665923ace3987867092e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f356611a75ba0cd1d7dd75eee4691a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-118bb926c7e9a4c58f225c7a7f2787ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-692a73a58fe09dad3e37cc201d1ca8b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5356fb09f0e154a6cdb002eda3c2e60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f4b8b55483559cb9d3fe6bb4982a997.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NAP-Tuning-Neural-Augmented-Prompt-Tuning-for-Adversarially-Robust-Vision-Language-Models"><a href="#NAP-Tuning-Neural-Augmented-Prompt-Tuning-for-Adversarially-Robust-Vision-Language-Models" class="headerlink" title="NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust   Vision-Language Models"></a>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust   Vision-Language Models</h2><p><strong>Authors:Jiaming Zhang, Xin Wang, Xingjun Ma, Lingyu Qiu, Yu-Gang Jiang, Jitao Sang</strong></p>
<p>Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations include: (1) extending AdvPT from text-only to multi-modal prompting across both text and visual modalities, (2) expanding from single-layer to multi-layer prompt architectures, and (3) proposing a novel architecture-level redesign through our Neural Augmentor approach, which implements feature purification to directly address the distortions introduced by adversarial attacks in feature space. Our NAP-Tuning approach incorporates token refiners that learn to reconstruct purified features through residual connections, allowing for modality-specific and layer-specific feature correction.Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on ViT-B32 architectures while maintaining competitive clean accuracy. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œé€šè¿‡è”åˆåµŒå…¥ç©ºé—´å±•ç¤ºäº†å¯¹è§†è§‰å’Œæ–‡æœ¬æ•°æ®ä¹‹é—´å…³ç³»çš„ç†è§£æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å¾ˆæœ‰æ•ˆï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ¨¡æ€æ–¹é¢ï¼Œè¿™å¼•å‘äº†é‡å¤§çš„å®‰å…¨æ‹…å¿§ã€‚åŸºäºæˆ‘ä»¬ä¹‹å‰å…³äºå¯¹æŠ—æ€§æç¤ºè°ƒæ•´ï¼ˆAdvPTï¼‰çš„å·¥ä½œï¼Œå¼•å…¥äº†å¯å­¦ä¹ çš„æ–‡æœ¬æç¤ºï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œå¹¿æ³›å‚æ•°è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜VLMsçš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªé‡è¦çš„æ‰©å±•ï¼Œå¼•å…¥ç¥ç»å¢å¼ºå™¨æ¡†æ¶è¿›è¡Œå¤šæ¨¡æ€å¯¹æŠ—æ€§æç¤ºè°ƒæ•´ï¼ˆNAP-Tuningï¼‰ã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å°†AdvPTä»çº¯æ–‡æœ¬æ‰©å±•åˆ°æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„å¤šæ¨¡æ€æç¤ºï¼Œï¼ˆ2ï¼‰ä»å•å±‚æ‰©å±•åˆ°å¤šå±‚æç¤ºæ¶æ„ï¼Œï¼ˆ3ï¼‰é€šè¿‡æˆ‘ä»¬çš„ç¥ç»å¢å¼ºå™¨æ–¹æ³•æå‡ºä¸€ç§æ–°çš„æ¶æ„çº§é‡æ–°è®¾è®¡ï¼Œå®ç°ç‰¹å¾å‡€åŒ–ï¼Œç›´æ¥è§£å†³ç‰¹å¾ç©ºé—´ä¸­å¯¹æŠ—æ”»å‡»å¼•å…¥çš„å¤±çœŸã€‚æˆ‘ä»¬çš„NAP-Tuningæ–¹æ³•ç»“åˆäº†æ ‡è®°ç²¾ç‚¼å™¨ï¼Œå­¦ä¹ é€šè¿‡æ®‹å·®è¿æ¥é‡å»ºå‡€åŒ–åçš„ç‰¹å¾ï¼Œå®ç°æ¨¡æ€ç‰¹å®šå’Œå±‚ç‰¹å®šçš„ç‰¹å¾æ ¡æ­£ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒNAP-Tuningåœ¨å„ç§æ•°æ®é›†å’Œæ”»å‡»ç±»å‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„AutoAttackåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨ViT-B16å’ŒViT-B32æ¶æ„ä¸Šåˆ†åˆ«è¶…è¶Šæœ€å¼ºåŸºçº¿33.5%å’Œ33.0%ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„æ¸…æ´ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12706v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPç­‰è·¨æ¨¡æ€æ¨¡å‹çš„è”åˆåµŒå…¥ç©ºé—´ï¼Œç†è§£è§†è§‰ä¸æ–‡æœ¬æ•°æ®é—´å…³ç³»çš„èƒ½åŠ›å·²å¾—åˆ°å¹¿æ³›è®¤å¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é¢ä¸´å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒï¼Œå­˜åœ¨æ˜¾è‘—çš„å®‰å…¨éšæ‚£ã€‚ä¸ºå¢å¼ºæ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¸Šçš„é²æ£’æ€§ï¼Œæˆ‘ä»¬åœ¨å…ˆå‰çš„AdvPTï¼ˆå¯¹æŠ—æ€§æç¤ºè°ƒä¼˜ï¼‰å·¥ä½œåŸºç¡€ä¸Šæ¨å‡ºæ˜¾è‘—æ‰©å±•ç‰ˆæœ¬ï¼Œå¼•å…¥ç¥ç»æ‰©å……å™¨æ¡†æ¶å®ç°å¤šæ¨¡æ€å¯¹æŠ—æ€§æç¤ºè°ƒä¼˜ï¼ˆNAP-Tuningï¼‰ã€‚NAP-Tuningå¼•å…¥ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šæ”¯æŒæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„å¤šæ¨¡æ€æç¤ºï¼Œç”±å•å±‚æ‰©å±•è‡³å¤šå±‚çš„æç¤ºæ¶æ„ï¼Œå¹¶æå‡ºå…¨æ–°çš„æ¶æ„çº§é‡æ–°è®¾è®¡ã€‚æˆ‘ä»¬é‡‡ç”¨ç‰¹å¾æçº¯çš„æ–¹æ³•ç›´æ¥åº”å¯¹å¯¹æŠ—æ€§æ”»å‡»å¼•èµ·çš„ç‰¹å¾ç©ºé—´æ‰­æ›²é—®é¢˜ã€‚NAP-Tuningè¿˜åŒ…æ‹¬tokenç²¾ç‚¼å™¨ï¼Œå®ƒé€šè¿‡æ®‹å·®è¿æ¥å­¦ä¹ é‡å»ºæçº¯çš„ç‰¹å¾ï¼Œå…è®¸é’ˆå¯¹ç‰¹å®šæ¨¡æ€å’Œç‰¹å®šå±‚çº§çš„ç‰¹å¾ä¿®æ­£ã€‚å®éªŒè¡¨æ˜ï¼ŒNAP-Tuningåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ”»å‡»ç±»å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨AutoAttackåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚åœ¨ViT-B16å’ŒViT-B32æ¶æ„ä¸Šåˆ†åˆ«è¶…è¶Šæœ€å¼ºåŸºçº¿æ¨¡å‹è¾¾33.5%å’Œ33.0%ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„æ¸…æ´å‡†ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMså¦‚CLIPè™½èƒ½æœ‰æ•ˆç†è§£è§†è§‰ä¸æ–‡æœ¬æ•°æ®é—´çš„å…³ç³»ï¼Œä½†é¢ä¸´å¯¹æŠ—æ€§æ”»å‡»çš„å®‰å…¨éšæ‚£ã€‚</li>
<li>NAP-Tuningåœ¨AdvPTåŸºç¡€ä¸Šè¿›è¡Œäº†æ˜¾è‘—æ‰©å±•ï¼Œæ”¯æŒå¤šæ¨¡æ€æç¤ºå’Œå¤šå±‚æç¤ºæ¶æ„ã€‚</li>
<li>NAP-Tuningå¼•å…¥ç¥ç»æ‰©å……å™¨æ¡†æ¶è¿›è¡Œæ¶æ„çº§è®¾è®¡ï¼Œé€šè¿‡ç‰¹å¾æçº¯åº”å¯¹å¯¹æŠ—æ€§æ”»å‡»ã€‚</li>
<li>NAP-TuningåŒ…å«tokenç²¾ç‚¼å™¨ï¼Œå¯å­¦ä¹ é‡å»ºæçº¯çš„ç‰¹å¾å¹¶å®ç°æ¨¡æ€å’Œå±‚çº§ç‰¹å®šçš„ç‰¹å¾ä¿®æ­£ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºNAP-Tuningåœ¨å¤šç§æ•°æ®é›†å’Œæ”»å‡»ç±»å‹ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨AutoAttackåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨ViT-B16å’ŒViT-B32æ¶æ„ä¸Šï¼ŒNAP-Tuningçš„é²æ£’æ€§æ”¹è¿›è¶…è¿‡æœ€å¼ºåŸºçº¿æ¨¡å‹è¾¾33%ä»¥ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb8d85a11e0a2a482c890f82baf47ec2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fe47b05142080d6cf8e5c7e15c01e90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd776fda3b245640acaef521b7e4be94.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-Cell-Type-Inference-in-Vision-Language-Models-Under-Varying-Visual-Context"><a href="#Evaluating-Cell-Type-Inference-in-Vision-Language-Models-Under-Varying-Visual-Context" class="headerlink" title="Evaluating Cell Type Inference in Vision Language Models Under Varying   Visual Context"></a>Evaluating Cell Type Inference in Vision Language Models Under Varying   Visual Context</h2><p><strong>Authors:Samarth Singhal, Sandeep Singhal</strong></p>
<p>Vision-Language Models (VLMs) have rapidly advanced alongside Large Language Models (LLMs). This study evaluates the capabilities of prominent generative VLMs, such as GPT-4.1 and Gemini 2.5 Pro, accessed via APIs, for histopathology image classification tasks, including cell typing. Using diverse datasets from public and private sources, we apply zero-shot and one-shot prompting methods to assess VLM performance, comparing them against custom-trained Convolutional Neural Networks (CNNs). Our findings demonstrate that while one-shot prompting significantly improves VLM performance over zero-shot ($p \approx 1.005 \times 10^{-5}$ based on Kappa scores), these general-purpose VLMs currently underperform supervised CNNs on most tasks. This work underscores both the promise and limitations of applying current VLMs to specialized domains like pathology via in-context learning. All code and instructions for reproducing the study can be accessed from the repository <a target="_blank" rel="noopener" href="https://www.github.com/a12dongithub/VLMCCE">https://www.github.com/a12dongithub/VLMCCE</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•è€Œè¿›æ­¥ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†é€šè¿‡APIè®¿é—®çš„å‰ç»æ€§ç”Ÿæˆå¼VLMsï¼Œå¦‚GPT-4.1å’ŒGemini 2.5 Proï¼Œåœ¨ç—…ç†å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ˆåŒ…æ‹¬ç»†èƒç±»å‹åˆ†ç±»ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªå…¬å…±å’Œç§æœ‰æºçš„å¤šæ ·æ•°æ®é›†ï¼Œé€šè¿‡é›¶æ ·æœ¬å’Œå•æ¬¡æ ·æœ¬æç¤ºæ–¹æ³•è¯„ä¼°VLMçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸å®šåˆ¶è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å•æ¬¡æ ·æœ¬æç¤ºæ–¹æ³•ç›¸å¯¹äºé›¶æ ·æœ¬æç¤ºæ˜¾è‘—æé«˜äº†VLMçš„æ€§èƒ½ï¼ˆåŸºäºkappaåˆ†æ•°çš„å·®å¼‚å€¼ä¸º$p \approx 1.005 \times 10^{-5}$ï¼‰ï¼Œä½†è¿™äº›é€šç”¨VLMsç›®å‰åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»åŠ£äºæœ‰ç›‘ç£å­¦ä¹ çš„CNNã€‚æœ¬ç ”ç©¶çªå‡ºäº†åœ¨å½“å‰VLMåº”ç”¨äºç—…ç†å­¦ç­‰ç‰¹æ®Šé¢†åŸŸæ—¶çš„æ½œåŠ›ä¸å±€é™æ€§ï¼Œå¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°ã€‚æ‰€æœ‰å…³äºå¤ç°è¯¥ç ”ç©¶çš„ä»£ç å’Œè¯´æ˜å¯ä»<a target="_blank" rel="noopener" href="https://www.github.com/a12dongithub/VLMCCE%E8%AE%BF%E9%97%AE%E3%80%82">https://www.github.com/a12dongithub/VLMCCEè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12683v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚GPT-4.1å’ŒGemini 2.5 Proåœ¨ç—…ç†å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ˆåŒ…æ‹¬ç»†èƒç±»å‹é‰´å®šï¼‰ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡é‡‡ç”¨æ¥è‡ªå…¬å…±å’Œç§æœ‰æºçš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œæœ¬æ–‡åˆ©ç”¨é›¶æ¬¡å­¦ä¹ å’Œä¸€æ¬¡å­¦ä¹ æç¤ºæ³•è¯„ä¼°äº†VLMçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å°†å…¶ä¸ç»è¿‡å®šåˆ¶çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ä¸€æ¬¡å­¦ä¹ æç¤ºå¯ä»¥æ˜¾è‘—æé«˜VLMçš„æ€§èƒ½è¡¨ç°ï¼Œä½†åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šï¼Œè¿™äº›é€šç”¨VLMçš„è¡¨ç°ä»ç„¶ä¸åŠç»è¿‡ç›‘ç£å­¦ä¹ çš„CNNã€‚æœ¬æ–‡å¼ºè°ƒäº†å°†å½“å‰VLMåº”ç”¨äºç—…ç†å­¦ç­‰ç‰¹æ®Šé¢†åŸŸæ—¶é¢ä¸´çš„æŒ‘æˆ˜å’Œå±€é™æ€§ã€‚æ‰€æœ‰ä»£ç å’Œå®éªŒé‡ç°æŒ‡å—å¯åœ¨<a target="_blank" rel="noopener" href="https://www.github.com/a12dongithub/VLMCCE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://www.github.com/a12dongithub/VLMCCEä¸­æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨ç—…ç†å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°è¢«è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶å¯¹è±¡åŒ…æ‹¬GPT-4.1å’ŒGemini 2.5 Proç­‰ä¸»æµç”Ÿæˆå¼VLMsã€‚</li>
<li>ä½¿ç”¨é›¶æ¬¡å’Œä¸€æ¬¡å­¦ä¹ æç¤ºæ–¹æ³•æ¥è¯„ä¼°VLMçš„æ€§èƒ½ã€‚</li>
<li>VLMsåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸åŠç»è¿‡è®­ç»ƒçš„CNNã€‚</li>
<li>ä¸€æ¬¡å­¦ä¹ æç¤ºå¯ä»¥æ˜¾è‘—æé«˜VLMçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡å¼ºè°ƒäº†å°†VLMåº”ç”¨äºç‰¹æ®Šé¢†åŸŸï¼ˆå¦‚ç—…ç†å­¦ï¼‰æ—¶çš„æŒ‘æˆ˜å’Œå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6cb924ba6a60206339dc818c1f58c2e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa8f407b30023a85743e795c0cc4f617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c31c5af1750c97c433135ab477d21682.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Deep-Feature-Fusion-and-Ensemble-Learning-for-Enhanced-Brain-Tumor-MRI-Classification"><a href="#Hierarchical-Deep-Feature-Fusion-and-Ensemble-Learning-for-Enhanced-Brain-Tumor-MRI-Classification" class="headerlink" title="Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced   Brain Tumor MRI Classification"></a>Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced   Brain Tumor MRI Classification</h2><p><strong>Authors:Zahid Ullah, Jihie Kim</strong></p>
<p>Accurate brain tumor classification is crucial in medical imaging to ensure reliable diagnosis and effective treatment planning. This study introduces a novel double ensembling framework that synergistically combines pre-trained deep learning (DL) models for feature extraction with optimized machine learning (ML) classifiers for robust classification. The framework incorporates comprehensive preprocessing and data augmentation of brain magnetic resonance images (MRI), followed by deep feature extraction using transfer learning with pre-trained Vision Transformer (ViT) networks. The novelty lies in the dual-level ensembling strategy: feature-level ensembling, which integrates deep features from the top-performing ViT models, and classifier-level ensembling, which aggregates predictions from hyperparameter-optimized ML classifiers. Experiments on two public Kaggle MRI brain tumor datasets demonstrate that this approach significantly surpasses state-of-the-art methods, underscoring the importance of feature and classifier fusion. The proposed methodology also highlights the critical roles of hyperparameter optimization (HPO) and advanced preprocessing techniques in improving diagnostic accuracy and reliability, advancing the integration of DL and ML for clinically relevant medical image analysis. </p>
<blockquote>
<p>ç²¾ç¡®çš„å¤§è„‘è‚¿ç˜¤åˆ†ç±»åœ¨åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œèƒ½ç¡®ä¿å¯é çš„è¯Šæ–­å’Œæœ‰æ•ˆçš„æ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŒé‡é›†æˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒç»“åˆäº†ç”¨äºç‰¹å¾æå–çš„é¢„è®­ç»ƒæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ä¸ç»è¿‡ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åˆ†ç±»å™¨ï¼Œä»¥å®ç°ç¨³å¥çš„åˆ†ç±»ã€‚è¯¥æ¡†æ¶åŒ…å«å¯¹å¤§è„‘ç£å…±æŒ¯å›¾åƒï¼ˆMRIï¼‰çš„ç»¼åˆé¢„å¤„ç†å’Œæ•°æ®å¢å¼ºï¼Œéšååˆ©ç”¨å€ŸåŠ©é¢„è®­ç»ƒå¥½çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ç½‘ç»œçš„è¿ç§»å­¦ä¹ è¿›è¡Œæ·±åº¦ç‰¹å¾æå–ã€‚å…¶æ–°é¢–ä¹‹å¤„åœ¨äºåŒé‡é›†æˆç­–ç•¥ï¼šç‰¹å¾çº§é›†æˆï¼Œå®ƒæ•´åˆäº†è¡¨ç°æœ€ä½³çš„ViTæ¨¡å‹çš„æ·±åº¦ç‰¹å¾ï¼›åˆ†ç±»å™¨çº§é›†æˆï¼Œå®ƒèšåˆäº†ç»è¿‡è¶…å‚æ•°ä¼˜åŒ–çš„MLåˆ†ç±»å™¨çš„é¢„æµ‹ç»“æœã€‚åœ¨ä¸¤ä¸ªå…¬å…±çš„Kaggle MRIå¤§è„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œçªæ˜¾äº†ç‰¹å¾å’Œåˆ†ç±»å™¨èåˆçš„é‡è¦æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿˜å¼ºè°ƒäº†è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰å’Œå…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯åœ¨æé«˜è¯Šæ–­å’Œåˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œæ¨åŠ¨äº†æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨ä¸´åºŠç›¸å…³çš„åŒ»å­¦å›¾åƒåˆ†æä¸­çš„èåˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12363v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹åŒé‡é›†æˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆé¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–åçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨è¿›è¡Œç¨³å¥åˆ†ç±»ã€‚é€šè¿‡ç»¼åˆè¿ç”¨ç£å…±æŒ¯å›¾åƒé¢„å¤„ç†å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé‡‡ç”¨åŸºäºé¢„è®­ç»ƒVision Transformerç½‘ç»œçš„è¿ç§»å­¦ä¹ è¿›è¡Œæ·±åº¦ç‰¹å¾æå–ã€‚å…¶æ–°é¢–ä¹‹å¤„åœ¨äºåŒé‡é›†æˆç­–ç•¥ï¼šç‰¹å¾çº§é›†æˆæ•´åˆäº†è¡¨ç°æœ€ä½³çš„Vision Transformeræ¨¡å‹çš„æ·±åº¦ç‰¹å¾ï¼Œåˆ†ç±»å™¨çº§é›†æˆåˆ™èšåˆäº†ç»è¿‡è¶…å‚æ•°ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨çš„é¢„æµ‹ç»“æœã€‚åœ¨å…¬å…±Kaggle MRIè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¼ºè°ƒäº†ç‰¹å¾å’Œåˆ†ç±»å™¨èåˆçš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†è¶…å‚æ•°ä¼˜åŒ–å’Œå…ˆè¿›é¢„å¤„ç†æŠ€æœ¯åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œæ¨åŠ¨äº†æ·±åº¦å­¦ä¹ åœ¨ä¸´åºŠåŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°å‹åŒé‡é›†æˆæ¡†æ¶ï¼Œç»“åˆäº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç‰¹å¾æå–å’Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨çš„ä¼˜åŒ–åˆ†ç±»ã€‚</li>
<li>è¯¥æ¡†æ¶è¿ç”¨äº†ç£å…±æŒ¯å›¾åƒçš„é¢„å¤„ç†å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>é‡‡ç”¨äº†åŸºäºé¢„è®­ç»ƒVision Transformerç½‘ç»œçš„è¿ç§»å­¦ä¹ è¿›è¡Œæ·±åº¦ç‰¹å¾æå–ã€‚</li>
<li>ç‰¹å¾çº§é›†æˆå’Œåˆ†ç±»å™¨çº§é›†æˆæ˜¯è¯¥æ¡†æ¶çš„åŒé‡é›†æˆç­–ç•¥çš„å…³é”®ã€‚</li>
<li>åœ¨å…¬å…±Kaggle MRIè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>è¶…å‚æ•°ä¼˜åŒ–åœ¨æå‡æ¨¡å‹æ€§èƒ½ä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f79837758e0c393a2fcb5c45c3f06fe4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Interpretable-Classification-of-Levantine-Ceramic-Thin-Sections-via-Neural-Networks"><a href="#Interpretable-Classification-of-Levantine-Ceramic-Thin-Sections-via-Neural-Networks" class="headerlink" title="Interpretable Classification of Levantine Ceramic Thin Sections via   Neural Networks"></a>Interpretable Classification of Levantine Ceramic Thin Sections via   Neural Networks</h2><p><strong>Authors:Sara Capriotti, Alessio Devoto, Simone Scardapane, Silvano Mignardi, Laura Medeghini</strong></p>
<p>Classification of ceramic thin sections is fundamental for understanding ancient pottery production techniques, provenance, and trade networks. Although effective, traditional petrographic analysis is time-consuming. This study explores the application of deep learning models, specifically Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), as complementary tools to support the classification of Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from 178 ceramic samples belonging to several archaeological sites across the Levantine area, mostly from the Bronze Age, with few samples dating to the Iron Age, was used to train and evaluate these models. The results demonstrate that transfer learning significantly improves classification performance, with a ResNet18 model achieving 92.11% accuracy and a ViT reaching 88.34%. Explainability techniques, including Guided Grad-CAM and attention maps, were applied to interpret and visualize the modelsâ€™ decisions, revealing that both CNNs and ViTs successfully focus on key mineralogical features for the classification of the samples into their respective petrographic fabrics. These findings highlight the potential of explainable AI in archaeometric studies, providing a reproducible and efficient methodology for ceramic analysis while maintaining transparency in model decision-making. </p>
<blockquote>
<p>é™¶ç“·è–„ç‰‡åˆ†ç±»å¯¹äºäº†è§£å¤ä»£é™¶å™¨ç”Ÿäº§æŠ€æœ¯ã€æ¥æºå’Œè´¸æ˜“ç½‘ç»œå…·æœ‰é‡è¦æ„ä¹‰ã€‚è™½ç„¶ä¼ ç»Ÿå²©çŸ³å­¦åˆ†ææœ‰æ•ˆï¼Œä½†è€—æ—¶è¾ƒé•¿ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œä½œä¸ºæ”¯æŒåŸºäºå²©çŸ³å­¦ç»‡æ„çš„é»å‡¡ç‰¹é™¶ç“·åˆ†ç±»çš„è¡¥å……å·¥å…·ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ¥è‡ªé»å‡¡ç‰¹åœ°åŒºå¤šä¸ªè€ƒå¤é—å€çš„178ä¸ªé™¶ç“·æ ·å“çš„è–„ç‰‡å›¾åƒæ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ï¼Œè¿™äº›æ ·å“ä¸»è¦æ¥è‡ªé’é“œæ—¶ä»£ï¼Œå°‘æ•°æ ·å“æ¥è‡ªé“å™¨æ—¶ä»£ã€‚ç»“æœè¡¨æ˜ï¼Œè¿ç§»å­¦ä¹ æ˜¾è‘—æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œå…¶ä¸­ResNet18æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†92.11%ï¼Œè€ŒViTè¾¾åˆ°äº†88.34%ã€‚é€šè¿‡æ¢¯åº¦å¼•å¯¼CAMå’Œæ³¨æ„åŠ›å›¾ç­‰è§£é‡Šæ€§æŠ€æœ¯æ¥è§£è¯»å’Œå¯è§†åŒ–æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œæ˜¾ç¤ºå‡ºCNNå’ŒViTéƒ½èƒ½æˆåŠŸå…³æ³¨åˆ°çŸ¿ç‰©å­¦ç‰¹å¾ï¼Œå°†æ ·æœ¬åˆ†ç±»åˆ°å„è‡ªçš„å²©çŸ³å­¦ç»‡æ„ä¸­ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¯è§£é‡Šçš„AIåœ¨è€ƒå¤è®¡é‡ç ”ç©¶ä¸­çš„æ½œåŠ›ï¼Œæä¾›äº†ä¸€ç§å¯é‡å¤å’Œé«˜æ•ˆçš„é™¶ç“·åˆ†ææ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹å†³ç­–çš„é€æ˜åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12250v1">PDF</a> Accepted for publication in Machine Learning: Science and Technology</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶åº”ç”¨æ·±åº¦å­¦ä¹ å’Œè§†è§‰è½¬æ¢å™¨æŠ€æœ¯è¾…åŠ©åˆ†ç±»Levantineé™¶å™¨è–„åˆ‡ç‰‡ã€‚åˆ©ç”¨æ¥è‡ªå¤šä¸ªè€ƒå¤é—å€çš„é™¶å™¨æ ·æœ¬å›¾åƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå‘ç°è¿ç§»å­¦ä¹ å¯æ˜¾è‘—æé«˜åˆ†ç±»æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒResNet18æ¨¡å‹å‡†ç¡®ç‡ä¸º92.11%ï¼Œè€ŒVision Transformerå‡†ç¡®ç‡ä¸º88.34%ã€‚è§£é‡ŠæŠ€æœ¯æ­ç¤ºäº†æ¨¡å‹å†³ç­–çš„å…³é”®çŸ¿ç‰©å­¦ç‰¹å¾ï¼Œæ˜¾ç¤ºå‡ºäººå·¥æ™ºèƒ½åœ¨è€ƒå¤åº¦é‡å­¦ç ”ç©¶ä¸­åº”ç”¨çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºé™¶ç“·åˆ†ææä¾›äº†å¯é‡å¤ã€é«˜æ•ˆçš„æ£€æµ‹æ–¹å¼å¹¶ä¿æŒå†³ç­–é€æ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ å’Œè§†è§‰è½¬æ¢å™¨æŠ€æœ¯è¾…åŠ©åˆ†ç±»Levantineé™¶ç“·è–„åˆ‡ç‰‡ï¼Œæ¢ç´¢äº†æ–°å…´æŠ€æœ¯åœ¨è€ƒå¤ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>é‡‡ç”¨åŒ…å«å¤šä¸ªé—å€çš„é™¶ç“·æ ·æœ¬æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œæå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ€§èƒ½è¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡è¿ç§»å­¦ä¹ æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼ŒResNet18æ¨¡å‹å’ŒVision Transformeråˆ†åˆ«å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>è§£é‡ŠæŠ€æœ¯æ­ç¤ºäº†æ¨¡å‹åœ¨åˆ†ç±»è¿‡ç¨‹ä¸­å…³æ³¨çš„å…³é”®çŸ¿ç‰©å­¦ç‰¹å¾ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ¨¡å‹çš„åº”ç”¨æä¾›äº†å¯é‡å¤å’Œé«˜æ•ˆçš„é™¶ç“·åˆ†ææ–¹æ³•ï¼Œä¿ƒè¿›äº†é™¶ç“·åˆ†æçš„è‡ªåŠ¨åŒ–è¿›ç¨‹ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨ä¿æŒå†³ç­–é€æ˜æ€§çš„åŒæ—¶å®ç°äº†è‡ªåŠ¨åŒ–å†³ç­–çš„åº”ç”¨ï¼Œå¯¹äºé™¶ç“·åˆ†ç±»çš„ç²¾å‡†åº¦å’Œå®¢è§‚æ€§æœ‰æ‰€ä¿éšœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18b93a6c2512d3472ddaf4dba385554e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-837bde8d0b4595a3b7498140c3918332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82c67c544190afe154e90e71da28c554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf3709458a469e38a863fd9c9bfb2a1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction"><a href="#BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction" class="headerlink" title="BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction"></a>BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction</h2><p><strong>Authors:Naomi Fridman, Bubby Solway, Tomer Fridman, Itamar Barnea, Anat Goldshtein</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+&#x2F;HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ—©æœŸæ£€æµ‹å’Œç²¾ç¡®çš„æ²»ç–—ååº”ç›‘æµ‹æˆä¸ºè‡³å…³é‡è¦çš„ä¼˜å…ˆäº‹é¡¹ã€‚æˆ‘ä»¬æ¨å‡ºäº†BreastDCEDLï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾é€‰çš„ã€å‡†å¤‡å¥½ç”¨äºæ·±åº¦å­¦ä¹ æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeé˜Ÿåˆ—çš„2070åä¹³è…ºç™Œæ‚£è€…çš„æ²»ç–—å‰3DåŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰æ‰«æï¼Œæ‰€æœ‰æ•°æ®æºå‡æ¥è‡ªç™Œç—‡æˆåƒæ¡£æ¡ˆã€‚åŸå§‹çš„DICOMæˆåƒæ•°æ®è¢«ä¸¥æ ¼è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯ï¼Œä¿ç•™äº†ä¿¡å·å®Œæ•´æ€§ï¼Œå¹¶é…æœ‰ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰ã€æ¿€ç´ å—ä½“ï¼ˆHRï¼‰å’ŒHER2çŠ¶æ€ã€‚å°½ç®¡DCE-MRIæä¾›äº†é‡è¦çš„è¯Šæ–­ä¿¡æ¯ï¼Œæ·±åº¦å­¦ä¹ åœ¨åˆ†ææ­¤ç±»å¤æ‚æ•°æ®æ–¹é¢æä¾›äº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹å¯è®¿é—®çš„ã€å…¬å¼€çš„ã€å¤šä¸­å¿ƒæ•°æ®é›†ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é™åˆ¶ã€‚BreastDCEDLé€šè¿‡æ”¯æŒå¼€å‘å…ˆè¿›æ¨¡å‹æ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒåŒ…æ‹¬éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„æœ€æ–°å˜å‹å™¨æ¶æ„ã€‚ä¸ºäº†å±•ç¤ºå…¶ç¨³å¥å»ºæ¨¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºå˜å‹å™¨çš„é¦–ä¸ªä¹³è…ºç™ŒDCE-MRIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨åœ¨ä¸‰ä¸ªå¯¹æ¯”é˜¶æ®µï¼ˆé¢„å¯¹æ¯”ã€æ—©æœŸå¯¹æ¯”åå’Œæ™šæœŸå¯¹æ¯”åï¼‰çš„RGBèåˆå›¾åƒä¸Šè®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¶æ„ã€‚æˆ‘ä»¬çš„ViTæ¨¡å‹åœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­å®ç°äº†æœ€å…ˆè¿›çš„pCRé¢„æµ‹æ€§èƒ½ï¼ˆAUC 0.94ï¼Œå‡†ç¡®ç‡0.93ï¼‰ã€‚BreastDCEDLåŒ…æ‹¬é¢„å®šä¹‰çš„åŸºå‡†åˆ†å‰²ï¼Œä¸ºå¯é‡å¤ç ”ç©¶æä¾›äº†æ¡†æ¶ï¼Œå¹¶åœ¨ä¹³è…ºç™Œæˆåƒä¸­å®ç°äº†ä¸´åºŠæ„ä¹‰çš„å»ºæ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12190v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¹³è…ºç™Œä»æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸæ£€æµ‹å’Œå‡†ç¡®çš„æ²»ç–—ååº”ç›‘æµ‹æ˜¯å…³é”®ã€‚æˆ‘ä»¬æ¨å‡ºBreastDCEDLæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeç­‰æ¥æºçš„ç™Œç—‡æˆåƒæ¡£æ¡ˆçš„2070ä¾‹ä¹³è…ºç™Œæ‚£è€…çš„é¢„æ²»ç–—ä¸‰ç»´åŠ¨æ€å¢å¼ºMRIæ‰«ææ•°æ®ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ã€‚åˆ©ç”¨è¿™ä¸€æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºVision Transformerï¼ˆViTï¼‰æ¶æ„çš„æ¨¡å‹ï¼Œå¯¹ä¸‰ç§å¯¹æ¯”é˜¶æ®µçš„RGBèåˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†åœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­pCRé¢„æµ‹çš„å“è¶Šæ€§èƒ½ã€‚BreastDCEDLåŒ…æ‹¬é¢„è®¾çš„åŸºå‡†åˆ†å‰²ï¼Œä¸ºå¯é‡å¤çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¹¶èƒ½åœ¨ä¹³è…ºç™Œæˆåƒä¸­è¿›è¡Œä¸´åºŠæ„ä¹‰çš„å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BreastDCEDLæ˜¯ä¸€ä¸ªä¸“ä¸ºæ·±åº¦å­¦ä¹ è®¾è®¡çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¤šä¸ªæ¥æºçš„ä¹³è…ºç™Œæ‚£è€…çš„é¢„æ²»ç–—ä¸‰ç»´åŠ¨æ€å¢å¼ºMRIæ‰«ææ•°æ®ã€‚</li>
<li>æ•°æ®é›†åŒ…æ‹¬æ ‡å‡†åŒ–çš„ä¸‰ç»´NIfTIä½“ç§¯æ•°æ®ã€ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ã€‚</li>
<li>ç¼ºä¹å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†é™åˆ¶äº†æ·±åº¦å­¦ä¹ åœ¨DCE-MRIæ•°æ®åˆ†ææ–¹é¢çš„è¿›å±•ï¼Œè€ŒBreastDCEDLå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚</li>
<li>åˆ©ç”¨Vision Transformerï¼ˆViTï¼‰æ¶æ„å¼€å‘äº†ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œç”¨äºå¤„ç†ä¹³è…ºç™ŒDCE-MRIæ•°æ®ï¼Œå®ç°äº†åœ¨ç‰¹å®šæ‚£è€…ç¾¤ä½“ä¸­pCRé¢„æµ‹çš„é«˜æ€§èƒ½è¡¨ç°ã€‚</li>
<li>BreastDCEDLè®¾å®šçš„åŸºå‡†åˆ†å‰²æœ‰åŠ©äºè¿›è¡Œå¯é‡å¤çš„ç ”ç©¶ã€‚</li>
<li>è¯¥æ•°æ®é›†æ”¯æŒé«˜çº§æ¨¡å‹çš„å¼€å‘ï¼Œå¹¶æœ‰åŠ©äºå®ç°ä¹³è…ºç™Œæˆåƒçš„ä¸´åºŠæ„ä¹‰å»ºæ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2191b7282780ed206572eca848d56daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7dad6459b40dbbb6956d89e57e1f6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afe39d0b5fcd50e887b1eddacca0241d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09dc62f96e8a397c731928d624089bbd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PerFormer-A-Permutation-Based-Vision-Transformer-for-Remaining-Useful-Life-Prediction"><a href="#PerFormer-A-Permutation-Based-Vision-Transformer-for-Remaining-Useful-Life-Prediction" class="headerlink" title="PerFormer: A Permutation Based Vision Transformer for Remaining Useful   Life Prediction"></a>PerFormer: A Permutation Based Vision Transformer for Remaining Useful   Life Prediction</h2><p><strong>Authors:Zhengyang Fan, Wanru Li, Kuo-chu Chang, Ting Yuan</strong></p>
<p>Accurately estimating the remaining useful life (RUL) for degradation systems is crucial in modern prognostic and health management (PHM). Convolutional Neural Networks (CNNs), initially developed for tasks like image and video recognition, have proven highly effectively in RUL prediction, demonstrating remarkable performance. However, with the emergence of the Vision Transformer (ViT), a Transformer model tailored for computer vision tasks such as image classification, and its demonstrated superiority over CNNs, there is a natural inclination to explore its potential in enhancing RUL prediction accuracy. Nonetheless, applying ViT directly to multivariate sensor data for RUL prediction poses challenges, primarily due to the ambiguous nature of spatial information in time series data. To address this issue, we introduce the PerFormer, a permutation-based vision transformer approach designed to permute multivariate time series data, mimicking spatial characteristics akin to image data, thereby making it suitable for ViT. To generate the desired permutation matrix, we introduce a novel permutation loss function aimed at guiding the convergence of any matrix towards a permutation matrix. Our experiments on NASAâ€™s C-MAPSS dataset demonstrate the PerFormerâ€™s superior performance in RUL prediction compared to state-of-the-art methods employing CNNs, Recurrent Neural Networks (RNNs), and various Transformer models. This underscores its effectiveness and potential in PHM applications. </p>
<blockquote>
<p>å¯¹é€€åŒ–ç³»ç»Ÿçš„å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰è¿›è¡Œå‡†ç¡®ä¼°è®¡æ˜¯ç°ä»£é¢„æµ‹ä¸å¥åº·ç®¡ç†ï¼ˆPHMï¼‰ä¸­çš„å…³é”®ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æœ€åˆæ˜¯ä¸ºå›¾åƒå’Œè§†é¢‘è¯†åˆ«ç­‰ä»»åŠ¡è€Œå¼€å‘çš„ï¼Œå…¶åœ¨RULé¢„æµ‹ä¸­è¡¨ç°å‡ºäº†é«˜åº¦çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€ä¸“ä¸ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡å¦‚å›¾åƒåˆ†ç±»è€Œå®šåˆ¶çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„å‡ºç°ï¼Œä»¥åŠå…¶ç›¸å¯¹äºCNNçš„ä¼˜è¶Šæ€§ï¼Œäººä»¬è‡ªç„¶å€¾å‘äºæ¢ç´¢å…¶åœ¨æé«˜RULé¢„æµ‹ç²¾åº¦æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†ViTåº”ç”¨äºå¤šå…ƒä¼ æ„Ÿå™¨æ•°æ®çš„RULé¢„æµ‹é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ—¶é—´åºåˆ—æ•°æ®ä¸­ç©ºé—´ä¿¡æ¯çš„æ¨¡ç³Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PerFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ’åˆ—çš„è§†è§‰å˜å‹å™¨æ–¹æ³•ï¼Œæ—¨åœ¨æ’åˆ—å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¨¡ä»¿ç±»ä¼¼äºå›¾åƒæ•°æ®çš„ç©ºé—´ç‰¹å¾ï¼Œä»è€Œä½¿å…¶é€‚åˆViTã€‚ä¸ºäº†ç”Ÿæˆæ‰€éœ€çš„æ’åˆ—çŸ©é˜µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ’åˆ—æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨å¼•å¯¼ä»»ä½•çŸ©é˜µå‘æ’åˆ—çŸ©é˜µæ”¶æ•›ã€‚æˆ‘ä»¬åœ¨NASAçš„C-MAPSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPerFormeråœ¨RULé¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºé‡‡ç”¨CNNã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå„ç§Transformeræ¨¡å‹çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™çªæ˜¾äº†å…¶åœ¨PHMåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00259v2">PDF</a> One of the coauthors does not want to post current version of paper,   and insists to withdraw the submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰é¢„æµ‹åœ¨ç°ä»£é¢„æµ‹ä¸å¥åº·ç®¡ç†ï¼ˆPHMï¼‰ä¸­çš„é‡è¦æ€§ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨RULé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œä½†éšç€ä¸“ä¸ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡è®¾è®¡çš„Vision Transformerï¼ˆViTï¼‰çš„å‡ºç°ï¼Œå…¶ä¼˜è¶Šæ€§å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†PerFormerï¼Œä¸€ç§åŸºäºæ’åˆ—çš„æ„¿æ™¯è½¬æ¢å™¨æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®æ’åˆ—æˆç±»ä¼¼å›¾åƒæ•°æ®çš„å½¢å¼ï¼Œä»è€Œé€‚ç”¨äºViTã€‚é€šè¿‡å¼•å…¥æ–°çš„æ’åˆ—æŸå¤±å‡½æ•°ï¼Œç”Ÿæˆæ‰€éœ€çš„æ’åˆ—çŸ©é˜µï¼Œå®ç°RULé¢„æµ‹æ€§èƒ½çš„æå‡ã€‚åœ¨NASAçš„C-MAPSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPerFormerç›¸è¾ƒäºé‡‡ç”¨CNNå’ŒRNNç­‰å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰é¢„æµ‹åœ¨ç°ä»£é¢„æµ‹ä¸å¥åº·ç®¡ç†ï¼ˆPHMï¼‰ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨RULé¢„æµ‹æ–¹é¢å·²è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰çš„å‡ºç°å¼•èµ·äº†åœ¨RULé¢„æµ‹ä¸­çš„å…³æ³¨ï¼Œå› å…¶å¯¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>PerFormeræ˜¯ä¸€ç§åŸºäºæ’åˆ—çš„æ„¿æ™¯è½¬æ¢å™¨æ–¹æ³•ï¼Œèƒ½å°†å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®æ’åˆ—æˆç±»ä¼¼å›¾åƒæ•°æ®çš„å½¢å¼ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ–°çš„æ’åˆ—æŸå¤±å‡½æ•°ï¼Œç”Ÿæˆæ’åˆ—çŸ©é˜µï¼Œæœ‰åŠ©äºæé«˜RULé¢„æµ‹æ€§èƒ½ã€‚</li>
<li>åœ¨NASAçš„C-MAPSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPerFormeråœ¨RULé¢„æµ‹æ–¹é¢ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e19218d3784cb84ea0372c84aff13cf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07987e73c12645a9a675a02d2ca34332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf163ed0a641d652e910aa5257af767a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d7e68fce904fa3cee18e5ac80262b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b222b0f4028bfb38e3c6700149875244.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dc4ba1cceef39874058b23060cc9934.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18f5e1ae723051de8e251f53be469cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efbadda203516f295385634ce64ffe3d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-Intermediate-Features-of-Vision-Transformer-for-Face-Anti-Spoofing"><a href="#Leveraging-Intermediate-Features-of-Vision-Transformer-for-Face-Anti-Spoofing" class="headerlink" title="Leveraging Intermediate Features of Vision Transformer for Face   Anti-Spoofing"></a>Leveraging Intermediate Features of Vision Transformer for Face   Anti-Spoofing</h2><p><strong>Authors:Mika Feng, Koichi Ito, Takafumi Aoki, Tetsushi Ohki, Masakatsu Nishigaki</strong></p>
<p>Face recognition systems are designed to be robust against changes in head pose, illumination, and blurring during image capture. If a malicious person presents a face photo of the registered user, they may bypass the authentication process illegally. Such spoofing attacks need to be detected before face recognition. In this paper, we propose a spoofing attack detection method based on Vision Transformer (ViT) to detect minute differences between live and spoofed face images. The proposed method utilizes the intermediate features of ViT, which have a good balance between local and global features that are important for spoofing attack detection, for calculating loss in training and score in inference. The proposed method also introduces two data augmentation methods: face anti-spoofing data augmentation and patch-wise data augmentation, to improve the accuracy of spoofing attack detection. We demonstrate the effectiveness of the proposed method through experiments using the OULU-NPU and SiW datasets. The project page is available at: <a target="_blank" rel="noopener" href="https://gsisaoki.github.io/FAS-ViT-CVPRW/">https://gsisaoki.github.io/FAS-ViT-CVPRW/</a> . </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿè®¾è®¡æ—¶è€ƒè™‘äº†å¤´éƒ¨å§¿æ€ã€å…‰ç…§å’Œå›¾åƒæ•æ‰è¿‡ç¨‹ä¸­çš„æ¨¡ç³Šå˜åŒ–ã€‚å¦‚æœæ¶æ„äººå£«å‡ºç¤ºæ³¨å†Œç”¨æˆ·çš„é¢éƒ¨ç…§ç‰‡ï¼Œä»–ä»¬å¯èƒ½ä¼šéæ³•ç»•è¿‡èº«ä»½éªŒè¯è¿‡ç¨‹ã€‚éœ€è¦åœ¨äººè„¸è¯†åˆ«ä¹‹å‰æ£€æµ‹æ­¤ç±»æ¬ºéª—æ”»å‡»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¬ºéª—æ”»å‡»æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å®æ—¶å’Œä¼ªé€ é¢éƒ¨å›¾åƒä¹‹é—´çš„å¾®å°å·®å¼‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ViTçš„ä¸­é—´ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åœ¨å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¹‹é—´æœ‰å¾ˆå¥½çš„å¹³è¡¡ï¼Œå¯¹äºæ¬ºéª—æ”»å‡»æ£€æµ‹å¾ˆé‡è¦ï¼Œç”¨äºè®¡ç®—è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå¾—åˆ†ã€‚è¯¥æ–¹æ³•è¿˜å¼•å…¥ä¸¤ç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼šé¢éƒ¨é˜²æ¬ºéª—æ•°æ®å¢å¼ºå’Œè¡¥ä¸çº§æ•°æ®å¢å¼ºï¼Œä»¥æé«˜æ¬ºéª—æ”»å‡»æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨OULU-NPUå’ŒSiWæ•°æ®é›†çš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://gsisaoki.github.io/FAS-ViT-CVPRW/">https://gsisaoki.github.io/FAS-ViT-CVPRW/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24402v2">PDF</a> 2025 IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition   Workshops (CVPRW)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºVision Transformerï¼ˆViTï¼‰çš„é˜²ä¼ªæ”»å‡»æ£€æµ‹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«çœŸå®ä¸å‡å†’äººè„¸å›¾åƒé—´çš„ç»†å¾®å·®å¼‚ï¼Œé€šè¿‡åˆ©ç”¨ViTçš„ä¸­é—´ç‰¹å¾è¿›è¡Œè®­ç»ƒä¸æ¨ç†ï¼Œå®ç°é˜²ä¼ªæ”»å‡»æ£€æµ‹ã€‚æ­¤æ³•å¼•å…¥ä¸¤ç§æ•°æ®å¢å¼ºæ–¹æ³•ä»¥æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚å®éªŒåœ¨OULU-NPUå’ŒSiWæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•åŸºäºVision Transformerï¼ˆViTï¼‰è®¾è®¡ï¼Œç”¨äºæ£€æµ‹çœŸå®å’Œå‡å†’äººè„¸å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>ä½¿ç”¨ViTçš„ä¸­é—´ç‰¹å¾è¿›è¡Œè®­ç»ƒä¸æ¨ç†ï¼Œè¿™äº›ç‰¹å¾åœ¨æœ¬åœ°å’Œå…¨å±€ç‰¹å¾ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œå¯¹é˜²ä¼ªæ”»å‡»æ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥ä¸¤ç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼šé¢éƒ¨é˜²ä¼ªæ•°æ®å¢å¼ºå’Œè¡¥ä¸çº§æ•°æ®å¢å¼ºï¼Œä»¥æé«˜é˜²ä¼ªæ”»å‡»æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨OULU-NPUå’ŒSiWæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨é˜²æ­¢æ¶æ„äººå£«é€šè¿‡ä¼ªé€ ç”¨æˆ·ç…§ç‰‡ç»•è¿‡é¢éƒ¨è¯†åˆ«è®¤è¯è¿‡ç¨‹ã€‚</li>
<li>è¯¥é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š<a target="_blank" rel="noopener" href="https://gsisaoki.github.io/FAS-ViT-CVPRW/">é¡¹ç›®é“¾æ¥</a>ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†Vision Transformeråœ¨é˜²ä¼ªæ”»å‡»æ£€æµ‹é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f4be1981dc972e0611871837e021373.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c110c5c424496bb0a5a9e7e9b7cd8afb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2111b5d1debe84295f55e222e4a342f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475527f8d0141e8745e1cf2976309410.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f4b8b55483559cb9d3fe6bb4982a997.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Egocentric Human-Object Interaction Detection A New Benchmark and   Method
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eb7ecb96a18e3cb871365015eeae8d4b.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  EVA02-AT Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
