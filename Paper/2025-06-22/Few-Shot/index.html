<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-22  SynPo Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d37b365832bd7f8ebcf6587df784a41c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    77 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-22-更新"><a href="#2025-06-22-更新" class="headerlink" title="2025-06-22 更新"></a>2025-06-22 更新</h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>大型视觉模型（LVMs）的出现为少数医学图像分割提供了新的机会。然而，基于LVMs的无训练方法无法有效利用负提示，导致在低对比度医学图像上的性能不佳。为了解决这一问题，我们提出了SynPo，这是一种基于LVMs的无训练少数方法（例如SAM），其核心见解是提高负提示的质量。为了在更可靠的置信图上选择点提示，我们结合DINOv2和SAM的优点，设计了一种新颖的置信图协同模块。基于置信图，我们选择前k个像素作为正点集，使用高斯分布选择负点集，然后对这两个集合分别进行独立的K-均值聚类。然后，这些选定的点被用作高质量提示，用于SAM获得分割结果。大量实验表明，SynPo的性能与基于训练的最先进少数方法相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型视觉模型（LVMs）的无训练医学图像分割方法面临无法有效利用负提示的问题，导致在低对比度医学图像上的表现不佳。为解决此问题，本文提出了名为SynPo的无训练少样本方法，核心在于改进负提示的质量。通过结合DINOv2和SAM的优点，设计了一种新型置信图协同模块，用于更可靠地选择点提示。实验表明，SynPo的性能与基于训练的最先进少样本方法相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉模型（LVMs）在少样本医学图像分割上具有新机遇。</li>
<li>现有基于LVMs的无训练方法无法有效利用负提示。</li>
<li>SynPo方法基于LVMs提出，核心在于改进负提示的质量。</li>
<li>设计了新型置信图协同模块，结合DINOv2和SAM的优点。</li>
<li>通过置信图选择正负点集，利用高斯分布和独立K-means聚类进行选择。</li>
<li>选定的点被用作高质量提示，用于SAM获得分割结果。</li>
<li>实验表明SynPo性能与基于训练的方法相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3cbb2811ca359b2646838708d0194be4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Chat-to-Checkup-Can-Large-Language-Models-Assist-in-Diabetes-Prediction"><a href="#From-Chat-to-Checkup-Can-Large-Language-Models-Assist-in-Diabetes-Prediction" class="headerlink" title="From Chat to Checkup: Can Large Language Models Assist in Diabetes   Prediction?"></a>From Chat to Checkup: Can Large Language Models Assist in Diabetes   Prediction?</h2><p><strong>Authors:Shadman Sakib, Oishy Fatema Akhand, Ajwad Abrar</strong></p>
<p>While Machine Learning (ML) and Deep Learning (DL) models have been widely used for diabetes prediction, the use of Large Language Models (LLMs) for structured numerical data is still not well explored. In this study, we test the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and three-shot prompting methods. We conduct an empirical analysis using the Pima Indian Diabetes Database (PIDD). We evaluate six LLMs, including four open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we compare their performance with three traditional machine learning models: Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use accuracy, precision, recall, and F1-score as evaluation metrics. Our results show that proprietary LLMs perform better than open-source ones, with GPT-4o and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably, Gemma-2-27B also outperforms the traditional ML models in terms of F1-score. However, there are still issues such as performance variation across prompting strategies and the need for domain-specific fine-tuning. This study shows that LLMs can be useful for medical prediction tasks and encourages future work on prompt engineering and hybrid approaches to improve healthcare predictions. </p>
<blockquote>
<p>虽然机器学习和深度学习模型在糖尿病预测方面得到了广泛应用，但大型语言模型在处理结构化数值数据方面的应用尚未得到充分探索。在这项研究中，我们测试了大型语言模型在零样本、单样本和三样本提示方法下预测糖尿病的有效性。我们使用皮马印第安糖尿病数据库进行实证分析。我们评估了六种大型语言模型，包括四个开源模型：Gemma-2-27B、Mistral-7B、Llama-3.1-8B和Llama-3.2-2B。我们还测试了两个专有模型：GPT-4o和Gemini Flash 2.0。此外，我们将它们的性能与三种传统机器学习模型进行比较：随机森林、逻辑回归和支持向量机（SVM）。我们使用准确性、精确性、召回率和F1分数作为评估指标。我们的结果表明，专有大型语言模型的性能优于开源模型，GPT-4o和Gemma-2-27B在少样本设置中实现了最高精度。值得注意的是，在F1分数方面，Gemma-2-27B也优于传统的机器学习模型。然而，仍然存在一些问题，如不同提示策略之间的性能差异和需要针对领域的微调。这项研究表明，大型语言模型可以用于医疗预测任务，并鼓励未来在提示工程和混合方法方面进行工作，以改善医疗保健预测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14949v1">PDF</a> Accepted in 1st IEEE QPAIN 2025</p>
<p><strong>摘要</strong></p>
<p>本研究探讨了大型语言模型（LLMs）在糖尿病预测中的应用效果。研究采用零样本、一样本和三样本提示方法，使用Pima印第安糖尿病数据库（PIDD）进行实证分析。对比了六种LLMs模型（包括四个开源模型和两个专有模型）与三种传统机器学习任务模型（随机森林、逻辑回归和支持向量机）的性能。结果显示，专有LLMs模型表现优于开源模型，GPT-4o和Gemma-2-27B在少样本设置中具有最高准确性。Gemma-2-27B在F1分数方面优于传统ML模型。但仍存在提示策略性能差异和需要特定领域的微调等问题。研究证明了LLMs在医疗预测任务中的潜力，并鼓励未来在提示工程和混合方法上进行改进以提高医疗预测的准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLMs）在糖尿病预测中的应用尚处于初步探索阶段。</li>
<li>专有LLMs模型性能通常优于开源模型。</li>
<li>GPT-4o和Gemma-2-27B在少样本环境中表现出最高准确性。</li>
<li>Gemma-2-27B在F1分数方面优于传统机器学习任务模型。</li>
<li>LLMs在医疗预测任务中具有潜力。</li>
<li>目前仍存在提示策略性能差异和需要特定领域的微调等挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14949">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-17d8a43fff527eb32915d70c8c3f798e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90001f8ebf635050e7a69c37ae0ae9c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c4841b021ca577653f4dd1036e028a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-993fb5f8ce288ce64a67a85e1b6fdcd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56eb40665ce05f2f9ca7dd1e6a59d174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c65204d4d5379f80f9c2cd5a9ba6d9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-834f520c7a25657fe1b3d23d5073cfc6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PictSure-Pretraining-Embeddings-Matters-for-In-Context-Learning-Image-Classifiers"><a href="#PictSure-Pretraining-Embeddings-Matters-for-In-Context-Learning-Image-Classifiers" class="headerlink" title="PictSure: Pretraining Embeddings Matters for In-Context Learning Image   Classifiers"></a>PictSure: Pretraining Embeddings Matters for In-Context Learning Image   Classifiers</h2><p><strong>Authors:Lukas Schiesser, Cornelius Wolff, Sophie Haas, Simon Pukrop</strong></p>
<p>Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model – its architecture, pretraining, and training dynamics – at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at <a target="_blank" rel="noopener" href="https://github.com/PictSure/pictsure-library">https://github.com/PictSure/pictsure-library</a>. </p>
<blockquote>
<p>在数据稀缺的领域中，构建图像分类模型仍然是一项艰巨的任务，因为在此类情况下收集大量带标签的数据集并不实际。情境学习（ICL）作为一种有前景的模式，在少样本图像分类（FSIC）中脱颖而出，它能够让模型在跨领域情况下进行泛化，而无需基于梯度的调整。然而，之前的工作大多忽略了基于ICL的FSIC管道中的一个关键组成部分：图像嵌入的作用。在这项工作中，我们提出了PictSure，这是一个以嵌入模型为中心的ICL框架，涵盖了其架构、预训练和培训动态。我们系统地研究了不同视觉编码器类型、预训练目标和微调策略对下游FSIC性能的影响。我们的实验表明，训练成功和跨领域性能在很大程度上取决于嵌入模型的预训练方式。因此，PictSure成功地在与训练分布差异显著的跨领域基准测试上超越了现有的基于ICL的FSIC模型，同时在内部任务上维持了相当的结果。代码可在<a target="_blank" rel="noopener" href="https://github.com/PictSure/pictsure-library%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PictSure/pictsure-library找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14842v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>在数据稀缺领域，构建图像分类模型仍然是一项艰巨的任务，因为在此类情况下收集大量有标签的数据集并不实用。情境学习（ICL）作为一种有前景的模式，在少量图像分类（FSIC）中展现出巨大的潜力，它使模型能够在无需梯度调整的情况下跨领域进行推广。然而，先前的研究在很大程度上忽视了ICL基于FSIC管道的关键组成部分——图像嵌入的作用。在本次研究中，我们推出了PictSure，一种以嵌入模型为中心的ICL框架，深入分析了其架构、预训练和训练动态等方面的影响。我们系统地研究了不同视觉编码器类型、预训练目标和微调策略对下游FSIC性能的影响。实验表明，训练成功度和跨域性能在很大程度上取决于嵌入模型的预训练方式。因此，PictSure在训练分布差异显著的跨域基准测试上表现优于现有的ICL基于FSIC模型，同时在域内任务上保持相当的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICL（In-context Learning）在少量图像分类（FSIC）中展现出巨大潜力，尤其在数据稀缺领域。</li>
<li>PictSure是一个以嵌入模型为中心的ICL框架，注重分析图像分类模型中的架构、预训练和训练动态等方面。</li>
<li>实验显示不同视觉编码器类型、预训练目标和微调策略对下游FSIC性能影响显著。</li>
<li>训练成功度和跨域性能高度依赖于嵌入模型的预训练方式。</li>
<li>PictSure在跨域基准测试上的表现优于现有ICL-based FSIC模型。</li>
<li>PictSure在维持域内任务性能的同时，实现了跨域性能的显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-777463465b46abeeeced774108ba5353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a36ef25757d1f3803674b416d9016f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-240dea26c73071e055feb3ffdb858b9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a0bbc685fa78ad1e3f8a3b8a8a5772.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Treasure-Hunt-Real-time-Targeting-of-the-Long-Tail-using-Training-Time-Markers"><a href="#Treasure-Hunt-Real-time-Targeting-of-the-Long-Tail-using-Training-Time-Markers" class="headerlink" title="Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time   Markers"></a>Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time   Markers</h2><p><strong>Authors:Daniel D’souza, Julia Kreutzer, Adrien Morisot, Ahmet Üstün, Sara Hooker</strong></p>
<p>One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: “Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?” We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations. </p>
<blockquote>
<p>现代机器学习面临的最深刻挑战之一是实现对罕见和代表性不足的特征的长尾表现良好。大型通用模型经过针对许多任务的训练，但在高频用例上表现最佳。训练后，很难使模型适应在训练语料库中代表性不足的具体用例上表现良好。依赖提示工程或少数样本示例来最大化特定测试用例的输出质量可能会令人沮丧，因为模型可能会对微小变化高度敏感，产生不可预测的反应，或者依赖固定的系统提示来维持性能。在这项工作中，我们提出的问题是：“我们是否可以优化我们的训练协议，以在推理时提高可控性和在代表性不足的用例上的性能？”我们重新审视了训练和推理技术之间的界限，以提高长尾性能，同时为用户提供一系列控制杆，该模型被训练成对控制杆做出反应。我们创建了数据特性和任务来源的详细分类，以明确控制生成属性和在推理时隐式地调节生成。我们对基础模型进行微调，以自动推断这些标记，使其在推理时变得可选。这种有原则且灵活的方法在性能上产生了显著改进，特别是在训练分布的长尾示例中。我们观察到开放式生成质量平均提升了5.7%的胜率使用我们的标记，在代表性不足的领域里，我们看到了超过9.1%的增幅。在代表性不足的任务（如代码修复）上，我们还观察到相对提升了高达14.1%，在遵循长度指令的评估上实现了绝对改善35.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨现代机器学习面临的一个重大挑战，即如何在罕见和代表性不足的特征的长尾部分表现良好。大型通用模型虽然经过多项任务训练，但在高频用例上表现最佳。训练后，难以适应模型以在训练语料库中代表性不足的特定用例上表现良好。本文旨在优化训练协议，以提高在代表性不足用例上的可控性和性能。通过重新审视训练和推理技术，我们在提高长尾性能的同时，为用户提供一系列控制杆，使模型在训练时能对一系列控制杠杆作出响应。我们创建了一个详细的数据特性和任务来源分类法，以在推理时显式控制生成属性和隐式条件生成。我们对基础模型进行微调，以自动推断这些标记，使其在推理时变得可选。这种有原则且灵活的方法在性能上产生了显著的提升，特别是在训练分布的长尾示例中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代机器学习面临在罕见和代表性不足的特征上的挑战。</li>
<li>大型通用模型在高频用例上表现最佳，但在特定用例上难以适应。</li>
<li>通过优化训练协议，可以提高在代表性不足用例上的可控性和性能。</li>
<li>提供了对训练和推理技术的重新审视，以提高长尾性能。</li>
<li>创建了一个详细的数据特性和任务来源分类法，以控制生成属性和条件生成。</li>
<li>通过微调基础模型，可以自动推断用于控制生成属性的标记。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-437b0f37b38bd8debcec48a0ae2a31da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-540405923942c8489abd9e8943b9dd9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2c7affc4d2cf89f21c7185713a51477.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0818a884839b17459841f3827673bd66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2200594ccd871c4387db470f6cf18d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeMaster-Training-Time-Series-Multimodal-LLMs-to-Reason-via-Reinforcement-Learning"><a href="#TimeMaster-Training-Time-Series-Multimodal-LLMs-to-Reason-via-Reinforcement-Learning" class="headerlink" title="TimeMaster: Training Time-Series Multimodal LLMs to Reason via   Reinforcement Learning"></a>TimeMaster: Training Time-Series Multimodal LLMs to Reason via   Reinforcement Learning</h2><p><strong>Authors:Junru Zhang, Lang Feng, Xu Guo, Yuhan Wu, Yabo Dong, Duanqing Xu</strong></p>
<p>Time-series reasoning remains a significant challenge in multimodal large language models (MLLMs) due to the dynamic temporal patterns, ambiguous semantics, and lack of temporal priors. In this work, we introduce TimeMaster, a reinforcement learning (RL)-based method that enables time-series MLLMs to perform structured, interpretable reasoning directly over visualized time-series inputs and task prompts. TimeMaster adopts a three-part structured output format, reasoning, classification, and domain-specific extension, and is optimized via a composite reward function that aligns format adherence, prediction accuracy, and open-ended insight quality. The model is trained using a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish a good initialization, followed by Group Relative Policy Optimization (GRPO) at the token level to enable stable and targeted reward-driven improvement in time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster achieves state-of-the-art performance, outperforming both classical time-series models and few-shot GPT-4o by over 14.6% and 7.3% performance gain, respectively. Notably, TimeMaster goes beyond time-series classification: it also exhibits expert-like reasoning behavior, generates context-aware explanations, and delivers domain-aligned insights. Our results highlight that reward-driven RL can be a scalable and promising path toward integrating temporal understanding into time-series MLLMs. </p>
<blockquote>
<p>时间序列推理在多模态大型语言模型（MLLMs）中仍然是一个重大挑战，这是由于存在动态时间模式、语义模糊以及缺乏时间先验知识。在这项工作中，我们引入了TimeMaster，一种基于强化学习（RL）的方法，使时间序列MLLMs能够直接对可视化时间序列输入和任务提示进行结构化、可解释的推理。TimeMaster采用三部分的结构化输出格式，包括推理、分类和领域特定扩展，并通过组合奖励函数进行优化，该函数字符合格式遵循、预测准确性和开放性洞察质量。该模型采用两阶段管道进行训练：我们首先应用监督微调（SFT）来建立良好的初始化，然后通过令牌级别的集团相对策略优化（GRPO）实现时间序列推理的稳定性和针对性奖励驱动改进。我们在基于Qwen2.5-VL-3B-Instruct的TimerBed基准测试上对六个真实世界分类任务评估了TimeMaster。TimeMaster达到了最先进的性能，相较于经典的时间序列模型和少镜头GPT-4o分别提高了超过14.6%和7.3%的性能。值得注意的是，TimeMaster不仅超越时间序列分类：它还表现出专家级的推理行为，生成上下文感知的解释，并提供与领域相符的见解。我们的结果强调，奖励驱动的RL可以是一条可扩展且前途光明的路径，旨在将时间理解整合到时间序列MLLMs中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13705v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了TimeMaster，一种基于强化学习的时间序列多模态大型语言模型方法。TimeMaster能在可视化时间序列输入和任务提示上进行结构化、可解释的推理。采用三阶段结构输出格式，并通过组合奖励函数进行优化，实现了时间序列推理的新水平。通过两个阶段的管道训练模型，先用监督微调进行初始化，再用分组相对策略优化在令牌级别进行奖励驱动改进。TimeMaster在TimerBed基准测试上表现突出，展现出超越现有模型的性能。除了时间序列分类外，TimeMaster还展现出专家级的推理行为，生成上下文相关的解释和领域对齐的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeMaster是一种基于强化学习的方法，用于增强多模态大型语言模型在时间序列推理方面的能力。</li>
<li>TimeMaster能直接处理可视化时间序列输入和任务提示，进行结构化、可解释的推理。</li>
<li>TimeMaster采用三阶段结构输出格式并优化组合奖励函数以对齐格式遵守、预测准确性和开放性的见解质量。</li>
<li>模型采用两个阶段进行训练：监督微调进行初始化，然后采用分组相对策略优化进行奖励驱动改进。</li>
<li>TimeMaster在TimerBed基准测试中表现卓越，超越现有模型表现，具有更高的分类性能。</li>
<li>TimeMaster展现了专家级的推理行为，可以生成上下文相关的解释和与特定领域相关的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13705">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d7e3a8ffa7f76ded1a1ce418de9e788e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653f022d516ce712bb1a28c7eccc2dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f608a58011de70a08635568edff806d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Verifying-the-Verifiers-Unveiling-Pitfalls-and-Potentials-in-Fact-Verifiers"><a href="#Verifying-the-Verifiers-Unveiling-Pitfalls-and-Potentials-in-Fact-Verifiers" class="headerlink" title="Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact   Verifiers"></a>Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact   Verifiers</h2><p><strong>Authors:Wooseok Seo, Seungju Han, Jaehun Jung, Benjamin Newman, Seungwon Lim, Seungbeen Lee, Ximing Lu, Yejin Choi, Youngjae Yu</strong></p>
<p>Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at <a target="_blank" rel="noopener" href="https://github.com/just1nseo/verifying-the-verifiers">https://github.com/just1nseo/verifying-the-verifiers</a> </p>
<blockquote>
<p>事实核查对于确保LLM应用程序的可靠性至关重要。在本研究中，我们使用来自14个事实核查基准的示例集，对12个预训练LLM和一个专门的事实核查器进行了评估，包括前沿LLM和开放权重推理LLM。我们分享了三项旨在指导未来开发更稳健的事实核查器的发现。首先，我们强调了解决数据集中的注释错误和模糊性的重要性，表明大约16％的模糊或错误标记的数据会显著影响模型排名。忽略此问题可能导致比较评估时得出误导性结论，我们建议采用一个系统的管道，利用LLM作为法官，帮助大规模识别这些问题。其次，我们发现具有少量上下文示例的前沿LLM（以前的研究经常忽略这一点）达到了顶尖的性能。因此，我们建议未来的研究要与这些简单但高效的基本线进行比较。最后，尽管它们很有效，但前沿LLM会带来巨大的成本，这推动了精细调整的事实核查器的开发。我们表明，这些小型模型仍有改进的空间，特别是在需要复杂推理的实例中。令人鼓舞的是，我们证明通过合成多跳推理数据来增强训练会显著提高这些模型在此类实例中的能力。我们在<a target="_blank" rel="noopener" href="https://github.com/just1nseo/verifying-the-verifiers%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/just1nseo/verifying-the-verifiers上发布了我们的代码、模型和数据集。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究评估了12个预训练的大型语言模型和1个专门的验证器在事实核查方面的表现。研究发现在数据集标注错误和模糊性问题上需重视，约16%的模糊或错误标注数据会影响模型排名。同时，前沿的大型语言模型在少数场景下的表现优异，但成本较高，建议开发小型且精细调整的验证器。通过合成多跳推理数据增强训练，可提高模型在复杂实例中的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据集的标注错误和模糊性对大型语言模型（LLM）的评估有重要影响，大约16%的问题数据会影响模型排名。</li>
<li>前沿的大型语言模型在少数场景（few-shot）下表现优异，应重视其在事实核查领域的应用。</li>
<li>在事实核查领域，开发小型且精细调整的验证器是一个可行方向，以降低模型成本。</li>
<li>通过合成多跳推理数据增强训练可以提高LLM在复杂实例中的能力。</li>
<li>系统性地利用大型语言模型作为判断工具可以帮助识别数据集中的问题。</li>
<li>在未来的研究中，需要包含与前沿大型语言模型的对比研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-29dabf2ae5239d12d9a10f2a52c7372c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c3839fc8cb4fe71a48ddc2f7835e615.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a84721fda063926294dc6e282f8a97ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-824e46453f5f49577208be1a9044fe40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1817debcf905f75cb6406cb5feb390f3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Active-Multimodal-Distillation-for-Few-shot-Action-Recognition"><a href="#Active-Multimodal-Distillation-for-Few-shot-Action-Recognition" class="headerlink" title="Active Multimodal Distillation for Few-shot Action Recognition"></a>Active Multimodal Distillation for Few-shot Action Recognition</h2><p><strong>Authors:Weijia Feng, Yichen Zhu, Ruojia Zhang, Chenyang Wang, Fei Ma, Xiaobao Wang, Xiaobai Li</strong></p>
<p>Owing to its rapid progress and broad application prospects, few-shot action recognition has attracted considerable interest. However, current methods are predominantly based on limited single-modal data, which does not fully exploit the potential of multimodal information. This paper presents a novel framework that actively identifies reliable modalities for each sample using task-specific contextual cues, thus significantly improving recognition performance. Our framework integrates an Active Sample Inference (ASI) module, which utilizes active inference to predict reliable modalities based on posterior distributions and subsequently organizes them accordingly. Unlike reinforcement learning, active inference replaces rewards with evidence-based preferences, making more stable predictions. Additionally, we introduce an active mutual distillation module that enhances the representation learning of less reliable modalities by transferring knowledge from more reliable ones. Adaptive multimodal inference is employed during the meta-test to assign higher weights to reliable modalities. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing approaches. </p>
<blockquote>
<p>少量样本动作识别因其快速进步和广泛应用前景而引起了人们的极大兴趣。然而，当前的方法主要基于有限的单模态数据，并没有充分利用多模态信息的潜力。本文提出了一种新型框架，该框架能够利用特定任务的上下文线索，主动为每个样本识别可靠的模态，从而显著提高识别性能。我们的框架集成了一个主动样本推理（ASI）模块，该模块利用主动推理，基于后验分布预测可靠的模态，然后相应地组织它们。与强化学习不同，主动推理用基于证据的偏好取代奖励，从而做出更稳定的预测。此外，我们还引入了一个主动相互蒸馏模块，通过从更可靠的模态转移知识，增强对不太可靠模态的表示学习。在元测试过程中采用自适应多模态推理，为可靠模态分配更高权重。在多个基准测试上的广泛实验表明，我们的方法显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13322v1">PDF</a> IJCAI 2025, the 34th International Joint Conference on Artificial   Intelligence</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于任务特定上下文线索的主动模态识别框架，通过Active Sample Inference模块预测可靠的模态，并据此进行组织，从而提高少样本动作识别的性能。引入主动互蒸馏模块，通过从可靠的模态转移知识，增强不可靠模态的表示学习。在元测试阶段采用自适应多模态推理，为可靠的模态分配更高的权重。该方法在多个基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前少样本动作识别方法主要基于单一模态数据，未充分利用多模态信息的潜力。</li>
<li>论文提出了一种新的框架，利用任务特定的上下文线索来主动识别可靠的模态。</li>
<li>引入Active Sample Inference（ASI）模块，基于后验分布进行预测并整理可靠的模态。</li>
<li>与强化学习不同，主动推理通过基于证据的偏好替代奖励，预测更加稳定。</li>
<li>论文还引入了主动互蒸馏模块，提高了对不可靠模态的表示学习，通过从可靠的模态转移知识。</li>
<li>在元测试阶段采用自适应多模态推理，为可靠模态分配更高权重。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13322">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9372d3b6c58dc61ea2cb4a41cec8f33e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f6e2d01ac33ed8d3e38d32b821de19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c5b43a93e9e5fa0e4ed388f0409822c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d5ab87654000c83794b0a91dc6b2634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e881509d061811674cf7aed5b82e6f3c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MetaEformer-Unveiling-and-Leveraging-Meta-patterns-for-Complex-and-Dynamic-Systems-Load-Forecasting"><a href="#MetaEformer-Unveiling-and-Leveraging-Meta-patterns-for-Complex-and-Dynamic-Systems-Load-Forecasting" class="headerlink" title="MetaEformer: Unveiling and Leveraging Meta-patterns for Complex and   Dynamic Systems Load Forecasting"></a>MetaEformer: Unveiling and Leveraging Meta-patterns for Complex and   Dynamic Systems Load Forecasting</h2><p><strong>Authors:Shaoyuan Huang, Tiancheng Zhang, Zhongtian Zhang, Xiaofei Wang, Lanjun Wang, Xin Wang</strong></p>
<p>Time series forecasting is a critical and practical problem in many real-world applications, especially for industrial scenarios, where load forecasting underpins the intelligent operation of modern systems like clouds, power grids and traffic networks.However, the inherent complexity and dynamics of these systems present significant challenges. Despite advances in methods such as pattern recognition and anti-non-stationarity have led to performance gains, current methods fail to consistently ensure effectiveness across various system scenarios due to the intertwined issues of complex patterns, concept-drift, and few-shot problems. To address these challenges simultaneously, we introduce a novel scheme centered on fundamental waveform, a.k.a., meta-pattern. Specifically, we develop a unique Meta-pattern Pooling mechanism to purify and maintain meta-patterns, capturing the nuanced nature of system loads. Complementing this, the proposed Echo mechanism adaptively leverages the meta-patterns, enabling a flexible and precise pattern reconstruction. Our Meta-pattern Echo transformer (MetaEformer) seamlessly incorporates these mechanisms with the transformer-based predictor, offering end-to-end efficiency and interpretability of core processes. Demonstrating superior performance across eight benchmarks under three system scenarios, MetaEformer marks a significant advantage in accuracy, with a 37% relative improvement on fifteen state-of-the-art baselines. </p>
<blockquote>
<p>时间序列预测是许多现实世界应用中的关键实际问题，特别是在工业场景中，负载预测是云、电网和交通网络等现代系统智能运行的基础。然而，这些系统的固有复杂性和动态性带来了巨大的挑战。尽管模式识别和抗非平稳性等方法的发展提高了性能，但由于复杂模式、概念漂移和少样本问题的交织，当前的方法无法在各种系统场景中持续确保有效性。为了同时解决这些挑战，我们引入了一种以基本波形（也称为元模式）为中心的新型方案。具体来说，我们开发了一种独特的元模式池化机制来净化和维护元模式，捕捉系统负载的微妙性质。作为补充，我们提出的回声机制自适应地利用元模式，实现灵活和精确的模式重建。我们的元模式回声转换器（MetaEformer）无缝地将这些机制与基于转换器的预测器相结合，提供端到端的效率和核心过程的可解释性。在三种系统场景的八个基准测试中表现出卓越的性能，MetaEformer在准确性方面具有显著优势，相对于十五个最先进的基线有37%的相对改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于时间序列预测在现代系统如云计算、电网和交通网络中的关键作用，针对当前面临的多重挑战，本文提出了一种基于元模式的新方案。通过开发独特的元模式池化机制和回声机制，该方案能够捕捉系统负载的微妙变化并灵活适应性地利用元模式。MetaEformer无缝结合了这些机制与基于变压器的预测器，提供端到端的效率和核心过程的可解释性，在三个系统场景的八个基准测试中表现出卓越的性能，相对于十五种最新技术基准有37%的相对改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列预测在多个现代系统中具有关键作用，如云计算、电网和交通网络中的智能操作。</li>
<li>当前的时间序列预测方法面临复杂模式、概念漂移和小样本问题的挑战。</li>
<li>提出了一种基于元模式的新方案，通过开发独特的元模式池化机制和回声机制来解决这些挑战。</li>
<li>元模式池化机制用于净化和维护元模式，捕捉系统负载的微妙变化。</li>
<li>Echo机制能够自适应地利用元模式，实现灵活和精确的模式重建。</li>
<li>MetaEformer结合了这些机制与基于变压器的预测器，提供端到端的效率和核心过程的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7a527f393ab2cadba7edb4d96ec8f7ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a135a81c9ccc676ebfd3c52b32065a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87ce89d27f7e1ebc56d6c47ba29da9f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ad8832c289d4c8a86c1d7fb57a328e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ebb167291cb8b7356b5f9d84024090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93d61f0d39c5e40c26fc5016156dddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b9b02cbaa74bac5da0c441e1917d35.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="COGNATE-Acceleration-of-Sparse-Tensor-Programs-on-Emerging-Hardware-using-Transfer-Learning"><a href="#COGNATE-Acceleration-of-Sparse-Tensor-Programs-on-Emerging-Hardware-using-Transfer-Learning" class="headerlink" title="COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware   using Transfer Learning"></a>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware   using Transfer Learning</h2><p><strong>Authors:Chamika Sudusinghe, Gerasimos Gerogiannis, Damitha Lenadora, Charles Block, Josep Torrellas, Charith Mendis</strong></p>
<p>Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM. </p>
<blockquote>
<p>稀疏张量程序在深度学习和图形分析中具有重要作用，推动了优化处理的需求。为满足这一需求，正在开发专用硬件加速器。由于程序性能高度敏感于稀疏输入的变动，且早期阶段的加速器依赖于昂贵的模拟器，因此用于在通用硬件上优化此类程序的基于机器学习的成本模型对于早期阶段的加速器往往无效，因为它们需要大量数据集进行适当训练。为此，我们引入了COGNATE框架，该框架利用来自通用硬件（如CPU）的廉价数据样本进行成本模型训练，然后在新兴硬件上进行少量样本微调。COGNATE利用跨硬件平台的输入特征的一致性，同时有效缓解不一致性，使得成本模型训练仅需使用与针对加速器的模型所需数据样本量的5%，即可实现相当的性能。我们进行了大量实验，证明COGNATE优于现有技术，实现了稀疏矩阵乘法（SpMM）的平均加速比达到1.47倍（最高至5.46倍），以及结构化稀疏动态矩阵乘法（SDDMM）的平均加速比达到1.39倍（最高至4.22倍）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00424v2">PDF</a> Accepted at the 42nd International Conference on Machine Learning</p>
<p><strong>Summary</strong></p>
<p>COGNATE框架利用通用硬件（如CPU）的廉价数据样本训练成本模型，并通过少量新兴硬件进行微调，有效解决了稀疏张量程序在专用硬件加速器上的优化挑战。该框架利用硬件平台间输入特征的同质性，同时有效缓解异质性，用仅5%的数据样本就能训练出与加速器特定模型相当性能的成本模型。实验表明，COGNATE在SpMM和SDDMM任务上平均加速1.47倍（最高5.46倍）和1.39倍（最高4.22倍）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稀疏张量程序在深度学习和图分析中具有重要作用，需要优化的处理。</li>
<li>专用硬件加速器的发展为满足这一需求提供了解决方案。</li>
<li>优化这些程序对于早期加速器面临两大挑战：性能对稀疏输入的敏感性以及依赖昂贵模拟器的早期阶段。</li>
<li>现有基于ML的成本模型在优化通用硬件上的程序时往往对早期加速器无效，因为它们需要大量数据集进行训练。</li>
<li>COGNATE框架利用通用硬件的廉价数据样本训练成本模型，并通过少量新兴硬件进行微调。</li>
<li>COGNATE框架利用硬件平台间输入特征的同质性，有效缓解数据异质性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00424">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2ca97cc5f3fa33743297304f33a1461c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5edf5ead70161b9ba358cd6628bf96fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da2b3e6dc2e49622af7237507a1291f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-622f63a3b708296131defb5646aa4563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-778541af240129f2123192b1c91d6e98.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 16.8 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/">https://github.com/roboflow/rf100-vl/</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a> </p>
<blockquote>
<p>通过基于互联网规模数据训练的视觉语言模型（VLMs）在常见对象（如汽车、卡车和行人）上的零样本检测性能显著。然而，最先进的模型仍然难以推广到其预训练未涉及的类别、任务以及成像模式。我们并不主张仅仅通过增加视觉数据对VLM进行再训练，而是认为应该通过包含少量视觉示例和丰富文本描述的注释指令来对齐VLM的新概念。为此，我们推出了Roboflow100-VL，这是一组大规模的多模式对象检测数据集，其中包含不常见的概念多样性集合且总共有多达百个数据集集合进行模型评估与微调。我们对此基准测试中最先进的模型进行了零样本、少样本、半监督和全监督设置下的评估，允许跨数据制度进行比较。值得注意的是，我们发现像GroundingDINO和Qwen2.5-VL这样的VLM在Roboflow100-VL中的挑战性医学成像数据集上的零样本准确率低于百分之二，这显示出进行少样本概念对齐的必要性。最后，我们讨论了最近的CVPR 2025基础FSOD竞赛并从社区分享了一些见解。值得注意的是，获胜队伍超过了我们的基线水平高达16.8 mAP！我们的代码和数据集可以在<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/">https://github.com/roboflow/rf100-vl/</a> 和 <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v2">PDF</a> The first two authors contributed equally. Project Page:   <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对互联网规模数据的视觉语言模型（VLMs）在常见物体上的零样本检测性能。然而，现有模型在泛化到超出分布范围的新类别和任务时表现欠佳。为此，文章提出了一种通过少量视觉示例和丰富的文本描述来对VLM进行概念对齐的方法。同时介绍了Roboflow100-VL大规模多模态目标检测数据集，该数据集包含罕见概念的数据集，用于评估模型在不同数据环境下的性能。研究还发现，现有模型在具有挑战性的医疗图像数据集上的零样本准确率较低，突显了概念对齐的必要性。最后，文章分享了CVPR 2025基础FSOD竞赛的见解和成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在互联网规模数据上表现出良好的零样本检测性能。</li>
<li>当前模型在泛化到新类别和任务时遇到困难。</li>
<li>Roboflow100-VL数据集是一个包含罕见概念的多模态目标检测数据集。</li>
<li>现有模型在挑战性医疗图像数据集上的零样本准确率较低。</li>
<li>通过少量视觉示例和丰富的文本描述对VLM进行概念对齐是一种有效的策略。</li>
<li>CVPR 2025基础FSOD竞赛展示了显著的成果和社区见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5aca01ad3a170a81e3314cec4f10ee82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8394e86845eed7bebbdef61422acc1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94525ec6f3d591f43894fc2c1a976188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8c9b05e81329b7cd6eaeb0ed2ecc447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8595510fa396ee11352235c0ab0ccab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa8f284bbe93c59ebf971defb34f815.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CAPO-Cost-Aware-Prompt-Optimization"><a href="#CAPO-Cost-Aware-Prompt-Optimization" class="headerlink" title="CAPO: Cost-Aware Prompt Optimization"></a>CAPO: Cost-Aware Prompt Optimization</h2><p><strong>Authors:Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automatic prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11&#x2F;15 cases with improvements up to 21%p in accuracy. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency. </p>
<blockquote>
<p>大型语言模型（LLM）通过简单的提示解决了广泛的自然语言处理任务，从而彻底改变了自然语言处理的格局。然而，它们的性能对提示的构思非常敏感。虽然自动提示优化可以通过找到最佳提示来解决这一挑战，但当前的方法需要大量的LLM调用和输入令牌，这使得提示优化成本高昂。我们引入了CAPO（基于成本的提示优化），这是一种通过集成AutoML技术提高提示优化效率的算法。CAPO是一种进化方法，以LLM作为操作员，结合了比赛以节省评估和基于性能的多目标优化来平衡提示长度。它同时优化指令和少量示例，并利用任务描述来提高稳健性。我们在不同的数据集和LLM上进行了大量实验，结果表明，在解决大尺度机器推理问题时，CAPO在大多数情况下的性能优于最先进的离散提示优化方法，准确率提高了高达百分之二十一。我们的算法在较小的预算下就能实现更好的性能，通过比赛节省评估时间，并通过长度惩罚减少平均提示长度，既经济又实用。即使没有少量的示例，CAPO也能超越竞争对手并保持对初始提示的稳健性。CAPO朝着提高提示优化的成本效益和可访问性迈出了重要的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16005v4">PDF</a> Submitted to AutoML 2025</p>
<p><strong>Summary</strong></p>
<p>LLMs的性能对提示语敏感度极高，自动提示优化通过寻找最佳提示来解决这一挑战，但现有方法需要大量LLM调用和输入令牌，使得提示优化成本高昂。引入CAPO（成本感知提示优化）算法，通过集成AutoML技术提高提示优化效率。CAPO采用进化方法，以LLMs作为操作员，结合竞赛以节省评估和多元目标优化来平衡性能和提示长度。它联合优化指令和少量示例，并利用任务描述提高稳健性。实验表明，CAPO在多数情况下优于最新离散提示优化方法，在准确性上最高提升了21%。CAPO在较小的预算下即可实现更好的性能，通过竞赛节省评估，并通过长度惩罚减少平均提示长度，既经济又注重成本效益。即使在没有少量示例的情况下，CAPO也能超越竞争对手并维持稳健性。这为使提示优化更加强大和普及做出了重要的一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs对提示语的敏感度极高，自动提示优化是解决这一挑战的关键。</li>
<li>当前自动提示优化方法存在成本高的问题。</li>
<li>CAPO算法通过集成AutoML技术提高提示优化效率。</li>
<li>CAPO采用进化方法，结合竞赛以节省评估，并平衡性能和提示长度。</li>
<li>CAPO联合优化指令和少量示例，利用任务描述增强稳健性。</li>
<li>实验显示CAPO在多数情况下优于其他方法，最高可提升21%的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5307e394c6b2db5b216b74d598c9fbeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f83c7edcce289110fc859d98264a6596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a95ee4ba1628b0c6db9186fa0a7690.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Švábenský, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators’ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>我们探索了大型语言模型（LLM）在自动评估学生开放文本反思和预测学业成绩方面的应用。传统的反思评估方法耗时且可能无法在教育环境中有效扩展。在这项工作中，我们采用LLM，使用两种评估策略（单代理和多代理）和两种提示技术（零样本和少样本），将学生反思转化为量化分数。我们在包含来自377名学生三个学术学期的5,278篇反思的数据集上进行的实验表明，使用少样本策略的单代理方式与人类评估的匹配率最高。此外，使用LLM评估的反思分数的模型在处于危险中的学生识别和成绩预测任务中的表现都优于基线。这些结果表明，LLM可以有效地自动进行反思评估，减少教育工作者的工作量，并为可能需要额外帮助的学生提供及时的支持。我们的工作强调了将先进的生成性AI技术融入教育实践中的潜力，以提高学生的参与度和学业成功。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v3">PDF</a> Published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025), see   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-981-96-8186-0_24">https://doi.org/10.1007/978-981-96-8186-0_24</a></p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLMs）在自动评估学生开放性反思和预测学业表现方面的应用。传统评估方法耗时且难以在教育环境中有效扩展。本研究采用LLMs，通过两种评估策略（单代理和多代理）和两种提示技术（零射击和少射击），将学生反思转化为量化分数。实验表明，采用少射击策略的单代理匹配率最高。此外，利用LLM评估的反思分数的模型在风险学生识别和成绩预测任务中的表现优于基线。这表明LLMs可以有效自动评估反思，减轻教育工作者的工作量，为可能需要额外支持的学生提供及时支持。本研究强调了将高级生成性AI技术融入教育实践以提高学生参与度和学业成功的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模语言模型（LLMs）可用于自动评估学生的开放性反思。</li>
<li>传统的学生反思评估方法耗时且难以扩展。</li>
<li>通过两种评估策略（单代理和多代理）和两种提示技术（零射击和少射击），LLMs能够有效转化学生反思为量化分数。</li>
<li>实验显示，采用少射击策略的单代理匹配率最高。</li>
<li>LLMs在风险学生识别和成绩预测任务中的表现优于传统方法。</li>
<li>LLMs的自动评估可以减轻教育工作者的工作量，为需要额外支持的学生提供及时支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-727cd303d72d5350a7210743478c92d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images"><a href="#FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images" class="headerlink" title="FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images"></a>FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images</h2><p><strong>Authors:Zhuohao Li, Zhicheng Huang, Wenchao Liu, Zhuxin Zhang, Jianming Miao</strong></p>
<p>Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes in images using only a limited number of annotated examples, has recently progressed in data-scarce domains. However, in this work, we show that the existing FSS methods often struggle to generalize to underwater environments. Specifically, the prior features extracted by pre-trained models used as feature extractors are fragile due to the unique challenges of underwater images. To address this, we propose FSSUWNet, a tailored FSS framework for underwater images with feature enhancement. FSSUWNet exploits the integration of complementary features, emphasizing both low-level and high-level image characteristics. In addition to employing a pre-trained model as the primary encoder, we propose an auxiliary encoder called Feature Enhanced Encoder which extracts complementary features to better adapt to underwater scene characteristics. Furthermore, a simple and effective Feature Alignment Module aims to provide global prior knowledge and align low-level features with high-level features in dimensions. Given the scarcity of underwater images, we introduce a cross-validation dataset version based on the Segmentation of Underwater Imagery dataset. Extensive experiments on public underwater segmentation datasets demonstrate that our approach achieves state-of-the-art performance. For example, our method outperforms the previous best method by 2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot and 5-shot scenarios in the datasets, respectively. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet">https://github.com/lizhh268/FSSUWNet</a>. </p>
<blockquote>
<p>少数样本语义分割（FSS）旨在利用有限的标注样本对图像中的新类别进行分割，最近在数据稀缺领域取得了进展。然而，在这项工作中，我们展示了现有的FSS方法往往难以推广到水下环境。具体来说，由于水下图像的独特挑战，由预训练模型提取的先验特征是脆弱的。为了解决这个问题，我们提出了FSSUWNet，这是一个针对水下图像的FSS框架，具有特征增强功能。FSSUWNet利用互补特征的集成，强调图像的低级和高级特征。除了使用预训练模型作为主编码器外，我们还提出了一个辅助编码器，称为特征增强编码器，用于提取互补特征，以更好地适应水下场景特征。此外，简单有效的特征对齐模块旨在提供全局先验知识，并在维度上对齐低级特征和高级特征。鉴于水下图像的稀缺性，我们基于水下图像分割数据集引入了一个交叉验证数据集版本。在公共水下分割数据集上的大量实验表明，我们的方法达到了最先进的性能。例如，我们的方法在数据集的一次射击和五次射击场景中，平均交并比指标分别比之前的最佳方法高出2.8%和2.6%。我们的实现可在<a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lizhh268/FSSUWNet上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00478v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对水下图像进行语义分割的Few-Shot Semantic Segmentation（FSS）的挑战。现有FSS方法在水下环境中泛化能力有限，因此提出一种针对水下图像的FSS框架——FSSUWNet，通过特征增强来应对水下环境的独特挑战。FSSUWNet结合了主要编码器和辅助编码器提取的互补特征，并引入特征对齐模块以提供全局先验知识。在公共水下分割数据集上的实验表明，该方法取得了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSS在水下环境中的泛化能力受限，需要专门的方法来处理水下图像的独特挑战。</li>
<li>FSSUWNet是一个针对水下图像的FSS框架，通过特征增强来提高模型的性能。</li>
<li>FSSUWNet结合了主要编码器和辅助编码器，以提取互补特征，更好地适应水下场景特性。</li>
<li>特征对齐模块旨在提供全局先验知识，并将低级别特征与高级别特征对齐。</li>
<li>缺乏水下图像数据，研究引入了基于Segmentation of Underwater Imagery数据集的交叉验证数据集版本。</li>
<li>在公共水下分割数据集上的实验表明，FSSUWNet取得了最佳性能，相较于之前的方法有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f27f1af420dbcac33e58aa0c16fffc6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a354beb29cd8c80a20116f4d0e2fb8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-508ecd23d8e902eb482806ef4ad176be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d365645dd7d4d6c540b2b0e4c0d0d9b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9df96537413c539b08ec870c7a312e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217ca76dc735c5e0fe0a90313ddc4416.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search"><a href="#Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search" class="headerlink" title="Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search"></a>Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search</h2><p><strong>Authors:Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, Yuyu Luo</strong></p>
<p>Text-to-SQL, which enables natural language interaction with databases, serves as a pivotal method across diverse industries. With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, zero-shot Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction. To address this challenge, we propose Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial reasoning states. To enhance the framework’s reasoning capabilities, we introduce LLM-as-Action-Model to dynamically generate SQL construction actions during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation. Experimental results show that Alpha-SQL achieves 69.7% execution accuracy on the BIRD development set, using a 32B open-source LLM without fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by 2.5% on the BIRD development set. </p>
<blockquote>
<p>文本到SQL的技术能够实现与数据库的自然语言交互，成为各行业关键的方法。随着每隔几个月就会出现新的、更强大的大型语言模型（LLM），微调已经变得非常昂贵、劳动密集型和易出错。作为一种替代方案，零样本文本到SQL，利用LLM中编码的日益增长的知识和推理能力，无需特定任务的微调，展现出一个有前景和更具挑战性的方向。为了应对这一挑战，我们提出了Alpha-SQL这一新方法，它利用蒙特卡洛树搜索（MCTS）框架来基于部分推理状态迭代地推断SQL构建操作。为了提高框架的推理能力，我们引入了LLM-作为行动模型，在MCTS过程中动态生成SQL构建操作，引导搜索朝着更有前途的SQL查询进行。此外，Alpha-SQL采用自我监督的奖励函数来评估候选SQL查询的质量，确保更准确、高效的查询生成。实验结果表明，Alpha-SQL在BIRD开发集上实现了69.7%的执行准确率，使用的是未进行微调的开源LLM 32B。在BIRD开发集上，Alpha-SQL比基于GPT-4o的最佳先前零样本方法高出2.5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17248v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>文本描述了Text-to-SQL的重要性以及随着大型语言模型（LLMs）的发展所面临的挑战。针对这些挑战，提出了一种名为Alpha-SQL的新方法，它利用蒙特卡洛树搜索（MCTS）框架和LLM-as-Action-Model来生成SQL查询，并在无微调的情况下实现了较高的执行准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQL已成为与数据库进行自然语言交互的关键方法，广泛应用于各行各业。</li>
<li>随着大型语言模型（LLMs）的快速发展，传统的微调方法变得成本高昂、劳动密集且易出错。</li>
<li>Alpha-SQL是一种新的解决方法，采用蒙特卡洛树搜索（MCTS）框架来推断SQL构建动作。</li>
<li>LLM-as-Action-Model在MCTS过程中动态生成SQL构建动作，引导搜索向更有前途的SQL查询方向进行。</li>
<li>Alpha-SQL使用自监督奖励函数来评估候选SQL查询的质量，确保更准确、更高效的查询生成。</li>
<li>在无微调的情况下，Alpha-SQL在BIRD开发集上实现了69.7%的执行准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17248">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46726183d8f3eba5dd9d774b7f15c207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cd3cb7a9f2abc871db5cbd24a0efac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ad956c43608f2b1c12c801c0e321e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4a126c6df204bc66f989a67d574cfd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0719f505961381ade5c61d79bfa6b220.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs"><a href="#Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs" class="headerlink" title="Stepwise Reasoning Error Disruption Attack of LLMs"></a>Stepwise Reasoning Error Disruption Attack of LLMs</h2><p><strong>Authors:Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu</strong></p>
<p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack">https://github.com/Applied-Machine-Learning-Lab/SEED-Attack</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在复杂推理任务中取得了显著的进步，但其在推理过程中的安全性和稳健性仍缺乏足够的探索。现有的针对LLM推理的攻击受限于特定场景或缺乏隐蔽性，限制了其可行性和通用性。为了解决这些挑战，我们提出了逐步推理误差干扰（SEED）攻击方法，该方法能微妙地将错误注入到先前的推理步骤中，误导模型产生错误的后续推理和最终答案。与以前的方法不同，SEED与零样本和少样本场景兼容，保持自然推理流程，并确保在不修改指令的情况下秘密执行。在四个不同模型、四个数据集上的大量实验证明了SEED的有效性，揭示了LLM对推理过程中断的脆弱性。这些发现强调了在实践应用中确保LLM推理稳健性的重要性，以保障其安全性。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/SEED-Attack上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11934v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在复杂推理任务中取得了显著进展，但其安全性和推理过程的稳健性尚未得到充分探索。针对现有LLM推理攻击特定设置或缺乏隐蔽性的问题，本文提出了逐步推理误差干扰（SEED）攻击方法。SEED通过微妙地注入错误来误导模型产生错误的后续推理和最终答案。与其他方法不同，SEED适用于零样本和小样本设置，保持自然推理流程，确保在不修改指令的情况下秘密执行。在四个数据集和四个不同模型上的广泛实验证明了SEED的有效性，揭示了LLM对推理过程中断的脆弱性。这些发现强调了在实际应用中关注LLM推理稳健性的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在复杂推理任务中表现出强大的能力，但安全性和稳健性有待提升。</li>
<li>现有对LLM的推理攻击方法受限于特定场景或缺乏隐蔽性，导致实际应用中可行性差和通用性不足。</li>
<li>提出了Stepwise rEasoning Error Disruption（SEED）攻击方法，能够微妙地注入错误以误导模型产生错误的推理和答案。</li>
<li>SEED攻击方法适用于零样本和小样本场景，保持自然推理流程，同时确保执行过程隐蔽。</li>
<li>广泛实验证明SEED攻击的有效性，揭示了LLM对推理过程中断的脆弱性。</li>
<li>实验结果强调在实际应用中关注LLM推理稳健性的重要性。</li>
<li>研究的代码已公开可访问。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c8c19e5fd4a29ba146dae4f32d556dfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00ca2d3282bea65c458df2505f4b40e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87b1c6209c61a9ecdf478d89801d4273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c845210b4c456a596c9b56ccbedf20ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d37b365832bd7f8ebcf6587df784a41c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>少数语义分割（Few-shot Semantic Segmentation）技术旨在解决仅使用少量标注样本对查询图像中的对象进行分割的挑战。然而，许多之前的最先进方法要么不得不放弃复杂的局部语义特征，要么面临高计算复杂度的问题。为了应对这些挑战，我们提出了一种基于Transformer架构的少数语义分割新框架。我们的方法引入了空间变换解码器和上下文掩码生成模块，以提高支持图像和查询图像之间的关系理解。此外，我们引入了多尺度解码器，以分层的方式融入不同分辨率的特征来优化分割掩码。同时，我们的方法整合了中间编码器阶段的全局特征，以提高上下文理解，同时保持轻量级结构以降低复杂度。性能和效率之间的这种平衡使我们的方法在PASCAL-5^i和COCO-20^i等基准数据集上能在1-shot和5-shot设置下实现具有竞争力的结果。值得注意的是，我们的模型仅有150万个参数，展示了出色的性能，并克服了现有方法的局限性。详情请访问：<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于Transformer架构的Few-shot语义分割框架，通过引入空间变换解码器、上下文掩膜生成模块和多尺度解码器，提高了对支持图像和查询图像之间关系的理解，实现了对少量标注样本下目标图像的精细分割。该模型在PASCAL-5^i和COCO-20^i等基准数据集上取得了具有竞争力的结果，且模型参数仅为1.5百万，展现出优越的性能和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于Transformer架构的Few-shot语义分割新框架。</li>
<li>框架包含空间变换解码器、上下文掩膜生成模块和多尺度解码器，以改善关系理解。</li>
<li>通过整合中间编码阶段的全球特征，提高了上下文理解。</li>
<li>模型实现了在基准数据集上的竞争性能，如PASCAL-5^i和COCO-20^i。</li>
<li>模型在1-shot和5-shot设置下均表现出良好性能。</li>
<li>模型参数数量仅为1.5百万，实现了性能和效率的平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dcdeef2671221c0cba44addb40e56bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6e719d0a34e103475f80a5b03bc20f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1232d75de621baac9ac76c250665a86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43b280a5d0cd66845ec22d0186712420.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SMILE-Speech-Meta-In-Context-Learning-for-Low-Resource-Language-Automatic-Speech-Recognition"><a href="#SMILE-Speech-Meta-In-Context-Learning-for-Low-Resource-Language-Automatic-Speech-Recognition" class="headerlink" title="SMILE: Speech Meta In-Context Learning for Low-Resource Language   Automatic Speech Recognition"></a>SMILE: Speech Meta In-Context Learning for Low-Resource Language   Automatic Speech Recognition</h2><p><strong>Authors:Ming-Hao Hsu, Hung-yi Lee</strong></p>
<p>Automatic Speech Recognition (ASR) models demonstrate outstanding performance on high-resource languages but face significant challenges when applied to low-resource languages due to limited training data and insufficient cross-lingual generalization. Existing adaptation strategies, such as shallow fusion, data augmentation, and direct fine-tuning, either rely on external resources, suffer computational inefficiencies, or fail in test-time adaptation scenarios. To address these limitations, we introduce Speech Meta In-Context LEarning (SMILE), an innovative framework that combines meta-learning with speech in-context learning (SICL). SMILE leverages meta-training from high-resource languages to enable robust, few-shot generalization to low-resource languages without explicit fine-tuning on the target domain. Extensive experiments on the ML-SUPERB benchmark show that SMILE consistently outperforms baseline methods, significantly reducing character and word error rates in training-free few-shot multilingual ASR tasks. </p>
<blockquote>
<p>自动语音识别（ASR）模型在高资源语言上表现出卓越的性能，但当应用于低资源语言时，由于训练数据有限和跨语言泛化能力不足，面临重大挑战。现有的适应策略，如浅融合、数据增强和直接微调，要么依赖外部资源，要么面临计算效率低下的问题，或在测试时适应场景失败。为了解决这些局限性，我们引入了语音元上下文学习（SMILE），这是一个结合元学习与语音上下文学习（SICL）的创新框架。SMILE利用高资源语言的元训练，实现强大的少样本泛化能力，无需在目标域上进行显式微调即可适应低资源语言。在ML-SUPERB基准测试上的大量实验表明，SMILE始终优于基准方法，在无需训练的多语种少样本ASR任务中显著降低了字符和单词错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10429v2">PDF</a> </p>
<p><strong>Summary</strong><br>自动语音识别（ASR）模型在高资源语言上的表现突出，但在应用于低资源语言时面临诸多挑战，如训练数据有限和跨语言泛化能力不足等。现有的适应策略存在依赖外部资源、计算效率低下或在测试时适应场景失败等问题。为解决这些局限，我们提出了一项创新的框架——Speech Meta In-Context LEarning（SMILE），它结合了元学习与语音上下文学习（SICL）。SMILE利用元训练从高资源语言中进行学习，使模型具备对低资源语言的稳健的少量样本泛化能力，无需在目标域进行显式微调。在ML-SUPERB基准测试上的大量实验表明，SMILE在无需训练或少量的训练情况下，在多语种ASR任务中显著降低了字符和单词错误率，表现优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）在低资源语言的应用面临诸多挑战。</li>
<li>当前适应策略存在依赖外部资源、计算效率低下等问题。</li>
<li>SMILE框架结合了元学习与语音上下文学习（SICL）。</li>
<li>SMILE利用元训练从高资源语言中进行学习。</li>
<li>SMILE使模型具备对低资源语言的少量样本泛化能力，无需显式微调。</li>
<li>在ML-SUPERB基准测试上，SMILE显著优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10429">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-76e28bbe08dc9acdf77d31c69d42e7e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87aae35ed2ca8d0e7eb5de4b9a989ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86fd44369aecfc74c661c1fc5c53f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42e37ef1a833a750965756be8a243aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b439ef07e405fe4b9475a8b4dc9a43e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0c16bcedaf8028e4f248959aa17eda9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abcb109bd4414192c6d3bc1870830df5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="WorldAPIs-The-World-Is-Worth-How-Many-APIs-A-Thought-Experiment"><a href="#WorldAPIs-The-World-Is-Worth-How-Many-APIs-A-Thought-Experiment" class="headerlink" title="WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment"></a>WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment</h2><p><strong>Authors:Jiefu Ou, Arda Uzunoglu, Benjamin Van Durme, Daniel Khashabi</strong></p>
<p>AI systems make decisions in physical environments through primitive actions or affordances that are accessed via API calls. While deploying AI agents in the real world involves numerous high-level actions, existing embodied simulators offer a limited set of domain-salient APIs. This naturally brings up the questions: how many primitive actions (APIs) are needed for a versatile embodied agent, and what should they look like? We explore this via a thought experiment: assuming that wikiHow tutorials cover a wide variety of human-written tasks, what is the space of APIs needed to cover these instructions? We propose a framework to iteratively induce new APIs by grounding wikiHow instruction to situated agent policies. Inspired by recent successes in large language models (LLMs) for embodied planning, we propose a few-shot prompting to steer GPT-4 to generate Pythonic programs as agent policies and bootstrap a universe of APIs by 1) reusing a seed set of APIs; and then 2) fabricate new API calls when necessary. The focus of this thought experiment is on defining these APIs rather than their executability. We apply the proposed pipeline on instructions from wikiHow tutorials. On a small fraction (0.5%) of tutorials, we induce an action space of 300+ APIs necessary for capturing the rich variety of tasks in the physical world. A detailed automatic and human analysis of the induction output reveals that the proposed pipeline enables effective reuse and creation of APIs. Moreover, a manual review revealed that existing simulators support only a small subset of the induced APIs (9 of the top 50 frequent APIs), motivating the development of action-rich embodied environments. </p>
<blockquote>
<p>人工智能系统通过通过API调用访问的原始动作或功能（affordances）在物理环境中进行决策。在现实世界中部署AI代理涉及许多高级动作，而现有的实体模拟器仅提供有限的领域特定API。这自然引发了以下问题：对于多才多艺的实体代理，需要多少原始动作（API），它们应该是什么样的？我们通过思想实验来探索这个问题：假设wikiHow教程涵盖了各种人类编写的任务，那么需要什么API空间来涵盖这些指令？我们提出了一个框架，通过将wikiHow指令与情境代理策略相结合，来迭代地引导新的API。我们受到大型语言模型在实体规划方面近期成功的启发，提出通过少量提示来引导GPT-4生成Pythonic程序作为代理策略，并通过1）重复使用种子集API；然后2）在必要时制造新的API调用，来构建API的宇宙。这个思想实验的重点在于定义这些API，而不是它们的可执行性。我们将所提议的管道应用于wikiHow教程中的指令。在少量（0.5%）的教程中，我们诱导出300多个捕获物理世界中丰富多样任务的必要API。对诱导输出的详细自动分析和人工分析表明，所提出的管道能够实现API的有效重用和创建。此外，手动审查表明，现有模拟器仅支持一小部分诱导的API（前50个常用API中的9个），这推动了丰富动作的实体环境的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07778v2">PDF</a> AAAI 2025 &amp; ACL 2024 NLRSE, 7 pages</p>
<p><strong>Summary</strong></p>
<p>本文探索了AI系统在物理环境中通过原始动作（通过API调用实现）进行决策的问题。作者通过假设wikiHow教程涵盖了许多人类任务，提出了一种基于框架的方法，通过迭代诱导出新的API来模拟物理世界的任务动作。该研究使用GPT-4生成代理策略并利用少量提示来引导模型，从而定义API而非关注其可执行性。对一小部分（约0.5%）的教程进行应用后，作者得出了需要超过300个API来捕捉现实世界任务的丰富多样性。分析表明，该方法可实现API的有效复用和创建。此外，对现有模拟器的评估显示，它们仅支持一小部分诱导出的API，这突显了开发丰富动作环境的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AI系统通过API调用实现原始动作以在物理环境中进行决策。</li>
<li>基于wikiHow教程的内容提出了一种迭代诱导API的方法框架。</li>
<li>利用GPT-4生成代理策略并采用少量提示进行引导。</li>
<li>定义API而非关注其可执行性，从而捕捉现实世界任务的丰富多样性。</li>
<li>需要超过300个API来覆盖这些任务动作空间。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07778">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8f596728264157ae213a96c42e45fb1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd2449a9b103e0c30f6c1b80bc1bf44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d82e64dab393f6187df42d717a23a74b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c6191750e5c3fa01292e5be94d6518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4a528ecab0ac380ef50d6aacf26193d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd174b17553b021885569d4f4dc1ad2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ce184cef58ddd5022120c939e815910.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures"><a href="#Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures" class="headerlink" title="Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures"></a>Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy</strong></p>
<p>Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLP’s potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">training code</a> and <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">weights</a> are public. </p>
<blockquote>
<p>近期外科计算机视觉应用的进展主要得益于仅依赖视觉的模型，这些模型在设计时并没有明确融入丰富的语言语义。这些方法依赖于手动注释的手术视频来预测固定的对象类别集，这限制了它们对未见过的手术程序和下游任务的泛化能力。在这项工作中，我们提出通过公开手术电子学习平台可用的手术视频讲座，可以提供有效的视觉和语言监督信号，用于多模式表示学习，而无需依赖手动注释。我们通过采用多个互补的自动语音识别系统来生成文本转录，以解决手术视频讲座中特有的语言挑战。然后，我们提出了一种新的方法，SurgVLP（手术视觉语言预训练），用于多模式表示学习。在多种手术程序和任务上的广泛实验表明，SurgVLP学习的多模式表示在手术视频分析中表现出强大的可迁移性和适应性。此外，我们的零样本评估凸显了SurgVLP作为手术工作流程分析的通用基础模型的潜力，减少了下游任务对大量手动注释的依赖，并促进了少样本学习等适应方法，为各种下游手术应用构建可扩展和高效的数据解决方案。 <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">训练代码</a>和<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">权重</a>均已公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15220v5">PDF</a> Accepted by Medical Image Analysis (MedIA), 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用公开手术电子学习平台上的手术视频讲座进行多模态表示学习的方法。通过采用多个互补的自动语音识别系统生成文本转录，解决手术视频讲座中的语言挑战。提出一种新型方法SurgVLP（手术视觉语言预训练），该方法可在无需手动注释的情况下，实现多模态表示学习。实验证明，SurgVLP所学习的多模态表示具有较强的迁移性和适应性，在手术视频分析中展现出卓越性能。此外，SurgVLP作为一个通用基础模型，具有潜力用于手术工作流程分析，减少下游任务对大量手动注释的依赖，并促进少样本学习等适应方法的开发，为各种下游手术应用提供可扩展和高效的数据解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期手术计算机视觉应用的进展主要依赖于仅依赖视觉的模型，未明确整合语言的丰富语义。</li>
<li>手动注释的手术视频限制了模型对未见手术程序和下游任务的泛化能力。</li>
<li>公开手术电子学习平台上的手术视频讲座可提供有效的视觉和语言监督信号，用于多模态表示学习，无需手动注释。</li>
<li>采用多个自动语音识别系统生成文本转录，以应对手术视频讲座中的语言挑战。</li>
<li>引入新型方法SurgVLP，实现多模态表示学习，展现出色的迁移性和适应性。</li>
<li>SurgVLP具有作为通用基础模型的潜力，用于手术工作流程分析，并促进少样本学习等方法的开发。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.15220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6fdecff725c84b2d6289a90f5cb446a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1403ccbf9c4143513d490e722b7e05c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c3add5ad8609d1635f8f4d1ed6baed8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e543d07694b38146aa2793cb90a02bf7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-22  Beyond Black Boxes Enhancing Interpretability of Transformers Trained   on Neural Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2097cc03b953e6a60cc7aa86dc447535.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-06-22  Making LLMs Better Many-to-Many Speech-to-Text Translators with   Curriculum Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
