<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  SynPo Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d37b365832bd7f8ebcf6587df784a41c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰çš„å‡ºç°ä¸ºå°‘æ•°åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼ŒåŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SynPoï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLVMsçš„æ— è®­ç»ƒå°‘æ•°æ–¹æ³•ï¼ˆä¾‹å¦‚SAMï¼‰ï¼Œå…¶æ ¸å¿ƒè§è§£æ˜¯æé«˜è´Ÿæç¤ºçš„è´¨é‡ã€‚ä¸ºäº†åœ¨æ›´å¯é çš„ç½®ä¿¡å›¾ä¸Šé€‰æ‹©ç‚¹æç¤ºï¼Œæˆ‘ä»¬ç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ã€‚åŸºäºç½®ä¿¡å›¾ï¼Œæˆ‘ä»¬é€‰æ‹©å‰kä¸ªåƒç´ ä½œä¸ºæ­£ç‚¹é›†ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€‰æ‹©è´Ÿç‚¹é›†ï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªé›†åˆåˆ†åˆ«è¿›è¡Œç‹¬ç«‹çš„K-å‡å€¼èšç±»ã€‚ç„¶åï¼Œè¿™äº›é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œç”¨äºSAMè·å¾—åˆ†å‰²ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„æœ€å…ˆè¿›å°‘æ•°æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰çš„æ— è®­ç»ƒåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•é¢ä¸´æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºçš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºSynPoçš„æ— è®­ç»ƒå°‘æ ·æœ¬æ–¹æ³•ï¼Œæ ¸å¿ƒåœ¨äºæ”¹è¿›è´Ÿæç¤ºçš„è´¨é‡ã€‚é€šè¿‡ç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼Œç”¨äºæ›´å¯é åœ°é€‰æ‹©ç‚¹æç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„æœ€å…ˆè¿›å°‘æ ·æœ¬æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨å°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå…·æœ‰æ–°æœºé‡ã€‚</li>
<li>ç°æœ‰åŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºã€‚</li>
<li>SynPoæ–¹æ³•åŸºäºLVMsæå‡ºï¼Œæ ¸å¿ƒåœ¨äºæ”¹è¿›è´Ÿæç¤ºçš„è´¨é‡ã€‚</li>
<li>è®¾è®¡äº†æ–°å‹ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼Œç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ã€‚</li>
<li>é€šè¿‡ç½®ä¿¡å›¾é€‰æ‹©æ­£è´Ÿç‚¹é›†ï¼Œåˆ©ç”¨é«˜æ–¯åˆ†å¸ƒå’Œç‹¬ç«‹K-meansèšç±»è¿›è¡Œé€‰æ‹©ã€‚</li>
<li>é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œç”¨äºSAMè·å¾—åˆ†å‰²ç»“æœã€‚</li>
<li>å®éªŒè¡¨æ˜SynPoæ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3cbb2811ca359b2646838708d0194be4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Chat-to-Checkup-Can-Large-Language-Models-Assist-in-Diabetes-Prediction"><a href="#From-Chat-to-Checkup-Can-Large-Language-Models-Assist-in-Diabetes-Prediction" class="headerlink" title="From Chat to Checkup: Can Large Language Models Assist in Diabetes   Prediction?"></a>From Chat to Checkup: Can Large Language Models Assist in Diabetes   Prediction?</h2><p><strong>Authors:Shadman Sakib, Oishy Fatema Akhand, Ajwad Abrar</strong></p>
<p>While Machine Learning (ML) and Deep Learning (DL) models have been widely used for diabetes prediction, the use of Large Language Models (LLMs) for structured numerical data is still not well explored. In this study, we test the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and three-shot prompting methods. We conduct an empirical analysis using the Pima Indian Diabetes Database (PIDD). We evaluate six LLMs, including four open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we compare their performance with three traditional machine learning models: Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use accuracy, precision, recall, and F1-score as evaluation metrics. Our results show that proprietary LLMs perform better than open-source ones, with GPT-4o and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably, Gemma-2-27B also outperforms the traditional ML models in terms of F1-score. However, there are still issues such as performance variation across prompting strategies and the need for domain-specific fine-tuning. This study shows that LLMs can be useful for medical prediction tasks and encourages future work on prompt engineering and hybrid approaches to improve healthcare predictions. </p>
<blockquote>
<p>è™½ç„¶æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç³–å°¿ç—…é¢„æµ‹æ–¹é¢å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–æ•°å€¼æ•°æ®æ–¹é¢çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œä¸‰æ ·æœ¬æç¤ºæ–¹æ³•ä¸‹é¢„æµ‹ç³–å°¿ç—…çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨çš®é©¬å°ç¬¬å®‰ç³–å°¿ç—…æ•°æ®åº“è¿›è¡Œå®è¯åˆ†æã€‚æˆ‘ä»¬è¯„ä¼°äº†å…­ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å››ä¸ªå¼€æºæ¨¡å‹ï¼šGemma-2-27Bã€Mistral-7Bã€Llama-3.1-8Bå’ŒLlama-3.2-2Bã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†ä¸¤ä¸ªä¸“æœ‰æ¨¡å‹ï¼šGPT-4oå’ŒGemini Flash 2.0ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å®ƒä»¬çš„æ€§èƒ½ä¸ä¸‰ç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼šéšæœºæ£®æ—ã€é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨å‡†ç¡®æ€§ã€ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¼˜äºå¼€æºæ¨¡å‹ï¼ŒGPT-4oå’ŒGemma-2-27Båœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­å®ç°äº†æœ€é«˜ç²¾åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨F1åˆ†æ•°æ–¹é¢ï¼ŒGemma-2-27Bä¹Ÿä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚ä¸åŒæç¤ºç­–ç•¥ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å’Œéœ€è¦é’ˆå¯¹é¢†åŸŸçš„å¾®è°ƒã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ç”¨äºåŒ»ç–—é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶é¼“åŠ±æœªæ¥åœ¨æç¤ºå·¥ç¨‹å’Œæ··åˆæ–¹æ³•æ–¹é¢è¿›è¡Œå·¥ä½œï¼Œä»¥æ”¹å–„åŒ»ç–—ä¿å¥é¢„æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14949v1">PDF</a> Accepted in 1st IEEE QPAIN 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç³–å°¿ç—…é¢„æµ‹ä¸­çš„åº”ç”¨æ•ˆæœã€‚ç ”ç©¶é‡‡ç”¨é›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œä¸‰æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œä½¿ç”¨Pimaå°ç¬¬å®‰ç³–å°¿ç—…æ•°æ®åº“ï¼ˆPIDDï¼‰è¿›è¡Œå®è¯åˆ†æã€‚å¯¹æ¯”äº†å…­ç§LLMsæ¨¡å‹ï¼ˆåŒ…æ‹¬å››ä¸ªå¼€æºæ¨¡å‹å’Œä¸¤ä¸ªä¸“æœ‰æ¨¡å‹ï¼‰ä¸ä¸‰ç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä»»åŠ¡æ¨¡å‹ï¼ˆéšæœºæ£®æ—ã€é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºï¼‰çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸“æœ‰LLMsæ¨¡å‹è¡¨ç°ä¼˜äºå¼€æºæ¨¡å‹ï¼ŒGPT-4oå’ŒGemma-2-27Båœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰æœ€é«˜å‡†ç¡®æ€§ã€‚Gemma-2-27Båœ¨F1åˆ†æ•°æ–¹é¢ä¼˜äºä¼ ç»ŸMLæ¨¡å‹ã€‚ä½†ä»å­˜åœ¨æç¤ºç­–ç•¥æ€§èƒ½å·®å¼‚å’Œéœ€è¦ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒç­‰é—®é¢˜ã€‚ç ”ç©¶è¯æ˜äº†LLMsåœ¨åŒ»ç–—é¢„æµ‹ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå¹¶é¼“åŠ±æœªæ¥åœ¨æç¤ºå·¥ç¨‹å’Œæ··åˆæ–¹æ³•ä¸Šè¿›è¡Œæ”¹è¿›ä»¥æé«˜åŒ»ç–—é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç³–å°¿ç—…é¢„æµ‹ä¸­çš„åº”ç”¨å°šå¤„äºåˆæ­¥æ¢ç´¢é˜¶æ®µã€‚</li>
<li>ä¸“æœ‰LLMsæ¨¡å‹æ€§èƒ½é€šå¸¸ä¼˜äºå¼€æºæ¨¡å‹ã€‚</li>
<li>GPT-4oå’ŒGemma-2-27Båœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­è¡¨ç°å‡ºæœ€é«˜å‡†ç¡®æ€§ã€‚</li>
<li>Gemma-2-27Båœ¨F1åˆ†æ•°æ–¹é¢ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ ä»»åŠ¡æ¨¡å‹ã€‚</li>
<li>LLMsåœ¨åŒ»ç–—é¢„æµ‹ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç›®å‰ä»å­˜åœ¨æç¤ºç­–ç•¥æ€§èƒ½å·®å¼‚å’Œéœ€è¦ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17d8a43fff527eb32915d70c8c3f798e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90001f8ebf635050e7a69c37ae0ae9c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c4841b021ca577653f4dd1036e028a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-993fb5f8ce288ce64a67a85e1b6fdcd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56eb40665ce05f2f9ca7dd1e6a59d174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c65204d4d5379f80f9c2cd5a9ba6d9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-834f520c7a25657fe1b3d23d5073cfc6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PictSure-Pretraining-Embeddings-Matters-for-In-Context-Learning-Image-Classifiers"><a href="#PictSure-Pretraining-Embeddings-Matters-for-In-Context-Learning-Image-Classifiers" class="headerlink" title="PictSure: Pretraining Embeddings Matters for In-Context Learning Image   Classifiers"></a>PictSure: Pretraining Embeddings Matters for In-Context Learning Image   Classifiers</h2><p><strong>Authors:Lukas Schiesser, Cornelius Wolff, Sophie Haas, Simon Pukrop</strong></p>
<p>Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model â€“ its architecture, pretraining, and training dynamics â€“ at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at <a target="_blank" rel="noopener" href="https://github.com/PictSure/pictsure-library">https://github.com/PictSure/pictsure-library</a>. </p>
<blockquote>
<p>åœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸä¸­ï¼Œæ„å»ºå›¾åƒåˆ†ç±»æ¨¡å‹ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œå› ä¸ºåœ¨æ­¤ç±»æƒ…å†µä¸‹æ”¶é›†å¤§é‡å¸¦æ ‡ç­¾çš„æ•°æ®é›†å¹¶ä¸å®é™…ã€‚æƒ…å¢ƒå­¦ä¹ ï¼ˆICLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ¨¡å¼ï¼Œåœ¨å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆFSICï¼‰ä¸­è„±é¢–è€Œå‡ºï¼Œå®ƒèƒ½å¤Ÿè®©æ¨¡å‹åœ¨è·¨é¢†åŸŸæƒ…å†µä¸‹è¿›è¡Œæ³›åŒ–ï¼Œè€Œæ— éœ€åŸºäºæ¢¯åº¦çš„è°ƒæ•´ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œå¤§å¤šå¿½ç•¥äº†åŸºäºICLçš„FSICç®¡é“ä¸­çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå›¾åƒåµŒå…¥çš„ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PictSureï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥åµŒå…¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„ICLæ¡†æ¶ï¼Œæ¶µç›–äº†å…¶æ¶æ„ã€é¢„è®­ç»ƒå’ŒåŸ¹è®­åŠ¨æ€ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒè§†è§‰ç¼–ç å™¨ç±»å‹ã€é¢„è®­ç»ƒç›®æ ‡å’Œå¾®è°ƒç­–ç•¥å¯¹ä¸‹æ¸¸FSICæ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒæˆåŠŸå’Œè·¨é¢†åŸŸæ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåµŒå…¥æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼ã€‚å› æ­¤ï¼ŒPictSureæˆåŠŸåœ°åœ¨ä¸è®­ç»ƒåˆ†å¸ƒå·®å¼‚æ˜¾è‘—çš„è·¨é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºäºICLçš„FSICæ¨¡å‹ï¼ŒåŒæ—¶åœ¨å†…éƒ¨ä»»åŠ¡ä¸Šç»´æŒäº†ç›¸å½“çš„ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PictSure/pictsure-library%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PictSure/pictsure-libraryæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14842v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸï¼Œæ„å»ºå›¾åƒåˆ†ç±»æ¨¡å‹ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œå› ä¸ºåœ¨æ­¤ç±»æƒ…å†µä¸‹æ”¶é›†å¤§é‡æœ‰æ ‡ç­¾çš„æ•°æ®é›†å¹¶ä¸å®ç”¨ã€‚æƒ…å¢ƒå­¦ä¹ ï¼ˆICLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ¨¡å¼ï¼Œåœ¨å°‘é‡å›¾åƒåˆ†ç±»ï¼ˆFSICï¼‰ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€æ¢¯åº¦è°ƒæ•´çš„æƒ…å†µä¸‹è·¨é¢†åŸŸè¿›è¡Œæ¨å¹¿ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ICLåŸºäºFSICç®¡é“çš„å…³é”®ç»„æˆéƒ¨åˆ†â€”â€”å›¾åƒåµŒå…¥çš„ä½œç”¨ã€‚åœ¨æœ¬æ¬¡ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PictSureï¼Œä¸€ç§ä»¥åµŒå…¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„ICLæ¡†æ¶ï¼Œæ·±å…¥åˆ†æäº†å…¶æ¶æ„ã€é¢„è®­ç»ƒå’Œè®­ç»ƒåŠ¨æ€ç­‰æ–¹é¢çš„å½±å“ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒè§†è§‰ç¼–ç å™¨ç±»å‹ã€é¢„è®­ç»ƒç›®æ ‡å’Œå¾®è°ƒç­–ç•¥å¯¹ä¸‹æ¸¸FSICæ€§èƒ½çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒæˆåŠŸåº¦å’Œè·¨åŸŸæ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåµŒå…¥æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼ã€‚å› æ­¤ï¼ŒPictSureåœ¨è®­ç»ƒåˆ†å¸ƒå·®å¼‚æ˜¾è‘—çš„è·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„ICLåŸºäºFSICæ¨¡å‹ï¼ŒåŒæ—¶åœ¨åŸŸå†…ä»»åŠ¡ä¸Šä¿æŒç›¸å½“çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLï¼ˆIn-context Learningï¼‰åœ¨å°‘é‡å›¾åƒåˆ†ç±»ï¼ˆFSICï¼‰ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸã€‚</li>
<li>PictSureæ˜¯ä¸€ä¸ªä»¥åµŒå…¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„ICLæ¡†æ¶ï¼Œæ³¨é‡åˆ†æå›¾åƒåˆ†ç±»æ¨¡å‹ä¸­çš„æ¶æ„ã€é¢„è®­ç»ƒå’Œè®­ç»ƒåŠ¨æ€ç­‰æ–¹é¢ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºä¸åŒè§†è§‰ç¼–ç å™¨ç±»å‹ã€é¢„è®­ç»ƒç›®æ ‡å’Œå¾®è°ƒç­–ç•¥å¯¹ä¸‹æ¸¸FSICæ€§èƒ½å½±å“æ˜¾è‘—ã€‚</li>
<li>è®­ç»ƒæˆåŠŸåº¦å’Œè·¨åŸŸæ€§èƒ½é«˜åº¦ä¾èµ–äºåµŒå…¥æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼ã€‚</li>
<li>PictSureåœ¨è·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰ICL-based FSICæ¨¡å‹ã€‚</li>
<li>PictSureåœ¨ç»´æŒåŸŸå†…ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†è·¨åŸŸæ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-777463465b46abeeeced774108ba5353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a36ef25757d1f3803674b416d9016f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-240dea26c73071e055feb3ffdb858b9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a0bbc685fa78ad1e3f8a3b8a8a5772.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Treasure-Hunt-Real-time-Targeting-of-the-Long-Tail-using-Training-Time-Markers"><a href="#Treasure-Hunt-Real-time-Targeting-of-the-Long-Tail-using-Training-Time-Markers" class="headerlink" title="Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time   Markers"></a>Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time   Markers</h2><p><strong>Authors:Daniel Dâ€™souza, Julia Kreutzer, Adrien Morisot, Ahmet ÃœstÃ¼n, Sara Hooker</strong></p>
<p>One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: â€œCan we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?â€ We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations. </p>
<blockquote>
<p>ç°ä»£æœºå™¨å­¦ä¹ é¢ä¸´çš„æœ€æ·±åˆ»æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å®ç°å¯¹ç½•è§å’Œä»£è¡¨æ€§ä¸è¶³çš„ç‰¹å¾çš„é•¿å°¾è¡¨ç°è‰¯å¥½ã€‚å¤§å‹é€šç”¨æ¨¡å‹ç»è¿‡é’ˆå¯¹è®¸å¤šä»»åŠ¡çš„è®­ç»ƒï¼Œä½†åœ¨é«˜é¢‘ç”¨ä¾‹ä¸Šè¡¨ç°æœ€ä½³ã€‚è®­ç»ƒåï¼Œå¾ˆéš¾ä½¿æ¨¡å‹é€‚åº”åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­ä»£è¡¨æ€§ä¸è¶³çš„å…·ä½“ç”¨ä¾‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚ä¾èµ–æç¤ºå·¥ç¨‹æˆ–å°‘æ•°æ ·æœ¬ç¤ºä¾‹æ¥æœ€å¤§åŒ–ç‰¹å®šæµ‹è¯•ç”¨ä¾‹çš„è¾“å‡ºè´¨é‡å¯èƒ½ä¼šä»¤äººæ²®ä¸§ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½ä¼šå¯¹å¾®å°å˜åŒ–é«˜åº¦æ•æ„Ÿï¼Œäº§ç”Ÿä¸å¯é¢„æµ‹çš„ååº”ï¼Œæˆ–è€…ä¾èµ–å›ºå®šçš„ç³»ç»Ÿæç¤ºæ¥ç»´æŒæ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šâ€œæˆ‘ä»¬æ˜¯å¦å¯ä»¥ä¼˜åŒ–æˆ‘ä»¬çš„è®­ç»ƒåè®®ï¼Œä»¥åœ¨æ¨ç†æ—¶æé«˜å¯æ§æ€§å’Œåœ¨ä»£è¡¨æ€§ä¸è¶³çš„ç”¨ä¾‹ä¸Šçš„æ€§èƒ½ï¼Ÿâ€æˆ‘ä»¬é‡æ–°å®¡è§†äº†è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ä¹‹é—´çš„ç•Œé™ï¼Œä»¥æé«˜é•¿å°¾æ€§èƒ½ï¼ŒåŒæ—¶ä¸ºç”¨æˆ·æä¾›ä¸€ç³»åˆ—æ§åˆ¶æ†ï¼Œè¯¥æ¨¡å‹è¢«è®­ç»ƒæˆå¯¹æ§åˆ¶æ†åšå‡ºååº”ã€‚æˆ‘ä»¬åˆ›å»ºäº†æ•°æ®ç‰¹æ€§å’Œä»»åŠ¡æ¥æºçš„è¯¦ç»†åˆ†ç±»ï¼Œä»¥æ˜ç¡®æ§åˆ¶ç”Ÿæˆå±æ€§å’Œåœ¨æ¨ç†æ—¶éšå¼åœ°è°ƒèŠ‚ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è‡ªåŠ¨æ¨æ–­è¿™äº›æ ‡è®°ï¼Œä½¿å…¶åœ¨æ¨ç†æ—¶å˜å¾—å¯é€‰ã€‚è¿™ç§æœ‰åŸåˆ™ä¸”çµæ´»çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šäº§ç”Ÿäº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒåˆ†å¸ƒçš„é•¿å°¾ç¤ºä¾‹ä¸­ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°å¼€æ”¾å¼ç”Ÿæˆè´¨é‡å¹³å‡æå‡äº†5.7%çš„èƒœç‡ä½¿ç”¨æˆ‘ä»¬çš„æ ‡è®°ï¼Œåœ¨ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸé‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°äº†è¶…è¿‡9.1%çš„å¢å¹…ã€‚åœ¨ä»£è¡¨æ€§ä¸è¶³çš„ä»»åŠ¡ï¼ˆå¦‚ä»£ç ä¿®å¤ï¼‰ä¸Šï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ç›¸å¯¹æå‡äº†é«˜è¾¾14.1%ï¼Œåœ¨éµå¾ªé•¿åº¦æŒ‡ä»¤çš„è¯„ä¼°ä¸Šå®ç°äº†ç»å¯¹æ”¹å–„35.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨ç°ä»£æœºå™¨å­¦ä¹ é¢ä¸´çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå³å¦‚ä½•åœ¨ç½•è§å’Œä»£è¡¨æ€§ä¸è¶³çš„ç‰¹å¾çš„é•¿å°¾éƒ¨åˆ†è¡¨ç°è‰¯å¥½ã€‚å¤§å‹é€šç”¨æ¨¡å‹è™½ç„¶ç»è¿‡å¤šé¡¹ä»»åŠ¡è®­ç»ƒï¼Œä½†åœ¨é«˜é¢‘ç”¨ä¾‹ä¸Šè¡¨ç°æœ€ä½³ã€‚è®­ç»ƒåï¼Œéš¾ä»¥é€‚åº”æ¨¡å‹ä»¥åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ç‰¹å®šç”¨ä¾‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚æœ¬æ–‡æ—¨åœ¨ä¼˜åŒ–è®­ç»ƒåè®®ï¼Œä»¥æé«˜åœ¨ä»£è¡¨æ€§ä¸è¶³ç”¨ä¾‹ä¸Šçš„å¯æ§æ€§å’Œæ€§èƒ½ã€‚é€šè¿‡é‡æ–°å®¡è§†è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ï¼Œæˆ‘ä»¬åœ¨æé«˜é•¿å°¾æ€§èƒ½çš„åŒæ—¶ï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ç³»åˆ—æ§åˆ¶æ†ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½å¯¹ä¸€ç³»åˆ—æ§åˆ¶æ æ†ä½œå‡ºå“åº”ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè¯¦ç»†çš„æ•°æ®ç‰¹æ€§å’Œä»»åŠ¡æ¥æºåˆ†ç±»æ³•ï¼Œä»¥åœ¨æ¨ç†æ—¶æ˜¾å¼æ§åˆ¶ç”Ÿæˆå±æ€§å’Œéšå¼æ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è‡ªåŠ¨æ¨æ–­è¿™äº›æ ‡è®°ï¼Œä½¿å…¶åœ¨æ¨ç†æ—¶å˜å¾—å¯é€‰ã€‚è¿™ç§æœ‰åŸåˆ™ä¸”çµæ´»çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šäº§ç”Ÿäº†æ˜¾è‘—çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒåˆ†å¸ƒçš„é•¿å°¾ç¤ºä¾‹ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æœºå™¨å­¦ä¹ é¢ä¸´åœ¨ç½•è§å’Œä»£è¡¨æ€§ä¸è¶³çš„ç‰¹å¾ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹é€šç”¨æ¨¡å‹åœ¨é«˜é¢‘ç”¨ä¾‹ä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†åœ¨ç‰¹å®šç”¨ä¾‹ä¸Šéš¾ä»¥é€‚åº”ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–è®­ç»ƒåè®®ï¼Œå¯ä»¥æé«˜åœ¨ä»£è¡¨æ€§ä¸è¶³ç”¨ä¾‹ä¸Šçš„å¯æ§æ€§å’Œæ€§èƒ½ã€‚</li>
<li>æä¾›äº†å¯¹è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯çš„é‡æ–°å®¡è§†ï¼Œä»¥æé«˜é•¿å°¾æ€§èƒ½ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªè¯¦ç»†çš„æ•°æ®ç‰¹æ€§å’Œä»»åŠ¡æ¥æºåˆ†ç±»æ³•ï¼Œä»¥æ§åˆ¶ç”Ÿæˆå±æ€§å’Œæ¡ä»¶ç”Ÿæˆã€‚</li>
<li>é€šè¿‡å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥è‡ªåŠ¨æ¨æ–­ç”¨äºæ§åˆ¶ç”Ÿæˆå±æ€§çš„æ ‡è®°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-437b0f37b38bd8debcec48a0ae2a31da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-540405923942c8489abd9e8943b9dd9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2c7affc4d2cf89f21c7185713a51477.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0818a884839b17459841f3827673bd66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2200594ccd871c4387db470f6cf18d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeMaster-Training-Time-Series-Multimodal-LLMs-to-Reason-via-Reinforcement-Learning"><a href="#TimeMaster-Training-Time-Series-Multimodal-LLMs-to-Reason-via-Reinforcement-Learning" class="headerlink" title="TimeMaster: Training Time-Series Multimodal LLMs to Reason via   Reinforcement Learning"></a>TimeMaster: Training Time-Series Multimodal LLMs to Reason via   Reinforcement Learning</h2><p><strong>Authors:Junru Zhang, Lang Feng, Xu Guo, Yuhan Wu, Yabo Dong, Duanqing Xu</strong></p>
<p>Time-series reasoning remains a significant challenge in multimodal large language models (MLLMs) due to the dynamic temporal patterns, ambiguous semantics, and lack of temporal priors. In this work, we introduce TimeMaster, a reinforcement learning (RL)-based method that enables time-series MLLMs to perform structured, interpretable reasoning directly over visualized time-series inputs and task prompts. TimeMaster adopts a three-part structured output format, reasoning, classification, and domain-specific extension, and is optimized via a composite reward function that aligns format adherence, prediction accuracy, and open-ended insight quality. The model is trained using a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish a good initialization, followed by Group Relative Policy Optimization (GRPO) at the token level to enable stable and targeted reward-driven improvement in time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster achieves state-of-the-art performance, outperforming both classical time-series models and few-shot GPT-4o by over 14.6% and 7.3% performance gain, respectively. Notably, TimeMaster goes beyond time-series classification: it also exhibits expert-like reasoning behavior, generates context-aware explanations, and delivers domain-aligned insights. Our results highlight that reward-driven RL can be a scalable and promising path toward integrating temporal understanding into time-series MLLMs. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—æ¨ç†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå­˜åœ¨åŠ¨æ€æ—¶é—´æ¨¡å¼ã€è¯­ä¹‰æ¨¡ç³Šä»¥åŠç¼ºä¹æ—¶é—´å…ˆéªŒçŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†TimeMasterï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œä½¿æ—¶é—´åºåˆ—MLLMsèƒ½å¤Ÿç›´æ¥å¯¹å¯è§†åŒ–æ—¶é—´åºåˆ—è¾“å…¥å’Œä»»åŠ¡æç¤ºè¿›è¡Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†ã€‚TimeMasteré‡‡ç”¨ä¸‰éƒ¨åˆ†çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…æ‹¬æ¨ç†ã€åˆ†ç±»å’Œé¢†åŸŸç‰¹å®šæ‰©å±•ï¼Œå¹¶é€šè¿‡ç»„åˆå¥–åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥å‡½æ•°å­—ç¬¦åˆæ ¼å¼éµå¾ªã€é¢„æµ‹å‡†ç¡®æ€§å’Œå¼€æ”¾æ€§æ´å¯Ÿè´¨é‡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼šæˆ‘ä»¬é¦–å…ˆåº”ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å»ºç«‹è‰¯å¥½çš„åˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡ä»¤ç‰Œçº§åˆ«çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å®ç°æ—¶é—´åºåˆ—æ¨ç†çš„ç¨³å®šæ€§å’Œé’ˆå¯¹æ€§å¥–åŠ±é©±åŠ¨æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨åŸºäºQwen2.5-VL-3B-Instructçš„TimerBedåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…­ä¸ªçœŸå®ä¸–ç•Œåˆ†ç±»ä»»åŠ¡è¯„ä¼°äº†TimeMasterã€‚TimeMasterè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç»å…¸çš„æ—¶é—´åºåˆ—æ¨¡å‹å’Œå°‘é•œå¤´GPT-4oåˆ†åˆ«æé«˜äº†è¶…è¿‡14.6%å’Œ7.3%çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTimeMasterä¸ä»…è¶…è¶Šæ—¶é—´åºåˆ—åˆ†ç±»ï¼šå®ƒè¿˜è¡¨ç°å‡ºä¸“å®¶çº§çš„æ¨ç†è¡Œä¸ºï¼Œç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§£é‡Šï¼Œå¹¶æä¾›ä¸é¢†åŸŸç›¸ç¬¦çš„è§è§£ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œå¥–åŠ±é©±åŠ¨çš„RLå¯ä»¥æ˜¯ä¸€æ¡å¯æ‰©å±•ä¸”å‰é€”å…‰æ˜çš„è·¯å¾„ï¼Œæ—¨åœ¨å°†æ—¶é—´ç†è§£æ•´åˆåˆ°æ—¶é—´åºåˆ—MLLMsä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13705v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†TimeMasterï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ—¶é—´åºåˆ—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•ã€‚TimeMasterèƒ½åœ¨å¯è§†åŒ–æ—¶é—´åºåˆ—è¾“å…¥å’Œä»»åŠ¡æç¤ºä¸Šè¿›è¡Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†ã€‚é‡‡ç”¨ä¸‰é˜¶æ®µç»“æ„è¾“å‡ºæ ¼å¼ï¼Œå¹¶é€šè¿‡ç»„åˆå¥–åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†æ—¶é—´åºåˆ—æ¨ç†çš„æ–°æ°´å¹³ã€‚é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„ç®¡é“è®­ç»ƒæ¨¡å‹ï¼Œå…ˆç”¨ç›‘ç£å¾®è°ƒè¿›è¡Œåˆå§‹åŒ–ï¼Œå†ç”¨åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–åœ¨ä»¤ç‰Œçº§åˆ«è¿›è¡Œå¥–åŠ±é©±åŠ¨æ”¹è¿›ã€‚TimeMasteråœ¨TimerBedåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°çªå‡ºï¼Œå±•ç°å‡ºè¶…è¶Šç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚é™¤äº†æ—¶é—´åºåˆ—åˆ†ç±»å¤–ï¼ŒTimeMasterè¿˜å±•ç°å‡ºä¸“å®¶çº§çš„æ¨ç†è¡Œä¸ºï¼Œç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„è§£é‡Šå’Œé¢†åŸŸå¯¹é½çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeMasteræ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>TimeMasterèƒ½ç›´æ¥å¤„ç†å¯è§†åŒ–æ—¶é—´åºåˆ—è¾“å…¥å’Œä»»åŠ¡æç¤ºï¼Œè¿›è¡Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†ã€‚</li>
<li>TimeMasteré‡‡ç”¨ä¸‰é˜¶æ®µç»“æ„è¾“å‡ºæ ¼å¼å¹¶ä¼˜åŒ–ç»„åˆå¥–åŠ±å‡½æ•°ä»¥å¯¹é½æ ¼å¼éµå®ˆã€é¢„æµ‹å‡†ç¡®æ€§å’Œå¼€æ”¾æ€§çš„è§è§£è´¨é‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šç›‘ç£å¾®è°ƒè¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶åé‡‡ç”¨åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¥–åŠ±é©±åŠ¨æ”¹è¿›ã€‚</li>
<li>TimeMasteråœ¨TimerBedåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¶…è¶Šç°æœ‰æ¨¡å‹è¡¨ç°ï¼Œå…·æœ‰æ›´é«˜çš„åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>TimeMasterå±•ç°äº†ä¸“å®¶çº§çš„æ¨ç†è¡Œä¸ºï¼Œå¯ä»¥ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„è§£é‡Šå’Œä¸ç‰¹å®šé¢†åŸŸç›¸å…³çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d7e3a8ffa7f76ded1a1ce418de9e788e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653f022d516ce712bb1a28c7eccc2dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f608a58011de70a08635568edff806d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Verifying-the-Verifiers-Unveiling-Pitfalls-and-Potentials-in-Fact-Verifiers"><a href="#Verifying-the-Verifiers-Unveiling-Pitfalls-and-Potentials-in-Fact-Verifiers" class="headerlink" title="Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact   Verifiers"></a>Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact   Verifiers</h2><p><strong>Authors:Wooseok Seo, Seungju Han, Jaehun Jung, Benjamin Newman, Seungwon Lim, Seungbeen Lee, Ximing Lu, Yejin Choi, Youngjae Yu</strong></p>
<p>Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at <a target="_blank" rel="noopener" href="https://github.com/just1nseo/verifying-the-verifiers">https://github.com/just1nseo/verifying-the-verifiers</a> </p>
<blockquote>
<p>äº‹å®æ ¸æŸ¥å¯¹äºç¡®ä¿LLMåº”ç”¨ç¨‹åºçš„å¯é æ€§è‡³å…³é‡è¦ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ª14ä¸ªäº‹å®æ ¸æŸ¥åŸºå‡†çš„ç¤ºä¾‹é›†ï¼Œå¯¹12ä¸ªé¢„è®­ç»ƒLLMå’Œä¸€ä¸ªä¸“é—¨çš„äº‹å®æ ¸æŸ¥å™¨è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å‰æ²¿LLMå’Œå¼€æ”¾æƒé‡æ¨ç†LLMã€‚æˆ‘ä»¬åˆ†äº«äº†ä¸‰é¡¹æ—¨åœ¨æŒ‡å¯¼æœªæ¥å¼€å‘æ›´ç¨³å¥çš„äº‹å®æ ¸æŸ¥å™¨çš„å‘ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è§£å†³æ•°æ®é›†ä¸­çš„æ³¨é‡Šé”™è¯¯å’Œæ¨¡ç³Šæ€§çš„é‡è¦æ€§ï¼Œè¡¨æ˜å¤§çº¦16ï¼…çš„æ¨¡ç³Šæˆ–é”™è¯¯æ ‡è®°çš„æ•°æ®ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ’åã€‚å¿½ç•¥æ­¤é—®é¢˜å¯èƒ½å¯¼è‡´æ¯”è¾ƒè¯„ä¼°æ—¶å¾—å‡ºè¯¯å¯¼æ€§ç»“è®ºï¼Œæˆ‘ä»¬å»ºè®®é‡‡ç”¨ä¸€ä¸ªç³»ç»Ÿçš„ç®¡é“ï¼Œåˆ©ç”¨LLMä½œä¸ºæ³•å®˜ï¼Œå¸®åŠ©å¤§è§„æ¨¡è¯†åˆ«è¿™äº›é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°å…·æœ‰å°‘é‡ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„å‰æ²¿LLMï¼ˆä»¥å‰çš„ç ”ç©¶ç»å¸¸å¿½ç•¥è¿™ä¸€ç‚¹ï¼‰è¾¾åˆ°äº†é¡¶å°–çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®æœªæ¥çš„ç ”ç©¶è¦ä¸è¿™äº›ç®€å•ä½†é«˜æ•ˆçš„åŸºæœ¬çº¿è¿›è¡Œæ¯”è¾ƒã€‚æœ€åï¼Œå°½ç®¡å®ƒä»¬å¾ˆæœ‰æ•ˆï¼Œä½†å‰æ²¿LLMä¼šå¸¦æ¥å·¨å¤§çš„æˆæœ¬ï¼Œè¿™æ¨åŠ¨äº†ç²¾ç»†è°ƒæ•´çš„äº‹å®æ ¸æŸ¥å™¨çš„å¼€å‘ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™äº›å°å‹æ¨¡å‹ä»æœ‰æ”¹è¿›çš„ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†çš„å®ä¾‹ä¸­ã€‚ä»¤äººé¼“èˆçš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜é€šè¿‡åˆæˆå¤šè·³æ¨ç†æ•°æ®æ¥å¢å¼ºè®­ç»ƒä¼šæ˜¾è‘—æé«˜è¿™äº›æ¨¡å‹åœ¨æ­¤ç±»å®ä¾‹ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/just1nseo/verifying-the-verifiers%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/just1nseo/verifying-the-verifiersä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†12ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œ1ä¸ªä¸“é—¨çš„éªŒè¯å™¨åœ¨äº‹å®æ ¸æŸ¥æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°åœ¨æ•°æ®é›†æ ‡æ³¨é”™è¯¯å’Œæ¨¡ç³Šæ€§é—®é¢˜ä¸Šéœ€é‡è§†ï¼Œçº¦16%çš„æ¨¡ç³Šæˆ–é”™è¯¯æ ‡æ³¨æ•°æ®ä¼šå½±å“æ¨¡å‹æ’åã€‚åŒæ—¶ï¼Œå‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘æ•°åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜å¼‚ï¼Œä½†æˆæœ¬è¾ƒé«˜ï¼Œå»ºè®®å¼€å‘å°å‹ä¸”ç²¾ç»†è°ƒæ•´çš„éªŒè¯å™¨ã€‚é€šè¿‡åˆæˆå¤šè·³æ¨ç†æ•°æ®å¢å¼ºè®­ç»ƒï¼Œå¯æé«˜æ¨¡å‹åœ¨å¤æ‚å®ä¾‹ä¸­çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é›†çš„æ ‡æ³¨é”™è¯¯å’Œæ¨¡ç³Šæ€§å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°æœ‰é‡è¦å½±å“ï¼Œå¤§çº¦16%çš„é—®é¢˜æ•°æ®ä¼šå½±å“æ¨¡å‹æ’åã€‚</li>
<li>å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘æ•°åœºæ™¯ï¼ˆfew-shotï¼‰ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œåº”é‡è§†å…¶åœ¨äº‹å®æ ¸æŸ¥é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>åœ¨äº‹å®æ ¸æŸ¥é¢†åŸŸï¼Œå¼€å‘å°å‹ä¸”ç²¾ç»†è°ƒæ•´çš„éªŒè¯å™¨æ˜¯ä¸€ä¸ªå¯è¡Œæ–¹å‘ï¼Œä»¥é™ä½æ¨¡å‹æˆæœ¬ã€‚</li>
<li>é€šè¿‡åˆæˆå¤šè·³æ¨ç†æ•°æ®å¢å¼ºè®­ç»ƒå¯ä»¥æé«˜LLMåœ¨å¤æ‚å®ä¾‹ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>ç³»ç»Ÿæ€§åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ¤æ–­å·¥å…·å¯ä»¥å¸®åŠ©è¯†åˆ«æ•°æ®é›†ä¸­çš„é—®é¢˜ã€‚</li>
<li>åœ¨æœªæ¥çš„ç ”ç©¶ä¸­ï¼Œéœ€è¦åŒ…å«ä¸å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29dabf2ae5239d12d9a10f2a52c7372c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c3839fc8cb4fe71a48ddc2f7835e615.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a84721fda063926294dc6e282f8a97ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-824e46453f5f49577208be1a9044fe40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1817debcf905f75cb6406cb5feb390f3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Active-Multimodal-Distillation-for-Few-shot-Action-Recognition"><a href="#Active-Multimodal-Distillation-for-Few-shot-Action-Recognition" class="headerlink" title="Active Multimodal Distillation for Few-shot Action Recognition"></a>Active Multimodal Distillation for Few-shot Action Recognition</h2><p><strong>Authors:Weijia Feng, Yichen Zhu, Ruojia Zhang, Chenyang Wang, Fei Ma, Xiaobao Wang, Xiaobai Li</strong></p>
<p>Owing to its rapid progress and broad application prospects, few-shot action recognition has attracted considerable interest. However, current methods are predominantly based on limited single-modal data, which does not fully exploit the potential of multimodal information. This paper presents a novel framework that actively identifies reliable modalities for each sample using task-specific contextual cues, thus significantly improving recognition performance. Our framework integrates an Active Sample Inference (ASI) module, which utilizes active inference to predict reliable modalities based on posterior distributions and subsequently organizes them accordingly. Unlike reinforcement learning, active inference replaces rewards with evidence-based preferences, making more stable predictions. Additionally, we introduce an active mutual distillation module that enhances the representation learning of less reliable modalities by transferring knowledge from more reliable ones. Adaptive multimodal inference is employed during the meta-test to assign higher weights to reliable modalities. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing approaches. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬åŠ¨ä½œè¯†åˆ«å› å…¶å¿«é€Ÿè¿›æ­¥å’Œå¹¿æ³›åº”ç”¨å‰æ™¯è€Œå¼•èµ·äº†äººä»¬çš„æå¤§å…´è¶£ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦åŸºäºæœ‰é™çš„å•æ¨¡æ€æ•°æ®ï¼Œå¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨ç‰¹å®šä»»åŠ¡çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œä¸»åŠ¨ä¸ºæ¯ä¸ªæ ·æœ¬è¯†åˆ«å¯é çš„æ¨¡æ€ï¼Œä»è€Œæ˜¾è‘—æé«˜è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸€ä¸ªä¸»åŠ¨æ ·æœ¬æ¨ç†ï¼ˆASIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ä¸»åŠ¨æ¨ç†ï¼ŒåŸºäºåéªŒåˆ†å¸ƒé¢„æµ‹å¯é çš„æ¨¡æ€ï¼Œç„¶åç›¸åº”åœ°ç»„ç»‡å®ƒä»¬ã€‚ä¸å¼ºåŒ–å­¦ä¹ ä¸åŒï¼Œä¸»åŠ¨æ¨ç†ç”¨åŸºäºè¯æ®çš„åå¥½å–ä»£å¥–åŠ±ï¼Œä»è€Œåšå‡ºæ›´ç¨³å®šçš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªä¸»åŠ¨ç›¸äº’è’¸é¦æ¨¡å—ï¼Œé€šè¿‡ä»æ›´å¯é çš„æ¨¡æ€è½¬ç§»çŸ¥è¯†ï¼Œå¢å¼ºå¯¹ä¸å¤ªå¯é æ¨¡æ€çš„è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨å…ƒæµ‹è¯•è¿‡ç¨‹ä¸­é‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€æ¨ç†ï¼Œä¸ºå¯é æ¨¡æ€åˆ†é…æ›´é«˜æƒé‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13322v1">PDF</a> IJCAI 2025, the 34th International Joint Conference on Artificial   Intelligence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡çº¿ç´¢çš„ä¸»åŠ¨æ¨¡æ€è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡Active Sample Inferenceæ¨¡å—é¢„æµ‹å¯é çš„æ¨¡æ€ï¼Œå¹¶æ®æ­¤è¿›è¡Œç»„ç»‡ï¼Œä»è€Œæé«˜å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«çš„æ€§èƒ½ã€‚å¼•å…¥ä¸»åŠ¨äº’è’¸é¦æ¨¡å—ï¼Œé€šè¿‡ä»å¯é çš„æ¨¡æ€è½¬ç§»çŸ¥è¯†ï¼Œå¢å¼ºä¸å¯é æ¨¡æ€çš„è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨å…ƒæµ‹è¯•é˜¶æ®µé‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€æ¨ç†ï¼Œä¸ºå¯é çš„æ¨¡æ€åˆ†é…æ›´é«˜çš„æƒé‡ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«æ–¹æ³•ä¸»è¦åŸºäºå•ä¸€æ¨¡æ€æ•°æ®ï¼Œæœªå……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„æ½œåŠ›ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„ä¸Šä¸‹æ–‡çº¿ç´¢æ¥ä¸»åŠ¨è¯†åˆ«å¯é çš„æ¨¡æ€ã€‚</li>
<li>å¼•å…¥Active Sample Inferenceï¼ˆASIï¼‰æ¨¡å—ï¼ŒåŸºäºåéªŒåˆ†å¸ƒè¿›è¡Œé¢„æµ‹å¹¶æ•´ç†å¯é çš„æ¨¡æ€ã€‚</li>
<li>ä¸å¼ºåŒ–å­¦ä¹ ä¸åŒï¼Œä¸»åŠ¨æ¨ç†é€šè¿‡åŸºäºè¯æ®çš„åå¥½æ›¿ä»£å¥–åŠ±ï¼Œé¢„æµ‹æ›´åŠ ç¨³å®šã€‚</li>
<li>è®ºæ–‡è¿˜å¼•å…¥äº†ä¸»åŠ¨äº’è’¸é¦æ¨¡å—ï¼Œæé«˜äº†å¯¹ä¸å¯é æ¨¡æ€çš„è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡ä»å¯é çš„æ¨¡æ€è½¬ç§»çŸ¥è¯†ã€‚</li>
<li>åœ¨å…ƒæµ‹è¯•é˜¶æ®µé‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€æ¨ç†ï¼Œä¸ºå¯é æ¨¡æ€åˆ†é…æ›´é«˜æƒé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9372d3b6c58dc61ea2cb4a41cec8f33e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f6e2d01ac33ed8d3e38d32b821de19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c5b43a93e9e5fa0e4ed388f0409822c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d5ab87654000c83794b0a91dc6b2634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e881509d061811674cf7aed5b82e6f3c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MetaEformer-Unveiling-and-Leveraging-Meta-patterns-for-Complex-and-Dynamic-Systems-Load-Forecasting"><a href="#MetaEformer-Unveiling-and-Leveraging-Meta-patterns-for-Complex-and-Dynamic-Systems-Load-Forecasting" class="headerlink" title="MetaEformer: Unveiling and Leveraging Meta-patterns for Complex and   Dynamic Systems Load Forecasting"></a>MetaEformer: Unveiling and Leveraging Meta-patterns for Complex and   Dynamic Systems Load Forecasting</h2><p><strong>Authors:Shaoyuan Huang, Tiancheng Zhang, Zhongtian Zhang, Xiaofei Wang, Lanjun Wang, Xin Wang</strong></p>
<p>Time series forecasting is a critical and practical problem in many real-world applications, especially for industrial scenarios, where load forecasting underpins the intelligent operation of modern systems like clouds, power grids and traffic networks.However, the inherent complexity and dynamics of these systems present significant challenges. Despite advances in methods such as pattern recognition and anti-non-stationarity have led to performance gains, current methods fail to consistently ensure effectiveness across various system scenarios due to the intertwined issues of complex patterns, concept-drift, and few-shot problems. To address these challenges simultaneously, we introduce a novel scheme centered on fundamental waveform, a.k.a., meta-pattern. Specifically, we develop a unique Meta-pattern Pooling mechanism to purify and maintain meta-patterns, capturing the nuanced nature of system loads. Complementing this, the proposed Echo mechanism adaptively leverages the meta-patterns, enabling a flexible and precise pattern reconstruction. Our Meta-pattern Echo transformer (MetaEformer) seamlessly incorporates these mechanisms with the transformer-based predictor, offering end-to-end efficiency and interpretability of core processes. Demonstrating superior performance across eight benchmarks under three system scenarios, MetaEformer marks a significant advantage in accuracy, with a 37% relative improvement on fifteen state-of-the-art baselines. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯è®¸å¤šç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å…³é”®å®é™…é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¥ä¸šåœºæ™¯ä¸­ï¼Œè´Ÿè½½é¢„æµ‹æ˜¯äº‘ã€ç”µç½‘å’Œäº¤é€šç½‘ç»œç­‰ç°ä»£ç³»ç»Ÿæ™ºèƒ½è¿è¡Œçš„åŸºç¡€ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿçš„å›ºæœ‰å¤æ‚æ€§å’ŒåŠ¨æ€æ€§å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡æ¨¡å¼è¯†åˆ«å’ŒæŠ—éå¹³ç¨³æ€§ç­‰æ–¹æ³•çš„å‘å±•æé«˜äº†æ€§èƒ½ï¼Œä½†ç”±äºå¤æ‚æ¨¡å¼ã€æ¦‚å¿µæ¼‚ç§»å’Œå°‘æ ·æœ¬é—®é¢˜çš„äº¤ç»‡ï¼Œå½“å‰çš„æ–¹æ³•æ— æ³•åœ¨å„ç§ç³»ç»Ÿåœºæ™¯ä¸­æŒç»­ç¡®ä¿æœ‰æ•ˆæ€§ã€‚ä¸ºäº†åŒæ—¶è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä»¥åŸºæœ¬æ³¢å½¢ï¼ˆä¹Ÿç§°ä¸ºå…ƒæ¨¡å¼ï¼‰ä¸ºä¸­å¿ƒçš„æ–°å‹æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç‹¬ç‰¹çš„å…ƒæ¨¡å¼æ± åŒ–æœºåˆ¶æ¥å‡€åŒ–å’Œç»´æŠ¤å…ƒæ¨¡å¼ï¼Œæ•æ‰ç³»ç»Ÿè´Ÿè½½çš„å¾®å¦™æ€§è´¨ã€‚ä½œä¸ºè¡¥å……ï¼Œæˆ‘ä»¬æå‡ºçš„å›å£°æœºåˆ¶è‡ªé€‚åº”åœ°åˆ©ç”¨å…ƒæ¨¡å¼ï¼Œå®ç°çµæ´»å’Œç²¾ç¡®çš„æ¨¡å¼é‡å»ºã€‚æˆ‘ä»¬çš„å…ƒæ¨¡å¼å›å£°è½¬æ¢å™¨ï¼ˆMetaEformerï¼‰æ— ç¼åœ°å°†è¿™äº›æœºåˆ¶ä¸åŸºäºè½¬æ¢å™¨çš„é¢„æµ‹å™¨ç›¸ç»“åˆï¼Œæä¾›ç«¯åˆ°ç«¯çš„æ•ˆç‡å’Œæ ¸å¿ƒè¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚åœ¨ä¸‰ç§ç³»ç»Ÿåœºæ™¯çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒMetaEformeråœ¨å‡†ç¡®æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸å¯¹äºåäº”ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿æœ‰37%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ—¶é—´åºåˆ—é¢„æµ‹åœ¨ç°ä»£ç³»ç»Ÿå¦‚äº‘è®¡ç®—ã€ç”µç½‘å’Œäº¤é€šç½‘ç»œä¸­çš„å…³é”®ä½œç”¨ï¼Œé’ˆå¯¹å½“å‰é¢ä¸´çš„å¤šé‡æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…ƒæ¨¡å¼çš„æ–°æ–¹æ¡ˆã€‚é€šè¿‡å¼€å‘ç‹¬ç‰¹çš„å…ƒæ¨¡å¼æ± åŒ–æœºåˆ¶å’Œå›å£°æœºåˆ¶ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿæ•æ‰ç³»ç»Ÿè´Ÿè½½çš„å¾®å¦™å˜åŒ–å¹¶çµæ´»é€‚åº”æ€§åœ°åˆ©ç”¨å…ƒæ¨¡å¼ã€‚MetaEformeræ— ç¼ç»“åˆäº†è¿™äº›æœºåˆ¶ä¸åŸºäºå˜å‹å™¨çš„é¢„æµ‹å™¨ï¼Œæä¾›ç«¯åˆ°ç«¯çš„æ•ˆç‡å’Œæ ¸å¿ƒè¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œåœ¨ä¸‰ä¸ªç³»ç»Ÿåœºæ™¯çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç›¸å¯¹äºåäº”ç§æœ€æ–°æŠ€æœ¯åŸºå‡†æœ‰37%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—é¢„æµ‹åœ¨å¤šä¸ªç°ä»£ç³»ç»Ÿä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œå¦‚äº‘è®¡ç®—ã€ç”µç½‘å’Œäº¤é€šç½‘ç»œä¸­çš„æ™ºèƒ½æ“ä½œã€‚</li>
<li>å½“å‰çš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•é¢ä¸´å¤æ‚æ¨¡å¼ã€æ¦‚å¿µæ¼‚ç§»å’Œå°æ ·æœ¬é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå…ƒæ¨¡å¼çš„æ–°æ–¹æ¡ˆï¼Œé€šè¿‡å¼€å‘ç‹¬ç‰¹çš„å…ƒæ¨¡å¼æ± åŒ–æœºåˆ¶å’Œå›å£°æœºåˆ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>å…ƒæ¨¡å¼æ± åŒ–æœºåˆ¶ç”¨äºå‡€åŒ–å’Œç»´æŠ¤å…ƒæ¨¡å¼ï¼Œæ•æ‰ç³»ç»Ÿè´Ÿè½½çš„å¾®å¦™å˜åŒ–ã€‚</li>
<li>Echoæœºåˆ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨å…ƒæ¨¡å¼ï¼Œå®ç°çµæ´»å’Œç²¾ç¡®çš„æ¨¡å¼é‡å»ºã€‚</li>
<li>MetaEformerç»“åˆäº†è¿™äº›æœºåˆ¶ä¸åŸºäºå˜å‹å™¨çš„é¢„æµ‹å™¨ï¼Œæä¾›ç«¯åˆ°ç«¯çš„æ•ˆç‡å’Œæ ¸å¿ƒè¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a527f393ab2cadba7edb4d96ec8f7ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a135a81c9ccc676ebfd3c52b32065a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87ce89d27f7e1ebc56d6c47ba29da9f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ad8832c289d4c8a86c1d7fb57a328e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ebb167291cb8b7356b5f9d84024090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93d61f0d39c5e40c26fc5016156dddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b9b02cbaa74bac5da0c441e1917d35.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="COGNATE-Acceleration-of-Sparse-Tensor-Programs-on-Emerging-Hardware-using-Transfer-Learning"><a href="#COGNATE-Acceleration-of-Sparse-Tensor-Programs-on-Emerging-Hardware-using-Transfer-Learning" class="headerlink" title="COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware   using Transfer Learning"></a>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware   using Transfer Learning</h2><p><strong>Authors:Chamika Sudusinghe, Gerasimos Gerogiannis, Damitha Lenadora, Charles Block, Josep Torrellas, Charith Mendis</strong></p>
<p>Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM. </p>
<blockquote>
<p>ç¨€ç–å¼ é‡ç¨‹åºåœ¨æ·±åº¦å­¦ä¹ å’Œå›¾å½¢åˆ†æä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨äº†ä¼˜åŒ–å¤„ç†çš„éœ€æ±‚ã€‚ä¸ºæ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œæ­£åœ¨å¼€å‘ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚ç”±äºç¨‹åºæ€§èƒ½é«˜åº¦æ•æ„Ÿäºç¨€ç–è¾“å…¥çš„å˜åŠ¨ï¼Œä¸”æ—©æœŸé˜¶æ®µçš„åŠ é€Ÿå™¨ä¾èµ–äºæ˜‚è´µçš„æ¨¡æ‹Ÿå™¨ï¼Œå› æ­¤ç”¨äºåœ¨é€šç”¨ç¡¬ä»¶ä¸Šä¼˜åŒ–æ­¤ç±»ç¨‹åºçš„åŸºäºæœºå™¨å­¦ä¹ çš„æˆæœ¬æ¨¡å‹å¯¹äºæ—©æœŸé˜¶æ®µçš„åŠ é€Ÿå™¨å¾€å¾€æ— æ•ˆï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤§é‡æ•°æ®é›†è¿›è¡Œé€‚å½“è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†COGNATEæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ¥è‡ªé€šç”¨ç¡¬ä»¶ï¼ˆå¦‚CPUï¼‰çš„å»‰ä»·æ•°æ®æ ·æœ¬è¿›è¡Œæˆæœ¬æ¨¡å‹è®­ç»ƒï¼Œç„¶ååœ¨æ–°å…´ç¡¬ä»¶ä¸Šè¿›è¡Œå°‘é‡æ ·æœ¬å¾®è°ƒã€‚COGNATEåˆ©ç”¨è·¨ç¡¬ä»¶å¹³å°çš„è¾“å…¥ç‰¹å¾çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆç¼“è§£ä¸ä¸€è‡´æ€§ï¼Œä½¿å¾—æˆæœ¬æ¨¡å‹è®­ç»ƒä»…éœ€ä½¿ç”¨ä¸é’ˆå¯¹åŠ é€Ÿå™¨çš„æ¨¡å‹æ‰€éœ€æ•°æ®æ ·æœ¬é‡çš„5%ï¼Œå³å¯å®ç°ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜COGNATEä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼ˆSpMMï¼‰çš„å¹³å‡åŠ é€Ÿæ¯”è¾¾åˆ°1.47å€ï¼ˆæœ€é«˜è‡³5.46å€ï¼‰ï¼Œä»¥åŠç»“æ„åŒ–ç¨€ç–åŠ¨æ€çŸ©é˜µä¹˜æ³•ï¼ˆSDDMMï¼‰çš„å¹³å‡åŠ é€Ÿæ¯”è¾¾åˆ°1.39å€ï¼ˆæœ€é«˜è‡³4.22å€ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00424v2">PDF</a> Accepted at the 42nd International Conference on Machine Learning</p>
<p><strong>Summary</strong></p>
<p>COGNATEæ¡†æ¶åˆ©ç”¨é€šç”¨ç¡¬ä»¶ï¼ˆå¦‚CPUï¼‰çš„å»‰ä»·æ•°æ®æ ·æœ¬è®­ç»ƒæˆæœ¬æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°‘é‡æ–°å…´ç¡¬ä»¶è¿›è¡Œå¾®è°ƒï¼Œæœ‰æ•ˆè§£å†³äº†ç¨€ç–å¼ é‡ç¨‹åºåœ¨ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨ä¸Šçš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¡¬ä»¶å¹³å°é—´è¾“å…¥ç‰¹å¾çš„åŒè´¨æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆç¼“è§£å¼‚è´¨æ€§ï¼Œç”¨ä»…5%çš„æ•°æ®æ ·æœ¬å°±èƒ½è®­ç»ƒå‡ºä¸åŠ é€Ÿå™¨ç‰¹å®šæ¨¡å‹ç›¸å½“æ€§èƒ½çš„æˆæœ¬æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCOGNATEåœ¨SpMMå’ŒSDDMMä»»åŠ¡ä¸Šå¹³å‡åŠ é€Ÿ1.47å€ï¼ˆæœ€é«˜5.46å€ï¼‰å’Œ1.39å€ï¼ˆæœ€é«˜4.22å€ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–å¼ é‡ç¨‹åºåœ¨æ·±åº¦å­¦ä¹ å’Œå›¾åˆ†æä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œéœ€è¦ä¼˜åŒ–çš„å¤„ç†ã€‚</li>
<li>ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨çš„å‘å±•ä¸ºæ»¡è¶³è¿™ä¸€éœ€æ±‚æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¼˜åŒ–è¿™äº›ç¨‹åºå¯¹äºæ—©æœŸåŠ é€Ÿå™¨é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ€§èƒ½å¯¹ç¨€ç–è¾“å…¥çš„æ•æ„Ÿæ€§ä»¥åŠä¾èµ–æ˜‚è´µæ¨¡æ‹Ÿå™¨çš„æ—©æœŸé˜¶æ®µã€‚</li>
<li>ç°æœ‰åŸºäºMLçš„æˆæœ¬æ¨¡å‹åœ¨ä¼˜åŒ–é€šç”¨ç¡¬ä»¶ä¸Šçš„ç¨‹åºæ—¶å¾€å¾€å¯¹æ—©æœŸåŠ é€Ÿå™¨æ— æ•ˆï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤§é‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>COGNATEæ¡†æ¶åˆ©ç”¨é€šç”¨ç¡¬ä»¶çš„å»‰ä»·æ•°æ®æ ·æœ¬è®­ç»ƒæˆæœ¬æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°‘é‡æ–°å…´ç¡¬ä»¶è¿›è¡Œå¾®è°ƒã€‚</li>
<li>COGNATEæ¡†æ¶åˆ©ç”¨ç¡¬ä»¶å¹³å°é—´è¾“å…¥ç‰¹å¾çš„åŒè´¨æ€§ï¼Œæœ‰æ•ˆç¼“è§£æ•°æ®å¼‚è´¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2ca97cc5f3fa33743297304f33a1461c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5edf5ead70161b9ba358cd6628bf96fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da2b3e6dc2e49622af7237507a1291f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-622f63a3b708296131defb5646aa4563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-778541af240129f2123192b1c91d6e98.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 16.8 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/">https://github.com/roboflow/rf100-vl/</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a> </p>
<blockquote>
<p>é€šè¿‡åŸºäºäº’è”ç½‘è§„æ¨¡æ•°æ®è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§å¯¹è±¡ï¼ˆå¦‚æ±½è½¦ã€å¡è½¦å’Œè¡Œäººï¼‰ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½æ˜¾è‘—ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»ç„¶éš¾ä»¥æ¨å¹¿åˆ°å…¶é¢„è®­ç»ƒæœªæ¶‰åŠçš„ç±»åˆ«ã€ä»»åŠ¡ä»¥åŠæˆåƒæ¨¡å¼ã€‚æˆ‘ä»¬å¹¶ä¸ä¸»å¼ ä»…ä»…é€šè¿‡å¢åŠ è§†è§‰æ•°æ®å¯¹VLMè¿›è¡Œå†è®­ç»ƒï¼Œè€Œæ˜¯è®¤ä¸ºåº”è¯¥é€šè¿‡åŒ…å«å°‘é‡è§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œæ–‡æœ¬æè¿°çš„æ³¨é‡ŠæŒ‡ä»¤æ¥å¯¹é½VLMçš„æ–°æ¦‚å¿µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ç»„å¤§è§„æ¨¡çš„å¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸å¸¸è§çš„æ¦‚å¿µå¤šæ ·æ€§é›†åˆä¸”æ€»å…±æœ‰å¤šè¾¾ç™¾ä¸ªæ•°æ®é›†é›†åˆè¿›è¡Œæ¨¡å‹è¯„ä¼°ä¸å¾®è°ƒã€‚æˆ‘ä»¬å¯¹æ­¤åŸºå‡†æµ‹è¯•ä¸­æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€åŠç›‘ç£å’Œå…¨ç›‘ç£è®¾ç½®ä¸‹çš„è¯„ä¼°ï¼Œå…è®¸è·¨æ•°æ®åˆ¶åº¦è¿›è¡Œæ¯”è¾ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åƒGroundingDINOå’ŒQwen2.5-VLè¿™æ ·çš„VLMåœ¨Roboflow100-VLä¸­çš„æŒ‘æˆ˜æ€§åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡ä½äºç™¾åˆ†ä¹‹äºŒï¼Œè¿™æ˜¾ç¤ºå‡ºè¿›è¡Œå°‘æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ€è¿‘çš„CVPR 2025åŸºç¡€FSODç«èµ›å¹¶ä»ç¤¾åŒºåˆ†äº«äº†ä¸€äº›è§è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè·èƒœé˜Ÿä¼è¶…è¿‡äº†æˆ‘ä»¬çš„åŸºçº¿æ°´å¹³é«˜è¾¾16.8 mAPï¼æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/">https://github.com/roboflow/rf100-vl/</a> å’Œ <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v2">PDF</a> The first two authors contributed equally. Project Page:   <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äº’è”ç½‘è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ³›åŒ–åˆ°è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ–°ç±»åˆ«å’Œä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§é€šè¿‡å°‘é‡è§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œçš„æ–‡æœ¬æè¿°æ¥å¯¹VLMè¿›è¡Œæ¦‚å¿µå¯¹é½çš„æ–¹æ³•ã€‚åŒæ—¶ä»‹ç»äº†Roboflow100-VLå¤§è§„æ¨¡å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç½•è§æ¦‚å¿µçš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ•°æ®ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡è¾ƒä½ï¼Œçªæ˜¾äº†æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æœ€åï¼Œæ–‡ç« åˆ†äº«äº†CVPR 2025åŸºç¡€FSODç«èµ›çš„è§è§£å’Œæˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æ³›åŒ–åˆ°æ–°ç±»åˆ«å’Œä»»åŠ¡æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>Roboflow100-VLæ•°æ®é›†æ˜¯ä¸€ä¸ªåŒ…å«ç½•è§æ¦‚å¿µçš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡è¾ƒä½ã€‚</li>
<li>é€šè¿‡å°‘é‡è§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œçš„æ–‡æœ¬æè¿°å¯¹VLMè¿›è¡Œæ¦‚å¿µå¯¹é½æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
<li>CVPR 2025åŸºç¡€FSODç«èµ›å±•ç¤ºäº†æ˜¾è‘—çš„æˆæœå’Œç¤¾åŒºè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5aca01ad3a170a81e3314cec4f10ee82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8394e86845eed7bebbdef61422acc1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94525ec6f3d591f43894fc2c1a976188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8c9b05e81329b7cd6eaeb0ed2ecc447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8595510fa396ee11352235c0ab0ccab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa8f284bbe93c59ebf971defb34f815.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CAPO-Cost-Aware-Prompt-Optimization"><a href="#CAPO-Cost-Aware-Prompt-Optimization" class="headerlink" title="CAPO: Cost-Aware Prompt Optimization"></a>CAPO: Cost-Aware Prompt Optimization</h2><p><strong>Authors:Tom Zehle, Moritz Schlager, Timo HeiÃŸ, Matthias Feurer</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automatic prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11&#x2F;15 cases with improvements up to 21%p in accuracy. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç®€å•çš„æç¤ºè§£å†³äº†å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä»è€Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å¯¹æç¤ºçš„æ„æ€éå¸¸æ•æ„Ÿã€‚è™½ç„¶è‡ªåŠ¨æç¤ºä¼˜åŒ–å¯ä»¥é€šè¿‡æ‰¾åˆ°æœ€ä½³æç¤ºæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†å½“å‰çš„æ–¹æ³•éœ€è¦å¤§é‡çš„LLMè°ƒç”¨å’Œè¾“å…¥ä»¤ç‰Œï¼Œè¿™ä½¿å¾—æç¤ºä¼˜åŒ–æˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬å¼•å…¥äº†CAPOï¼ˆåŸºäºæˆæœ¬çš„æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡çš„ç®—æ³•ã€‚CAPOæ˜¯ä¸€ç§è¿›åŒ–æ–¹æ³•ï¼Œä»¥LLMä½œä¸ºæ“ä½œå‘˜ï¼Œç»“åˆäº†æ¯”èµ›ä»¥èŠ‚çœè¯„ä¼°å’ŒåŸºäºæ€§èƒ½çš„å¤šç›®æ ‡ä¼˜åŒ–æ¥å¹³è¡¡æç¤ºé•¿åº¦ã€‚å®ƒåŒæ—¶ä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡æè¿°æ¥æé«˜ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„æ•°æ®é›†å’ŒLLMä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨è§£å†³å¤§å°ºåº¦æœºå™¨æ¨ç†é—®é¢˜æ—¶ï¼ŒCAPOåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹çš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾ç™¾åˆ†ä¹‹äºŒåä¸€ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å°±èƒ½å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œé€šè¿‡æ¯”èµ›èŠ‚çœè¯„ä¼°æ—¶é—´ï¼Œå¹¶é€šè¿‡é•¿åº¦æƒ©ç½šå‡å°‘å¹³å‡æç¤ºé•¿åº¦ï¼Œæ—¢ç»æµåˆå®ç”¨ã€‚å³ä½¿æ²¡æœ‰å°‘é‡çš„ç¤ºä¾‹ï¼ŒCAPOä¹Ÿèƒ½è¶…è¶Šç«äº‰å¯¹æ‰‹å¹¶ä¿æŒå¯¹åˆå§‹æç¤ºçš„ç¨³å¥æ€§ã€‚CAPOæœç€æé«˜æç¤ºä¼˜åŒ–çš„æˆæœ¬æ•ˆç›Šå’Œå¯è®¿é—®æ€§è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16005v4">PDF</a> Submitted to AutoML 2025</p>
<p><strong>Summary</strong></p>
<p>LLMsçš„æ€§èƒ½å¯¹æç¤ºè¯­æ•æ„Ÿåº¦æé«˜ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–é€šè¿‡å¯»æ‰¾æœ€ä½³æç¤ºæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡LLMè°ƒç”¨å’Œè¾“å…¥ä»¤ç‰Œï¼Œä½¿å¾—æç¤ºä¼˜åŒ–æˆæœ¬é«˜æ˜‚ã€‚å¼•å…¥CAPOï¼ˆæˆæœ¬æ„ŸçŸ¥æç¤ºä¼˜åŒ–ï¼‰ç®—æ³•ï¼Œé€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡ã€‚CAPOé‡‡ç”¨è¿›åŒ–æ–¹æ³•ï¼Œä»¥LLMsä½œä¸ºæ“ä½œå‘˜ï¼Œç»“åˆç«èµ›ä»¥èŠ‚çœè¯„ä¼°å’Œå¤šå…ƒç›®æ ‡ä¼˜åŒ–æ¥å¹³è¡¡æ€§èƒ½å’Œæç¤ºé•¿åº¦ã€‚å®ƒè”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡æè¿°æé«˜ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCAPOåœ¨å¤šæ•°æƒ…å†µä¸‹ä¼˜äºæœ€æ–°ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§ä¸Šæœ€é«˜æå‡äº†21%ã€‚CAPOåœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å³å¯å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œé€šè¿‡ç«èµ›èŠ‚çœè¯„ä¼°ï¼Œå¹¶é€šè¿‡é•¿åº¦æƒ©ç½šå‡å°‘å¹³å‡æç¤ºé•¿åº¦ï¼Œæ—¢ç»æµåˆæ³¨é‡æˆæœ¬æ•ˆç›Šã€‚å³ä½¿åœ¨æ²¡æœ‰å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼ŒCAPOä¹Ÿèƒ½è¶…è¶Šç«äº‰å¯¹æ‰‹å¹¶ç»´æŒç¨³å¥æ€§ã€‚è¿™ä¸ºä½¿æç¤ºä¼˜åŒ–æ›´åŠ å¼ºå¤§å’Œæ™®åŠåšå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯¹æç¤ºè¯­çš„æ•æ„Ÿåº¦æé«˜ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„å…³é”®ã€‚</li>
<li>å½“å‰è‡ªåŠ¨æç¤ºä¼˜åŒ–æ–¹æ³•å­˜åœ¨æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>CAPOç®—æ³•é€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡ã€‚</li>
<li>CAPOé‡‡ç”¨è¿›åŒ–æ–¹æ³•ï¼Œç»“åˆç«èµ›ä»¥èŠ‚çœè¯„ä¼°ï¼Œå¹¶å¹³è¡¡æ€§èƒ½å’Œæç¤ºé•¿åº¦ã€‚</li>
<li>CAPOè”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œåˆ©ç”¨ä»»åŠ¡æè¿°å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCAPOåœ¨å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæœ€é«˜å¯æå‡21%çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5307e394c6b2db5b216b74d598c9fbeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f83c7edcce289110fc859d98264a6596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a95ee4ba1628b0c6db9186fa0a7690.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Å vÃ¡benskÃ½, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educatorsâ€™ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ–‡æœ¬åæ€å’Œé¢„æµ‹å­¦ä¸šæˆç»©æ–¹é¢çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„åæ€è¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”å¯èƒ½æ— æ³•åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆæ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨LLMï¼Œä½¿ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•ä»£ç†å’Œå¤šä»£ç†ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ï¼‰ï¼Œå°†å­¦ç”Ÿåæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨åŒ…å«æ¥è‡ª377åå­¦ç”Ÿä¸‰ä¸ªå­¦æœ¯å­¦æœŸçš„5,278ç¯‡åæ€çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å°‘æ ·æœ¬ç­–ç•¥çš„å•ä»£ç†æ–¹å¼ä¸äººç±»è¯„ä¼°çš„åŒ¹é…ç‡æœ€é«˜ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨å¤„äºå±é™©ä¸­çš„å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°éƒ½ä¼˜äºåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°è‡ªåŠ¨è¿›è¡Œåæ€è¯„ä¼°ï¼Œå‡å°‘æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å°†å…ˆè¿›çš„ç”Ÿæˆæ€§AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µä¸­çš„æ½œåŠ›ï¼Œä»¥æé«˜å­¦ç”Ÿçš„å‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v3">PDF</a> Published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025), see   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-981-96-8186-0_24">https://doi.org/10.1007/978-981-96-8186-0_24</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ€§åæ€å’Œé¢„æµ‹å­¦ä¸šè¡¨ç°æ–¹é¢çš„åº”ç”¨ã€‚ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆæ‰©å±•ã€‚æœ¬ç ”ç©¶é‡‡ç”¨LLMsï¼Œé€šè¿‡ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•ä»£ç†å’Œå¤šä»£ç†ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶å°„å‡»å’Œå°‘å°„å‡»ï¼‰ï¼Œå°†å­¦ç”Ÿåæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨å°‘å°„å‡»ç­–ç•¥çš„å•ä»£ç†åŒ¹é…ç‡æœ€é«˜ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºåŸºçº¿ã€‚è¿™è¡¨æ˜LLMså¯ä»¥æœ‰æ•ˆè‡ªåŠ¨è¯„ä¼°åæ€ï¼Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œä¸ºå¯èƒ½éœ€è¦é¢å¤–æ”¯æŒçš„å­¦ç”Ÿæä¾›åŠæ—¶æ”¯æŒã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†é«˜çº§ç”Ÿæˆæ€§AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µä»¥æé«˜å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºè‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿçš„å¼€æ”¾æ€§åæ€ã€‚</li>
<li>ä¼ ç»Ÿçš„å­¦ç”Ÿåæ€è¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥æ‰©å±•ã€‚</li>
<li>é€šè¿‡ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•ä»£ç†å’Œå¤šä»£ç†ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶å°„å‡»å’Œå°‘å°„å‡»ï¼‰ï¼ŒLLMsèƒ½å¤Ÿæœ‰æ•ˆè½¬åŒ–å­¦ç”Ÿåæ€ä¸ºé‡åŒ–åˆ†æ•°ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œé‡‡ç”¨å°‘å°„å‡»ç­–ç•¥çš„å•ä»£ç†åŒ¹é…ç‡æœ€é«˜ã€‚</li>
<li>LLMsåœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>LLMsçš„è‡ªåŠ¨è¯„ä¼°å¯ä»¥å‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œä¸ºéœ€è¦é¢å¤–æ”¯æŒçš„å­¦ç”Ÿæä¾›åŠæ—¶æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-727cd303d72d5350a7210743478c92d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images"><a href="#FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images" class="headerlink" title="FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images"></a>FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images</h2><p><strong>Authors:Zhuohao Li, Zhicheng Huang, Wenchao Liu, Zhuxin Zhang, Jianming Miao</strong></p>
<p>Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes in images using only a limited number of annotated examples, has recently progressed in data-scarce domains. However, in this work, we show that the existing FSS methods often struggle to generalize to underwater environments. Specifically, the prior features extracted by pre-trained models used as feature extractors are fragile due to the unique challenges of underwater images. To address this, we propose FSSUWNet, a tailored FSS framework for underwater images with feature enhancement. FSSUWNet exploits the integration of complementary features, emphasizing both low-level and high-level image characteristics. In addition to employing a pre-trained model as the primary encoder, we propose an auxiliary encoder called Feature Enhanced Encoder which extracts complementary features to better adapt to underwater scene characteristics. Furthermore, a simple and effective Feature Alignment Module aims to provide global prior knowledge and align low-level features with high-level features in dimensions. Given the scarcity of underwater images, we introduce a cross-validation dataset version based on the Segmentation of Underwater Imagery dataset. Extensive experiments on public underwater segmentation datasets demonstrate that our approach achieves state-of-the-art performance. For example, our method outperforms the previous best method by 2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot and 5-shot scenarios in the datasets, respectively. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet">https://github.com/lizhh268/FSSUWNet</a>. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ ·æœ¬å¯¹å›¾åƒä¸­çš„æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œæœ€è¿‘åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„FSSæ–¹æ³•å¾€å¾€éš¾ä»¥æ¨å¹¿åˆ°æ°´ä¸‹ç¯å¢ƒã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºæ°´ä¸‹å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç”±é¢„è®­ç»ƒæ¨¡å‹æå–çš„å…ˆéªŒç‰¹å¾æ˜¯è„†å¼±çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FSSUWNetï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å›¾åƒçš„FSSæ¡†æ¶ï¼Œå…·æœ‰ç‰¹å¾å¢å¼ºåŠŸèƒ½ã€‚FSSUWNetåˆ©ç”¨äº’è¡¥ç‰¹å¾çš„é›†æˆï¼Œå¼ºè°ƒå›¾åƒçš„ä½çº§å’Œé«˜çº§ç‰¹å¾ã€‚é™¤äº†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»ç¼–ç å™¨å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¾…åŠ©ç¼–ç å™¨ï¼Œç§°ä¸ºç‰¹å¾å¢å¼ºç¼–ç å™¨ï¼Œç”¨äºæå–äº’è¡¥ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç®€å•æœ‰æ•ˆçš„ç‰¹å¾å¯¹é½æ¨¡å—æ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨ç»´åº¦ä¸Šå¯¹é½ä½çº§ç‰¹å¾å’Œé«˜çº§ç‰¹å¾ã€‚é‰´äºæ°´ä¸‹å›¾åƒçš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬åŸºäºæ°´ä¸‹å›¾åƒåˆ†å‰²æ•°æ®é›†å¼•å…¥äº†ä¸€ä¸ªäº¤å‰éªŒè¯æ•°æ®é›†ç‰ˆæœ¬ã€‚åœ¨å…¬å…±æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®é›†çš„ä¸€æ¬¡å°„å‡»å’Œäº”æ¬¡å°„å‡»åœºæ™¯ä¸­ï¼Œå¹³å‡äº¤å¹¶æ¯”æŒ‡æ ‡åˆ†åˆ«æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º2.8%å’Œ2.6%ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lizhh268/FSSUWNetä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00478v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ°´ä¸‹å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„Few-Shot Semantic Segmentationï¼ˆFSSï¼‰çš„æŒ‘æˆ˜ã€‚ç°æœ‰FSSæ–¹æ³•åœ¨æ°´ä¸‹ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå› æ­¤æå‡ºä¸€ç§é’ˆå¯¹æ°´ä¸‹å›¾åƒçš„FSSæ¡†æ¶â€”â€”FSSUWNetï¼Œé€šè¿‡ç‰¹å¾å¢å¼ºæ¥åº”å¯¹æ°´ä¸‹ç¯å¢ƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚FSSUWNetç»“åˆäº†ä¸»è¦ç¼–ç å™¨å’Œè¾…åŠ©ç¼–ç å™¨æå–çš„äº’è¡¥ç‰¹å¾ï¼Œå¹¶å¼•å…¥ç‰¹å¾å¯¹é½æ¨¡å—ä»¥æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ã€‚åœ¨å…¬å…±æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSSåœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å—é™ï¼Œéœ€è¦ä¸“é—¨çš„æ–¹æ³•æ¥å¤„ç†æ°´ä¸‹å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>FSSUWNetæ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å›¾åƒçš„FSSæ¡†æ¶ï¼Œé€šè¿‡ç‰¹å¾å¢å¼ºæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>FSSUWNetç»“åˆäº†ä¸»è¦ç¼–ç å™¨å’Œè¾…åŠ©ç¼–ç å™¨ï¼Œä»¥æå–äº’è¡¥ç‰¹å¾ï¼Œæ›´å¥½åœ°é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹æ€§ã€‚</li>
<li>ç‰¹å¾å¯¹é½æ¨¡å—æ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†ä½çº§åˆ«ç‰¹å¾ä¸é«˜çº§åˆ«ç‰¹å¾å¯¹é½ã€‚</li>
<li>ç¼ºä¹æ°´ä¸‹å›¾åƒæ•°æ®ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºSegmentation of Underwater Imageryæ•°æ®é›†çš„äº¤å‰éªŒè¯æ•°æ®é›†ç‰ˆæœ¬ã€‚</li>
<li>åœ¨å…¬å…±æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFSSUWNetå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f27f1af420dbcac33e58aa0c16fffc6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a354beb29cd8c80a20116f4d0e2fb8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-508ecd23d8e902eb482806ef4ad176be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d365645dd7d4d6c540b2b0e4c0d0d9b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9df96537413c539b08ec870c7a312e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217ca76dc735c5e0fe0a90313ddc4416.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search"><a href="#Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search" class="headerlink" title="Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search"></a>Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search</h2><p><strong>Authors:Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, Yuyu Luo</strong></p>
<p>Text-to-SQL, which enables natural language interaction with databases, serves as a pivotal method across diverse industries. With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, zero-shot Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction. To address this challenge, we propose Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial reasoning states. To enhance the frameworkâ€™s reasoning capabilities, we introduce LLM-as-Action-Model to dynamically generate SQL construction actions during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation. Experimental results show that Alpha-SQL achieves 69.7% execution accuracy on the BIRD development set, using a 32B open-source LLM without fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by 2.5% on the BIRD development set. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLçš„æŠ€æœ¯èƒ½å¤Ÿå®ç°ä¸æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€äº¤äº’ï¼Œæˆä¸ºå„è¡Œä¸šå…³é”®çš„æ–¹æ³•ã€‚éšç€æ¯éš”å‡ ä¸ªæœˆå°±ä¼šå‡ºç°æ–°çš„ã€æ›´å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¾®è°ƒå·²ç»å˜å¾—éå¸¸æ˜‚è´µã€åŠ³åŠ¨å¯†é›†å‹å’Œæ˜“å‡ºé”™ã€‚ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°SQLï¼Œåˆ©ç”¨LLMä¸­ç¼–ç çš„æ—¥ç›Šå¢é•¿çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œå±•ç°å‡ºä¸€ä¸ªæœ‰å‰æ™¯å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„æ–¹å‘ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Alpha-SQLè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶æ¥åŸºäºéƒ¨åˆ†æ¨ç†çŠ¶æ€è¿­ä»£åœ°æ¨æ–­SQLæ„å»ºæ“ä½œã€‚ä¸ºäº†æé«˜æ¡†æ¶çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLM-ä½œä¸ºè¡ŒåŠ¨æ¨¡å‹ï¼Œåœ¨MCTSè¿‡ç¨‹ä¸­åŠ¨æ€ç”ŸæˆSQLæ„å»ºæ“ä½œï¼Œå¼•å¯¼æœç´¢æœç€æ›´æœ‰å‰é€”çš„SQLæŸ¥è¯¢è¿›è¡Œã€‚æ­¤å¤–ï¼ŒAlpha-SQLé‡‡ç”¨è‡ªæˆ‘ç›‘ç£çš„å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°å€™é€‰SQLæŸ¥è¯¢çš„è´¨é‡ï¼Œç¡®ä¿æ›´å‡†ç¡®ã€é«˜æ•ˆçš„æŸ¥è¯¢ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlpha-SQLåœ¨BIRDå¼€å‘é›†ä¸Šå®ç°äº†69.7%çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œä½¿ç”¨çš„æ˜¯æœªè¿›è¡Œå¾®è°ƒçš„å¼€æºLLM 32Bã€‚åœ¨BIRDå¼€å‘é›†ä¸Šï¼ŒAlpha-SQLæ¯”åŸºäºGPT-4oçš„æœ€ä½³å…ˆå‰é›¶æ ·æœ¬æ–¹æ³•é«˜å‡º2.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17248v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†Text-to-SQLçš„é‡è¦æ€§ä»¥åŠéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAlpha-SQLçš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶å’ŒLLM-as-Action-Modelæ¥ç”ŸæˆSQLæŸ¥è¯¢ï¼Œå¹¶åœ¨æ— å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†è¾ƒé«˜çš„æ‰§è¡Œå‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQLå·²æˆä¸ºä¸æ•°æ®åº“è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’çš„å…³é”®æ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºå„è¡Œå„ä¸šã€‚</li>
<li>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å˜å¾—æˆæœ¬é«˜æ˜‚ã€åŠ³åŠ¨å¯†é›†ä¸”æ˜“å‡ºé”™ã€‚</li>
<li>Alpha-SQLæ˜¯ä¸€ç§æ–°çš„è§£å†³æ–¹æ³•ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶æ¥æ¨æ–­SQLæ„å»ºåŠ¨ä½œã€‚</li>
<li>LLM-as-Action-Modelåœ¨MCTSè¿‡ç¨‹ä¸­åŠ¨æ€ç”ŸæˆSQLæ„å»ºåŠ¨ä½œï¼Œå¼•å¯¼æœç´¢å‘æ›´æœ‰å‰é€”çš„SQLæŸ¥è¯¢æ–¹å‘è¿›è¡Œã€‚</li>
<li>Alpha-SQLä½¿ç”¨è‡ªç›‘ç£å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°å€™é€‰SQLæŸ¥è¯¢çš„è´¨é‡ï¼Œç¡®ä¿æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„æŸ¥è¯¢ç”Ÿæˆã€‚</li>
<li>åœ¨æ— å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒAlpha-SQLåœ¨BIRDå¼€å‘é›†ä¸Šå®ç°äº†69.7%çš„æ‰§è¡Œå‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46726183d8f3eba5dd9d774b7f15c207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cd3cb7a9f2abc871db5cbd24a0efac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ad956c43608f2b1c12c801c0e321e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4a126c6df204bc66f989a67d574cfd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0719f505961381ade5c61d79bfa6b220.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs"><a href="#Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs" class="headerlink" title="Stepwise Reasoning Error Disruption Attack of LLMs"></a>Stepwise Reasoning Error Disruption Attack of LLMs</h2><p><strong>Authors:Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu</strong></p>
<p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEEDâ€™s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack">https://github.com/Applied-Machine-Learning-Lab/SEED-Attack</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§å’Œç¨³å¥æ€§ä»ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚ç°æœ‰çš„é’ˆå¯¹LLMæ¨ç†çš„æ”»å‡»å—é™äºç‰¹å®šåœºæ™¯æˆ–ç¼ºä¹éšè”½æ€§ï¼Œé™åˆ¶äº†å…¶å¯è¡Œæ€§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€æ­¥æ¨ç†è¯¯å·®å¹²æ‰°ï¼ˆSEEDï¼‰æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¾®å¦™åœ°å°†é”™è¯¯æ³¨å…¥åˆ°å…ˆå‰çš„æ¨ç†æ­¥éª¤ä¸­ï¼Œè¯¯å¯¼æ¨¡å‹äº§ç”Ÿé”™è¯¯çš„åç»­æ¨ç†å’Œæœ€ç»ˆç­”æ¡ˆã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼ŒSEEDä¸é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯å…¼å®¹ï¼Œä¿æŒè‡ªç„¶æ¨ç†æµç¨‹ï¼Œå¹¶ç¡®ä¿åœ¨ä¸ä¿®æ”¹æŒ‡ä»¤çš„æƒ…å†µä¸‹ç§˜å¯†æ‰§è¡Œã€‚åœ¨å››ä¸ªä¸åŒæ¨¡å‹ã€å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†SEEDçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†LLMå¯¹æ¨ç†è¿‡ç¨‹ä¸­æ–­çš„è„†å¼±æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å®è·µåº”ç”¨ä¸­ç¡®ä¿LLMæ¨ç†ç¨³å¥æ€§çš„é‡è¦æ€§ï¼Œä»¥ä¿éšœå…¶å®‰å…¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/SEED-Attackä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11934v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å®‰å…¨æ€§å’Œæ¨ç†è¿‡ç¨‹çš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚é’ˆå¯¹ç°æœ‰LLMæ¨ç†æ”»å‡»ç‰¹å®šè®¾ç½®æˆ–ç¼ºä¹éšè”½æ€§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é€æ­¥æ¨ç†è¯¯å·®å¹²æ‰°ï¼ˆSEEDï¼‰æ”»å‡»æ–¹æ³•ã€‚SEEDé€šè¿‡å¾®å¦™åœ°æ³¨å…¥é”™è¯¯æ¥è¯¯å¯¼æ¨¡å‹äº§ç”Ÿé”™è¯¯çš„åç»­æ¨ç†å’Œæœ€ç»ˆç­”æ¡ˆã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒSEEDé€‚ç”¨äºé›¶æ ·æœ¬å’Œå°æ ·æœ¬è®¾ç½®ï¼Œä¿æŒè‡ªç„¶æ¨ç†æµç¨‹ï¼Œç¡®ä¿åœ¨ä¸ä¿®æ”¹æŒ‡ä»¤çš„æƒ…å†µä¸‹ç§˜å¯†æ‰§è¡Œã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œå››ä¸ªä¸åŒæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†SEEDçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†LLMå¯¹æ¨ç†è¿‡ç¨‹ä¸­æ–­çš„è„†å¼±æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­å…³æ³¨LLMæ¨ç†ç¨³å¥æ€§çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®‰å…¨æ€§å’Œç¨³å¥æ€§æœ‰å¾…æå‡ã€‚</li>
<li>ç°æœ‰å¯¹LLMçš„æ¨ç†æ”»å‡»æ–¹æ³•å—é™äºç‰¹å®šåœºæ™¯æˆ–ç¼ºä¹éšè”½æ€§ï¼Œå¯¼è‡´å®é™…åº”ç”¨ä¸­å¯è¡Œæ€§å·®å’Œé€šç”¨æ€§ä¸è¶³ã€‚</li>
<li>æå‡ºäº†Stepwise rEasoning Error Disruptionï¼ˆSEEDï¼‰æ”»å‡»æ–¹æ³•ï¼Œèƒ½å¤Ÿå¾®å¦™åœ°æ³¨å…¥é”™è¯¯ä»¥è¯¯å¯¼æ¨¡å‹äº§ç”Ÿé”™è¯¯çš„æ¨ç†å’Œç­”æ¡ˆã€‚</li>
<li>SEEDæ”»å‡»æ–¹æ³•é€‚ç”¨äºé›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯ï¼Œä¿æŒè‡ªç„¶æ¨ç†æµç¨‹ï¼ŒåŒæ—¶ç¡®ä¿æ‰§è¡Œè¿‡ç¨‹éšè”½ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜SEEDæ”»å‡»çš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†LLMå¯¹æ¨ç†è¿‡ç¨‹ä¸­æ–­çš„è„†å¼±æ€§ã€‚</li>
<li>å®éªŒç»“æœå¼ºè°ƒåœ¨å®é™…åº”ç”¨ä¸­å…³æ³¨LLMæ¨ç†ç¨³å¥æ€§çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c8c19e5fd4a29ba146dae4f32d556dfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00ca2d3282bea65c458df2505f4b40e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87b1c6209c61a9ecdf478d89801d4273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c845210b4c456a596c9b56ccbedf20ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d37b365832bd7f8ebcf6587df784a41c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>å°‘æ•°è¯­ä¹‰åˆ†å‰²ï¼ˆFew-shot Semantic Segmentationï¼‰æŠ€æœ¯æ—¨åœ¨è§£å†³ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒä¸­çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè®¸å¤šä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•è¦ä¹ˆä¸å¾—ä¸æ”¾å¼ƒå¤æ‚çš„å±€éƒ¨è¯­ä¹‰ç‰¹å¾ï¼Œè¦ä¹ˆé¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„å°‘æ•°è¯­ä¹‰åˆ†å‰²æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©ç ç”Ÿæˆæ¨¡å—ï¼Œä»¥æé«˜æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥åˆ†å±‚çš„æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©ç ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨å±€ç‰¹å¾ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚åº¦ã€‚æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„è¿™ç§å¹³è¡¡ä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸Šèƒ½åœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…æœ‰150ä¸‡ä¸ªå‚æ•°ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„Few-shotè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨ã€ä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—å’Œå¤šå°ºåº¦è§£ç å™¨ï¼Œæé«˜äº†å¯¹æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´å…³ç³»çš„ç†è§£ï¼Œå®ç°äº†å¯¹å°‘é‡æ ‡æ³¨æ ·æœ¬ä¸‹ç›®æ ‡å›¾åƒçš„ç²¾ç»†åˆ†å‰²ã€‚è¯¥æ¨¡å‹åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œä¸”æ¨¡å‹å‚æ•°ä»…ä¸º1.5ç™¾ä¸‡ï¼Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºTransformeræ¶æ„çš„Few-shotè¯­ä¹‰åˆ†å‰²æ–°æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…å«ç©ºé—´å˜æ¢è§£ç å™¨ã€ä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—å’Œå¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥æ”¹å–„å…³ç³»ç†è§£ã€‚</li>
<li>é€šè¿‡æ•´åˆä¸­é—´ç¼–ç é˜¶æ®µçš„å…¨çƒç‰¹å¾ï¼Œæé«˜äº†ä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>æ¨¡å‹å®ç°äº†åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç«äº‰æ€§èƒ½ï¼Œå¦‚PASCAL-5^iå’ŒCOCO-20^iã€‚</li>
<li>æ¨¡å‹åœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å‚æ•°æ•°é‡ä»…ä¸º1.5ç™¾ä¸‡ï¼Œå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcdeef2671221c0cba44addb40e56bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6e719d0a34e103475f80a5b03bc20f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1232d75de621baac9ac76c250665a86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43b280a5d0cd66845ec22d0186712420.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SMILE-Speech-Meta-In-Context-Learning-for-Low-Resource-Language-Automatic-Speech-Recognition"><a href="#SMILE-Speech-Meta-In-Context-Learning-for-Low-Resource-Language-Automatic-Speech-Recognition" class="headerlink" title="SMILE: Speech Meta In-Context Learning for Low-Resource Language   Automatic Speech Recognition"></a>SMILE: Speech Meta In-Context Learning for Low-Resource Language   Automatic Speech Recognition</h2><p><strong>Authors:Ming-Hao Hsu, Hung-yi Lee</strong></p>
<p>Automatic Speech Recognition (ASR) models demonstrate outstanding performance on high-resource languages but face significant challenges when applied to low-resource languages due to limited training data and insufficient cross-lingual generalization. Existing adaptation strategies, such as shallow fusion, data augmentation, and direct fine-tuning, either rely on external resources, suffer computational inefficiencies, or fail in test-time adaptation scenarios. To address these limitations, we introduce Speech Meta In-Context LEarning (SMILE), an innovative framework that combines meta-learning with speech in-context learning (SICL). SMILE leverages meta-training from high-resource languages to enable robust, few-shot generalization to low-resource languages without explicit fine-tuning on the target domain. Extensive experiments on the ML-SUPERB benchmark show that SMILE consistently outperforms baseline methods, significantly reducing character and word error rates in training-free few-shot multilingual ASR tasks. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å½“åº”ç”¨äºä½èµ„æºè¯­è¨€æ—¶ï¼Œç”±äºè®­ç»ƒæ•°æ®æœ‰é™å’Œè·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„é€‚åº”ç­–ç•¥ï¼Œå¦‚æµ…èåˆã€æ•°æ®å¢å¼ºå’Œç›´æ¥å¾®è°ƒï¼Œè¦ä¹ˆä¾èµ–å¤–éƒ¨èµ„æºï¼Œè¦ä¹ˆé¢ä¸´è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæˆ–åœ¨æµ‹è¯•æ—¶é€‚åº”åœºæ™¯å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­éŸ³å…ƒä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆSMILEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå…ƒå­¦ä¹ ä¸è¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆSICLï¼‰çš„åˆ›æ–°æ¡†æ¶ã€‚SMILEåˆ©ç”¨é«˜èµ„æºè¯­è¨€çš„å…ƒè®­ç»ƒï¼Œå®ç°å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€åœ¨ç›®æ ‡åŸŸä¸Šè¿›è¡Œæ˜¾å¼å¾®è°ƒå³å¯é€‚åº”ä½èµ„æºè¯­è¨€ã€‚åœ¨ML-SUPERBåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSMILEå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„å¤šè¯­ç§å°‘æ ·æœ¬ASRä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†å­—ç¬¦å’Œå•è¯é”™è¯¯ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10429v2">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°çªå‡ºï¼Œä½†åœ¨åº”ç”¨äºä½èµ„æºè¯­è¨€æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒæ•°æ®æœ‰é™å’Œè·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ä¸è¶³ç­‰ã€‚ç°æœ‰çš„é€‚åº”ç­–ç•¥å­˜åœ¨ä¾èµ–å¤–éƒ¨èµ„æºã€è®¡ç®—æ•ˆç‡ä½ä¸‹æˆ–åœ¨æµ‹è¯•æ—¶é€‚åº”åœºæ™¯å¤±è´¥ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€é¡¹åˆ›æ–°çš„æ¡†æ¶â€”â€”Speech Meta In-Context LEarningï¼ˆSMILEï¼‰ï¼Œå®ƒç»“åˆäº†å…ƒå­¦ä¹ ä¸è¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆSICLï¼‰ã€‚SMILEåˆ©ç”¨å…ƒè®­ç»ƒä»é«˜èµ„æºè¯­è¨€ä¸­è¿›è¡Œå­¦ä¹ ï¼Œä½¿æ¨¡å‹å…·å¤‡å¯¹ä½èµ„æºè¯­è¨€çš„ç¨³å¥çš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€åœ¨ç›®æ ‡åŸŸè¿›è¡Œæ˜¾å¼å¾®è°ƒã€‚åœ¨ML-SUPERBåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSMILEåœ¨æ— éœ€è®­ç»ƒæˆ–å°‘é‡çš„è®­ç»ƒæƒ…å†µä¸‹ï¼Œåœ¨å¤šè¯­ç§ASRä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†å­—ç¬¦å’Œå•è¯é”™è¯¯ç‡ï¼Œè¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ä½èµ„æºè¯­è¨€çš„åº”ç”¨é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰é€‚åº”ç­–ç•¥å­˜åœ¨ä¾èµ–å¤–éƒ¨èµ„æºã€è®¡ç®—æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚</li>
<li>SMILEæ¡†æ¶ç»“åˆäº†å…ƒå­¦ä¹ ä¸è¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆSICLï¼‰ã€‚</li>
<li>SMILEåˆ©ç”¨å…ƒè®­ç»ƒä»é«˜èµ„æºè¯­è¨€ä¸­è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>SMILEä½¿æ¨¡å‹å…·å¤‡å¯¹ä½èµ„æºè¯­è¨€çš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€æ˜¾å¼å¾®è°ƒã€‚</li>
<li>åœ¨ML-SUPERBåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSMILEæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-76e28bbe08dc9acdf77d31c69d42e7e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87aae35ed2ca8d0e7eb5de4b9a989ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86fd44369aecfc74c661c1fc5c53f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42e37ef1a833a750965756be8a243aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b439ef07e405fe4b9475a8b4dc9a43e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0c16bcedaf8028e4f248959aa17eda9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abcb109bd4414192c6d3bc1870830df5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="WorldAPIs-The-World-Is-Worth-How-Many-APIs-A-Thought-Experiment"><a href="#WorldAPIs-The-World-Is-Worth-How-Many-APIs-A-Thought-Experiment" class="headerlink" title="WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment"></a>WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment</h2><p><strong>Authors:Jiefu Ou, Arda Uzunoglu, Benjamin Van Durme, Daniel Khashabi</strong></p>
<p>AI systems make decisions in physical environments through primitive actions or affordances that are accessed via API calls. While deploying AI agents in the real world involves numerous high-level actions, existing embodied simulators offer a limited set of domain-salient APIs. This naturally brings up the questions: how many primitive actions (APIs) are needed for a versatile embodied agent, and what should they look like? We explore this via a thought experiment: assuming that wikiHow tutorials cover a wide variety of human-written tasks, what is the space of APIs needed to cover these instructions? We propose a framework to iteratively induce new APIs by grounding wikiHow instruction to situated agent policies. Inspired by recent successes in large language models (LLMs) for embodied planning, we propose a few-shot prompting to steer GPT-4 to generate Pythonic programs as agent policies and bootstrap a universe of APIs by 1) reusing a seed set of APIs; and then 2) fabricate new API calls when necessary. The focus of this thought experiment is on defining these APIs rather than their executability. We apply the proposed pipeline on instructions from wikiHow tutorials. On a small fraction (0.5%) of tutorials, we induce an action space of 300+ APIs necessary for capturing the rich variety of tasks in the physical world. A detailed automatic and human analysis of the induction output reveals that the proposed pipeline enables effective reuse and creation of APIs. Moreover, a manual review revealed that existing simulators support only a small subset of the induced APIs (9 of the top 50 frequent APIs), motivating the development of action-rich embodied environments. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç³»ç»Ÿé€šè¿‡é€šè¿‡APIè°ƒç”¨è®¿é—®çš„åŸå§‹åŠ¨ä½œæˆ–åŠŸèƒ½ï¼ˆaffordancesï¼‰åœ¨ç‰©ç†ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ã€‚åœ¨ç°å®ä¸–ç•Œä¸­éƒ¨ç½²AIä»£ç†æ¶‰åŠè®¸å¤šé«˜çº§åŠ¨ä½œï¼Œè€Œç°æœ‰çš„å®ä½“æ¨¡æ‹Ÿå™¨ä»…æä¾›æœ‰é™çš„é¢†åŸŸç‰¹å®šAPIã€‚è¿™è‡ªç„¶å¼•å‘äº†ä»¥ä¸‹é—®é¢˜ï¼šå¯¹äºå¤šæ‰å¤šè‰ºçš„å®ä½“ä»£ç†ï¼Œéœ€è¦å¤šå°‘åŸå§‹åŠ¨ä½œï¼ˆAPIï¼‰ï¼Œå®ƒä»¬åº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿæˆ‘ä»¬é€šè¿‡æ€æƒ³å®éªŒæ¥æ¢ç´¢è¿™ä¸ªé—®é¢˜ï¼šå‡è®¾wikiHowæ•™ç¨‹æ¶µç›–äº†å„ç§äººç±»ç¼–å†™çš„ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä»€ä¹ˆAPIç©ºé—´æ¥æ¶µç›–è¿™äº›æŒ‡ä»¤ï¼Ÿæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡å°†wikiHowæŒ‡ä»¤ä¸æƒ…å¢ƒä»£ç†ç­–ç•¥ç›¸ç»“åˆï¼Œæ¥è¿­ä»£åœ°å¼•å¯¼æ–°çš„APIã€‚æˆ‘ä»¬å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®ä½“è§„åˆ’æ–¹é¢è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæå‡ºé€šè¿‡å°‘é‡æç¤ºæ¥å¼•å¯¼GPT-4ç”ŸæˆPythonicç¨‹åºä½œä¸ºä»£ç†ç­–ç•¥ï¼Œå¹¶é€šè¿‡1ï¼‰é‡å¤ä½¿ç”¨ç§å­é›†APIï¼›ç„¶å2ï¼‰åœ¨å¿…è¦æ—¶åˆ¶é€ æ–°çš„APIè°ƒç”¨ï¼Œæ¥æ„å»ºAPIçš„å®‡å®™ã€‚è¿™ä¸ªæ€æƒ³å®éªŒçš„é‡ç‚¹åœ¨äºå®šä¹‰è¿™äº›APIï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„å¯æ‰§è¡Œæ€§ã€‚æˆ‘ä»¬å°†æ‰€æè®®çš„ç®¡é“åº”ç”¨äºwikiHowæ•™ç¨‹ä¸­çš„æŒ‡ä»¤ã€‚åœ¨å°‘é‡ï¼ˆ0.5%ï¼‰çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¯±å¯¼å‡º300å¤šä¸ªæ•è·ç‰©ç†ä¸–ç•Œä¸­ä¸°å¯Œå¤šæ ·ä»»åŠ¡çš„å¿…è¦APIã€‚å¯¹è¯±å¯¼è¾“å‡ºçš„è¯¦ç»†è‡ªåŠ¨åˆ†æå’Œäººå·¥åˆ†æè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®¡é“èƒ½å¤Ÿå®ç°APIçš„æœ‰æ•ˆé‡ç”¨å’Œåˆ›å»ºã€‚æ­¤å¤–ï¼Œæ‰‹åŠ¨å®¡æŸ¥è¡¨æ˜ï¼Œç°æœ‰æ¨¡æ‹Ÿå™¨ä»…æ”¯æŒä¸€å°éƒ¨åˆ†è¯±å¯¼çš„APIï¼ˆå‰50ä¸ªå¸¸ç”¨APIä¸­çš„9ä¸ªï¼‰ï¼Œè¿™æ¨åŠ¨äº†ä¸°å¯ŒåŠ¨ä½œçš„å®ä½“ç¯å¢ƒçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07778v2">PDF</a> AAAI 2025 &amp; ACL 2024 NLRSE, 7 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†AIç³»ç»Ÿåœ¨ç‰©ç†ç¯å¢ƒä¸­é€šè¿‡åŸå§‹åŠ¨ä½œï¼ˆé€šè¿‡APIè°ƒç”¨å®ç°ï¼‰è¿›è¡Œå†³ç­–çš„é—®é¢˜ã€‚ä½œè€…é€šè¿‡å‡è®¾wikiHowæ•™ç¨‹æ¶µç›–äº†è®¸å¤šäººç±»ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¡†æ¶çš„æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£è¯±å¯¼å‡ºæ–°çš„APIæ¥æ¨¡æ‹Ÿç‰©ç†ä¸–ç•Œçš„ä»»åŠ¡åŠ¨ä½œã€‚è¯¥ç ”ç©¶ä½¿ç”¨GPT-4ç”Ÿæˆä»£ç†ç­–ç•¥å¹¶åˆ©ç”¨å°‘é‡æç¤ºæ¥å¼•å¯¼æ¨¡å‹ï¼Œä»è€Œå®šä¹‰APIè€Œéå…³æ³¨å…¶å¯æ‰§è¡Œæ€§ã€‚å¯¹ä¸€å°éƒ¨åˆ†ï¼ˆçº¦0.5%ï¼‰çš„æ•™ç¨‹è¿›è¡Œåº”ç”¨åï¼Œä½œè€…å¾—å‡ºäº†éœ€è¦è¶…è¿‡300ä¸ªAPIæ¥æ•æ‰ç°å®ä¸–ç•Œä»»åŠ¡çš„ä¸°å¯Œå¤šæ ·æ€§ã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯å®ç°APIçš„æœ‰æ•ˆå¤ç”¨å’Œåˆ›å»ºã€‚æ­¤å¤–ï¼Œå¯¹ç°æœ‰æ¨¡æ‹Ÿå™¨çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬ä»…æ”¯æŒä¸€å°éƒ¨åˆ†è¯±å¯¼å‡ºçš„APIï¼Œè¿™çªæ˜¾äº†å¼€å‘ä¸°å¯ŒåŠ¨ä½œç¯å¢ƒçš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIç³»ç»Ÿé€šè¿‡APIè°ƒç”¨å®ç°åŸå§‹åŠ¨ä½œä»¥åœ¨ç‰©ç†ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ã€‚</li>
<li>åŸºäºwikiHowæ•™ç¨‹çš„å†…å®¹æå‡ºäº†ä¸€ç§è¿­ä»£è¯±å¯¼APIçš„æ–¹æ³•æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨GPT-4ç”Ÿæˆä»£ç†ç­–ç•¥å¹¶é‡‡ç”¨å°‘é‡æç¤ºè¿›è¡Œå¼•å¯¼ã€‚</li>
<li>å®šä¹‰APIè€Œéå…³æ³¨å…¶å¯æ‰§è¡Œæ€§ï¼Œä»è€Œæ•æ‰ç°å®ä¸–ç•Œä»»åŠ¡çš„ä¸°å¯Œå¤šæ ·æ€§ã€‚</li>
<li>éœ€è¦è¶…è¿‡300ä¸ªAPIæ¥è¦†ç›–è¿™äº›ä»»åŠ¡åŠ¨ä½œç©ºé—´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8f596728264157ae213a96c42e45fb1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd2449a9b103e0c30f6c1b80bc1bf44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d82e64dab393f6187df42d717a23a74b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c6191750e5c3fa01292e5be94d6518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4a528ecab0ac380ef50d6aacf26193d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd174b17553b021885569d4f4dc1ad2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ce184cef58ddd5022120c939e815910.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures"><a href="#Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures" class="headerlink" title="Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures"></a>Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy</strong></p>
<p>Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLPâ€™s potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">training code</a> and <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">weights</a> are public. </p>
<blockquote>
<p>è¿‘æœŸå¤–ç§‘è®¡ç®—æœºè§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦å¾—ç›Šäºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨è®¾è®¡æ—¶å¹¶æ²¡æœ‰æ˜ç¡®èå…¥ä¸°å¯Œçš„è¯­è¨€è¯­ä¹‰ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æ³¨é‡Šçš„æ‰‹æœ¯è§†é¢‘æ¥é¢„æµ‹å›ºå®šçš„å¯¹è±¡ç±»åˆ«é›†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æœªè§è¿‡çš„æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å…¬å¼€æ‰‹æœ¯ç”µå­å­¦ä¹ å¹³å°å¯ç”¨çš„æ‰‹æœ¯è§†é¢‘è®²åº§ï¼Œå¯ä»¥æä¾›æœ‰æ•ˆçš„è§†è§‰å’Œè¯­è¨€ç›‘ç£ä¿¡å·ï¼Œç”¨äºå¤šæ¨¡å¼è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œæ— éœ€ä¾èµ–æ‰‹åŠ¨æ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨å¤šä¸ªäº’è¡¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ¥ç”Ÿæˆæ–‡æœ¬è½¬å½•ï¼Œä»¥è§£å†³æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­ç‰¹æœ‰çš„è¯­è¨€æŒ‘æˆ˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ŒSurgVLPï¼ˆæ‰‹æœ¯è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼‰ï¼Œç”¨äºå¤šæ¨¡å¼è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨å¤šç§æ‰‹æœ¯ç¨‹åºå’Œä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSurgVLPå­¦ä¹ çš„å¤šæ¨¡å¼è¡¨ç¤ºåœ¨æ‰‹æœ¯è§†é¢‘åˆ†æä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬è¯„ä¼°å‡¸æ˜¾äº†SurgVLPä½œä¸ºæ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æçš„é€šç”¨åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå‡å°‘äº†ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶ä¿ƒè¿›äº†å°‘æ ·æœ¬å­¦ä¹ ç­‰é€‚åº”æ–¹æ³•ï¼Œä¸ºå„ç§ä¸‹æ¸¸æ‰‹æœ¯åº”ç”¨æ„å»ºå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆã€‚ <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">è®­ç»ƒä»£ç </a>å’Œ<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">æƒé‡</a>å‡å·²å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15220v5">PDF</a> Accepted by Medical Image Analysis (MedIA), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å…¬å¼€æ‰‹æœ¯ç”µå­å­¦ä¹ å¹³å°ä¸Šçš„æ‰‹æœ¯è§†é¢‘è®²åº§è¿›è¡Œå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨å¤šä¸ªäº’è¡¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿç”Ÿæˆæ–‡æœ¬è½¬å½•ï¼Œè§£å†³æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­çš„è¯­è¨€æŒ‘æˆ˜ã€‚æå‡ºä¸€ç§æ–°å‹æ–¹æ³•SurgVLPï¼ˆæ‰‹æœ¯è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼‰ï¼Œè¯¥æ–¹æ³•å¯åœ¨æ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼ŒSurgVLPæ‰€å­¦ä¹ çš„å¤šæ¨¡æ€è¡¨ç¤ºå…·æœ‰è¾ƒå¼ºçš„è¿ç§»æ€§å’Œé€‚åº”æ€§ï¼Œåœ¨æ‰‹æœ¯è§†é¢‘åˆ†æä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSurgVLPä½œä¸ºä¸€ä¸ªé€šç”¨åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰æ½œåŠ›ç”¨äºæ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æï¼Œå‡å°‘ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶ä¿ƒè¿›å°‘æ ·æœ¬å­¦ä¹ ç­‰é€‚åº”æ–¹æ³•çš„å¼€å‘ï¼Œä¸ºå„ç§ä¸‹æ¸¸æ‰‹æœ¯åº”ç”¨æä¾›å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸæ‰‹æœ¯è®¡ç®—æœºè§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦ä¾èµ–äºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ï¼Œæœªæ˜ç¡®æ•´åˆè¯­è¨€çš„ä¸°å¯Œè¯­ä¹‰ã€‚</li>
<li>æ‰‹åŠ¨æ³¨é‡Šçš„æ‰‹æœ¯è§†é¢‘é™åˆ¶äº†æ¨¡å‹å¯¹æœªè§æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å…¬å¼€æ‰‹æœ¯ç”µå­å­¦ä¹ å¹³å°ä¸Šçš„æ‰‹æœ¯è§†é¢‘è®²åº§å¯æä¾›æœ‰æ•ˆçš„è§†è§‰å’Œè¯­è¨€ç›‘ç£ä¿¡å·ï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>é‡‡ç”¨å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿç”Ÿæˆæ–‡æœ¬è½¬å½•ï¼Œä»¥åº”å¯¹æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­çš„è¯­è¨€æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ–°å‹æ–¹æ³•SurgVLPï¼Œå®ç°å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œå±•ç°å‡ºè‰²çš„è¿ç§»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>SurgVLPå…·æœ‰ä½œä¸ºé€šç”¨åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œç”¨äºæ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æï¼Œå¹¶ä¿ƒè¿›å°‘æ ·æœ¬å­¦ä¹ ç­‰æ–¹æ³•çš„å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.15220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6fdecff725c84b2d6289a90f5cb446a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1403ccbf9c4143513d490e722b7e05c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c3add5ad8609d1635f8f4d1ed6baed8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e543d07694b38146aa2793cb90a02bf7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Beyond Black Boxes Enhancing Interpretability of Transformers Trained   on Neural Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2097cc03b953e6a60cc7aa86dc447535.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Making LLMs Better Many-to-Many Speech-to-Text Translators with   Curriculum Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
