<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="Mono-Modalizing-Extremely-Heterogeneous-Multi-Modal-Medical-Image-Registration"><a href="#Mono-Modalizing-Extremely-Heterogeneous-Multi-Modal-Medical-Image-Registration" class="headerlink" title="Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration"></a>Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration</h2><p><strong>Authors:Kyobin Choo, Hyunkyung Han, Jinyeong Kim, Chanyong Yoon, Seong Jae Hwang</strong></p>
<p>In clinical practice, imaging modalities with functional characteristics, such as positron emission tomography (PET) and fractional anisotropy (FA), are often aligned with a structural reference (e.g., MRI, CT) for accurate interpretation or group analysis, necessitating multi-modal deformable image registration (DIR). However, due to the extreme heterogeneity of these modalities compared to standard structural scans, conventional unsupervised DIR methods struggle to learn reliable spatial mappings and often distort images. We find that the similarity metrics guiding these models fail to capture alignment between highly disparate modalities. To address this, we propose M2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal DIR models using only mono-modal similarity while preserving the established architectural paradigm for seamless integration into existing models. We also introduce GradCyCon, a regularizer that leverages M2M-Regâ€™s cyclic training scheme to promote diffeomorphism. Furthermore, our framework naturally extends to a semi-supervised setting, integrating pre-aligned and unaligned pairs only, without requiring ground-truth transformations or segmentation masks. Experiments on the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for PET-MRI and FA-MRI registration, highlighting its effectiveness in handling highly heterogeneous multi-modal DIR. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MICV-yonsei/M2M-Reg">https://github.com/MICV-yonsei/M2M-Reg</a>. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œå…·æœ‰åŠŸèƒ½ç‰¹å¾çš„æˆåƒæ–¹å¼ï¼Œå¦‚æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å’Œåˆ†æ•°å¼‚æ–¹å·®æ€§ï¼ˆFAï¼‰ï¼Œé€šå¸¸éœ€è¦ä¸ç»“æ„å‚è€ƒï¼ˆä¾‹å¦‚MRIã€CTï¼‰å¯¹é½ï¼Œä»¥å®ç°å‡†ç¡®çš„è§£é‡Šæˆ–ç¾¤ä½“åˆ†æï¼Œè¿™å°±éœ€è¦è¿›è¡Œå¤šæ¨¡æ€å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æˆåƒæ–¹å¼ä¸æ ‡å‡†ç»“æ„æ‰«æç›¸æ¯”å…·æœ‰æå¤§çš„å¼‚è´¨æ€§ï¼Œä¼ ç»Ÿçš„æ— ç›‘ç£DIRæ–¹æ³•éš¾ä»¥å­¦ä¹ å¯é çš„ç©ºé—´æ˜ å°„å…³ç³»ï¼Œå¹¶ç»å¸¸å¯¼è‡´å›¾åƒå¤±çœŸã€‚æˆ‘ä»¬å‘ç°ï¼ŒæŒ‡å¯¼è¿™äº›æ¨¡å‹çš„ç›¸ä¼¼åº¦åº¦é‡æ— æ³•æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2M-Regï¼ˆå¤šå¯¹å•æ³¨å†Œï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å•æ¨¡æ€ç›¸ä¼¼åº¦è®­ç»ƒå¤šæ¨¡æ€DIRæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™äº†æ— ç¼é›†æˆç°æœ‰æ¨¡å‹çš„æ—¢å®šæ¶æ„èŒƒå¼ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†GradCyConæ­£åˆ™åŒ–å™¨ï¼Œå®ƒåˆ©ç”¨M2M-Regçš„å¾ªç¯è®­ç»ƒæ–¹æ¡ˆæ¥ä¿ƒè¿›å¾®åˆ†åŒèƒšã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è‡ªç„¶åœ°æ‰©å±•åˆ°åŠç›‘ç£ç¯å¢ƒï¼Œåªæ•´åˆé¢„å…ˆå¯¹é½å’Œæœªå¯¹é½çš„é…å¯¹ï¼Œæ— éœ€ä¾èµ–åœ°é¢çœŸå®å˜æ¢æˆ–åˆ†å‰²æ©è†œã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ç¥ç»å½±åƒå­¦å€¡è®®ï¼ˆADNIï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM2M-Regåœ¨PET-MRIå’ŒFA-MRIé…å‡†æ–¹é¢çš„DSCå¾—åˆ†è¾¾åˆ°äº†å…ˆå‰æ–¹æ³•çš„ä¸¤å€ä»¥ä¸Šï¼Œå‡¸æ˜¾å…¶åœ¨å¤„ç†é«˜åº¦å¼‚è´¨çš„å¤šæ¨¡æ€DIRæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MICV-yonsei/M2M-Reg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MICV-yonsei/M2M-Regè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15596v1">PDF</a> 11 pages, 3 figures, 2 tables, Accepted at Medical Image Computing   and Computer Assisted Intervention (MICCAI) 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å›¾åƒé…å‡†æ¡†æ¶M2M-Regï¼Œè¯¥æ¡†æ¶å¯åœ¨å…·æœ‰é«˜åº¦å¼‚è´¨æ€§å¤šæ¨¡æ€å›¾åƒä¸­æä¾›å¯é çš„ç©ºé—´æ˜ å°„ã€‚å®ƒä½¿ç”¨å•æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡Œå¤šæ¨¡æ€å¯å˜å½¢å›¾åƒé…å‡†æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶å¼•å…¥GradCyConæ­£åˆ™åŒ–å™¨ä»¥ä¿ƒè¿›å¾®åˆ†åŒèƒšã€‚M2M-Regæ¡†æ¶å¯è‡ªç„¶åœ°æ‰©å±•åˆ°åŠç›‘ç£è®¾ç½®ï¼Œä»…é›†æˆå·²å¯¹é½å’Œæœªå¯¹é½çš„å¯¹ï¼Œæ— éœ€åœ°é¢çœŸå®å˜æ¢æˆ–åˆ†å‰²æ©è†œã€‚åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM2M-Regåœ¨PET-MRIå’ŒFA-MRIé…å‡†æ–¹é¢çš„æ•ˆæœæ˜¯ä»¥å‰æ–¹æ³•çš„æœ€é«˜å¯è¾¾ä¸¤å€ã€‚è¿™ä¸€æ–°æ–¹æ³•å¯ä»¥æœ‰æ•ˆå¤„ç†å…·æœ‰é«˜åº¦å¼‚è´¨æ€§ç‰¹ç‚¹çš„å¤šæ¨¡æ€å¯å˜å½¢å›¾åƒé…å‡†é—®é¢˜ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡MICV-yonsei&#x2F;M2M-Regé“¾æ¥è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€æˆåƒä¸­ï¼ŒåŠŸèƒ½ç‰¹æ€§æˆåƒæ¨¡å¼ï¼ˆå¦‚PETå’ŒFAï¼‰ä¸ç»“æ„å‚è€ƒï¼ˆå¦‚MRIã€CTï¼‰çš„å‡†ç¡®è§£è¯»æˆ–ç¾¤ç»„åˆ†æéœ€è¦å¤šæ¨¡æ€å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ã€‚</li>
<li>ç”±äºæˆåƒæ¨¡æ€çš„æç«¯å¼‚è´¨æ€§ï¼Œä¼ ç»Ÿæ— ç›‘ç£DIRæ–¹æ³•éš¾ä»¥å­¦ä¹ å¯é çš„ç©ºé—´æ˜ å°„ï¼Œå¹¶å¯èƒ½å¯¼è‡´å›¾åƒå¤±çœŸã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸­çš„ç›¸ä¼¼æ€§åº¦é‡æ— æ³•æ•æ‰é«˜åº¦ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶M2M-Regï¼Œä½¿ç”¨å•æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡Œå¤šæ¨¡æ€DIRæ¨¡å‹çš„è®­ç»ƒï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>M2M-Regæ¡†æ¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰æ¨¡å‹ä¸­ï¼Œå¹¶ä¿æŒå·²å»ºç«‹çš„æ¶æ„èŒƒå¼ã€‚</li>
<li>å¼•å…¥GradCyConæ­£åˆ™åŒ–å™¨ï¼Œåˆ©ç”¨M2M-Regçš„å¾ªç¯è®­ç»ƒæ–¹æ¡ˆä¿ƒè¿›å¾®åˆ†åŒèƒšã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ae89e84d6e9053a7e3fd3b51bc85698.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00beff875d3d20d879a74b3ee124ab1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10e1d75aa748a0df493c4413d017c878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05a4efb1c884273774b625f6a3a6a101.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multimodal-Large-Language-Models-for-Medical-Report-Generation-via-Customized-Prompt-Tuning"><a href="#Multimodal-Large-Language-Models-for-Medical-Report-Generation-via-Customized-Prompt-Tuning" class="headerlink" title="Multimodal Large Language Models for Medical Report Generation via   Customized Prompt Tuning"></a>Multimodal Large Language Models for Medical Report Generation via   Customized Prompt Tuning</h2><p><strong>Authors:Chunlei Li, Jingyang Hou, Yilei Shi, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Medical report generation from imaging data remains a challenging task in clinical practice. While large language models (LLMs) show great promise in addressing this challenge, their effective integration with medical imaging data still deserves in-depth exploration. In this paper, we present MRG-LLM, a novel multimodal large language model (MLLM) that combines a frozen LLM with a learnable visual encoder and introduces a dynamic prompt customization mechanism. Our key innovation lies in generating instance-specific prompts tailored to individual medical images through conditional affine transformations derived from visual features. We propose two implementations: prompt-wise and promptbook-wise customization, enabling precise and targeted report generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets demonstrate that MRG-LLM achieves state-of-the-art performance in medical report generation. Our code will be made publicly available. </p>
<blockquote>
<p>åŒ»ç–—æŠ¥å‘Šä»æˆåƒæ•°æ®ç”Ÿæˆä»ç„¶æ˜¯ä¸´åºŠå®è·µä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”å¯¹è¿™ä¸€æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ä¸åŒ»å­¦æˆåƒæ•°æ®çš„æœ‰æ•ˆæ•´åˆä»éœ€è¦è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MRG-LLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒå°†å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¯å­¦ä¹ çš„è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŠ¨æ€æç¤ºå®šåˆ¶æœºåˆ¶ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºé€šè¿‡æ¥è‡ªè§†è§‰ç‰¹å¾çš„æ¡ä»¶ä»¿å°„å˜æ¢ä¸ºå•ä¸ªåŒ»å­¦å›¾åƒç”Ÿæˆç‰¹å®šå®ä¾‹çš„æç¤ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å®ç°æ–¹æ³•ï¼šæç¤ºçº§å’Œæç¤ºä¹¦çº§å®šåˆ¶ï¼Œä»¥å®ç°ç²¾ç¡®å’Œæœ‰é’ˆå¯¹æ€§çš„æŠ¥å‘Šç”Ÿæˆã€‚åœ¨IU Xå…‰ç‰‡å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMRG-LLMåœ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15477v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒæŠ¥å‘Šç”Ÿæˆæ˜¯ä¸´åºŠå®è·µä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMRG-LLMï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆå†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€å¯å­¦ä¹ çš„è§†è§‰ç¼–ç å™¨å’ŒåŠ¨æ€æç¤ºå®šåˆ¶æœºåˆ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å…³é”®åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡è§†è§‰ç‰¹å¾æ´¾ç”Ÿçš„æ¡ä»¶ä»¿å°„å˜æ¢ä¸ºå•ä¸ªåŒ»å­¦å›¾åƒç”Ÿæˆç‰¹å®šå®ä¾‹çš„æç¤ºã€‚åœ¨IU Xå°„çº¿å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMRG-LLMåœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæŠ¥å‘Šç”Ÿæˆæ˜¯ä¸´åºŠå®è·µçš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>MRG-LLMæ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç¼–ç å™¨å’ŒåŠ¨æ€æç¤ºå®šåˆ¶æœºåˆ¶ã€‚</li>
<li>MRG-LLMé€šè¿‡æ¡ä»¶ä»¿å°„å˜æ¢ä¸ºåŒ»å­¦å›¾åƒç”Ÿæˆç‰¹å®šå®ä¾‹çš„æç¤ºã€‚</li>
<li>MRG-LLMæœ‰ä¸¤ç§å®ç°æ–¹å¼ï¼šåŸºäºæç¤ºå’ŒåŸºäºæç¤ºä¹¦çš„å®šåˆ¶ï¼Œå®ç°äº†ç²¾ç¡®å’Œæœ‰é’ˆå¯¹æ€§çš„æŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>åœ¨IU Xå°„çº¿å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMRG-LLMåœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>MRG-LLMçš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-76640990b4e16981177f1cbeec9a5fc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ec4fdd8d958d27cc0edd0a7eec2a8f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56f4aa7aba1772afa7d7a2cd76a9026b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="XDoppler-axial-velocity-estimation-using-row-column-arrays"><a href="#XDoppler-axial-velocity-estimation-using-row-column-arrays" class="headerlink" title="XDoppler axial velocity estimation using row-column arrays"></a>XDoppler axial velocity estimation using row-column arrays</h2><p><strong>Authors:Henri Leroy, Adrien Bertolo, Guillaume Goudot, Mickael Tanter, Thomas Deffieux, Mathieu Pernot</strong></p>
<p>Accurate volumetric velocity estimation in ultrasound imaging is crucial for applications in diagnostic and therapeutic medicine. Traditional ultrasound systems, while effective in two-dimensional imaging, face significant challenges in achieving 3D imaging due to hardware and computational demands. Row-column addressed (RCA) ultrasound probes present a promising alternative, reducing hardware complexity, thus contributing to bridge the gap between research and clinical systems. Yet, this usually comes at the cost of lower signal-to-noise ratio (SNR), reduced sensitivity and stronger sidelobes compared to 3D matrices, even though recent methods have tried to tackle the later issue. In this study, we introduce a method to use the phase of the signals acquired by rows and columns of RCA arrays to compute a new velocity estimator based on cross correlation of orthogonal apertures, extending the XDoppler scheme initially introduced for power Doppler imaging to the estimation of velocities. Our results indicate that XDoppler estimator can measure faithfully axial velocities (RMSE&#x3D;9.21%, R2&#x3D;0.88) and outperforms the traditional phase shift autocorrelator with a theoretical Nyquist velocity twice higher. Moreover, the in vitro data indicate a better sensitivity to flow velocities below 10mm&#x2F;s and a lower bias for flow rate estimation (-0.17mL&#x2F;min as opposed to -9.33mL&#x2F;min for reference method). In vivo data acquired on a carotid artery confirm the reduced sensitivity to aliasing effects and show that this new technique can dynamically follow blood velocity evolutions linked to arterial pulsatility. This suggests that this new estimator could be of use for improved volumetric blood velocity imaging with a RCA probe. </p>
<blockquote>
<p>åœ¨è¶…å£°æˆåƒä¸­è¿›è¡Œå‡†ç¡®çš„ä½“ç§¯é€Ÿåº¦ä¼°è®¡å¯¹äºè¯Šæ–­å’Œæ²»ç–—åŒ»å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶ä¼ ç»Ÿè¶…å£°ç³»ç»Ÿåœ¨äºŒç»´æˆåƒä¸­éå¸¸æœ‰æ•ˆï¼Œä½†ç”±äºç¡¬ä»¶å’Œè®¡ç®—éœ€æ±‚ï¼Œå®ƒä»¬åœ¨å®ç°3Dæˆåƒæ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è¡Œåˆ—å¯»å€ï¼ˆRCAï¼‰è¶…å£°æ¢å¤´è¡¨ç°å‡ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé™ä½äº†ç¡¬ä»¶å¤æ‚æ€§ï¼Œä»è€Œæœ‰åŠ©äºå¼¥åˆç ”ç©¶å’Œä¸´åºŠç³»ç»Ÿä¹‹é—´çš„å·®è·ã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸ä»¥è¾ƒä½çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€é™ä½çš„çµæ•åº¦å’Œè¾ƒä¸¥é‡çš„æ—ç“£ç›¸æ¯”äº3DçŸ©é˜µï¼Œå°½ç®¡æœ€è¿‘çš„æ–¹æ³•è¯•å›¾è§£å†³åé¢çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨RCAé˜µåˆ—çš„è¡Œå’Œåˆ—è·å¾—çš„ä¿¡å·çš„ç›¸ä½æ¥è®¡ç®—åŸºäºæ­£äº¤å­”å¾„äº’ç›¸å…³çš„æ–°é€Ÿåº¦ä¼°è®¡å™¨ï¼Œæ‰©å±•äº†æœ€åˆä¸ºåŠŸç‡å¤šæ™®å‹’æˆåƒå¼•å…¥çš„XDoppleræ–¹æ¡ˆç”¨äºé€Ÿåº¦ä¼°è®¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒXDopplerä¼°è®¡å™¨å¯ä»¥å¿ å®åœ°æµ‹é‡è½´å‘é€Ÿåº¦ï¼ˆRMSE&#x3D;9.21%ï¼ŒR2&#x3D;0.88ï¼‰ï¼Œå¹¶ä¸”ç†è®ºä¸Šå°¼å¥æ–¯ç‰¹é€Ÿåº¦æé«˜äº†ä¸€å€åœ°è¶…è¶Šäº†ä¼ ç»Ÿçš„ç›¸ä½è‡ªç›¸å…³å™¨ã€‚æ­¤å¤–ï¼Œä½“å¤–æ•°æ®è¡¨æ˜å¯¹ä½äº10mm&#x2F;sçš„æµé€Ÿå…·æœ‰æ›´é«˜çš„çµæ•åº¦ï¼Œå¹¶ä¸”å¯¹æµé€Ÿä¼°è®¡çš„åå·®è¾ƒä½ï¼ˆ-0.17mL&#x2F;minç›¸å¯¹äºå‚è€ƒæ–¹æ³•çš„-9.33mL&#x2F;minï¼‰ã€‚åœ¨é¢ˆåŠ¨è„‰ä¸Šé‡‡é›†çš„åœ¨ä½“æ•°æ®è¯å®äº†è¾ƒå°‘çš„æ··å æ•ˆåº”æ•æ„Ÿæ€§ï¼Œå¹¶æ˜¾ç¤ºè¿™ç§æ–°æŠ€æœ¯èƒ½å¤ŸåŠ¨æ€è·Ÿè¸ªä¸åŠ¨è„‰è„‰åŠ¨ç›¸å…³çš„è¡€æ¶²é€Ÿåº¦å˜åŒ–ã€‚è¿™è¡¨æ˜è¿™ç§æ–°å‹ä¼°è®¡å™¨å¯ç”¨äºé‡‡ç”¨RCAæ¢é’ˆæ”¹è¿›ä½“ç§¯è¡€æ¶²é€Ÿåº¦æˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15469v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åŸºäºè¡Œ-åˆ—å¯»å€ï¼ˆRCAï¼‰é˜µåˆ—çš„è¶…å£°æˆåƒæ–°æŠ€æœ¯ï¼Œç”¨äºç²¾ç¡®ä¼°ç®—è¡€æµé€Ÿåº¦ã€‚æœ¬ç ”ç©¶é€šè¿‡å°†XDoppleræ–¹æ¡ˆæ‰©å±•åˆ°é€Ÿåº¦ä¼°ç®—ï¼Œé‡‡ç”¨RCAé˜µåˆ—è¡Œå’Œåˆ—é‡‡é›†ä¿¡å·çš„ç›¸ä½ï¼Œé€šè¿‡æ­£äº¤å­”å¾„çš„äº’ç›¸å…³è®¡ç®—æ–°çš„é€Ÿåº¦ä¼°ç®—å™¨ã€‚ç»“æœæ˜¾ç¤ºï¼ŒXDopplerä¼°ç®—å™¨èƒ½å‡†ç¡®æµ‹é‡è½´å‘é€Ÿåº¦ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„ç›¸ä½è‡ªç›¸å…³å™¨ï¼Œç†è®ºä¸Šçš„Nyquisté€Ÿåº¦æé«˜äº†ä¸€å€ã€‚ä½“å¤–æ•°æ®è¡¨æ˜ï¼Œè¯¥æŠ€æœ¯åœ¨æµé€Ÿä½äº10mm&#x2F;sæ—¶çš„çµæ•åº¦æ›´é«˜ï¼Œæµé€Ÿä¼°è®¡çš„åå·®è¾ƒä½ã€‚ä½“å†…é¢ˆåŠ¨è„‰æ•°æ®éªŒè¯äº†è¯¥æŠ€æœ¯å¯¹æ··å æ•ˆåº”çš„æ•æ„Ÿæ€§é™ä½ï¼Œå¹¶èƒ½åŠ¨æ€è·Ÿè¸ªä¸åŠ¨è„‰æåŠ¨ç›¸å…³çš„è¡€æµé€Ÿåº¦å˜åŒ–ã€‚è¿™è¡¨æ˜ï¼Œè¿™ä¸€æ–°æŠ€æœ¯å¯ç”¨äºæ”¹è¿›åŸºäºRCAæ¢é’ˆçš„è¡€æµé€Ÿåº¦æˆåƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¡Œ-åˆ—å¯»å€ï¼ˆRCAï¼‰è¶…å£°æ¢é’ˆåœ¨ä¸‰ç»´æˆåƒä¸­å…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤Ÿå‡å°‘ç¡¬ä»¶å¤æ‚æ€§ï¼Œå¹¶æœ‰åŠ©äºç¼©å°ç ”ç©¶ä¸ä¸´åºŠç³»ç»Ÿä¹‹é—´çš„å·®è·ã€‚</li>
<li>æœ¬ç ”ç©¶å°†XDoppleræ–¹æ¡ˆæ‰©å±•åˆ°é€Ÿåº¦ä¼°ç®—ï¼Œä»¥æé«˜è¡€æµé€Ÿåº¦æµ‹é‡çš„å‡†ç¡®æ€§ã€‚</li>
<li>XDopplerä¼°ç®—å™¨èƒ½å¤Ÿå‡†ç¡®æµ‹é‡è½´å‘é€Ÿåº¦ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿç›¸ä½è‡ªç›¸å…³å™¨ï¼Œä¸”ç†è®ºNyquisté€Ÿåº¦æé«˜äº†ä¸€å€ã€‚</li>
<li>ä½“å¤–å®éªŒè¡¨æ˜ï¼Œæ–°æŠ€æœ¯åœ¨æµé€Ÿè¾ƒä½æ—¶çš„çµæ•åº¦æ›´é«˜ï¼Œæµé€Ÿä¼°è®¡åå·®è¾ƒä½ã€‚</li>
<li>ä½“å†…å®éªŒéªŒè¯äº†æ–°æŠ€æœ¯å¯¹æ··å æ•ˆåº”çš„æ•æ„Ÿæ€§é™ä½ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è·Ÿè¸ªè¡€æµé€Ÿåº¦å˜åŒ–ã€‚</li>
<li>æ–°æŠ€æœ¯èƒ½å¤Ÿæé«˜åŸºäºRCAæ¢é’ˆçš„è¡€æµé€Ÿåº¦æˆåƒçš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e927fa80e84126bf6e76052cdc41e7b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03edcf0ae6fcb271e65aed3f3433ea3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ee37d79c456674b4543cc2472fcfae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3d28da543e6ead931a3e020db03272d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f9b225ac9d4704ae7bc3c9ed081810f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775fd1e7be55987e720da15268addbac.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BCRNet-Enhancing-Landmark-Detection-in-Laparoscopic-Liver-Surgery-via-Bezier-Curve-Refinement"><a href="#BCRNet-Enhancing-Landmark-Detection-in-Laparoscopic-Liver-Surgery-via-Bezier-Curve-Refinement" class="headerlink" title="BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via   Bezier Curve Refinement"></a>BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via   Bezier Curve Refinement</h2><p><strong>Authors:Qian Li, Feng Liu, Shuojue Yang, Daiyun Shen, Yueming Jin</strong></p>
<p>Laparoscopic liver surgery, while minimally invasive, poses significant challenges in accurately identifying critical anatomical structures. Augmented reality (AR) systems, integrating MRI&#x2F;CT with laparoscopic images based on 2D-3D registration, offer a promising solution for enhancing surgical navigation. A vital aspect of the registration progress is the precise detection of curvilinear anatomical landmarks in laparoscopic images. In this paper, we propose BCRNet (Bezier Curve Refinement Net), a novel framework that significantly enhances landmark detection in laparoscopic liver surgery primarily via the Bezier curve refinement strategy. The framework starts with a Multi-modal Feature Extraction (MFE) module designed to robustly capture semantic features. Then we propose Adaptive Curve Proposal Initialization (ACPI) to generate pixel-aligned Bezier curves and confidence scores for reliable initial proposals. Additionally, we design the Hierarchical Curve Refinement (HCR) mechanism to enhance these proposals iteratively through a multi-stage process, capturing fine-grained contextual details from multi-scale pixel-level features for precise Bezier curve adjustment. Extensive evaluations on the L3D and P2ILF datasets demonstrate that BCRNet outperforms state-of-the-art methods, achieving significant performance improvements. Code will be available. </p>
<blockquote>
<p>è…¹è…”é•œè‚æ‰‹æœ¯è™½ç„¶å…·æœ‰å¾®åˆ›æ€§ï¼Œä½†åœ¨å‡†ç¡®è¯†åˆ«å…³é”®è§£å‰–ç»“æ„æ–¹é¢ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å¢å¼ºç°å®ï¼ˆARï¼‰ç³»ç»Ÿé€šè¿‡MRI&#x2F;CTä¸è…¹è…”é•œå›¾åƒçš„äºŒç»´åˆ°ä¸‰ç»´æ³¨å†Œé›†æˆï¼Œä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¯å¢å¼ºæ‰‹æœ¯å¯¼èˆªã€‚æ³¨å†Œè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯ç²¾ç¡®æ£€æµ‹è…¹è…”é•œå›¾åƒä¸­çš„æ›²çº¿è§£å‰–æ ‡å¿—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BCRNetï¼ˆBezieræ›²çº¿ç»†åŒ–ç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œä¸»è¦é€šè¿‡Bezieræ›²çº¿ç»†åŒ–ç­–ç•¥æå¤§åœ°æé«˜äº†è…¹è…”é•œè‚æ‰‹æœ¯ä¸­çš„æ ‡å¿—æ£€æµ‹ã€‚è¯¥æ¡†æ¶å§‹äºå¤šæ¨¡æ€ç‰¹å¾æå–ï¼ˆMFEï¼‰æ¨¡å—ï¼Œæ—¨åœ¨ç¨³å¥åœ°æ•è·è¯­ä¹‰ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ›²çº¿ææ¡ˆåˆå§‹åŒ–ï¼ˆACPIï¼‰ï¼Œä»¥ç”Ÿæˆåƒç´ å¯¹é½çš„Bezieræ›²çº¿å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼Œç”¨äºå¯é çš„åˆå§‹ææ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†åˆ†å±‚æ›²çº¿ç»†åŒ–ï¼ˆHCRï¼‰æœºåˆ¶ï¼Œé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹é€æ­¥å¢å¼ºè¿™äº›ææ¡ˆï¼Œä»å¤šå°ºåº¦åƒç´ çº§ç‰¹å¾ä¸­æ•è·ç²¾ç»†çš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œè¿›è¡Œç²¾ç¡®çš„Bezieræ›²çº¿è°ƒæ•´ã€‚åœ¨L3Då’ŒP2ILFæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒBCRNetä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç›¸å…³ä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15279v1">PDF</a> Accepted at MICCAI 2025, 11 pages, 2 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦è…¹è…”é•œè‚æ‰‹æœ¯ä¸­çš„ç²¾å‡†è§£å‰–ç»“æ„è¯†åˆ«æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å¢å®æŠ€æœ¯ï¼ˆARï¼‰ç»“åˆMRI&#x2F;CTä¸è…¹è…”é•œå›¾åƒçš„2D-3Dæ³¨å†ŒæŠ€æœ¯ï¼Œä¸ºæé«˜æ‰‹æœ¯å¯¼èˆªæä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚è®ºæ–‡æå‡ºBCRNetæ¡†æ¶ï¼Œé€šè¿‡Bezieræ›²çº¿ç²¾ä¿®ç­–ç•¥å¤§å¹…æé«˜äº†è…¹è…”é•œè‚æ‰‹æœ¯ä¸­çš„åœ°æ ‡æ£€æµ‹ç²¾åº¦ã€‚è¯¥æ¡†æ¶åŒ…å«å¤šæ¨¡æ€ç‰¹å¾æå–æ¨¡å—ã€è‡ªé€‚åº”æ›²çº¿ææ¡ˆåˆå§‹åŒ–åŠåˆ†å±‚æ›²çº¿ç²¾ä¿®æœºåˆ¶ã€‚åœ¨L3Då’ŒP2ILFæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒBCRNetä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è…¹è…”é•œè‚æ‰‹æœ¯åœ¨å‡†ç¡®è¯†åˆ«è§£å‰–ç»“æ„æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¢å®æŠ€æœ¯ï¼ˆARï¼‰ç»“åˆMRI&#x2F;CTä¸è…¹è…”é•œå›¾åƒçš„2D-3Dæ³¨å†ŒæŠ€æœ¯ä¸ºæ‰‹æœ¯å¯¼èˆªæä¾›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>BCRNetæ¡†æ¶é€šè¿‡Bezieræ›²çº¿ç²¾ä¿®ç­–ç•¥æé«˜è…¹è…”é•œè‚æ‰‹æœ¯ä¸­çš„åœ°æ ‡æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>BCRNetåŒ…å«å¤šæ¨¡æ€ç‰¹å¾æå–æ¨¡å—ï¼Œç”¨äºç¨³å¥æ•è·è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>è‡ªé€‚åº”æ›²çº¿ææ¡ˆåˆå§‹åŒ–ç”Ÿæˆåƒç´ å¯¹é½çš„Bezieræ›²çº¿å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼Œç”¨äºå¯é åˆå§‹ææ¡ˆã€‚</li>
<li>åˆ†å±‚æ›²çº¿ç²¾ä¿®æœºåˆ¶é€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹å¢å¼ºææ¡ˆï¼Œä»å¤šå°ºåº¦åƒç´ çº§ç‰¹å¾æ•è·ç»†èŠ‚ï¼Œå®ç°ç²¾ç¡®çš„Bezieræ›²çº¿è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d50b9964c557a7c98e3b354e6e6ff5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a001708255060efd9bb6346a6de261a5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DM-FNet-Unified-multimodal-medical-image-fusion-via-diffusion-process-trained-encoder-decoder"><a href="#DM-FNet-Unified-multimodal-medical-image-fusion-via-diffusion-process-trained-encoder-decoder" class="headerlink" title="DM-FNet: Unified multimodal medical image fusion via diffusion   process-trained encoder-decoder"></a>DM-FNet: Unified multimodal medical image fusion via diffusion   process-trained encoder-decoder</h2><p><strong>Authors:Dan He, Weisheng Li, Guofen Wang, Yuping Huang, Shiqiang Liu</strong></p>
<p>Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the modelâ€™s feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused imageâ€™s brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HeDan-11/DM-FNet">https://github.com/HeDan-11/DM-FNet</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰ä»å¤šä¸ªæºå›¾åƒä¸­æå–æœ€æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œä½¿è¯Šæ–­æ›´å…¨é¢ã€æ›´å‡†ç¡®ã€‚å®ç°é«˜è´¨é‡çš„èåˆç»“æœéœ€è¦åœ¨äº®åº¦ã€é¢œè‰²ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚ä¹‹é—´å–å¾—å¾®å¦™çš„å¹³è¡¡ï¼›è¿™ç¡®ä¿äº†èåˆå›¾åƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ˜¾ç¤ºç›¸å…³çš„è§£å‰–ç»“æ„å¹¶åæ˜ ç»„ç»‡çš„åŠŸèƒ½çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MMIFæ–¹æ³•åœ¨å¸¸è§„è®­ç»ƒè¿‡ç¨‹ä¸­æ•æ‰è¯¦ç»†ç‰¹å¾çš„èƒ½åŠ›æœ‰é™ï¼Œå¹¶ä¸”åœ¨è·¨æ¨¡æ€ç‰¹å¾äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´èåˆå›¾åƒè´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹çš„èåˆç½‘ç»œï¼ˆDM-FNetï¼‰æ¥å®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ‰©æ•£è¿‡ç¨‹è®­ç»ƒUNetè¿›è¡Œå›¾åƒé‡å»ºã€‚UNeté€šè¿‡æ¸è¿›çš„å»å™ªæ•è·è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶ä»£è¡¨å¤šçº§æ•°æ®ï¼Œä¸ºéšåçš„èåˆç½‘ç»œæä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†ä¸åŒæ­¥éª¤çš„å™ªå£°å›¾åƒè¾“å…¥èåˆç½‘ç»œï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç‰¹å¾è¯†åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†ä¸‰ä¸ªå…³é”®èåˆæ¨¡å—ï¼Œä»¥è‡ªé€‚åº”åœ°å¤„ç†æ¥è‡ªä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒã€‚æœ€ç»ˆï¼Œé€šè¿‡æ•´åˆç¨³å¥çš„ç½‘ç»œç»“æ„å’Œæ··åˆæŸå¤±å‡½æ•°æ¥åè°ƒèåˆå›¾åƒçš„äº®åº¦ã€é¢œè‰²ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚ï¼Œä»è€Œæé«˜å…¶è´¨é‡å’Œä¿¡æ¯å¯†åº¦ã€‚åœ¨å¤šç§åŒ»å­¦å›¾åƒç±»å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚èåˆå›¾åƒä¿æŒäº†é€‚å½“çš„äº®åº¦ã€å…¨é¢çš„æ”¾å°„æ€§ç¤ºè¸ªå‰‚åˆ†å¸ƒã€ä¸°å¯Œçš„çº¹ç†å’Œæ¸…æ™°çš„è¾¹ç¼˜ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HeDan-11/DM-FNet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HeDan-11/DM-FNetè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15218v1">PDF</a> This paper has been accepted by IEEE Transactions on Multimedia (TMM)   in March 2025</p>
<p><strong>Summary</strong><br>     åŸºäºä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆç½‘ç»œï¼ˆDM-FNetï¼‰èƒ½æœ‰æ•ˆèåˆå¤šæºåŒ»å­¦å›¾åƒä¿¡æ¯ï¼Œæé«˜è¯Šæ–­çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡æ‰©æ•£è¿‡ç¨‹è®­ç»ƒUNetè¿›è¡Œå›¾åƒé‡å»ºï¼Œå¹¶æ•´åˆä¸‰ä¸ªå…³é”®èåˆæ¨¡å—ï¼Œä»¥è‡ªé€‚åº”å¤„ç†ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å®¢è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèåˆå›¾åƒä¿æŒé€‚å½“çš„äº®åº¦ã€æ”¾å°„æ€§ç¤ºè¸ªç‰©åˆ†å¸ƒå…¨é¢ã€çº¹ç†ä¸°å¯Œã€è¾¹ç¼˜æ¸…æ™°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰èƒ½å¤Ÿä»å¤šä¸ªæºå›¾åƒä¸­æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œä¸ºå…¨é¢å‡†ç¡®çš„è¯Šæ–­æä¾›æ”¯æŒã€‚</li>
<li>ç°æœ‰çš„MMIFæ–¹æ³•åœ¨å¸¸è§„è®­ç»ƒä¸­å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å……åˆ†æ•æ‰è¯¦ç»†ç‰¹å¾ï¼Œå¹¶ä¸”åœ¨è·¨æ¨¡æ€ç‰¹å¾äº¤äº’æ–¹é¢è¡¨ç°ä¸è¶³ã€‚</li>
<li>æ­¤ç ”ç©¶æå‡ºåŸºäºä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼ˆDM-FNetï¼‰çš„èåˆç½‘ç»œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè¯¥ç½‘ç»œé€šè¿‡æ‰©æ•£è¿‡ç¨‹è®­ç»ƒUNetè¿›è¡Œå›¾åƒé‡å»ºï¼Œæä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µèåˆç½‘ç»œèƒ½å¤Ÿå¢å¼ºæ¨¡å‹çš„ç‰¹å¾è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ•´åˆä¸‰ä¸ªå…³é”®èåˆæ¨¡å—è‡ªé€‚åº”å¤„ç†ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–ç½‘ç»œç»“æ„å’Œæ··åˆæŸå¤±å‡½æ•°ï¼Œå®ç°äº†èåˆå›¾åƒçš„äº®åº¦ã€è‰²å½©ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚çš„å¹³è¡¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®¢è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèåˆå›¾åƒè´¨é‡é«˜ï¼Œä¿¡æ¯å¯†åº¦ä¸°å¯Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0f74d70bc1b2d739361fe08eb85efe5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39df3c3ec68df88464dcea4a6aa44ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-711d5d23c108bb0b76cdd05d6ed68e26.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Classification-of-Multi-Parametric-Body-MRI-Series-Using-Deep-Learning"><a href="#Classification-of-Multi-Parametric-Body-MRI-Series-Using-Deep-Learning" class="headerlink" title="Classification of Multi-Parametric Body MRI Series Using Deep Learning"></a>Classification of Multi-Parametric Body MRI Series Using Deep Learning</h2><p><strong>Authors:Boah Kim, Tejas Sudharshan Mathai, Kimberly Helm, Peter A. Pinto, Ronald M. Summers</strong></p>
<p>Multi-parametric magnetic resonance imaging (mpMRI) exams have various series types acquired with different imaging protocols. The DICOM headers of these series often have incorrect information due to the sheer diversity of protocols and occasional technologist errors. To address this, we present a deep learning-based classification model to classify 8 different body mpMRI series types so that radiologists read the exams efficiently. Using mpMRI data from various institutions, multiple deep learning-based classifiers of ResNet, EfficientNet, and DenseNet are trained to classify 8 different MRI series, and their performance is compared. Then, the best-performing classifier is identified, and its classification capability under the setting of different training data quantities is studied. Also, the model is evaluated on the out-of-training-distribution datasets. Moreover, the model is trained using mpMRI exams obtained from different scanners in two training strategies, and its performance is tested. Experimental results show that the DenseNet-121 model achieves the highest F1-score and accuracy of 0.966 and 0.972 over the other classification models with p-value$&lt;$0.05. The model shows greater than 0.95 accuracy when trained with over 729 studies of the training data, whose performance improves as the training data quantities grew larger. On the external data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and 0.810 accuracy for each. These results indicate that in both the internal and external datasets, the DenseNet-121 model attains high accuracy for the task of classifying 8 body MRI series types. </p>
<blockquote>
<p>å¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰æ£€æŸ¥é€šè¿‡å„ç§ä¸åŒçš„æˆåƒåè®®è·å¾—ä¸åŒçš„ç³»åˆ—ç±»å‹ã€‚è¿™äº›åºåˆ—çš„DICOMæ ‡å¤´ç”±äºåè®®çš„å¤šæ ·æ€§å’Œå¶å°”çš„æŠ€æœ¯äººå‘˜é”™è¯¯ï¼Œå¾€å¾€å­˜åœ¨é”™è¯¯çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†ç±»æ¨¡å‹ï¼Œå¯¹8ç§ä¸åŒçš„èº«ä½“mpMRIç³»åˆ—ç±»å‹è¿›è¡Œåˆ†ç±»ï¼Œä»¥ä¾¿æ”¾å°„ç§‘åŒ»ç”Ÿèƒ½å¤Ÿé«˜æ•ˆåœ°é˜…è¯»æ£€æŸ¥ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªä¸åŒæœºæ„çš„mpMRIæ•°æ®ï¼Œè®­ç»ƒäº†åŸºäºResNetã€EfficientNetå’ŒDenseNetçš„å¤šä¸ªæ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ï¼Œå¯¹8ç§ä¸åŒçš„MRIç³»åˆ—è¿›è¡Œåˆ†ç±»ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ã€‚ç„¶åï¼Œç¡®å®šäº†è¡¨ç°æœ€ä½³çš„åˆ†ç±»å™¨ï¼Œå¹¶ç ”ç©¶äº†å…¶åœ¨ä¸åŒè®­ç»ƒæ•°æ®é‡è®¾ç½®ä¸‹çš„åˆ†ç±»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒåˆ†å¸ƒå¤–çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è€Œä¸”ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æ¥è‡ªä¸¤ç§è®­ç»ƒç­–ç•¥çš„ä¸åŒæ‰«æä»ªè·å¾—çš„mpMRIæ£€æŸ¥è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†æµ‹è¯•ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15182v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†ç±»æ¨¡å‹ï¼Œç”¨äºå¯¹å¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰çš„8ç§ç³»åˆ—ç±»å‹è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶é€šè¿‡ä½¿ç”¨ä¸åŒæœºæ„çš„mpMRIæ•°æ®è®­ç»ƒäº†å¤šç§æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ï¼Œå¦‚ResNetã€EfficientNetå’ŒDenseNetï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ã€‚æœ€ç»ˆå‘ç°DenseNet-121æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œåœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ä¸Šå‡è¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰çš„DICOMæ ‡å¤´ç”±äºåè®®å¤šæ ·æ€§å’ŒæŠ€æœ¯äººå‘˜é”™è¯¯ï¼Œå¸¸æœ‰é”™è¯¯çš„ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹èƒ½å¤Ÿè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿæ›´æœ‰æ•ˆåœ°é˜…è¯»æ£€æŸ¥ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†ResNetã€EfficientNetå’ŒDenseNetç­‰æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>DenseNet-121æ¨¡å‹åœ¨åˆ†ç±»8ç§ä¸åŒMRIç³»åˆ—çš„ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼ŒF1å¾—åˆ†å’Œå‡†ç¡®åº¦åˆ†åˆ«ä¸º0.966å’Œ0.972ã€‚</li>
<li>å½“ä½¿ç”¨è¶…è¿‡729é¡¹ç ”ç©¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®æ€§è¶…è¿‡0.95ï¼Œå¹¶ä¸”éšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šæœ‰æ‰€æé«˜ã€‚</li>
<li>åœ¨å¤–éƒ¨æ•°æ®é›†DLDSå’ŒCPTAC-UCECä¸Šï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§åˆ†åˆ«ä¸º0.872å’Œ0.810ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c51be3d97aed59984d90c87fceb46686.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d81a56af2fb9a3eb21e85009e5d8b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89daf5d3ef24669c570018fbc896b8a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f90d486bd7ce1f22b94c76bafd28c784.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4e65f922f2244d2fe98c7ed7761221e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Echo-DND-A-dual-noise-diffusion-model-for-robust-and-precise-left-ventricle-segmentation-in-echocardiography"><a href="#Echo-DND-A-dual-noise-diffusion-model-for-robust-and-precise-left-ventricle-segmentation-in-echocardiography" class="headerlink" title="Echo-DND: A dual noise diffusion model for robust and precise left   ventricle segmentation in echocardiography"></a>Echo-DND: A dual noise diffusion model for robust and precise left   ventricle segmentation in echocardiography</h2><p><strong>Authors:Abdur Rahman, Keerthiveena Balraj, Manojkumar Ramteke, Anurag Singh Rathore</strong></p>
<p>Recent advancements in diffusion probabilistic models (DPMs) have revolutionized image processing, demonstrating significant potential in medical applications. Accurate segmentation of the left ventricle (LV) in echocardiograms is crucial for diagnostic procedures and necessary treatments. However, ultrasound images are notoriously noisy with low contrast and ambiguous LV boundaries, thereby complicating the segmentation process. To address these challenges, this paper introduces Echo-DND, a novel dual-noise diffusion model specifically designed for this task. Echo-DND leverages a unique combination of Gaussian and Bernoulli noises. It also incorporates a multi-scale fusion conditioning module to improve segmentation precision. Furthermore, it utilizes spatial coherence calibration to maintain spatial integrity in segmentation masks. The modelâ€™s performance was rigorously validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations demonstrate that the proposed framework outperforms existing SOTA models. It achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively. The proposed Echo-DND model establishes a new standard in echocardiogram segmentation, and its architecture holds promise for broader applicability in other medical imaging tasks, potentially improving diagnostic accuracy across various medical domains. Project page: <a target="_blank" rel="noopener" href="https://abdur75648.github.io/Echo-DND">https://abdur75648.github.io/Echo-DND</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰çš„æœ€æ–°è¿›å±•åœ¨å›¾åƒå¤„ç†é¢†åŸŸå¼•èµ·äº†é©å‘½æ€§çš„å˜é©ï¼Œå¹¶åœ¨åŒ»ç–—åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚åœ¨è¶…å£°å¿ƒåŠ¨å›¾ä¸­å‡†ç¡®åˆ†å‰²å·¦å¿ƒå®¤ï¼ˆLVï¼‰å¯¹äºè¯Šæ–­ç¨‹åºå’Œå¿…è¦æ²»ç–—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¶…å£°å›¾åƒä»¥å™ªå£°å¤§ã€å¯¹æ¯”åº¦ä½ä»¥åŠå·¦å¿ƒå®¤è¾¹ç•Œæ¨¡ç³Šè€Œè‘—ç§°ï¼Œä»è€Œå¢åŠ äº†åˆ†å‰²çš„éš¾åº¦ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†Echo-DNDï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºæ­¤ä»»åŠ¡çš„æ–°å‹åŒå™ªå£°æ‰©æ•£æ¨¡å‹ã€‚Echo-DNDç»“åˆäº†é«˜æ–¯å™ªå£°å’Œä¼¯åŠªåˆ©å™ªå£°çš„ç‹¬ç‰¹ç»„åˆã€‚å®ƒè¿˜é‡‡ç”¨å¤šå°ºåº¦èåˆæ¡ä»¶æ¨¡å—æ¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç©ºé—´ä¸€è‡´æ€§æ ¡å‡†æ¥ä¿æŒåˆ†å‰²æ©è†œçš„ç©ºé—´å®Œæ•´æ€§ã€‚è¯¥æ¨¡å‹åœ¨CAMUSå’ŒEchoNet-Dynamicæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä¼˜äºç°æœ‰çš„å…ˆè¿›æŠ€æœ¯æ¨¡å‹ã€‚å®ƒåœ¨è¿™äº›æ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº†é«˜è¾¾0.962å’Œ0.939çš„Diceå¾—åˆ†ã€‚æ‰€æå‡ºçš„Echo-DNDæ¨¡å‹åœ¨è¶…å£°å¿ƒåŠ¨å›¾åˆ†å‰²æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œå…¶æ¶æ„åœ¨å…¶ä»–åŒ»å­¦å½±åƒä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›åœ¨å¤šä¸ªåŒ»ç–—é¢†åŸŸæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://abdur75648.github.io/Echo-DND">https://abdur75648.github.io/Echo-DND</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15166v1">PDF</a> Version of record published in Discover Applied Sciences (Springer   Nature). The definitive article is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/s42452-025-07055-5">https://doi.org/10.1007/s42452-025-07055-5</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰çš„æœ€æ–°è¿›å±•åŠå…¶åœ¨å›¾åƒå¤„ç†é¢†åŸŸçš„é©å‘½æ€§ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚é’ˆå¯¹è¶…å£°å›¾åƒä¸­å·¦å¿ƒå®¤ï¼ˆLVï¼‰åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŒå™ªå£°æ‰©æ•£æ¨¡å‹Echo-DNDã€‚è¯¥æ¨¡å‹ç»“åˆäº†é«˜æ–¯å™ªå£°å’Œä¼¯åŠªåˆ©å™ªå£°ï¼Œå¹¶é‡‡ç”¨äº†å¤šå°ºåº¦èåˆæ¡ä»¶æ¨¡å—ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨ç©ºé—´ä¸€è‡´æ€§æ ¡å‡†æ¥ä¿æŒåˆ†å‰²æ©è†œçš„ç©ºé—´å®Œæ•´æ€§ã€‚åœ¨CAMUSå’ŒEchoNet-Dynamicæ•°æ®é›†ä¸Šçš„ä¸¥æ ¼éªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå®ç°äº†é«˜Diceå¾—åˆ†ã€‚Echo-DNDæ¨¡å‹ä¸ºè¶…å£°å¿ƒåŠ¨å›¾åˆ†å‰²å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œå…¶æ¶æ„åœ¨å…¶ä»–åŒ»å­¦æˆåƒä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›æé«˜ä¸åŒåŒ»å­¦é¢†åŸŸçš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰åœ¨å›¾åƒå¤„ç†é¢†åŸŸå–å¾—é‡å¤§è¿›å±•ï¼Œå°¤å…¶åœ¨åŒ»å­¦åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>Echo-DNDæ˜¯ä¸€ç§æ–°å‹åŒå™ªå£°æ‰©æ•£æ¨¡å‹ï¼Œä¸“ä¸ºå·¦å¿ƒå®¤ï¼ˆLVï¼‰è¶…å£°å›¾åƒåˆ†å‰²è®¾è®¡ã€‚</li>
<li>Echo-DNDç»“åˆäº†é«˜æ–¯å™ªå£°å’Œä¼¯åŠªåˆ©å™ªå£°ï¼Œå¹¶é‡‡ç”¨å¤šå°ºåº¦èåˆæ¡ä»¶æ¨¡å—æé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨ç©ºé—´ä¸€è‡´æ€§æ ¡å‡†ç»´æŒåˆ†å‰²æ©è†œçš„ç©ºé—´å®Œæ•´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒEcho-DNDæ¨¡å‹çš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>Echo-DNDå®ç°äº†é«˜Diceå¾—åˆ†ï¼Œä¸ºè¶…å£°å¿ƒåŠ¨å›¾åˆ†å‰²æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17db04e1312db53ce696dcaba1133c7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05460226eefab7c60c71d54336c50ff4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c28bc320d0c206e3d69d3846b6a6235f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰çš„å‡ºç°ä¸ºå°‘æ•°åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼ŒåŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLVMsçš„æ— è®­ç»ƒå°‘æ•°æ ·æœ¬æ–¹æ³•SynPoï¼ˆä»¥SAMä¸ºä¾‹ï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæé«˜è´Ÿæç¤ºçš„è´¨é‡ã€‚ä¸ºäº†åœ¨æ›´å¯é çš„ç½®ä¿¡åº¦åœ°å›¾ä¸­é€‰æ‹©ç‚¹æç¤ºï¼Œæˆ‘ä»¬ç»“åˆäº†DINOv2å’ŒSAMçš„ä¼˜åŠ¿ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç½®ä¿¡åº¦æ˜ å°„ååŒæ¨¡å—ã€‚åŸºäºç½®ä¿¡åº¦åœ°å›¾ï¼Œæˆ‘ä»¬é€‰æ‹©å‰kä¸ªåƒç´ ä½œä¸ºæ­£ç‚¹é›†ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€‰æ‹©è´Ÿç‚¹é›†ï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªé›†åˆåˆ†åˆ«è¿›è¡Œç‹¬ç«‹çš„K-meansèšç±»ã€‚ç„¶åï¼Œè¿™äº›é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œä¾›SAMè·å¾—åˆ†å‰²ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„å°‘æ•°æ ·æœ¬æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰çš„æ— è®­ç»ƒåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•å­˜åœ¨åˆ©ç”¨è´Ÿæç¤ºä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºSynPoçš„æ— è®­ç»ƒå°‘æ•°é•œå¤´æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ”¹è¿›è´Ÿæç¤ºçš„è´¨é‡æ¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼ˆConfidence Map Synergy Moduleï¼‰ï¼Œç»“åˆäº†DINOv2å’ŒSAMçš„ä¼˜ç‚¹æ¥ç”Ÿæˆç½®ä¿¡å›¾ï¼Œæ ¹æ®ç½®ä¿¡å›¾é€‰æ‹©å¯é çš„ç‚¹æç¤ºã€‚é€šè¿‡é«˜æ–¯åˆ†å¸ƒå’Œç‹¬ç«‹K-å‡å€¼èšç±»ï¼Œæˆ‘ä»¬é€‰æ‹©äº†æ­£ç‚¹é›†å’Œè´Ÿç‚¹é›†ä½œä¸ºé«˜è´¨é‡æç¤ºç”¨äºSAMè¿›è¡Œåˆ†å‰²ã€‚å®éªŒè¯æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„æœ€å…ˆè¿›å°‘æ•°é•œå¤´æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„å°‘æ•°é•œå¤´å­¦ä¹ ä¸­æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>ç°æœ‰åŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>SynPoæ–¹æ³•é€šè¿‡æ”¹è¿›è´Ÿæç¤ºçš„è´¨é‡æ¥æå‡æ€§èƒ½ã€‚</li>
<li>è®¾è®¡äº†ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼ˆConfidence Map Synergy Moduleï¼‰ï¼Œç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ç”Ÿæˆç½®ä¿¡å›¾ã€‚</li>
<li>æ ¹æ®ç½®ä¿¡å›¾é€‰æ‹©å¯é çš„ç‚¹æç¤ºï¼Œé€šè¿‡é«˜æ–¯åˆ†å¸ƒå’Œç‹¬ç«‹K-å‡å€¼èšç±»é€‰æ‹©æ­£ç‚¹é›†å’Œè´Ÿç‚¹é›†ã€‚</li>
<li>é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œç”¨äºSAMè¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3cbb2811ca359b2646838708d0194be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Frequency-Calibrated-Membership-Inference-Attacks-on-Medical-Image-Diffusion-Models"><a href="#Frequency-Calibrated-Membership-Inference-Attacks-on-Medical-Image-Diffusion-Models" class="headerlink" title="Frequency-Calibrated Membership Inference Attacks on Medical Image   Diffusion Models"></a>Frequency-Calibrated Membership Inference Attacks on Medical Image   Diffusion Models</h2><p><strong>Authors:Xinkai Zhao, Yuta Tokuoka, Junichiro Iwasawa, Keita Oda</strong></p>
<p>The increasing use of diffusion models for image generation, especially in sensitive areas like medical imaging, has raised significant privacy concerns. Membership Inference Attack (MIA) has emerged as a potential approach to determine if a specific image was used to train a diffusion model, thus quantifying privacy risks. Existing MIA methods often rely on diffusion reconstruction errors, where member images are expected to have lower reconstruction errors than non-member images. However, applying these methods directly to medical images faces challenges. Reconstruction error is influenced by inherent image difficulty, and diffusion models struggle with high-frequency detail reconstruction. To address these issues, we propose a Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical image diffusion models. By focusing on reconstruction errors within a specific mid-frequency range and excluding both high-frequency (difficult to reconstruct) and low-frequency (less informative) regions, our frequency-selective approach mitigates the confounding factor of inherent image difficulty. Specifically, we analyze the reverse diffusion process, obtain the mid-frequency reconstruction error, and compute the structural similarity index score between the reconstructed and original images. Membership is determined by comparing this score to a threshold. Experiments on several medical image datasets demonstrate that our FCRE method outperforms existing MIA methods. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒç­‰æ•æ„Ÿé¢†åŸŸï¼Œå¼•å‘äº†äººä»¬å¯¹éšç§çš„é‡å¤§å…³æ³¨ã€‚æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰å·²ç»æˆä¸ºä¸€ç§æ½œåœ¨çš„æ–¹æ³•ï¼Œç”¨äºç¡®å®šç‰¹å®šå›¾åƒæ˜¯å¦ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œé‡åŒ–éšç§é£é™©ã€‚ç°æœ‰çš„MIAæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰©æ•£é‡å»ºè¯¯å·®ï¼Œå…¶ä¸­æˆå‘˜å›¾åƒçš„é‡å»ºè¯¯å·®é¢„æœŸä¼šä½äºéæˆå‘˜å›¾åƒã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒé¢ä¸´æŒ‘æˆ˜ã€‚é‡å»ºè¯¯å·®å—åˆ°å›¾åƒå›ºæœ‰éš¾åº¦çš„å½±å“ï¼Œæ‰©æ•£æ¨¡å‹åœ¨é‡å»ºé«˜é¢‘ç»†èŠ‚æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹åŒ»å­¦å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šçš„MIAæå‡ºäº†ä¸€ç§é¢‘ç‡æ ¡å‡†é‡å»ºè¯¯å·®ï¼ˆFCREï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¢‘ç‡é€‰æ‹©æ–¹æ³•ä¸“æ³¨äºç‰¹å®šä¸­é¢‘èŒƒå›´å†…çš„é‡å»ºè¯¯å·®ï¼Œå¹¶æ’é™¤äº†é«˜é¢‘ï¼ˆéš¾ä»¥é‡å»ºï¼‰å’Œä½é¢‘ï¼ˆä¿¡æ¯è¾ƒå°‘ï¼‰åŒºåŸŸï¼Œä»è€Œå‡è½»äº†å›ºæœ‰å›¾åƒéš¾åº¦è¿™ä¸€æ··æ·†å› ç´ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ†æäº†åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè·å¾—äº†ä¸­é¢‘é‡å»ºè¯¯å·®ï¼Œå¹¶è®¡ç®—äº†é‡æ„å›¾åƒå’ŒåŸå§‹å›¾åƒä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åˆ†æ•°ã€‚æˆå‘˜èº«ä»½çš„ç¡®å®šæ˜¯é€šè¿‡å°†æ­¤åˆ†æ•°ä¸é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒå¾—å‡ºçš„ã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FCREæ–¹æ³•ä¼˜äºç°æœ‰çš„MIAæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14919v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒæ‰©æ•£æ¨¡å‹çš„æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œæå‡ºä¸€ç§é¢‘ç‡æ ¡å‡†é‡å»ºè¯¯å·®ï¼ˆFCREï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•å…³æ³¨ç‰¹å®šä¸­é¢‘èŒƒå›´å†…çš„é‡å»ºè¯¯å·®ï¼Œæ’é™¤é«˜é¢‘ï¼ˆéš¾ä»¥é‡å»ºï¼‰å’Œä½é¢‘ï¼ˆä¿¡æ¯è¾ƒå°‘ï¼‰åŒºåŸŸï¼Œä»¥å‡è½»å›¾åƒæœ¬èº«éš¾åº¦å¯¹éšç§æ³„éœ²çš„å½±å“ã€‚é€šè¿‡å¯¹æ¯”é‡å»ºå›¾åƒä¸åŸå§‹å›¾åƒçš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°å¾—åˆ†ï¼Œç¡®å®šæˆå‘˜èº«ä»½ã€‚å®éªŒè¯æ˜ï¼ŒFCREæ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰MIAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†éšç§æ‹…å¿§ã€‚</li>
<li>æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰å¯ç”¨æ¥ç¡®å®šç‰¹å®šå›¾åƒæ˜¯å¦ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œè¯„ä¼°éšç§é£é™©ã€‚</li>
<li>ç°æœ‰MIAæ–¹æ³•ä¸»è¦åŸºäºæ‰©æ•£é‡å»ºè¯¯å·®ï¼Œä½†ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åŒ»å­¦å›¾åƒæœ¬èº«çš„éš¾åº¦å½±å“é‡å»ºè¯¯å·®ï¼Œæ‰©æ•£æ¨¡å‹åœ¨é«˜é¢‘ç»†èŠ‚é‡å»ºä¸Šè¡¨ç°æŒ£æ‰ã€‚</li>
<li>æå‡ºFCREæ–¹æ³•ï¼Œå…³æ³¨ä¸­é¢‘èŒƒå›´çš„é‡å»ºè¯¯å·®ï¼Œæ’é™¤é«˜é¢‘å’Œä½é¢‘åŒºåŸŸï¼Œä»¥å‡è½»å›¾åƒéš¾åº¦å¯¹éšç§æ³„éœ²çš„å¹²æ‰°ã€‚</li>
<li>FCREæ–¹æ³•é€šè¿‡å¯¹æ¯”é‡å»ºå›¾åƒä¸åŸå§‹å›¾åƒçš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°å¾—åˆ†ï¼Œç¡®å®šæˆå‘˜èº«ä»½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cca4a6861729ae0def3346306ce09652.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23f846394c41cf6bc2721a24052a726b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Self-consistent-population-synthesis-of-AGN-from-observational-constraints-in-the-X-rays"><a href="#Self-consistent-population-synthesis-of-AGN-from-observational-constraints-in-the-X-rays" class="headerlink" title="Self-consistent population synthesis of AGN from observational   constraints in the X-rays"></a>Self-consistent population synthesis of AGN from observational   constraints in the X-rays</h2><p><strong>Authors:D. Gerolymatou, S. Paltani, C. Ricci, M. Regamey</strong></p>
<p>The cosmic X-ray background (CXB) is produced by the emission of unresolved active galactic nuclei (AGN), thus providing key information about the properties of the primary and reprocessed X-ray emission components of the AGN population. Equally important, studies of individual sources provide additional constraints on the properties of AGN, such as their luminosity and obscuration. Until now, these constraints have not been self-consistently addressed by intrinsically linking emission, absorption, and reflection. Here we perform numerical simulations with the ray-tracing code, RefleX, which allows us to self-consistently model the X-ray emission of AGN with flexible geometries for the circumnuclear medium. Using the RefleX-simulated emission of an AGN population, we attempt to simultaneously reproduce the CXB and absorption properties measured in the X-rays, namely the observed fraction of $N_{\mathrm{H}}$ in bins of log($N_{\mathrm{H}}$) and the fraction of absorbed AGN, including their redshift and luminosity evolution. We sample an intrinsic X-ray luminosity function and construct gradually more complex physically motivated geometrical models. We examine how well each model can match all observational constraints using a simulation-based inference (SBI) approach. We find that, while the simple unification model can reproduce the CXB, a luminosity dependent dusty torus is needed to reproduce the absorption properties. When adding an accretion disc, the model best matches all constraints simultaneously. Our synthetic population is able to reproduce the dependence of the covering factor on luminosity, the AGN number counts from several surveys, and the observed correlation between reflection and obscuration. Finally, we derive an intrinsic Compton-thick fraction of 21$\pm$7%, consistent with local observations. </p>
<blockquote>
<p>å®‡å®™Xå°„çº¿èƒŒæ™¯ï¼ˆCXBï¼‰æ˜¯ç”±æœªè§£å†³çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰çš„å‘å°„äº§ç”Ÿçš„ï¼Œä»è€Œä¸ºå…³äºAGNç¾¤ä½“ä¸»è¦å’Œå†å¤„ç†Xå°„çº¿å‘å°„æˆåˆ†çš„å±æ€§æä¾›äº†å…³é”®ä¿¡æ¯ã€‚åŒæ ·é‡è¦çš„æ˜¯ï¼Œå¯¹ä¸ªäººæºçš„ç ”ç©¶å¯¹AGNsçš„å±æ€§æä¾›äº†é¢å¤–çš„é™åˆ¶ï¼Œä¾‹å¦‚å®ƒä»¬çš„äº®åº¦å’Œé®è”½æ€§ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œè¿™äº›çº¦æŸå°šæœªé€šè¿‡å†…åœ¨åœ°è”ç³»å‘å°„ã€å¸æ”¶å’Œåå°„æ¥è‡ªæˆ‘ä¸€è‡´åœ°è§£å†³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å…‰çº¿è¿½è¸ªä»£ç RefleXè¿›è¡Œæ•°å€¼æ¨¡æ‹Ÿï¼Œå®ƒå…è®¸æˆ‘ä»¬æ ¹æ®æ ¸å‘¨ä»‹è´¨çš„çµæ´»å‡ ä½•å½¢çŠ¶æ¥è‡ªæˆ‘ä¸€è‡´åœ°æ¨¡æ‹ŸAGNsçš„Xå°„çº¿å‘å°„ã€‚æˆ‘ä»¬ä½¿ç”¨RefleXæ¨¡æ‹Ÿçš„AGNsç¾¤ä½“çš„å‘å°„ï¼Œè¯•å›¾åŒæ—¶å¤åˆ¶åœ¨Xå°„çº¿ä¸‹æµ‹é‡çš„CXBå’Œå±æ€§ï¼Œå³è§‚å¯Ÿåˆ°çš„$N_{\mathrm{H}}$åˆ†æ•°åœ¨log($N_{\mathrm{H}}$)çš„å®¹å™¨ä¸­å’Œå¸æ”¶çš„AGNsçš„åˆ†æ•°ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„çº¢ç§»å’Œäº®åº¦æ¼”åŒ–ã€‚æˆ‘ä»¬å¯¹å†…åœ¨çš„Xå°„çº¿å…‰åº¦å‡½æ•°è¿›è¡ŒæŠ½æ ·ï¼Œå¹¶æ„å»ºé€æ¸æ›´å¤æ‚çš„ç‰©ç†é©±åŠ¨å‡ ä½•æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡æ‹Ÿçš„æ¨æ–­ï¼ˆSBIï¼‰æ–¹æ³•ï¼Œæ£€æŸ¥æ¯ä¸ªæ¨¡å‹æ˜¯å¦ç¬¦åˆæ‰€æœ‰è§‚æµ‹çº¦æŸã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶ç®€å•çš„ç»Ÿä¸€æ¨¡å‹å¯ä»¥å¤åˆ¶CXBï¼Œä½†éœ€è¦ä¾èµ–äºäº®åº¦çš„å°˜åŸƒç¯æ¥å¤åˆ¶å¸æ”¶ç‰¹æ€§ã€‚å½“åŠ å…¥å¸ç§¯ç›˜æ—¶ï¼Œè¯¥æ¨¡å‹æœ€èƒ½åŒæ—¶æ»¡è¶³æ‰€æœ‰çº¦æŸã€‚æˆ‘ä»¬çš„åˆæˆç¾¤ä½“èƒ½å¤Ÿå†ç°è¦†ç›–å› å­ä¸äº®åº¦ä¹‹é—´çš„ä¾èµ–æ€§ã€æ¥è‡ªå¤šä¸ªè°ƒæŸ¥çš„AGNsæ•°é‡è®¡æ•°ä»¥åŠåå°„å’Œé®è”½ä¹‹é—´çš„è§‚å¯Ÿåˆ°çš„ç›¸å…³æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¾—å‡ºå†…åœ¨çš„åº·æ™®é¡¿åšè¾¾21Â±7%ï¼Œä¸å½“åœ°è§‚å¯Ÿç»“æœä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14885v1">PDF</a> Accepted for publication in A&amp;A</p>
<p><strong>Summary</strong><br>     æ­¤æ–‡æœ¬æ¢è®¨äº†å®‡å®™Xå°„çº¿èƒŒæ™¯ï¼ˆCXBï¼‰ç”±æœªè§£å†³çš„æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å‘å°„äº§ç”Ÿçš„é—®é¢˜ã€‚é€šè¿‡æ•°å€¼æ¨¡æ‹Ÿå’ŒRefleXå°„çº¿è¿½è¸ªä»£ç ï¼Œå¯¹AGNçš„Xå°„çº¿å‘å°„è¿›è¡Œè‡ªæ´½å»ºæ¨¡ï¼Œå¹¶å°è¯•åŒæ—¶å†ç°CXBå’Œåœ¨Xå°„çº¿ä¸‹æµ‹å¾—çš„å¸æ”¶ç‰¹æ€§ã€‚ç ”ç©¶å‘ç°ç®€å•ç»Ÿä¸€æ¨¡å‹èƒ½å†ç°CXBï¼Œä½†è¦å†ç°å¸æ”¶ç‰¹æ€§éœ€è¦å…‰åº¦ä¾èµ–çš„å°˜åŸƒç¯ã€‚åŠ å…¥æ˜Ÿå‘¨ç›˜åï¼Œæ¨¡å‹æœ€èƒ½åŒæ—¶æ»¡è¶³æ‰€æœ‰çº¦æŸã€‚è¯¥åˆæˆäººå£èƒ½å¤Ÿå†ç°è¦†ç›–å› å­å¯¹å…‰åº¦çš„ä¾èµ–æ€§ã€å¤šä¸ªè°ƒæŸ¥çš„æ´»è·ƒæ˜Ÿç³»æ ¸è®¡æ•°ä»¥åŠåå°„å’Œé®è”½ä¹‹é—´çš„è§‚æµ‹ç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®‡å®™Xå°„çº¿èƒŒæ™¯ï¼ˆCXBï¼‰ç”±æœªè§£å†³çš„æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å‘å°„äº§ç”Ÿï¼Œæä¾›äº†å…³äºAGNä¸»è¦å’Œå†å¤„ç†Xå°„çº¿å‘å°„æˆåˆ†çš„å…³é”®ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡RefleXå°„çº¿è¿½è¸ªä»£ç è¿›è¡Œæ•°å€¼æ¨¡æ‹Ÿï¼Œèƒ½å¤Ÿè‡ªæ´½åœ°å»ºæ¨¡å…·æœ‰å¯å˜å‡ ä½•ç»“æ„çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNsï¼‰çš„Xå°„çº¿å‘å°„ã€‚</li>
<li>ç®€å•ç»Ÿä¸€æ¨¡å‹å¯ä»¥å†ç°CXBï¼Œä½†è¦å‡†ç¡®æè¿°å¸æ”¶ç‰¹æ€§ï¼Œéœ€è¦å…‰åº¦ä¾èµ–çš„å°˜åŸƒç¯æ¨¡å‹ã€‚</li>
<li>åŠ å…¥æ˜Ÿå‘¨ç›˜åï¼Œæ¨¡å‹æœ€ä½³åŒ¹é…æ‰€æœ‰è§‚æµ‹çº¦æŸï¼Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘å‘å°„ã€å¸æ”¶å’Œåå°„ã€‚</li>
<li>åˆæˆäººå£æ¨¡å‹èƒ½å¤ŸæˆåŠŸæ¨¡æ‹Ÿè¦†ç›–å› å­ä¸å…‰åº¦ä¹‹é—´çš„ä¾èµ–æ€§ã€å¤šä¸ªè°ƒæŸ¥çš„æ´»è·ƒæ˜Ÿç³»æ ¸è®¡æ•°ä»¥åŠåå°„ä¸é®è”½ä¹‹é—´çš„è§‚æµ‹å…³ç³»ã€‚</li>
<li>ç ”ç©¶å‘ç°å­˜åœ¨å†…åœ¨åº·æ™®é¡¿åšè¾¾21Â±7%çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼Œè¿™ä¸æœ¬åœ°è§‚æµ‹ç»“æœä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df7d3d940731df777e775c441309c38a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8181ae82d0a0b1d1ec9a77b4f15cc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b74b82714b0f4d9553a85012621fb16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d8ee44d3ef467d7610c9648e54ec9e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83c9ff2491a045f38b369e89d4a45664.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f47f2de8119e7168fd8cdd79b316fc53.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adapting-Lightweight-Vision-Language-Models-for-Radiological-Visual-Question-Answering"><a href="#Adapting-Lightweight-Vision-Language-Models-for-Radiological-Visual-Question-Answering" class="headerlink" title="Adapting Lightweight Vision Language Models for Radiological Visual   Question Answering"></a>Adapting Lightweight Vision Language Models for Radiological Visual   Question Answering</h2><p><strong>Authors:Aditya Shourya, Michel Dumontier, Chang Sun</strong></p>
<p>Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€ç³»ç»Ÿçš„è¿›æ­¥æé«˜äº†æ”¾å°„å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨æ¨¡å‹å‘å±•çš„æ¯ä¸ªé˜¶æ®µä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼šä¸“å®¶æ ‡æ³¨çš„å›¾åƒæ•°é‡æœ‰é™ï¼Œé˜»ç¢äº†å¤§è§„æ¨¡æ•°æ®é‡‡è´­ï¼›æ”¾å°„å›¾åƒçš„å¤æ‚å’Œç»†å¾®æ¨¡å¼ä½¿å»ºæ¨¡å˜å¾—å›ºæœ‰åœ°å›°éš¾ï¼›ç¼ºä¹è¯„ä¼°åŠªåŠ›ä½¿å¾—éš¾ä»¥ç¡®å®šæ¨¡å‹å¯èƒ½å‡ºç°é—®é¢˜çš„æƒ…å†µã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªè½»é‡çº§çš„3Bå‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œç”¨äºæ”¾å°„å­¦VQAï¼Œç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨ç²¾é€‰æ•°æ®é€‚å½“è°ƒæ•´æ—¶ï¼Œå°å‹æ¨¡å‹å¯ä»¥åœ¨å¼€æ”¾å’Œå°é—­é—®é¢˜ä¸­éƒ½å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»æµå®æƒ çš„è®­ç»ƒæµç¨‹ï¼Œä»åˆæˆé—®é¢˜ç­”æ¡ˆå¯¹ç”Ÿæˆåˆ°åœ¨ç‰¹å®šæ”¾å°„å­¦é¢†åŸŸç›®æ ‡æ•°æ®é›†ï¼ˆä¾‹å¦‚ROCO v2.0ã€MedPix v2.0ï¼‰ä¸Šçš„å¤šé˜¶æ®µå¾®è°ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ¨¡å‹è§„æ¨¡åªæœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚LLaVA-Medï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œä½†è€ƒè™‘åˆ°å…¶è¾ƒå°çš„å‚æ•°è§„æ¨¡å’Œæœ‰é™çš„è®­ç»ƒæ•°æ®é‡ï¼Œå…¶æ€§èƒ½è¿˜æ˜¯å¾ˆæœ‰å¸Œæœ›çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºæ˜¾è‘—æ€§çš„è¯Šæ–­å·¥å…·ï¼Œä½¿é¢†åŸŸä¸“å®¶èƒ½å¤Ÿæ£€æŸ¥VQAæ¨¡å‹æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ˜¾è‘—æ€§åˆ†æç¡®å®šä¸è‰¯çš„å¤±è´¥æ¨¡å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14451v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè§†è§‰è¯­è¨€ç³»ç»Ÿçš„è¿›æ­¥æé«˜äº†æ”¾å°„å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä½†ä»å­˜åœ¨æ•°æ®è·å–ã€å»ºæ¨¡éš¾åº¦å’Œè¯„ä¼°å›°éš¾ç­‰æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¾®è°ƒäº†ä¸€ä¸ªè½»é‡çº§çš„3Bå‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ”¾å°„å­¦VQAä»»åŠ¡ï¼Œè¯æ˜é€‚å½“è°ƒä¼˜å’Œç²¾é€‰æ•°æ®å¯ä½¿å°æ¨¡å‹åœ¨å¼€æ”¾å’Œå°é—­é—®é¢˜ä¸­éƒ½è¡¨ç°ç¨³å¥ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»æµé«˜æ•ˆçš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ä»åˆæˆé—®ç­”å¯¹ç”Ÿæˆåˆ°ç‰¹å®šæ”¾å°„å­¦é¢†åŸŸæ•°æ®é›†çš„å¤šé˜¶æ®µå¾®è°ƒã€‚å°½ç®¡æ¨¡å‹è§„æ¨¡è¿œå°äºå½“å‰å…ˆè¿›æŠ€æœ¯ï¼Œä½†è¡¨ç°ä»ç„¶ä»¤äººé¼“èˆã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„åŸºäºæ˜¾è‘—æ€§çš„è¯Šæ–­å·¥å…·ï¼Œå¸®åŠ©é¢†åŸŸä¸“å®¶æ£€æŸ¥VQAæ¨¡å‹æ€§èƒ½å¹¶è¯†åˆ«ä¸è‰¯çš„å¤±æ•ˆæ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€ç³»ç»Ÿçš„è¿›æ­¥æé«˜äº†æ”¾å°„å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ•°æ®è·å–ã€å»ºæ¨¡éš¾åº¦å’Œè¯„ä¼°å›°éš¾ä»æ˜¯VQAæ¨¡å‹å‘å±•çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¾®è°ƒè½»é‡çº§æ¨¡å‹å¹¶é…ä»¥ç²¾é€‰æ•°æ®ï¼Œå¯ä»¥åœ¨æ”¾å°„å­¦VQAä»»åŠ¡ä¸­å–å¾—ç¨³å¥è¡¨ç°ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»æµé«˜æ•ˆçš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬åˆæˆé—®ç­”å¯¹ç”Ÿæˆå’Œå¤šé˜¶æ®µå¾®è°ƒã€‚</li>
<li>è½»é‡çº§æ¨¡å‹åœ¨è§„æ¨¡è¿œå°äºå½“å‰å…ˆè¿›æŠ€æœ¯çš„æƒ…å†µä¸‹ä»è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„åŸºäºæ˜¾è‘—æ€§çš„è¯Šæ–­å·¥å…·ï¼Œä»¥æ£€æŸ¥VQAæ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-561be8b97427a2b87a73e98e2f939be0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae1816da7dece50772954f9454fa7dc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c85235a8e8c6bcd6760de3b08ee22af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6eecbf3541af992ed76f2158abe4804.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a1ced51fda28a9ea5fa9b3e9b655cb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BRISC-Annotated-Dataset-for-Brain-Tumor-Segmentation-and-Classification-with-Swin-HAFNet"><a href="#BRISC-Annotated-Dataset-for-Brain-Tumor-Segmentation-and-Classification-with-Swin-HAFNet" class="headerlink" title="BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification   with Swin-HAFNet"></a>BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification   with Swin-HAFNet</h2><p><strong>Authors:Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh</strong></p>
<p>Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, largely due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a new curated MRI dataset designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians, spanning three major tumor types-glioma, meningioma, and pituitary-as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. Our method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. Importantly, this study serves primarily as an introduction to the dataset, establishing foundational benchmarks for future research. We envision this dataset as a valuable resource for advancing machine learning applications in neuro-oncology, supporting both academic research and clinical decision-support development. datasetlink: <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/briscdataset/brisc2025/">https://www.kaggle.com/datasets/briscdataset/brisc2025/</a> </p>
<blockquote>
<p>ä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å‡†ç¡®åˆ†å‰²å’Œåˆ†ç±»è„‘è‚¿ç˜¤ä»æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡ã€å‡è¡¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä¸“ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è®¾è®¡çš„æ–°MRIæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ç”±è®¤è¯æ”¾å°„å­¦å®¶å’ŒåŒ»ç”Ÿæ³¨é‡Šçš„6000ä¸ªå¯¹æ¯”å¢å¼ºçš„T1åŠ æƒMRIæ‰«æï¼Œæ¶µç›–ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹ï¼šèƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼Œä»¥åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚æ¯ä¸ªæ ·æœ¬éƒ½åŒ…æ‹¬é«˜åˆ†è¾¨ç‡æ ‡ç­¾ï¼Œå¹¶æ ¹æ®è½´å‘ã€çŸ¢çŠ¶é¢å’Œå† çŠ¶é¢æˆåƒå¹³é¢è¿›è¡Œåˆ†ç±»ï¼Œä»¥ä¿ƒè¿›ç¨³å¥çš„æ¨¡å‹å¼€å‘å’Œè·¨è§†å›¾æ³›åŒ–ã€‚ä¸ºäº†è¯æ˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸ç°æœ‰çš„åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†åŠ æƒå¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ä¸º82.3%çš„æœ€é«˜å€¼ï¼Œæ‰€æœ‰è‚¿ç˜¤ç±»åˆ«å‡æœ‰æ‰€æ”¹è¿›ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™é¡¹ç ”ç©¶ä¸»è¦æ˜¯ä¸ºäº†ä»‹ç»æ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶å»ºç«‹åŸºç¡€åŸºå‡†ã€‚æˆ‘ä»¬æœŸæœ›è¯¥æ•°æ®é›†èƒ½æˆä¸ºç¥ç»è‚¿ç˜¤å­¦ä¸­æœºå™¨å­¦ä¹ åº”ç”¨çš„é‡è¦èµ„æºï¼Œæ”¯æŒå­¦æœ¯ç ”ç©¶å’Œä¸´åºŠå†³ç­–æ”¯æŒå¼€å‘ã€‚æ•°æ®é›†é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/briscdataset/brisc2025/">https://www.kaggle.com/datasets/briscdataset/brisc2025/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14318v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è®¾è®¡çš„æ–°MRIæ•°æ®é›†ï¼ŒåŒ…å«ç”±è®¤è¯æ”¾å°„ç§‘åŒ»ç”Ÿå’ŒåŒ»å¸ˆæ³¨é‡Šçš„6000ä¸ªå¯¹æ¯”å¢å¼ºçš„T1åŠ æƒMRIæ‰«æï¼Œæ¶µç›–ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚ä¸ºå±•ç¤ºè¯¥æ•°æ®é›†å®ç”¨æ€§ï¼Œç ”ç©¶æå‡ºäº†åŸºäºtransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€é«˜åŠ æƒå¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ä¸º82.3%ã€‚æ­¤æ•°æ®é›†æœ‰æœ›ä¸ºç¥ç»è‚¿ç˜¤å­¦æœºå™¨å­¦ä¹ åº”ç”¨æä¾›å®è´µèµ„æºï¼Œæ”¯æŒå­¦æœ¯ç ”ç©¶å’Œä¸´åºŠå†³ç­–æ”¯æŒå¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªæ–°MRIæ•°æ®é›†ï¼Œä¸“ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>æ•°æ®é›†åŒ…å«6000ä¸ªå¯¹æ¯”å¢å¼ºçš„T1åŠ æƒMRIæ‰«æï¼Œæ¶µç›–ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚</li>
<li>æ•°æ®é›†åŒ…å«é«˜åˆ†è¾¨ç‡æ ‡ç­¾ï¼Œåˆ†ç±»è½´å‘ã€çŸ¢çŠ¶å’Œå† çŠ¶æˆåƒå¹³é¢ï¼Œæœ‰åŠ©äºç¨³å¥æ¨¡å‹å¼€å‘å’Œè·¨è§†å›¾æ¨å¹¿ã€‚</li>
<li>ä¸ºå±•ç¤ºæ•°æ®é›†å®ç”¨æ€§ï¼Œç ”ç©¶æå‡ºäº†åŸºäºtransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¨¡å‹è¾¾åˆ°æœ€é«˜åŠ æƒå¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ä¸º82.3%ï¼Œåœ¨æ‰€æœ‰è‚¿ç˜¤ç±»åˆ«ä¸­å‡è§‚å¯Ÿåˆ°æ”¹è¿›ã€‚</li>
<li>æ­¤æ•°æ®é›†ä¸ºæœºå™¨å­¦ä¹ å’Œç¥ç»è‚¿ç˜¤å­¦ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e7690c373a36a706caf9d2e440a9850.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BraTS-orchestrator-Democratizing-and-Disseminating-state-of-the-art-brain-tumor-image-analysis"><a href="#BraTS-orchestrator-Democratizing-and-Disseminating-state-of-the-art-brain-tumor-image-analysis" class="headerlink" title="BraTS orchestrator : Democratizing and Disseminating state-of-the-art   brain tumor image analysis"></a>BraTS orchestrator : Democratizing and Disseminating state-of-the-art   brain tumor image analysis</h2><p><strong>Authors:Florian Kofler, Marcel Rosier, Mehdi Astaraki, Ujjwal Baid, Hendrik MÃ¶ller, Josef A. Buchner, Felix Steinbauer, Eva Oswald, Ezequiel de la Rosa, Ivan Ezhov, Constantin von See, Jan Kirschke, Anton Schmick, Sarthak Pati, Akis Linardos, Carla Pitarch, Sanyukta Adap, Jeffrey Rudie, Maria Correia de Verdier, Rachit Saluja, Evan Calabrese, Dominic LaBella, Mariam Aboian, Ahmed W. Moawad, Nazanin Maleki, Udunna Anazodo, Maruf Adewole, Marius George Linguraru, Anahita Fathi Kazerooni, Zhifan Jiang, Gian Marco Conte, Hongwei Li, Juan Eugenio Iglesias, Spyridon Bakas, Benedikt Wiestler, Marie Piraud, Bjoern Menze</strong></p>
<p>The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (<a target="_blank" rel="noopener" href="https://github.com/BrainLesion/BraTS">https://github.com/BrainLesion/BraTS</a>), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æŒ‘æˆ˜èµ›ç³»åˆ—é€šè¿‡æä¾›å¤§è§„æ¨¡ã€ç²¾é€‰çš„æ•°æ®é›†å¹¶è§£å†³ä¸ä¸´åºŠç›¸å…³çš„ä»»åŠ¡ï¼Œæå¤§åœ°æ¨åŠ¨äº†è„‘è‚¿ç˜¤å›¾åƒåˆ†æçš„å‘å±•ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶å–å¾—äº†æˆåŠŸå¹¶å¹¿å—æ¬¢è¿ï¼Œä½†é€šè¿‡åœ¨BraTSä¸­å¼€å‘çš„ç®—æ³•å’Œæ¨¡å‹åœ¨ç§‘å­¦ç•Œå’Œä¸´åºŠç•Œçš„é‡‡ç”¨ä»ç„¶æœ‰é™ã€‚ä¸ºäº†åŠ é€Ÿå…¶æ™®åŠï¼Œæˆ‘ä»¬å¼•å…¥äº†BraTSç¼–æ’å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„Pythonè½¯ä»¶åŒ…ï¼Œæ—¨åœ¨æ— ç¼è®¿é—®BraTSæŒ‘æˆ˜èµ›ç”Ÿæ€ç³»ç»Ÿä¸­å„ç§è‚¿ç˜¤çš„æœ€æ–°åˆ†å‰²å’Œåˆæˆç®—æ³•ã€‚è¯¥è½¯ä»¶åŒ…å·²åœ¨GitHubä¸Šæä¾›ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/BrainLesion/BraTS%EF%BC%89%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%8C%85%E5%90%AB%E9%92%88%E5%AF%B9%E7%BC%96%E7%A8%8B%E7%BB%8F%E9%AA%8C%E6%9C%80%E5%B0%91%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%9B%B4%E8%A7%82%E6%95%99%E7%A8%8B%EF%BC%8C%E4%BD%BF%E7%A0%94%E7%A9%B6%E8%80%85%E5%92%8C%E4%B8%B4%E5%BA%8A%E5%8C%BB%E7%94%9F%E8%83%BD%E5%A4%9F%E8%BD%BB%E6%9D%BE%E9%83%A8%E7%BD%B2%E8%B5%A2%E5%BE%97BraTS%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E7%90%86%E3%80%82%E9%80%9A%E8%BF%87%E6%8A%BD%E8%B1%A1%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%8D%E6%9D%82%E6%80%A7%EF%BC%8CBraTS%E7%BC%96%E6%8E%92%E5%99%A8%E4%BD%BF%E6%9D%A5%E8%87%AABraTS%E7%A4%BE%E5%8C%BA%E7%9A%84%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86%E5%BE%97%E4%BB%A5%E6%99%AE%E5%8F%8A%EF%BC%8C%E4%BD%BF%E8%BF%99%E4%BA%9B%E8%BF%9B%E5%B1%95%E8%83%BD%E5%A4%9F%E8%BD%BB%E6%9D%BE%E5%9C%B0%E4%B8%BA%E6%9B%B4%E5%B9%BF%E6%B3%9B%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%94%BE%E5%B0%84%E5%AD%A6%E5%92%8C%E7%A5%9E%E7%BB%8F%E8%82%BF%E7%98%A4%E5%AD%A6%E5%8F%97%E4%BC%97%E6%89%80%E5%88%A9%E7%94%A8%E3%80%82">https://github.com/BrainLesion/BraTSï¼‰ï¼Œå…¶ä¸­åŒ…å«é’ˆå¯¹ç¼–ç¨‹ç»éªŒæœ€å°‘çš„ç”¨æˆ·è®¾è®¡çš„ç›´è§‚æ•™ç¨‹ï¼Œä½¿ç ”ç©¶è€…å’Œä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿè½»æ¾éƒ¨ç½²èµ¢å¾—BraTSç®—æ³•çš„æ¨ç†ã€‚é€šè¿‡æŠ½è±¡ç°ä»£æ·±åº¦å­¦ä¹ çš„å¤æ‚æ€§ï¼ŒBraTSç¼–æ’å™¨ä½¿æ¥è‡ªBraTSç¤¾åŒºçš„ä¸“ä¸šçŸ¥è¯†å¾—ä»¥æ™®åŠï¼Œä½¿è¿™äº›è¿›å±•èƒ½å¤Ÿè½»æ¾åœ°ä¸ºæ›´å¹¿æ³›çš„ç¥ç»æ”¾å°„å­¦å’Œç¥ç»è‚¿ç˜¤å­¦å—ä¼—æ‰€åˆ©ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13807v1">PDF</a> 27p, 2figs, 3tabs</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BraTSæŒ‘æˆ˜é›†ç¾¤å¯¹è„‘è‚¿ç˜¤å›¾åƒåˆ†æçš„é‡å¤§æ¨åŠ¨ä½œç”¨ï¼Œä»¥åŠä¸ºåŠ é€Ÿç›¸å…³ç®—æ³•å’Œæ¨¡å‹åœ¨ç§‘ç ”åŠä¸´åºŠç¤¾åŒºçš„æ™®åŠï¼Œæ¨å‡ºçš„å¼€æºPythonåŒ…â€”â€”BraTS orchestratorã€‚è¯¥åŒ…æä¾›å¯¹BraTSæŒ‘æˆ˜ç”Ÿæ€ç³»ç»Ÿä¸­å…ˆè¿›åˆ†å‰²å’Œåˆæˆç®—æ³•çš„ä¾¿æ·è®¿é—®ï¼Œå¹¶è®¾è®¡æœ‰ç›´è§‚æ•™ç¨‹ï¼Œä¾¿äºç”¨æˆ·éƒ¨ç½²ä½¿ç”¨è·å¥–çš„BraTSç®—æ³•è¿›è¡Œæ¨ç†ã€‚é€šè¿‡æŠ½è±¡ç°ä»£æ·±åº¦å­¦ä¹ çš„å¤æ‚æ€§ï¼ŒBraTS orchestratorä½¿å¾—BraTSç¤¾åŒºå†…çš„ä¸“ä¸šçŸ¥è¯†å¾—ä»¥æ°‘ä¸»åŒ–è®¿é—®ï¼Œä½¿å¾—ç¥ç»æ”¾å°„å­¦å’Œç¥ç»è‚¿ç˜¤å­¦é¢†åŸŸçš„æ›´å¹¿æ³›å—ä¼—èƒ½è½»æ¾åˆ©ç”¨è¿™äº›è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BraTSæŒ‘æˆ˜é›†ç¾¤æ˜¾è‘—æ¨åŠ¨äº†è„‘è‚¿ç˜¤å›¾åƒåˆ†æçš„å‘å±•ã€‚</li>
<li>BraTSé€šè¿‡æä¾›å¤§é‡ç»è¿‡æ•´ç†çš„æ•°æ®åº“å’Œåº”å¯¹ä¸´åºŠç›¸å…³ä»»åŠ¡ï¼Œä¿ƒè¿›äº†ç›¸å…³ç ”ç©¶ã€‚</li>
<li>å°½ç®¡ç›¸å…³ç ”ç©¶å–å¾—äº†å¾ˆå¤šè¿›å±•ï¼Œä½†åœ¨ç§‘ç ”åŠä¸´åºŠç¤¾åŒºä¸­ç®—æ³•çš„é‡‡ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>ä¸ºäº†åŠ é€Ÿæ™®åŠï¼Œæ¨å‡ºäº†åä¸ºBraTS orchestratorçš„å¼€æºPythonåŒ…ã€‚</li>
<li>BraTS orchestratoræä¾›å¯¹BraTSæŒ‘æˆ˜ç”Ÿæ€ç³»ç»Ÿä¸­å…ˆè¿›ç®—æ³•çš„ä¾¿æ·è®¿é—®ã€‚</li>
<li>è¯¥åŒ…è®¾è®¡æœ‰ç›´è§‚æ•™ç¨‹ï¼Œé€‚åˆç¼ºä¹ç¼–ç¨‹ç»éªŒçš„ç”¨æˆ·ï¼Œä¾¿äºéƒ¨ç½²ä½¿ç”¨è·å¥–çš„BraTSç®—æ³•è¿›è¡Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3176c81f81b5df45b86c0d3d3fe774e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14bd6faa5cb6d4c2e2cf2ff0fdd21266.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PRO-Projection-Domain-Synthesis-for-CT-Imaging"><a href="#PRO-Projection-Domain-Synthesis-for-CT-Imaging" class="headerlink" title="PRO: Projection Domain Synthesis for CT Imaging"></a>PRO: Projection Domain Synthesis for CT Imaging</h2><p><strong>Authors:Kang Chen, Bin Huang, Xuebin Yang, Junyan Zhang, Qiegen Liu</strong></p>
<p>Synthesizing high quality CT projection data remains a significant challenge due to the limited availability of annotated data and the complex nature of CT imaging. In this work, we present PRO, a projection domain synthesis foundation model for CT imaging. To the best of our knowledge, this is the first study that performs CT synthesis in the projection domain. Unlike previous approaches that operate in the image domain, PRO learns rich structural representations from raw projection data and leverages anatomical text prompts for controllable synthesis. This projection domain strategy enables more faithful modeling of underlying imaging physics and anatomical structures. Moreover, PRO functions as a foundation model, capable of generalizing across diverse downstream tasks by adjusting its generative behavior via prompt inputs. Experimental results demonstrated that incorporating our synthesized data significantly improves performance across multiple downstream tasks, including low-dose and sparse-view reconstruction. These findings underscore the versatility and scalability of PRO in data generation for various CT applications. These results highlight the potential of projection domain synthesis as a powerful tool for data augmentation and robust CT imaging. Our source code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/PRO">https://github.com/yqx7150/PRO</a>. </p>
<blockquote>
<p>åˆæˆé«˜è´¨é‡çš„CTæŠ•å½±æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ ‡æ³¨æ•°æ®çš„æœ‰é™æ€§å’ŒCTæˆåƒçš„å¤æ‚æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PROï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºCTæˆåƒçš„æŠ•å½±åŸŸåˆæˆåŸºç¡€æ¨¡å‹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹åœ¨æŠ•å½±åŸŸè¿›è¡ŒCTåˆæˆçš„ç ”ç©¶ã€‚ä¸åœ¨å›¾åƒåŸŸè¿è¡Œçš„å‰æ–¹æ³•ä¸åŒï¼ŒPROä»åŸå§‹æŠ•å½±æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„ç»“æ„è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è§£å‰–æ–‡æœ¬æç¤ºè¿›è¡Œå¯æ§åˆæˆã€‚è¿™ç§æŠ•å½±åŸŸç­–ç•¥èƒ½å¤Ÿæ›´çœŸå®åœ°æ¨¡æ‹Ÿæ½œåœ¨çš„æˆåƒç‰©ç†å’Œè§£å‰–ç»“æ„ã€‚æ­¤å¤–ï¼ŒPROä½œä¸ºä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡æç¤ºè¾“å…¥è°ƒæ•´å…¶ç”Ÿæˆè¡Œä¸ºï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­é€šç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬åˆæˆçš„æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä½å‰‚é‡å’Œç¨€ç–è§†å›¾é‡å»ºã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†PROåœ¨å„ç§CTåº”ç”¨ä¸­çš„æ•°æ®ç”Ÿæˆä¸­çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¿™äº›ç»“æœçªå‡ºäº†æŠ•å½±åŸŸåˆæˆä½œä¸ºæ•°æ®å¢å¼ºå’Œç¨³å¥CTæˆåƒçš„å¼ºå¤§å·¥å…·çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/yqx7150/PRO%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/yqx7150/PROå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13443v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†é’ˆå¯¹CTæˆåƒçš„æŠ•å½±åŸŸåˆæˆåŸºç¡€æ¨¡å‹PROã€‚è¯¥æ¨¡å‹åœ¨æŠ•å½±åŸŸè¿›è¡ŒCTåˆæˆï¼Œä¸åŒäºä»¥å¾€åœ¨å›¾åƒåŸŸæ“ä½œçš„æ–¹æ³•ï¼ŒPROä»åŸå§‹æŠ•å½±æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„ç»“æ„è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è§£å‰–æ–‡æœ¬æç¤ºè¿›è¡Œå¯æ§åˆæˆã€‚è¿™ç§æŠ•å½±åŸŸç­–ç•¥èƒ½å¤Ÿæ›´å¥½åœ°æ¨¡æ‹Ÿæˆåƒç‰©ç†è¿‡ç¨‹å’Œè§£å‰–ç»“æ„ã€‚æ­¤å¤–ï¼ŒPROä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡æç¤ºè¾“å…¥è°ƒæ•´å…¶ç”Ÿæˆè¡Œä¸ºï¼Œå¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆçš„æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ä½å‰‚é‡å’Œç¨€ç–è§†å›¾é‡å»ºï¼‰çš„æ€§èƒ½ã€‚æ­¤æ‘˜è¦å¼ºè°ƒäº†PROåœ¨æ•°æ®ç”Ÿæˆæ–¹é¢çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå±•ç¤ºäº†æŠ•å½±åŸŸåˆæˆåœ¨æ•°æ®å¢å¼ºå’Œç¨³å¥CTæˆåƒä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PROæ˜¯é¦–ä¸ªåœ¨æŠ•å½±åŸŸè¿›è¡ŒCTåˆæˆçš„æ¨¡å‹ã€‚</li>
<li>PROä»åŸå§‹æŠ•å½±æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„ç»“æ„è¡¨ç¤ºï¼Œåˆ©ç”¨è§£å‰–æ–‡æœ¬æç¤ºè¿›è¡Œå¯æ§åˆæˆã€‚</li>
<li>æŠ•å½±åŸŸç­–ç•¥èƒ½å¤Ÿæ›´å¥½åœ°æ¨¡æ‹Ÿæˆåƒç‰©ç†è¿‡ç¨‹å’Œè§£å‰–ç»“æ„ã€‚</li>
<li>PROä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥å¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨PROåˆæˆçš„æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ä½å‰‚é‡å’Œç¨€ç–è§†å›¾é‡å»ºï¼‰çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†PROåœ¨æ•°æ®ç”Ÿæˆæ–¹é¢çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>PROçš„æºä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9b1f77653085999f697ca5954b356389.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0efc29033605ea2565226ff30c84400e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eb94396e3c8ec7e15b0ea0b94f2be18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc9ece49b09154b9119bd668c28b6153.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Simple-is-what-you-need-for-efficient-and-accurate-medical-image-segmentation"><a href="#Simple-is-what-you-need-for-efficient-and-accurate-medical-image-segmentation" class="headerlink" title="Simple is what you need for efficient and accurate medical image   segmentation"></a>Simple is what you need for efficient and accurate medical image   segmentation</h2><p><strong>Authors:Xiang Yu, Yayan Chen, Guannan He, Qing Zeng, Yue Qin, Meiling Liang, Dandan Luo, Yimei Liao, Zeyu Ren, Cheng Kang, Delong Yang, Bocheng Liang, Bin Pu, Ying Yuan, Shengli Li</strong></p>
<p>While modern segmentation models often prioritize performance over practicality, we advocate a design philosophy prioritizing simplicity and efficiency, and attempted high performance segmentation model design. This paper presents SimpleUNet, a scalable ultra-lightweight medical image segmentation model with three key innovations: (1) A partial feature selection mechanism in skip connections for redundancy reduction while enhancing segmentation performance; (2) A fixed-width architecture that prevents exponential parameter growth across network stages; (3) An adaptive feature fusion module achieving enhanced representation with minimal computational overhead. With a record-breaking 16 KB parameter configuration, SimpleUNet outperforms LBUNet and other lightweight benchmarks across multiple public datasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and accuracy, attaining a mean DSC&#x2F;IoU of 85.76%&#x2F;75.60% on multi-center breast lesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin lesion datasets (ISIC 2017&#x2F;2018: mDice 84.86%&#x2F;88.77%) and endoscopic polyp segmentation (KVASIR-SEG: 86.46%&#x2F;76.48% mDice&#x2F;mIoU) confirm consistent dominance over state-of-the-art models. This work demonstrates that extreme model compression need not compromise performance, providing new insights for efficient and accurate medical image segmentation. Codes can be found at <a target="_blank" rel="noopener" href="https://github.com/Frankyu5666666/SimpleUNet">https://github.com/Frankyu5666666/SimpleUNet</a>. </p>
<blockquote>
<p>æ‘˜è¦ï¼šå°½ç®¡ç°ä»£åˆ†å‰²æ¨¡å‹é€šå¸¸æ›´é‡è§†æ€§èƒ½è€Œéå®ç”¨æ€§ï¼Œä½†æˆ‘ä»¬æå€¡ä»¥ç®€æ´å’Œé«˜æ•ˆä¸ºè®¾è®¡ä¼˜å…ˆç†å¿µï¼Œå¹¶å°è¯•è®¾è®¡é«˜æ€§èƒ½åˆ†å‰²æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†SimpleUNetï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„ã€è¶…è½»é‡çº§çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€ã€è·³è·ƒè¿æ¥ä¸­çš„éƒ¨åˆ†ç‰¹å¾é€‰æ‹©æœºåˆ¶ï¼Œæ—¨åœ¨å‡å°‘å†—ä½™çš„åŒæ—¶æé«˜åˆ†å‰²æ€§èƒ½ï¼›äºŒã€å›ºå®šå®½åº¦æ¶æ„ï¼Œé˜²æ­¢ç½‘ç»œé˜¶æ®µå‚æ•°å‘ˆæŒ‡æ•°å¢é•¿ï¼›ä¸‰ã€è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€å®ç°å¢å¼ºçš„è¡¨ç¤ºã€‚å‡­å€Ÿåˆ›çºªå½•çš„ä»…å ç”¨ 16 KB å‚æ•°çš„é…ç½®ï¼ŒSimpleUNet åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šä¼˜äº LBUNet å’Œå…¶ä»–è½»é‡çº§åŸºå‡†æµ‹è¯•ã€‚å…¶ä¸­ï¼Œ0.67 MB çš„å˜ä½“å±•ç°å‡ºå“è¶Šçš„æ•ˆç‡ï¼ˆGFLOPs è¾¾åˆ° 8.6ï¼‰å’Œå‡†ç¡®åº¦ï¼Œåœ¨å¤šä¸­å¿ƒä¹³è…ºç—…å˜æ•°æ®é›†ä¸Šå¹³å‡ DSC&#x2F;IoU è¾¾åˆ° 85.76%&#x2F;75.6%ã€‚è¯¥æ¨¡å‹ä¸ä»…è¶…è¶Šäº† U-Net å’Œ TransUNetï¼Œä¸”åœ¨çš®è‚¤ç—…å˜æ•°æ®é›†ï¼ˆISIC 2017&#x2F;2018 å¹´çš„ mDice åˆ†åˆ«ä¸º 84.86%&#x2F;88.77%ï¼‰ä»¥åŠå†…é•œæ¯è‚‰åˆ†å‰²ï¼ˆKVASIR-SEG çš„ mDice&#x2F;mIoU ä¸º 86.46%&#x2F;76.48%ï¼‰ä¸Šçš„è¯„ä¼°ç»“æœä¹Ÿè¯æ˜äº†å…¶åœ¨æœ€æ–°æŠ€æœ¯æ¨¡å‹ä¸­çš„ä¸€è‡´æ€§ä¼˜åŠ¿ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†æåº¦çš„æ¨¡å‹å‹ç¼©å¹¶ä¸ä¸€å®šä¼šå½±å“æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆä¸”å‡†ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„è§è§£ã€‚ä»£ç å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/Frankyu5666666/SimpleUNet">https://github.com/Frankyu5666666/SimpleUNet</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13415v1">PDF</a> 15 pages, 11 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§ç®€æ´é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹SimpleUNetï¼Œå…·æœ‰ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šé‡‡ç”¨éƒ¨åˆ†ç‰¹å¾é€‰æ‹©æœºåˆ¶æé«˜åˆ†å‰²æ€§èƒ½å¹¶å‡å°‘å†—ä½™ï¼›é‡‡ç”¨å›ºå®šå®½åº¦æ¶æ„é˜²æ­¢ç½‘ç»œé˜¶æ®µå‚æ•°æŒ‡æ•°å¢é•¿ï¼›ä»¥åŠè‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—å®ç°å¢å¼ºè¡¨ç¤ºå¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚SimpleUNetåœ¨å¤šå…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¶…è¶ŠLBUNetå’Œå…¶ä»–è½»é‡çº§åŸºå‡†çš„æ€§èƒ½ï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimpleUNetæ¨¡å‹å¼ºè°ƒç®€æ´å’Œæ•ˆç‡çš„è®¾è®¡å“²å­¦ï¼ŒåŒæ—¶å®ç°é«˜æ€§èƒ½çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>æ¨¡å‹å…·æœ‰ä¸‰å¤§å…³é”®åˆ›æ–°ï¼šéƒ¨åˆ†ç‰¹å¾é€‰æ‹©ã€å›ºå®šå®½åº¦æ¶æ„å’Œè‡ªé€‚åº”ç‰¹å¾èåˆã€‚</li>
<li>SimpleUNetå®ç°äº†å‚æ•°é…ç½®çš„çªç ´ï¼Œä»…æœ‰16KBå‚æ•°ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šï¼ŒSimpleUNetè¶…è¶Šäº†LBUNetå’Œå…¶ä»–è½»é‡çº§åŸºå‡†ã€‚</li>
<li>0.67MBçš„å˜ä½“åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¾¾åˆ°85.76%çš„å‡å€¼DSC&#x2F;IoUã€‚</li>
<li>åœ¨çš®è‚¤ç—…å˜æ•°æ®é›†å’Œå†…é•œæ¯è‚‰åˆ†å‰²ä¸Šçš„è¯„ä»·è¯æ˜äº†SimpleUNetçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38ce86f83a86b1318d28f9a23ac8b38f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b22ab6c64ce7a389a790c6c057510f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff9a9d088ee5bae1fce6ea256c0cf46b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ce17536912ef0bc467fda62a3750d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcb7dfd6d73811a8aa66239bb5f601e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d920ff33d760b9b8340dc49487d2b6ac.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MorphSAM-Learning-the-Morphological-Prompts-from-Atlases-for-Spine-Image-Segmentation"><a href="#MorphSAM-Learning-the-Morphological-Prompts-from-Atlases-for-Spine-Image-Segmentation" class="headerlink" title="MorphSAM: Learning the Morphological Prompts from Atlases for Spine   Image Segmentation"></a>MorphSAM: Learning the Morphological Prompts from Atlases for Spine   Image Segmentation</h2><p><strong>Authors:Dingwei Fan, Junyong Zhao, Chunlin Li, Xinlong Wang, Ronghan Zhang, Mingliang Wang, Qi Zhu, Haipeng Si, Daoqiang Zhang, Liang Sun</strong></p>
<p>Spine image segmentation is crucial for clinical diagnosis and treatment of spine diseases. The complex structure of the spine and the high morphological similarity between individual vertebrae and adjacent intervertebral discs make accurate spine segmentation a challenging task. Although the Segment Anything Model (SAM) has been developed, it still struggles to effectively capture and utilize morphological information, limiting its ability to enhance spine image segmentation performance. To address these challenges, in this paper, we propose a MorphSAM that explicitly learns morphological information from atlases, thereby strengthening the spine image segmentation performance of SAM. Specifically, the MorphSAM includes two fully automatic prompt learning networks, 1) an anatomical prompt learning network that directly learns morphological information from anatomical atlases, and 2) a semantic prompt learning network that derives morphological information from text descriptions converted from the atlases. Then, the two learned morphological prompts are fed into the SAM model to boost the segmentation performance. We validate our MorphSAM on two spine image segmentation tasks, including a spine anatomical structure segmentation task with CT images and a lumbosacral plexus segmentation task with MR images. Experimental results demonstrate that our MorphSAM achieves superior segmentation performance when compared to the state-of-the-art methods. </p>
<blockquote>
<p>è„Šæ¤å›¾åƒåˆ†å‰²å¯¹äºè„Šæ¤ç–¾ç—…çš„ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚è„Šæ¤çš„å¤æ‚ç»“æ„ä»¥åŠå„ä¸ªæ¤ä½“å’Œç›¸é‚»æ¤é—´ç›˜ä¹‹é—´çš„é«˜åº¦å½¢æ€ç›¸ä¼¼æ€§ï¼Œä½¿å¾—å‡†ç¡®çš„è„Šæ¤åˆ†å‰²æˆä¸ºä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶å·²ç»å‡ºç°åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œä½†å®ƒä»ç„¶éš¾ä»¥æœ‰æ•ˆåœ°æ•è·å’Œåˆ©ç”¨å½¢æ€ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨æé«˜è„Šæ¤å›¾åƒåˆ†å‰²æ€§èƒ½æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§MorphSAMï¼Œå®ƒå¯ä»¥ä»å›¾è°±ä¸­æ˜¾å¼å­¦ä¹ å½¢æ€ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºSAMçš„è„Šæ¤å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒMorphSAMåŒ…æ‹¬ä¸¤ä¸ªå…¨è‡ªåŠ¨çš„æç¤ºå­¦ä¹ ç½‘ç»œï¼š1ï¼‰è§£å‰–æç¤ºå­¦ä¹ ç½‘ç»œï¼Œç›´æ¥ä»è§£å‰–å›¾è°±ä¸­å­¦ä¹ å½¢æ€ä¿¡æ¯ï¼›2ï¼‰è¯­ä¹‰æç¤ºå­¦ä¹ ç½‘ç»œï¼Œä»ç”±å›¾è°±è½¬æ¢çš„æ–‡æœ¬æè¿°ä¸­æ´¾ç”Ÿå½¢æ€ä¿¡æ¯ã€‚ç„¶åï¼Œå°†è¿™ä¸¤ä¸ªå­¦åˆ°çš„å½¢æ€æç¤ºè¾“å…¥åˆ°SAMæ¨¡å‹ä¸­ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªè„Šæ¤å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„MorphSAMï¼ŒåŒ…æ‹¬ä½¿ç”¨CTå›¾åƒçš„è„Šæ¤è§£å‰–ç»“æ„åˆ†å‰²ä»»åŠ¡å’Œä½¿ç”¨MRå›¾åƒçš„è…°éª¶ä¸›åˆ†å‰²ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„MorphSAMå®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13094v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹è„Šæ¤å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MorphSAMæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä»å›¾è°±ä¸­æ˜¾å¼å­¦ä¹ å½¢æ€ä¿¡æ¯ï¼Œå¢å¼ºäº†Segment Anything Modelï¼ˆSAMï¼‰å¯¹è„Šæ¤å›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚MorphSAMåŒ…æ‹¬ä¸¤ä¸ªå…¨è‡ªåŠ¨æç¤ºå­¦ä¹ ç½‘ç»œï¼šä¸€ä¸ªæ˜¯ä»å›¾è°±ç›´æ¥å­¦ä¹ å½¢æ€ä¿¡æ¯çš„è§£å‰–æç¤ºå­¦ä¹ ç½‘ç»œï¼Œå¦ä¸€ä¸ªæ˜¯ä»æ–‡æœ¬æè¿°ä¸­å­¦ä¹ å½¢æ€ä¿¡æ¯çš„è¯­ä¹‰æç¤ºå­¦ä¹ ç½‘ç»œã€‚è¿™ä¸¤ä¸ªå­¦åˆ°çš„å½¢æ€æç¤ºè¢«è¾“å…¥åˆ°SAMæ¨¡å‹ä¸­ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚åœ¨ä¸¤é¡¹è„Šæ¤å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬ä½¿ç”¨CTå›¾åƒçš„è„Šæ¤ç»“æ„åˆ†å‰²ä»»åŠ¡å’Œä½¿ç”¨MRå›¾åƒçš„è…°éª¶ä¸›åˆ†å‰²ä»»åŠ¡ï¼ŒéªŒè¯äº†MorphSAMçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„Šæ¤å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è„Šæ¤ç–¾ç—…ä¸­å…·æœ‰é‡è¦æ€§ã€‚</li>
<li>è„Šæ¤çš„å¤æ‚ç»“æ„å’Œä¸ªä½“æ¤éª¨ä¸ç›¸é‚»æ¤é—´ç›˜çš„é«˜å½¢æ€ç›¸ä¼¼æ€§ä½¿å¾—å‡†ç¡®åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨è„Šæ¤å›¾åƒåˆ†å‰²ä¸­è™½æœ‰ä¸€å®šæˆæ•ˆï¼Œä½†éš¾ä»¥æœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨å½¢æ€ä¿¡æ¯ã€‚</li>
<li>MorphSAMæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒé€šè¿‡ä»å›¾è°±ä¸­æ˜¾å¼å­¦ä¹ å½¢æ€ä¿¡æ¯æ¥å¢å¼ºSAMçš„æ€§èƒ½ã€‚</li>
<li>MorphSAMåŒ…æ‹¬ä¸¤ä¸ªå…¨è‡ªåŠ¨æç¤ºå­¦ä¹ ç½‘ç»œï¼šè§£å‰–æç¤ºå­¦ä¹ ç½‘ç»œå’Œè¯­ä¹‰æç¤ºå­¦ä¹ ç½‘ç»œï¼Œåˆ†åˆ«ä»å›¾è°±å’Œæ–‡æœ¬æè¿°ä¸­å­¦ä¹ å½¢æ€ä¿¡æ¯ã€‚</li>
<li>ä¸¤ä¸ªå­¦åˆ°çš„å½¢æ€æç¤ºè¢«è¾“å…¥åˆ°SAMæ¨¡å‹ä¸­ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e60234b3962bf79d5f6b00ff5ae2d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfa5bcb7b6835287696a90469f5cf10a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e3f60e7f0ebc1041c12b07e589c72dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eee29db6a20e1a1f295f35bfc58c6b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-846836f831fb618127f653a584027ea0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Unleashing-Diffusion-and-State-Space-Models-for-Medical-Image-Segmentation"><a href="#Unleashing-Diffusion-and-State-Space-Models-for-Medical-Image-Segmentation" class="headerlink" title="Unleashing Diffusion and State Space Models for Medical Image   Segmentation"></a>Unleashing Diffusion and State Space Models for Medical Image   Segmentation</h2><p><strong>Authors:Rong Wu, Ziqi Chen, Liming Zhong, Heng Li, Hai Shu</strong></p>
<p>Existing segmentation models trained on a single medical imaging dataset often lack robustness when encountering unseen organs or tumors. Developing a robust model capable of identifying rare or novel tumor categories not present during training is crucial for advancing medical imaging applications. We propose DSM, a novel framework that leverages diffusion and state space models to segment unseen tumor categories beyond the training data. DSM utilizes two sets of object queries trained within modified attention decoders to enhance classification accuracy. Initially, the model learns organ queries using an object-aware feature grouping strategy to capture organ-level visual features. It then refines tumor queries by focusing on diffusion-based visual prompts, enabling precise segmentation of previously unseen tumors. Furthermore, we incorporate diffusion-guided feature fusion to improve semantic segmentation performance. By integrating CLIP text embeddings, DSM captures category-sensitive classes to improve linguistic transfer knowledge, thereby enhancing the modelâ€™s robustness across diverse scenarios and multi-label tasks. Extensive experiments demonstrate the superior performance of DSM in various tumor segmentation tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Rows21/KMax-Mamba">https://github.com/Rows21/KMax-Mamba</a>. </p>
<blockquote>
<p>ç°æœ‰åŸºäºå•ä¸€åŒ»å­¦æˆåƒæ•°æ®é›†çš„åˆ†å‰²æ¨¡å‹åœ¨é¢ä¸´æœªè§è¿‡çš„å™¨å®˜æˆ–è‚¿ç˜¤æ—¶ï¼Œå…¶ç¨³å¥æ€§å¾€å¾€ä¸è¶³ã€‚å¼€å‘ä¸€ç§èƒ½å¤Ÿè¯†åˆ«è®­ç»ƒæœŸé—´ä¸å­˜åœ¨çš„ç½•è§æˆ–æ–°å‹è‚¿ç˜¤ç±»åˆ«çš„ç¨³å¥æ¨¡å‹ï¼Œå¯¹äºæ¨åŠ¨åŒ»å­¦æˆåƒåº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†DSMï¼ˆä¸€ä¸ªåˆ©ç”¨æ‰©æ•£çŠ¶æ€ç©ºé—´æ¨¡å‹è¿›è¡Œæœªè§è‚¿ç˜¤ç±»åˆ«åˆ†å‰²çš„æ–°å‹æ¡†æ¶ï¼‰ã€‚DSMåˆ©ç”¨ä¸¤ç»„å¯¹è±¡æŸ¥è¯¢ï¼Œåœ¨ä¿®æ”¹åçš„æ³¨æ„åŠ›è§£ç å™¨å†…è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚é¦–å…ˆï¼Œæ¨¡å‹ä½¿ç”¨å¯¹è±¡æ„ŸçŸ¥ç‰¹å¾åˆ†ç»„ç­–ç•¥å­¦ä¹ å™¨å®˜æŸ¥è¯¢ï¼Œä»¥æ•è·å™¨å®˜çº§åˆ«çš„è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œå®ƒé€šè¿‡ä¸“æ³¨äºåŸºäºæ‰©æ•£çš„è§†è§‰æç¤ºæ¥ä¼˜åŒ–è‚¿ç˜¤æŸ¥è¯¢ï¼Œä»è€Œå®ç°å…ˆå‰æœªè§è‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†æ‰©æ•£å¼•å¯¼çš„ç‰¹å¾èåˆï¼Œä»¥æé«˜è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡é›†æˆCLIPæ–‡æœ¬åµŒå…¥ï¼ŒDSMæ•è·ç±»åˆ«æ•æ„Ÿç±»ï¼Œä»¥æ”¹å–„è¯­è¨€è½¬ç§»çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨å„ç§åœºæ™¯å’Œå¤šæ ‡ç­¾ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒDSMåœ¨å„ç§è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½å“è¶Šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Rows21/KMax-Mamba%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Rows21/KMax-Mambaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12747v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶DSMï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ä»¥å¤–çš„è‚¿ç˜¤ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚DSMé€šè¿‡ä¸¤ç»„å¯¹è±¡æŸ¥è¯¢åœ¨ä¿®æ”¹çš„å…³æ³¨è§£ç å™¨ä¸­è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ç»“åˆCLIPæ–‡æœ¬åµŒå…¥æ¥æé«˜æ¨¡å‹çš„è·¨åœºæ™¯å’Œå¤šæ ‡ç­¾ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSMåˆ©ç”¨æ‰©æ•£å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹æ¥åˆ†å‰²åŒ»å­¦å›¾åƒä¸­æœªè§çš„è‚¿ç˜¤ç±»åˆ«ã€‚</li>
<li>é€šè¿‡ä¸¤ç»„å¯¹è±¡æŸ¥è¯¢åœ¨ä¿®æ”¹çš„å…³æ³¨è§£ç å™¨ä¸­è¿›è¡Œè®­ç»ƒï¼Œæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨å¯¹è±¡æ„ŸçŸ¥ç‰¹å¾åˆ†ç»„ç­–ç•¥æ¥å­¦ä¹ å™¨å®˜æŸ¥è¯¢ï¼Œé€šè¿‡æ‰©æ•£è§†è§‰æç¤ºæ¥ç²¾ç»†è‚¿ç˜¤æŸ¥è¯¢ã€‚</li>
<li>å¼•å…¥æ‰©æ•£å¼•å¯¼çš„ç‰¹å¾èåˆä»¥æé«˜è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç»“åˆCLIPæ–‡æœ¬åµŒå…¥ï¼Œæ•è·ç±»åˆ«æ•æ„Ÿç±»ï¼Œæé«˜è¯­è¨€è½¬ç§»çŸ¥è¯†ã€‚</li>
<li>DSMæ¡†æ¶åœ¨å¤šç§è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e0208cff66b69526e2c8f685a8b1b41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f776a155fff4b6adba49e84ac2610c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a105525e1ec3edc29bacbb359b37e0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d714d9e175a0f260565ecfb1e3a4c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3666043ee1b00b71cb711a62361533bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MVP-CBM-Multi-layer-Visual-Preference-enhanced-Concept-Bottleneck-Model-for-Explainable-Medical-Image-Classification"><a href="#MVP-CBM-Multi-layer-Visual-Preference-enhanced-Concept-Bottleneck-Model-for-Explainable-Medical-Image-Classification" class="headerlink" title="MVP-CBM:Multi-layer Visual Preference-enhanced Concept Bottleneck Model   for Explainable Medical Image Classification"></a>MVP-CBM:Multi-layer Visual Preference-enhanced Concept Bottleneck Model   for Explainable Medical Image Classification</h2><p><strong>Authors:Chunjiang Wang, Kun Zhang, Yandong Liu, Zhiyang He, Xiaodong Tao, S. Kevin Zhou</strong></p>
<p>The concept bottleneck model (CBM), as a technique improving interpretability via linking predictions to human-understandable concepts, makes high-risk and life-critical medical image classification credible. Typically, existing CBM methods associate the final layer of visual encoders with concepts to explain the modelâ€™s predictions. However, we empirically discover the phenomenon of concept preference variation, that is, the concepts are preferably associated with the features at different layers than those only at the final layer; yet a blind last-layer-based association neglects such a preference variation and thus weakens the accurate correspondences between features and concepts, impairing model interpretability. To address this issue, we propose a novel Multi-layer Visual Preference-enhanced Concept Bottleneck Model (MVP-CBM), which comprises two key novel modules: (1) intra-layer concept preference modeling, which captures the preferred association of different concepts with features at various visual layers, and (2) multi-layer concept sparse activation fusion, which sparsely aggregates concept activations from multiple layers to enhance performance. Thus, by explicitly modeling concept preferences, MVP-CBM can comprehensively leverage multi-layer visual information to provide a more nuanced and accurate explanation of model decisions. Extensive experiments on several public medical classification benchmarks demonstrate that MVP-CBM achieves state-of-the-art accuracy and interoperability, verifying its superiority. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wcj6/MVP-CBM">https://github.com/wcj6/MVP-CBM</a>. </p>
<blockquote>
<p>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰ä½œä¸ºä¸€ç§é€šè¿‡é“¾æ¥é¢„æµ‹ç»“æœå’Œäººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥æé«˜è§£é‡Šæ€§çš„æŠ€æœ¯ï¼Œä½¿å¾—é«˜é£é™©å’Œå…³é”®ç”Ÿå‘½çš„åŒ»å­¦å›¾åƒåˆ†ç±»æ›´åŠ å¯ä¿¡ã€‚é€šå¸¸ï¼Œç°æœ‰çš„CBMæ–¹æ³•ä¼šå°†è§†è§‰ç¼–ç å™¨çš„æœ€åä¸€å±‚ä¸æ¦‚å¿µç›¸å…³è”ï¼Œä»¥è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶å‘ç°äº†æ¦‚å¿µåå¥½å˜åŒ–çš„ç°è±¡ï¼Œå³æ¦‚å¿µæ›´å€¾å‘äºä¸ä¸åŒå±‚çº§çš„ç‰¹å¾ç›¸å…³è”ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€åä¸€å±‚ï¼›ç„¶è€Œï¼ŒåŸºäºæœ€åä¸€å±‚çš„ç›²ç›®å…³è”å¿½è§†äº†è¿™ç§åå¥½å˜åŒ–ï¼Œä»è€Œå‰Šå¼±äº†ç‰¹å¾ä¸æ¦‚å¿µä¹‹é—´çš„å‡†ç¡®å¯¹åº”å…³ç³»ï¼ŒæŸå®³äº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šå±‚è§†è§‰åå¥½å¢å¼ºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆMVP-CBMï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå…³é”®çš„æ–°æ¨¡å—ï¼šï¼ˆ1ï¼‰å±‚å†…æ¦‚å¿µåå¥½å»ºæ¨¡ï¼Œå®ƒæ•æ‰ä¸åŒæ¦‚å¿µä¸å„ç§è§†è§‰å±‚ç‰¹å¾çš„ä¼˜é€‰å…³è”ï¼›ï¼ˆ2ï¼‰å¤šå±‚æ¦‚å¿µç¨€ç–æ¿€æ´»èåˆï¼Œå®ƒç¨€ç–åœ°èšåˆå¤šå±‚çš„æ¦‚å¿µæ¿€æ´»ä»¥å¢å¼ºæ€§èƒ½ã€‚å› æ­¤ï¼Œé€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡æ¦‚å¿µåå¥½ï¼ŒMVP-CBMå¯ä»¥å…¨é¢åœ°åˆ©ç”¨å¤šå±‚çš„è§†è§‰ä¿¡æ¯ï¼Œä¸ºæ¨¡å‹çš„å†³ç­–æä¾›æ›´ç»†å¾®å’Œå‡†ç¡®çš„è§£é‡Šã€‚åœ¨å‡ ä¸ªå…¬å¼€çš„åŒ»å­¦åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMVP-CBMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œå¯æ“ä½œæ€§ï¼ŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wcj6/MVP-CBM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wcj6/MVP-CBMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12568v1">PDF</a> 7 pages, 6 figures,</p>
<p><strong>æ‘˜è¦</strong><br>    æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰é€šè¿‡å…³è”é¢„æµ‹ä¸äººç±»å¯ç†è§£çš„æ¦‚å¿µï¼Œæé«˜äº†åŒ»å­¦å›¾åƒåˆ†ç±»çš„å¯ä¿¡åº¦ã€‚ä½†ç°æœ‰CBMæ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰ç¼–ç å™¨çš„æœ€åä¸€å±‚ä¸æ¦‚å¿µçš„è”ç³»ï¼Œå¿½ç•¥äº†æ¦‚å¿µåå¥½å˜åŒ–ç°è±¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå±‚çº§è§†è§‰åå¥½å¢å¼ºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆMVP-CBMï¼‰ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼š1ï¼‰å†…å±‚æ¦‚å¿µåå¥½å»ºæ¨¡ï¼Œæ•æ‰ä¸åŒæ¦‚å¿µä¸ä¸åŒè§†è§‰å±‚ç‰¹å¾çš„åå¥½å…³è”ï¼›2ï¼‰å¤šå±‚æ¦‚å¿µç¨€ç–æ¿€æ´»èåˆï¼Œç¨€ç–åœ°èšåˆå¤šå±‚æ¦‚å¿µæ¿€æ´»ä»¥æé«˜æ€§èƒ½ã€‚MVP-CBMèƒ½å…¨é¢åˆ©ç”¨å¤šå±‚è§†è§‰ä¿¡æ¯ï¼Œæä¾›æ›´ç»†è‡´å‡†ç¡®çš„æ¨¡å‹å†³ç­–è§£é‡Šã€‚åœ¨å¤šä¸ªå…¬å¼€åŒ»å­¦åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†MVP-CBMçš„å“è¶Šæ€§å’Œæœ€æ–°å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰æé«˜äº†åŒ»å­¦å›¾åƒåˆ†ç±»çš„å¯ä¿¡åº¦ï¼Œé€šè¿‡è¿æ¥é¢„æµ‹ä¸äººç±»å¯ç†è§£çš„æ¦‚å¿µã€‚</li>
<li>ç°æœ‰CBMæ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰ç¼–ç å™¨çš„æœ€åä¸€å±‚ä¸æ¦‚å¿µçš„è”ç³»ï¼Œè¿™å¿½ç•¥äº†æ¦‚å¿µåå¥½çš„å˜åŒ–ã€‚</li>
<li>æå‡ºäº†å¤šå±‚çº§è§†è§‰åå¥½å¢å¼ºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆMVP-CBMï¼‰ï¼ŒåŒ…æ‹¬å†…å±‚æ¦‚å¿µåå¥½å»ºæ¨¡å’Œå¤šå±‚æ¦‚å¿µç¨€ç–æ¿€æ´»èåˆä¸¤ä¸ªå…³é”®æ¨¡å—ã€‚</li>
<li>MVP-CBMèƒ½å…¨é¢åˆ©ç”¨å¤šå±‚è§†è§‰ä¿¡æ¯ï¼Œæä¾›æ›´å‡†ç¡®å’Œç»†è‡´çš„æ¨¡å‹å†³ç­–è§£é‡Šã€‚</li>
<li>MVP-CBMåœ¨å¤šä¸ªå…¬å¼€åŒ»å­¦åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°å‡†ç¡®æ€§å’Œå“è¶Šæ€§èƒ½ã€‚</li>
<li>MVP-CBMé€šè¿‡æ˜ç¡®å»ºæ¨¡æ¦‚å¿µåå¥½ï¼Œå®ç°äº†å¯¹æ¨¡å‹å†³ç­–çš„æ›´æ·±å…¥å’Œå…¨é¢çš„ç†è§£ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šåˆ†äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd00343a20586a25961f8092b37e6aa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f171cb5595dcccfc092b6722d081d170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6128a646ca0d2dd626a3d84fee97bbcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d79494f63f74717f8d4500768e8959e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-623edf939892e1da11ef2e0e24319684.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Deep-Feature-Fusion-and-Ensemble-Learning-for-Enhanced-Brain-Tumor-MRI-Classification"><a href="#Hierarchical-Deep-Feature-Fusion-and-Ensemble-Learning-for-Enhanced-Brain-Tumor-MRI-Classification" class="headerlink" title="Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced   Brain Tumor MRI Classification"></a>Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced   Brain Tumor MRI Classification</h2><p><strong>Authors:Zahid Ullah, Jihie Kim</strong></p>
<p>Accurate brain tumor classification is crucial in medical imaging to ensure reliable diagnosis and effective treatment planning. This study introduces a novel double ensembling framework that synergistically combines pre-trained deep learning (DL) models for feature extraction with optimized machine learning (ML) classifiers for robust classification. The framework incorporates comprehensive preprocessing and data augmentation of brain magnetic resonance images (MRI), followed by deep feature extraction using transfer learning with pre-trained Vision Transformer (ViT) networks. The novelty lies in the dual-level ensembling strategy: feature-level ensembling, which integrates deep features from the top-performing ViT models, and classifier-level ensembling, which aggregates predictions from hyperparameter-optimized ML classifiers. Experiments on two public Kaggle MRI brain tumor datasets demonstrate that this approach significantly surpasses state-of-the-art methods, underscoring the importance of feature and classifier fusion. The proposed methodology also highlights the critical roles of hyperparameter optimization (HPO) and advanced preprocessing techniques in improving diagnostic accuracy and reliability, advancing the integration of DL and ML for clinically relevant medical image analysis. </p>
<blockquote>
<p>ç²¾ç¡®çš„å¤§è„‘è‚¿ç˜¤åˆ†ç±»åœ¨åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œèƒ½ç¡®ä¿å¯é çš„è¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŒé‡é›†æˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒç»“åˆäº†é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä»¥åŠä¼˜åŒ–åçš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åˆ†ç±»å™¨è¿›è¡Œç¨³å¥åˆ†ç±»ã€‚è¯¥æ¡†æ¶åŒ…å«å¯¹å¤§è„‘ç£å…±æŒ¯å›¾åƒï¼ˆMRIï¼‰çš„å…¨é¢é¢„å¤„ç†å’Œæ•°æ®å¢å¼ºï¼Œéšåä½¿ç”¨åŸºäºè¿ç§»å­¦ä¹ çš„æ·±åº¦ç‰¹å¾æå–å’Œé¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰ç½‘ç»œã€‚å…¶æ–°é¢–ä¹‹å¤„åœ¨äºåŒé‡é›†æˆç­–ç•¥ï¼šç‰¹å¾çº§é›†æˆï¼Œé›†æˆäº†è¡¨ç°æœ€ä½³çš„ViTæ¨¡å‹çš„æ·±åº¦ç‰¹å¾ï¼›åˆ†ç±»å™¨çº§é›†æˆï¼Œèšåˆäº†è¶…å‚æ•°ä¼˜åŒ–MLåˆ†ç±»å™¨çš„é¢„æµ‹ã€‚åœ¨Kaggleçš„ä¸¤ä¸ªå…¬å…±MRIè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œçªæ˜¾äº†ç‰¹å¾å’Œåˆ†ç±»å™¨èåˆçš„é‡è¦æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿˜å¼ºè°ƒäº†è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰å’Œå…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯åœ¨æé«˜è¯Šæ–­å’Œå¯é æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œæ¨åŠ¨äº†æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨ä¸´åºŠç›¸å…³åŒ»å­¦å›¾åƒåˆ†æä¸­çš„èåˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12363v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹åŒé‡é›†æˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆé¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä»¥åŠä¼˜åŒ–çš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»ï¼Œå®ç°äº†ç²¾ç¡®çš„è„‘è‚¿ç˜¤åˆ†ç±»ã€‚é€šè¿‡ç»¼åˆè¿ç”¨é¢„å¤„ç†å’Œç£å…±æŒ¯å›¾åƒæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œç»“åˆä½¿ç”¨é¢„è®­ç»ƒVision Transformerç½‘ç»œçš„æ·±åº¦ç‰¹å¾æå–æ–¹æ³•ã€‚å…¶æ–°é¢–ä¹‹å¤„åœ¨äºåŒé‡é›†æˆç­–ç•¥ï¼šç‰¹å¾çº§é›†æˆå’Œåˆ†ç±»å™¨çº§é›†æˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¼ºè°ƒäº†ç‰¹å¾èåˆå’Œåˆ†ç±»å™¨èåˆçš„é‡è¦æ€§ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹Ÿå¼ºè°ƒäº†è¶…å‚æ•°ä¼˜åŒ–å’Œé«˜çº§é¢„å¤„ç†æŠ€æœ¯åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®è„‘è‚¿ç˜¤åˆ†ç±»åœ¨åŒ»å­¦æˆåƒä¸­çš„é‡è¦æ€§åŠå…¶å¯¹äºå¯é è¯Šæ–­å’Œæœ‰æ•ˆæ²»ç–—è®¡åˆ’çš„æ„ä¹‰ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹åŒé‡é›†æˆæ¡†æ¶ï¼Œç»“åˆäº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç‰¹å¾æå–å’Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨çš„ä¼˜åŒ–åˆ†ç±»ã€‚</li>
<li>æ¡†æ¶ä¸­ç»¼åˆè¿ç”¨äº†é¢„å¤„ç†å’Œç£å…±æŒ¯å›¾åƒæ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨äº†é¢„è®­ç»ƒçš„Vision Transformerç½‘ç»œè¿›è¡Œæ·±åº¦ç‰¹å¾æå–ã€‚</li>
<li>åŒé‡é›†æˆç­–ç•¥åŒ…æ‹¬ç‰¹å¾çº§é›†æˆå’Œåˆ†ç±»å™¨çº§é›†æˆã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶å¼ºè°ƒäº†ç‰¹å¾èåˆå’Œåˆ†ç±»å™¨èåˆçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f79837758e0c393a2fcb5c45c3f06fe4.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Accuracy-and-Precision-of-Random-Walk-with-Barrier-Model-Fitting-Simulations-and-Applications-in-Head-and-Neck-Cancers"><a href="#Accuracy-and-Precision-of-Random-Walk-with-Barrier-Model-Fitting-Simulations-and-Applications-in-Head-and-Neck-Cancers" class="headerlink" title="Accuracy and Precision of Random Walk with Barrier Model Fitting:   Simulations and Applications in Head and Neck Cancers"></a>Accuracy and Precision of Random Walk with Barrier Model Fitting:   Simulations and Applications in Head and Neck Cancers</h2><p><strong>Authors:Jiaren Zou, Yue Cao</strong></p>
<p>Promising results have been reported in quantifying microstructural parameters (free diffusivity, cell size, and membrane permeability) in head and neck cancers (HNCs) using time-dependent diffusion MRI fitted to Random Walk with Barrier Model (RWBM). However, model fitting remains challenging due to limited number of measurements, low signal-to-noise ratio and complex nonlinear biophysical model. In this work, we comprehensively investigated and elucidated the dependence of RWBM fitting performance on tissue property, data acquisition and processing, and provided insights on improving data acquisition and model fitting. We numerically evaluated the accuracy and precision of RWBM fitting using non-linear least squares as a function of model parameters, noise levels, and maximum effective diffusion times over a wide range of microstructural parameter values. We then elucidated these results by examining the modelâ€™s degeneracy and fitting landscape. In vivo fitting results on patients with HNCs were analyzed and were interpreted using the numerical results. We observed that free diffusivity estimates were accurate and precise over a wide range of microstructural parameters, whereas the accuracy and precision of surface-to-volume ratio and membrane permeability depended on the values of the parameters. In addition, a maximum effective diffusion time of 200 ms provided the lowest bias and variance in membrane permeability estimations. By fitting to the short-time limit expression, we observed that the variance of parameter estimation was reduced, and that a more accurate and precise estimation of membrane permeability was achieved. In vivo fitting results were consistent with the numerical findings. In conclusion, this work provides a comprehensive analysis of RWBM fitting in clinical settings and has the potential to guide further optimizations of data acquisition and model fitting methods. </p>
<blockquote>
<p>ä½¿ç”¨åŸºäºéšæœºæ¸¸èµ°å±éšœæ¨¡å‹ï¼ˆRWBMï¼‰çš„æ—¶é—´ç›¸å…³æ‰©æ•£MRIå¯¹å¤´éƒ¨å’Œé¢ˆéƒ¨ç™Œç—‡ï¼ˆHNCsï¼‰çš„å¾®ç»“æ„å‚æ•°ï¼ˆè‡ªç”±æ‰©æ•£ç‡ã€ç»†èƒå¤§å°å’Œè†œé€šé€æ€§ï¼‰è¿›è¡Œé‡åŒ–å·²æŠ¥é“æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œç”±äºæµ‹é‡æ¬¡æ•°æœ‰é™ã€ä¿¡å™ªæ¯”ä½ä»¥åŠå¤æ‚çš„éçº¿æ€§ç”Ÿç‰©ç‰©ç†æ¨¡å‹ï¼Œæ¨¡å‹æ‹Ÿåˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…¨é¢ç ”ç©¶å’Œé˜æ˜äº†RWBMæ‹Ÿåˆæ€§èƒ½å¯¹ç»„ç»‡ç‰¹æ€§ã€æ•°æ®è·å–å’Œå¤„ç†çš„ä¾èµ–æ€§ï¼Œå¹¶æä¾›äº†æ”¹è¿›æ•°æ®è·å–å’Œæ¨¡å‹æ‹Ÿåˆçš„è§è§£ã€‚æˆ‘ä»¬é€šè¿‡éçº¿æ€§æœ€å°äºŒä¹˜æ³•æ•°å€¼è¯„ä¼°äº†RWBMæ‹Ÿåˆçš„å‡†ç¡®æ€§å’Œç²¾åº¦ï¼Œä½œä¸ºæ¨¡å‹å‚æ•°ã€å™ªå£°æ°´å¹³å’Œæœ€å¤§æœ‰æ•ˆæ‰©æ•£æ—¶é—´ä»¥åŠä¸€ç³»åˆ—å¾®ç»“æ„å‚æ•°å€¼çš„å‡½æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥æ¨¡å‹çš„é€€åŒ–å’Œæ‹Ÿåˆæ™¯è§‚æ¥é˜æ˜è¿™äº›ç»“æœã€‚åˆ†æäº†HNCæ‚£è€…ä½“å†…æ‹Ÿåˆç»“æœï¼Œå¹¶ç»“åˆæ•°å€¼ç»“æœè¿›è¡Œäº†è§£é‡Šã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°è‡ªç”±æ‰©æ•£ç‡ä¼°è®¡åœ¨å¹¿æ³›çš„å¾®ç»“æ„å‚æ•°èŒƒå›´å†…æ˜¯å‡†ç¡®å’Œç²¾ç¡®çš„ï¼Œè€Œè¡¨é¢ç§¯ä¸ä½“ç§¯æ¯”å’Œè†œé€šé€æ€§çš„å‡†ç¡®æ€§å’Œç²¾ç¡®æ€§åˆ™å–å†³äºå‚æ•°å€¼ã€‚æ­¤å¤–ï¼Œæœ€å¤§æœ‰æ•ˆæ‰©æ•£æ—¶é—´ä¸º200æ¯«ç§’æ—¶ï¼Œè†œé€šé€æ€§ä¼°è®¡çš„åå·®å’Œæ–¹å·®æœ€ä½ã€‚é€šè¿‡æ‹ŸåˆçŸ­æ—¶é—´æé™è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å‚æ•°ä¼°è®¡çš„æ–¹å·®é™ä½ï¼Œå®ç°äº†è†œé€šé€æ€§çš„æ›´å‡†ç¡®å’Œç²¾ç¡®çš„ä¼°è®¡ã€‚ä½“å†…æ‹Ÿåˆç»“æœä¸æ•°å€¼å‘ç°ä¸€è‡´ã€‚æ€»ä¹‹ï¼Œè¿™é¡¹å·¥ä½œæä¾›äº†RWBMåœ¨ä¸´åºŠç¯å¢ƒä¸­åº”ç”¨çš„ç»¼åˆåˆ†æï¼Œå¹¶æœ‰æ½œåŠ›æŒ‡å¯¼è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®è·å–å’Œæ¨¡å‹æ‹Ÿåˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12228v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨åŸºäºéšæœºæ¸¸èµ°éšœç¢æ¨¡å‹ï¼ˆRWBMï¼‰çš„æ—¶é—´ä¾èµ–æ‰©æ•£MRIï¼Œå¯¹å¤´é¢ˆéƒ¨ç™Œç—‡ï¼ˆHNCsï¼‰çš„å¾®ç»“æ„å‚æ•°ï¼ˆè‡ªç”±æ‰©æ•£ç‡ã€ç»†èƒå¤§å°å’Œè†œé€šé€æ€§ï¼‰è¿›è¡Œé‡åŒ–åˆ†æï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ã€‚ç„¶è€Œï¼Œæ¨¡å‹æ‹Ÿåˆä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æµ‹é‡æ¬¡æ•°æœ‰é™ã€ä¿¡å™ªæ¯”ä½ä»¥åŠå¤æ‚çš„éçº¿æ€§ç”Ÿç‰©ç‰©ç†æ¨¡å‹ã€‚æœ¬ç ”ç©¶å…¨é¢æ¢è®¨äº†RWBMæ‹Ÿåˆæ€§èƒ½å¯¹ç»„ç»‡ç‰¹æ€§ã€æ•°æ®é‡‡é›†å’Œå¤„ç†çš„ä¾èµ–æ€§ï¼Œå¹¶æä¾›äº†ä¼˜åŒ–æ•°æ®é‡‡é›†å’Œæ¨¡å‹æ‹Ÿåˆçš„è§è§£ã€‚é€šè¿‡éçº¿æ€§æœ€å°äºŒä¹˜æ³•ï¼Œå¯¹RWBMæ‹Ÿåˆçš„å‡†ç¡®æ€§å’Œç²¾åº¦è¿›è¡Œäº†æ•°å€¼è¯„ä¼°ï¼Œä½œä¸ºæ¨¡å‹å‚æ•°ã€å™ªå£°æ°´å¹³å’Œæœ€å¤§æœ‰æ•ˆæ‰©æ•£æ—¶é—´ç­‰çš„å‡½æ•°ï¼Œå¹¶å¯¹ä¸€ç³»åˆ—å¹¿æ³›çš„å¾®ç»“æ„å‚æ•°å€¼è¿›è¡Œäº†è¯„ä¼°ã€‚é€šè¿‡æ£€æŸ¥æ¨¡å‹çš„é€€åŒ–å’Œæ‹Ÿåˆæ™¯è§‚ï¼Œé˜æ˜äº†è¿™äº›ç»“æœã€‚åˆ†æäº†å¤´é¢ˆéƒ¨ç™Œç—‡æ‚£è€…çš„ä½“å†…æ‹Ÿåˆç»“æœï¼Œå¹¶ç»“åˆæ•°å€¼ç»“æœè¿›è¡Œäº†è§£é‡Šã€‚è§‚å¯Ÿå‘ç°ï¼Œè‡ªç”±æ‰©æ•£ç‡çš„ä¼°è®¡åœ¨å¹¿æ³›çš„å¾®ç»“æ„å‚æ•°èŒƒå›´å†…æ˜¯å‡†ç¡®ä¸”ç²¾ç¡®çš„ï¼Œè€Œè¡¨é¢ç§¯ä¸ä½“ç§¯æ¯”å’Œè†œé€šé€æ€§çš„å‡†ç¡®æ€§å’Œç²¾ç¡®æ€§åˆ™å–å†³äºå‚æ•°å€¼ã€‚æ­¤å¤–ï¼Œæœ€å¤§æœ‰æ•ˆæ‰©æ•£æ—¶é—´ä¸º200æ¯«ç§’æ—¶ï¼Œè†œé€šé€æ€§ä¼°è®¡çš„åå·®å’Œæ–¹å·®æœ€ä½ã€‚é€šè¿‡æ‹ŸåˆçŸ­æ—¶é—´æé™è¡¨è¾¾å¼ï¼Œè§‚å¯Ÿåˆ°å‚æ•°ä¼°è®¡çš„æ–¹å·®é™ä½ï¼Œè†œé€šé€æ€§çš„ä¼°è®¡æ›´ä¸ºå‡†ç¡®å’Œç²¾ç¡®ã€‚ä½“å†…æ‹Ÿåˆç»“æœä¸æ•°å€¼å‘ç°ä¸€è‡´ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶å¯¹RWBMåœ¨ä¸´åºŠè¯•éªŒä¸­çš„æ‹Ÿåˆè¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œæœ‰æœ›ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®é‡‡é›†å’Œæ¨¡å‹æ‹Ÿåˆæ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ol>
<li>ä½¿ç”¨RWBMæ¨¡å‹ç»“åˆæ‰©æ•£MRIåœ¨å¤´é¢ˆéƒ¨ç™Œç—‡å¾®ç»“æ„å‚æ•°é‡åŒ–ä¸Šå–å¾—äº†è‰¯å¥½æˆæœã€‚</li>
<li>æ¨¡å‹æ‹Ÿåˆé¢ä¸´æµ‹é‡æ¬¡æ•°ã€ä¿¡å™ªæ¯”å’Œå¤æ‚ç”Ÿç‰©ç‰©ç†æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†RWBMæ‹Ÿåˆæ€§èƒ½ä¸å¤šç§å› ç´ çš„å…³ç³»ï¼ŒåŒ…æ‹¬ç»„ç»‡ç‰¹æ€§ã€æ•°æ®é‡‡é›†å’Œå¤„ç†ã€‚</li>
<li>æ•°å€¼è¯„ä¼°äº†RWBMçš„å‡†ç¡®æ€§å’Œç²¾åº¦ä¸æ¨¡å‹å‚æ•°ã€å™ªå£°å’Œæ‰©æ•£æ—¶é—´çš„å…³ç³»ã€‚</li>
<li>å‘ç°è‡ªç”±æ‰©æ•£ç‡çš„ä¼°è®¡åœ¨ä¸€å®šèŒƒå›´å†…æ˜¯å‡†ç¡®ä¸”ç²¾ç¡®çš„ã€‚</li>
<li>æœ€å¤§æœ‰æ•ˆæ‰©æ•£æ—¶é—´ä¸º200æ¯«ç§’æ—¶ï¼Œè†œé€šé€æ€§çš„ä¼°è®¡åå·®å’Œæ–¹å·®æœ€ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfa0febe2707af093626356f62359803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-006d4732728dc92da10382e3bb0f6f4b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae5ea89989805d02f0cf79d2c2aa9f32.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  TTSOps A Closed-Loop Corpus Optimization Framework for Training   Multi-Speaker TTS Models from Dark Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ecca4c96afd842b656d17e9af142efbc.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
