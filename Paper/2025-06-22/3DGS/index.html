<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Particle-Grid Neural Dynamics for Learning Deformable Object Models from   RGB-D Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f4f31c71b13baf35ade11b902689c1d3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="Particle-Grid-Neural-Dynamics-for-Learning-Deformable-Object-Models-from-RGB-D-Videos"><a href="#Particle-Grid-Neural-Dynamics-for-Learning-Deformable-Object-Models-from-RGB-D-Videos" class="headerlink" title="Particle-Grid Neural Dynamics for Learning Deformable Object Models from   RGB-D Videos"></a>Particle-Grid Neural Dynamics for Learning Deformable Object Models from   RGB-D Videos</h2><p><strong>Authors:Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li</strong></p>
<p>Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects â€“ such as ropes, cloths, stuffed animals, and paper bags â€“ from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at <a target="_blank" rel="noopener" href="https://kywind.github.io/pgnd">https://kywind.github.io/pgnd</a> . </p>
<blockquote>
<p>å¯¹å¯å˜ç‰©ä½“çš„åŠ¨æ€è¿›è¡Œå»ºæ¨¡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å¤šæ ·çš„ç‰©ç†å±æ€§ä»¥åŠä»æœ‰é™çš„è§†è§‰ä¿¡æ¯ä¸­ä¼°è®¡çŠ¶æ€çš„éš¾åº¦ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆç‰©ä½“ç²’å­å’Œç©ºé—´ç½‘æ ¼çš„æ··åˆè¡¨ç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§ç¥ç»åŠ¨åŠ›å­¦æ¡†æ¶ã€‚æˆ‘ä»¬çš„ç²’å­ç½‘æ ¼æ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¨å±€å½¢çŠ¶å’Œè¿åŠ¨ä¿¡æ¯ï¼ŒåŒæ—¶é¢„æµ‹ç²’å­å¯†é›†è¿åŠ¨ï¼Œä»è€Œèƒ½å¤Ÿå¯¹å„ç§å½¢çŠ¶å’Œææ–™çš„ç‰©ä½“è¿›è¡Œå»ºæ¨¡ã€‚ç²’å­ä»£è¡¨ç‰©ä½“çš„å½¢çŠ¶ï¼Œè€Œç©ºé—´ç½‘æ ¼åˆ™å°†ä¸‰ç»´ç©ºé—´ç¦»æ•£åŒ–ï¼Œä»¥ç¡®ä¿ç©ºé—´è¿ç»­æ€§å¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚ç»“åˆç”¨äºè§†è§‰æ¸²æŸ“çš„é«˜æ–¯Splattingsï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†åŸºäºå­¦ä¹ çš„å¯å˜ç‰©ä½“çš„æ•°å­—å­ªç”Ÿï¼Œå¹¶ç”Ÿæˆäº†ä¸‰ç»´åŠ¨ä½œæ¡ä»¶è§†é¢‘ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»æœºå™¨äººä¸ç‰©ä½“äº’åŠ¨çš„ç¨€ç–è§†å›¾RGB-Dè®°å½•ä¸­å­¦ä¹ å„ç§ç‰©ä½“çš„åŠ¨æ€ï¼Œä¾‹å¦‚ç»³ç´¢ã€å¸ƒæ–™ã€å¡«å……åŠ¨ç‰©å’Œçº¸è¢‹ç­‰ï¼Œå¹¶èƒ½å¯¹æœªè§è¿‡çš„å®ä¾‹è¿›è¡Œç±»åˆ«çº§åˆ«çš„æ¨å¹¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºå­¦ä¹ å’ŒåŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™ç›¸æœºè§†è§’çš„åœºæ™¯ä¸‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€å­¦æ¨¡å‹åœ¨åŸºäºæ¨¡å‹çš„è§„åˆ’ä¸­çš„å®ç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸­å®ç°ç›®æ ‡æ¡ä»¶ä¸‹çš„ç‰©ä½“æ“ä½œã€‚é¡¹ç›®é¡µé¢å¯åœ¨[<a target="_blank" rel="noopener" href="https://kywind.github.io/pgnd]%E8%AE%BF%E9%97%AE%E3%80%82">https://kywind.github.io/pgnd]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15680v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://kywind.github.io/pgnd">https://kywind.github.io/pgnd</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†ä¸€ä¸ªç¥ç»åŠ¨åŠ›å­¦æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆå¯¹è±¡ç²’å­å’Œç©ºé—´ç½‘æ ¼çš„æ··åˆè¡¨ç¤ºæ¥è§£å†³å˜å½¢ç‰©ä½“å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚è¯¥ç²’å­ç½‘æ ¼æ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¨å±€å½¢çŠ¶å’Œè¿åŠ¨ä¿¡æ¯ï¼ŒåŒæ—¶é¢„æµ‹ç²’å­å¯†é›†è¿åŠ¨ï¼Œå®ç°å¯¹å„ç§å½¢çŠ¶å’Œææ–™ç‰©ä½“çš„å»ºæ¨¡ã€‚é€šè¿‡é«˜æ–¯å–·æº…è¿›è¡Œè§†è§‰æ¸²æŸ“ï¼Œå®ç°åŸºäºå­¦ä¹ çš„å˜å½¢ç‰©ä½“æ•°å­—å­ªç”Ÿï¼Œç”Ÿæˆ3DåŠ¨ä½œæ¡ä»¶è§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»ç¨€ç–è§†è§’çš„RGB-Dè®°å½•ä¸­å­¦ä¹ å„ç§ç‰©ä½“çš„åŠ¨åŠ›å­¦ç‰¹æ€§ï¼ŒåŒ…æ‹¬ç»³ç´¢ã€å¸ƒæ–™ã€å¡«å……ç©å…·å’Œçº¸è¢‹ç­‰ï¼Œå¹¶èƒ½åœ¨æœªè§è¿‡çš„å®ä¾‹ä¸­è¿›è¡Œåˆ†ç±»çº§åˆ«çš„æ¨å¹¿ã€‚è¯¥é¡¹ç›®åœ¨æœ‰é™çš„ç›¸æœºè§†è§’åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨æ¨¡å‹åŸºç¡€è§„åˆ’ä¸­å±•ç¤ºäº†å…¶å­¦ä¹ æ¨¡å‹çš„å®ç”¨æ€§ï¼Œå¯å®ç°ç›®æ ‡æ¡ä»¶ä¸‹çš„ç‰©ä½“æ“ä½œä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ç¥ç»åŠ¨åŠ›å­¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¯¹è±¡ç²’å­å’Œç©ºé—´ç½‘æ ¼çš„æ··åˆè¡¨ç¤ºæ¥è§£å†³å˜å½¢ç‰©ä½“å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚</li>
<li>ç²’å­ç½‘æ ¼æ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¨å±€å½¢çŠ¶å’Œè¿åŠ¨ä¿¡æ¯ï¼Œå¹¶é¢„æµ‹å¯†é›†ç²’å­è¿åŠ¨ï¼Œé€‚ç”¨äºå„ç§å½¢çŠ¶å’Œææ–™çš„ç‰©ä½“å»ºæ¨¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡é«˜æ–¯å–·æº…è¿›è¡Œè§†è§‰æ¸²æŸ“ï¼Œå®ç°äº†åŸºäºå­¦ä¹ çš„å˜å½¢ç‰©ä½“æ•°å­—å­ªç”Ÿä½“çš„åˆ›å»ºï¼Œå¯ç”Ÿæˆ3DåŠ¨ä½œæ¡ä»¶è§†é¢‘ã€‚</li>
<li>æ¨¡å‹èƒ½ä»ç¨€ç–è§†è§’çš„RGB-Dè®°å½•ä¸­å­¦ä¹ å„ç§ç‰©ä½“çš„åŠ¨åŠ›å­¦ç‰¹æ€§ï¼ŒåŒ…æ‹¬ç»³ç´¢ã€å¸ƒæ–™ç­‰æŸ”è½¯ç‰©ä½“ä»¥åŠçº¸è¢‹ç­‰ç¡¬è´¨ç‰©ä½“çš„æ··åˆç±»å‹ã€‚</li>
<li>æ¨¡å‹åœ¨æœªè§è¿‡çš„å®ä¾‹ä¸­èƒ½å¤Ÿè¿›è¡Œåˆ†ç±»çº§åˆ«çš„æ¨å¹¿ï¼Œä¸”å¯¹æœ‰é™çš„ç›¸æœºè§†è§’åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77c1575685773075b1a00efa0f77aaf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f047f77d9758abba0846cad18acc97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a4e6446386f9510946ca01149ad6b22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89409850d203ba268df9e37f25afd5c7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RA-NeRF-Robust-Neural-Radiance-Field-Reconstruction-with-Accurate-Camera-Pose-Estimation-under-Complex-Trajectories"><a href="#RA-NeRF-Robust-Neural-Radiance-Field-Reconstruction-with-Accurate-Camera-Pose-Estimation-under-Complex-Trajectories" class="headerlink" title="RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate   Camera Pose Estimation under Complex Trajectories"></a>RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate   Camera Pose Estimation under Complex Trajectories</h2><p><strong>Authors:Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng</strong></p>
<p>Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged as powerful tools for 3D reconstruction and SLAM tasks. However, their performance depends heavily on accurate camera pose priors. Existing approaches attempt to address this issue by introducing external constraints but fall short of achieving satisfactory accuracy, particularly when camera trajectories are complex. In this paper, we propose a novel method, RA-NeRF, capable of predicting highly accurate camera poses even with complex camera trajectories. Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF with photometric consistency and incorporates flow-driven pose regulation to enhance robustness during initialization and localization. Additionally, RA-NeRF employs an implicit pose filter to capture the camera movement pattern and eliminate the noise for pose estimation. To validate our method, we conduct extensive experiments on the Tanks&amp;Temple dataset for standard evaluation, as well as the NeRFBuster dataset, which presents challenging camera pose trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in both camera pose estimation and visual quality, demonstrating its effectiveness and robustness in scene reconstruction under complex pose trajectories. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰å·²ç»æˆä¸ºä¸‰ç»´é‡å»ºå’ŒSLAMä»»åŠ¡ä¸­çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå‡†ç¡®çš„ç›¸æœºå§¿æ€å…ˆéªŒã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å¼•å…¥å¤–éƒ¨çº¦æŸæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨è¾¾åˆ°æ»¡æ„çš„å‡†ç¡®æ€§æ–¹é¢è¿˜å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›¸æœºè½¨è¿¹å¤æ‚çš„æƒ…å†µä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•RA-NeRFï¼Œå³ä½¿åœ¨å¤æ‚çš„ç›¸æœºè½¨è¿¹ä¸‹ï¼Œä¹Ÿèƒ½é¢„æµ‹å‡ºé«˜åº¦å‡†ç¡®çš„ç›¸æœºå§¿æ€ã€‚éµå¾ªå¢é‡ç®¡é“ï¼ŒRA-NeRFä½¿ç”¨NeRFè¿›è¡Œåœºæ™¯é‡å»ºï¼Œé€šè¿‡å…‰åº¦ä¸€è‡´æ€§å¹¶ç»“åˆæµé©±åŠ¨çš„å§¿æ€è°ƒèŠ‚ï¼Œä»¥æé«˜åˆå§‹åŒ–å’Œå®šä½è¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒRA-NeRFè¿˜é‡‡ç”¨éšå¼å§¿æ€æ»¤æ³¢å™¨æ•æ‰ç›¸æœºè¿åŠ¨æ¨¡å¼ï¼Œä¸ºå§¿æ€ä¼°è®¡æ¶ˆé™¤å™ªå£°ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†çš„Tanks&amp;Templeæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥åŠåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç›¸æœºå§¿æ€è½¨è¿¹çš„NeRFBusteræ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼ŒRA-NeRFåœ¨ç›¸æœºå§¿æ€ä¼°è®¡å’Œè§†è§‰è´¨é‡æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚å§¿æ€è½¨è¿¹ä¸‹çš„åœºæ™¯é‡å»ºçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15242v1">PDF</a> IROS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RA-NeRFæ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§ç”¨äºé¢„æµ‹å¤æ‚ç›¸æœºè½¨è¿¹ä¸‹çš„é«˜ç²¾åº¦ç›¸æœºå§¿æ€çš„æ–°å‹æŠ€æœ¯ã€‚ç»“åˆå¢é‡ç®¡çº¿ã€NeRFé‡å»ºæŠ€æœ¯ã€å…‰ç¨‹ä¸€è‡´æ€§ä»¥åŠæµé‡é©±åŠ¨çš„å§¿æ€è°ƒèŠ‚ï¼ŒRA-NeRFèƒ½å¤Ÿåœ¨è¿›è¡Œåœºæ™¯é‡å»ºæ—¶å¢å¼ºåˆå§‹åŒ–å’Œå®šä½é˜¶æ®µçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨éšå¼å§¿æ€è¿‡æ»¤å™¨æ¥æ•æ‰ç›¸æœºè¿åŠ¨æ¨¡å¼å¹¶æ¶ˆé™¤å§¿æ€ä¼°è®¡ä¸­çš„å™ªå£°ã€‚åœ¨æ ‡å‡†è¯„ä¼°çš„Tanksï¼†Templeæ•°æ®é›†ä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„ç›¸æœºå§¿æ€è½¨è¿¹çš„NeRFBusteræ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒRA-NeRFåœ¨ç›¸æœºå§¿æ€ä¼°è®¡å’Œè§†è§‰è´¨é‡æ–¹é¢å‡è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RA-NeRFè§£å†³äº†NeRFå’Œ3DGSåœ¨3Dé‡å»ºå’ŒSLAMä»»åŠ¡ä¸­å¯¹äºå‡†ç¡®ç›¸æœºå§¿æ€å…ˆéªŒçš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>RA-NeRFé€šè¿‡ç»“åˆå¢é‡ç®¡çº¿ã€NeRFæŠ€æœ¯ã€å…‰ç¨‹ä¸€è‡´æ€§ä»¥åŠæµé‡é©±åŠ¨çš„å§¿æ€è°ƒèŠ‚ï¼Œæå‡äº†åœºæ™¯é‡å»ºçš„ç¨³å¥æ€§ã€‚</li>
<li>éšå¼å§¿æ€è¿‡æ»¤å™¨è¢«ç”¨äºæ•æ‰ç›¸æœºè¿åŠ¨æ¨¡å¼å¹¶æ¶ˆé™¤å§¿æ€ä¼°è®¡ä¸­çš„å™ªå£°ã€‚</li>
<li>åœ¨Tanksï¼†Templeæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒRA-NeRFåœ¨ç›¸æœºå§¿æ€ä¼°è®¡æ–¹é¢å…·æœ‰ä¸šç•Œé¢†å…ˆçš„è¡¨ç°ã€‚</li>
<li>RA-NeRFåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„NeRFBusteræ•°æ®é›†ä¸ŠåŒæ ·è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯´æ˜å…¶é€‚åº”å¤æ‚ç›¸æœºè½¨è¿¹çš„èƒ½åŠ›ã€‚</li>
<li>RA-NeRFæ–¹æ³•åœ¨è§†è§‰è´¨é‡æ–¹é¢è¾¾åˆ°äº†é«˜æ ‡å‡†ï¼Œè¯æ˜äº†å…¶åœ¨åœºæ™¯é‡å»ºä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e4cf4a771b4375b1165b66b3e546aed8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4a121e3e7b9bdec95e09946691308a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a38cd4fa1a959b242541497c1f78c765.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3db55e8ba5cdaf21c42a1f2b224799a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4dbe171fcf30dc9ab90f5b9fb9ea69e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Peering-into-the-Unknown-Active-View-Selection-with-Neural-Uncertainty-Maps-for-3D-Reconstruction"><a href="#Peering-into-the-Unknown-Active-View-Selection-with-Neural-Uncertainty-Maps-for-3D-Reconstruction" class="headerlink" title="Peering into the Unknown: Active View Selection with Neural Uncertainty   Maps for 3D Reconstruction"></a>Peering into the Unknown: Active View Selection with Neural Uncertainty   Maps for 3D Reconstruction</h2><p><strong>Authors:Zhengquan Zhang, Feng Xu, Mengmi Zhang</strong></p>
<p>Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training. </p>
<blockquote>
<p>ä¸€äº›è§†è§’å¤©ç„¶æ¯”å…¶ä»–çš„è§†è§’æä¾›æ›´å¤šä¿¡æ¯ã€‚äººå·¥æ™ºèƒ½ç³»ç»Ÿå¦‚ä½•ç¡®å®šå“ªä¸ªè§†è§’èƒ½ä¸ºå‡†ç¡®é«˜æ•ˆçš„3Då¯¹è±¡é‡å»ºæä¾›æœ€æœ‰ä»·å€¼çš„æ´å¯ŸåŠ›ï¼Ÿ3Dé‡å»ºä¸­çš„ä¸»åŠ¨è§†è§’é€‰æ‹©ï¼ˆAVSï¼‰ä»æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾å‡ºèƒ½å¸¦æ¥æœ€å‡†ç¡®3Dé‡å»ºçš„æœ€å°è§†è§’é›†ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰ä»å½“å‰è§‚å¯Ÿä¸­å­¦ä¹ è¾å°„åœºï¼ˆå¦‚NeRFæˆ–3Dé«˜æ–¯å±•å¸ƒï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªå€™é€‰è§†è§’è®¡ç®—ä¸ç¡®å®šæ€§ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§ç”±åä¸ºUPNetçš„è½»é‡çº§å‰é¦ˆæ·±åº¦ç¥ç»ç½‘ç»œé¢„æµ‹ç¥ç»ä¸ç¡®å®šæ€§åœ°å›¾æ¥å¼•å¯¼çš„æ–°å‹AVSæ–¹æ³•ã€‚UPNetæ¥æ”¶ä¸€ä¸ª3Då¯¹è±¡çš„å•ä¸€è¾“å…¥å›¾åƒï¼Œå¹¶è¾“å‡ºä¸€ä¸ªé¢„æµ‹çš„ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œä»£è¡¨æ‰€æœ‰å¯èƒ½çš„å€™é€‰è§†è§’çš„ä¸ç¡®å®šæ€§å€¼ã€‚æˆ‘ä»¬åˆ©ç”¨ä»è§‚å¯Ÿè®¸å¤šè‡ªç„¶å¯¹è±¡åŠå…¶ç›¸å…³çš„ä¸ç¡®å®šæ€§æ¨¡å¼å¾—å‡ºçš„å¯å‘å¼æ–¹æ³•ï¼Œè®­ç»ƒUPNetå­¦ä¹ ä»è§†è§’å¤–è§‚åˆ°å…¶åº•å±‚ä½“ç§¯è¡¨ç¤ºä¸­çš„ä¸ç¡®å®šæ€§çš„ç›´æ¥æ˜ å°„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èšåˆæ‰€æœ‰å…ˆå‰é¢„æµ‹çš„ç¥ç»ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œä»¥æŠ‘åˆ¶å†—ä½™çš„å€™é€‰è§†è§’ï¼Œå¹¶æœ‰æ•ˆåœ°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’ã€‚ä½¿ç”¨è¿™äº›é€‰å®šçš„è§†è§’ï¼Œæˆ‘ä»¬è®­ç»ƒäº†3Dç¥ç»æ¸²æŸ“æ¨¡å‹ï¼Œå¹¶ä¸å…¶ä»–æœ‰ç«äº‰åŠ›çš„AVSæ–¹æ³•è¯„ä¼°äº†åˆæˆæ–°è§†è§’çš„è´¨é‡ã€‚ä»¤äººç©ç›®çš„æ˜¯ï¼Œå°½ç®¡åªä½¿ç”¨äº†æ¯”ä¸Šç•Œå°‘ä¸€åŠçš„è§†è§’ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»èƒ½è¾¾åˆ°ç›¸å½“çš„é‡å»ºç²¾åº¦ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨AVSæœŸé—´æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾400å€çš„åŠ é€Ÿï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡50%çš„CPUã€RAMå’ŒGPUä½¿ç”¨é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ¶‰åŠæ–°å‹å¯¹è±¡ç±»åˆ«çš„AVSä»»åŠ¡ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14856v1">PDF</a> 9 pages, 3 figures in the main text. Under review for NeurIPS 2025</p>
<p><strong>Summary</strong><br>     å¼•å…¥åä¸ºUPNetçš„è½»é‡çº§å‰é¦ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼Œé¢„æµ‹ç¥ç»ä¸ç¡®å®šæ€§åœ°å›¾ï¼ŒæŒ‡å¯¼ä¸»åŠ¨è§†å›¾é€‰æ‹©ï¼ˆAVSï¼‰è¿›è¡Œ3Dé‡å»ºã€‚UPNetä»å•ä¸€è¾“å…¥å›¾åƒé¢„æµ‹ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œé€šè¿‡å¯å‘å¼æ–¹æ³•å­¦ä¹ ä»è§†ç‚¹å¤–è§‚åˆ°ä½“ç§¯è¡¨ç¤ºä¸­çš„ä¸ç¡®å®šæ€§çš„ç›´æ¥æ˜ å°„ã€‚æ­¤æ–¹æ³•èƒ½æœ‰æ•ˆé€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†ç‚¹ï¼Œé™ä½è®¡ç®—å†—ä½™ï¼Œæé«˜é‡å»ºç²¾åº¦å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥UPNetç¥ç»ç½‘ç»œé¢„æµ‹ç¥ç»ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œç”¨äºæŒ‡å¯¼ä¸»åŠ¨è§†å›¾é€‰æ‹©ï¼ˆAVSï¼‰åœ¨3Dé‡å»ºä¸­çš„è¿‡ç¨‹ã€‚</li>
<li>UPNetèƒ½ä»å•ä¸€è¾“å…¥å›¾åƒé¢„æµ‹ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œå±•ç¤ºæ‰€æœ‰å¯èƒ½å€™é€‰è§†ç‚¹çš„ä¸ç¡®å®šæ€§å€¼ã€‚</li>
<li>é€šè¿‡å¯å‘å¼æ–¹æ³•ï¼ŒUPNetå­¦ä¹ ä»è§†ç‚¹å¤–è§‚åˆ°ä½“ç§¯è¡¨ç¤ºä¸­çš„ä¸ç¡®å®šæ€§çš„ç›´æ¥æ˜ å°„ã€‚</li>
<li>é€šè¿‡å¯¹ä»¥å¾€é¢„æµ‹çš„ç¥ç»ä¸ç¡®å®šæ€§åœ°å›¾è¿›è¡Œèšåˆï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæŠ‘åˆ¶å†—ä½™çš„å€™é€‰è§†ç‚¹ï¼Œå¹¶æœ‰æ•ˆåœ°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†ç‚¹ã€‚</li>
<li>ä½¿ç”¨é€‰å®šçš„è§†ç‚¹è®­ç»ƒ3Dç¥ç»æ¸²æŸ“æ¨¡å‹ï¼Œå¹¶åœ¨ä¸å…¶ä»–ç«äº‰æ€§AVSæ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡çš„å…¨æ–°è§†å›¾åˆæˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨ä¸€åŠè§†ç‚¹çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸ä¸Šé™ç›¸å½“çš„é‡å»ºç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb9e8bd04c47cc5dcb8a174e6e0db07d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d415201cf6b76c2cba7c8f55b5c3718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ada5332370ef12d4a92827ed8049071.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SyncTalk-High-Fidelity-and-Efficient-Synchronized-Talking-Heads-Synthesis-Using-Gaussian-Splatting"><a href="#SyncTalk-High-Fidelity-and-Efficient-Synchronized-Talking-Heads-Synthesis-Using-Gaussian-Splatting" class="headerlink" title="SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads   Synthesis Using Gaussian Splatting"></a>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads   Synthesis Using Gaussian Splatting</h2><p><strong>Authors:Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, Zhaoxin Fan</strong></p>
<p>Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the â€˜â€™devilâ€™â€™ in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: <a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk++">https://ziqiaopeng.github.io/synctalk++</a>. </p>
<blockquote>
<p>åœ¨å®ç°é«˜åº¦åŒæ­¥çš„é€¼çœŸè¯­éŸ³é©±åŠ¨å¼è¯´è¯äººå¤´è§†é¢‘åˆæˆä¸­ï¼Œå­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚ä¸€ä¸ªé€¼çœŸçš„è¯´è¯äººå¤´éœ€è¦åŒæ­¥åè°ƒä¸»ä½“èº«ä»½ã€å˜´å”‡åŠ¨ä½œã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿ã€‚ç¼ºä¹è¿™äº›åŒæ­¥æ˜¯æ ¹æœ¬ç¼ºé™·ï¼Œä¼šå¯¼è‡´ä¸çœŸå®çš„ç»“æœã€‚ä¸ºäº†è§£å†³åŒæ­¥è¿™ä¸€å…³é”®é—®é¢˜ï¼Œè¢«è§†ä¸ºåˆ›å»ºé€¼çœŸè¯´è¯äººçš„â€œé­”é¬¼â€ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SyncTalk++ï¼Œå®ƒå…·å¤‡åŠ¨æ€è‚–åƒæ¸²æŸ“å™¨å’Œé«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ä¸»ä½“èº«ä»½çš„ä¸€è‡´æ€§å’Œé¢éƒ¨åŒæ­¥æ§åˆ¶å™¨ï¼Œè¯¥æ§åˆ¶å™¨ä½¿å˜´å”‡åŠ¨ä½œä¸è¯­éŸ³å¯¹é½ï¼ŒåŒæ—¶åˆ›æ–°åœ°ä½¿ç”¨3Dé¢éƒ¨æ··åˆå½¢çŠ¶æ¨¡å‹æ¥é‡å»ºå‡†ç¡®çš„é¢éƒ¨è¡¨æƒ…ã€‚ä¸ºäº†ç¡®ä¿å¤´éƒ¨è‡ªç„¶ç§»åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†å¤´éƒ¨åŒæ­¥ç¨³å®šå™¨ï¼Œå®ƒä¼˜åŒ–äº†å¤´éƒ¨å§¿åŠ¿ï¼Œæé«˜äº†ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒSyncTalk++é€šè¿‡å¼•å…¥è¡¨æƒ…ç”Ÿæˆå™¨å’Œèº¯å¹²æ¢å¤å™¨ï¼Œå¢å¼ºäº†å¤„ç†ç¦»ç¾¤å€¼éŸ³é¢‘çš„ç¨³å¥æ€§ï¼Œç”Ÿæˆä¸è¯­éŸ³ç›¸åŒ¹é…çš„è¡¨æƒ…å’Œæ— ç¼çš„èº¯å¹²åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†è·¨å¸§çš„è§†è§‰ç»†èŠ‚çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ï¼Œå¹¶å¤§å¤§æé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡ï¼Œè¾¾åˆ°äº†æ¯ç§’101å¸§ã€‚å¤§é‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒSyncTalk++åœ¨åŒæ­¥å’Œé€¼çœŸåº¦æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æ¨èè§‚çœ‹è¡¥å……è§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk++">https://ziqiaopeng.github.io/synctalk++</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14742v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åœ¨åˆæˆé€¼çœŸçš„è¯­éŸ³é©±åŠ¨è°ˆè¯å¤´è§†é¢‘æ—¶å®ç°é«˜åŒæ­¥åŒ–çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†SyncTalk++æŠ€æœ¯ï¼ŒåŒ…æ‹¬åŠ¨æ€è‚–åƒæ¸²æŸ“å™¨ã€é¢éƒ¨åŒæ­¥æ§åˆ¶å™¨å’Œå¤´éƒ¨åŒæ­¥ç¨³å®šå™¨ã€‚è¿™äº›æŠ€æœ¯ç¡®ä¿äº†ä¸»ä½“èº«ä»½çš„åŒæ­¥åè°ƒã€å”‡éƒ¨åŠ¨ä½œä¸è¯­éŸ³çš„å¯¹é½ã€é¢éƒ¨è¡¨æƒ…çš„å‡†ç¡®é‡å»ºä»¥åŠå¤´éƒ¨å§¿æ€çš„è‡ªç„¶ä¼˜åŒ–ã€‚SyncTalk++è¿˜å¢å¼ºäº†å¯¹äºç¦»ç¾¤éŸ³é¢‘çš„ç¨³å¥æ€§ï¼Œé€šè¿‡è¡¨æƒ…ç”Ÿæˆå™¨å’Œèº¯å¹²æ¢å¤å™¨ç”Ÿæˆä¸è¯­éŸ³ç›¸åŒ¹é…çš„è¡¨æƒ…å’Œæ— ç¼çš„èº¯å¹²åŒºåŸŸã€‚SyncTalk++ä¿æŒäº†è·¨å¸§çš„è§†è§‰ç»†èŠ‚çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œç”»è´¨ã€‚å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒSyncTalk++åœ¨åŒæ­¥å’Œé€¼çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncTalk++æŠ€æœ¯è§£å†³äº†åˆæˆè¯­éŸ³é©±åŠ¨è°ˆè¯å¤´è§†é¢‘æ—¶çš„é«˜åŒæ­¥åŒ–æŒ‘æˆ˜ã€‚</li>
<li>åŠ¨æ€è‚–åƒæ¸²æŸ“å™¨ä½¿ç”¨é«˜æ–¯å–·ç»˜æŠ€æœ¯ç¡®ä¿ä¸»ä½“èº«ä»½çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ã€‚</li>
<li>é¢éƒ¨åŒæ­¥æ§åˆ¶å™¨é€šè¿‡å¯¹å”‡éƒ¨åŠ¨ä½œã€è¯­éŸ³å’Œé¢éƒ¨è¡¨æƒ…çš„åè°ƒï¼Œå®ç°å‡†ç¡®çš„åŒæ­¥ã€‚</li>
<li>å¤´éƒ¨åŒæ­¥ç¨³å®šå™¨ä¼˜åŒ–å¤´éƒ¨å§¿æ€ï¼Œä½¿å¤´éƒ¨åŠ¨ä½œæ›´è‡ªç„¶ã€‚</li>
<li>SyncTalk++å¢å¼ºäº†å¯¹äºç¦»ç¾¤éŸ³é¢‘çš„ç¨³å¥æ€§ï¼Œé€šè¿‡è¡¨æƒ…ç”Ÿæˆå™¨å’Œèº¯å¹²æ¢å¤å™¨åº”å¯¹è¯­éŸ³åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>SyncTalk++æé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œç”»è´¨ï¼Œå®ç°äº†é«˜å¸§ç‡è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15c66b633743e877bdd3801913d82173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037c4b1b81c911d27bb468c64ce77801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c30f4d6defa9eea186e6506bc2cf0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff62d47e48bac53ae737a1a6efbdfb97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a0490a95147fedee062bab74ef0f2b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3DGS-IEval-15K-A-Large-scale-Image-Quality-Evaluation-Database-for-3D-Gaussian-Splatting"><a href="#3DGS-IEval-15K-A-Large-scale-Image-Quality-Evaluation-Database-for-3D-Gaussian-Splatting" class="headerlink" title="3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D   Gaussian-Splatting"></a>3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D   Gaussian-Splatting</h2><p><strong>Authors:Yuke Xing, Jiarui Wang, Peizhi Niu, Wenjie Huang, Guangtao Zhai, Yiling Xu</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at <a target="_blank" rel="noopener" href="https://github.com/YukeXing/3DGS-IEval-15K">https://github.com/YukeXing/3DGS-IEval-15K</a>. </p>
<blockquote>
<p>3Dé«˜æ–¯ç‚¹ææŠ€æœ¯ï¼ˆ3DGSï¼‰ä½œä¸ºæ–°é¢–çš„è§†ç‚¹åˆæˆæ–¹æ³•ï¼Œå…·æœ‰å®æ—¶æ¸²æŸ“å’Œé«˜è§†è§‰ä¿çœŸåº¦çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„å­˜å‚¨éœ€æ±‚ä¸ºå®é™…åº”ç”¨å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å½“å‰å…ˆè¿›çš„3DGSæ–¹æ³•è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨äº†ä¸“ç”¨å‹ç¼©æ¨¡å—ï¼Œä½†ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°å…¶å¯¹æ„ŸçŸ¥çš„å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†3DGS-IEval-15Kï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºå‹ç¼©çš„3DGSè¡¨ç¤ºè®¾è®¡çš„å¤§å‹å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä»10ä¸ªçœŸå®åœºæ™¯ä¸­æ¸²æŸ“çš„15,200å¼ å›¾åƒï¼Œé€šè¿‡6ç§å…·æœ‰ä»£è¡¨æ€§çš„3DGSç®—æ³•åœ¨20ä¸ªæˆ˜ç•¥é€‰æ‹©çš„è§‚ç‚¹è¿›è¡Œæ¸²æŸ“ï¼Œä¸åŒçš„å‹ç¼©æ°´å¹³å¯¼è‡´å„ç§å¤±çœŸæ•ˆæœã€‚é€šè¿‡å—æ§çš„ä¸»è§‚å®éªŒï¼Œæˆ‘ä»¬ä»60åè§‚ä¼—é‚£é‡Œæ”¶é›†äº†äººç±»æ„ŸçŸ¥æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡åœºæ™¯å¤šæ ·æ€§å’Œå¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰åˆ†å¸ƒåˆ†æéªŒè¯äº†æ•°æ®é›†çš„è´¨é‡ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåŒ…å«30ä¸ªä»£è¡¨æ€§IQAæŒ‡æ ‡çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚ä½œä¸ºè¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„3DGSè´¨é‡è¯„ä¼°æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºå¼€å‘ä¸“é—¨çš„3DGS IQAæŒ‡æ ‡æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºç ”ç©¶ç‹¬ç‰¹çš„ä¸è§†ç‚¹ç›¸å…³çš„è´¨é‡åˆ†å¸ƒæ¨¡å¼æä¾›äº†å¿…è¦çš„æ•°æ®ï¼Œè¿™äº›æ¨¡å¼åœ¨3DGSä¸­æ˜¯ç‹¬ç‰¹çš„ã€‚æ•°æ®åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YukeXing/3DGS-IEval-15K%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/YukeXing/3DGS-IEval-15Kå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14642v1">PDF</a> </p>
<p><strong>Summary</strong><br>     3DGS-IEval-15Kæ•°æ®é›†ç”¨äºè¯„ä¼°å‹ç¼©3DGSè¡¨ç¤ºçš„å›¾åƒè´¨é‡ï¼ŒåŒ…å«15,200å¼ å›¾åƒï¼Œæ¶µç›–å¤šç§å‹ç¼©çº§åˆ«å’Œå¤±çœŸæ•ˆåº”ã€‚è¯¥æ•°æ®é›†ä¸ºå¼€å‘é’ˆå¯¹3DGSçš„ä¸“é—¨å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code>* 3D Gaussian Splatting (3DGS)æ˜¯ä¸€ç§ç”¨äºæ–°å‹è§†å›¾åˆæˆçš„æ–¹æ³•ï¼Œå…·æœ‰å®æ—¶æ¸²æŸ“å’Œé«˜è§†è§‰ä¿çœŸåº¦çš„ç‰¹ç‚¹ã€‚
* 3DGSå­˜åœ¨å¤§é‡å­˜å‚¨éœ€æ±‚çš„é—®é¢˜ï¼Œè¿™å¯¹å®é™…åº”ç”¨æå‡ºäº†æŒ‘æˆ˜ã€‚
* å½“å‰å…ˆè¿›çš„3DGSæ–¹æ³•è™½ç„¶ä¼šç»“åˆä¸“é—¨çš„å‹ç¼©æ¨¡å—ï¼Œä½†ç¼ºä¹ä¸€ä¸ªç»¼åˆæ¡†æ¶æ¥è¯„ä¼°å®ƒä»¬çš„æ„ŸçŸ¥å½±å“ã€‚
* å¼•å…¥æ–°çš„æ•°æ®é›†3DGS-IEval-15Kï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºå‹ç¼©çš„3DGSè¡¨ç¤ºè®¾è®¡çš„ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ•°æ®é›†ã€‚
* æ•°æ®é›†åŒ…å«ä»çœŸå®ä¸–ç•Œçš„åä¸ªåœºæ™¯ä¸­æ¸²æŸ“å‡ºçš„å›¾åƒï¼Œå¹¶é€šè¿‡å…­ç§ä»£è¡¨æ€§çš„3DGSç®—æ³•å’ŒäºŒåä¸ªæˆ˜ç•¥é€‰æ‹©çš„è§‚ç‚¹è¿›è¡Œå±•ç¤ºã€‚
* æ•°æ®é›†åŒ…å«äº†ä¸åŒå‹ç¼©çº§åˆ«å¯¼è‡´çš„å„ç§å¤±çœŸæ•ˆåº”ï¼Œå¹¶é€šè¿‡æ§åˆ¶ä¸»è§‚å®éªŒæ”¶é›†äººç±»æ„ŸçŸ¥æ•°æ®ã€‚
* æ•°æ®é›†è´¨é‡é€šè¿‡åœºæ™¯å¤šæ ·æ€§å’Œå¹³å‡æ„è§å¾—åˆ†åˆ†å¸ƒåˆ†æè¿›è¡ŒéªŒè¯ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸‰åç§ä»£è¡¨æ€§çš„IQAæŒ‡æ ‡ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ca79fc90f71bdf3df786a0c89b59ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cba82ff55decd4d4fcab6ddefb2e3f9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3a7151614255eeaa329a8ba2202409d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0edf92288986bf260fd4144cf07a4992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8596a1df207fe2b489bcf0f1f741d15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-406632609d18c2ce8551a1fc098e9650.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ImmerseGen-Agent-Guided-Immersive-World-Generation-with-Alpha-Textured-Proxies"><a href="#ImmerseGen-Agent-Guided-Immersive-World-Generation-with-Alpha-Textured-Proxies" class="headerlink" title="ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured   Proxies"></a>ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured   Proxies</h2><p><strong>Authors:Jinyan Yuan, Bangbang Yang, Keke Wang, Panwang Pan, Lin Ma, Xuehai Zhang, Xiao Liu, Zhaopeng Cui, Yuewen Ma</strong></p>
<p>Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery. This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: <a target="_blank" rel="noopener" href="https://immersegen.github.io/">https://immersegen.github.io</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åˆ›å»ºç”¨äºæ²‰æµ¸å¼è™šæ‹Ÿç°å®ï¼ˆVRï¼‰çš„ä¸‰ç»´åœºæ™¯ä¸€ç›´æ˜¯æ•°åå¹´çš„é‡å¤§ç ”ç©¶é‡ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé«˜å¤šè¾¹å½¢ç½‘æ ¼å»ºæ¨¡å’Œåç®€åŒ–çš„æ–¹æ³•ï¼Œæˆ–è€…å¤§è§„æ¨¡çš„ä¸‰ç»´é«˜æ–¯æ¨¡å‹ï¼Œè¿™å¯¼è‡´äº†å¤æ‚çš„æµç¨‹æˆ–æœ‰é™çš„è§†è§‰é€¼çœŸåº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å®ç°å¼•äººæ³¨ç›®çš„æ²‰æµ¸å¼ä½“éªŒå¹¶ä¸éœ€è¦å¦‚æ­¤è¯¦å°½çš„å»ºæ¨¡ã€‚æˆ‘ä»¬ä»‹ç»äº†ImmerseGenï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç´§å‡‘å’Œé€¼çœŸçš„ä¸–ç•Œå»ºæ¨¡çš„æ–°å‹ä»£ç†å¼•å¯¼æ¡†æ¶ã€‚ImmerseGenå°†åœºæ™¯è¡¨ç¤ºä¸ºè½»é‡çº§å‡ ä½•ä»£ç†çš„å±‚æ¬¡ç»“æ„ç»„åˆï¼Œå³ç®€åŒ–çš„åœ°å½¢å’Œå‘Šç¤ºç‰Œç½‘æ ¼ï¼Œå¹¶é€šè¿‡åœ¨è¿™äº›ä»£ç†ä¸ŠåˆæˆRGBAçº¹ç†æ¥äº§ç”Ÿé€¼çœŸçš„å¤–è§‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨æˆ·ä¸ºä¸­å¿ƒçš„åŸºåœ°ä¸–ç•Œåˆæˆçš„åœ°å½¢æ¡ä»¶çº¹ç†åˆæˆæŠ€æœ¯ï¼Œä»¥åŠç”¨äºä¸­æ™¯å’Œå‰æ™¯åœºæ™¯çš„RGBAèµ„äº§çº¹ç†åˆæˆæŠ€æœ¯ã€‚è¿™ç§é‡æ–°è¡¨è¿°æä¾›äº†å‡ ä¸ªä¼˜ç‚¹ï¼šï¼ˆiï¼‰å®ƒç®€åŒ–äº†å»ºæ¨¡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿå¼•å¯¼ç”Ÿæˆæ¨¡å‹äº§ç”Ÿæ— ç¼é›†æˆåœºæ™¯çš„è¿è´¯çº¹ç†ï¼›ï¼ˆiiï¼‰å®ƒé€šè¿‡ç›´æ¥åœ¨ä»£ç†ä¸Šåˆæˆé€¼çœŸçš„çº¹ç†ç»•è¿‡å¤æ‚çš„å‡ ä½•åˆ›å»ºå’Œåˆ å‡ï¼Œä»è€Œä¿æŒè§†è§‰è´¨é‡è€Œä¸é™è´¨ï¼›ï¼ˆiiiï¼‰å®ƒèƒ½å¤Ÿå®ç°é€‚ç”¨äºç§»åŠ¨VRè€³æœºå®æ—¶æ¸²æŸ“çš„ç´§å‡‘è¡¨ç¤ºã€‚ä¸ºäº†ä»æ–‡æœ¬æç¤ºè‡ªåŠ¨åˆ›å»ºåœºæ™¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºVLMçš„å»ºæ¨¡ä»£ç†ï¼Œé€šè¿‡åŸºäºè¯­ä¹‰ç½‘æ ¼çš„åˆ†æå¢å¼ºç©ºé—´æ¨ç†å’Œå‡†ç¡®çš„èµ„äº§æ”¾ç½®ã€‚ImmerseGenè¿˜é€šè¿‡æ·»åŠ åŠ¨æ€æ•ˆæœå’Œç¯ç»•å£°éŸ³æ¥ä¸°å¯Œåœºæ™¯ï¼Œä»¥æ”¯æŒå¤šæ„Ÿå®˜æ²‰æµ¸ã€‚åœºæ™¯ç”Ÿæˆå’Œå®æ—¶VRå±•ç¤ºçš„å®éªŒè¡¨æ˜ï¼ŒImmerseGenåœ¨é€¼çœŸåº¦ã€ç©ºé—´è¿è´¯æ€§å’Œæ¸²æŸ“æ•ˆç‡æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚é¡¹ç›®ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="https://immersegen.github.io./">https://immersegen.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14315v2">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://immersegen.github.io/">https://immersegen.github.io</a></p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ä»£ç†å¼•å¯¼æ¡†æ¶ImmerseGenï¼Œç”¨äºç´§å‡‘ä¸”é€¼çœŸçš„ä¸–ç•Œå»ºæ¨¡ã€‚å®ƒç®€åŒ–äº†è™šæ‹Ÿåœºæ™¯åˆ›å»ºè¿‡ç¨‹ï¼Œé€šè¿‡å±‚æ¬¡ç»“æ„ç»„åˆè½»é‡çº§å‡ ä½•ä»£ç†ï¼Œå¦‚ç®€åŒ–åœ°å½¢å’Œæ‹›ç‰Œç½‘æ ¼ï¼Œå¹¶åˆæˆRGBAçº¹ç†è´´å›¾å®ç°é€¼çœŸå¤–è§‚ã€‚å¼•å…¥åœ°å½¢æ¡ä»¶çº¹ç†è´´å›¾å’ŒRGBAèµ„äº§çº¹ç†è´´å›¾ï¼Œå®ç°ç”¨æˆ·ä¸­å¿ƒåŒ–åŸºç¡€ä¸–ç•ŒåˆæˆåŠä¸­æ™¯å’Œå‰æ™¯é£æ™¯çš„åˆæˆã€‚è¯¥é¡¹ç›®è¿˜è‡´åŠ›äºé€šè¿‡æ–‡æœ¬æç¤ºè‡ªåŠ¨åˆ›å»ºåœºæ™¯ï¼Œå¼•å…¥åŸºäºVLMçš„å»ºæ¨¡ä»£ç†ï¼Œå¹¶ç»“åˆè¯­ä¹‰ç½‘æ ¼åˆ†ææ”¹å–„ç©ºé—´æ¨ç†å’Œèµ„äº§æ”¾ç½®ã€‚ImmerseGenè¿˜ä¸°å¯Œäº†åœºæ™¯çš„åŠ¨æ€æ•ˆæœå’Œç¯ç»•éŸ³é¢‘ï¼Œæ”¯æŒå¤šæ„Ÿå®˜æ²‰æµ¸ã€‚å®éªŒè¡¨æ˜ï¼ŒImmerseGenåœ¨åœºæ™¯ç”Ÿæˆå’Œå®æ—¶VRå±•ç¤ºä¸­å®ç°äº†è¾ƒé«˜çš„é€¼çœŸåº¦ã€ç©ºé—´è¿è´¯æ€§å’Œæ¸²æŸ“æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ImmerseGenæ˜¯ä¸€ä¸ªæ–°é¢–çš„ä»£ç†å¼•å¯¼æ¡†æ¶ï¼Œç”¨äºç´§å‡‘ä¸”çœŸå®åœ°æ¨¡æ‹Ÿä¸–ç•Œåœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•ç®€åŒ–äº†è™šæ‹Ÿåœºæ™¯åˆ›å»ºæµç¨‹ï¼Œé€šè¿‡å±‚æ¬¡ç»“æ„ç»„åˆè½»é‡çº§å‡ ä½•ä»£ç†å®ç°ã€‚</li>
<li>ImmerseGenå¼•å…¥äº†åœ°å½¢æ¡ä»¶çº¹ç†è´´å›¾å’ŒRGBAèµ„äº§çº¹ç†è´´å›¾æŠ€æœ¯ï¼Œç”¨äºåˆæˆç”¨æˆ·ä¸­å¿ƒåŒ–åŸºç¡€ä¸–ç•ŒåŠä¸­æ™¯å’Œå‰æ™¯ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬æç¤ºå¯è‡ªåŠ¨åˆ›å»ºåœºæ™¯ï¼Œé‡‡ç”¨VLMå»ºæ¨¡ä»£ç†å¹¶ç»“åˆè¯­ä¹‰ç½‘æ ¼åˆ†ææ”¹å–„ç©ºé—´æ¨ç†å’Œèµ„äº§æ”¾ç½®ã€‚</li>
<li>ImmerseGenä¸°å¯Œäº†åœºæ™¯çš„åŠ¨æ€æ•ˆæœå’Œç¯ç»•éŸ³é¢‘ï¼Œå¢å¼ºäº†å¤šæ„Ÿå®˜æ²‰æµ¸ä½“éªŒã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒImmerseGenåœ¨åœºæ™¯ç”Ÿæˆå’Œå®æ—¶VRå±•ç¤ºæ–¹é¢å…·æœ‰è¾ƒé«˜çš„é€¼çœŸåº¦ã€ç©ºé—´è¿è´¯æ€§å’Œæ¸²æŸ“æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02dfbec754a7b94a0665573e866b010b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3f8c58244f3ee325370287b2217ccee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe106a590b4b4bb9c457b4c1cb8b16a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b8de25682f4fd05dd849c379a7f6e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d59c443c8f6e2194faaf94e7668e084.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f5d5679d175719cfa432560f37dc16a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HRGS-Hierarchical-Gaussian-Splatting-for-Memory-Efficient-High-Resolution-3D-Reconstruction"><a href="#HRGS-Hierarchical-Gaussian-Splatting-for-Memory-Efficient-High-Resolution-3D-Reconstruction" class="headerlink" title="HRGS: Hierarchical Gaussian Splatting for Memory-Efficient   High-Resolution 3D Reconstruction"></a>HRGS: Hierarchical Gaussian Splatting for Memory-Efficient   High-Resolution 3D Reconstruction</h2><p><strong>Authors:Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, Baochang Zhang</strong></p>
<p>3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks. </p>
<blockquote>
<p>3Dé«˜æ–¯Splattingï¼ˆ3DGSï¼‰åœ¨å®æ—¶3Dåœºæ™¯é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹å­˜åœ¨å†…å­˜å¯æ‰©å±•æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚é«˜æ–¯Splattingï¼ˆHRGSï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰åˆ†å±‚å—çº§ä¼˜åŒ–çš„å†…å­˜é«˜æ•ˆæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»ä½åˆ†è¾¨ç‡æ•°æ®ä¸­ç”Ÿæˆå…¨å±€ç²—ç•¥çš„é«˜æ–¯è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åœºæ™¯åˆ†å‰²æˆå¤šä¸ªå—ï¼Œä½¿ç”¨é«˜åˆ†è¾¨ç‡æ•°æ®å¯¹æ¯ä¸ªå—è¿›è¡Œç»†åŒ–ã€‚åˆ†åŒºæ¶‰åŠä¸¤ä¸ªæ­¥éª¤ï¼šé«˜æ–¯åˆ†åŒºï¼Œå°†ä¸è§„åˆ™åœºæ™¯å½’ä¸€åŒ–åˆ°å…·æœ‰ç»Ÿä¸€ç½‘æ ¼çš„æœ‰ç•Œç«‹æ–¹ç©ºé—´ä¸­ä»¥è¿›è¡Œä»»åŠ¡åˆ†é…ï¼›ä»¥åŠè®­ç»ƒæ•°æ®åˆ†åŒºï¼Œåªä¿ç•™æ¯ä¸ªå—çš„ç›¸å…³è§‚æµ‹å€¼ã€‚é€šè¿‡ç”¨ç²—ç•¥çš„é«˜æ–¯å…ˆéªŒå¼•å¯¼å—ç»†åŒ–ï¼Œæˆ‘ä»¬ç¡®ä¿ç›¸é‚»å—ä¹‹é—´çš„æ— ç¼é«˜æ–¯èåˆã€‚ä¸ºäº†å‡å°‘è®¡ç®—éœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†é‡è¦æ€§é©±åŠ¨çš„é«˜æ–¯ä¿®å‰ªï¼ˆIDGPï¼‰ï¼Œå®ƒè®¡ç®—æ¯ä¸ªé«˜æ–¯çš„é‡è¦æ€§å¾—åˆ†å¹¶åˆ é™¤è´¡çŒ®æœ€å°çš„éƒ¨åˆ†ï¼Œä»è€ŒåŠ å¿«æ”¶æ•›å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†æ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹çš„æ³•çº¿å…ˆéªŒï¼Œä»¥æé«˜è¡¨é¢é‡å»ºè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å†…å­˜é™åˆ¶ä¸‹å®ç°é«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡çš„3Dåœºæ™¯é‡å»ºã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHRGSåœ¨é«˜åˆ†è¾¨ç‡æ–°é¢–è§†å›¾åˆæˆï¼ˆNVSï¼‰å’Œè¡¨é¢é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14229v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåˆ†å±‚é«˜æ–¯é‡‡æ ·ï¼ˆHRGSï¼‰çš„å®æ—¶ä¸‰ç»´åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œè§£å†³äº†é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹å†…å­˜å¯æ‰©å±•æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå…¨å±€ç²—ç•¥é«˜æ–¯è¡¨ç¤ºï¼Œå¯¹åœºæ™¯è¿›è¡Œåˆ†å±‚å—çº§ä¼˜åŒ–ã€‚é¦–å…ˆé€šè¿‡ä½åˆ†è¾¨ç‡æ•°æ®å¾—åˆ°åˆæ­¥ä¼°è®¡ï¼Œç„¶åå°†åœºæ™¯åˆ†å‰²æˆå¤šä¸ªå—å¹¶åˆ©ç”¨é«˜åˆ†è¾¨ç‡æ•°æ®è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚å¼•å…¥é‡è¦æ€§é©±åŠ¨çš„é«˜æ–¯å‰ªæç­–ç•¥æ¥åŠ é€Ÿæ”¶æ•›å’Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡åœºæ™¯è¡¨é¢é‡å»ºè´¨é‡å¹¶åº”ç”¨äºé«˜è´¨é‡é«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´åœºæ™¯é‡å»ºä»»åŠ¡ä¸­ï¼Œå±•ç°å‡ºæœ€ä¼˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HRGSè§£å†³äº†é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹å®æ—¶ä¸‰ç»´åœºæ™¯é‡å»ºä¸­çš„å†…å­˜å¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>HRGSé€šè¿‡ç”Ÿæˆå…¨å±€ç²—ç•¥é«˜æ–¯è¡¨ç¤ºå’Œåˆ†å±‚å—çº§ä¼˜åŒ–æ¥å¤„ç†åœºæ™¯ã€‚</li>
<li>æ–¹æ³•åœ¨ä½åˆ†è¾¨ç‡æ•°æ®åŸºç¡€ä¸Šå¼•å…¥é«˜åˆ†è¾¨ç‡æ•°æ®è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>é€šè¿‡é«˜æ–¯åˆ†åŒºå’Œè®­ç»ƒæ•°æ®åˆ†åŒºä¸¤æ­¥å®ç°åœºæ™¯åˆ†åŒºã€‚</li>
<li>å¼•å…¥é‡è¦æ€§é©±åŠ¨çš„é«˜æ–¯å‰ªæç­–ç•¥æ¥å‡å°‘è®¡ç®—éœ€æ±‚å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³•çº¿å…ˆéªŒæå‡è¡¨é¢é‡å»ºè´¨é‡ã€‚</li>
<li>HRGSåœ¨æ–°å‹è§†æ™¯åˆæˆå’Œè¡¨é¢é‡å»ºä»»åŠ¡ä¸­å®ç°æœ€ä¼˜æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9d2fca7f5187decdf72b69128ffb7d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4f31c71b13baf35ade11b902689c1d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b58be1dbe13cafc42f51e4b0e154a283.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GAF-Gaussian-Action-Field-as-a-Dvnamic-World-Model-for-Robotic-Mlanipulation"><a href="#GAF-Gaussian-Action-Field-as-a-Dvnamic-World-Model-for-Robotic-Mlanipulation" class="headerlink" title="GAF: Gaussian Action Field as a Dvnamic World Model for Robotic   Mlanipulation"></a>GAF: Gaussian Action Field as a Dvnamic World Model for Robotic   Mlanipulation</h2><p><strong>Authors:Ying Chai, Litao Deng, Ruizhi Shao, Jiajun Zhang, Liangjun Xing, Hongwen Zhang, Yebin Liu</strong></p>
<p>Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: <a target="_blank" rel="noopener" href="http://chaiying1.github.io/GAF.github.io/project_page/">http://chaiying1.github.io/GAF.github.io/project_page/</a> </p>
<blockquote>
<p>åŸºäºè§†è§‰çš„æœºå™¨äººæ“ä½œå¯¹ç²¾ç¡®çš„åŠ¨ä½œæ¨æ–­è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éµå¾ªè§†è§‰åˆ°åŠ¨ä½œï¼ˆV-Aï¼‰èŒƒå¼ï¼Œç›´æ¥ä»è§†è§‰è¾“å…¥é¢„æµ‹åŠ¨ä½œï¼Œæˆ–è€…è§†è§‰åˆ°ä¸‰ç»´åˆ°åŠ¨ä½œï¼ˆV-3D-Aï¼‰èŒƒå¼ï¼Œåˆ©ç”¨ä¸­é—´ä¸‰ç»´è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å› æ“ä½œåœºæ™¯çš„å¤æ‚æ€§å’ŒåŠ¨æ€æ€§è€Œé¢ä¸´åŠ¨ä½œä¸å‡†ç¡®çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§V-4D-Aæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé€šè¿‡é«˜æ–¯åŠ¨ä½œåœºï¼ˆGAFï¼‰ä»æ„ŸçŸ¥è¿åŠ¨çš„4Dè¡¨ç¤ºä¸­è¿›è¡Œç›´æ¥åŠ¨ä½œæ¨ç†ã€‚GAFé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è¿åŠ¨å±æ€§æ‰©å±•äº†ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ï¼Œèƒ½å¤ŸåŒæ—¶å¯¹åŠ¨æ€åœºæ™¯å’Œæ“ä½œåŠ¨ä½œè¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†å­¦ä¹ éšæ—¶é—´å˜åŒ–çš„åœºæ™¯å‡ ä½•å’Œæ„ŸçŸ¥æœºå™¨äººè¿åŠ¨ï¼ŒGAFæ”¯æŒä¸‰ç§å…³é”®æŸ¥è¯¢ç±»å‹ï¼šé‡å»ºå½“å‰åœºæ™¯ã€é¢„æµ‹æœªæ¥å¸§ä»¥åŠé€šè¿‡æœºå™¨äººè¿åŠ¨ä¼°è®¡åˆå§‹åŠ¨ä½œã€‚æ­¤å¤–ï¼ŒGAFç”Ÿæˆçš„é«˜è´¨é‡å½“å‰å’Œæœªæ¥å¸§é€šè¿‡GAFå¼•å¯¼æ‰©æ•£æ¨¡å‹ä¿ƒè¿›äº†æ“ä½œåŠ¨ä½œçš„æ”¹è¿›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGAFåœ¨é‡å»ºè´¨é‡ä¸Šå®ç°äº†+11.5385 dB PSNRå’Œ-0.5574 LPIPSçš„æ”¹è¿›ï¼ŒåŒæ—¶åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å°†å¹³å‡æˆåŠŸç‡æé«˜äº†10.33%ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="http://chaiying1.github.io/GAF.github.io/project_page/">http://chaiying1.github.io/GAF.github.io/project_page/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14135v1">PDF</a> <a target="_blank" rel="noopener" href="http://chaiying1.github.io/GAF.github.io/project_page/">http://chaiying1.github.io/GAF.github.io/project_page/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå››ç»´é«˜æ–¯åŠ¨ä½œåœºï¼ˆGAFï¼‰çš„V-4D-Aæ¡†æ¶ï¼Œç”¨äºä»è¿åŠ¨æ„ŸçŸ¥çš„4Dè¡¨ç¤ºä¸­è¿›è¡Œç›´æ¥åŠ¨ä½œæ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è¿åŠ¨å±æ€§ï¼Œæ‰©å±•äº†ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰ï¼Œæ”¯æŒå¯¹åŠ¨æ€åœºæ™¯å’Œæ“çºµåŠ¨ä½œçš„åŒæ­¥å»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒGAFé€šè¿‡ç”Ÿæˆé«˜è´¨é‡å½“å‰å¸§å’Œé¢„æµ‹æœªæ¥å¸§ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŠ¨ä½œç»†åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGAFåœ¨é‡å»ºè´¨é‡å’Œæœºå™¨äººæ“ä½œä»»åŠ¡çš„æˆåŠŸç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªV-4D-Aæ¡†æ¶ï¼Œç»“åˆäº†è¿åŠ¨æ„ŸçŸ¥çš„4Dè¡¨ç¤ºå’Œä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ï¼ˆGAFï¼‰ã€‚</li>
<li>GAFé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è¿åŠ¨å±æ€§ï¼Œå®ç°äº†å¯¹åŠ¨æ€åœºæ™¯å’Œæ“çºµåŠ¨ä½œçš„åŒæ­¥å»ºæ¨¡ã€‚</li>
<li>GAFæ”¯æŒä¸‰ç§å…³é”®æŸ¥è¯¢ç±»å‹ï¼šé‡å»ºå½“å‰åœºæ™¯ã€é¢„æµ‹æœªæ¥å¸§å’Œä¼°è®¡åˆå§‹åŠ¨ä½œã€‚</li>
<li>GAFç”Ÿæˆçš„é«˜è´¨é‡å½“å‰å’Œæœªæ¥å¸§æœ‰åŠ©äºé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŠ¨ä½œç»†åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-801651c2f1a8bcf307648ca9c0ea3a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca909a6be98754d4eec57ec2f2e302c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf057b05d638f2f468455cd61d4a35f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4dad1ccdbc771401e009a9f1a79ef9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Micro-macro-Gaussian-Splatting-with-Enhanced-Scalability-for-Unconstrained-Scene-Reconstruction"><a href="#Micro-macro-Gaussian-Splatting-with-Enhanced-Scalability-for-Unconstrained-Scene-Reconstruction" class="headerlink" title="Micro-macro Gaussian Splatting with Enhanced Scalability for   Unconstrained Scene Reconstruction"></a>Micro-macro Gaussian Splatting with Enhanced Scalability for   Unconstrained Scene Reconstruction</h2><p><strong>Authors:Yihui Li, Chengxin Lv, Hongyu Yang, Di Huang</strong></p>
<p>Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at <a target="_blank" rel="noopener" href="https://github.com/Kidleyh/SMW-GS">https://github.com/Kidleyh/SMW-GS</a>. </p>
<blockquote>
<p>ä»éçº¦æŸå›¾åƒé›†åˆé‡å»º3Dåœºæ™¯ç”±äºå¤–è§‚å˜åŒ–è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¯ä¼¸ç¼©å¾®å®è§‚å°æ³¢çš„é«˜æ–¯æ¶‚æŠ¹ï¼ˆSMW-GSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åˆ†è§£åœºæ™¯è¡¨ç¤ºä¸ºå…¨å±€ã€ç²¾ç»†å’Œå†…åœ¨æˆåˆ†æ¥å¢å¼ºä¸åŒå°ºåº¦ä¸Š3Dé‡å»ºæ•ˆæœçš„æ–°æ–¹æ³•ã€‚SMW-GSç»“åˆäº†ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼šå¾®å®è§‚æŠ•å½±ï¼Œå®ƒä½¿é«˜æ–¯ç‚¹èƒ½å¤Ÿé‡‡æ ·å¤šå°ºåº¦ç»†èŠ‚ï¼Œæé«˜äº†å¤šæ ·æ€§ï¼›åŸºäºå°æ³¢çš„é‡‡æ ·ï¼Œåˆ©ç”¨é¢‘åŸŸä¿¡æ¯ç»†åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤æ‚åœºæ™¯çš„å¤–è§‚ã€‚ä¸ºäº†å®ç°å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡åœºæ™¯æå‡ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–ç›¸æœºè§†è§’åˆ†é…ç»™åœºæ™¯åˆ†åŒºï¼Œä»¥æœ€å¤§åŒ–å…¶å¯¹é«˜æ–¯ç‚¹çš„è´¡çŒ®ï¼Œå³ä½¿åœ¨å¹¿é˜”çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°ä¸€è‡´ä¸”é«˜è´¨é‡çš„é‡å»ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSMW-GSåœ¨é‡å»ºè´¨é‡å’Œå¯æ‰©å±•æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜ç…§æ˜å˜åŒ–çš„å¤§è§„æ¨¡åŸå¸‚ç¯å¢ƒä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚é¡¹ç›®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Kidleyh/SMW-GS%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Kidleyh/SMW-GSè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13516v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SMW-GSæ˜¯ä¸€ç§ç”¨äºä»éçº¦æŸå›¾åƒé›†åˆé‡å»º3Dåœºæ™¯çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ†è§£åœºæ™¯è¡¨ç¤ºä¸ºå…¨å±€ã€ç²¾ç»†å’Œå†…åœ¨æˆåˆ†ï¼Œåœ¨å¤šç§å°ºåº¦ä¸Šæé«˜äº†3Dé‡å»ºçš„æ•ˆæœã€‚æ­¤æ–¹æ³•åŒ…æ‹¬Micro-macroæŠ•å½±å’ŒåŸºäºWaveletçš„é‡‡æ ·ç­‰åˆ›æ–°æŠ€æœ¯ï¼Œèƒ½æ”¹å–„é«˜æ–¯ç‚¹çš„å¤šå°ºåº¦ç»†èŠ‚é‡‡æ ·ï¼Œå¹¶ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºä»¥æ•è·å¤æ‚çš„åœºæ™¯å¤–è§‚ã€‚ä¸ºå®ç°å¯æ‰©å±•æ€§ï¼Œè¿˜æå‡ºäº†å¤§è§„æ¨¡åœºæ™¯æ¨å¹¿ç­–ç•¥ï¼Œé€šè¿‡æœ€å¤§åŒ–å…¶å¯¹é«˜æ–¯ç‚¹çš„è´¡çŒ®æ¥ä¼˜åŒ–ç›¸æœºè§†è§’çš„åœºæ™¯åˆ†é…ï¼Œå³ä½¿åœ¨å¤§å‹ç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°ä¸€è‡´ä¸”é«˜è´¨é‡çš„é‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMW-GSæ˜¯ä¸€ç§é’ˆå¯¹éçº¦æŸå›¾åƒé›†åˆçš„3Dé‡å»ºæ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é‡å»ºè´¨é‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£åœºæ™¯è¡¨ç¤ºä¸ºå…¨å±€ã€ç²¾ç»†å’Œå†…åœ¨æˆåˆ†ï¼Œåœ¨å¤šç§å°ºåº¦ä¸Šæ”¹å–„3Dé‡å»ºã€‚</li>
<li>Micro-macroæŠ•å½±æŠ€æœ¯èƒ½å¤Ÿæ”¹å–„é«˜æ–¯ç‚¹çš„å¤šå°ºåº¦ç»†èŠ‚é‡‡æ ·ã€‚</li>
<li>åŸºäºWaveletçš„é‡‡æ ·æŠ€æœ¯ç”¨äºä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°æ•è·å¤æ‚çš„åœºæ™¯å¤–è§‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡åœºæ™¯æ¨å¹¿ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–ç›¸æœºè§†è§’åˆ†é…å®ç°ä¸€è‡´ä¸”é«˜è´¨é‡çš„é‡å»ºã€‚</li>
<li>SMW-GSåœ¨é‡å»ºè´¨é‡å’Œå¯æ‰©å±•æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤§å‹åŸå¸‚ç¯å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd7e0051c341bdb41b5c49944c4a592d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78d066bd136b6de08141a0c720bb24ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9e7b444b8a0f5b88c24ff16aec93b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aabddf1a47c69d7f7212b3bdbd3fae9a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multiview-Geometric-Regularization-of-Gaussian-Splatting-for-Accurate-Radiance-Fields"><a href="#Multiview-Geometric-Regularization-of-Gaussian-Splatting-for-Accurate-Radiance-Fields" class="headerlink" title="Multiview Geometric Regularization of Gaussian Splatting for Accurate   Radiance Fields"></a>Multiview Geometric Regularization of Gaussian Splatting for Accurate   Radiance Fields</h2><p><strong>Authors:Jungeon Kim, Geonsoo Park, Seungyong Lee</strong></p>
<p>Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes. </p>
<blockquote>
<p>è¿‘æœŸçš„æ–¹æ³•ï¼Œå¦‚äºŒç»´é«˜æ–¯å¹³é“ºï¼ˆ2D Gaussian Splattingï¼‰å’Œé«˜æ–¯ä¸é€æ˜åº¦åœºï¼ˆGaussian Opacity Fieldsï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3D Gaussian Splattingï¼‰çš„å‡ ä½•è¯¯å·®é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå…¶å“è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨é‡å»ºå¹³æ»‘å¯é çš„å‡ ä½•å½¢çŠ¶æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒè§†ç‚¹é—´å­˜åœ¨æ˜¾è‘—é¢œè‰²å˜åŒ–çš„åœºæ™¯ä¸­ï¼Œå› ä¸ºå®ƒä»¬é‡‡ç”¨é€ç‚¹å¤–è§‚å»ºæ¨¡å’Œå•è§†å›¾ä¼˜åŒ–çº¦æŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šè§†å›¾å‡ ä½•æ­£åˆ™åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†å¤šè§†å›¾ç«‹ä½“æµ‹é‡ï¼ˆMVSï¼‰æ·±åº¦ã€RGBå’Œæ³•çº¿çº¦æŸé›†æˆåˆ°é«˜æ–¯å¹³é“ºçš„åˆå§‹åŒ–å’Œä¼˜åŒ–ä¸­ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯MVSè¡ç”Ÿçš„æ·±åº¦ç‚¹ä¸é«˜æ–¯å¹³é“ºä¼˜åŒ–ä½ç½®ä¹‹é—´çš„äº’è¡¥å…³ç³»ï¼šMVSé€šè¿‡åŸºäºå±€éƒ¨è¡¥ä¸çš„åŒ¹é…å’Œæçº¿çº¦æŸç¨³å¥åœ°ä¼°è®¡é«˜é¢œè‰²å˜åŒ–åŒºåŸŸçš„å‡ ä½•å½¢çŠ¶ï¼Œè€Œé«˜æ–¯å¹³é“ºåˆ™æä¾›æ›´å¯é ã€æ›´å°‘å™ªå£°çš„æ·±åº¦ä¼°è®¡ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹è±¡è¾¹ç•Œå’Œé¢œè‰²å˜åŒ–è¾ƒä½çš„åŒºåŸŸã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¸­é—´æ·±åº¦çš„å¤šè§†å›¾ç›¸å¯¹æ·±åº¦æŸå¤±ï¼Œå¹¶å¸¦æœ‰ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæœ‰æ•ˆåœ°å°†MVSæ·±åº¦ä¿¡æ¯é›†æˆåˆ°é«˜æ–¯å¹³é“ºä¼˜åŒ–ä¸­ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MVSå¼•å¯¼çš„é«˜æ–¯å¹³é“ºåˆå§‹åŒ–ï¼Œä»¥é¿å…é«˜æ–¯é™·å…¥æ¬¡ä¼˜ä½ç½®ã€‚å¤§é‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸç»“åˆäº†è¿™äº›ä¼˜ç‚¹ï¼Œæé«˜äº†å®¤å†…å¤–åœºæ™¯çš„å‡ ä½•ç²¾åº¦å’Œæ¸²æŸ“è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13508v1">PDF</a> Accepted to Computer Graphics Forum (EGSR 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šè§†è§’å‡ ä½•æ­£åˆ™åŒ–ç­–ç•¥ï¼Œå°†å¤šè§†è§’ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ã€RGBå’Œæ³•å‘é‡çº¦æŸé›†æˆåˆ°é«˜æ–¯è´´å›¾åˆå§‹åŒ–ä¸ä¼˜åŒ–ä¸­ã€‚é€šè¿‡ç»“åˆMVSæ´¾ç”Ÿçš„æ·±åº¦ç‚¹ä¸é«˜æ–¯è´´å›¾ä¼˜åŒ–çš„ä½ç½®ï¼Œè¯¥ç­–ç•¥åœ¨é¢œè‰²å˜åŒ–å¤§çš„åŒºåŸŸé€šè¿‡å±€éƒ¨å—åŒ¹é…å’Œæçº¿çº¦æŸç¨³å¥åœ°ä¼°è®¡å‡ ä½•å½¢çŠ¶ï¼ŒåŒæ—¶åœ¨å¯¹è±¡è¾¹ç•Œå’Œé¢œè‰²å˜åŒ–è¾ƒå°çš„åŒºåŸŸæä¾›æ›´å¯é ã€å™ªå£°æ›´å°‘çš„æ·±åº¦ä¼°è®¡ã€‚æå‡ºä¸€ç§åŸºäºä¸­ä½æ·±åº¦çš„å¤šè§†è§’ç›¸å¯¹æ·±åº¦æŸå¤±åŠä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ï¼Œæœ‰æ•ˆæ•´åˆMVSæ·±åº¦ä¿¡æ¯åˆ°é«˜æ–¯è´´å›¾ä¼˜åŒ–ä¸­ã€‚åŒæ—¶ï¼Œä¹Ÿæå‡ºäº†MVSå¼•å¯¼çš„é«˜æ–¯è´´å›¾åˆå§‹åŒ–æ–¹æ³•ï¼Œé¿å…é«˜æ–¯è½å…¥æ¬¡ä¼˜ä½ç½®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸç»“åˆäº†è¿™äº›ä¼˜ç‚¹ï¼Œæé«˜äº†å‡ ä½•ç²¾åº¦å’Œæ¸²æŸ“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å¤šè§†è§’å‡ ä½•æ­£åˆ™åŒ–ç­–ç•¥ï¼Œé›†æˆäº†MVSæ·±åº¦ã€RGBå’Œæ³•å‘é‡çº¦æŸåˆ°é«˜æ–¯è´´å›¾çš„åˆå§‹åŒ–ä¸ä¼˜åŒ–çš„æµç¨‹ä¸­ã€‚</li>
<li>ç»“åˆMVSæ´¾ç”Ÿçš„æ·±åº¦ç‚¹ä¸é«˜æ–¯è´´å›¾çš„ä¼˜åŒ–ä½ç½®ï¼Œä»¥å¤„ç†é¢œè‰²å˜åŒ–å¤§çš„åŒºåŸŸçš„å‡ ä½•å½¢çŠ¶ä¼°è®¡ã€‚</li>
<li>åœ¨å¯¹è±¡è¾¹ç•Œå’Œé¢œè‰²å˜åŒ–è¾ƒå°çš„åŒºåŸŸï¼Œé«˜æ–¯è´´å›¾æä¾›æ›´å¯é ã€å™ªå£°æ›´å°‘çš„æ·±åº¦ä¼°è®¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¸­ä½æ·±åº¦çš„å¤šè§†è§’ç›¸å¯¹æ·±åº¦æŸå¤±æ–¹æ³•ï¼Œæœ‰æ•ˆæ•´åˆMVSæ·±åº¦ä¿¡æ¯åˆ°é«˜æ–¯è´´å›¾çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚</li>
<li>å¼•å…¥MVSå¼•å¯¼çš„é«˜æ–¯è´´å›¾åˆå§‹åŒ–æ–¹æ³•ï¼Œé¿å…é«˜æ–¯è½å…¥æ¬¡ä¼˜ä½ç½®ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æé«˜å‡ ä½•ç²¾åº¦å’Œæ¸²æŸ“è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e6ed945a7642a1a61449b1e5cf72821.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1c3f47327309ce69b7f62bb57909e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e607f9a67907dcb2c6a9bbdc0e78cacb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GS-2DGS-Geometrically-Supervised-2DGS-for-Reflective-Object-Reconstruction"><a href="#GS-2DGS-Geometrically-Supervised-2DGS-for-Reflective-Object-Reconstruction" class="headerlink" title="GS-2DGS: Geometrically Supervised 2DGS for Reflective Object   Reconstruction"></a>GS-2DGS: Geometrically Supervised 2DGS for Reflective Object   Reconstruction</h2><p><strong>Authors:Jinguang Tong, Xuesong li, Fahira Afzal Maken, Sundaram Muthu, Lars Petersson, Chuong Nguyen, Hongdong Li</strong></p>
<p>3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hirotong/GS2DGS">https://github.com/hirotong/GS2DGS</a> </p>
<blockquote>
<p>å¯¹äºé«˜åå°„ç‰©ä½“çš„ä¸‰ç»´å»ºæ¨¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬çš„å¤–è§‚å…·æœ‰å¼ºçƒˆçš„è§†è§’ä¾èµ–æ€§ã€‚è™½ç„¶ä¹‹å‰çš„åŸºäºSDFçš„æ–¹æ³•å¯ä»¥æ¢å¤é«˜è´¨é‡çš„ä¸‰ç»´ç½‘æ ¼ï¼Œä½†å®ƒä»¬é€šå¸¸è€—æ—¶ä¸”å€¾å‘äºäº§ç”Ÿè¿‡äºå¹³æ»‘çš„è¡¨é¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸‰ç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼ˆ3DGSï¼‰å…·æœ‰é«˜é€Ÿå’Œè¯¦ç»†çš„å®æ—¶æ¸²æŸ“ä¼˜åŠ¿ï¼Œä½†ç”±äºç¼ºä¹å‡ ä½•çº¦æŸï¼Œä»é«˜æ–¯æ•°æ®ä¸­æå–è¡¨é¢å¯èƒ½ä¼šäº§ç”Ÿå™ªå£°ã€‚ä¸ºäº†å¼¥åˆè¿™äº›æ–¹æ³•ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºäºŒç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼ˆ2DGSï¼‰çš„æ–°å‹é‡å»ºæ–¹æ³•ï¼Œç”¨äºå¤„ç†åå°„ç‰©ä½“ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºGS-2DGSã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†é«˜æ–¯æ‹¼è´´çš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›ä»¥åŠä¸åŸºç¡€æ¨¡å‹çš„å‡ ä½•ä¿¡æ¯ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºå’Œé‡æ–°ç…§æ˜æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºé«˜æ–¯çš„æŠ€æœ¯ï¼Œå¹¶ä¸”æ€§èƒ½ä¸åŸºäºSDFçš„æ–¹æ³•ç›¸å½“ï¼Œä½†é€Ÿåº¦æ›´å¿«ä¸€ä¸ªæ•°é‡çº§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hirotong/GS2DGS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hirotong/GS2DGSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13110v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é«˜åº¦åå°„ç‰©ä½“çš„ä¸‰ç»´å»ºæ¨¡æŒ‘æˆ˜ï¼Œç»“åˆäº†äºŒç»´é«˜æ–¯æº…å°„ï¼ˆ2DGSï¼‰çš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›ä¸åŸºç¡€æ¨¡å‹ä¸­çš„å‡ ä½•ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGS-2DGSçš„æ–°å‹é‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨é‡å»ºå’Œé‡æ–°ç…§æ˜æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºé«˜æ–¯çš„æŠ€æœ¯ï¼ŒåŒæ—¶å…¶æ€§èƒ½ä¸åŸºäºSDFçš„æ–¹æ³•ç›¸å½“ï¼Œä½†é€Ÿåº¦æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åº¦åå°„ç‰©ä½“çš„ä¸‰ç»´å»ºæ¨¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬çš„å¤–è§‚å…·æœ‰å¼ºçƒˆçš„è§†è§’ä¾èµ–æ€§ã€‚</li>
<li>ä¹‹å‰åŸºäºSDFçš„æ–¹æ³•å¯ä»¥æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä½†è€—æ—¶ä¸”å®¹æ˜“äº§ç”Ÿè¿‡åº¦å¹³æ»‘çš„è¡¨é¢ã€‚</li>
<li>3DGSæä¾›é«˜é€Ÿå’Œè¯¦ç»†çš„å®æ—¶æ¸²æŸ“ï¼Œä½†æå–è¡¨é¢å¯èƒ½å› ç¼ºä¹å‡ ä½•çº¦æŸè€Œå˜ˆæ‚ã€‚</li>
<li>GS-2DGSæ–¹æ³•ç»“åˆäº†Gaussian Splattingçš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›ä¸åŸºç¡€æ¨¡å‹ä¸­çš„å‡ ä½•ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜GS-2DGSåœ¨é‡å»ºå’Œé‡æ–°ç…§æ˜æ–¹é¢ä¼˜äºåŸºäºé«˜æ–¯çš„æ–¹æ³•ã€‚</li>
<li>GS-2DGSçš„æ€§èƒ½ä¸åŸºäºSDFçš„æ–¹æ³•ç›¸å½“ï¼Œä½†é€Ÿåº¦æ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa3631c80a1fba6c3bfdaaef36220cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2562ecd2d30cfa347658bab5083e6eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c641de541cb548ac35a7ec9f1095cf8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ae37558936ce052a999eeb2129b926f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d0611875f9d38dfd862b7f971b1101e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d568b62540e5169b06052725b091aab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a01f5079e02067022f5a26328ea0b382.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Metropolis-Hastings-Sampling-for-3D-Gaussian-Reconstruction"><a href="#Metropolis-Hastings-Sampling-for-3D-Gaussian-Reconstruction" class="headerlink" title="Metropolis-Hastings Sampling for 3D Gaussian Reconstruction"></a>Metropolis-Hastings Sampling for 3D Gaussian Reconstruction</h2><p><strong>Authors:Hyunjin Kim, Haebeom Jung, Jaesik Park</strong></p>
<p>We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or the premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep Blending, show that our approach reduces the number of Gaussians needed, enhancing computational efficiency while matching or modestly surpassing the view-synthesis quality of state-of-the-art models. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹ä¸‰ç»´é«˜æ–¯æ··åˆï¼ˆ3DGSï¼‰æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç»Ÿä¸€çš„å¤§éƒ½å¸‚ä¸»ä¹‰-é»‘æ–¯å»·æ–¯æ–¹æ³•å†…çš„ç»¼åˆå¤šè§†å›¾å…‰åº¦è¯¯å·®ä¿¡å·ã€‚ä¼ ç»Ÿçš„ä¸‰ç»´GSæ–¹æ³•ä¸¥é‡ä¾èµ–äºåŸºäºå¯å‘å¼çš„å¯†åº¦æ§åˆ¶æœºåˆ¶ï¼ˆä¾‹å¦‚å…‹éš†ã€æ‹†åˆ†å’Œä¿®å‰ªï¼‰ï¼Œè¿™å¯èƒ½å¯¼è‡´å†—ä½™è®¡ç®—æˆ–è¿‡æ—©åˆ é™¤æœ‰ç›Šçš„é«˜æ–¯æ··åˆã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡é‡æ–°åˆ¶å®šå¯†é›†åŒ–å’Œä¿®å‰ªä¸ºæ¦‚ç‡é‡‡æ ·è¿‡ç¨‹æ¥å…‹æœè¿™äº›é™åˆ¶ï¼Œæ ¹æ®èšåˆçš„å¤šè§†å›¾è¯¯å·®å’Œä¸é€æ˜åº¦åˆ†æ•°åŠ¨æ€æ’å…¥å’Œé‡æ–°å®šä½é«˜æ–¯æ··åˆã€‚åœ¨åŸºäºè¿™äº›åŸºäºè¯¯å·®çš„é‡è¦æ€§åˆ†æ•°å¾—å‡ºçš„è´å¶æ–¯æ¥å—æµ‹è¯•çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æå¤§åœ°å‡å°‘äº†å¯å‘å¼çš„ä¾èµ–ï¼Œæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢„è®¾åœºæ™¯å¤æ‚æ€§å³å¯è‡ªé€‚åº”æ¨æ–­é«˜æ–¯åˆ†å¸ƒã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬Mip-NeRF360ã€å¦å…‹ä¸ç¥åº™å’Œæ·±åº¦æ··åˆï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å‡å°‘äº†æ‰€éœ€çš„é«˜æ–¯æ··åˆæ•°é‡ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶è¾¾åˆ°äº†æˆ–ç•¥å¾®è¶…è¿‡äº†æœ€æ–°æ¨¡å‹çš„è§†å›¾åˆæˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12945v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://hjhyunjinkim.github.io/MH-3DGS">https://hjhyunjinkim.github.io/MH-3DGS</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è‡ªé€‚åº”é‡‡æ ·æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›ä¸‰ç»´é«˜æ–¯æ··åˆï¼ˆ3DGSï¼‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç»Ÿä¸€çš„åéªŒé‡‡æ ·è¿‡ç¨‹ç»“åˆå¤šè§’åº¦ç…§ç‰‡è¯¯å·®ä¿¡å·è¿›è¡Œç²¾ç»†åŒ–ä¼˜åŒ–å’Œæ·˜æ±°é€‰æ‹©ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„åŸºäºå¯å‘å¼æ–¹æ³•çš„æœºåˆ¶æ›´çµæ´»ã€é«˜æ•ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å‡å°‘äº†æ‰€éœ€çš„é«˜æ–¯æ•°é‡ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶åŒ¹é…æˆ–è¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„è§†å›¾åˆæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºè‡ªé€‚åº”é‡‡æ ·æ¡†æ¶ç”¨äºæ”¹è¿›ä¸‰ç»´é«˜æ–¯æ··åˆï¼ˆ3DGSï¼‰ã€‚</li>
<li>ç»“åˆå¤šè§’åº¦ç…§ç‰‡è¯¯å·®ä¿¡å·å’Œä¸ç¡®å®šæ€§åˆ†æè¿›è¡Œä¼˜åŒ–å’Œè°ƒæ•´è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨åéªŒé‡‡æ ·æ–¹æ³•å¤„ç†ä¸‰ç»´é‡å»ºè¿‡ç¨‹ä¸­çš„ç²¾ç»†åŒ–å’Œæ·˜æ±°é€‰æ‹©ä»»åŠ¡ã€‚</li>
<li>å‡å°‘å†—ä½™è®¡ç®—å¹¶é˜²æ­¢äº†é‡è¦é«˜æ–¯çš„è¿‡æ—©æ·˜æ±°ã€‚</li>
<li>å°†é«˜æ–¯çš„æ’å…¥å’Œç§»é™¤è½¬æ¢ä¸ºæ¦‚ç‡é‡‡æ ·è¿‡ç¨‹ä»¥å¢å¼ºå…¶çµæ´»æ€§ã€‚</li>
<li>ç»“åˆè´å¶æ–¯æµ‹è¯•æ¥å†³å®šè‡ªé€‚åº”åˆ†å¸ƒæŠ½æ ·è€Œä¸ä¾èµ–äºé¢„å…ˆå®šä¹‰åœºæ™¯çš„å¤æ‚æ€§åˆ†æã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-201612c1e51bfb6bfd9eb605ed3fa4ce.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rasterizing-Wireless-Radiance-Field-via-Deformable-2D-Gaussian-Splatting"><a href="#Rasterizing-Wireless-Radiance-Field-via-Deformable-2D-Gaussian-Splatting" class="headerlink" title="Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting"></a>Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting</h2><p><strong>Authors:Mufan Liu, Cixiao Zhang, Qi Yang, Yujie Cao, Yiling Xu, Yin Xu, Shu Sun, Mingzeng Dai, Yunfeng Guan</strong></p>
<p>Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is <a target="_blank" rel="noopener" href="https://evan-sudo.github.io/swiftwrf/">https://evan-sudo.github.io/swiftwrf/</a>. </p>
<blockquote>
<p>æ— çº¿è¾å°„åœºï¼ˆWireless Radiance Fieldï¼ŒWRFï¼‰å»ºæ¨¡æ˜¯ç°ä»£é€šä¿¡ç³»ç»Ÿçš„åŸºç¡€ï¼Œèƒ½å¤Ÿå®ç°å®šä½ã€æ„ŸçŸ¥å’Œä¿¡é“ä¼°è®¡ç­‰å…³é”®ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºç»éªŒå…¬å¼æˆ–ç‰©ç†ä»¿çœŸï¼Œå¾€å¾€å­˜åœ¨ç²¾åº¦æœ‰é™æˆ–éœ€è¦å¼ºçƒˆçš„åœºæ™¯å…ˆéªŒçŸ¥è¯†çš„ç¼ºé™·ã€‚æœ€è¿‘åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ–¹æ³•é€šè¿‡å¯å¾®åˆ†çš„ä½“ç§¯æ¸²æŸ“æé«˜äº†é‡å»ºçš„é€¼çœŸåº¦ï¼Œä½†å®ƒä»¬å¯¹è®¡ç®—å¯†é›†å‹çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æŸ¥è¯¢çš„ä¾èµ–é˜»ç¢äº†å®æ—¶éƒ¨ç½²ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†é«˜æ–¯å–·ç»˜ï¼ˆGaussian Splattingï¼ŒGSï¼‰å¼•å…¥åˆ°æ— çº¿é¢†åŸŸï¼Œåˆ©ç”¨å…¶åœ¨å¯¹å…‰å­¦è¾å°„åœºå»ºæ¨¡æ–¹é¢çš„æ•ˆç‡ï¼Œå®ç°ç´§å‡‘ä¸”å‡†ç¡®çš„WRFé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†SwiftWRFï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å˜å½¢çš„2Dé«˜æ–¯å–·ç»˜æ¡†æ¶ï¼Œå¯åœ¨å•ä¾§æ”¶å‘å™¨ç§»åŠ¨æ€§çš„æƒ…å†µä¸‹åˆæˆä»»æ„ä½ç½®çš„WRFå…‰è°±ã€‚SwiftWRFé‡‡ç”¨CUDAåŠ é€Ÿçš„å…‰æ …åŒ–ï¼Œä»¥è¶…è¿‡100000å¸§æ¯ç§’çš„é€Ÿåº¦æ¸²æŸ“å…‰è°±ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§çš„å¤šå±‚æ„ŸçŸ¥å™¨å¯¹2Dé«˜æ–¯å˜å½¢è¿›è¡Œå»ºæ¨¡ï¼Œæœ‰æ•ˆåœ°æ•æ‰ç§»åŠ¨å¼•èµ·çš„WRFå˜åŒ–ã€‚é™¤äº†æ–°é¢–çš„å…‰è°±åˆæˆä¹‹å¤–ï¼ŒSwiftWRFåœ¨åˆ°è¾¾è§’ï¼ˆAoAï¼‰å’Œæ¥æ”¶ä¿¡å·å¼ºåº¦æŒ‡ç¤ºï¼ˆRSSIï¼‰é¢„æµ‹æ–¹é¢çš„åº”ç”¨ä¹Ÿè¿›ä¸€æ­¥è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨çœŸå®å’Œåˆæˆå®¤å†…åœºæ™¯ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSwiftWRFå¯ä»¥æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«500å€é‡å»ºWRFå…‰è°±ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜ä¿¡å·è´¨é‡ã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://evan-sudo.github.io/swiftwrf/%E3%80%82">https://evan-sudo.github.io/swiftwrf/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12787v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰çš„æ–¹æ³•åˆ©ç”¨å¯å¾®ä½“ç§¯æ¸²æŸ“æé«˜äº†é‡å»ºç²¾åº¦ï¼Œä½†å…¶åœ¨æ— çº¿é¢†åŸŸçš„åº”ç”¨å—é™äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æŸ¥è¯¢çš„è®¡ç®—æˆæœ¬ã€‚æœ¬ç ”ç©¶å¼•å…¥é«˜æ–¯å¹³é“ºï¼ˆGSï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨å…¶åœ¨å…‰å­¦è¾å°„åœºå»ºæ¨¡ä¸­çš„é«˜æ•ˆæ€§ï¼Œå®ç°ç´§å‡‘ä¸”ç²¾ç¡®çš„æ— çº¿è¾å°„åœºï¼ˆWRFï¼‰é‡å»ºã€‚æå‡ºçš„SwiftWRFæ¡†æ¶é‡‡ç”¨å¯å˜å½¢äºŒç»´é«˜æ–¯å¹³é“ºæŠ€æœ¯ï¼Œå¯åœ¨å•ä¾§æ”¶å‘å™¨ç§»åŠ¨æ€§æ¡ä»¶ä¸‹åˆæˆä»»æ„ä½ç½®çš„WRFå…‰è°±ã€‚SwiftWRFä½¿ç”¨CUDAåŠ é€Ÿæ¸²æŸ“ï¼Œä»¥è¶…è¿‡100000å¸§&#x2F;ç§’çš„é€Ÿåº¦æ¸²æŸ“å…‰è°±ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§MLPå¯¹äºŒç»´é«˜æ–¯è¿›è¡Œå˜å½¢å»ºæ¨¡ï¼Œæœ‰æ•ˆæ•æ‰ç§»åŠ¨å¼•èµ·çš„WRFå˜åŒ–ã€‚æ­¤å¤–ï¼ŒSwiftWRFåœ¨åˆ°è¾¾è§’ï¼ˆAoAï¼‰å’Œæ¥æ”¶ä¿¡å·å¼ºåº¦æŒ‡æ ‡ï¼ˆRSSIï¼‰é¢„æµ‹æ–¹é¢çš„åº”ç”¨ä¹Ÿè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨çœŸå®å’Œåˆæˆå®¤å†…åœºæ™¯ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSwiftWRFçš„WRFå…‰è°±é‡å»ºé€Ÿåº¦æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«500å€ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†ä¿¡å·è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å»ºæ¨¡æ— çº¿è¾å°„åœºï¼ˆWRFï¼‰æ˜¯ç°ä»£é€šä¿¡ç³»ç»Ÿçš„åŸºç¡€ä»»åŠ¡ï¼ŒåŒ…æ‹¬å®šä½ã€æ„ŸçŸ¥å’Œä¿¡é“ä¼°è®¡ç­‰å…³é”®ä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç»éªŒå…¬å¼æˆ–ç‰©ç†æ¨¡æ‹Ÿï¼Œå­˜åœ¨ç²¾åº¦æœ‰é™æˆ–éœ€è¦å¼ºçƒˆåœºæ™¯å…ˆéªŒçš„é—®é¢˜ã€‚</li>
<li>åŸºäºNeRFçš„æ–¹æ³•é€šè¿‡å¯å¾®ä½“ç§¯æ¸²æŸ“æé«˜äº†é‡å»ºç²¾åº¦ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æŸ¥è¯¢æ–¹é¢ã€‚</li>
<li>å¼•å…¥é«˜æ–¯å¹³é“ºï¼ˆGSï¼‰æŠ€æœ¯åˆ°æ— çº¿é¢†åŸŸï¼Œåˆ©ç”¨å…¶åœ¨å…‰å­¦è¾å°„åœºå»ºæ¨¡ä¸­çš„é«˜æ•ˆæ€§è¿›è¡ŒWRFé‡å»ºã€‚</li>
<li>æå‡ºSwiftWRFæ¡†æ¶ï¼Œé‡‡ç”¨å¯å˜å½¢äºŒç»´é«˜æ–¯å¹³é“ºæŠ€æœ¯ï¼Œåˆæˆä»»æ„ä½ç½®çš„WRFå…‰è°±ï¼Œå¹¶å…·å¤‡å•ä¾§æ”¶å‘å™¨ç§»åŠ¨æ€§æ¡ä»¶ä¸‹çš„é€‚åº”æ€§ã€‚</li>
<li>SwiftWRFä½¿ç”¨CUDAåŠ é€Ÿæ¸²æŸ“ï¼Œä»¥é«˜é€Ÿåº¦æ¸²æŸ“å…‰è°±ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§MLPæ•æ‰ç§»åŠ¨å¼•èµ·çš„WRFå˜åŒ–ã€‚</li>
<li>SwiftWRFåœ¨è§’åº¦åˆ°è¾¾ï¼ˆAoAï¼‰å’Œæ¥æ”¶ä¿¡å·å¼ºåº¦æŒ‡æ ‡ï¼ˆRSSIï¼‰é¢„æµ‹ä¸­çš„åº”ç”¨è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”åœ¨çœŸå®å’Œåˆæˆå®¤å†…åœºæ™¯ä¸Šçš„å®éªŒè¡¨æ˜å…¶æ¯”ç°æœ‰æ–¹æ³•æ›´å¿«ä¸”æ›´ç²¾ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11a3f49f8f3a502a23c40aba79791ce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d1dc4635bee2dd3337a105c3cd3df1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366889da6a74349426dee2689f68cfdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7690cfa0e881e88e9a5c6333834fe29d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7686ad30bdf591019c63df597897efd8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generative-4D-Scene-Gaussian-Splatting-with-Object-View-Synthesis-Priors"><a href="#Generative-4D-Scene-Gaussian-Splatting-with-Object-View-Synthesis-Priors" class="headerlink" title="Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors"></a>Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors</h2><p><strong>Authors:Wen-Hsuan Chu, Lei Ke, Jianmeng Liu, Mingxiao Huo, Pavel Tokmakov, Katerina Fragkiadaki</strong></p>
<p>We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches. </p>
<blockquote>
<p>æˆ‘ä»¬åº”å¯¹ä»å•ç›®å¤šç‰©ä½“è§†é¢‘ç”ŸæˆåŠ¨æ€å››ç»´åœºæ™¯çš„æŒ‘æˆ˜ï¼Œè¿™äº›è§†é¢‘å­˜åœ¨ä¸¥é‡çš„é®æŒ¡é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥GenMOJOè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒå°†åŸºäºæ¸²æŸ“çš„å¯å˜å½¢ä¸‰ç»´é«˜æ–¯ä¼˜åŒ–ä¸ç”¨äºè§†å›¾åˆæˆçš„ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ç›¸ç»“åˆã€‚è™½ç„¶ç°æœ‰æ¨¡å‹åœ¨å­¤ç«‹ç‰©ä½“çš„æ–°å‹è§†å›¾åˆæˆæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬åœ¨æ¨å¹¿åˆ°å¤æ‚æ··ä¹±çš„åœºæ™¯æ—¶å´éš¾ä»¥å‘æŒ¥åŠŸæ•ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒGenMOJOå°†åœºæ™¯åˆ†è§£ä¸ºå•ä¸ªç‰©ä½“ï¼Œé’ˆå¯¹æ¯ä¸ªç‰©ä½“ä¼˜åŒ–ä¸€ç»„å¯åŒºåˆ†çš„å¯å˜å½¢é«˜æ–¯åˆ†å¸ƒã€‚è¿™ç§æŒ‰å¯¹è±¡åˆ†è§£çš„æ–¹æ³•å…è®¸åˆ©ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ‰©æ•£æ¨¡å‹æ¥æ¨æ–­æ–°å‹è§‚ç‚¹ä¸­çš„æœªè§‚å¯ŸåŒºåŸŸã€‚å®ƒæ‰§è¡Œè”åˆé«˜æ–¯è´´å›¾æ¸²æŸ“æ•´ä¸ªåœºæ™¯ï¼Œæ•æ‰è·¨å¯¹è±¡é®æŒ¡ï¼Œå¹¶å®ç°é®æŒ¡æ„ŸçŸ¥ç›‘ç£ã€‚ä¸ºå¼¥åˆå¯¹è±¡ä¸ºä¸­å¿ƒçš„å…ˆéªŒçŸ¥è¯†å’Œè§†é¢‘å…¨å±€å¸§ä¸ºä¸­å¿ƒçš„åæ ‡ç³»ä¹‹é—´çš„å·®è·ï¼ŒGenMOJOä½¿ç”¨å¯åŒºåˆ†è½¬æ¢æ¥åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¯¹é½ç”Ÿæˆå’Œæ¸²æŸ“çº¦æŸã€‚æ‰€å¾—åˆ°çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šç”Ÿæˆå››ç»´ç‰©ä½“é‡å»ºï¼Œå¹¶ä»å•ç›®è¾“å…¥ä¸­äº§ç”Ÿå‡†ç¡®çš„äºŒç»´å’Œä¸‰ç»´ç‚¹è½¨è¿¹ã€‚å®šé‡è¯„ä¼°å’Œæ„ŸçŸ¥äººç±»ç ”ç©¶è¯å®ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGenMOJOç”Ÿæˆçš„æ–°å‹åœºæ™¯è§†å›¾æ›´åŠ çœŸå®ï¼Œäº§ç”Ÿçš„ç‚¹è½¨è¿¹ä¹Ÿæ›´åŠ å‡†ç¡®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12716v1">PDF</a> This is an updated and extended version of our CVPR paper â€œRobust   Multi-Object 4D Generation in Complex Video Scenariosâ€</p>
<p><strong>Summary</strong></p>
<p>GenMOJOæ˜¯ä¸€ç§é’ˆå¯¹å•ç›®å¤šç‰©ä½“è§†é¢‘ç”ŸæˆåŠ¨æ€4Dåœºæ™¯çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡é›†æˆåŸºäºæ¸²æŸ“çš„å¯å˜å½¢3Dé«˜æ–¯ä¼˜åŒ–å’Œç”Ÿæˆå…ˆéªŒæ¥å®ç°åœºæ™¯åˆæˆã€‚è¯¥æ–¹æ³•å°†åœºæ™¯åˆ†è§£ä¸ºå•ä¸ªç‰©ä½“ï¼Œé’ˆå¯¹æ¯ä¸ªç‰©ä½“ä¼˜åŒ–å¯å˜å½¢é«˜æ–¯ï¼Œå¹¶åˆ©ç”¨å¯¹è±¡çº§æ‰©æ•£æ¨¡å‹æ¨æ–­æ–°è§†è§’ä¸­çš„æœªè§‚å¯ŸåŒºåŸŸã€‚é€šè¿‡è”åˆé«˜æ–¯è´´ç‰‡æŠ€æœ¯æ¸²æŸ“æ•´ä¸ªåœºæ™¯ï¼Œæ•æ‰å¯¹è±¡é—´çš„é®æŒ¡ï¼Œå¹¶å®ç°é®æŒ¡æ„ŸçŸ¥ç›‘ç£ã€‚GenMOJOä½¿ç”¨å¯åŒºåˆ†å˜æ¢åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¯¹é½ç”Ÿæˆå’Œæ¸²æŸ“çº¦æŸï¼Œä»è€Œç¼©å°å¯¹è±¡çº§å…ˆéªŒä¸è§†é¢‘å…¨å±€å¸§çº§åæ ‡ç³»ä¹‹é—´çš„å·®è·ã€‚è¯¥æ–¹æ³•ç”Ÿæˆäº†4Då¯¹è±¡åœ¨æ—¶ç©ºä¸Šçš„é‡å»ºï¼Œå¹¶ä»å•ç›®è¾“å…¥ä¸­äº§ç”Ÿå‡†ç¡®çš„2Då’Œ3Dç‚¹è½¨è¿¹ã€‚è¯„ä¼°å’Œæ„ŸçŸ¥äººç±»ç ”ç©¶è¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGenMOJOç”Ÿæˆçš„åœºæ™¯æ–°é¢–æ€§æ›´çœŸå®ï¼Œç‚¹è½¨è¿¹æ›´å‡†ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GenMOJOè§£å†³äº†ä»å•ç›®å¤šç‰©ä½“è§†é¢‘ä¸­ç”ŸæˆåŠ¨æ€4Dåœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡é›†æˆæ¸²æŸ“åŸºäºå¯å˜å½¢3Dé«˜æ–¯ä¼˜åŒ–ä¸ç”Ÿæˆå…ˆéªŒï¼Œå®ç°åœºæ™¯åˆæˆã€‚</li>
<li>å°†åœºæ™¯åˆ†è§£ä¸ºå•ä¸ªç‰©ä½“ï¼Œé’ˆå¯¹æ¯ä¸ªç‰©ä½“ä¼˜åŒ–å¯å˜å½¢é«˜æ–¯ã€‚</li>
<li>åˆ©ç”¨å¯¹è±¡çº§æ‰©æ•£æ¨¡å‹æ¨æ–­æ–°è§†è§’ä¸­çš„æœªè§‚å¯ŸåŒºåŸŸã€‚</li>
<li>é€šè¿‡è”åˆé«˜æ–¯è´´ç‰‡æŠ€æœ¯æ¸²æŸ“åœºæ™¯ï¼Œæ•æ‰å¯¹è±¡é—´çš„é®æŒ¡ï¼Œå¹¶å®ç°é®æŒ¡æ„ŸçŸ¥ç›‘ç£ã€‚</li>
<li>GenMOJOä½¿ç”¨å¯åŒºåˆ†å˜æ¢å¯¹é½ç”Ÿæˆå’Œæ¸²æŸ“çº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01735a972a608b817594ed8647c1b801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bbd0c2b4183830c85bed5d9630bb5e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-228db414f43ebbcc23d93d1b44a69050.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb89f59474ddeda95fa30ca02ebe6e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a96538da947e1204a62b867a156476ef.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Perceptual-GS-Scene-adaptive-Perceptual-Densification-for-Gaussian-Splatting"><a href="#Perceptual-GS-Scene-adaptive-Perceptual-Densification-for-Gaussian-Splatting" class="headerlink" title="Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian   Splatting"></a>Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian   Splatting</h2><p><strong>Authors:Hongbi Zhou, Zhangkai Ni</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a \cameraready{perceptual sensitivity-adaptive distribution} to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/eezkni/Perceptual-GS">https://github.com/eezkni/Perceptual-GS</a> </p>
<blockquote>
<p>3Dé«˜æ–¯Splattingï¼ˆ3DGSï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ–°æŠ€æœ¯ï¼Œå·²ç»å´­éœ²å¤´è§’ï¼Œç”¨äºåˆæˆæ–°é¢–çš„è§†è§’ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ ¹æ®åœºæ™¯ç‰¹æ€§è‡ªé€‚åº”ä¼˜åŒ–é«˜æ–¯åŸå§‹æ•°æ®çš„åˆ†å¸ƒï¼Œä»è€Œéš¾ä»¥åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å—äººç±»æ„ŸçŸ¥çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåœºæ™¯æ„ŸçŸ¥çš„é«˜æ–¯Splattingå¯†é›†åŒ–ï¼ˆPerceptual-GSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ„ŸçŸ¥æ•æ„Ÿåº¦é›†æˆåˆ°3DGSè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ–°å‹æ¡†æ¶ï¼Œä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ„ŸçŸ¥æ„è¯†è¡¨ç¤ºï¼Œå®ƒæ¨¡æ‹Ÿäº†äººç±»è§†è§‰æ•æ„Ÿåº¦ï¼ŒåŒæ—¶çº¦æŸäº†é«˜æ–¯åŸå§‹æ•°æ®çš„æ•°é‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§\cameraready{æ„ŸçŸ¥æ•æ„Ÿåº¦è‡ªé€‚åº”åˆ†å¸ƒ}ï¼Œä¸ºè§†è§‰å…³é”®åŒºåŸŸåˆ†é…æ›´ç²¾ç»†çš„é«˜æ–¯ç²’åº¦ï¼Œæé«˜äº†é‡å»ºè´¨é‡å’Œé²æ£’æ€§ã€‚åœ¨åŒ…æ‹¬ç”¨äºå¤§è§„æ¨¡åœºæ™¯çš„BungeeNeRFç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPerceptual-GSåœ¨é‡å»ºè´¨é‡ã€æ•ˆç‡å’Œé²æ£’æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/eezkni/Perceptual-GS">https://github.com/eezkni/Perceptual-GS</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12400v1">PDF</a> Accepted to International Conference on Machine Learning (ICML) 2025</p>
<p><strong>Summary</strong></p>
<p>3DGSæŠ€æœ¯å·²æˆä¸ºæ–°é¢–è§†è§’åˆæˆä¸­çš„å¼ºå¤§å·¥å…·ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥æ ¹æ®åœºæ™¯ç‰¹æ€§è‡ªé€‚åº”ä¼˜åŒ–é«˜æ–¯åŸè¯­åˆ†å¸ƒï¼Œä»è€Œéš¾ä»¥å¹³è¡¡é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆæ„ŸçŸ¥æ•æ„Ÿæ€§æ„ŸçŸ¥çš„Gaussian Splattingï¼ˆPerceptual-GSï¼‰æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ„ŸçŸ¥æ•æ„Ÿæ€§çš„è¡¨ç¤ºå½¢å¼ï¼ŒåŒæ—¶é™åˆ¶äº†é«˜æ–¯åŸè¯­çš„æ•°é‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§æ„ŸçŸ¥æ•æ„Ÿæ€§è‡ªé€‚åº”åˆ†å¸ƒï¼Œä¸ºè§†è§‰å…³é”®åŒºåŸŸæä¾›æ›´ç²¾ç»†çš„é«˜æ–¯ç²’åº¦ï¼Œä»¥æé«˜é‡å»ºè´¨é‡å’Œé²æ£’æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPerceptual-GSåœ¨é‡å»ºè´¨é‡ã€æ•ˆç‡å’Œé²æ£’æ€§æ–¹é¢è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D Gaussian Splatting (3DGS)åœ¨æ–°å‹è§†è§’åˆæˆä¸­å…·æœ‰å¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”åœºæ™¯ç‰¹æ€§ä¼˜åŒ–é«˜æ–¯åŸè¯­åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶Perceptual-GSï¼Œç»“åˆæ„ŸçŸ¥æ•æ„Ÿæ€§è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼•å…¥æ„ŸçŸ¥æ•æ„Ÿæ€§çš„è¡¨ç¤ºå½¢å¼ï¼Œé™åˆ¶é«˜æ–¯åŸè¯­æ•°é‡ã€‚</li>
<li>å¼€å‘æ„ŸçŸ¥æ•æ„Ÿæ€§è‡ªé€‚åº”åˆ†å¸ƒï¼Œæé«˜è§†è§‰å…³é”®åŒºåŸŸçš„é‡å»ºè´¨é‡å’Œé²æ£’æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒPerceptual-GSåœ¨é‡å»ºè´¨é‡ã€æ•ˆç‡å’Œé²æ£’æ€§æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1af0d84959e26236a89fc2908a335c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0e0f04c7e82f6bf218f4cfc71c796de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5bd10ddd8c8ce97baf581f00e1b5e51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b14a93a19d8113521640a3bb6255d34.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Hardware-Rasterized-Ray-Based-Gaussian-Splatting"><a href="#Hardware-Rasterized-Ray-Based-Gaussian-Splatting" class="headerlink" title="Hardware-Rasterized Ray-Based Gaussian Splatting"></a>Hardware-Rasterized Ray-Based Gaussian Splatting</h2><p><strong>Authors:Samuel Rota BulÃ², Nemanja Bartolovic, Lorenzo Porzi, Peter Kontschieder</strong></p>
<p>We present a novel, hardware rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), obtaining both fast and high-quality results for novel view synthesis. Our work contains a mathematically rigorous and geometrically intuitive derivation about how to efficiently estimate all relevant quantities for rendering RayGS models, structured with respect to standard hardware rasterization shaders. Our solution is the first enabling rendering RayGS models at sufficiently high frame rates to support quality-sensitive applications like Virtual and Mixed Reality. Our second contribution enables alias-free rendering for RayGS, by addressing MIP-related issues arising when rendering diverging scales during training and testing. We demonstrate significant performance gains, across different benchmark scenes, while retaining state-of-the-art appearance quality of RayGS. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬ä»¶å…‰æ …åŒ–æ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºåŸºäºå°„çº¿çš„ä¸‰ç»´é«˜æ–¯æ··åˆï¼ˆRayGSï¼‰ï¼Œä¸ºæ–°é¢–è§†å›¾åˆæˆè·å¾—å¿«é€Ÿä¸”é«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬çš„å·¥ä½œåŒ…å«å…³äºå¦‚ä½•æœ‰æ•ˆåœ°ä¼°è®¡æ‰€æœ‰ä¸æ¸²æŸ“RayGSæ¨¡å‹ç›¸å…³çš„æ•°é‡çš„ä¸¥è°¨æ•°å­¦å’Œç›´è§‚å‡ ä½•æ¨å¯¼ï¼Œä»¥æ ‡å‡†çš„ç¡¬ä»¶å…‰æ …åŒ–ç€è‰²å™¨è¿›è¡Œç»“æ„åŒ–ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆé¦–æ¬¡æ”¯æŒä»¥è¶³å¤Ÿé«˜çš„å¸§ç‡æ¸²æŸ“RayGSæ¨¡å‹ï¼Œä»¥æ”¯æŒåƒè™šæ‹Ÿå’Œæ··åˆç°å®è¿™æ ·çš„å¯¹è´¨é‡æ•æ„Ÿçš„åº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯é€šè¿‡è§£å†³åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´å‡ºç°ä¸MIPç›¸å…³çš„å‘æ•£å°ºåº¦é—®é¢˜ï¼Œå®ç°äº†RayGSçš„æ— é‡å½±æ¸²æŸ“ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•åœºæ™¯ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†RayGSçš„å…ˆè¿›å¤–è§‚è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18682v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬ä»¶å…‰æ …åŒ–æ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºåŸºäºå°„çº¿çš„ä¸‰ç»´é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ï¼ˆRayGSï¼‰ï¼Œå®ç°å¿«é€Ÿé«˜è´¨é‡çš„æ–°è§†è§’åˆæˆæ•ˆæœã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†å¦‚ä½•æœ‰æ•ˆåœ°ä¼°ç®—RayGSæ¨¡å‹æ¸²æŸ“æ‰€éœ€çš„æ‰€æœ‰ç›¸å…³å‚æ•°ï¼Œå¹¶é¦–æ¬¡å®ç°äº†ä»¥è¶³å¤Ÿé«˜çš„å¸§ç‡æ¸²æŸ“RayGSæ¨¡å‹ï¼Œæ”¯æŒè™šæ‹Ÿç°å®å’Œæ··åˆç°å®ç­‰è´¨é‡æ•æ„Ÿå‹åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è§£å†³äº†MIPç›¸å…³çš„æ¸²æŸ“é—®é¢˜ï¼Œå®ç°äº†RayGSçš„æ— æ··å æ¸²æŸ“ã€‚æ•´ä½“ä¸Šï¼Œè¯¥ç ”ç©¶åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†RayGSçš„é¢†å…ˆå¤–è§‚è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬ä»¶å…‰æ …åŒ–æ¸²æŸ“æ–¹æ³•ç”¨äºRayGSæ¨¡å‹æ¸²æŸ“ã€‚</li>
<li>ä¸ºRayGSæ¨¡å‹æä¾›äº†æ•°å­¦ä¸¥è°¨å’Œå‡ ä½•ç›´è§‚çš„å‚æ•°ä¼°ç®—æ–¹æ³•ã€‚</li>
<li>ç¬¬ä¸€æ¬¡å®ç°äº†ä»¥é«˜å¸§ç‡è¿›è¡ŒRayGSæ¨¡å‹çš„æ¸²æŸ“ï¼Œé€‚ç”¨äºVRå’Œæ··åˆç°å®ç­‰åº”ç”¨ã€‚</li>
<li>è§£å†³MIPé—®é¢˜ï¼Œå®ç°äº†RayGSæ¨¡å‹çš„æ— æ··å æ¸²æŸ“ã€‚</li>
<li>åœ¨ä¸åŒåŸºå‡†æµ‹è¯•åœºæ™¯ä¸­å±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ca7ffb9e6d97a0aedcdbcb6a43d63da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-469e5b1478daa04198fdaddedb160b7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce52d244ff81aa5dc43e0238caca814c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-408c0b0fc4e13e0acc36841564792c0e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Niagara-Normal-Integrated-Geometric-Affine-Field-for-Scene-Reconstruction-from-a-Single-View"><a href="#Niagara-Normal-Integrated-Geometric-Affine-Field-for-Scene-Reconstruction-from-a-Single-View" class="headerlink" title="Niagara: Normal-Integrated Geometric Affine Field for Scene   Reconstruction from a Single View"></a>Niagara: Normal-Integrated Geometric Affine Field for Scene   Reconstruction from a Single View</h2><p><strong>Authors:Xianzu Wu, Zhenxin Ai, Harry Yang, Ser-Nam Lim, Jun Liu, Huan Wang</strong></p>
<p>Recent advances in single-view 3D scene reconstruction have highlighted the challenges in capturing fine geometric details and ensuring structural consistency, particularly in high-fidelity outdoor scene modeling. This paper presents Niagara, a new single-view 3D scene reconstruction framework that can faithfully reconstruct challenging outdoor scenes from a single input image for the first time.   Our approach integrates monocular depth and normal estimation as input, which substantially improves its ability to capture fine details, mitigating common issues like geometric detail loss and deformation.   Additionally, we introduce a geometric affine field (GAF) and 3D self-attention as geometry-constraint, which combines the structural properties of explicit geometry with the adaptability of implicit feature fields, striking a balance between efficient rendering and high-fidelity reconstruction.   Our framework finally proposes a specialized encoder-decoder architecture, where a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian parameters, which can be used for novel view synthesis. Extensive results and analyses suggest that our Niagara surpasses prior SoTA approaches such as Flash3D in both single-view and dual-view settings, significantly enhancing the geometric accuracy and visual fidelity, especially in outdoor scenes. </p>
<blockquote>
<p>è¿‘æœŸå•è§†å›¾ä¸‰ç»´åœºæ™¯é‡å»ºçš„æœ€æ–°è¿›å±•å‡¸æ˜¾äº†æ•æ‰ç²¾ç»†å‡ ä½•ç»†èŠ‚å’Œç¡®ä¿ç»“æ„ä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ä¿çœŸå®¤å¤–åœºæ™¯å»ºæ¨¡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†Niagaraï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å•è§†å›¾ä¸‰ç»´åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œèƒ½å¤Ÿé¦–æ¬¡ä»å•ä¸ªè¾“å…¥å›¾åƒä¸­å‡†ç¡®åœ°é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å¤–åœºæ™¯ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬çš„æ–¹æ³•å°†å•ç›®æ·±åº¦ä¼°è®¡å’Œæ³•çº¿ä¼°è®¡ä½œä¸ºè¾“å…¥è¿›è¡Œé›†æˆï¼Œè¿™å¤§å¤§æé«˜äº†æ•æ‰ç»†èŠ‚çš„èƒ½åŠ›ï¼Œç¼“è§£äº†å¸¸è§çš„å‡ ä½•ç»†èŠ‚ä¸¢å¤±å’Œå˜å½¢ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ ä½•ä»¿å°„åœºï¼ˆGAFï¼‰å’Œä¸‰ç»´è‡ªæ³¨æ„åŠ›ä½œä¸ºå‡ ä½•çº¦æŸï¼Œå®ƒå°†æ˜¾å¼å‡ ä½•çš„ç»“æ„å±æ€§ä¸éšå¼ç‰¹å¾åœºçš„é€‚åº”æ€§ç›¸ç»“åˆï¼Œåœ¨é«˜æ•ˆæ¸²æŸ“å’Œé«˜ä¿çœŸé‡å»ºä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12553v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Niagaraï¼Œä¸€ç§æ–°å‹çš„å•è§†å›¾3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼Œå¯é¦–æ¬¡ä»å•ä¸€è¾“å…¥å›¾åƒä¸­å¿ å®é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æˆ·å¤–åœºæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå•ç›®æ·±åº¦ä¸æ³•çº¿ä¼°è®¡ï¼Œæé«˜äº†æ•æ‰ç²¾ç»†ç»†èŠ‚çš„èƒ½åŠ›ï¼Œè§£å†³äº†å¸¸è§çš„å‡ ä½•ç»†èŠ‚ä¸¢å¤±å’Œå˜å½¢é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†å‡ ä½•ä»¿å°„åœºï¼ˆGAFï¼‰å’Œ3Dè‡ªæ³¨æ„åŠ›ä½œä¸ºå‡ ä½•çº¦æŸï¼Œç»“åˆäº†æ˜¾å¼å‡ ä½•çš„ç»“æ„å±æ€§å’Œéšå¼ç‰¹å¾åœºçš„é€‚åº”æ€§ï¼Œå®ç°äº†é«˜æ•ˆæ¸²æŸ“ä¸é«˜ä¿çœŸé‡å»ºä¹‹é—´çš„å¹³è¡¡ã€‚æœ€åï¼Œè¯¥æ¡†æ¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦çš„3Dé«˜æ–¯è§£ç å™¨ï¼Œç”¨äºé¢„æµ‹3Dé«˜æ–¯å‚æ•°ï¼Œå¯ç”¨äºæ–°é¢–è§†å›¾åˆæˆã€‚å®éªŒç»“æœä¸åˆ†ææ˜¾ç¤ºï¼ŒNiagaraåœ¨å•è§†å›¾å’ŒåŒè§†å›¾è®¾ç½®ä¸­å‡è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†å‡ ä½•ç²¾åº¦å’Œè§†è§‰ä¿çœŸåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æˆ·å¤–åœºæ™¯æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Niagaraæ˜¯ä¸€ç§æ–°å‹çš„å•è§†å›¾3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼Œèƒ½å¤Ÿå¿ å®é‡å»ºæˆ·å¤–åœºæ™¯ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå•ç›®æ·±åº¦ä¸æ³•çº¿ä¼°è®¡ï¼Œæé«˜äº†æ•æ‰ç²¾ç»†ç»†èŠ‚çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†å‡ ä½•ä»¿å°„åœºï¼ˆGAFï¼‰å’Œ3Dè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†é«˜æ•ˆæ¸²æŸ“ä¸é«˜ä¿çœŸé‡å»ºä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æ¡†æ¶ä¸­çš„æ·±åº¦åŸºäºçš„3Dé«˜æ–¯è§£ç å™¨å¯é¢„æµ‹3Dé«˜æ–¯å‚æ•°ï¼Œç”¨äºæ–°é¢–è§†å›¾åˆæˆã€‚</li>
<li>Niagaraæ˜¾è‘—æé«˜äº†å‡ ä½•ç²¾åº¦å’Œè§†è§‰ä¿çœŸåº¦ï¼Œå°¤å…¶æ˜¯åœ¨æˆ·å¤–åœºæ™¯æ–¹é¢ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å•è§†å›¾å’ŒåŒè§†å›¾è®¾ç½®ä¸­éƒ½è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d0a921dcee621a30d13954c72b2d98a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-343f566f2e780d25ddf5412dc7f2abbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3195ebcc300455855de2a52b9fb1ae4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efa6f78874061e97f4dc25291d7ed818.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GS-QA-Comprehensive-Quality-Assessment-Benchmark-for-Gaussian-Splatting-View-Synthesis"><a href="#GS-QA-Comprehensive-Quality-Assessment-Benchmark-for-Gaussian-Splatting-View-Synthesis" class="headerlink" title="GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting   View Synthesis"></a>GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting   View Synthesis</h2><p><strong>Authors:Pedro Martin, AntÃ³nio Rodrigues, JoÃ£o Ascenso, Maria Paula Queluz</strong></p>
<p>Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics. </p>
<blockquote>
<p>é«˜æ–¯è´´å›¾ï¼ˆGSï¼‰ä¸ºå®æ—¶3Dåœºæ™¯æ¸²æŸ“æä¾›äº†å¦ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå³å¯¹ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ›¿ä»£ã€‚ä½¿ç”¨ä¸€ç»„3Dé«˜æ–¯åˆ†å¸ƒæ¥è¡¨ç¤ºå¤æ‚çš„å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ï¼ŒGSå®ç°äº†æ¯”NeRFä¸­ä½¿ç”¨çš„ç¥ç»ç½‘ç»œæ–¹æ³•æ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜æ¶ˆè€—ã€‚ç„¶è€Œï¼ŒGSç”Ÿæˆçš„é™æ€å†…å®¹çš„è´¨é‡è¯„ä¼°å°šæœªæ·±å…¥æ¢ç´¢ã€‚æœ¬æ–‡æè¿°äº†ä¸€é¡¹ä¸»è§‚è´¨é‡è¯„ä¼°ç ”ç©¶ï¼Œæ—¨åœ¨è¯„ä¼°é‡‡ç”¨å‡ ç§æœ€æ–°é™æ€GSæŠ€æœ¯åˆæˆçš„è§†é¢‘çš„è´¨é‡ã€‚è¿™äº›æ–¹æ³•åº”ç”¨äºå¤šç§è§†è§‰åœºæ™¯ï¼Œæ¶µç›–360åº¦ä»¥åŠé¢å‘å‰çš„ç›¸æœºè½¨è¿¹ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ä¸»è§‚ç ”ç©¶å¾—å‡ºçš„åˆ†æ•°å¯¹18é¡¹å®¢è§‚è´¨é‡æŒ‡æ ‡çš„æ€§èƒ½è¿›è¡Œäº†åˆ†æï¼Œæ·±å…¥äº†è§£äº†å®ƒä»¬çš„ä¼˜ç‚¹ã€å±€é™æ€§ä»¥åŠä¸äººç±»æ„ŸçŸ¥çš„å¥‘åˆåº¦ã€‚æ‰€æœ‰è§†é¢‘å’Œåˆ†æ•°å‡æä¾›ï¼Œå½¢æˆä¸€ä¸ªç»¼åˆæ•°æ®åº“ï¼Œå¯ç”¨äºGSè§†å›¾åˆæˆå’Œå®¢è§‚è´¨é‡æŒ‡æ ‡çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13196v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜æ–¯Splattingï¼ˆGSï¼‰ä¸ºå®æ—¶3Dåœºæ™¯æ¸²æŸ“æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç›¸è¾ƒäºç¥ç»ç½‘ç»œæ–¹æ³•NeRFï¼Œå®ƒä½¿ç”¨ä¸€ç»„3Dé«˜æ–¯æ¥ä»£è¡¨å¤æ‚çš„å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ï¼Œå®ç°æ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜æ¶ˆè€—ã€‚ç„¶è€Œï¼ŒGSç”Ÿæˆçš„é™æ€å†…å®¹çš„å“è´¨è¯„ä¼°å°šæœªè¢«æ·±å…¥æ¢ç´¢ã€‚æœ¬æ–‡æè¿°äº†ä¸€é¡¹ä¸»è§‚è´¨é‡è¯„ä¼°ç ”ç©¶ï¼Œæ—¨åœ¨è¯„ä¼°ä½¿ç”¨å¤šç§é™æ€GSå‰æ²¿æ–¹æ³•åˆæˆçš„è§†é¢‘è´¨é‡ã€‚è¿™äº›æ–¹æ³•åº”ç”¨äºå¤šç§è§†è§‰åœºæ™¯ï¼ŒåŒ…æ‹¬å…¨æ™¯å’Œé¢å‘å‰çš„ç›¸æœºè½¨è¿¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹ä¸»è§‚ç ”ç©¶ç»“æœçš„å¾—åˆ†åˆ†æï¼Œå¯¹18ç§å®¢è§‚è´¨é‡æŒ‡æ ‡çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œæ·±å…¥æ­ç¤ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ä¸ä¸äººçš„æ„ŸçŸ¥çš„ä¸€è‡´æ€§ã€‚æ‰€æœ‰è§†é¢‘å’Œå¾—åˆ†æ•°æ®å‡å·²å…¬å¼€æä¾›ï¼Œä¸ºGSè§†å›¾åˆæˆå’Œå®¢è§‚è´¨é‡æŒ‡æ ‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ•°æ®åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯Splattingï¼ˆGSï¼‰æä¾›äº†ä¸€ç§å®æ—¶3Dåœºæ™¯æ¸²æŸ“çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç›¸æ¯”NeRFæœ‰æ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜æ¶ˆè€—ã€‚</li>
<li>GSä½¿ç”¨ä¸€ç»„3Dé«˜æ–¯æ¥ä»£è¡¨å¤æ‚çš„å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ã€‚</li>
<li>å½“å‰ç ”ç©¶å¯¹GSç”Ÿæˆçš„é™æ€å†…å®¹çš„å“è´¨è¯„ä¼°å°šæœªæ·±å…¥æ¢ç´¢ã€‚</li>
<li>è®ºæ–‡è¿›è¡Œäº†ä¸€é¡¹ä¸»è§‚è´¨é‡è¯„ä¼°ç ”ç©¶ï¼Œè¯„ä¼°äº†å¤šç§é™æ€GSæ–¹æ³•åˆæˆçš„è§†é¢‘è´¨é‡ã€‚</li>
<li>è¿™äº›æ–¹æ³•åº”ç”¨äºä¸åŒçš„è§†è§‰åœºæ™¯ï¼ŒåŒ…æ‹¬å…¨æ™¯å’Œé¢å‘å‰çš„ç›¸æœºè½¨è¿¹ã€‚</li>
<li>å¯¹18ç§å®¢è§‚è´¨é‡æŒ‡æ ‡çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ä»¥åŠä¸äººç±»æ„ŸçŸ¥çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32f62fbe606ac3934a4026180aeece77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2b3726ed806426b0da77021c706920b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4bdcc0093975866d42ed0e159e77de2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e71ed3d0328482fb811dffc8f7088ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-753a5a73fabbe4b08da6da25f8b16340.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-073fbd39640fe883dbce33fb63ceecf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fcf79e08c86ce3c0980082d377ccbff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b3ff5fb4c127a550822477da2e512cc.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Deblur-Avatar-Animatable-Avatars-from-Motion-Blurred-Monocular-Videos"><a href="#Deblur-Avatar-Animatable-Avatars-from-Motion-Blurred-Monocular-Videos" class="headerlink" title="Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos"></a>Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</h2><p><strong>Authors:Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin</strong></p>
<p>We introduce a novel framework for modeling high-fidelity, animatable 3D human avatars from motion-blurred monocular video inputs. Motion blur is prevalent in real-world dynamic video capture, especially due to human movements in 3D human avatar modeling. Existing methods either (1) assume sharp image inputs, failing to address the detail loss introduced by motion blur, or (2) mainly consider blur by camera movements, neglecting the human motion blur which is more common in animatable avatars. Our proposed approach integrates a human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By explicitly modeling human motion trajectories during exposure time, we jointly optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality human avatars. We employ a pose-dependent fusion mechanism to distinguish moving body regions, optimizing both blurred and sharp areas effectively. Extensive experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing methods in rendering quality and quantitative metrics, producing sharp avatar reconstructions and enabling real-time rendering under challenging motion blur conditions. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç”¨äºä»è¿åŠ¨æ¨¡ç³Šçš„å•ç›®è§†é¢‘è¾“å…¥ä¸­å»ºç«‹é«˜ä¿çœŸã€å¯åŠ¨ç”»çš„3Däººç±»åŒ–èº«ã€‚è¿åŠ¨æ¨¡ç³Šåœ¨çœŸå®ä¸–ç•Œçš„åŠ¨æ€è§†é¢‘æ•æ‰ä¸­æ™®éå­˜åœ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨3Däººç±»åŒ–èº«å»ºæ¨¡ä¸­ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆï¼ˆ1ï¼‰å‡è®¾å›¾åƒè¾“å…¥æ˜¯æ¸…æ™°çš„ï¼Œæ— æ³•å¤„ç†è¿åŠ¨æ¨¡ç³Šå¼•å…¥çš„ç»†èŠ‚æŸå¤±ï¼›è¦ä¹ˆï¼ˆ2ï¼‰ä¸»è¦å…³æ³¨ç”±ç›¸æœºè¿åŠ¨å¼•èµ·çš„æ¨¡ç³Šï¼Œè€Œå¿½è§†åœ¨å¯åŠ¨ç”»åŒ–èº«ä¸­æ›´ä¸ºæ™®éçš„äººç±»è¿åŠ¨æ¨¡ç³Šã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å°†åŸºäºäººç±»è¿åŠ¨çš„è¿åŠ¨æ¨¡ç³Šæ¨¡å‹é›†æˆåˆ°3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ä¸­ã€‚é€šè¿‡æ˜¾å¼å»ºæ¨¡æ›å…‰æœŸé—´çš„äººç±»è¿åŠ¨è½¨è¿¹ï¼Œæˆ‘ä»¬è”åˆä¼˜åŒ–è½¨è¿¹å’Œ3Dé«˜æ–¯ï¼Œä»¥é‡å»ºæ¸…æ™°ã€é«˜è´¨é‡çš„äººç±»åŒ–èº«ã€‚æˆ‘ä»¬é‡‡ç”¨å§¿æ€ä¾èµ–çš„èåˆæœºåˆ¶æ¥åŒºåˆ†ç§»åŠ¨çš„èº«ä½“åŒºåŸŸï¼Œæœ‰æ•ˆåœ°ä¼˜åŒ–æ¨¡ç³Šå’Œæ¸…æ™°åŒºåŸŸã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ¸…æ™°çš„åŒ–èº«é‡å»ºï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¿åŠ¨æ¨¡ç³Šæ¡ä»¶ä¸‹å®ç°å®æ—¶æ¸²æŸ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13335v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿä»è¿åŠ¨æ¨¡ç³Šçš„å•ç›®è§†é¢‘è¾“å…¥ä¸­å»ºç«‹é«˜ä¿çœŸã€å¯åŠ¨ç”»çš„3Däººç±»è§’è‰²æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåŸºäºäººç±»è¿åŠ¨çš„æ¨¡ç³Šæ¨¡å‹ä¸ä¸‰ç»´é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼ˆ3DGSï¼‰ï¼Œæ˜¾å¼æ¨¡æ‹Ÿæ›å…‰æœŸé—´çš„äººç±»è¿åŠ¨è½¨è¿¹ï¼Œè”åˆä¼˜åŒ–è½¨è¿¹å’Œä¸‰ç»´é«˜æ–¯è´´å›¾æ¥é‡å»ºæ¸…æ™°ã€é«˜è´¨é‡çš„3Däººç±»è§’è‰²æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œé‡åŒ–æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¿åŠ¨æ¨¡ç³Šæ¡ä»¶ä¸‹å®ç°æ¸…æ™°çš„è§’è‰²é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿä»è¿åŠ¨æ¨¡ç³Šçš„å•ç›®è§†é¢‘å»ºç«‹3Däººç±»è§’è‰²æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶é›†æˆäº†åŸºäºäººç±»è¿åŠ¨çš„æ¨¡ç³Šæ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼ˆ3DGSï¼‰ã€‚</li>
<li>æ˜¾å¼æ¨¡æ‹Ÿæ›å…‰æœŸé—´çš„äººç±»è¿åŠ¨è½¨è¿¹ã€‚</li>
<li>é€šè¿‡è”åˆä¼˜åŒ–è½¨è¿¹å’Œä¸‰ç»´é«˜æ–¯è´´å›¾æ¥é‡å»ºæ¸…æ™°ã€é«˜è´¨é‡çš„3Dè§’è‰²æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨äº†å§¿æ€ä¾èµ–çš„èåˆæœºåˆ¶ï¼Œä»¥åŒºåˆ†ç§»åŠ¨çš„èº«ä½“åŒºåŸŸï¼Œæœ‰æ•ˆä¼˜åŒ–æ¨¡ç³Šå’Œæ¸…æ™°åŒºåŸŸã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œé‡åŒ–æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00953aa0fe8317fbdf95a8f77c618d3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61d01936c09d86fd6d8b68700e04de13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a99003aaf07d8f05093e518b76cade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68d73820f0f35573efd460a31c4a5d50.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MaskGaussian-Adaptive-3D-Gaussian-Representation-from-Probabilistic-Masks"><a href="#MaskGaussian-Adaptive-3D-Gaussian-Representation-from-Probabilistic-Masks" class="headerlink" title="MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic   Masks"></a>MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic   Masks</h2><p><strong>Authors:Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun</strong></p>
<p>While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: <a target="_blank" rel="noopener" href="https://github.com/kaikai23/MaskGaussian">https://github.com/kaikai23/MaskGaussian</a> </p>
<blockquote>
<p>è™½ç„¶3Dé«˜æ–¯æ‘Šé“ºï¼ˆ3DGSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆå’Œå®æ—¶æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ç”±äºä½¿ç”¨äº†æ•°ç™¾ä¸‡ä¸ªé«˜æ–¯ï¼Œå…¶é«˜å†…å­˜æ¶ˆè€—é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»é€šè¿‡åˆ é™¤ä¸å¿…è¦çš„é«˜æ–¯è¿›è¡Œäº†æ”¹è¿›ï¼Œæ–¹æ³•åŒ…æ‹¬ä½¿ç”¨æ‰‹å·¥åˆ¶å®šçš„æ ‡å‡†æˆ–ä½¿ç”¨å­¦ä¹ åˆ°çš„æ©è†œã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åŸºäºä¿®å‰ªæ—¶åˆ»çš„å¿«ç…§ç¡®å®šæ€§åœ°åˆ é™¤é«˜æ–¯ï¼Œä»é•¿è¿œæ¥çœ‹å¯¼è‡´é‡å»ºæ€§èƒ½æ¬¡ä¼˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MaskGaussianï¼Œå®ƒå°†é«˜æ–¯å»ºæ¨¡ä¸ºæ¦‚ç‡å®ä½“ï¼Œè€Œä¸æ˜¯æ°¸ä¹…åˆ é™¤å®ƒä»¬ï¼Œå¹¶æ ¹æ®å…¶å­˜åœ¨çš„æ¦‚ç‡æ¥ä½¿ç”¨å®ƒä»¬ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ©è†œå…‰æ …åŒ–æŠ€æœ¯ï¼Œä½¿æœªä½¿ç”¨ä½†æ¦‚ç‡å­˜åœ¨çš„é«˜æ–¯èƒ½å¤Ÿè·å¾—æ¢¯åº¦ï¼Œä»è€Œèƒ½å¤ŸåŠ¨æ€è¯„ä¼°å®ƒä»¬å¯¹ä¸æ–­å˜åŒ–çš„åœºæ™¯çš„è´¡çŒ®å¹¶è°ƒæ•´å…¶å­˜åœ¨çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œé«˜æ–¯çš„é‡è¦æ€§ä¼šè¿­ä»£åœ°æ”¹å˜ï¼Œè¢«ä¿®å‰ªçš„é«˜æ–¯é€‰æ‹©ä¹Ÿå¤šæ ·åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è¾¾åˆ°æ›´å¥½çš„æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºä»¥å‰çš„é«˜æ–¯ä¿®å‰ªæ–¹æ³•ï¼Œå¹³å‡ä¿®å‰ªè¶…è¿‡60%çš„é«˜æ–¯ï¼Œè€ŒPSNRä»…ä¸‹é™0.02ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/kaikai23/MaskGaussian">https://github.com/kaikai23/MaskGaussian</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20522v3">PDF</a> CVPR 2025; Project page:<a target="_blank" rel="noopener" href="https://maskgaussian.github.io/">https://maskgaussian.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºä¸‰ç»´é«˜æ–¯å·ç§¯æŠ€æœ¯çš„æ¸²æŸ“æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ã€‚å†…å­˜æ¶ˆè€—è¿‡å¤§é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†MaskGaussianæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†é«˜æ–¯å·ç§¯è§†ä¸ºæ¦‚ç‡å®ä½“è€Œéæ°¸ä¹…ç§»é™¤ï¼Œé€šè¿‡åŠ¨æ€è¯„ä¼°å…¶å¯¹åœºæ™¯å˜åŒ–çš„è´¡çŒ®æ¥è°ƒæ•´å…¶å­˜åœ¨æ¦‚ç‡ã€‚é‡‡ç”¨æ©ç›–æ¸²æŸ“æŠ€æœ¯ä½¿å¾—æœªè¢«ä½¿ç”¨çš„æ½œåœ¨é«˜æ–¯å·ç§¯å¾—ä»¥æ¥å—æ¢¯åº¦æ›´æ–°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•çš„æ¸²æŸ“è´¨é‡æ›´ä½³ï¼Œå¹¶èƒ½å¤Ÿåœ¨å¹³å‡æƒ…å†µä¸‹å‰”é™¤è¶…è¿‡å…­æˆçš„é«˜æ–¯å·ç§¯è€Œä¸å½±å“PSNRä»…è½»å¾®ä¸‹é™ã€‚ç›®å‰ç ”ç©¶ä»£ç å·²ä¸Šä¼ è‡³GitHubä»“åº“ä¾›ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å½“å‰ä¸»æµä¸‰ç»´é«˜æ–¯å·ç§¯æ¸²æŸ“æŠ€æœ¯çš„ç“¶é¢ˆï¼Œå†…å­˜æ¶ˆè€—è¿‡é«˜ï¼Œé™åˆ¶å…¶å®ç”¨æ€§ã€‚</li>
<li>æå‡ºMaskGaussianæ–¹æ³•ï¼Œå°†é«˜æ–¯å·ç§¯è§†ä¸ºæ¦‚ç‡å®ä½“è€Œéå›ºå®šç§»é™¤ï¼Œä»¥åŠ¨æ€è¯„ä¼°å…¶å¯¹åœºæ™¯å˜åŒ–çš„è´¡çŒ®æ¥è°ƒæ•´å…¶å­˜åœ¨æ¦‚ç‡ã€‚è¿™ä¸€ç­–ç•¥ä½¿é€‰æ‹©åˆ é™¤çš„é«˜æ–¯æ›´å…·å¤šæ ·æ€§ã€‚</li>
<li>MaskGaussianåˆ©ç”¨æ©ç›–æ¸²æŸ“æŠ€æœ¯ä½¿æ½œåœ¨é«˜æ–¯èƒ½å¤Ÿæ¥æ”¶æ¢¯åº¦æ›´æ–°ï¼Œæå‡å¯¹åœºæ™¯åŠ¨æ€çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a9098fa1e21c6bfb072bb733303cc6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e527205d731c6c386552e2ea38669f99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2d0765651b6968a22fd7d2ceb6ec2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48ceff63829ec22e11fbf3b662df60df.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-646273fa036c24b95a127a7dc987a8f0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  RA-NeRF Robust Neural Radiance Field Reconstruction with Accurate   Camera Pose Estimation under Complex Trajectories
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c00f94d61873b744a72e97f748e23fd.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Public Acceptance of Cybernetic Avatars in the service sector Evidence   from a Large-Scale Survey in Dubai
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
