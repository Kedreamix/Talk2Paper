<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Egocentric Human-Object Interaction Detection A New Benchmark and   Method">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6f4b8b55483559cb9d3fe6bb4982a997.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="Egocentric-Human-Object-Interaction-Detection-A-New-Benchmark-and-Method"><a href="#Egocentric-Human-Object-Interaction-Detection-A-New-Benchmark-and-Method" class="headerlink" title="Egocentric Human-Object Interaction Detection: A New Benchmark and   Method"></a>Egocentric Human-Object Interaction Detection: A New Benchmark and   Method</h2><p><strong>Authors:Kunyuan Deng, Yi Wang, Lap-Pui Chau</strong></p>
<p>Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: <a target="_blank" rel="noopener" href="https://dengkunyuan.github.io/EgoHOIBench/">https://dengkunyuan.github.io/EgoHOIBench/</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¯¹äººç±»ä¸ç‰©ä½“ä¹‹é—´äº¤äº’çš„ç†è§£å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰çš„åŸºäºäººæœºäº¤äº’ï¼ˆHOIï¼‰çš„æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾§é‡äºç¬¬ä¸‰äººç§°è§†è§’ï¼Œå¿½ç•¥äº†ä»äººæœºäº¤äº’çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§’ï¼ˆå³Ego-HOIï¼‰å‡ºå‘çš„æ›´ç›´è§‚çš„æ–¹å¼ã€‚æœ¬æ–‡ä»‹ç»äº†Ego-HOIBenchæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨æ¨åŠ¨Ego-HOIæ£€æµ‹çš„æ€§èƒ½åŸºå‡†å’Œå‘å±•ã€‚æˆ‘ä»¬çš„Ego-HOIBenchåŒ…å«è¶…è¿‡27Kå¼ ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„é«˜è´¨é‡å›¾åƒï¼Œå…¶ä¸­åŒ…å«æ‰‹-åŠ¨è¯-å¯¹è±¡ä¸‰å…ƒç»„æ³¨é‡Šï¼Œæ¶µç›–è¶…è¿‡123ä¸ªç²¾ç»†äº¤äº’ç±»åˆ«å’Œä½ç½®ï¼Œæ¶µç›–ä¸°å¯Œçš„åœºæ™¯ã€å¯¹è±¡ç±»å‹å’Œæ—¥å¸¸æ´»åŠ¨ä¸­çš„æ‰‹éƒ¨é…ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢å¹¶é€‚åº”äº†ç¬¬ä¸‰äººç§°çš„HOIæ£€æµ‹æ–¹æ³•ï¼Œä»¥åº”ç”¨äºEgo-HOIBenchï¼Œå¹¶è¯´æ˜äº†æ‰‹éƒ¨é®æŒ¡ç‰©ä½“ä»¥åŠå•æ‰‹å’ŒåŒæ‰‹äº¤äº’çš„å¤æ‚æ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å»ºç«‹æ–°çš„åŸºå‡†ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰‹éƒ¨å‡ ä½•å’Œäº¤äº’ç»†åŒ–ï¼ˆHGIRï¼‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨æ‰‹éƒ¨å§¿åŠ¿å’Œå‡ ä½•ä¿¡æ¯ä½œä¸ºè§£é‡Šäº¤äº’çš„æœ‰ä»·å€¼çº¿ç´¢ã€‚å…·ä½“è€Œè¨€ï¼ŒHGIRæ–¹æ¡ˆä»ä¼°è®¡çš„æ‰‹éƒ¨å§¿åŠ¿ææ¡ˆä¸­æ˜ç¡®æå–å…¨å±€æ‰‹éƒ¨å‡ ä½•ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å§¿æ€äº¤äº’æ³¨æ„åŠ›ç»†åŒ–ç‰¹å®šäº¤äº’çš„ç‰¹å¾ã€‚è¯¥æ–¹æ¡ˆä½¿æ¨¡å‹èƒ½å¤Ÿè·å¾—ç¨³å¥ä¸”å¼ºå¤§çš„äº¤äº’è¡¨ç¤ºï¼Œä»è€Œæå¤§åœ°æé«˜äº†Ego-HOIæ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•è½»å·§æœ‰æ•ˆï¼Œå¯è½»æ¾åº”ç”¨äºHOIåŸºçº¿é‡‡ç”¨éšæ’å³ç”¨æ–¹å¼æ¥å®ç°æœ€æ–°çš„æˆæœè¡¨ç°ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://dengkunyuan.github.io/EgoHOIBench/">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14189v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Ego-HOIBenchï¼Œç”¨äºæ¨åŠ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº’åŠ¨æ£€æµ‹ï¼ˆEgo-HOIï¼‰çš„åŸºå‡†æµ‹è¯•å’Œç®—æ³•å‘å±•ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡2ä¸‡å¼ é«˜è´¨é‡çš„æ‰‹éƒ¨åŠ¨ä½œå¯¹è±¡ä¸‰å…ƒç»„æ ‡æ³¨çš„å›¾åƒï¼Œæ¶µç›–æ—¥å¸¸æ´»åŠ¨ä¸­çš„ä¸°å¯Œåœºæ™¯ã€å¯¹è±¡ç±»å‹å’Œæ‰‹éƒ¨é…ç½®ã€‚æ–‡ç« è¿˜æ¢è®¨äº†ä»ç¬¬ä¸‰äººç§°äº’åŠ¨æ£€æµ‹å‘Ego-HOIBenchçš„è½¬å˜ï¼Œå¹¶ä»‹ç»äº†åœ¨å•æ‰‹é®æŒ¡å’Œå•æ‰‹åŠåŒæ‰‹äº’åŠ¨æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å»ºç«‹æ–°çš„åŸºå‡†çº¿ï¼Œæœ¬æ–‡æå‡ºäº†æ‰‹å‹å‡ ä½•ä¸äº’åŠ¨æ€§ç²¾ç‚¼ï¼ˆHGIRï¼‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨æ‰‹éƒ¨å§¿åŠ¿å’Œå‡ ä½•ä¿¡æ¯ä½œä¸ºè§£é‡Šäº’åŠ¨çš„å®è´µçº¿ç´¢ã€‚è¯¥æ–¹æ¡ˆèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„äº’åŠ¨è¡¨ç°åŠ›ï¼Œæ˜¾è‘—æé«˜Ego-HOIæ£€æµ‹èƒ½åŠ›ã€‚æ­¤æ–¹æ¡ˆè½»ä¾¿æœ‰æ•ˆï¼Œå¯è½»æ¾åº”ç”¨äºç°æœ‰HOIåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚æ›´å¤šä¿¡æ¯å¯è®¿é—®é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://dengkunyuan.github.io/EgoHOIBench/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Ego-HOIBenchï¼Œä¸“æ³¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº’åŠ¨æ£€æµ‹ï¼ˆEgo-HOIï¼‰ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§åœºæ™¯ã€å¯¹è±¡ç±»å‹å’Œæ‰‹éƒ¨é…ç½®çš„é«˜è´¨é‡çš„å›¾åƒæ•°æ®ã€‚</li>
<li>æ¢è®¨äº†ä»ç¬¬ä¸‰äººç§°è§†è§’çš„äº’åŠ¨æ£€æµ‹åˆ°Ego-HOIBenchçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ‰‹é®æŒ¡å’Œå•æ‰‹åŠåŒæ‰‹äº’åŠ¨çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ‰‹å‹å‡ ä½•ä¸äº’åŠ¨æ€§ç²¾ç‚¼ï¼ˆHGIRï¼‰æ–¹æ¡ˆï¼Œç»“åˆæ‰‹éƒ¨å§¿åŠ¿å’Œå‡ ä½•ä¿¡æ¯æå‡æ¨¡å‹çš„äº’åŠ¨è¡¨ç°åŠ›ã€‚</li>
<li>HGIRæ–¹æ¡ˆèƒ½æœ‰æ•ˆæé«˜Ego-HOIæ£€æµ‹èƒ½åŠ›ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ¡ˆè½»ä¾¿æœ‰æ•ˆï¼Œå¯è½»æ¾é›†æˆåˆ°ç°æœ‰çš„HOIåŸºå‡†æµ‹è¯•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f11527044db6039879bccc25a8d246a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d267b96595749904f6e12c5b03ca8a41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d487c8b6d18d2ef7ab0f9a750d1b404b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ead272bad36f9699c524713ced1459c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7d23f0902a25338e2c477b40c42fbe6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Combining-Self-attention-and-Dilation-Convolutional-for-Semantic-Segmentation-of-Coal-Maceral-Groups"><a href="#Combining-Self-attention-and-Dilation-Convolutional-for-Semantic-Segmentation-of-Coal-Maceral-Groups" class="headerlink" title="Combining Self-attention and Dilation Convolutional for Semantic   Segmentation of Coal Maceral Groups"></a>Combining Self-attention and Dilation Convolutional for Semantic   Segmentation of Coal Maceral Groups</h2><p><strong>Authors:Zhenghao Xi, Zhengnan Lv, Yang Zheng, Xiang Liu, Zhuang Yu, Junran Chen, Jing Hu, Yaqi Liu</strong></p>
<p>The segmentation of coal maceral groups can be described as a semantic segmentation process of coal maceral group images, which is of great significance for studying the chemical properties of coal. Generally, existing semantic segmentation models of coal maceral groups use the method of stacking parameters to achieve higher accuracy. It leads to increased computational requirements and impacts model training efficiency. At the same time, due to the professionalism and diversity of coal maceral group images sampling, obtaining the number of samples for model training requires a long time and professional personnel operation. To address these issues, We have innovatively developed an IoT-based DA-VIT parallel network model. By utilizing this model, we can continuously broaden the dataset through IoT and achieving sustained improvement in the accuracy of coal maceral groups segmentation. Besides, we decouple the parallel network from the backbone network to ensure the normal using of the backbone network during model data updates. Secondly, DCSA mechanism of DA-VIT is introduced to enhance the local feature information of coal microscopic images. This DCSA can decompose the large kernels of convolutional attention into multiple scales and reduce 81.18% of parameters.Finally, we performed the contrast experiment and ablation experiment between DA-VIT and state-of-the-art methods at lots of evaluation metrics. Experimental results show that DA-VIT-Base achieves 92.14% pixel accuracy and 63.18% mIoU. Params and FLOPs of DA-VIT-Tiny are 4.95M and 8.99G, respectively. All of the evaluation metrics of the proposed DA-VIT are better than other state-of-the-art methods. </p>
<blockquote>
<p>ç…¤å²©ç»„åˆ†åˆ†å‰²å¯ä»¥æè¿°ä¸ºç…¤å²©ç»„åˆ†å›¾åƒçš„è¯­ä¹‰åˆ†å‰²è¿‡ç¨‹ï¼Œå¯¹ç ”ç©¶ç…¤çš„åŒ–å­¦æ€§è´¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç›®å‰ï¼Œç°æœ‰çš„ç…¤å²©ç»„åˆ†è¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸é‡‡ç”¨å †å å‚æ•°çš„æ–¹æ³•æ¥æé«˜ç²¾åº¦ï¼Œè¿™å¯¼è‡´äº†è®¡ç®—éœ€æ±‚çš„å¢åŠ å’Œæ¨¡å‹è®­ç»ƒæ•ˆç‡çš„å½±å“ã€‚åŒæ—¶ï¼Œç”±äºç…¤å²©ç»„åˆ†å›¾åƒé‡‡æ ·çš„ä¸“ä¸šæ€§å’Œå¤šæ ·æ€§ï¼Œè·å–æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ·æœ¬æ•°é‡éœ€è¦å¾ˆé•¿æ—¶é—´å’Œä¸“ä¸šäººå‘˜çš„æ“ä½œã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°å¼€å‘äº†ä¸€ç§åŸºäºç‰©è”ç½‘çš„DA-VITå¹¶è¡Œç½‘ç»œæ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‰©è”ç½‘ä¸æ–­æ‰©å±•æ•°æ®é›†ï¼Œå®ç°ç…¤å²©ç»„åˆ†åˆ†å‰²ç²¾åº¦çš„æŒç»­æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¹¶è¡Œç½‘ç»œä¸éª¨å¹²ç½‘è§£è€¦ï¼Œä»¥ç¡®ä¿åœ¨æ¨¡å‹æ•°æ®æ›´æ–°æœŸé—´éª¨å¹²ç½‘çš„æ­£å¸¸ä½¿ç”¨ã€‚å…¶æ¬¡ï¼Œä»‹ç»äº†DA-VITçš„DCSAæœºåˆ¶ï¼Œä»¥æé«˜ç…¤æ˜¾å¾®å›¾åƒçš„å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚DCSAå¯ä»¥å°†åœ¨åˆ†å‰²å’Œè½¬åŒ–å­¦ä¹ ä¸­å›¾åƒçš„å¯è®­ç»ƒå¯æ¥æ”¶é‡çš„å€¼å°†é™ä½åˆ°ç™¾åˆ†ä¹‹å››å·¦å³ã€‚è¿™ä¸ªDCSAå¯ä»¥å°†å·ç§¯æ³¨æ„åŠ›çš„å¤§å†…æ ¸åˆ†è§£æˆå¤šä¸ªå°ºåº¦å¹¶å‡å°‘81.18%çš„å‚æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è®¸å¤šè¯„ä¼°æŒ‡æ ‡ä¸Šå¯¹DA-VITå’Œæœ€æ–°æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”å®éªŒå’Œæ¶ˆèå®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDA-VIT-Baseçš„åƒç´ ç²¾åº¦è¾¾åˆ°92.14%ï¼ŒmIoUè¾¾åˆ°63.18%ã€‚DA-VIT-Tinyçš„å‚æ•°å’ŒFLOPsåˆ†åˆ«ä¸º4.95Må’Œ8.99Gã€‚æ‰€æDA-VITçš„æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å‡ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12712v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç…¤æ˜¾å¾®å›¾åƒåˆ†å‰²çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›æ–°æ€§åœ°æå‡ºäº†åŸºäºç‰©è”ç½‘çš„DA-VITå¹¶è¡Œç½‘ç»œæ¨¡å‹ã€‚è¯¥æ¨¡å‹å¯ä»¥æŒç»­æ‹“å±•æ•°æ®é›†ï¼Œæå‡åˆ†å‰²ç²¾åº¦ï¼Œå¹¶å¼•å…¥DCSAæœºåˆ¶å¼ºåŒ–å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDA-VITåœ¨åƒç´ ç²¾åº¦å’ŒmIoUç­‰æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç…¤æ˜¾å¾®å›¾åƒçš„è¯­ä¹‰åˆ†å‰²å¯¹ç ”ç©¶ç…¤çš„åŒ–å­¦æ€§è´¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰è¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸é€šè¿‡å †å å‚æ•°æ¥æé«˜ç²¾åº¦ï¼Œä½†è®¡ç®—éœ€æ±‚å¤§ï¼Œå½±å“è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>åˆ›æ–°æ€§åœ°æå‡ºäº†åŸºäºç‰©è”ç½‘çš„DA-VITå¹¶è¡Œç½‘ç»œæ¨¡å‹ï¼Œå¯æŒç»­æ‹“å±•æ•°æ®é›†å¹¶æå‡åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>å¼•å…¥DCSAæœºåˆ¶å¼ºåŒ–ç…¤æ˜¾å¾®å›¾åƒçš„å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>DA-VITæ¨¡å‹å…·æœ‰ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬åƒç´ ç²¾åº¦å’ŒmIoUç­‰æŒ‡æ ‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e75817d38c5e5547aaa80629fd47bc8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c6c2c45344c5e7e3505b6abf7c9efeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02cd0fd9b358665923ace3987867092e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f356611a75ba0cd1d7dd75eee4691a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-118bb926c7e9a4c58f225c7a7f2787ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-692a73a58fe09dad3e37cc201d1ca8b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5356fb09f0e154a6cdb002eda3c2e60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4b8b55483559cb9d3fe6bb4982a997.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 16.8 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/">https://github.com/roboflow/rf100-vl/</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a> </p>
<blockquote>
<p>åœ¨äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§å¯¹è±¡ï¼ˆå¦‚æ±½è½¦ã€å¡è½¦å’Œè¡Œäººï¼‰ä¸Šå®ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»ç„¶éš¾ä»¥æ¨å¹¿åˆ°å…¶é¢„è®­ç»ƒä¸­æ²¡æœ‰å‡ºç°çš„ç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼ã€‚æˆ‘ä»¬ä¸»å¼ ä¸åº”ä»…é€šè¿‡æ›´å¤šçš„è§†è§‰æ•°æ®é‡æ–°è®­ç»ƒVLMsï¼Œè€Œåº”é€šè¿‡åŒ…å«å°‘é‡è§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œæ–‡æœ¬æè¿°çš„æ³¨é‡ŠæŒ‡ä»¤æ¥å¯¹é½VLMsä»¥ç†è§£æ–°æ¦‚å¿µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªå¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«VLMé¢„è®­ç»ƒä¸­ä¸å¸¸è§çš„å„ç§æ¦‚å¿µã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šå¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€åŠç›‘ç£å’Œå…¨ç›‘ç£ç¯å¢ƒçš„è¯„ä¼°ï¼Œä»¥ä¾¿åœ¨ä¸åŒæ•°æ®ç¯å¢ƒä¸‹è¿›è¡Œæ¯”è¾ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åƒGroundingDINOå’ŒQwen2.5-VLè¿™æ ·çš„VLMåœ¨Roboflow100-VLä¸­çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡ä½äº2%ï¼Œè¿™è¯æ˜äº†è¿›è¡Œå°‘æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ€è¿‘çš„CVPR 2025åŸºç¡€FSODç«èµ›å¹¶ä»ç¤¾åŒºä¸­åˆ†äº«äº†è§è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå† å†›å›¢é˜Ÿæ¯”æˆ‘ä»¬çš„åŸºçº¿é«˜å‡º16.8 mAPï¼æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/]%E5%92%8C[https://universe.roboflow.com/rf100-vl/%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82](https://github.com/roboflow/rf100-vl/%EF%BC%89%E5%92%8C%E3%80%82)">https://github.com/roboflow/rf100-vl/]å’Œ[https://universe.roboflow.com/rf100-vl/ä¸­æ‰¾åˆ°ã€‚](https://github.com/roboflow/rf100-vl/%EF%BC%89%E5%92%8C%E3%80%82)</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v2">PDF</a> The first two authors contributed equally. Project Page:   <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–°æŒ‘æˆ˜ã€‚è™½ç„¶VLMsåœ¨å¸¸è§„ç‰©ä½“æ£€æµ‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹éå¸¸è§„åˆ†å¸ƒç±»åˆ«ã€ä»»åŠ¡å’Œæ–°æˆåƒæ¨¡å¼æ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å—é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªå¤šæ¨¡å¼ç‰©ä½“æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œæ¶µç›–äº†VLMé¢„è®­ç»ƒä¸å¸¸è§çš„å¤§é‡æ¦‚å¿µã€‚ä½œè€…è¯„ä¼°äº†å¤šç§çŠ¶æ€æ¨¡å‹åœ¨å„ç§è®¾ç½®ä¸‹çš„æ€§èƒ½ï¼Œå¹¶å‘ç°æŒ‘æˆ˜åŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šçš„VLMsé›¶å‡†ç¡®ç‡è¾ƒä½ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†CVPR 2025çš„FSODç«èµ›åŠç¤¾åŒºè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VLMsåœ¨å¸¸è§„ç‰©ä½“æ£€æµ‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹éå¸¸è§„åˆ†å¸ƒç±»åˆ«å’Œä»»åŠ¡æ—¶æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>Roboflow100-VLæ˜¯ä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ¦‚å¿µçš„å¤§è§„æ¨¡å¤šæ¨¡å¼ç‰©ä½“æ£€æµ‹æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³VLMé¢„è®­ç»ƒä¸å¸¸è§çš„é—®é¢˜ã€‚</li>
<li>åœ¨Roboflow100-VLçš„æŒ‘æˆ˜åŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šï¼ŒVLMsçš„é›¶å‡†ç¡®ç‡è¾ƒä½ã€‚</li>
<li>ä»‹ç»äº†CVPR 2025çš„FSODç«èµ›åŠå…¶ç¤¾åŒºè§è§£ï¼Œå…¶ä¸­è·èƒœé˜Ÿä¼æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿ã€‚</li>
<li>éœ€è¦é€šè¿‡å°‘é‡æ ·æœ¬æ¦‚å¿µå¯¹é½æ¥æé«˜VLMsçš„æ€§èƒ½ã€‚</li>
<li>ä½œè€…æä¾›äº†Roboflow100-VLçš„ä»£ç å’Œæ•°æ®é›†é“¾æ¥ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5aca01ad3a170a81e3314cec4f10ee82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8394e86845eed7bebbdef61422acc1ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94525ec6f3d591f43894fc2c1a976188.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8c9b05e81329b7cd6eaeb0ed2ecc447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8595510fa396ee11352235c0ab0ccab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa8f284bbe93c59ebf971defb34f815.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images"><a href="#FSSUWNet-Mitigating-the-Fragility-of-Pre-trained-Models-with-Feature-Enhancement-for-Few-Shot-Semantic-Segmentation-in-Underwater-Images" class="headerlink" title="FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images"></a>FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature   Enhancement for Few-Shot Semantic Segmentation in Underwater Images</h2><p><strong>Authors:Zhuohao Li, Zhicheng Huang, Wenchao Liu, Zhuxin Zhang, Jianming Miao</strong></p>
<p>Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes in images using only a limited number of annotated examples, has recently progressed in data-scarce domains. However, in this work, we show that the existing FSS methods often struggle to generalize to underwater environments. Specifically, the prior features extracted by pre-trained models used as feature extractors are fragile due to the unique challenges of underwater images. To address this, we propose FSSUWNet, a tailored FSS framework for underwater images with feature enhancement. FSSUWNet exploits the integration of complementary features, emphasizing both low-level and high-level image characteristics. In addition to employing a pre-trained model as the primary encoder, we propose an auxiliary encoder called Feature Enhanced Encoder which extracts complementary features to better adapt to underwater scene characteristics. Furthermore, a simple and effective Feature Alignment Module aims to provide global prior knowledge and align low-level features with high-level features in dimensions. Given the scarcity of underwater images, we introduce a cross-validation dataset version based on the Segmentation of Underwater Imagery dataset. Extensive experiments on public underwater segmentation datasets demonstrate that our approach achieves state-of-the-art performance. For example, our method outperforms the previous best method by 2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot and 5-shot scenarios in the datasets, respectively. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet">https://github.com/lizhh268/FSSUWNet</a>. </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰ä¸“æ³¨äºä½¿ç”¨æœ‰é™æ•°é‡çš„æ ‡æ³¨æ ·ä¾‹å¯¹å›¾åƒä¸­çš„æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œæœ€è¿‘åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ç°æœ‰çš„FSSæ–¹æ³•å¾€å¾€éš¾ä»¥æ¨å¹¿åˆ°æ°´ä¸‹ç¯å¢ƒã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºæ°´ä¸‹å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨æ‰€æå–çš„å…ˆéªŒç‰¹å¾æ˜¯è„†å¼±çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FSSUWNetï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å›¾åƒè¿›è¡Œç‰¹å¾å¢å¼ºçš„FSSæ¡†æ¶ã€‚FSSUWNetç»“åˆäº†äº’è¡¥ç‰¹å¾ï¼Œå¼ºè°ƒå›¾åƒçš„ä½çº§å’Œé«˜çº§ç‰¹å¾ã€‚é™¤äº†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»ç¼–ç å™¨å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºç‰¹å¾å¢å¼ºç¼–ç å™¨çš„è¾…åŠ©ç¼–ç å™¨ï¼Œç”¨äºæå–äº’è¡¥ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç®€å•æœ‰æ•ˆçš„ç‰¹å¾å¯¹é½æ¨¡å—æ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨ç»´åº¦ä¸Šå¯¹é½ä½çº§ç‰¹å¾å’Œé«˜çº§ç‰¹å¾ã€‚é‰´äºæ°´ä¸‹å›¾åƒçš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬åŸºäºæ°´ä¸‹å›¾åƒåˆ†å‰²æ•°æ®é›†å¼•å…¥äº†ä¸€ä¸ªäº¤å‰éªŒè¯æ•°æ®é›†ç‰ˆæœ¬ã€‚åœ¨å…¬å…±æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®é›†çš„å•æ ·æœ¬å’Œäº”æ ·æœ¬åœºæ™¯ä¸‹çš„å¹³å‡äº¤å¹¶æ¯”æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º2.8%å’Œ2.6%ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lizhh268/FSSUWNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lizhh268/FSSUWNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00478v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ°´ä¸‹å›¾åƒè¿›è¡Œå°‘é‡æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”æ°´ä¸‹ç¯å¢ƒï¼Œå› æ­¤æå‡ºä¸€ç§æ–°çš„æ°´ä¸‹å›¾åƒFSSæ¡†æ¶â€”â€”FSSUWNetï¼Œé€šè¿‡ç‰¹å¾å¢å¼ºå’Œäº’è¡¥ç‰¹å¾èåˆæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å¼•å…¥è¾…åŠ©ç¼–ç å™¨Feature Enhanced Encoderå’Œç‰¹å¾å¯¹é½æ¨¡å—Feature Alignment Moduleï¼Œä»¥æ›´å¥½åœ°é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹æ€§å¹¶æå‡ç‰¹å¾è´¨é‡ã€‚åœ¨å…¬å¼€çš„æ°´ä¸‹åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œç›¸è¾ƒäºä¹‹å‰æœ€ä½³æ–¹æ³•ï¼Œåœ¨1-shotå’Œ5-shotåœºæ™¯ä¸‹çš„å¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰æŒ‡æ ‡åˆ†åˆ«æå‡äº†2.8%å’Œ2.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FSSUWNetè§£å†³äº†ç°æœ‰FSSæ–¹æ³•åœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„åº”ç”¨é—®é¢˜ã€‚</li>
<li>FSSUWNeté‡‡ç”¨ç‰¹å¾å¢å¼ºå’Œäº’è¡¥ç‰¹å¾èåˆçš„ç­–ç•¥æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è¾…åŠ©ç¼–ç å™¨Feature Enhanced Encoderï¼Œç”¨äºæå–é€‚åº”æ°´ä¸‹åœºæ™¯ç‰¹æ€§çš„äº’è¡¥ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨ç‰¹å¾å¯¹é½æ¨¡å—Feature Alignment Moduleï¼Œæ—¨åœ¨æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†å¹¶èåˆé«˜ä½å±‚ç‰¹å¾ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒFSSUWNetè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f27f1af420dbcac33e58aa0c16fffc6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a354beb29cd8c80a20116f4d0e2fb8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-508ecd23d8e902eb482806ef4ad176be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d365645dd7d4d6c540b2b0e4c0d0d9b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9df96537413c539b08ec870c7a312e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217ca76dc735c5e0fe0a90313ddc4416.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Concept-Guided-Co-salient-Object-Detection"><a href="#Concept-Guided-Co-salient-Object-Detection" class="headerlink" title="Concept Guided Co-salient Object Detection"></a>Concept Guided Co-salient Object Detection</h2><p><strong>Authors:Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu</strong></p>
<p>Co-salient object detection (Co-SOD) aims to identify common salient objects across a group of related images. While recent methods have made notable progress, they typically rely on low-level visual patterns and lack semantic priors, limiting their detection performance. We propose ConceptCoSOD, a concept-guided framework that introduces high-level semantic knowledge to enhance co-saliency detection. By extracting shared text-based concepts from the input image group, ConceptCoSOD provides semantic guidance that anchors the detection process. To further improve concept quality, we analyze the effect of diffusion timesteps and design a resampling strategy that selects more informative steps for learning robust concepts. This semantic prior, combined with the resampling-enhanced representation, enables accurate and consistent segmentation even in challenging visual conditions. Extensive experiments on three benchmark datasets and five corrupted settings demonstrate that ConceptCoSOD significantly outperforms existing methods in both accuracy and generalization. </p>
<blockquote>
<p>ååŒæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆCo-SODï¼‰æ—¨åœ¨è¯†åˆ«ä¸€ç»„ç›¸å…³å›¾åƒä¸­çš„å…±åŒæ˜¾è‘—ç›®æ ‡ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºä½çº§çš„è§†è§‰æ¨¡å¼ï¼Œç¼ºä¹è¯­ä¹‰å…ˆéªŒï¼Œé™åˆ¶äº†å®ƒä»¬çš„æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºConceptCoSODï¼Œä¸€ä¸ªæ¦‚å¿µå¼•å¯¼æ¡†æ¶ï¼Œå¼•å…¥é«˜çº§è¯­ä¹‰çŸ¥è¯†ä»¥å¢å¼ºååŒæ˜¾è‘—æ€§æ£€æµ‹ã€‚ConceptCoSODé€šè¿‡ä»è¾“å…¥å›¾åƒç»„ä¸­æå–åŸºäºæ–‡æœ¬çš„å…±äº«æ¦‚å¿µï¼Œä¸ºæ£€æµ‹è¿‡ç¨‹æä¾›è¯­ä¹‰æŒ‡å¯¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¦‚å¿µè´¨é‡ï¼Œæˆ‘ä»¬åˆ†æäº†æ‰©æ•£æ—¶é—´æ­¥é•¿çš„å½±å“ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é‡é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©æ›´æœ‰ä¿¡æ¯é‡çš„æ­¥éª¤æ¥å­¦ä¹ ç¨³å¥çš„æ¦‚å¿µã€‚è¿™ç§è¯­ä¹‰å…ˆéªŒä¸é‡é‡‡æ ·å¢å¼ºè¡¨ç¤ºç›¸ç»“åˆï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¡ä»¶ä¸‹ä¹Ÿèƒ½å®ç°å‡†ç¡®ä¸”ä¸€è‡´çš„åˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œäº”ç§è…èš€ç¯å¢ƒä¸‹çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒConceptCoSODåœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16609v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¦‚å¿µå¼•å¯¼ååŒæ˜¾è‘—æ€§å¯¹è±¡æ£€æµ‹ï¼ˆConceptCoSODï¼‰æ—¨åœ¨åˆ©ç”¨é«˜å±‚æ¬¡çš„è¯­ä¹‰çŸ¥è¯†è¯†åˆ«ç›¸å…³å›¾åƒç¾¤ç»„ä¸­çš„å…±åŒæ˜¾è‘—å¯¹è±¡ã€‚å®ƒé€šè¿‡æå–è¾“å…¥å›¾åƒç¾¤ä¸­çš„å…±äº«æ–‡æœ¬æ¦‚å¿µæ¥æä¾›è¯­ä¹‰å¼•å¯¼ï¼Œè¿›è€Œå¼ºåŒ–ååŒæ˜¾è‘—æ€§æ£€æµ‹è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œä¸ºäº†æé«˜æ¦‚å¿µè´¨é‡ï¼Œæœ¬æ–‡åˆ†æäº†æ‰©æ•£æ—¶åºæ­¥çš„å½±å“å¹¶è®¾è®¡äº†ä¸€ç§é‡é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©æ›´æœ‰ä¿¡æ¯é‡çš„æ­¥éª¤æ¥å­¦ä¹ ç¨³å¥çš„æ¦‚å¿µã€‚è¿™ç§è¯­ä¹‰å…ˆéªŒä¸é‡é‡‡æ ·å¢å¼ºè¡¨ç¤ºç›¸ç»“åˆï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¡ä»¶ä¸‹ä¹Ÿèƒ½å®ç°å‡†ç¡®ä¸”ä¸€è‡´çš„åˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œäº”ç§è…èš€ç¯å¢ƒä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒConceptCoSODåœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µå¼•å¯¼ååŒæ˜¾è‘—æ€§å¯¹è±¡æ£€æµ‹ï¼ˆConceptCoSODï¼‰åˆ©ç”¨é«˜å±‚æ¬¡çš„è¯­ä¹‰çŸ¥è¯†è¯†åˆ«ç›¸å…³å›¾åƒç¾¤ç»„ä¸­çš„å…±åŒæ˜¾è‘—å¯¹è±¡ã€‚</li>
<li>é€šè¿‡æå–è¾“å…¥å›¾åƒç¾¤ä¸­çš„å…±äº«æ–‡æœ¬æ¦‚å¿µï¼ŒConceptCoSODä¸ºæ£€æµ‹è¿‡ç¨‹æä¾›è¯­ä¹‰å¼•å¯¼ã€‚</li>
<li>ä¸ºæé«˜æ¦‚å¿µè´¨é‡ï¼ŒConceptCoSODåˆ†æäº†æ‰©æ•£æ—¶åºæ­¥çš„å½±å“å¹¶è®¾è®¡äº†é‡é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>é‡é‡‡æ ·ç­–ç•¥é€‰æ‹©æ›´æœ‰ä¿¡æ¯é‡çš„æ­¥éª¤ï¼Œä»¥å­¦ä¹ ç¨³å¥çš„æ¦‚å¿µè¡¨ç¤ºã€‚</li>
<li>è¿™ç§è¯­ä¹‰å…ˆéªŒä¸é‡é‡‡æ ·å¢å¼ºè¡¨ç¤ºç›¸ç»“åˆï¼Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œè…èš€ç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒConceptCoSODåœ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0dec85005b0263838607af870074fa14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd81e76e96a0607dda2a22efa8ba1c9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b88ecf52e019e190ba4e337f78873a0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f836e0820f2f953c958f37df48442af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2842b4d72b0d2f62ffe077b1ae437c39.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-Based-Quality-Control-of-Histopathology-Whole-Slide-Images"><a href="#Semantic-Segmentation-Based-Quality-Control-of-Histopathology-Whole-Slide-Images" class="headerlink" title="Semantic Segmentation Based Quality Control of Histopathology Whole   Slide Images"></a>Semantic Segmentation Based Quality Control of Histopathology Whole   Slide Images</h2><p><strong>Authors:Abhijeet Patil, Garima Jain, Harsh Diwakar, Jay Sawant, Tripti Bameta, Swapnil Rane, Amit Sethi</strong></p>
<p>We developed a software pipeline for quality control (QC) of histopathology whole slide images (WSIs) that segments various regions, such as blurs of different levels, tissue regions, tissue folds, and pen marks. Given the necessity and increasing availability of GPUs for processing WSIs, the proposed pipeline comprises multiple lightweight deep learning models to strike a balance between accuracy and speed. The pipeline was evaluated in all TCGAs, which is the largest publicly available WSI dataset containing more than 11,000 histopathological images from 28 organs. It was compared to a previous work, which was not based on deep learning, and it showed consistent improvement in segmentation results across organs. To minimize annotation effort for tissue and blur segmentation, annotated images were automatically prepared by mosaicking patches (sub-images) from various WSIs whose labels were identified using a patch classification tool HistoROI. Due to the generality of our trained QC pipeline and its extensive testing the potential impact of this work is broad. It can be used for automated pre-processing any WSI cohort to enhance the accuracy and reliability of large-scale histopathology image analysis for both research and clinical use. We have made the trained models, training scripts, training data, and inference results publicly available at <a target="_blank" rel="noopener" href="https://github.com/abhijeetptl5/wsisegqc">https://github.com/abhijeetptl5/wsisegqc</a>, which should enable the research community to use the pipeline right out of the box or further customize it to new datasets and applications in the future. </p>
<blockquote>
<p>æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”¨äºç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è´¨é‡æ§åˆ¶çš„è½¯ä»¶ç®¡é“ï¼ˆQCï¼‰ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿå¯¹ä¸åŒåŒºåŸŸè¿›è¡Œåˆ†å‰²ï¼Œä¾‹å¦‚ä¸åŒçº§åˆ«çš„æ¨¡ç³Šã€ç»„ç»‡åŒºåŸŸã€ç»„ç»‡æŠ˜å å’Œç¬”è¿¹ã€‚è€ƒè™‘åˆ°å¤„ç†WSIæ—¶å¯¹GPUçš„éœ€æ±‚åŠå…¶æ—¥ç›Šæ™®åŠæ€§ï¼Œæ‰€æå‡ºçš„ç®¡é“åŒ…å«å¤šä¸ªè½»é‡çº§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¯¥ç®¡é“åœ¨TCGAï¼ˆå³åŒ…å«æ¥è‡ª28ä¸ªå™¨å®˜çš„è¶…è¿‡11,000ä¸ªç»„ç»‡ç—…ç†å­¦å›¾åƒçš„æœ€å¤§çš„å…¬å¼€WSIæ•°æ®é›†ï¼‰ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ä¸ä¹‹å‰çš„éæ·±åº¦å­¦ä¹ ä¸ºåŸºç¡€çš„å·¥ä½œç›¸æ¯”ï¼Œå®ƒåœ¨å„å™¨å®˜çš„åˆ†å‰²ç»“æœä¸­è¡¨ç°å‡ºæŒç»­æ”¹è¿›ã€‚ä¸ºäº†å‡å°‘ç»„ç»‡å’Œæ¨¡ç³Šåˆ†å‰²çš„æ ‡æ³¨å·¥ä½œé‡ï¼Œé€šè¿‡åˆå¹¶æ¥è‡ªå„ç§WSIçš„è¡¥ä¸ï¼ˆå­å›¾åƒï¼‰æ¥è‡ªåŠ¨å‡†å¤‡æ ‡æ³¨å›¾åƒï¼Œä½¿ç”¨è¡¥ä¸åˆ†ç±»å·¥å…·HistoROIæ¥è¯†åˆ«è¿™äº›æ ‡ç­¾ã€‚ç”±äºæˆ‘ä»¬è®­ç»ƒçš„QCç®¡é“çš„é€šç”¨æ€§å’Œå¹¿æ³›çš„æµ‹è¯•ï¼Œè¿™é¡¹å·¥ä½œå…·æœ‰å¹¿æ³›çš„å½±å“æ½œåŠ›ã€‚å®ƒå¯ä»¥ç”¨äºè‡ªåŠ¨é¢„å¤„ç†ä»»ä½•WSIé˜Ÿåˆ—ï¼Œä»¥æé«˜å¤§è§„æ¨¡ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œæ— è®ºæ˜¯ç”¨äºç ”ç©¶è¿˜æ˜¯ä¸´åºŠã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/abhijeetptl5/wsisegqc">https://github.com/abhijeetptl5/wsisegqc</a>ä¸Šå…¬å¼€äº†è®­ç»ƒæ¨¡å‹ã€è®­ç»ƒè„šæœ¬ã€è®­ç»ƒæ•°æ®å’Œæ¨ç†ç»“æœï¼Œç ”ç©¶ç¤¾åŒºå¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤ç®¡é“æˆ–è¿›ä¸€æ­¥å®šåˆ¶å®ƒä»¥é€‚åº”æœªæ¥æ–°çš„æ•°æ®é›†å’Œåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03289v2">PDF</a> 14 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è´¨é‡æ§åˆ¶çš„è½¯ä»¶ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿåˆ†å‰²ä¸åŒçš„åŒºåŸŸï¼Œå¦‚ä¸åŒçº§åˆ«çš„æ¨¡ç³Šã€ç»„ç»‡åŒºåŸŸã€ç»„ç»‡æŠ˜å å’Œå¢¨è¿¹ã€‚ç®¡é“é‡‡ç”¨å¤šä¸ªè½»é‡çº§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåœ¨ä¿éšœå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæå‡äº†å¤„ç†é€Ÿåº¦ã€‚è¯¥ç®¡é“åœ¨TCGAå¤§å‹å…¬å¼€WSIæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸éæ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ˜¾ç¤ºå‡ºè·¨å™¨å®˜çš„åˆ†å‰²ç»“æœä¸€è‡´æ€§æå‡ã€‚ä¸ºå‡å°‘ç»„ç»‡å’Œæ¨¡ç³Šåˆ†å‰²çš„æ ‡æ³¨å·¥ä½œï¼Œé€šè¿‡HistoROIè¡¥ä¸åˆ†ç±»å·¥å…·è‡ªåŠ¨å‡†å¤‡æ ‡æ³¨å›¾åƒã€‚æ­¤å·¥ä½œçš„æ½œåœ¨å½±å“å¹¿æ³›ï¼Œå¯ç”¨äºè‡ªåŠ¨é¢„å¤„ç†ä»»ä½•WSIé˜Ÿåˆ—ï¼Œæé«˜å¤§è§„æ¨¡ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œæ—¢å¯ç”¨äºç ”ç©¶ï¼Œä¹Ÿå¯ç”¨äºä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶å¼€å‘äº†ç”¨äºç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒè´¨é‡æ§åˆ¶çš„ç®¡é“ï¼Œå®ç°å¤šç§åŒºåŸŸåˆ†å‰²ã€‚</li>
<li>ç®¡é“é‡‡ç”¨è½»é‡çº§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå…¼é¡¾å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚</li>
<li>ç®¡é“åœ¨TCGAå¤§å‹å…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå¹¶ä¸éæ·±åº¦å­¦ä¹ æ–¹æ³•æ¯”è¾ƒï¼Œæ˜¾ç¤ºè·¨å™¨å®˜åˆ†å‰²ç»“æœæå‡ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨å‡†å¤‡æ ‡æ³¨å›¾åƒï¼Œå‡å°äº†æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>ç®¡é“å…·æœ‰é€šç”¨æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºä»»ä½•WSIçš„é¢„å¤„ç†ã€‚</li>
<li>ç®¡é“èƒ½æé«˜å¤§è§„æ¨¡ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œæ—¢é€‚ç”¨äºç ”ç©¶ä¹Ÿé€‚ç”¨äºä¸´åºŠã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b7c428a3bc6c8d484e2a42fc2125e21.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. <a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a> </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFew-shot Semantic Segmentationï¼‰åº”å¯¹äº†ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œå¯¹è±¡åˆ†å‰²çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè®¸å¤šä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•è¦ä¹ˆä¸å¾—ä¸æ”¾å¼ƒå¤æ‚çš„å±€éƒ¨è¯­ä¹‰ç‰¹å¾ï¼Œè¦ä¹ˆé¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºTransformeræ¶æ„çš„æ–°å‹å°‘é‡æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©æ¨¡ç”Ÿæˆæ¨¡å—ï¼Œä»¥æ”¹å–„æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥åˆ†å±‚çš„æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©æ¨¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨å±€ç‰¹å¾ä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚åº¦ã€‚æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„è¿™ç§å¹³è¡¡ä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ— è®ºæ˜¯å•å¼ å›¾ç‰‡ä¸€æ¬¡è®­ç»ƒï¼ˆ1-shotï¼‰è¿˜æ˜¯äº”æ¬¡è®­ç»ƒï¼ˆ5-shotï¼‰çš„æƒ…å†µä¸‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…æœ‰150ä¸‡ä¸ªå‚æ•°ï¼Œåœ¨å…‹æœç°æœ‰æ–¹æ³•å±€é™æ€§çš„åŒæ—¶è¡¨ç°å‡ºäº†ç«äº‰åŠ›ã€‚ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³GitHubï¼š<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/MSDNet">https://github.com/amirrezafateh/MSDNet</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformeræ¶æ„ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—ï¼Œæé«˜äº†æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚åŒæ—¶é‡‡ç”¨å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥å±‚æ¬¡æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©è†œã€‚è¯¥æ–¹æ³•è¿˜èåˆäº†ä¸­é—´ç¼–ç é˜¶æ®µçš„å…¨å±€ç‰¹å¾ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚æ€§ã€‚åœ¨PASCAL-5iå’ŒCOCO-20iç­‰åŸºå‡†æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨1-shotå’Œ5-shotè®¾ç½®ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†æ–°çš„åŸºäºTransformerçš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>é€šè¿‡ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©è†œç”Ÿæˆæ¨¡å—æé«˜å›¾åƒé—´å…³ç³»ç†è§£ã€‚</li>
<li>é‡‡ç”¨å¤šå°ºåº¦è§£ç å™¨ä¼˜åŒ–åˆ†å‰²æ©è†œï¼Œèåˆä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾ã€‚</li>
<li>èåˆä¸­é—´ç¼–ç é˜¶æ®µçš„å…¨å±€ç‰¹å¾ï¼Œæé«˜ä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚æ€§ã€‚</li>
<li>åœ¨PASCAL-5iå’ŒCOCO-20iç­‰åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dcdeef2671221c0cba44addb40e56bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6e719d0a34e103475f80a5b03bc20f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1232d75de621baac9ac76c250665a86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43b280a5d0cd66845ec22d0186712420.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Test-time-Contrastive-Concepts-for-Open-world-Semantic-Segmentation-with-Vision-Language-Models"><a href="#Test-time-Contrastive-Concepts-for-Open-world-Semantic-Segmentation-with-Vision-Language-Models" class="headerlink" title="Test-time Contrastive Concepts for Open-world Semantic Segmentation with   Vision-Language Models"></a>Test-time Contrastive Concepts for Open-world Semantic Segmentation with   Vision-Language Models</h2><p><strong>Authors:Monika WysoczaÅ„ska, Antonin Vobecky, Amaia Cardiel, Tomasz TrzciÅ„ski, Renaud Marlet, Andrei Bursuc, Oriane SimÃ©oni</strong></p>
<p>Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts of image-text pairs to align both modalities with a simple contrastive objective, have paved the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image pixels are assigned the closest query in feature space. However, this works well when a user exhaustively lists all possible visual concepts in an image that contrast against each other for the assignment. This corresponds to the current evaluation setup in the literature, which relies on having access to a list of in-domain relevant concepts, typically classes of a benchmark dataset. Here, we consider the more challenging (and realistic) scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic â€˜backgroundâ€™ text, we propose two different approaches to automatically generate, at test time, query-specific textual contrastive concepts. We do so by leveraging the distribution of text in the VLMâ€™s training set or crafted LLM prompts. We also propose a metric designed to evaluate this scenario and show the relevance of our approach on commonly used datasets. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒCLIPç±»ä¼¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å¤§é‡å›¾åƒæ–‡æœ¬å¯¹çš„é¢„è®­ç»ƒï¼Œä»¥ç®€å•çš„å¯¹æ¯”ç›®æ ‡å¯¹é½ä¸¤ç§æ¨¡æ€ï¼Œä¸ºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é“ºå¹³äº†é“è·¯ã€‚ç»™å®šä»»æ„æ–‡æœ¬æŸ¥è¯¢ï¼Œå›¾åƒåƒç´ è¢«åˆ†é…åˆ°ç‰¹å¾ç©ºé—´ä¸­æœ€è¿‘çš„æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œå½“ç”¨æˆ·åœ¨åˆ†é…æ—¶è¯¦å°½åœ°åˆ—å‡ºå›¾åƒä¸­æ‰€æœ‰å¯èƒ½çš„è§†è§‰æ¦‚å¿µä»¥è¿›è¡Œå¯¹æ¯”æ—¶ï¼Œè¿™ç§æ–¹æ³•æ•ˆæœå¾ˆå¥½ã€‚è¿™ä¸æ–‡çŒ®ä¸­çš„å½“å‰è¯„ä¼°è®¾ç½®ç›¸å¯¹åº”ï¼Œè¯¥è®¾ç½®ä¾èµ–äºè®¿é—®é¢†åŸŸç›¸å…³æ¦‚å¿µçš„åˆ—è¡¨ï¼Œé€šå¸¸æ˜¯åŸºå‡†æ•°æ®é›†çš„ç±»å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§ï¼ˆå’Œæ›´ç°å®ï¼‰çš„åœºæ™¯ï¼Œå³ç»™å®šæ–‡æœ¬æç¤ºå¯¹å•ä¸ªæ¦‚å¿µè¿›è¡Œåˆ†å‰²ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ã€‚é™¤äº†ä¸é€šç”¨çš„â€œèƒŒæ™¯â€æ–‡æœ¬è¿›è¡Œå¯¹æ¯”ä¹‹å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä¸åŒæ–¹æ³•åœ¨æµ‹è¯•æ—¶è‡ªåŠ¨ç”Ÿæˆç‰¹å®šæŸ¥è¯¢çš„å¯¹æ¯”æ¦‚å¿µã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨VLMè®­ç»ƒé›†ä¸­çš„æ–‡æœ¬åˆ†å¸ƒæˆ–ç²¾å¿ƒè®¾è®¡çš„LLMæç¤ºæ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°æ­¤åœºæ™¯çš„æŒ‡æ ‡ï¼Œå¹¶åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.05061v3">PDF</a> TMLR camera-ready</p>
<p><strong>Summary</strong><br>    CLIPç±»è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°äº†è·¨æ¨¡æ€å¯¹é½çš„ç®€å•å¯¹æ¯”ç›®æ ‡ï¼Œä¸ºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é“ºå¹³äº†é“è·¯ã€‚ç»™å®šä»»æ„æ–‡æœ¬æŸ¥è¯¢ï¼Œå›¾åƒåƒç´ è¢«åˆ†é…ç»™ç‰¹å¾ç©ºé—´ä¸­æœ€è¿‘çš„æŸ¥è¯¢ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”¨æˆ·è¯¦å°½åœ°åˆ—å‡ºå›¾åƒä¸­æ‰€æœ‰å¯èƒ½çš„è§†è§‰æ¦‚å¿µä»¥ä¾›åˆ†é…æ—¶ï¼Œæ¨¡å‹è¡¨ç°è‰¯å¥½ã€‚æœ¬æ–‡è€ƒè™‘æ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„åœºæ™¯ï¼Œå³ä»…æ ¹æ®æ–‡æœ¬æç¤ºå¯¹å•ä¸ªæ¦‚å¿µè¿›è¡Œåˆ†å‰²ã€‚é™¤äº†ä¸é€šç”¨â€œèƒŒæ™¯â€æ–‡æœ¬è¿›è¡Œå¯¹æ¯”å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§åœ¨æµ‹è¯•æ—¶è‡ªåŠ¨ç”Ÿæˆç‰¹å®šæŸ¥è¯¢å¯¹æ¯”æ¦‚å¿µçš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨VLMè®­ç»ƒé›†ä¸­çš„æ–‡æœ¬åˆ†å¸ƒæˆ–å®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºæ¥å®ç°ã€‚æˆ‘ä»¬è¿˜ä¸ºæ­¤åœºæ™¯è®¾è®¡äº†ä¸€ç§è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šå±•ç¤ºäº†æ–¹æ³•çš„ç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP-like Vision-Language Models (VLMs) é€šè¿‡å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹é¢„è®­ç»ƒï¼Œå®ç°äº†è·¨æ¨¡æ€å¯¹é½çš„ç®€å•å¯¹æ¯”ç›®æ ‡ï¼Œä¸ºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æä¾›äº†åŸºç¡€ã€‚</li>
<li>å½“å‰æ–‡çŒ®ä¸­çš„è¯„ä¼°è®¾ç½®ä¾èµ–äºè·å–ç‰¹å®šé¢†åŸŸçš„ç›¸å…³æ¦‚å¿µåˆ—è¡¨ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­å¯èƒ½éœ€è¦è€ƒè™‘æ›´å¤æ‚çš„å› ç´ ã€‚</li>
<li>é’ˆå¯¹ä»…æ ¹æ®æ–‡æœ¬æç¤ºå¯¹å•ä¸ªæ¦‚å¿µè¿›è¡Œåˆ†å‰²çš„æ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„åœºæ™¯ï¼Œæå‡ºäº†ä¸¤ç§è‡ªåŠ¨ç”Ÿæˆç‰¹å®šæŸ¥è¯¢å¯¹æ¯”æ¦‚å¿µçš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨VLMè®­ç»ƒé›†ä¸­çš„æ–‡æœ¬åˆ†å¸ƒæˆ–å®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºæ¥å®ç°è‡ªåŠ¨ç”Ÿæˆçš„æŸ¥è¯¢å¯¹æ¯”æ¦‚å¿µã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œå›¾åƒåˆ†å‰²çš„åœºæ™¯ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¸¸ç”¨æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„ç›¸å…³æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.05061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1803c37ceca2a1bc39607ea6825e3980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ded018b0d58df0be1703db785e1765f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd01333f649f195da18d78c62390231.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6efa04687e34bb5b76af8d48687f6128.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Foundation Artificial Intelligence Models for Health Recognition Using   Face Photographs (FAHR-Face)
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6be101dced6ae52c25293095f5eb1857.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  OpenPath Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
