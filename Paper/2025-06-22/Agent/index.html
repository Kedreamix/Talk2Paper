<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-06-22  Embodied Web Agents Bridging Physical-Digital Realms for Integrated   Agent Intelligence">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-cebd040a0fd5cefd5a58fe9d9506828a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    67 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-22-更新"><a href="#2025-06-22-更新" class="headerlink" title="2025-06-22 更新"></a>2025-06-22 更新</h1><h2 id="Embodied-Web-Agents-Bridging-Physical-Digital-Realms-for-Integrated-Agent-Intelligence"><a href="#Embodied-Web-Agents-Bridging-Physical-Digital-Realms-for-Integrated-Agent-Intelligence" class="headerlink" title="Embodied Web Agents: Bridging Physical-Digital Realms for Integrated   Agent Intelligence"></a>Embodied Web Agents: Bridging Physical-Digital Realms for Integrated   Agent Intelligence</h2><p><strong>Authors:Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang</strong></p>
<p>AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page <a target="_blank" rel="noopener" href="https://embodied-web-agent.github.io/">https://embodied-web-agent.github.io/</a>. </p>
<blockquote>
<p>当前的人工智能代理大多处于独立状态，它们要么检索并推理大量在线获得的数字信息和知识，要么通过与物理世界的实体感知、规划和行动进行交互，但很少两者兼具。这种分离限制了它们解决需要融合物理和数字化智能的任务的能力，例如根据在线食谱烹饪、使用动态地图数据进行导航，或使用网络知识解释现实世界的地标。我们引入了嵌入式Web代理（Embodied Web Agents）这一新型的人工智能代理范式，该范式能够灵活地桥接实体和网页规模推理。为了实施这一概念，我们首先开发了嵌入式Web代理任务环境，这是一个统一的仿真平台，紧密集成了现实的3D室内和室外环境与功能网页界面。在此基础上，我们构建并发布了嵌入式Web代理基准测试（Embodied Web Agents Benchmark），它包含一系列多样化的任务，包括烹饪、导航、购物、旅游和地理定位等，所有这些任务都需要在物理和数字领域进行协调推理，以系统地评估跨域智能。实验结果揭示了最先进的人工智能系统与人类能力之间的显著性能差距，确定了融合实体认知和网页规模知识访问的交叉点上的挑战和机遇。所有数据集、代码和网站都可在我们的项目页面<a target="_blank" rel="noopener" href="https://embodied-web-agent.github.io/%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://embodied-web-agent.github.io/上公开获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了当前AI代理的局限性，它们无法同时处理大规模的数字信息和物理世界的交互任务。为解决这一问题，提出了Embodied Web Agents这一新型范式，实现了实体与网页推理的桥梁。为此，建立了统一的模拟平台并发布了相关任务环境，包括烹饪、导航、购物、旅游等多样化任务，以评估跨领域的智能水平。实验结果表明，当前最先进的AI系统仍与人类能力存在显著差距，这为体认认知和网页知识访问的交叉点带来了挑战和机遇。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前AI代理主要局限于数字信息处理和物理世界交互任务的处理，无法同时处理两者。</li>
<li>Embodied Web Agents解决了这一局限性，实现了实体与网页推理的桥梁。</li>
<li>建立了统一的模拟平台以支持Embodied Web Agents的任务环境。</li>
<li>发布了一系列多样化任务，包括烹饪、导航、购物、旅游等，以评估跨领域的智能水平。</li>
<li>实验结果表明当前AI系统仍有人类能力上的差距。</li>
<li>该项目公开了数据集、代码和项目网站，为研究者提供了便利。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ea5ea88a08a15a684419ae3dd8c8d225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f51ec387115f7fbefd6c991d1c57b6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80455fe93a9160ac53592f9e52493a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa68fbf15297bc02a035f7ed4d0f488a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SwarmAgentic-Towards-Fully-Automated-Agentic-System-Generation-via-Swarm-Intelligence"><a href="#SwarmAgentic-Towards-Fully-Automated-Agentic-System-Generation-via-Swarm-Intelligence" class="headerlink" title="SwarmAgentic: Towards Fully Automated Agentic System Generation via   Swarm Intelligence"></a>SwarmAgentic: Towards Fully Automated Agentic System Generation via   Swarm Intelligence</h2><p><strong>Authors:Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp</strong></p>
<p>The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at <a target="_blank" rel="noopener" href="https://yaoz720.github.io/SwarmAgentic/">https://yaoz720.github.io/SwarmAgentic/</a>. </p>
<blockquote>
<p>随着大型语言模型的快速发展，智能体系统在决策、协调和执行任务方面取得了长足的进步。然而，现有的智能体系统生成框架缺乏完全的自主性，缺少从头开始生成智能体、智能体功能的自我优化以及协作能力，从而限制了适应性和可扩展性。我们提出了SwarmAgentic，这是一个用于全自动智能体系统生成的框架，它从底层构建智能体系统，并通过语言驱动的探索来联合优化智能体功能和协作能力作为相互依赖的组件。为了在系统级别结构上实现有效的搜索，SwarmAgentic维护了一组候选系统，并通过反馈指导的更新来进化它们，这得益于粒子群优化（PSO）的启发。我们在涉及高级规划、系统级协调和创造性推理的六个现实世界的开放性和探索性任务上评估了我们的方法。仅给出任务描述和目标函数，SwarmAgentic的表现优于所有基线，在TravelPlanner基准测试中相对于ADAS实现了+261.8%的相对改进，突显出完全自动化在结构不受约束的任务中的有效性。该框架标志着朝着可扩展和自主的智能体系统设计迈出了重要一步，架起了群体智能与全自动多智能体系统生成之间的桥梁。我们的代码已公开发布在<a target="_blank" rel="noopener" href="https://yaoz720.github.io/SwarmAgentic/%E4%B8%8A%E3%80%82">https://yaoz720.github.io/SwarmAgentic/上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15672v1">PDF</a> 41 pages</p>
<p><strong>Summary</strong></p>
<p>大型语言模型的快速发展推动了决策、协调和任务执行中的代理系统进步。然而，现有的代理系统生成框架缺乏完全自主性，缺少从头开始生成代理、自我优化代理功能以及协作能力，这限制了其适应性和可扩展性。为此，我们提出了SwarmAgentic，这是一个用于全自动代理系统生成的框架，它通过语言驱动的探索，从头构建代理系统，并联合优化代理功能和协作作为相互依赖的组件。通过粒子群优化算法的灵感，SwarmAgentic通过反馈指导的更新来进化候选系统种群，以实现系统级结构的高效搜索。在涉及高级规划、系统级协调和创造性推理的六个现实世界的开放和探索性任务上，我们的方法在仅给出任务描述和目标函数的情况下，超越了所有基线，在TravelPlanner基准测试中相对于ADAS实现了+261.8%的相对改进，证明了全自动化的有效性。这一框架是朝着可扩展和自主代理系统设计迈出的重要一步，将群体智能与全自动多代理系统生成相结合。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的进步推动了决策、协调和任务执行中代理系统的进步。</li>
<li>现有代理系统生成框架缺乏完全自主性，限制了适应性和可扩展性。</li>
<li>SwarmAgentic框架实现了全自动代理系统生成，包括从头构建代理系统。</li>
<li>SwarmAgentic通过语言驱动的探索联合优化代理功能和协作。</li>
<li>SwarmAgentic通过反馈指导的更新进化候选系统种群，借鉴了粒子群优化算法的灵感。</li>
<li>在多个现实世界的任务上，SwarmAgentic超越了现有方法，实现了显著的性能改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-38cb83ecf2dab629bd291eb0553fb40b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4eeb17b0426971b1844bf62cbdcbb479.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-029bf913f94ef2640518a1c433d03f91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88e11a142c092dd6438cb6e904520942.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PhishDebate-An-LLM-Based-Multi-Agent-Framework-for-Phishing-Website-Detection"><a href="#PhishDebate-An-LLM-Based-Multi-Agent-Framework-for-Phishing-Website-Detection" class="headerlink" title="PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website   Detection"></a>PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website   Detection</h2><p><strong>Authors:Wenhao Li, Selvakumar Manickam, Yung-wey Chong, Shankar Karuppayah</strong></p>
<p>Phishing websites continue to pose a significant cybersecurity threat, often leveraging deceptive structures, brand impersonation, and social engineering tactics to evade detection. While recent advances in large language models (LLMs) have enabled improved phishing detection through contextual understanding, most existing approaches rely on single-agent classification facing the risks of hallucination and lack interpretability or robustness. To address these limitations, we propose PhishDebate, a modular multi-agent LLM-based debate framework for phishing website detection. PhishDebate employs four specialized agents to independently analyze different textual aspects of a webpage–URL structure, HTML composition, semantic content, and brand impersonation–under the coordination of a Moderator and a final Judge. Through structured debate and divergent thinking, the framework delivers more accurate and interpretable decisions. Extensive evaluations on commercial LLMs demonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate (TPR) on a real-world phishing dataset, and outperforms single-agent and Chain of Thought (CoT) baselines. Additionally, its modular design allows agent-level configurability, enabling adaptation to varying resource and application requirements. </p>
<blockquote>
<p>钓鱼网站继续构成重大网络安全威胁，它们经常利用欺骗性结构、品牌伪装和社会工程策略来躲避检测。尽管大型语言模型（LLM）的最新进展已经能够通过上下文理解改进钓鱼检测，但大多数现有方法仍然依赖于单代理分类，面临产生幻觉的风险，并且缺乏可解释性或稳健性。为了解决这些局限性，我们提出了PhishDebate，这是一个基于多代理的LLM辩论框架，用于钓鱼网站检测。PhishDebate采用四个专业代理来独立分析网页的四个不同文本方面——URL结构、HTML组合、语义内容和品牌伪装，由一个协调者和最终判定者进行协调。通过结构化辩论和发散思维，该框架能够做出更准确、更可解释的决定。在商业LLM上的广泛评估表明，PhishDebate在真实世界的钓鱼数据集上达到了98.2%的查全率和98.2%的真正阳性率（TPR），并且优于单代理和思维链（CoT）基线。此外，其模块化设计允许代理级别的配置，能够适应不同的资源和应用需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15656v1">PDF</a> </p>
<p><strong>Summary</strong><br>     钓鱼网站持续构成重大网络安全威胁，常利用欺骗性结构、品牌伪装和社会工程学策略来躲避检测。针对现有方法易产生幻觉且缺乏可解释性和稳健性的问题，提出PhishDebate框架，该框架利用模块化多主体大型语言模型（LLM）进行辩论以实现钓鱼网站检测。PhishDebate框架设有四组独立代理对网页的不同文本方面进行单独分析——网址结构、HTML组合、语义内容和品牌伪装。这些工作在协调器和最终评判员的协助下进行。通过结构化的辩论和发散思维，框架决策更精准、更直观。在大型商业语言模型上的广泛评估显示，PhishDebate在真实世界钓鱼数据集上实现了98.2%的召回率和98.2%的真实阳性率（TPR），且优于单一主体基线方法和思维链基线方法。此外，其模块化设计可实现代理级别的配置灵活性，满足不同资源和应用需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>钓鱼网站仍然是一个严重的网络安全威胁，它们使用各种策略来躲避检测。</li>
<li>当前基于LLM的钓鱼网站检测方法存在局限性，如易产生幻觉、缺乏可解释性和稳健性。</li>
<li>PhishDebate是一个基于模块化多主体的LLM辩论框架，用于更准确地检测钓鱼网站。</li>
<li>PhishDebate通过四个专业代理对网页的不同文本方面进行分析，包括URL结构、HTML组合、语义内容和品牌伪装。</li>
<li>PhishDebate实现了高达98.2%的召回率和真实阳性率，且在评估中表现优于其他方法。</li>
<li>PhishDebate的模块化设计使其具有灵活性，可以根据资源和应用需求进行调整。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15656">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-acb091eaef921bf977419eeb31d6e9b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e3277797b2db9ff45f573d489de3ac6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-705cef6a86ff3776c07875522ca16a18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d1f92cd048c55c5f6e9adebc30f3dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d631248cddec5c773f9286a7eebc3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c601620dcb82eb6d03a5e443647e69d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18406afc67f6d564f7be05fbc386d04c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FindingDory-A-Benchmark-to-Evaluate-Memory-in-Embodied-Agents"><a href="#FindingDory-A-Benchmark-to-Evaluate-Memory-in-Embodied-Agents" class="headerlink" title="FindingDory: A Benchmark to Evaluate Memory in Embodied Agents"></a>FindingDory: A Benchmark to Evaluate Memory in Embodied Agents</h2><p><strong>Authors:Karmesh Yadav, Yusuf Ali, Gunshi Gupta, Yarin Gal, Zsolt Kira</strong></p>
<p>Large vision-language models have recently demonstrated impressive performance in planning and control tasks, driving interest in their application to real-world robotics. However, deploying these models for reasoning in embodied contexts is limited by their ability to incorporate long-term experience collected across multiple days and represented by vast collections of images. Current VLMs typically struggle to process more than a few hundred images concurrently, highlighting the need for more efficient mechanisms to handle long-term memory in embodied settings. To effectively evaluate these models for long-horizon control, a benchmark must specifically target scenarios where memory is crucial for success. Existing long-video QA benchmarks overlook embodied challenges like object manipulation and navigation, which demand low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together rather than in isolation. In this work, we introduce a new benchmark for long-range embodied tasks in the Habitat simulator. This benchmark evaluates memory-based capabilities across 60 tasks requiring sustained engagement and contextual awareness in an environment. The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We also present baselines that integrate state-of-the-art VLMs with low level navigation policies, assessing their performance on these memory-intensive tasks and highlight areas for improvement. </p>
<blockquote>
<p>大型视觉语言模型最近在规划和控制任务中表现出了令人印象深刻的性能，激发了它们在现实世界机器人技术中的应用兴趣。然而，将这些模型应用于实体环境中的推理受到了它们整合跨多天收集的长期经验的能力的限制，这些经验由大量图像集合表示。当前的主流视觉语言模型（VLMs）通常难以同时处理超过几百张图像，这突显了需要更有效的机制来处理实体环境中的长期记忆。为了有效地评估这些模型进行长期规划控制的能力，基准测试必须专门针对记忆对成功至关重要的场景。现有的长视频问答基准测试忽视了实体挑战，如对象操作和导航，这些挑战需要低级技能和基于过去互动的精细推理。此外，实体代理中的有效记忆整合既涉及回忆相关历史信息，又涉及基于这些信息的行动执行，因此，将这两方面结合起来研究而非孤立地研究至关重要。在这项工作中，我们在 Habitat 模拟环境中引入了一个新的长期实体任务基准测试。该基准测试评估了跨 60 个任务的基于记忆的能力，这些任务要求在环境中持续参与和情境意识。这些任务还可以按程序扩展为更长期和更具挑战性的版本，实现对记忆和推理的可扩展评估。我们还介绍了基线，将最新的视觉语言模型与低级导航策略相结合，评估它们在这些内存密集型任务上的表现，并强调改进的领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15635v1">PDF</a> Our dataset and code will be made available at:   <a target="_blank" rel="noopener" href="https://findingdory-benchmark.github.io/">https://findingdory-benchmark.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大型视觉语言模型在规划和控制任务中展现出令人印象深刻的性能，推动了其在现实世界的机器人技术中的应用。然而，将这些模型部署于实体环境中的推理仍受限于其处理来自多日的丰富经验的能力，这些经验以大量图像形式呈现。当前的大型视觉语言模型往往难以同时处理超过几百张图像，这突显出在实体环境中处理长期记忆的需要更高效机制。为了有效评估这些模型进行长期规划的能力，必须针对记忆对成功至关重要的场景建立专门的基准测试。现有的长视频问答基准测试忽视了实体挑战，如物体操作和导航，这需要低层次的技能和过去的互动的精细推理。此外，实体代理中的有效记忆整合包括回忆相关历史信息和基于该信息执行动作，因此研究这两个方面比单独研究更为重要。在这项工作中，我们在 Habitat 模拟器中引入了一个用于长期实体任务的新基准测试。此基准测试评估了跨越60个任务的记忆能力，这些任务要求在环境中持续参与和情境意识。这些任务还可以通过程序扩展为更长期和更具挑战性的版本，实现对记忆和推理的可扩展评估。我们还介绍了基线，这些基线将最新的大型视觉语言模型与低级导航策略相结合，评估它们在这些内存密集型任务上的性能，并强调了改进领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉语言模型在规划和控制任务中表现出色，但在实体环境中的长期记忆应用受限。</li>
<li>当前模型在处理大量图像时存在困难，需要更高效的长期记忆机制。</li>
<li>现有长视频问答基准测试忽视实体挑战，如物体操作和导航。</li>
<li>记忆整合对于实体代理至关重要，需要同时研究回忆和执行动作。</li>
<li>引入新的基准测试，评估在模拟环境中的长期记忆和任务完成能力。</li>
<li>基线评估结合了最新大型视觉语言模型和低级导航策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c73302f1bdc3cbb7fb31e08aa2ddbf07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c78a3efdf15f8a283c66c19ae6fb727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3370b790683e797f89099278fd72fb38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66a709bb6fc92603de451d0f316684c3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AgentGroupChat-V2-Divide-and-Conquer-Is-What-LLM-Based-Multi-Agent-System-Need"><a href="#AgentGroupChat-V2-Divide-and-Conquer-Is-What-LLM-Based-Multi-Agent-System-Need" class="headerlink" title="AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent   System Need"></a>AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent   System Need</h2><p><strong>Authors:Zhouhong Gu, Xiaoxuan Zhu, Yin Cai, Hao Shen, Xingzhou Chen, Qingyi Wang, Jialin Li, Xiaoran Shi, Haoran Guo, Wenxuan Huang, Hongwei Feng, Yanghua Xiao, Zheyu Ye, Yao Hu, Shaosheng Cao</strong></p>
<p>Large language model based multi-agent systems have demonstrated significant potential in social simulation and complex task resolution domains. However, current frameworks face critical challenges in system architecture design, cross-domain generalizability, and performance guarantees, particularly as task complexity and number of agents increases. We introduces AgentGroupChat-V2, a novel framework addressing these challenges through three core innovations: (1) a divide-and-conquer fully parallel architecture that decomposes user queries into hierarchical task forest structures enabling dependency management and distributed concurrent processing. (2) an adaptive collaboration engine that dynamically selects heterogeneous LLM combinations and interaction modes based on task characteristics. (3) agent organization optimization strategies combining divide-and-conquer approaches for efficient problem decomposition. Extensive experiments demonstrate AgentGroupChat-V2’s superior performance across diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best baseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME (nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance advantages become increasingly pronounced with higher task difficulty, particularly on Level 5 MATH problems where improvements exceed 11 percentage points compared to state-of-the-art baselines. These results confirm that AgentGroupChat-V2 provides a comprehensive solution for building efficient, general-purpose LLM multi-agent systems with significant advantages in complex reasoning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MikeGu721/AgentGroupChat-V2">https://github.com/MikeGu721/AgentGroupChat-V2</a>. </p>
<blockquote>
<p>基于大型语言模型的多智能体系统在社会模拟和复杂任务解决领域表现出了巨大的潜力。然而，当前框架在系统设计、跨域泛化能力和性能保证方面面临严峻挑战，尤其是随着任务复杂性和智能体数量的增加。我们引入了AgentGroupChat-V2，这一新型框架通过三项核心创新来解决这些挑战：（1）一种分而治之的全并行架构，将用户查询分解成层次化的任务森林结构，实现依赖管理和分布式并发处理。（2）一个自适应协作引擎，根据任务特性动态选择异质的大型语言模型组合和交互模式。（3）结合分而治之方法的智能体组织优化策略，以实现有效的问题分解。大量实验表明，AgentGroupChat-V2在各个领域具有卓越的性能，在GSM8K上达到91.50%的准确率（超过最佳基准值5.6个百分点），在竞赛级的AIME上达到30.4%的准确率（几乎是其他方法的一倍），在人类评估的HumanEval上达到79.20%的通过率。性能优势随着任务难度的增加而变得更加明显，特别是在Level 5 MATH问题上，与最新基准相比，改进超过了11个百分点。这些结果证实，AgentGroupChat-V2为构建高效、通用的大型语言模型多智能体系统提供了全面解决方案，在复杂推理场景中具有显著优势。代码可访问于 <a target="_blank" rel="noopener" href="https://github.com/MikeGu721/AgentGroupChat-V2%E3%80%82">https://github.com/MikeGu721/AgentGroupChat-V2。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15451v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型为基础的多智能体系统在社会仿真和复杂任务解决领域展现了巨大潜力，但当前框架在系统架构设计、跨域泛化能力和性能保证等方面面临挑战。提出AgentGroupChat-V2框架，通过三项核心创新解决这些挑战：一是采用分而治之的完全并行架构，将用户查询分解成层次任务森林结构以实现依赖管理和分布式并发处理；二是自适应协作引擎能根据任务特性动态选择异构LLM组合和交互模式；三是优化智能体组织策略，结合分而治之方法实现高效问题分解。实验表明，AgentGroupChat-V2在多个领域表现卓越，如GSM8K准确率91.5%，AIME准确率30.4%，HumanEval的pass@1率达79.2%。随着任务难度增加，性能优势愈发显著。该框架为构建高效、通用的LLM多智能体系统提供了全面解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型为基础的多智能体系统在特定领域有巨大潜力。</li>
<li>当前框架面临系统架构、泛化能力和性能保证的挑战。</li>
<li>AgentGroupChat-V2通过三项核心创新解决这些挑战。</li>
<li>框架采用分而治之架构实现依赖管理和分布式并发处理。</li>
<li>自适应协作引擎能根据任务特性选择LLM组合和交互模式。</li>
<li>AgentGroupChat-V2在多个领域表现卓越，如GSM8K、AIME和HumanEval。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15451">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d2b48eec333776abde06c9cfca66824.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de50d99bb26e72cc02ad6318da4fbb05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f67ca625cf2abbf82c615c673ab5f9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63c7e89ad6f39ef7503e2e6b295f3149.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e2582deaf716f2c44415ecf37764148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-526467972ead5fc18c7cc9676680939d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RAS-Eval-A-Comprehensive-Benchmark-for-Security-Evaluation-of-LLM-Agents-in-Real-World-Environments"><a href="#RAS-Eval-A-Comprehensive-Benchmark-for-Security-Evaluation-of-LLM-Agents-in-Real-World-Environments" class="headerlink" title="RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM   Agents in Real-World Environments"></a>RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM   Agents in Real-World Environments</h2><p><strong>Authors:Yuchuan Fu, Xiaohan Yuan, Dongxia Wang</strong></p>
<p>The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/lanzer-tree/RAS-Eval">https://github.com/lanzer-tree/RAS-Eval</a>. </p>
<blockquote>
<p>大型语言模型（LLM）代理在医疗和财务等重要领域的快速部署需要稳健的安全框架。为了解决这些代理在动态环境中缺乏标准化评估基准的问题，我们引入了RAS-Eval，这是一个支持模拟和真实世界工具执行的全面安全基准。RAS-Eval包含80个测试用例和3802个攻击任务，映射到11个通用弱点枚举（CWE）类别，工具采用JSON、LangGraph和模型上下文协议（MCP）格式实现。我们在多种场景下评估了6个最先进的大型语言模型，揭示了显著的漏洞：攻击使代理任务完成率（TCR）平均降低了36.78%，在学术环境中的成功率达到了85.65%。值得注意的是，安全能力的规模定律同样适用，大型模型的表现优于小型模型。我们的研究揭示了真实世界代理部署中的关键风险，为未来安全研究提供了基础框架。相关代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/lanzer-tree/RAS-Eval%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lanzer-tree/RAS-Eval获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15253v1">PDF</a> 12 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）代理在医疗和金融等重要领域中的快速部署需要可靠的安全框架。为解决动态环境中针对这些代理的标准评估基准的缺失问题，我们推出RAS-Eval综合安全基准，支持模拟和现实世界工具执行的评估。RAS-Eval包含80个测试用例和3802个攻击任务，映射到11个通用弱点枚举（CWE）类别，工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。我们评估了6种最新的大型语言模型，揭示出显著的漏洞：攻击使代理任务完成率（TCR）平均降低了36.78%，学术环境中的攻击成功率达到85.65%。值得注意的是，安全能力也遵循规模法则，大型模型表现出优于小型模型的能力。我们的研究揭示了将代理部署到现实世界中的关键风险，并为未来的安全研究提供了基础框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在医疗和金融等领域的快速应用需要建立可靠的安全框架。</li>
<li>缺乏针对LLM代理的标准评估基准，因此需要开发综合安全基准如RAS-Eval。</li>
<li>RAS-Eval包含80个测试用例和多样化的攻击任务，涉及多种格式和标准。</li>
<li>攻击可能导致代理任务完成率显著降低，学术环境中攻击成功率高。</li>
<li>大型语言模型在安全性能上表现较好，较大的模型通常优于较小的模型。</li>
<li>现有LLM存在显著漏洞，需加强安全研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed6c8ceab318fdea070c3be2db7d6c1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3ac615b6a4b5cf57d6be23d90dacd97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b374e55450ef43f90c233957e7f9b24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6fb3dd6548065098432a99649b4772c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e10fe627af9fee58d8ed47788d77866.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c9fe69fb6efa413406099984c3f0ca4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea394024e54fcf5bb7ae0a37df2aff9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-511ef9de4a3e4eb2d2d752574b0b00bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb11cf1806d980a00f7732e4a5966d51.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Reinforcement-Learning-for-Autonomous-Multi-Satellite-Earth-Observation-A-Realistic-Case-Study"><a href="#Multi-Agent-Reinforcement-Learning-for-Autonomous-Multi-Satellite-Earth-Observation-A-Realistic-Case-Study" class="headerlink" title="Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth   Observation: A Realistic Case Study"></a>Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth   Observation: A Realistic Case Study</h2><p><strong>Authors:Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk</strong></p>
<p>The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions. </p>
<blockquote>
<p>低地球轨道（LEO）卫星的指数级增长已经彻底改变了地球观测（EO）任务，解决了气候监测、灾害管理等方面的挑战。然而，多卫星系统中的自主协调仍然是一个基本挑战。传统优化方法难以满足动态地球观测任务的实时决策需求，需要使用强化学习（RL）和多智能体强化学习（MARL）。在本文中，我们通过模拟单卫星操作并扩展到多卫星星座的MARL框架，研究了基于RL的自主地球观测任务规划。我们解决了关键挑战，包括能源和数据存储限制、卫星观测的不确定性以及在部分观测下的分散协调的复杂性。通过利用近乎现实的卫星仿真环境，我们评估了最先进的MARL算法的训练稳定性和性能，包括PPO、IPPO、MAPPO和HAPPO。我们的结果表明，MARL可以有效地平衡成像和资源管理，同时解决多卫星协调中的非稳定性和奖励互赖性问题。本研究所得见解为自主卫星操作提供了基础，为改进分散式地球观测任务中的政策学习提供了实用指南。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着低地球轨道（LEO）卫星数量的指数增长，地球观测（EO）任务经历了革命性的变化，应对气候监测、灾害管理等领域的挑战。然而，多卫星系统的自主协调仍是基本挑战。传统优化方法难以满足动态EO任务的实时决策需求，需要采用强化学习（RL）和多智能体强化学习（MARL）。本文研究基于RL的自主EO任务规划，通过建模单卫星操作并扩展到多卫星星座的MARL框架。解决关键挑战，包括能源和数据存储限制、卫星观测的不确定性以及部分观测下的分散协调的复杂性。通过利用近现实的卫星仿真环境，我们评估了最新MARL算法的训练稳定性和性能，包括PPO、IPPO、MAPPO和HAPPO。结果证明MARL能有效平衡成像和资源管理，解决多卫星协调中的非稳定性和奖励互依性问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LEO卫星的增长推动了EO任务的革新，提升了气候监测和灾害管理等方面的能力。</li>
<li>自主协调在多卫星系统中仍是一个主要挑战。</li>
<li>传统优化方法无法满足动态EO任务的实时决策需求。</li>
<li>RL和MARL对于解决此类问题具有潜力。</li>
<li>研究通过MARL框架建模单卫星操作并扩展到多卫星星座。</li>
<li>解决能源、数据存储限制等关键挑战是实施自主卫星操作的关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4b1d165dbcb3d0af31ef0193c563eb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fc5937e25b8e1d567f1ba9325448463.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fair-Algorithms-with-Probing-for-Multi-Agent-Multi-Armed-Bandits"><a href="#Fair-Algorithms-with-Probing-for-Multi-Agent-Multi-Armed-Bandits" class="headerlink" title="Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits"></a>Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</h2><p><strong>Authors:Tianyi Xu, Jiaxin Liu, Zizhan Zheng</strong></p>
<p>We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency. </p>
<blockquote>
<p>我们提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在确保智能体之间的公平结果，同时最大化整体系统性能。在此设置中，一个关键挑战是在有限的关于手臂奖励的信息下进行决策。为解决这一问题，我们引入了一种新的探测框架，该框架在分配之前会策略性地收集关于选定手臂的信息。在奖励分布已知的情况下，我们利用子模块属性设计了一种贪婪探测算法，具有可证明的性能界限。对于更复杂的在线环境，我们开发了一种算法，在保持公平性的同时实现次线性遗憾。在合成数据和真实数据集上的广泛实验表明，我们的方法优于基线方法，实现了更好的公平性和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14988v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>提出一种多智能体多臂老虎机（MA-MAB）框架，旨在确保智能体之间的公平结果并最大化整体系统性能。在奖励分配有限信息的情况下，引入了一种新的探测框架，以在分配前对选定手臂进行战略性信息收集。离线场景下利用子模块属性设计贪婪探测算法，并提供可证明的性能界限。对于更复杂的在线场景，开发了一种既保证公平性又实现次线性遗憾的算法。在合成和真实数据集上的广泛实验表明，该方法优于基线方法，实现了更好的公平性和效率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了多智能体多臂老虎机（MA-MAB）框架，旨在平衡系统性能和公平性。</li>
<li>针对奖励分配信息有限的问题，引入了探测框架来收集手臂信息。</li>
<li>在离线场景下利用子模块属性设计贪婪探测算法，具有可证明的性能界限。</li>
<li>对于在线场景，开发了一种保证公平性和实现次线性遗憾的算法。</li>
<li>该方法通过广泛实验验证，在合成和真实数据集上表现优于基线方法。</li>
<li>该方法能在确保公平性的同时提高系统效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cebd040a0fd5cefd5a58fe9d9506828a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f3fb8a635e41cbc149267f3978b5c9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ImmerseGen-Agent-Guided-Immersive-World-Generation-with-Alpha-Textured-Proxies"><a href="#ImmerseGen-Agent-Guided-Immersive-World-Generation-with-Alpha-Textured-Proxies" class="headerlink" title="ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured   Proxies"></a>ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured   Proxies</h2><p><strong>Authors:Jinyan Yuan, Bangbang Yang, Keke Wang, Panwang Pan, Lin Ma, Xuehai Zhang, Xiao Liu, Zhaopeng Cui, Yuewen Ma</strong></p>
<p>Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery. This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: <a target="_blank" rel="noopener" href="https://immersegen.github.io/">https://immersegen.github.io</a>. </p>
<blockquote>
<p>针对沉浸式虚拟现实存在的自动创建三维场景一直是几十年的研究重点。然而，现有方法往往依赖于具有事后简化的高多边形网格建模或大规模三维高斯建模，导致管道复杂或视觉逼真度受限。在本文中，我们证明了实现引人注目的沉浸式体验并不需要如此详尽的建模。我们介绍了ImmerseGen，这是一种用于紧凑和逼真的世界建模的新型代理引导框架。ImmerseGen将场景表示为轻便几何代理的层次结构组合，即简化地形和招牌网格，并通过在这些代理上合成RGBA纹理来生成逼真的外观。具体来说，我们提出了针对用户中心基地世界合成的地形条件纹理贴图，以及用于中景和前景景色的RGBA资产纹理贴图。这种重新表述提供了几个优点：（i）它简化了建模过程，使代理能够引导生成模型产生无缝集成场景的连贯纹理；（ii）它通过直接在代理上合成逼真的纹理绕过复杂的几何创建和减少，同时保留视觉质量而不会降级；（iii）它支持在移动VR耳机上进行实时渲染的紧凑表示形式。为了从文本提示自动创建场景，我们引入了基于VLM的建模代理，并增强了基于语义网格的分析以改善空间推理和准确的资产放置。ImmerseGen还通过动态效果和环绕声音进一步丰富了场景，以支持多感官沉浸。场景生成和实时VR展示的实验表明，ImmerseGen在逼真度、空间连贯性和渲染效率方面优于先前的方法。项目网页：<a target="_blank" rel="noopener" href="https://immersegen.github.io./">https://immersegen.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14315v2">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://immersegen.github.io/">https://immersegen.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的沉浸式虚拟现实场景创建方法——ImmerseGen。该方法采用代理引导的框架，以简化地形和贴图网格的层次结构表示场景，并通过合成RGBA纹理赋予场景逼真的外观。此外，还引入了基于文本提示的场景创建自动化方法，增强了空间推理和资产放置的准确性。ImmerseGen丰富了场景的动态效果和环绕音效，支持多感官沉浸。相较于以往的方法，ImmerseGen在场景生成、逼真度、空间连贯性和渲染效率上表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ImmerseGen是一个新型的沉浸式虚拟现实场景创建方法，采用代理引导的框架进行场景建模。</li>
<li>ImmerseGen通过简化地形和贴图网格的层次结构来表示场景，并合成RGBA纹理以赋予场景逼真的外观。</li>
<li>该方法引入了基于文本提示的场景创建自动化方法，增强空间推理和资产放置的准确性。</li>
<li>ImmerseGen通过丰富场景的动态效果和环绕音效，支持多感官沉浸。</li>
<li>ImmerseGen在场景生成、逼真度、空间连贯性和渲染效率方面优于以往的方法。</li>
<li>ImmerseGen适用于移动VR头盔的实时渲染，具有紧凑的场景表示形式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-02dfbec754a7b94a0665573e866b010b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f8c58244f3ee325370287b2217ccee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe106a590b4b4bb9c457b4c1cb8b16a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b8de25682f4fd05dd849c379a7f6e94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d59c443c8f6e2194faaf94e7668e084.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f5d5679d175719cfa432560f37dc16a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Wasserstein-Barycenter-Consensus-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#Wasserstein-Barycenter-Consensus-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Wasserstein-Barycenter Consensus for Cooperative Multi-Agent   Reinforcement Learning"></a>Wasserstein-Barycenter Consensus for Cooperative Multi-Agent   Reinforcement Learning</h2><p><strong>Authors:Ali Baheri</strong></p>
<p>Cooperative multi-agent reinforcement learning (MARL) demands principled mechanisms to align heterogeneous policies while preserving the capacity for specialized behavior. We introduce a novel consensus framework that defines the team strategy as the entropic-regularized $p$-Wasserstein barycenter of agents’ joint state–action visitation measures. By augmenting each agent’s policy objective with a soft penalty proportional to its Sinkhorn divergence from this barycenter, the proposed approach encourages coherent group behavior without enforcing rigid parameter sharing. We derive an algorithm that alternates between Sinkhorn-barycenter computation and policy-gradient updates, and we prove that, under standard Lipschitz and compactness assumptions, the maximal pairwise policy discrepancy contracts at a geometric rate. Empirical evaluation on a cooperative navigation case study demonstrates that our OT-barycenter consensus outperforms an independent learners baseline in convergence speed and final coordination success. </p>
<blockquote>
<p>多智能体强化学习（MARL）需要具有原则性的机制来对齐异质策略，同时保留执行特殊行为的能力。我们引入了一种新的共识框架，将团队策略定义为智能体联合状态-行动访问度量的熵正则化p-Wasserstein重心。通过增加每个智能体的策略目标，同时采用与其与此重心的Sinkhorn分歧的软罚分比例，该方法鼓励连贯的群体行为，而不会强制执行严格的参数共享。我们提出了一种算法，该算法在Sinkhorn重心计算和策略梯度更新之间进行交替，并在标准的Lipschitz和紧凑性假设下证明，最大成对策略差异以几何速率收缩。在合作导航案例研究中的经验评估表明，我们的OT重心共识方法在收敛速度和最终协调成功方面优于独立学习者的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12497v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种新型的基于合作的多智能体强化学习（MARL）框架。它利用基于熵正则化的p-Wasserstein质心来定义团队策略，并通过对每个智能体的策略目标增加一个与其偏离该质心的软罚项来鼓励一致性的群体行为。实验结果表明，在合作导航场景中，这种新的基于OT-barycenter一致性方法的性能优于独立学习基线，能够在收敛速度和最终协调成功率方面实现更优的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的合作多智能体强化学习框架，利用p-Wasserstein质心定义团队策略。</li>
<li>通过为每个智能体的策略目标增加软罚项，鼓励群体行为的一致性。</li>
<li>介绍了交替进行Sinkhorn-barycenter计算和策略梯度更新的算法。</li>
<li>在理论层面，证明了在标准的Lipschitz和紧凑性假设下，最大策略差异会以几何速率收缩。</li>
<li>实证评估表明，新框架在收敛速度和最终协调成功率方面优于独立学习基线。</li>
<li>框架设计保留了智能体的异质性，同时允许它们保持一致性行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-106558c4de1df8498f32fc2988deba95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68fe63fe0c4187a4727fa7fe7afc8dc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4400650edbe5055e83683d79d2ca3989.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CORA-Coalitional-Rational-Advantage-Decomposition-for-Multi-Agent-Policy-Gradients"><a href="#CORA-Coalitional-Rational-Advantage-Decomposition-for-Multi-Agent-Policy-Gradients" class="headerlink" title="CORA: Coalitional Rational Advantage Decomposition for Multi-Agent   Policy Gradients"></a>CORA: Coalitional Rational Advantage Decomposition for Multi-Agent   Policy Gradients</h2><p><strong>Authors:Mengda Ji, Genjiu Xu, Liying Wang</strong></p>
<p>This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to suboptimal policy updates as it fails to account for the distinct contributions of agents. Although numerous methods consider global or individual contributions for credit assignment, a detailed analysis at the coalition level remains lacking in many approaches. This work analyzes the over-updating problem during multi-agent policy updates from a coalition-level perspective. To address this issue, we propose a credit assignment method called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates coalitional advantages via marginal contributions from all possible coalitions and decomposes advantages using the core solution from cooperative game theory, ensuring coalitional rationality. To reduce computational overhead, CORA employs random coalition sampling. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that CORA outperforms strong baselines, particularly in tasks with multiple local optima. These findings highlight the importance of coalition-aware credit assignment for improving MARL performance. </p>
<blockquote>
<p>本文关注合作多智能体强化学习（MARL）中的信用分配问题。在智能体之间共享全局优势往往会导致次优的策略更新，因为它无法解释智能体的不同贡献。尽管许多方法考虑了全局或个人贡献来进行信用分配，但在联盟层面进行详细分析的方法仍然缺乏。本文从联盟的角度分析了多智能体策略更新过程中的过度更新问题。为了解决这一问题，我们提出了一种名为“联盟理性优势分解”（CORA）的信用分配方法。CORA通过所有可能联盟的边际贡献来评估联盟优势，并使用合作博弈论的核心解决方案进行优势分解，确保联盟理性。为了减少计算开销，CORA采用随机联盟抽样。在矩阵游戏、差异游戏和多智能体协作基准测试上的实验表明，CORA优于强大的基线，特别是在具有多个局部最优的任务中。这些发现强调了联盟意识信用分配对改善MARL性能的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04265v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该工作聚焦在合作多智能体强化学习中的信用分配问题。分析全局优势在智能体间的共享会导致策略更新次优，因为未能考虑到智能体的独特贡献。文章从联盟层面分析了多智能体策略更新过程中的过度更新问题，并提出了名为“联盟理性优势分解”（CORA）的信用分配方法。CORA通过评估所有可能联盟的边际贡献来衡量联盟优势，并使用合作博弈论的核解来保证联盟理性。为减少计算开销，CORA采用随机联盟采样。在矩阵游戏、差异游戏和多智能体协作基准测试上的实验表明，CORA在具有多个局部最优的任务中表现优于强大的基线。这表明联盟感知的信用分配对于提高MARL性能至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该工作研究了合作多智能体强化学习中的信用分配问题，指出全局优势共享可能导致策略更新次优。</li>
<li>现有方法多未能从联盟层面详细分析多智能体间的信用分配。</li>
<li>提出了名为“联盟理性优势分解”（CORA）的信用分配方法，通过评估联盟边际贡献来衡量联盟优势。</li>
<li>CORA使用合作博弈论的核解来保证联盟理性。</li>
<li>为减少计算成本，CORA采用随机联盟采样。</li>
<li>实验表明，CORA在多种任务上表现优异，特别是具有多个局部最优的任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-506da872f0eee6ff4e84475319e09120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34ee189f3a22386cd7490095f4024ef1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ChemHAS-Hierarchical-Agent-Stacking-for-Enhancing-Chemistry-Tools"><a href="#ChemHAS-Hierarchical-Agent-Stacking-for-Enhancing-Chemistry-Tools" class="headerlink" title="ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools"></a>ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</h2><p><strong>Authors:Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi</strong></p>
<p>Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: &#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;ChemHAS-01E4&#x2F;README.md. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理已经显示出通过选择适当的工具来提高在化学相关任务中的性能的能力。然而，它们的有效性仍然受到化学工具固有预测误差的限制。在本文中，我们更进一步探索了如何反过来利用基于LLM的代理来减少工具的预测误差。为此，我们提出了ChemHAS（化学分层代理堆叠）方法，这是一种通过优化有限数据下的代理堆叠结构来增强化学工具性能的有效方法。ChemHAS在四个基本化学任务上达到了最先进的性能水平，证明了我们的方法可以有效地补偿工具的预测误差。此外，我们还确定了四种不同的代理堆叠行为并对其进行了表征，这可能会提高可解释性并揭示了人工智能代理在科学研究中应用的新可能性。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ChemHAS-01E4/README.md%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/ChemHAS-01E4/README.md上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21569v2">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>基于大语言模型的代理在化学相关任务中表现出了性能提升的能力，但仍受到化学工具固有预测误差的限制。本研究提出ChemHAS（化学分层代理堆叠）方法，通过优化代理堆叠结构来增强化学工具的性能，从有限数据中获益。ChemHAS在四项基本化学任务上实现了卓越性能，有效弥补了工具的预测误差。此外，研究还确定了四种不同的代理堆叠行为，提高了可解释性，并揭示了人工智能代理在科学研究中新的应用可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型代理能够改善化学相关任务的性能，但受限于化学工具的预测误差。</li>
<li>ChemHAS方法通过优化代理堆叠结构增强化学工具性能。</li>
<li>ChemHAS在四项基本化学任务上实现卓越性能，有效补偿工具预测误差。</li>
<li>研究确定了四种不同的代理堆叠行为，提高可解释性。</li>
<li>ChemHAS公开可用的代码和数据集为科学研究提供了新工具。</li>
<li>此研究揭示了人工智能代理在科学研究中新的应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21569">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c079a28035ebfca698ddb462fe60bf9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c6f61f725acd2c3489601566e1217ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9aa282ee20be9012486a3ce852fc021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fccc45a26c9f3247925f4ce818f3253.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20fb4b288108cf4ec0aa2e3beb5ece4f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Švábenský, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators’ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>我们探索了大型语言模型（LLM）在自动评估学生开放性反思和预测学业表现方面的应用。传统的反思评估方法耗时且可能无法在教育环境中有效地扩展。在这项工作中，我们采用LLM，使用两种评估策略（单智能体和多智能体）和两种提示技术（零样本和少样本），将学生反思转化为量化分数。我们在包含来自377名学生在三个学术学期内的5,278篇反思的数据集上进行的实验表明，采用少样本策略的单智能体评估匹配率最高。此外，使用LLM评估的反思分数的模型在处于风险的学生识别和成绩预测任务中的表现均优于基线。这些结果表明，LLM可以有效地自动进行反思评估，减少教育工作者的工作量，并为可能需要额外帮助的学生提供及时的支持。我们的工作强调了将先进的生成性AI技术整合到教育实践中以提高学生参与度和学业成功的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v3">PDF</a> Published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025), see   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-981-96-8186-0_24">https://doi.org/10.1007/978-981-96-8186-0_24</a></p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLMs）可用于自动评估学生的开放文本反思和预测学业表现。传统评估方法耗时且难以在教育环境中有效扩展。本研究利用LLMs将学生的反思转化为量化分数，采用两种评估策略（单智能体和多智能体）和两种提示技术（零样本和少样本）。实验表明，使用少样本策略的单智能体方法与人评的匹配率最高。此外，使用LLM评估的反思分数的模型在风险学生识别和成绩预测任务中的表现优于基线。这表明LLMs可以有效自动化反思评估，减轻教育工作者的工作量，并为可能需要额外帮助的学生提供及时支持。本研究强调了将高级生成式AI技术融入教育实践以提高学生参与度和学业成功的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLMs）被用于自动评估学生的开放文本反思和预测学业表现，旨在提高效率和准确性。</li>
<li>研究采用了两种评估策略：单智能体和多智能体，以及两种提示技术：零样本和少样本。</li>
<li>实验结果表明，少样本策略的单智能体方法在与人评的匹配方面表现最佳。</li>
<li>使用LLM评估的反思分数的模型在风险学生识别和成绩预测任务中优于基线方法。</li>
<li>LLMs的自动化评估可以有效减轻教育工作者的负担，并提供及时的学生支持。</li>
<li>研究结果强调了将高级AI技术融入教育实践的潜力，以提高学生的参与度和学业成功。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-727cd303d72d5350a7210743478c92d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="I-MCTS-Enhancing-Agentic-AutoML-via-Introspective-Monte-Carlo-Tree-Search"><a href="#I-MCTS-Enhancing-Agentic-AutoML-via-Introspective-Monte-Carlo-Tree-Search" class="headerlink" title="I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree   Search"></a>I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree   Search</h2><p><strong>Authors:Zujie Liang, Feng Wei, Wujiang Xu, Lin Chen, Yuxi Qian, Xinhui Wu</strong></p>
<p>Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process. Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node’s solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier. Applied to the various ML tasks, our approach demonstrates a 6% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems. Resource available at <a target="_blank" rel="noopener" href="https://github.com/jokieleung/I-MCTS">https://github.com/jokieleung/I-MCTS</a> </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步在自动化机器学习任务方面显示出显著潜力。然而，现有的基于LLM的代理人在代码生成方面常常面临低多样性和非最优的问题。尽管最近有研究工作引入了蒙特卡洛树搜索（MCTS）来解决这些问题，但在生成思想的品质和多样性以及用于节点选择的标量值反馈机制方面仍存在局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14693v3">PDF</a> </p>
<p><strong>Summary</strong><br>大语言模型（LLM）在自动化机器学习任务中展现出巨大潜力，但仍存在代码生成多样性不足和次优问题。本研究引入内省蒙特卡洛树搜索（I-MCTS），通过迭代扩展树节点，分析父节点和兄弟节点的解决方案和结果，提高决策质量。结合LLM价值模型直接评估节点解决方案，实施混合奖励机制，实现LLM估计分数与实际性能分数的无缝过渡。在机器学习任务中应用，相比强大的开源自动机器学习代理人，性能绝对提高了6%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）在自动化机器学习任务中具有巨大潜力，但存在代码生成多样性和优化问题。</li>
<li>内省蒙特卡洛树搜索（I-MCTS）通过迭代分析父节点和兄弟节点的解决方案和结果，提高决策质量。</li>
<li>I-MCTS方法促进搜索树中节点的持续改进，从而增强整体决策过程。</li>
<li>结合LLM价值模型直接评估节点解决方案，减少全面计算滚动所需的步骤。</li>
<li>实施混合奖励机制，实现LLM估计分数与实际性能分数的无缝过渡。</li>
<li>I-MCTS在机器学习任务中的应用相比现有技术有显著改进，性能提高了6%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-faf8a9051717354ecde5e342fc369821.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8b805e419baf981be91c55eda30c171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84c1da5035bdf65c90365f64cb862b6d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Learning-Strategic-Language-Agents-in-the-Werewolf-Game-with-Iterative-Latent-Space-Policy-Optimization"><a href="#Learning-Strategic-Language-Agents-in-the-Werewolf-Game-with-Iterative-Latent-Space-Policy-Optimization" class="headerlink" title="Learning Strategic Language Agents in the Werewolf Game with Iterative   Latent Space Policy Optimization"></a>Learning Strategic Language Agents in the Werewolf Game with Iterative   Latent Space Policy Optimization</h2><p><strong>Authors:Zelai Xu, Wanjun Gu, Chao Yu, Yi Wu, Yu Wang</strong></p>
<p>Large language model (LLM) agents have recently demonstrated impressive capabilities in various domains like open-ended conversation and multi-step decision-making. However, it remains challenging for these agents to solve strategic language games, such as Werewolf, which demand both strategic decision-making and free-form language interactions. Existing LLM agents often suffer from intrinsic bias in their action distributions and limited exploration of the unbounded text action space, resulting in suboptimal performance. To address these challenges, we propose Latent Space Policy Optimization (LSPO), an iterative framework that combines game-theoretic methods with LLM fine-tuning to build strategic language agents. LSPO leverages the observation that while the language space is combinatorially large, the underlying strategy space is relatively compact. We first map free-form utterances into a finite latent strategy space, yielding an abstracted extensive-form game. Then we apply game-theoretic methods like Counterfactual Regret Minimization (CFR) to optimize the policy in the latent space. Finally, we fine-tune the LLM via Direct Preference Optimization (DPO) to align with the learned policy. By iteratively alternating between these steps, our LSPO agents progressively enhance both strategic reasoning and language communication. Experiment on the Werewolf game shows that our agents iteratively expand the strategy space with improving performance and outperform existing Werewolf agents, underscoring their effectiveness in free-form language games with strategic interactions. </p>
<blockquote>
<p>大型语言模型（LLM）代理最近在开放对话和多步决策等各个领域表现出了令人印象深刻的能力。然而，对于战略语言游戏（如Werewolf），这些代理仍然面临挑战，这些游戏需要战略决策和自由的文本交互。现有的LLM代理经常受到其行动分布中的固有偏见和无界文本行动空间有限探索的影响，导致性能不佳。为了解决这些挑战，我们提出了潜在空间策略优化（LSPO），这是一个结合博弈论方法和LLM微调来构建战略语言代理的迭代框架。LSPO利用了一个观察结果，即虽然语言空间是组合性的巨大，但潜在的策略空间是相对紧凑的。我们首先将自由形式的言语映射到一个有限的潜在策略空间，形成一个抽象的扩展形式的游戏。然后，我们应用博弈论方法，如反事实后悔最小化（CFR）来优化潜在空间中的策略。最后，我们通过直接偏好优化（DPO）对LLM进行微调，以与所学的策略对齐。通过在这些步骤之间迭代交替，我们的LSPO代理逐步提高了战略推理和语言沟通的能力。在Werewolf游戏上的实验表明，我们的代理通过迭代扩展策略空间，性能得到提升，并超越了现有的Werewolf代理，这证明了它们在具有战略交互的自由形式语言游戏中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04686v3">PDF</a> Published in ICML 2025</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）代理在开放对话和多步决策制定等域中展现了令人印象深刻的能力。然而，对于战略语言游戏如Werewolf这类既需要战略决策又需自由形式语言交互的游戏，这些代理仍面临挑战。针对现有LLM代理在行动分布上的内在偏见和无界文本行动空间探索的局限性，我们提出了潜在空间策略优化（LSPO）方法。这是一种结合博弈论方法和LLM微调来构建战略性语言代理的迭代框架。LSPO利用了一种观察结果，即虽然语言空间组合庞大，但潜在的策略空间相对紧凑。我们首先将自由形式的言论映射到有限的潜在策略空间，形成一个抽象化的扩展形式游戏。然后应用博弈论方法如反事实后悔最小化（CFR）来优化潜在空间的策略。最后，我们通过直接偏好优化（DPO）来微调LLM，使其与学到的策略相一致。通过反复交替这些步骤，我们的LSPO代理逐步提高了战略推理和语言沟通能力。在Werewolf游戏上的实验表明，我们的代理通过迭代扩展策略空间，性能得到提升，并超越了现有的Werewolf代理，突显了它们在具有战略交互的自由形式语言游戏中的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在战略语言游戏中的表现面临挑战。</li>
<li>现有LLM代理在行动分布上存在内在偏见，且对无界文本行动空间的探索有限。</li>
<li>提出了潜在空间策略优化（LSPO）方法来解决这些问题。</li>
<li>LSPO将自由形式的言论映射到有限的潜在策略空间，形成一个抽象化的扩展形式游戏。</li>
<li>通过应用博弈论方法如反事实后悔最小化（CFR）来优化潜在空间的策略。</li>
<li>通过直接偏好优化（DPO）微调LLM，使其与学到的策略相一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04686">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8929e2a2752a4f0a3f1060c3b3ce9e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84f67500f43b5cc421bd9fdb39ed597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abaa7bad61a4ba35fdd37c804a86d3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-802e68226f4d2e5ca47ac91db4d3452e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Wolfpack-Adversarial-Attack-for-Robust-Multi-Agent-Reinforcement-Learning"><a href="#Wolfpack-Adversarial-Attack-for-Robust-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement   Learning"></a>Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, Seungyul Han</strong></p>
<p>Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering systemwide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sunwoolee0504/WALL">https://github.com/sunwoolee0504/WALL</a>. </p>
<blockquote>
<p>在多智能体强化学习（MARL）中，传统的鲁棒方法经常在合作场景中对抗协调攻击时遇到挑战。为了解决这个问题，我们提出了狼群对抗攻击框架，该框架借鉴了狼群狩猎策略，以破坏某个初始智能体及其辅助智能体之间的合作为目标。此外，我们还引入了狼群对抗性强化学习（WALL）框架，它通过促进系统级的协作，训练出能抵抗所提出的狼群攻击的鲁棒MARL策略。实验结果突出了狼群攻击的巨大影响以及WALL在鲁棒性方面的显著改进。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sunwoolee0504/WALL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/sunwoolee0504/WALL获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02844v3">PDF</a> 9 pages main, 23 pages appendix with reference. Accepeted by ICML   2025</p>
<p><strong>Summary</strong></p>
<p>该文本描述了传统稳健的多智能体强化学习（MARL）方法在面对合作场景中的协同对抗攻击时的局限。为解决这个问题，研究者提出以狼群狩猎策略为灵感的狼群对抗攻击框架，目标是干扰初始智能体及其辅助智能体的合作。同时，他们引入了狼群对抗学习MARL（WALL）框架，通过促进整体系统协作，训练出能够抵御狼群攻击的策略。实验结果显示狼群攻击的巨大破坏性，以及WALL框架实现的显著鲁棒性提升。相关代码可通过相关链接访问。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是该文本的关键要点：</p>
<ul>
<li>传统稳健的多智能体强化学习方法在面对合作场景中的协同对抗攻击时面临挑战。</li>
<li>提出狼群对抗攻击框架，模仿狼群狩猎策略干扰智能体间的合作。</li>
<li>引入狼群对抗学习MARL（WALL）框架，旨在训练出能够抵御狼群攻击的策略。</li>
<li>实验结果显示狼群攻击的破坏性，以及WALL框架在提升鲁棒性方面的显著成效。</li>
<li>狼群对抗攻击不仅影响被攻击的智能体，还会干扰其辅助智能体的协作。</li>
<li>WALL框架通过促进系统整体协作来增强鲁棒性，有助于多智能体在复杂环境下的适应和学习。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f0d49782f399fa018901fed1bd3c4a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd5a4ad9175b88aa2301fab87d6a9bc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24a93ccdc21866de43968312bb82d86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7b7255c26ac0654d01096b7639d8be2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8632af54298a0b7d8bdd4a977b05904c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-078fddf8466a425d5a2ff5c9531d5022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a983a496660ebc6b5aa34929ca4f440c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="YOLO-MARL-You-Only-LLM-Once-for-Multi-Agent-Reinforcement-Learning"><a href="#YOLO-MARL-You-Only-LLM-Once-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning"></a>YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Yuan Zhuang, Yi Shen, Zhili Zhang, Yuxiao Chen, Fei Miao</strong></p>
<p>Advancements in deep multi-agent reinforcement learning (MARL) have positioned it as a promising approach for decision-making in cooperative games. However, it still remains challenging for MARL agents to learn cooperative strategies for some game environments. Recently, large language models (LLMs) have demonstrated emergent reasoning capabilities, making them promising candidates for enhancing coordination among the agents. However, due to the model size of LLMs, it can be expensive to frequently infer LLMs for actions that agents can take. In this work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that leverages the high-level task planning capabilities of LLMs to improve the policy learning process of multi-agents in cooperative games. Notably, for each game environment, YOLO-MARL only requires one time interaction with LLMs in the proposed strategy generation, state interpretation and planning function generation modules, before the MARL policy training process. This avoids the ongoing costs and computational time associated with frequent LLMs API calls during training. Moreover, trained decentralized policies based on normal-sized neural networks operate independently of the LLM. We evaluate our method across two different environments and demonstrate that YOLO-MARL outperforms traditional MARL algorithms. </p>
<blockquote>
<p>深度多智能体强化学习（MARL）的进展使其成为一种在合作游戏中进行决策的有前途的方法。然而，对于某些游戏环境，MARL智能体学习合作策略仍然具有挑战性。最近，大型语言模型（LLM）表现出了涌现的推理能力，使其成为增强智能体之间协调性的有前途的候选者。然而，由于LLM的模型规模，频繁推断LLM来进行智能体可以采取的行动可能会很昂贵。在这项工作中，我们提出了You Only LLM Once for MARL（YOLO-MARL），这是一个利用LLM的高级任务规划能力来改善合作游戏中多智能体的策略学习过程的新框架。值得注意的是，对于每个游戏环境，YOLO-MARL仅在策略生成、状态解释和规划功能生成模块中要求与LLM进行一次交互，然后在MARL政策训练过程开始之前。这避免了训练过程中频繁的LLM API调用相关的持续成本和计算时间。此外，基于正常大小神经网络的训练后的分散策略独立于LLM运行。我们在两个不同的环境中评估了我们的方法，并证明YOLO-MARL优于传统的MARL算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03997v2">PDF</a> accepted to International Conference on Intelligent Robots and   Systems (IROS2025)</p>
<p><strong>Summary</strong></p>
<p>深多智能体强化学习（MARL）在合作游戏中展现出潜力，但在某些游戏环境下，学习合作策略仍是挑战。大型语言模型（LLM）具备推理能力，可增强智能体间的协调。然而，LLM模型尺寸大，频繁用于动作推断成本高昂。本研究提出YOLO-MARL框架，利用LLM的高级任务规划能力改善多智能体的策略学习过程。YOLO-MARL仅需一次与LLM的互动，即可在策略生成、状态解读和规划功能生成模块中，进行MARL政策训练过程，降低计算时间和成本。在独立于LLM的正常大小神经网络训练分散政策后，表现出优于传统MARL算法的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深多智能体强化学习（MARL）在合作游戏中面临学习合作策略的挑战。</li>
<li>大型语言模型（LLM）具备推理能力，可增强智能体间的协调，但模型尺寸大导致频繁推断成本高昂。</li>
<li>YOLO-MARL框架利用LLM的高级任务规划能力，改善多智能体的策略学习过程。</li>
<li>YOLO-MARL仅需一次与LLM互动，降低计算时间和成本。</li>
<li>YOLO-MARL框架包括策略生成、状态解读和规划功能生成模块。</li>
<li>训练后的分散政策独立于LLM，表现优于传统MARL算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03997">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bed5b889b4104802e54211d1cfc87911.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ca7d565303aba07fc11b08b2a883245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d69046fbd8c49633b1a674242c8886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f742dcfcedfba99b94540a2bfcc090c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9c5c61b8f866302bf17ac6a27ca5901.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2097cc03b953e6a60cc7aa86dc447535.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-06-22  Making LLMs Better Many-to-Many Speech-to-Text Translators with   Curriculum Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fce97b4f6def7d65ec1be062205a3d29.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-22  PhantomHunter Detecting Unseen Privately-Tuned LLM-Generated Text via   Family-Aware Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
