<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-ecca4c96afd842b656d17e9af142efbc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="Evolutionary-Caching-to-Accelerate-Your-Off-the-Shelf-Diffusion-Model"><a href="#Evolutionary-Caching-to-Accelerate-Your-Off-the-Shelf-Diffusion-Model" class="headerlink" title="Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"></a>Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model</h2><p><strong>Authors:Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam</strong></p>
<p>Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECADâ€™s learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at <a target="_blank" rel="noopener" href="https://aniaggarwal.github.io/ecad">https://aniaggarwal.github.io/ecad</a> and our code is available at <a target="_blank" rel="noopener" href="https://github.com/aniaggarwal/ecad">https://github.com/aniaggarwal/ecad</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆæ¨¡å‹æ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆå†…å®¹ï¼Œä½†æ¨ç†è¿‡ç¨‹ç¼“æ…¢ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ—©æœŸçš„å·¥ä½œå°è¯•é€šè¿‡ç¼“å­˜å’Œé‡ç”¨æ‰©æ•£å˜å‹å™¨ä¸­çš„ç‰¹å¾æ¥åŠ é€Ÿæ¨ç†æ­¥éª¤ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºåƒµç¡¬çš„å¯å‘å¼æ–¹æ³•ï¼Œå¯¼è‡´åŠ é€Ÿæœ‰é™æˆ–æ¶æ„æ³›åŒ–æ€§å·®ã€‚æˆ‘ä»¬æå‡ºè¿›åŒ–ç¼“å­˜ä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼ˆECADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é—ä¼ ç®—æ³•ï¼Œå®ƒåªä½¿ç”¨ä¸€å°éƒ¨åˆ†æ ¡å‡†æç¤ºï¼Œå°±èƒ½å­¦ä¹ é«˜æ•ˆçš„ã€é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„ç¼“å­˜æ—¶é—´è¡¨ï¼Œå½¢æˆå¸•ç´¯æ‰˜å‰æ²¿ã€‚ECADä¸éœ€è¦å¯¹ç½‘ç»œå‚æ•°æˆ–å‚è€ƒå›¾åƒè¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚å®ƒæä¾›äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œèƒ½å¤Ÿå®ç°å¯¹è´¨é‡-å»¶è¿Ÿæƒè¡¡çš„ç²¾ç»†æ§åˆ¶ï¼Œå¹¶æ— ç¼é€‚åº”ä¸åŒçš„æ‰©æ•£æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒECADå­¦ä¹ çš„æ—¶é—´è¡¨å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ ¡å‡†æœŸé—´æœªè§åˆ°çš„åˆ†è¾¨ç‡å’Œæ¨¡å‹å˜ä½“ã€‚æˆ‘ä»¬åœ¨PixArt-alphaã€PixArt-Sigmaå’ŒFLUX-1.devä¸Šè¯„ä¼°äº†ECADï¼Œä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼ˆFIDã€CLIPã€å›¾åƒå¥–åŠ±ï¼‰åœ¨å¤šç§åŸºå‡†æµ‹è¯•ï¼ˆCOCOã€MJHQ-30kã€PartiPromptsï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨ä¹‹å‰æ–¹æ³•ä¸Šçš„æŒç»­æ”¹è¿›ã€‚åœ¨PixArt-alphaä¸Šï¼ŒECADæ‰¾åˆ°çš„æ—¶é—´è¡¨åœ¨COCO FIDä¸Šä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•4.47ï¼ŒåŒæ—¶å°†æ¨ç†é€Ÿåº¦ä»2.35å€æé«˜åˆ°2.58å€ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†ECADåœ¨åŠ é€Ÿæ‰©æ•£æ¨ç†æ–¹é¢çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ä½äº<a target="_blank" rel="noopener" href="https://aniaggarwal.github.io/ecad%EF%BC%8C%E4%BB%A3%E7%A0%81%E4%BD%8D%E4%BA%8E">https://aniaggarwal.github.io/ecadï¼Œä»£ç ä½äº</a> <a target="_blank" rel="noopener" href="https://github.com/aniaggarwal/ecad%E3%80%82">https://github.com/aniaggarwal/ecadã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15682v1">PDF</a> 29 pages, 22 figures, 9 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒè™½ç„¶è´¨é‡é«˜ï¼Œä½†æ¨ç†è¿‡ç¨‹ç¼“æ…¢ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚ç°æœ‰å·¥ä½œå°è¯•é€šè¿‡ç¼“å­˜å’Œé‡ç”¨æ‰©æ•£è½¬æ¢å™¨ä¸­çš„ç‰¹å¾æ¥åŠ é€Ÿæ¨ç†ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ä¾èµ–åƒµåŒ–çš„å¯å‘å¼æŠ€æœ¯ï¼ŒåŠ é€Ÿæœ‰é™æˆ–é€šç”¨æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºè¿›åŒ–ç¼“å­˜åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼ˆECADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é—ä¼ ç®—æ³•ï¼Œèƒ½å¤Ÿå­¦ä¹ é«˜æ•ˆçš„ã€é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„ç¼“å­˜è°ƒåº¦ï¼Œå½¢æˆå¸•ç´¯æ‰˜å‰æ²¿ï¼Œåªéœ€ä¸€å°éƒ¨åˆ†æ ¡å‡†æç¤ºã€‚ECADæ— éœ€ä¿®æ”¹ç½‘ç»œå‚æ•°æˆ–å‚è€ƒå›¾åƒï¼Œæä¾›äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œèƒ½å¤Ÿç²¾ç»†æ§åˆ¶è´¨é‡å»¶è¿Ÿæƒè¡¡ï¼Œå¹¶æ— ç¼é€‚åº”ä¸åŒçš„æ‰©æ•£æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒECADçš„è°ƒåº¦å¯ä»¥æ¨å¹¿åˆ°æ ¡å‡†æœŸé—´æœªè§åˆ°çš„åˆ†è¾¨ç‡å’Œæ¨¡å‹å˜ä½“ã€‚æˆ‘ä»¬åœ¨PixArt-alphaã€PixArt-Sigmaå’ŒFLUX-1.devä¸Šè¯„ä¼°äº†ECADï¼Œä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼ˆFIDã€CLIPã€å›¾åƒå¥–åŠ±ï¼‰å’Œå¤šç§åŸºå‡†æµ‹è¯•ï¼ˆCOCOã€MJHQ-30kã€PartiPromptsï¼‰ï¼Œæ˜¾ç¤ºå‡ºå¯¹ä»¥å‰æ–¹æ³•çš„ä¸€è‡´æ”¹è¿›ã€‚åœ¨PixArt-alphaä¸Šï¼ŒECADæ‰¾åˆ°äº†ä¸€ç§è°ƒåº¦æ–¹æ³•ï¼Œåœ¨COCO FIDä¸Šä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•4.47ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦ä»2.35å€æé«˜åˆ°2.58å€ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†ECADåœ¨åŠ é€Ÿæ‰©æ•£æ¨ç†æ–¹é¢çš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ä½äº<a target="_blank" rel="noopener" href="https://aniaggarwal.github.io/ecad">https://aniaggarwal.github.io/ecad</a>ï¼Œä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/aniaggarwal/ecad">https://github.com/aniaggarwal/ecad</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒå†…å®¹ï¼Œä½†æ¨ç†è¿‡ç¨‹ç¼“æ…¢ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡ç¼“å­˜å’Œé‡ç”¨ç‰¹å¾æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ECADåˆ©ç”¨é—ä¼ ç®—æ³•å­¦ä¹ é«˜æ•ˆçš„ã€é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„ç¼“å­˜è°ƒåº¦ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
<li>ECADä¸ä¿®æ”¹ç½‘ç»œå‚æ•°æˆ–å‚è€ƒå›¾åƒï¼Œå¹¶æä¾›äº†è´¨é‡å»¶è¿Ÿçš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>ECADèƒ½å¤Ÿæ— ç¼é€‚åº”ä¸åŒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸”å…¶è°ƒåº¦å¯ä»¥æ¨å¹¿åˆ°ä¸åŒçš„åˆ†è¾¨ç‡å’Œæ¨¡å‹å˜ä½“ã€‚</li>
<li>åœ¨å¤šä¸ªè¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒECADè¡¨ç°å‡ºå¯¹ä»¥å‰æ–¹æ³•çš„ä¸€è‡´æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d58169ab8caa0aecc1c35df653c79379.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cd77a36c987ea0b95f6058ce7ac718f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89085995ebb373b6aacae866c488e9f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97ca6b19ac90093eb7f01f79afd17fb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc47032a57f7c5bbff04a21d056ac5ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ab26c71609cdee0d8c9d3eec83fac2e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Control-and-Realism-Best-of-Both-Worlds-in-Layout-to-Image-without-Training"><a href="#Control-and-Realism-Best-of-Both-Worlds-in-Layout-to-Image-without-Training" class="headerlink" title="Control and Realism: Best of Both Worlds in Layout-to-Image without   Training"></a>Control and Realism: Best of Both Worlds in Layout-to-Image without   Training</h2><p><strong>Authors:Bonan Li, Yinhan Hu, Songhua Liu, Xinchao Wang</strong></p>
<p>Layout-to-Image generation aims to create complex scenes with precise control over the placement and arrangement of subjects. Existing works have demonstrated that pre-trained Text-to-Image diffusion models can achieve this goal without training on any specific data; however, they often face challenges with imprecise localization and unrealistic artifacts. Focusing on these drawbacks, we propose a novel training-free method, WinWinLay. At its core, WinWinLay presents two key strategies, Non-local Attention Energy Function and Adaptive Update, that collaboratively enhance control precision and realism. On one hand, we theoretically demonstrate that the commonly used attention energy function introduces inherent spatial distribution biases, hindering objects from being uniformly aligned with layout instructions. To overcome this issue, non-local attention prior is explored to redistribute attention scores, facilitating objects to better conform to the specified spatial conditions. On the other hand, we identify that the vanilla backpropagation update rule can cause deviations from the pre-trained domain, leading to out-of-distribution artifacts. We accordingly introduce a Langevin dynamics-based adaptive update scheme as a remedy that promotes in-domain updating while respecting layout constraints. Extensive experiments demonstrate that WinWinLay excels in controlling element placement and achieving photorealistic visual fidelity, outperforming the current state-of-the-art methods. </p>
<blockquote>
<p>å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ—¨åœ¨åˆ›å»ºå…·æœ‰ç²¾ç¡®æ§åˆ¶ä¸»é¢˜æ”¾ç½®å’Œæ’åˆ—çš„å¤æ‚åœºæ™¯ã€‚ç°æœ‰ä½œå“å·²ç»è¯æ˜ï¼Œé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•ç‰¹å®šæ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°è¿™ä¸€ç›®æ ‡ï¼›ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸é¢ä¸´å®šä½ä¸ç²¾ç¡®å’Œä¸åˆ‡å®é™…çš„è‰ºæœ¯å“ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹æ–¹æ³•WinWinLayã€‚WinWinLayçš„æ ¸å¿ƒåœ¨äºä¸¤ä¸ªå…³é”®ç­–ç•¥ï¼šéå±€éƒ¨æ³¨æ„åŠ›èƒ½é‡å‡½æ•°å’Œè‡ªé€‚åº”æ›´æ–°ï¼Œè¿™ä¸¤ä¸ªç­–ç•¥å…±åŒæé«˜äº†æ§åˆ¶ç²¾åº¦å’Œé€¼çœŸåº¦ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜ï¼Œå¸¸ç”¨çš„æ³¨æ„åŠ›èƒ½é‡å‡½æ•°ä¼šå¼•å…¥å›ºæœ‰çš„ç©ºé—´åˆ†å¸ƒåè§ï¼Œé˜»ç¢ç‰©ä½“ä¸å¸ƒå±€æŒ‡ä»¤çš„ç»Ÿä¸€å¯¹é½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†éå±€éƒ¨æ³¨æ„åŠ›å…ˆéªŒæ¥é‡æ–°åˆ†é…æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¸®åŠ©ç‰©ä½“æ›´å¥½åœ°ç¬¦åˆæŒ‡å®šçš„ç©ºé—´æ¡ä»¶ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å‘ç°ç®€å•çš„åå‘ä¼ æ’­æ›´æ–°è§„åˆ™å¯èƒ½å¯¼è‡´åç¦»é¢„è®­ç»ƒåŸŸï¼Œä»è€Œå¯¼è‡´åˆ†å¸ƒå¤–è‰ºæœ¯å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›¸åº”åœ°å¼•å…¥äº†ä¸€ç§åŸºäºæœ—æ ¼æ–‡åŠ¨åŠ›å­¦çš„è‡ªé€‚åº”æ›´æ–°æ–¹æ¡ˆä½œä¸ºè¡¥æ•‘æªæ–½ï¼Œè¯¥æ–¹æ¡ˆåœ¨å°Šé‡å¸ƒå±€çº¦æŸçš„åŒæ—¶ï¼Œä¿ƒè¿›äº†åŸŸå†…æ›´æ–°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒWinWinLayåœ¨æ§åˆ¶å…ƒç´ æ”¾ç½®å’Œå®ç°é€¼çœŸçš„è§†è§‰ä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†å½“å‰å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15563v1">PDF</a> Accepted by ICML2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•WinWinLayï¼Œç”¨äºæ”¹è¿›Layout-to-Imageç”Ÿæˆä¸­çš„å…ƒç´ æ”¾ç½®æ§åˆ¶å’Œè§†è§‰çœŸå®æ„Ÿã€‚é€šè¿‡Non-local Attention Energy Functionå’ŒAdaptive Updateä¸¤å¤§ç­–ç•¥ï¼ŒWinWinLayå…‹æœäº†ç°æœ‰å·¥ä½œä¸­å­˜åœ¨çš„å®šä½ä¸ç²¾ç¡®å’Œå›¾åƒå¤±çœŸé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šè§£å†³äº†æ³¨æ„åŠ›èƒ½é‡å‡½æ•°çš„ç©ºé—´åˆ†å¸ƒåè§é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦çš„è‡ªé€‚åº”æ›´æ–°æ–¹æ¡ˆï¼Œä»¥ä¿æŒæ›´æ–°åœ¨é¢„è®­ç»ƒåŸŸå†…å¹¶éµå®ˆå¸ƒå±€çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼ŒWinWinLayåœ¨å…ƒç´ æ”¾ç½®æ§åˆ¶å’Œè§†è§‰çœŸå®æ„Ÿæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WinWinLayæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„Layout-to-Imageç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨åˆ›å»ºå…·æœ‰ç²¾ç¡®æ§åˆ¶ä¸»é¢˜çš„å¤æ‚åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¸¤å¤§ç­–ç•¥å®ç°ç›®æ ‡ï¼šNon-local Attention Energy Functionè§£å†³ç©ºé—´åˆ†å¸ƒåè§é—®é¢˜ï¼Œä½¿å¯¹è±¡æ›´ç¬¦åˆå¸ƒå±€æŒ‡ä»¤ï¼›Adaptive Updateè§£å†³å› åå‘ä¼ æ’­æ›´æ–°è§„åˆ™å¯¼è‡´çš„åå·®é—®é¢˜ï¼Œå¼•å…¥åŸºäºæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦çš„è‡ªé€‚åº”æ›´æ–°æ–¹æ¡ˆã€‚</li>
<li>Non-local Attention Energy Functioné€šè¿‡é‡æ–°åˆ†é…æ³¨æ„åŠ›åˆ†æ•°ï¼Œæœ‰åŠ©äºå¯¹è±¡æ›´å¥½åœ°ç¬¦åˆæŒ‡å®šçš„ç©ºé—´æ¡ä»¶ã€‚</li>
<li>Adaptive Updateæ–¹æ¡ˆä¿ƒè¿›åœ¨é¢„è®­ç»ƒåŸŸå†…çš„æ›´æ–°å¹¶éµå®ˆå¸ƒå±€çº¦æŸã€‚</li>
<li>WinWinLayåœ¨å…ƒç´ æ”¾ç½®æ§åˆ¶å’Œè§†è§‰çœŸå®æ„Ÿæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼Œå¹¶è¯æ˜å…¶æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f93b1b0d3349ede893beefdded760ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56322d43adacc48a13f252c98f312d44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac5464494232b1e9b851d4281fa28c04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbd9e50e9ad64961bfdd330a71347f0f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Provable-Maximum-Entropy-Manifold-Exploration-via-Diffusion-Models"><a href="#Provable-Maximum-Entropy-Manifold-Exploration-via-Diffusion-Models" class="headerlink" title="Provable Maximum Entropy Manifold Exploration via Diffusion Models"></a>Provable Maximum Entropy Manifold Exploration via Diffusion Models</h2><p><strong>Authors:Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, Andreas Krause</strong></p>
<p>Exploration is critical for solving real-world decision-making problems such as scientific discovery, where the objective is to generate truly novel designs rather than mimic existing data distributions. In this work, we address the challenge of leveraging the representational power of generative models for exploration without relying on explicit uncertainty quantification. We introduce a novel framework that casts exploration as entropy maximization over the approximate data manifold implicitly defined by a pre-trained diffusion model. Then, we present a novel principle for exploration based on density estimation, a problem well-known to be challenging in practice. To overcome this issue and render this method truly scalable, we leverage a fundamental connection between the entropy of the density induced by a diffusion model and its score function. Building on this, we develop an algorithm based on mirror descent that solves the exploration problem as sequential fine-tuning of a pre-trained diffusion model. We prove its convergence to the optimal exploratory diffusion model under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we empirically evaluate our approach on both synthetic and high-dimensional text-to-image diffusion, demonstrating promising results. </p>
<blockquote>
<p>å¯¹äºè§£å†³ç°å®ä¸–ç•Œä¸­çš„å†³ç­–åˆ¶å®šé—®é¢˜ï¼ˆå¦‚ç§‘å­¦å‘ç°ï¼‰è€Œè¨€ï¼Œæ¢ç´¢è‡³å…³é‡è¦ã€‚è¿™ç±»é—®é¢˜çš„ç›®æ ‡æ˜¯ç”ŸæˆçœŸæ­£æ–°é¢–çš„è®¾è®¡ï¼Œè€Œéæ¨¡ä»¿ç°æœ‰æ•°æ®åˆ†å¸ƒã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›è¿›è¡Œæ¢ç´¢çš„æŒ‘æˆ˜ï¼Œä¸”æ— éœ€ä¾èµ–æ˜ç¡®çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå°†æ¢ç´¢è§†ä¸ºåœ¨ç”±é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹éšå¼å®šä¹‰çš„æ•°æ®æµå½¢ä¸Šè¿›è¡Œçš„ç†µæœ€å¤§åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åŸºäºå¯†åº¦ä¼°è®¡æå‡ºäº†ä¸€ä¸ªç”¨äºæ¢ç´¢çš„æ–°åŸåˆ™ï¼Œè¿™åœ¨å®è·µä¸­æ˜¯ä¸€ä¸ªå…¬è®¤çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜å¹¶ç¡®ä¿æ–¹æ³•çš„çœŸæ­£å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¯±å¯¼çš„å¯†åº¦ç†µä¸å…¶å¾—åˆ†å‡½æ•°ä¹‹é—´çš„åŸºæœ¬è”ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºé•œåƒä¸‹é™çš„ç®—æ³•ï¼Œå°†æ¢ç´¢é—®é¢˜è§†ä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¿ç»­å¾®è°ƒã€‚é€šè¿‡åˆ©ç”¨å¯¹é•œåƒæµçš„æœ€æ–°ç†è§£ï¼Œæˆ‘ä»¬åœ¨å®é™…å‡è®¾ä¸‹è¯æ˜äº†å…¶æ”¶æ•›è‡³æœ€ä½³æ¢ç´¢æ€§æ‰©æ•£æ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨åˆæˆæ•°æ®å’Œé«˜ç»´æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£ä¸Šå®è¯è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è·å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15385v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨è§£å†³ç°å®ä¸–ç•Œå†³ç­–é—®é¢˜ï¼ˆå¦‚ç§‘å­¦å‘ç°ï¼‰æ—¶æ¢ç´¢çš„é‡è¦æ€§ï¼Œæ—¨åœ¨ç”ŸæˆçœŸæ­£æ–°é¢–çš„è®¾è®¡è€Œéæ¨¡ä»¿ç°æœ‰æ•°æ®åˆ†å¸ƒã€‚ä¸ºè§£å†³åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›è¿›è¡Œæ¢ç´¢çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå°†æ¢ç´¢è§†ä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹éšå¼å®šä¹‰çš„æ•°æ®æµå½¢ä¸Šçš„ç†µæœ€å¤§åŒ–ã€‚åŸºäºå¯†åº¦ä¼°è®¡çš„åŸåˆ™ï¼Œæå‡ºä¸€ç§åŸºäºå¯†åº¦è¯±å¯¼æ‰©æ•£æ¨¡å‹çš„ç†µä¸å…¶å¾—åˆ†å‡½æ•°ä¹‹é—´åŸºæœ¬è”ç³»çš„è§£å†³æ–¹æ¡ˆã€‚åˆ©ç”¨é•œåƒä¸‹é™æ³•å¼€å‘ç®—æ³•ï¼Œå°†æ¢ç´¢é—®é¢˜è§†ä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åºåˆ—å¾®è°ƒã€‚æœ€ååœ¨åˆæˆå’Œé«˜ç»´æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ä¸Šè¿›è¡Œå®è¯è¯„ä¼°ï¼Œæ˜¾ç¤ºç»“æœå…·å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢ç´¢åœ¨è§£å†³ç°å®ä¸–ç•Œå†³ç­–é—®é¢˜ä¸­çš„é‡è¦æ€§ï¼Œå¼ºè°ƒç”ŸæˆçœŸæ­£æ–°é¢–è®¾è®¡è€Œéæ¨¡ä»¿ç°æœ‰æ•°æ®åˆ†å¸ƒã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå°†æ¢ç´¢è§†ä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹éšå¼å®šä¹‰çš„æ•°æ®æµå½¢ä¸Šçš„ç†µæœ€å¤§åŒ–ï¼Œè§£å†³åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›è¿›è¡Œæ¢ç´¢çš„æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºå¯†åº¦ä¼°è®¡çš„åŸåˆ™æ¥æ¨åŠ¨æ¢ç´¢ï¼Œå¹¶åˆ©ç”¨å¯†åº¦è¯±å¯¼æ‰©æ•£æ¨¡å‹çš„ç†µä¸å…¶å¾—åˆ†å‡½æ•°ä¹‹é—´çš„åŸºæœ¬è”ç³»æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é•œåƒä¸‹é™æ³•å¼€å‘ç®—æ³•ï¼Œå°†æ¢ç´¢é—®é¢˜è§†ä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åºåˆ—å¾®è°ƒï¼Œä»¥å…‹æœå®è·µä¸­çš„æŒ‘æˆ˜å¹¶å®ç°çœŸæ­£çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>è¯æ˜äº†è¯¥ç®—æ³•åœ¨åˆç†å‡è®¾ä¸‹æ”¶æ•›äºæœ€ä¼˜æ¢ç´¢æ€§æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®å’Œé«˜ç»´æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£é—®é¢˜ä¸Šè¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚</li>
<li>å±•ç¤ºçš„ç»“æœå…·æœ‰å‰æ™¯ï¼Œè¡¨æ˜è¯¥æ¡†æ¶åœ¨è§£å†³ç°å®ä¸–ç•Œå†³ç­–é—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0d795e99bc0df256ce30107e60c976b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-796d1ac377eea57a44d1a95cb54619d1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="When-Model-Knowledge-meets-Diffusion-Model-Diffusion-assisted-Data-free-Image-Synthesis-with-Alignment-of-Domain-and-Class"><a href="#When-Model-Knowledge-meets-Diffusion-Model-Diffusion-assisted-Data-free-Image-Synthesis-with-Alignment-of-Domain-and-Class" class="headerlink" title="When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free   Image Synthesis with Alignment of Domain and Class"></a>When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free   Image Synthesis with Alignment of Domain and Class</h2><p><strong>Authors:Yujin Kim, Hyunsoo Kim, Hyunwoo J. Kim, Suhyun Kim</strong></p>
<p>Open-source pre-trained models hold great potential for diverse applications, but their utility declines when their training data is unavailable. Data-Free Image Synthesis (DFIS) aims to generate images that approximate the learned data distribution of a pre-trained model without accessing the original data. However, existing DFIS meth ods produce samples that deviate from the training data distribution due to the lack of prior knowl edge about natural images. To overcome this limitation, we propose DDIS, the first Diffusion-assisted Data-free Image Synthesis method that leverages a text-to-image diffusion model as a powerful image prior, improving synthetic image quality. DDIS extracts knowledge about the learned distribution from the given model and uses it to guide the diffusion model, enabling the generation of images that accurately align with the training data distribution. To achieve this, we introduce Domain Alignment Guidance (DAG) that aligns the synthetic data domain with the training data domain during the diffusion sampling process. Furthermore, we optimize a single Class Alignment Token (CAT) embedding to effectively capture class-specific attributes in the training dataset. Experiments on PACS and Ima geNet demonstrate that DDIS outperforms prior DFIS methods by generating samples that better reflect the training data distribution, achieving SOTA performance in data-free applications. </p>
<blockquote>
<p>å¼€æºé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šç§åº”ç”¨ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å½“å…¶è®­ç»ƒæ•°æ®æ— æ³•ä½¿ç”¨æˆ–ä¸å¯è®¿é—®æ—¶ï¼Œå…¶æ•ˆç”¨ä¼šé™ä½ã€‚æ•°æ®è‡ªç”±å›¾åƒåˆæˆï¼ˆDFISï¼‰æ—¨åœ¨ç”Ÿæˆèƒ½å¤Ÿè¿‘ä¼¼é¢„è®­ç»ƒæ¨¡å‹å­¦ä¹ åˆ°çš„æ•°æ®åˆ†å¸ƒçš„å›¾åƒï¼Œè€Œæ— éœ€è®¿é—®åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DFISæ–¹æ³•ç”Ÿæˆçš„æ ·æœ¬åç¦»äº†è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œå› ä¸ºç¼ºä¹å…³äºè‡ªç„¶å›¾åƒçš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DDISï¼ˆæ•°æ®è‡ªç”±å›¾åƒåˆæˆçš„æ‰©æ•£è¾…åŠ©æ–¹æ³•ï¼‰ã€‚DDISé¦–æ¬¡åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„å›¾åƒå…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†åˆæˆå›¾åƒçš„è´¨é‡ã€‚DDISä»ç»™å®šçš„æ¨¡å‹ä¸­æå–å…³äºå­¦ä¹ åˆ°çš„åˆ†å¸ƒçš„çŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨å®ƒæ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œä»è€Œç”Ÿæˆå‡†ç¡®å¯¹é½è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„å›¾åƒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸŸå¯¹é½æŒ‡å¯¼ï¼ˆDAGï¼‰ï¼Œåœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­å°†åˆæˆæ•°æ®åŸŸä¸è®­ç»ƒæ•°æ®åŸŸå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–å•ä¸ªç±»åˆ«å¯¹é½ä»¤ç‰Œï¼ˆCATï¼‰åµŒå…¥æ¥æœ‰æ•ˆåœ°æ•è·è®­ç»ƒæ•°æ®é›†ç‰¹å®šçš„ç±»åˆ«å±æ€§ã€‚åœ¨PACSå’ŒImageNetä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDDISä¼˜äºå…ˆå‰çš„DFISæ–¹æ³•ï¼Œç”Ÿæˆçš„æ ·æœ¬æ›´å¥½åœ°åæ˜ äº†è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œåœ¨æ•°æ®è‡ªç”±åº”ç”¨ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15381v1">PDF</a> Published at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é©±åŠ¨çš„å…æ•°æ®å›¾åƒåˆæˆï¼ˆDDISï¼‰æ˜¯ä¸€ç§æ–°å‹çš„å›¾åƒåˆæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒå…ˆéªŒï¼Œæ”¹è¿›äº†ç°æœ‰çš„å…æ•°æ®å›¾åƒåˆæˆï¼ˆDFISï¼‰æ–¹æ³•ã€‚DDISèƒ½å¤Ÿä»ç»™å®šçš„é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–å­¦ä¹ åˆ°çš„åˆ†å¸ƒçŸ¥è¯†ï¼Œå¹¶å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒç²¾ç¡®å¯¹é½çš„å›¾åƒã€‚é€šè¿‡å¼•å…¥åŸŸå¯¹é½å¼•å¯¼ï¼ˆDAGï¼‰å’Œç±»åˆ«å¯¹é½ä»¤ç‰Œï¼ˆCATï¼‰åµŒå…¥ä¼˜åŒ–ï¼ŒDDISåœ¨PACSå’ŒImageNetä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨æ— æ•°æ®åº”ç”¨ä¸­ç”Ÿæˆæ ·æœ¬çš„æ€§èƒ½è¶…è¿‡äº†å…ˆå‰çš„DFISæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é©±åŠ¨çš„å…æ•°æ®å›¾åƒåˆæˆï¼ˆDDISï¼‰æ˜¯ä¸€ç§æ–°çš„å›¾åƒåˆæˆæ–¹æ³•ã€‚</li>
<li>DDISåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒå…ˆéªŒï¼Œæ”¹è¿›äº†ç°æœ‰çš„å…æ•°æ®å›¾åƒåˆæˆï¼ˆDFISï¼‰æ–¹æ³•ã€‚</li>
<li>DDISèƒ½å¤Ÿä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–å­¦ä¹ åˆ°çš„åˆ†å¸ƒçŸ¥è¯†ï¼Œå¹¶æ®æ­¤ç”Ÿæˆå›¾åƒã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸŸå¯¹é½å¼•å¯¼ï¼ˆDAGï¼‰ï¼ŒDDISèƒ½å¤Ÿåœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­å°†åˆæˆæ•°æ®åŸŸä¸è®­ç»ƒæ•°æ®åŸŸå¯¹é½ã€‚</li>
<li>DDISé€šè¿‡ä¼˜åŒ–ç±»åˆ«å¯¹é½ä»¤ç‰Œï¼ˆCATï¼‰åµŒå…¥æ¥æœ‰æ•ˆæ•æ‰è®­ç»ƒæ•°æ®é›†ä¸­çš„ç±»åˆ«ç‰¹å®šå±æ€§ã€‚</li>
<li>åœ¨PACSå’ŒImageNetä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDDISåœ¨å…æ•°æ®åº”ç”¨ä¸­ç”Ÿæˆæ ·æœ¬çš„æ€§èƒ½è¶…è¿‡äº†å…ˆå‰çš„DFISæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77bcfe927cd4e2440cd8f8b602706189.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dbb72bc9fb50d1ef19879ba6898a343.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3564882f1e5b42cabf50bebad56f8a9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1372ebc933836b96bd7101d48090e3c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Acoustic-Waveform-Inversion-with-Image-to-Image-Schrodinger-Bridges"><a href="#Acoustic-Waveform-Inversion-with-Image-to-Image-Schrodinger-Bridges" class="headerlink" title="Acoustic Waveform Inversion with Image-to-Image SchrÃ¶dinger Bridges"></a>Acoustic Waveform Inversion with Image-to-Image SchrÃ¶dinger Bridges</h2><p><strong>Authors:A. S. Stankevich, I. B. Petrov</strong></p>
<p>Recent developments in application of deep learning models to acoustic Full Waveform Inversion (FWI) are marked by the use of diffusion models as prior distributions for Bayesian-like inference procedures. The advantage of these methods is the ability to generate high-resolution samples, which are otherwise unattainable with classical inversion methods or other deep learning-based solutions. However, the iterative and stochastic nature of sampling from diffusion models along with heuristic nature of output control remain limiting factors for their applicability. For instance, an optimal way to include the approximate velocity model into diffusion-based inversion scheme remains unclear, even though it is considered an essential part of FWI pipeline. We address the issue by employing a Schr&quot;odinger Bridge that interpolates between the distributions of ground truth and smoothed velocity models. To facilitate the learning of nonlinear drifts that transfer samples between distributions we extend the concept of Image-to-Image Schr&quot;odinger Bridge ($\text{I}^2\text{SB}$) to conditional sampling, resulting in a conditional Image-to-Image Schr&quot;odinger Bridge (c$\text{I}^2\text{SB}$) framework. To validate our method, we assess its effectiveness in reconstructing the reference velocity model from its smoothed approximation, coupled with the observed seismic signal of fixed shape. Our experiments demonstrate that the proposed solution outperforms our reimplementation of conditional diffusion model suggested in earlier works, while requiring only a few neural function evaluations (NFEs) to achieve sample fidelity superior to that attained with supervised learning-based approach. The supplementary code implementing the algorithms described in this paper can be found in the repository <a target="_blank" rel="noopener" href="https://github.com/stankevich-mipt/seismic_inversion_via_I2SB">https://github.com/stankevich-mipt/seismic_inversion_via_I2SB</a>. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å£°æ³¢å…¨æ³¢å½¢åæ¼”ï¼ˆFWIï¼‰ä¸­çš„åº”ç”¨å‘å±•ï¼Œä»¥æ‰©æ•£æ¨¡å‹ä½œä¸ºè´å¶æ–¯ç±»æ¨æ–­è¿‡ç¨‹çš„å…ˆéªŒåˆ†å¸ƒä¸ºæ ‡å¿—ã€‚è¿™äº›æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡æ ·æœ¬ï¼Œè¿™æ˜¯ç»å…¸åæ¼”æ–¹æ³•æˆ–å…¶ä»–åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ— æ³•è¾¾åˆ°çš„ã€‚ç„¶è€Œï¼Œä»æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·çš„è¿­ä»£å’Œéšæœºæ€§è´¨ï¼Œä»¥åŠè¾“å‡ºæ§åˆ¶çš„å¯å‘å¼æ€§è´¨ï¼Œä»ç„¶æ˜¯å…¶é€‚ç”¨æ€§çš„é™åˆ¶å› ç´ ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶å°†è¿‘ä¼¼é€Ÿåº¦æ¨¡å‹çº³å…¥åŸºäºæ‰©æ•£çš„åæ¼”æ–¹æ¡ˆè¢«è®¤ä¸ºæ˜¯FWIæµç¨‹ä¸­çš„åŸºæœ¬éƒ¨åˆ†ï¼Œä½†å¦‚ä½•å°†å…¶çº³å…¥ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨è–›å®šè°”æ¡¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡¥åœ¨çœŸå®åˆ†å¸ƒå’Œå¹³æ»‘é€Ÿåº¦æ¨¡å‹åˆ†å¸ƒä¹‹é—´è¿›è¡Œæ’å€¼ã€‚ä¸ºäº†å­¦ä¹ åœ¨åˆ†å¸ƒä¹‹é—´è½¬ç§»æ ·æœ¬çš„éçº¿æ€§æ¼‚ç§»ï¼Œæˆ‘ä»¬å°†å›¾åƒåˆ°å›¾åƒè–›å®šè°”æ¡¥ï¼ˆI^2SBï¼‰çš„æ¦‚å¿µæ‰©å±•åˆ°æ¡ä»¶é‡‡æ ·ï¼Œä»è€Œå¾—åˆ°æ¡ä»¶å›¾åƒåˆ°å›¾åƒè–›å®šè°”æ¡¥ï¼ˆcI^2SBï¼‰æ¡†æ¶ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¯„ä¼°å…¶åœ¨ä»å¹³æ»‘è¿‘ä¼¼é‡æ„å‚è€ƒé€Ÿåº¦æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ç»“åˆå›ºå®šçš„è§‚æµ‹åœ°éœ‡ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆä¼˜äºæˆ‘ä»¬æ—©æœŸå·¥ä½œä¸­å»ºè®®çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„é‡æ–°å®ç°ï¼Œå¹¶ä¸”ä»…éœ€è¦å°‘é‡çš„ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰å³å¯å®ç°ä¼˜äºåŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•çš„æ ·æœ¬ä¿çœŸåº¦ã€‚å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/stankevich-mipt/seismic_inversion_via_I2SB%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%BE%E5%88%B0%E5%AE%9E%E7%8E%B0%E6%9C%AC%E6%96%87%E4%B8%AD%E6%8F%8F%E8%BF%B0%E7%9A%84%E7%AE%97%E6%B3%95%E7%9A%84%E8%A1%A5%E5%85%85%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/stankevich-mipt/seismic_inversion_via_I2SBä»“åº“ä¸­æ‰¾åˆ°å®ç°æœ¬æ–‡ä¸­æè¿°çš„ç®—æ³•çš„è¡¥å……ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15346v1">PDF</a> Submitted to â€œComputational Mathematics And Mathematical Physicsâ€,   ISSN 1555-6662, issue 8, August 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å£°æ³¢å…¨æ³¢å½¢åæ¼”ï¼ˆFWIï¼‰ä¸­çš„åº”ç”¨æœ€æ–°è¿›å±•è¡¨ç°ä¸ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºè´å¶æ–¯ç±»æ¨æ–­è¿‡ç¨‹çš„å…ˆéªŒåˆ†å¸ƒã€‚è¿™äº›æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡æ ·æœ¬ï¼Œè¿™æ˜¯ç»å…¸åæ¼”æ–¹æ³•æˆ–å…¶ä»–æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆæ— æ³•è¾¾åˆ°çš„ã€‚ç„¶è€Œï¼Œä»æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·å…·æœ‰è¿­ä»£æ€§å’Œéšæœºæ€§ï¼Œä»¥åŠè¾“å‡ºæ§åˆ¶çš„å¯å‘å¼æ€§è´¨ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚ä¾‹å¦‚ï¼Œå°†è¿‘ä¼¼é€Ÿåº¦æ¨¡å‹çº³å…¥åŸºäºæ‰©æ•£çš„åæ¼”æ–¹æ¡ˆçš„æœ€ä½³æ–¹å¼ä»ä¸æ˜ç¡®ï¼Œå°½ç®¡å®ƒè¢«è®¤ä¸ºæ˜¯FWIç®¡é“çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨è–›å®šè°”æ¡¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡¥åœ¨çœŸå®åˆ†å¸ƒå’Œå¹³æ»‘é€Ÿåº¦æ¨¡å‹åˆ†å¸ƒä¹‹é—´è¿›è¡Œæ’å€¼ã€‚ä¸ºäº†å­¦ä¹ åœ¨åˆ†å¸ƒä¹‹é—´è½¬ç§»æ ·æœ¬çš„éçº¿æ€§æ¼‚ç§»ï¼Œæˆ‘ä»¬å°†å›¾åƒåˆ°å›¾åƒçš„è–›å®šè°”æ¡¥ï¼ˆI^2SBï¼‰æ¦‚å¿µæ‰©å±•åˆ°æ¡ä»¶é‡‡æ ·ï¼Œå½¢æˆæ¡ä»¶å›¾åƒåˆ°å›¾åƒçš„è–›å®šè°”æ¡¥ï¼ˆcI^2SBï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬é€šè¿‡ä»å…¶å¹³æ»‘è¿‘ä¼¼é‡å»ºå‚è€ƒé€Ÿåº¦æ¨¡å‹ï¼Œå¹¶ç»“åˆå›ºå®šçš„åœ°éœ‡ä¿¡å·å½¢çŠ¶æ¥è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæˆ‘ä»¬æ—©æœŸå·¥ä½œä¸­æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„é‡æ–°å®ç°ï¼Œå¹¶ä¸”åªéœ€è¦å¾ˆå°‘çš„ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰å°±èƒ½è¾¾åˆ°ä¼˜äºç›‘ç£å­¦ä¹ æ–¹æ³•çš„æ ·æœ¬ä¿çœŸåº¦ã€‚ç›¸å…³ç®—æ³•å®ç°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stankevich-mipt/seismic_inversion_via_I2SB">https://github.com/stankevich-mipt/seismic_inversion_via_I2SB</a> ä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¢«åº”ç”¨äºå£°æ³¢å…¨æ³¢å½¢åæ¼”ä¸­ï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡æ ·æœ¬ï¼Œè¿™æ˜¯ä¼ ç»Ÿæ–¹æ³•æ— æ³•å®ç°çš„ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·å…·æœ‰è¿­ä»£å’Œéšæœºæ€§è´¨ï¼Œä»¥åŠè¾“å‡ºæ§åˆ¶çš„å¯å‘å¼æ€§è´¨ï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>å¼•å…¥è–›å®šè°”æ¡¥æ¥è§£å†³å°†è¿‘ä¼¼é€Ÿåº¦æ¨¡å‹çº³å…¥åŸºäºæ‰©æ•£çš„åæ¼”æ–¹æ¡ˆçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¯¹çœŸå®åˆ†å¸ƒå’Œå¹³æ»‘é€Ÿåº¦æ¨¡å‹åˆ†å¸ƒè¿›è¡Œæ’å€¼çš„è–›å®šè°”æ¡¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>æ‰©å±•å›¾åƒåˆ°å›¾åƒçš„è–›å®šè°”æ¡¥æ¦‚å¿µï¼Œå½¢æˆæ¡ä»¶å›¾åƒåˆ°å›¾åƒçš„è–›å®šè°”æ¡¥æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ åœ¨åˆ†å¸ƒé—´è½¬ç§»æ ·æœ¬çš„éçº¿æ€§æ¼‚ç§»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•ä¸æ—©æœŸæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å®ç°ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ï¼Œèƒ½ä»¥è¾ƒå°‘çš„ç¥ç»åŠŸèƒ½è¯„ä¼°æ¬¡æ•°è¾¾åˆ°è¾ƒé«˜çš„æ ·æœ¬ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9315a92227ba4da4b8621c05b9f8e16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9b46dc7010c539ae5ee2bcb083a1fcb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="One-shot-Face-Sketch-Synthesis-in-the-Wild-via-Generative-Diffusion-Prior-and-Instruction-Tuning"><a href="#One-shot-Face-Sketch-Synthesis-in-the-Wild-via-Generative-Diffusion-Prior-and-Instruction-Tuning" class="headerlink" title="One-shot Face Sketch Synthesis in the Wild via Generative Diffusion   Prior and Instruction Tuning"></a>One-shot Face Sketch Synthesis in the Wild via Generative Diffusion   Prior and Instruction Tuning</h2><p><strong>Authors:Han Wu, Junyao Li, Kangbo Zhao, Sen Zhang, Yukai Shi, Liang Lin</strong></p>
<p>Face sketch synthesis is a technique aimed at converting face photos into sketches. Existing face sketch synthesis research mainly relies on training with numerous photo-sketch sample pairs from existing datasets. However, these large-scale discriminative learning methods will have to face problems such as data scarcity and high human labor costs. Once the training data becomes scarce, their generative performance significantly degrades. In this paper, we propose a one-shot face sketch synthesis method based on diffusion models. We optimize text instructions on a diffusion model using face photo-sketch image pairs. Then, the instructions derived through gradient-based optimization are used for inference. To simulate real-world scenarios more accurately and evaluate method effectiveness more comprehensively, we introduce a new benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark consists of 400 pairs of face photo-sketch images, including sketches with different styles and photos with different backgrounds, ages, sexes, expressions, illumination, etc. For a solid out-of-distribution evaluation, we select only one pair of images for training at each time, with the rest used for inference. Extensive experiments demonstrate that the proposed method can convert various photos into realistic and highly consistent sketches in a one-shot context. Compared to other methods, our approach offers greater convenience and broader applicability. The dataset will be available at: <a target="_blank" rel="noopener" href="https://github.com/HanWu3125/OS-Sketch">https://github.com/HanWu3125/OS-Sketch</a> </p>
<blockquote>
<p>é¢éƒ¨ç´ æåˆæˆæ˜¯ä¸€ç§å°†é¢éƒ¨ç…§ç‰‡è½¬æ¢ä¸ºç´ æçš„æŠ€æœ¯ã€‚ç°æœ‰çš„é¢éƒ¨ç´ æåˆæˆç ”ç©¶ä¸»è¦ä¾èµ–äºä½¿ç”¨ç°æœ‰æ•°æ®é›†ä¸­çš„å¤§é‡ç…§ç‰‡-ç´ ææ ·æœ¬å¯¹è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™äº›å¤§è§„æ¨¡åˆ¤åˆ«å­¦ä¹ æ–¹æ³•å°†ä¸å¾—ä¸é¢å¯¹æ•°æ®ç¨€ç¼ºå’Œé«˜æ˜‚çš„äººåŠ›æˆæœ¬ç­‰é—®é¢˜ã€‚ä¸€æ—¦è®­ç»ƒæ•°æ®å˜å¾—ç¨€ç¼ºï¼Œå®ƒä»¬çš„ç”Ÿæˆæ€§èƒ½å°±ä¼šæ˜¾è‘—ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å•é•œå¤´é¢éƒ¨ç´ æåˆæˆæ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨é¢éƒ¨ç…§ç‰‡-ç´ æå›¾åƒå¯¹ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸Šçš„æ–‡æœ¬æŒ‡ä»¤ã€‚ç„¶åï¼Œé€šè¿‡åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å¾—åˆ°çš„æŒ‡ä»¤ç”¨äºæ¨ç†ã€‚ä¸ºäº†æ›´å‡†ç¡®åœ°æ¨¡æ‹ŸçœŸå®åœºæ™¯å¹¶æ›´å…¨é¢åœ°è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºOne-shoté¢éƒ¨ç´ ææ•°æ®é›†ï¼ˆOS-Sketchï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•ç”±400å¯¹é¢éƒ¨ç…§ç‰‡-ç´ æå›¾åƒç»„æˆï¼ŒåŒ…æ‹¬ä¸åŒé£æ ¼çš„ç´ æå’Œä¸åŒèƒŒæ™¯ã€å¹´é¾„ã€æ€§åˆ«ã€è¡¨æƒ…ã€ç…§æ˜ç­‰çš„ç…§ç‰‡ã€‚ä¸ºäº†è¿›è¡Œåšå®çš„ç¦»åˆ†å¸ƒè¯„ä¼°ï¼Œæˆ‘ä»¬æ¯æ¬¡ä»…é€‰æ‹©ä¸€å¯¹å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå…¶ä½™å›¾åƒç”¨äºæ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥åœ¨å•é•œå¤´æƒ…å†µä¸‹å°†å„ç§ç…§ç‰‡è½¬æ¢ä¸ºé€¼çœŸä¸”é«˜åº¦ä¸€è‡´çš„ç´ æã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æ›´å¤§çš„ä¾¿åˆ©æ€§å’Œæ›´å¹¿æ³›çš„åº”ç”¨æ€§ã€‚æ•°æ®é›†å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/HanWu3125/OS-Sketch">https://github.com/HanWu3125/OS-Sketch</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15312v1">PDF</a> We propose a novel framework for face sketch synthesis, where merely   a single pair of samples suffices to enable in-the-wild face sketch synthesis</p>
<p><strong>æ‘˜è¦</strong><br>äººè„¸è¯†åˆ«æŠ€æœ¯çš„æ–°è¿›å±•ï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸€æ¬¡æ€§é¢éƒ¨ç´ æåˆæˆæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–å›¾åƒå¯¹å’Œæ–‡å­—æŒ‡ä»¤è¿›è¡Œè®­ç»ƒï¼Œå¯åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸‹å®ç°ç²¾å‡†æ¨¡æ‹Ÿä¸å…¨é¢è¯„ä¼°ã€‚è¯¥ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªåä¸ºOS-Sketchçš„æ–°åŸºå‡†æ•°æ®é›†ï¼Œå®ç°äº†å¤šæ ·åŒ–çš„é¢éƒ¨ç…§ç‰‡å‘çœŸå®ç´ æçš„è½¬æ¢ã€‚ç›¸æ¯”å…¶ä»–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ›´ä¾¿æ·ã€é€‚ç”¨æ€§æ›´å¹¿ã€‚æ•°æ®é›†å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸€æ¬¡æ€§é¢éƒ¨ç´ æåˆæˆæ–¹æ³•ï¼Œé€‚ç”¨äºä¸åŒé£æ ¼çš„ç´ æå’ŒèƒŒæ™¯ã€å¹´é¾„ã€æ€§åˆ«ã€è¡¨æƒ…ç­‰å¤šæ ·åŒ–çš„é¢éƒ¨ç…§ç‰‡ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æŒ‡ä»¤åœ¨æ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ¥ç”ŸæˆæŒ‡ä»¤è¿›è¡Œæ¨æ–­ã€‚</li>
<li>ä¸ºäº†æ›´å‡†ç¡®åœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¯å¢ƒå¹¶æ›´å…¨é¢åœ°è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†OS-Sketchã€‚</li>
<li>OS-Sketchæ•°æ®é›†åŒ…å«400å¯¹é¢éƒ¨ç…§ç‰‡å’Œç´ æå›¾åƒï¼Œç”¨äºå®éªŒéªŒè¯æ–¹æ³•çš„å¯è¡Œæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¸€æ¬¡è®­ç»ƒå¯¹ä¸€ç»„å›¾åƒè¿›è¡Œæ“ä½œï¼Œå…¶ä½™å›¾åƒç”¨äºæ¨æ–­ï¼Œå±•ç¤ºäº†å…¶åœ¨éåˆ†å¸ƒç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„é«˜åº¦ä¸€è‡´çš„ç´ æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8869fcb700c206e335e3a0d1c6b5024a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2d12d05883b974ef0423e6b74b15c8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c49f117df0ff524189a44c728401426.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c928088635639a2262e1e304d12d7b6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DM-FNet-Unified-multimodal-medical-image-fusion-via-diffusion-process-trained-encoder-decoder"><a href="#DM-FNet-Unified-multimodal-medical-image-fusion-via-diffusion-process-trained-encoder-decoder" class="headerlink" title="DM-FNet: Unified multimodal medical image fusion via diffusion   process-trained encoder-decoder"></a>DM-FNet: Unified multimodal medical image fusion via diffusion   process-trained encoder-decoder</h2><p><strong>Authors:Dan He, Weisheng Li, Guofen Wang, Yuping Huang, Shiqiang Liu</strong></p>
<p>Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the modelâ€™s feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused imageâ€™s brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HeDan-11/DM-FNet">https://github.com/HeDan-11/DM-FNet</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰ä»å¤šä¸ªæºå›¾åƒä¸­æå–æœ€æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œä½¿è¯Šæ–­æ›´å…¨é¢å’Œå‡†ç¡®ã€‚å®ç°é«˜è´¨é‡çš„èåˆç»“æœéœ€è¦åœ¨äº®åº¦ã€é¢œè‰²ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚ä¹‹é—´å–å¾—è°¨æ…çš„å¹³è¡¡ï¼›è¿™ç¡®ä¿äº†èåˆå›¾åƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ˜¾ç¤ºç›¸å…³çš„è§£å‰–ç»“æ„å¹¶åæ˜ ç»„ç»‡çš„åŠŸèƒ½çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MMIFæ–¹æ³•åœ¨å¸¸è§„è®­ç»ƒè¿‡ç¨‹ä¸­æ•è·è¯¦ç»†ç‰¹å¾çš„èƒ½åŠ›æœ‰é™ï¼Œå¹¶ä¸”å­˜åœ¨è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´èåˆå›¾åƒè´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹çš„èåˆç½‘ç»œï¼ˆDM-FNetï¼‰æ¥å®ç°ç»Ÿä¸€çš„MMIFã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡æ‰©æ•£è¿‡ç¨‹è®­ç»ƒUNetè¿›è¡Œå›¾åƒé‡å»ºã€‚UNeté€šè¿‡æ¸è¿›çš„å»å™ªæ•è·è¯¦ç»†ä¿¡æ¯å¹¶ä»£è¡¨å¤šçº§æ•°æ®ï¼Œä¸ºéšåçš„èåˆç½‘ç»œæä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†ä¸åŒæ­¥éª¤çš„å™ªå£°å›¾åƒè¾“å…¥èåˆç½‘ç»œï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç‰¹å¾è¯†åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†ä¸‰ä¸ªå…³é”®èåˆæ¨¡å—ï¼Œä»¥è‡ªé€‚åº”åœ°å¤„ç†æ¥è‡ªä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒã€‚æœ€ç»ˆï¼Œé€šè¿‡æ•´åˆç¨³å¥çš„ç½‘ç»œç»“æ„å’Œæ··åˆæŸå¤±å‡½æ•°ï¼Œåè°ƒèåˆå›¾åƒçš„äº®åº¦ã€é¢œè‰²ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚ï¼Œä»è€Œæé«˜å…¶è´¨é‡å’Œä¿¡æ¯å¯†åº¦ã€‚åœ¨å¤šç§åŒ»å­¦å›¾åƒç±»å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚èåˆå›¾åƒä¿æŒäº†é€‚å½“çš„äº®åº¦ã€å…¨é¢çš„æ”¾å°„æ€§ç¤ºè¸ªå‰‚åˆ†å¸ƒã€ä¸°å¯Œçš„çº¹ç†å’Œæ¸…æ™°çš„è¾¹ç¼˜ã€‚ä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/HeDan-11/DM-FNet%E3%80%82">https://github.com/HeDan-11/DM-FNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15218v1">PDF</a> This paper has been accepted by IEEE Transactions on Multimedia (TMM)   in March 2025</p>
<p><strong>Summary</strong><br>     åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸¤é˜¶æ®µèåˆç½‘ç»œï¼ˆDM-FNetï¼‰å®ç°å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰ï¼Œæå‡è¯Šæ–­çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç½‘ç»œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹è®­ç»ƒUNetè¿›è¡Œå›¾åƒé‡å»ºï¼Œæ•è·è¯¦ç»†ä¿¡æ¯å¹¶å‘ˆç°å¤šçº§æ•°æ®ï¼Œä¸ºåç»­çš„èåˆç½‘ç»œæä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚ç¬¬äºŒé˜¶æ®µè¾“å…¥ä¸åŒæ­¥éª¤çš„å™ªå£°å›¾åƒï¼Œå¢å¼ºæ¨¡å‹çš„ç‰¹å¾è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶é›†æˆä¸‰ä¸ªå…³é”®èåˆæ¨¡å—ä»¥è‡ªé€‚åº”å¤„ç†ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒã€‚æœ€ç»ˆï¼Œé€šè¿‡æ•´åˆç½‘ç»œç»“æ„å’Œæ··åˆæŸå¤±å‡½æ•°ï¼Œåè°ƒèåˆå›¾åƒçš„äº®åº¦ã€è‰²å½©ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚ï¼Œæå‡å›¾åƒè´¨é‡å’Œä¿¡æ¯å¯†åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å®¢è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèåˆå›¾åƒä¿æŒé€‚å½“çš„äº®åº¦ã€å…¨é¢çš„æ”¾å°„æ€§ç¤ºè¸ªç‰©åˆ†å¸ƒã€ä¸°å¯Œçš„çº¹ç†å’Œæ¸…æ™°çš„è¾¹ç¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰èƒ½å¤Ÿæå–å¤šä¸ªæºå›¾åƒä¸­æœ€æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œä¸ºåŒ»ç”Ÿæä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„è¯Šæ–­ä¾æ®ã€‚</li>
<li>ç°æœ‰MMIFæ–¹æ³•åœ¨å¸¸è§„è®­ç»ƒä¸­å­˜åœ¨æ•è·è¯¦ç»†ç‰¹å¾çš„èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œä¸”è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ä¸è¶³ï¼Œå¯¼è‡´èåˆå›¾åƒè´¨é‡ä¸ä½³ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹èåˆç½‘ç»œï¼ˆDM-FNetï¼‰é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€å›¾åƒèåˆã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹è®­ç»ƒUNetè¿›è¡Œå›¾åƒé‡å»ºï¼Œæä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¢å¼ºæ¨¡å‹çš„ç‰¹å¾è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶é›†æˆä¸‰ä¸ªå…³é”®èåˆæ¨¡å—ä»¥å¤„ç†ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒã€‚</li>
<li>è¯¥ç½‘ç»œé€šè¿‡åè°ƒèåˆå›¾åƒçš„äº®åº¦ã€è‰²å½©ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚ï¼Œæå‡å›¾åƒè´¨é‡å’Œä¿¡æ¯å¯†åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å®¢è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèåˆå›¾åƒå…·æœ‰é€‚å½“çš„äº®åº¦ã€å…¨é¢çš„æ”¾å°„æ€§ç¤ºè¸ªç‰©åˆ†å¸ƒã€ä¸°å¯Œçš„çº¹ç†å’Œæ¸…æ™°çš„è¾¹ç¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f74d70bc1b2d739361fe08eb85efe5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39df3c3ec68df88464dcea4a6aa44ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-711d5d23c108bb0b76cdd05d6ed68e26.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Echo-DND-A-dual-noise-diffusion-model-for-robust-and-precise-left-ventricle-segmentation-in-echocardiography"><a href="#Echo-DND-A-dual-noise-diffusion-model-for-robust-and-precise-left-ventricle-segmentation-in-echocardiography" class="headerlink" title="Echo-DND: A dual noise diffusion model for robust and precise left   ventricle segmentation in echocardiography"></a>Echo-DND: A dual noise diffusion model for robust and precise left   ventricle segmentation in echocardiography</h2><p><strong>Authors:Abdur Rahman, Keerthiveena Balraj, Manojkumar Ramteke, Anurag Singh Rathore</strong></p>
<p>Recent advancements in diffusion probabilistic models (DPMs) have revolutionized image processing, demonstrating significant potential in medical applications. Accurate segmentation of the left ventricle (LV) in echocardiograms is crucial for diagnostic procedures and necessary treatments. However, ultrasound images are notoriously noisy with low contrast and ambiguous LV boundaries, thereby complicating the segmentation process. To address these challenges, this paper introduces Echo-DND, a novel dual-noise diffusion model specifically designed for this task. Echo-DND leverages a unique combination of Gaussian and Bernoulli noises. It also incorporates a multi-scale fusion conditioning module to improve segmentation precision. Furthermore, it utilizes spatial coherence calibration to maintain spatial integrity in segmentation masks. The modelâ€™s performance was rigorously validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations demonstrate that the proposed framework outperforms existing SOTA models. It achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively. The proposed Echo-DND model establishes a new standard in echocardiogram segmentation, and its architecture holds promise for broader applicability in other medical imaging tasks, potentially improving diagnostic accuracy across various medical domains. Project page: <a target="_blank" rel="noopener" href="https://abdur75648.github.io/Echo-DND">https://abdur75648.github.io/Echo-DND</a> </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å›¾åƒå¤„ç†é¢†åŸŸå¼•èµ·äº†é©å‘½æ€§çš„å˜é©ï¼Œå¹¶åœ¨åŒ»ç–—åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å·¦å¿ƒå®¤ï¼ˆLVï¼‰åœ¨å¿ƒè¶…å›¾ä¸Šçš„ç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ç¨‹åºå’Œå¿…è¦æ²»ç–—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¶…å£°å›¾åƒä»¥å™ªå£°å¤šã€å¯¹æ¯”åº¦ä½å’Œå·¦å¿ƒå®¤è¾¹ç•Œæ¨¡ç³Šè€Œè‘—ç§°ï¼Œä½¿å¾—åˆ†å‰²è¿‡ç¨‹å¤æ‚åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†Echo-DNDï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºæ­¤ä»»åŠ¡çš„æ–°å‹åŒå™ªå£°æ‰©æ•£æ¨¡å‹ã€‚Echo-DNDç»“åˆäº†é«˜æ–¯å™ªå£°å’Œä¼¯åŠªåˆ©å™ªå£°çš„ç‹¬ç‰¹ç»„åˆã€‚å®ƒè¿˜é‡‡ç”¨å¤šå°ºåº¦èåˆæ¡ä»¶æ¨¡å—æ¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç©ºé—´ä¸€è‡´æ€§æ ¡å‡†æ¥ä¿æŒåˆ†å‰²æ©è†œçš„ç©ºé—´å®Œæ•´æ€§ã€‚è¯¥æ¨¡å‹åœ¨CAMUSå’ŒEchoNet-Dynamicæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚å®ƒåœ¨è¿™äº›æ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº†é«˜è¾¾0.962å’Œ0.939çš„Diceå¾—åˆ†ã€‚æ‰€æå‡ºçš„Echo-DNDæ¨¡å‹åœ¨å¿ƒè¶…å›¾åˆ†å‰²æ–¹é¢æ ‘ç«‹äº†æ–°æ ‡å‡†ï¼Œå…¶æ¶æ„åœ¨å…¶ä»–åŒ»å­¦å½±åƒä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›åœ¨å¤šä¸ªåŒ»ç–—é¢†åŸŸæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://abdur75648.github.io/Echo-DND">https://abdur75648.github.io/Echo-DND</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15166v1">PDF</a> Version of record published in Discover Applied Sciences (Springer   Nature). The definitive article is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/s42452-025-07055-5">https://doi.org/10.1007/s42452-025-07055-5</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å›¾åƒå¤„ç†é¢†åŸŸå¼•èµ·äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œå¹¶åœ¨åŒ»ç–—åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹è¶…å£°å›¾åƒå™ªå£°å¤§ã€å¯¹æ¯”åº¦ä½ä»¥åŠå·¦å¿ƒå®¤ï¼ˆLVï¼‰è¾¹ç•Œæ¨¡ç³Šç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEcho-DNDçš„æ–°å‹åŒå™ªå£°æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†é«˜æ–¯å™ªå£°å’Œä¼¯åŠªåˆ©å™ªå£°ï¼Œå¹¶èå…¥äº†å¤šå°ºåº¦èåˆæ¡ä»¶æ¨¡å—ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨ç©ºé—´ä¸€è‡´æ€§æ ¡å‡†æ¥ä¿æŒåˆ†å‰²æ©è†œçš„ç©ºé—´å®Œæ•´æ€§ã€‚åœ¨CAMUSå’ŒEchoNet-Dynamicæ•°æ®é›†ä¸Šçš„ä¸¥æ ¼éªŒè¯è¡¨æ˜ï¼ŒEcho-DNDæ¨¡å‹æ€§èƒ½å“è¶Šï¼Œè¾¾åˆ°äº†é«˜Diceåˆ†æ•°ï¼Œè¶…è¿‡äº†ç°æœ‰æœ€ä½³æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ºå¿ƒç”µå›¾åˆ†å‰²æ ‘ç«‹äº†æ–°æ ‡å‡†ï¼Œå…¶æ¶æ„åœ¨å…¶ä»–åŒ»å­¦æˆåƒä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›æé«˜ä¸åŒåŒ»å­¦é¢†åŸŸçš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰åœ¨å›¾åƒå¤„ç†é¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸ºåŒ»ç–—åº”ç”¨æä¾›äº†æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>Echo-DNDæ¨¡å‹æ˜¯ä¸€ç§æ–°å‹åŒå™ªå£°æ‰©æ•£æ¨¡å‹ï¼Œä¸“ä¸ºå¤„ç†è¶…å£°å›¾åƒè€Œè®¾è®¡ã€‚</li>
<li>Echo-DNDæ¨¡å‹ç»“åˆäº†é«˜æ–¯å™ªå£°å’Œä¼¯åŠªåˆ©å™ªå£°ä»¥æé«˜åˆ†å‰²æ•ˆæœã€‚</li>
<li>å¤šå°ºåº¦èåˆæ¡ä»¶æ¨¡å—å’Œç©ºé—´ä¸€è‡´æ€§æ ¡å‡†æŠ€æœ¯è¢«èå…¥æ¨¡å‹ä»¥æé«˜åˆ†å‰²ç²¾åº¦å’Œä¿æŒç©ºé—´å®Œæ•´æ€§ã€‚</li>
<li>Echo-DNDæ¨¡å‹åœ¨CAMUSå’ŒEchoNet-Dynamicæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ¨¡å‹ã€‚</li>
<li>Echo-DNDæ¨¡å‹è¾¾åˆ°é«˜Diceåˆ†æ•°ï¼Œæ˜¾ç¤ºäº†å…¶ä¼˜ç§€æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17db04e1312db53ce696dcaba1133c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05460226eefab7c60c71d54336c50ff4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28bc320d0c206e3d69d3846b6a6235f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Frequency-Calibrated-Membership-Inference-Attacks-on-Medical-Image-Diffusion-Models"><a href="#Frequency-Calibrated-Membership-Inference-Attacks-on-Medical-Image-Diffusion-Models" class="headerlink" title="Frequency-Calibrated Membership Inference Attacks on Medical Image   Diffusion Models"></a>Frequency-Calibrated Membership Inference Attacks on Medical Image   Diffusion Models</h2><p><strong>Authors:Xinkai Zhao, Yuta Tokuoka, Junichiro Iwasawa, Keita Oda</strong></p>
<p>The increasing use of diffusion models for image generation, especially in sensitive areas like medical imaging, has raised significant privacy concerns. Membership Inference Attack (MIA) has emerged as a potential approach to determine if a specific image was used to train a diffusion model, thus quantifying privacy risks. Existing MIA methods often rely on diffusion reconstruction errors, where member images are expected to have lower reconstruction errors than non-member images. However, applying these methods directly to medical images faces challenges. Reconstruction error is influenced by inherent image difficulty, and diffusion models struggle with high-frequency detail reconstruction. To address these issues, we propose a Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical image diffusion models. By focusing on reconstruction errors within a specific mid-frequency range and excluding both high-frequency (difficult to reconstruct) and low-frequency (less informative) regions, our frequency-selective approach mitigates the confounding factor of inherent image difficulty. Specifically, we analyze the reverse diffusion process, obtain the mid-frequency reconstruction error, and compute the structural similarity index score between the reconstructed and original images. Membership is determined by comparing this score to a threshold. Experiments on several medical image datasets demonstrate that our FCRE method outperforms existing MIA methods. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒç­‰æ•æ„Ÿé¢†åŸŸï¼Œéšç§ä¿æŠ¤é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ä½œä¸ºä¸€ç§å¯èƒ½çš„æ–¹æ³•å‡ºç°ï¼Œç”¨äºç¡®å®šç‰¹å®šå›¾åƒæ˜¯å¦ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œé‡åŒ–éšç§é£é™©ã€‚ç°æœ‰çš„MIAæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰©æ•£é‡å»ºè¯¯å·®ï¼Œå…¶ä¸­æˆå‘˜å›¾åƒçš„é‡å»ºè¯¯å·®é¢„è®¡ä¼šä½äºéæˆå‘˜å›¾åƒã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒé¢ä¸´æŒ‘æˆ˜ã€‚é‡å»ºè¯¯å·®å—å›¾åƒæœ¬èº«éš¾åº¦çš„å›ºæœ‰å½±å“ï¼Œæ‰©æ•£æ¨¡å‹åœ¨é«˜é¢‘ç»†èŠ‚é‡å»ºæ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒæ‰©æ•£æ¨¡å‹çš„é¢‘ç‡æ ¡å‡†é‡å»ºè¯¯å·®ï¼ˆFCREï¼‰æ–¹æ³•è¿›è¡ŒMIAã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸“æ³¨äºç‰¹å®šä¸­é¢‘èŒƒå›´å†…çš„é‡å»ºè¯¯å·®ï¼Œæ’é™¤äº†é«˜é¢‘ï¼ˆéš¾ä»¥é‡å»ºï¼‰å’Œä½é¢‘ï¼ˆä¿¡æ¯è¾ƒå°‘ï¼‰åŒºåŸŸï¼Œä»è€Œå‡è½»äº†å›¾åƒå›ºæœ‰éš¾åº¦çš„å¹²æ‰°å› ç´ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ†æåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè·å–ä¸­é¢‘é‡å»ºè¯¯å·®ï¼Œå¹¶è®¡ç®—é‡å»ºå›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åˆ†æ•°ã€‚é€šè¿‡å°†æ­¤åˆ†æ•°ä¸é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒæ¥ç¡®å®šæˆå‘˜èº«ä»½ã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FCREæ–¹æ³•ä¼˜äºç°æœ‰çš„MIAæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14919v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šæ™®åŠï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒç­‰æ•æ„Ÿé¢†åŸŸï¼Œå¼•å‘äº†éšç§æ–¹é¢çš„æ‹…å¿§ã€‚æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ä½œä¸ºä¸€ç§åˆ¤æ–­ç‰¹å®šå›¾åƒæ˜¯å¦ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ç”¨äºè¡¡é‡éšç§é£é™©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MIAæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰©æ•£é‡å»ºè¯¯å·®ï¼Œç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒæ‰©æ•£æ¨¡å‹çš„é¢‘ç‡æ ¡å‡†é‡å»ºè¯¯å·®ï¼ˆFCREï¼‰æ–¹æ³•ã€‚é€šè¿‡å…³æ³¨ç‰¹å®šä¸­é¢‘èŒƒå›´å†…çš„é‡å»ºè¯¯å·®ï¼Œå¹¶æ’é™¤é«˜é¢‘ï¼ˆéš¾ä»¥é‡å»ºï¼‰å’Œä½é¢‘ï¼ˆä¿¡æ¯è¾ƒå°‘ï¼‰åŒºåŸŸï¼Œæˆ‘ä»¬çš„é¢‘ç‡é€‰æ‹©æ€§æ–¹æ³•å‡è½»äº†å›¾åƒå›ºæœ‰éš¾åº¦çš„å¹²æ‰°å› ç´ ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„FCREæ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„MIAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼Œå¼•å‘äº†éšç§æ–¹é¢çš„å…³æ³¨ã€‚</li>
<li>æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰æ˜¯åˆ¤æ–­ç‰¹å®šå›¾åƒæ˜¯å¦ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ç”¨äºè¡¡é‡éšç§é£é™©ã€‚</li>
<li>ç°æœ‰MIAæ–¹æ³•é€šå¸¸åŸºäºæ‰©æ•£é‡å»ºè¯¯å·®ï¼Œä½†ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>é‡å»ºè¯¯å·®å—å›¾åƒå›ºæœ‰éš¾åº¦å½±å“ï¼Œæ‰©æ•£æ¨¡å‹åœ¨é«˜é¢‘ç»†èŠ‚é‡å»ºæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é¢‘ç‡æ ¡å‡†é‡å»ºè¯¯å·®ï¼ˆFCREï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºä¸­é¢‘èŒƒå›´å†…çš„é‡å»ºè¯¯å·®ã€‚</li>
<li>FCREæ–¹æ³•é€šè¿‡æ’é™¤é«˜é¢‘å’Œä½é¢‘åŒºåŸŸï¼Œå‡è½»äº†å›¾åƒå›ºæœ‰éš¾åº¦çš„å¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cca4a6861729ae0def3346306ce09652.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23f846394c41cf6bc2721a24052a726b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Risk-Estimation-of-Knee-Osteoarthritis-Progression-via-Predictive-Multi-task-Modelling-from-Efficient-Diffusion-Model-using-X-ray-Images"><a href="#Risk-Estimation-of-Knee-Osteoarthritis-Progression-via-Predictive-Multi-task-Modelling-from-Efficient-Diffusion-Model-using-X-ray-Images" class="headerlink" title="Risk Estimation of Knee Osteoarthritis Progression via Predictive   Multi-task Modelling from Efficient Diffusion Model using X-ray Images"></a>Risk Estimation of Knee Osteoarthritis Progression via Predictive   Multi-task Modelling from Efficient Diffusion Model using X-ray Images</h2><p><strong>Authors:David Butler, Adrian Hilton, Gustavo Carneiro</strong></p>
<p>Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒåœ¨è¯„ä¼°è†éª¨å…³èŠ‚ç‚ï¼ˆOAï¼‰é£é™©ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿå®ç°æ—©æœŸæ£€æµ‹å’Œç–¾ç—…ç›‘æµ‹ã€‚æœ€è¿‘çš„æœºå™¨å­¦ä¹ æŠ€æœ¯é€šè¿‡åŒ»å­¦å½±åƒæ”¹å–„äº†é£é™©é¢„ä¼°ï¼ˆå³é¢„æµ‹ç–¾ç—…è¿›å±•çš„å¯èƒ½æ€§ï¼‰å’Œé¢„æµ‹å»ºæ¨¡ï¼ˆå³åŸºäºå½“å‰æ•°æ®é¢„æµ‹æœªæ¥ç»“æœï¼‰ï¼Œä½†ä¸´åºŠåº”ç”¨ä»ç„¶æœ‰é™ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ç°æœ‰çš„ç”¨äºé£é™©é¢„ä¼°çš„æœªæ¥å›¾åƒç”Ÿæˆæ–¹æ³•æ—¢å¤æ‚åˆä¸å®ç”¨ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„æ–¹æ³•æœªèƒ½å®šä½è†å…³èŠ‚è§£å‰–åœ°æ ‡ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°çš„å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥å¼¥è¡¥è¿™äº›å·®è·ï¼Œé€šè¿‡å¤šä»»åŠ¡é¢„æµ‹å»ºæ¨¡æ¥ä¼°è®¡è†éª¨å…³èŠ‚ç‚è¿›å±•çš„é£é™©ï¼Œè¯¥å»ºæ¨¡èƒ½å¤Ÿåˆ†ç±»æœªæ¥è†éª¨å…³èŠ‚ç‚çš„ä¸¥é‡ç¨‹åº¦ï¼Œå¹¶ä»é«˜æ•ˆç”Ÿæˆçš„é«˜è´¨é‡æœªæ¥å›¾åƒä¸­é¢„æµ‹è†å…³èŠ‚è§£å‰–åœ°æ ‡ã€‚è¿™ç§å›¾åƒç”Ÿæˆæ˜¯é€šè¿‡åˆ©ç”¨ç±»åˆ«æ¡ä»¶æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£æ¨¡å‹æ¥é¢„æµ‹ç–¾ç—…è¿›å±•å®ç°çš„ï¼Œæä¾›ç‰¹å®šå¥åº·æƒ…å†µå¯èƒ½å¦‚ä½•å‘å±•çš„è§†è§‰è¡¨ç¤ºã€‚åº”ç”¨äºéª¨å…³èŠ‚ç‚å€¡è®®æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†å½“å‰æœ€ä½³æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œé¢„æµ‹è†éª¨å…³èŠ‚ç‚è¿›å±•çš„AUCè¾¾åˆ°0.71ï¼ŒåŒæ—¶æä¾›çº¦9%æ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14560v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦æˆåƒåœ¨è¯„ä¼°è†éª¨å…³èŠ‚ç‚é£é™©ä¸­çš„é‡è¦ä½œç”¨ï¼Œä»¥åŠæœºå™¨å­¦ä¹ åœ¨è†éª¨å…³èŠ‚ç‚é£é™©é¢„æµ‹å’Œé¢„æµ‹å»ºæ¨¡æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„å¤æ‚æ€§å’Œç¼ºä¹å¯è§£é‡Šæ€§é™åˆ¶äº†å…¶åœ¨ä¸´åºŠçš„å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥ä¼°è®¡è†éª¨å…³èŠ‚ç‚è¿›å±•çš„é£é™©ï¼Œé€šè¿‡å¤šä»»åŠ¡é¢„æµ‹å»ºæ¨¡ï¼Œä¸ä»…åˆ†ç±»æœªæ¥è†éª¨å…³èŠ‚ç‚çš„ä¸¥é‡ç¨‹åº¦ï¼Œè¿˜ä»é«˜æ•ˆç”Ÿæˆçš„é«˜è´¨é‡æœªæ¥å›¾åƒä¸­é¢„æµ‹è†å…³èŠ‚è§£å‰–æ ‡å¿—ã€‚ä½¿ç”¨æ‰©æ•£æ¨¡å‹åœ¨ç±»åˆ«æ¡ä»¶æ½œåœ¨ç©ºé—´å†…é¢„æµ‹ç–¾ç—…è¿›å±•ï¼Œæä¾›ç‰¹å®šå¥åº·æ¡ä»¶å¯èƒ½å¦‚ä½•å‘å±•çš„è§†è§‰è¡¨ç¤ºã€‚åº”ç”¨äºéª¨å…³èŠ‚ç‚å€¡è®®æ•°æ®é›†ï¼Œè¯¥æ–¹æ³•æ”¹è¿›äº†å½“å‰æœ€ä½³æ°´å¹³ï¼Œé¢„æµ‹è†éª¨å…³èŠ‚ç‚è¿›å±•çš„AUCè¾¾åˆ°0.71ï¼ŒåŒæ—¶æä¾›çº¦9%æ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒåœ¨è¯„ä¼°è†éª¨å…³èŠ‚ç‚é£é™©ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿå®ç°æ—©æœŸæ£€æµ‹å’Œç–¾ç—…ç›‘æµ‹ã€‚</li>
<li>ç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨è†éª¨å…³èŠ‚ç‚é£é™©é¢„æµ‹å’Œé¢„æµ‹å»ºæ¨¡æ–¹é¢å·²æœ‰æ‰€æ”¹è¿›ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§é™åˆ¶äº†ä¸´åºŠé‡‡ç”¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºåœ¨ç±»åˆ«æ¡ä»¶æ½œåœ¨ç©ºé—´å†…ç”Ÿæˆé«˜è´¨é‡æœªæ¥å›¾åƒï¼Œä»¥é¢„æµ‹è†éª¨å…³èŠ‚ç‚çš„è¿›å±•ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡å¤šä»»åŠ¡é¢„æµ‹å»ºæ¨¡ï¼ŒåŒæ—¶åˆ†ç±»æœªæ¥è†éª¨å…³èŠ‚ç‚çš„ä¸¥é‡ç¨‹åº¦å¹¶é¢„æµ‹è†å…³èŠ‚è§£å‰–æ ‡å¿—ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨é¢„æµ‹è†éª¨å…³èŠ‚ç‚è¿›å±•æ–¹é¢æé«˜äº†2%ï¼Œè¾¾åˆ°AUC 0.71ã€‚</li>
<li>æ–°æ–¹æ³•æä¾›äº†æ›´å¿«çš„æ¨ç†æ—¶é—´ï¼Œçº¦ä¸º9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f83411f7ce0b89d89c72738deb2b9d1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819f08dc5aa8a313cc0bf5b9ffbdbec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b7c97b464f1afb1a9e074153baff938.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FRIDU-Functional-Map-Refinement-with-Guided-Image-Diffusion"><a href="#FRIDU-Functional-Map-Refinement-with-Guided-Image-Diffusion" class="headerlink" title="FRIDU: Functional Map Refinement with Guided Image Diffusion"></a>FRIDU: Functional Map Refinement with Guided Image Diffusion</h2><p><strong>Authors:Avigail Cohen Rimon, Mirela Ben-Chen, Or Litany</strong></p>
<p>We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›ä¸¤ä¸ªå½¢çŠ¶ä¹‹é—´ç»™å®šå¯¹åº”åœ°å›¾çš„æ–°æ–¹æ³•ã€‚ä½œä¸ºåŠŸèƒ½å›¾è¡¨ç¤ºçš„å¯¹åº”åœ°å›¾ï¼Œå³åŸºå˜æ¢çŸ©é˜µï¼Œå¯ä»¥å¦å¤–ä½œä¸ºäºŒç»´å›¾åƒå¤„ç†ã€‚ä»è¿™ä¸ªè§’åº¦å‡ºå‘ï¼Œæˆ‘ä»¬åœ¨åŠŸèƒ½å›¾çš„ç©ºé—´ä¸­ç›´æ¥è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ä¸å‡†ç¡®çš„åˆå§‹åœ°å›¾ç”Ÿæˆå‡†ç¡®çš„åœ°å›¾ã€‚è®­ç»ƒæ˜¯åœ¨åŠŸèƒ½ç©ºé—´å†…å®Œæˆçš„ï¼Œå› æ­¤æ•ˆç‡å¾ˆé«˜ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹åº”äºå½“å‰åŠŸèƒ½å›¾çš„ç‚¹å¯¹åœ°å›¾ä½œä¸ºæ‰©æ•£è¿‡ç¨‹ä¸­çš„æŒ‡å¯¼ã€‚æŒ‡å¯¼è¿˜å¯ä»¥é¼“åŠ±ä¸åŒçš„åŠŸèƒ½å›¾ç›®æ ‡ï¼Œå¦‚ä¸Laplace-Beltramiç®—å­çš„æ­£äº¤æ€§å’Œå¯äº¤æ¢æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„åœ°å›¾ä¼˜åŒ–æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸ºåŠŸèƒ½å›¾å¤„ç†æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14322v1">PDF</a> Accepted to SGP 2025 (Symposium on Geometry Processing)</p>
<p><strong>Summary</strong></p>
<p>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ–¹æ³•å¯¹ä¸¤ä¸ªå½¢çŠ¶ä¹‹é—´çš„å¯¹åº”åœ°å›¾è¿›è¡Œç»†åŒ–ã€‚å°†ä½œä¸ºåŠŸèƒ½å›¾çš„å¯¹åº”åœ°å›¾è§†ä¸ºäºŒç»´å›¾åƒï¼Œå¹¶ç›´æ¥åœ¨åŠŸèƒ½å›¾ç©ºé—´ä¸­è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œç”ŸæˆåŸºäºä¸å‡†ç¡®åˆå§‹åœ°å›¾çš„å‡†ç¡®åœ°å›¾ã€‚è¿™ç§è®­ç»ƒæ–¹æ³•éå¸¸é«˜æ•ˆï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨å½“å‰çš„ç‚¹å¯¹ç‚¹æ˜ å°„ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„æŒ‡å¯¼æ¥å®ç°ä¸åŒçš„åŠŸèƒ½æ˜ å°„ç›®æ ‡ã€‚å±•ç¤ºè¯¥æ–¹æ¡ˆåœ¨åœ°å›¾ç»†åŒ–æ–¹é¢çš„ç«äº‰åŠ›ï¼Œå¹¶è¡¨æ˜å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸ºåŠŸèƒ½å›¾å¤„ç†æä¾›äº†æœ‰å‰é€”çš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ç”¨äºæ”¹è¿›ä¸¤ä¸ªå½¢çŠ¶ä¹‹é—´çš„å¯¹åº”åœ°å›¾ã€‚</li>
<li>å°†å¯¹åº”åœ°å›¾è§†ä¸ºäºŒç»´å›¾åƒè¿›è¡Œå¤„ç†ã€‚</li>
<li>åœ¨åŠŸèƒ½å›¾ç©ºé—´ä¸­ç›´æ¥è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”ŸæˆåŸºäºä¸å‡†ç¡®åˆå§‹åœ°å›¾çš„å‡†ç¡®åœ°å›¾ã€‚</li>
<li>è®­ç»ƒæ–¹æ³•é«˜åº¦é«˜æ•ˆï¼Œç›´æ¥åœ¨åŠŸèƒ½ç©ºé—´ä¸­è¿›è¡Œã€‚</li>
<li>ä½¿ç”¨ç‚¹å¯¹ç‚¹æ˜ å°„ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„æŒ‡å¯¼ã€‚</li>
<li>å¯ä»¥å®ç°ä¸åŒçš„åŠŸèƒ½æ˜ å°„ç›®æ ‡ï¼Œå¦‚æ­£äº¤æ€§å’Œä¸Laplace-Beltramiç®—å­çš„å¯äº¤æ¢æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39761e7d8b1d926935b38fd6c01d2893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c952a575a19bafec08b4aceffc43b6ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7338626bcc0316edad2aa5eda81462ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d8b2d5206af65fccff7c0e089cd381a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85d32daa422f118465b8b50658568fcd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CausalDiffTab-Mixed-Type-Causal-Aware-Diffusion-for-Tabular-Data-Generation"><a href="#CausalDiffTab-Mixed-Type-Causal-Aware-Diffusion-for-Tabular-Data-Generation" class="headerlink" title="CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data   Generation"></a>CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data   Generation</h2><p><strong>Authors:Jia-Chen Zhang, Zheng Zhou, Yu-Jie Xiong, Chun-Ming Xia, Fei Dai</strong></p>
<p>Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the modelâ€™s performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Godz-z/CausalDiffTab">https://github.com/Godz-z/CausalDiffTab</a>. </p>
<blockquote>
<p>è®­ç»ƒæ•°æ®å·²è¢«è¯æ˜æ˜¯è®­ç»ƒç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸­æœ€å…³é”®çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œè·å–é«˜è´¨é‡çš„æ•°æ®ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ•°æ®éšç§é—®é¢˜æ˜¯å…¶ä¸­çš„ä¸€å¤§éšœç¢ã€‚ä¸ºäº†è§£å†³å¯¹é«˜è´¨é‡æ•°æ®çš„éœ€æ±‚ï¼Œåˆæˆæ•°æ®å·²æˆä¸ºä¸»æµè§£å†³æ–¹æ¡ˆï¼Œåœ¨å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç”Ÿæˆæ··åˆç±»å‹çš„æ•°æ®ï¼Œå°¤å…¶æ˜¯é«˜è´¨é‡çš„è¡¨æ ¼æ•°æ®ï¼Œä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜ä¸»è¦åŒ…æ‹¬å…¶å›ºæœ‰çš„å¼‚è´¨æ•°æ®ç±»å‹ã€å¤æ‚çš„å˜é‡é—´å…³ç³»ä»¥åŠå¤æ‚çš„åˆ—çº§åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CausalDiffTabï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†åŒ…å«æ•°å€¼å’Œç±»åˆ«ç‰¹å¾çš„æ··åˆè¡¨æ ¼æ•°æ®ï¼ŒåŒæ—¶èƒ½å¤Ÿæ›´çµæ´»åœ°æ•æ‰å˜é‡ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºåˆ†å±‚å…ˆéªŒèåˆåŸç†çš„æ··åˆè‡ªé€‚åº”å› æœæ­£åˆ™åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥è‡ªé€‚åº”åœ°æ§åˆ¶å› æœæ­£åˆ™åŒ–çš„æƒé‡ï¼Œåœ¨ä¸æŸå®³æ¨¡å‹ç”Ÿæˆèƒ½åŠ›çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCausalDiffTabåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Godz-z/CausalDiffTab%E3%80%82">https://github.com/Godz-z/CausalDiffTabã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14206v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹CausalDiffTabï¼Œä¸“é—¨ç”¨äºå¤„ç†åŒ…å«æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾çš„æ··åˆè¡¨æ ¼æ•°æ®ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿçµæ´»æ•æ‰å˜é‡é—´çš„å¤æ‚äº¤äº’ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºåˆ†å±‚å…ˆéªŒèåˆåŸç†çš„æ··åˆè‡ªé€‚åº”å› æœæ­£åˆ™åŒ–æ–¹æ³•ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ä¸”ä¸æŸå¤±å…¶ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCausalDiffTabåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIçš„è®­ç»ƒä¸­ï¼Œè®­ç»ƒæ•°æ®æ˜¯è‡³å…³é‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œä½†è·å–é«˜è´¨é‡æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ•°æ®éšç§é—®é¢˜æ˜¯å…¶ä¸­çš„ä¸»è¦éšœç¢ã€‚</li>
<li>åˆæˆæ•°æ®å·²æˆä¸ºè§£å†³é«˜è´¨é‡æ•°æ®éœ€æ±‚çš„ä¸»æµè§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç”Ÿæˆæ··åˆç±»å‹æ•°æ®ï¼Œå°¤å…¶æ˜¯é«˜è´¨é‡è¡¨æ ¼æ•°æ®ï¼Œä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…¶å†…åœ¨å¼‚è´¨æ•°æ®ç±»å‹ã€å¤æ‚çš„å˜é‡é—´å…³ç³»ä»¥åŠç²¾ç»†çš„åˆ—çº§åˆ†å¸ƒã€‚</li>
<li>CausalDiffTabæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†åŒ…å«æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾çš„æ··åˆè¡¨æ ¼æ•°æ®ã€‚</li>
<li>CausalDiffTabæ¨¡å‹ç»“åˆäº†ä¸€ç§æ··åˆè‡ªé€‚åº”å› æœæ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºåˆ†å±‚å…ˆéªŒèåˆåŸç†ï¼Œèƒ½è‡ªé€‚åº”æ§åˆ¶å› æœæ­£åˆ™åŒ–çš„æƒé‡ã€‚</li>
<li>ç»¼åˆå®éªŒè¯æ˜ï¼ŒCausalDiffTabåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚</li>
<li>CausalDiffTabçš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f3dca94dca9f1d9ce8665b605508cdb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63155267226a6561fdd8bf3f422ff62f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d4e9126ffbd17e58f36347fe02e8852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0655f3d12bfef180e02d14fff9f65af4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ASMR-Augmenting-Life-Scenario-using-Large-Generative-Models-for-Robotic-Action-Reflection"><a href="#ASMR-Augmenting-Life-Scenario-using-Large-Generative-Models-for-Robotic-Action-Reflection" class="headerlink" title="ASMR: Augmenting Life Scenario using Large Generative Models for Robotic   Action Reflection"></a>ASMR: Augmenting Life Scenario using Large Generative Models for Robotic   Action Reflection</h2><p><strong>Authors:Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen</strong></p>
<p>When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robotâ€™s action selection capabilities, achieving the state-of-the-art performance. </p>
<blockquote>
<p>åœ¨è®¾è®¡ç”¨äºè¾…åŠ©äººç±»æ—¥å¸¸æ´»åŠ¨çš„æœºå™¨äººæ—¶ï¼Œé€šè¿‡å¢å¼ºç”¨æˆ·çš„è¯·æ±‚ä¸å‘¨å›´ç¯å¢ƒçš„è§†è§‰çº¿ç´¢æ¥æ”¹å–„æ„å›¾ç†è§£æ˜¯éå¸¸å…³é”®çš„ã€‚è¿™ä¸€è¿‡ç¨‹è¢«å®šä¹‰ä¸ºå¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ”¶é›†åŒ…å«è§†è§‰å’Œè¯­è¨€å­¦å…ƒç´ çš„å¤§è§„æ¨¡æ•°æ®é›†æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒæ˜¯å……æ»¡æŒ‘æˆ˜ä¸”è€—æ—¶çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„è®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œä¸“æ³¨äºæœºå™¨äººè¾…åŠ©åœºæ™¯ä¸­çš„æ•°æ®å¢å¼ºï¼Œæ¶µç›–å¯¹è¯å’Œç›¸å…³ç¯å¢ƒå›¾åƒã€‚è¯¥æ–¹æ³•æ¶‰åŠåˆ©ç”¨å¤æ‚çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ¨¡æ‹Ÿæ½œåœ¨çš„å¯¹è¯å’Œç¯å¢ƒèƒŒæ™¯ï¼Œéšåä½¿ç”¨ç¨³å®šçš„æ‰©æ•£æ¨¡å‹æ¥åˆ›å»ºæç»˜è¿™äº›ç¯å¢ƒçš„å›¾åƒã€‚é¢å¤–ç”Ÿæˆçš„æ•°æ®ç”¨äºä¼˜åŒ–æœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ ¹æ®ä¸ç”¨æˆ·æœ‰é™ç›®æ ‡æ•°æ®çš„äº¤äº’æ¥ç¡®å®šé€‚å½“çš„è¡ŒåŠ¨ã€‚æˆ‘ä»¬åŸºäºä»çœŸå®åœºæ™¯æ”¶é›†çš„æ•°æ®é›†è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æœºå™¨äººçš„åŠ¨ä½œé€‰æ‹©èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13956v1">PDF</a> IWSDS 2024 Best Paper Award</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç”¨äºåœ¨æœºå™¨äººè¾…åŠ©åœºæ™¯ä¸­é€šè¿‡æ•°æ®æ‰©å……æé«˜ç”¨æˆ·æ„å›¾ç†è§£ã€‚æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¯¹è¯å’Œç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œå†é€šè¿‡æ‰©æ•£æ¨¡å‹åˆ›å»ºç›¸å…³ç¯å¢ƒå›¾åƒã€‚ç”Ÿæˆçš„æ•°æ®ç”¨äºä¼˜åŒ–æœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿å…¶èƒ½æ›´å‡†ç¡®åœ°æ ¹æ®ç”¨æˆ·äº¤äº’æ•°æ®é€‰æ‹©é€‚å½“è¡ŒåŠ¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜æœºå™¨äººçš„åŠ¨ä½œé€‰æ‹©èƒ½åŠ›ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººè¾…åŠ©æ—¥å¸¸æ´»åŠ¨ä¸­ï¼Œé€šè¿‡å¢å¼ºç”¨æˆ·è¯·æ±‚å’Œç¯å¢ƒè§†è§‰çº¿ç´¢æ¥æé«˜æ„å›¾ç†è§£æ˜¯å…³é”®ã€‚</li>
<li>å¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡æ¶‰åŠè§†è§‰å’Œè¯­è¨€å…ƒç´ çš„ç»“åˆã€‚</li>
<li>æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ç”¨äºæœºå™¨äººæ¨¡å‹è®­ç»ƒæ˜¯æŒ‘æˆ˜æ€§å’Œè€—æ—¶çš„ã€‚</li>
<li>æ–°å‹æ¡†æ¶ä¾§é‡äºæœºå™¨äººè¾…åŠ©åœºæ™¯ä¸­çš„æ•°æ®æ‰©å……ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¯¹è¯å’Œç¯å¢ƒä¸Šä¸‹æ–‡ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆ›å»ºç›¸å…³ç¯å¢ƒå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14a72aed7d64cbfb524eb95be695fe6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c82948eb600101cebe38e034f0d25b1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecca4c96afd842b656d17e9af142efbc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MultiViT2-A-Data-augmented-Multimodal-Neuroimaging-Prediction-Framework-via-Latent-Diffusion-Model"><a href="#MultiViT2-A-Data-augmented-Multimodal-Neuroimaging-Prediction-Framework-via-Latent-Diffusion-Model" class="headerlink" title="MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework   via Latent Diffusion Model"></a>MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework   via Latent Diffusion Model</h2><p><strong>Authors:Bi Yuda, Jia Sihan, Gao Yutong, Abrol Anees, Fu Zening, Calhoun Vince</strong></p>
<p>Multimodal medical imaging integrates diverse data types, such as structural and functional neuroimaging, to provide complementary insights that enhance deep learning predictions and improve outcomes. This study focuses on a neuroimaging prediction framework based on both structural and functional neuroimaging data. We propose a next-generation prediction model, \textbf{MultiViT2}, which combines a pretrained representative learning base model with a vision transformer backbone for prediction output. Additionally, we developed a data augmentation module based on the latent diffusion model that enriches input data by generating augmented neuroimaging samples, thereby enhancing predictive performance through reduced overfitting and improved generalizability. We show that MultiViT2 significantly outperforms the first-generation model in schizophrenia classification accuracy and demonstrates strong scalability and portability. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å½±åƒå°†ä¸åŒç±»å‹çš„æ•°æ®ï¼ˆå¦‚ç»“æ„å’ŒåŠŸèƒ½ç¥ç»æˆåƒï¼‰è¿›è¡Œèåˆï¼Œæä¾›äº’è¡¥çš„æ´å¯Ÿï¼Œå¢å¼ºæ·±åº¦å­¦ä¹ é¢„æµ‹å¹¶æ”¹å–„ç»“æœã€‚æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨åŸºäºç»“æ„å’ŒåŠŸèƒ½ç¥ç»æˆåƒæ•°æ®çš„ç¥ç»å½±åƒé¢„æµ‹æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‹ä¸€ä»£é¢„æµ‹æ¨¡å‹â€”â€”MultiViT2ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„ä»£è¡¨æ€§å­¦ä¹ åŸºç¡€æ¨¡å‹ä¸ç”¨äºé¢„æµ‹è¾“å‡ºçš„è§†è§‰è½¬æ¢å™¨ä¸»å¹²ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹å¼€å‘äº†ä¸€ä¸ªæ•°æ®å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆå¢å¼ºçš„ç¥ç»æˆåƒæ ·æœ¬ä¸°å¯Œè¾“å…¥æ•°æ®ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆç°è±¡ï¼Œæé«˜é¢„æµ‹æ€§èƒ½çš„å¯æ¨å¹¿æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒMultiViT2åœ¨ç²¾ç¥åˆ†è£‚ç—‡åˆ†ç±»å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç¬¬ä¸€ä»£æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œå¯ç§»æ¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€åŒ»å­¦å½±åƒåœ¨æ·±åº¦å­¦ä¹ é¢„æµ‹ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ç»“åˆç»“æ„æ€§å’ŒåŠŸèƒ½æ€§ç¥ç»å½±åƒæ•°æ®ï¼Œæé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMultiViT2çš„ä¸‹ä¸€ä»£é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†é¢„è®­ç»ƒçš„ä»£è¡¨æ€§å­¦ä¹ åŸºç¡€æ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨åç«¯è¿›è¡Œé¢„æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ä¸ªåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆå¢å¼ºçš„ç¥ç»å½±åƒæ ·æœ¬ï¼Œæé«˜äº†é¢„æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMultiViT2åœ¨ç²¾ç¥åˆ†è£‚ç—‡åˆ†ç±»å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç¬¬ä¸€ä»£æ¨¡å‹ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œå¯ç§»æ¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å½±åƒç»“åˆäº†å¤šç§æ•°æ®ç±»å‹ï¼Œå¦‚ç»“æ„æ€§å’ŒåŠŸèƒ½æ€§ç¥ç»å½±åƒï¼Œä»¥æé«˜æ·±åº¦å­¦ä¹ é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é¢„æµ‹æ¨¡å‹MultiViT2ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨åç«¯ã€‚</li>
<li>MultiViT2æ¨¡å‹åœ¨ç²¾ç¥åˆ†è£‚ç—‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜äºç¬¬ä¸€ä»£æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆå¢å¼ºçš„ç¥ç»å½±åƒæ ·æœ¬ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>æ•°æ®å¢å¼ºæ¨¡å—æœ‰åŠ©äºå‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MultiViT2æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œå¯ç§»æ¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b77fffe5a6f627fc420b34e471765d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ee0dab38bf30fd978e91b68263b70f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476614ded6dd8d006d43f3d604bb9547.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f975075ea1308074b8860982ccab4b5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Exploiting-the-Exact-Denoising-Posterior-Score-in-Training-Free-Guidance-of-Diffusion-Models"><a href="#Exploiting-the-Exact-Denoising-Posterior-Score-in-Training-Free-Guidance-of-Diffusion-Models" class="headerlink" title="Exploiting the Exact Denoising Posterior Score in Training-Free Guidance   of Diffusion Models"></a>Exploiting the Exact Denoising Posterior Score in Training-Free Guidance   of Diffusion Models</h2><p><strong>Authors:Gregory Bellchambers</strong></p>
<p>The success of diffusion models has driven interest in performing conditional sampling via training-free guidance of the denoising process to solve image restoration and other inverse problems. A popular class of methods, based on Diffusion Posterior Sampling (DPS), attempts to approximate the intractable posterior score function directly. In this work, we present a novel expression for the exact posterior score for purely denoising tasks that is tractable in terms of the unconditional score function. We leverage this result to analyze the time-dependent error in the DPS score for denoising tasks and compute step sizes on the fly to minimize the error at each time step. We demonstrate that these step sizes are transferable to related inverse problems such as colorization, random inpainting, and super resolution. Despite its simplicity, this approach is competitive with state-of-the-art techniques and enables sampling with fewer time steps than DPS. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æˆåŠŸæ¿€å‘äº†é€šè¿‡å»å™ªè¿‡ç¨‹çš„éè®­ç»ƒå¼•å¯¼æ¥è¿›è¡Œæ¡ä»¶é‡‡æ ·çš„å…´è¶£ï¼Œä»¥è§£å†³å›¾åƒæ¢å¤å’Œå…¶ä»–é€†å‘é—®é¢˜ã€‚åŸºäºæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰çš„ä¸€ç±»æµè¡Œæ–¹æ³•è¯•å›¾ç›´æ¥é€¼è¿‘éš¾ä»¥å¤„ç†çš„åéªŒåˆ†æ•°å‡½æ•°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºçº¯å»å™ªä»»åŠ¡æå‡ºäº†ç²¾ç¡®åéªŒåˆ†æ•°çš„æ–°è¡¨è¾¾å¼ï¼Œè¯¥è¡¨è¾¾å¼åœ¨æ— æ¡ä»¶åˆ†æ•°å‡½æ•°æ–¹é¢æ˜¯å¯è¡Œçš„ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç»“æœåˆ†æäº†å»å™ªä»»åŠ¡ä¸­DPSåˆ†æ•°çš„æ—¶å˜è¯¯å·®ï¼Œå¹¶è®¡ç®—äº†å³æ—¶æ­¥éª¤å¤§å°ï¼Œä»¥æœ€å°åŒ–æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„è¯¯å·®ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™äº›æ­¥éª¤å¤§å°å¯ä»¥è½¬ç§»åˆ°ç›¸å…³çš„é€†å‘é—®é¢˜ï¼Œä¾‹å¦‚å½©è‰²åŒ–ã€éšæœºå¡«å……å’Œè¶…åˆ†è¾¨ç‡ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼Œä½†è¿™ç§æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æŠ€æœ¯ç›¸ç«äº‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨æ¯”DPSæ›´å°‘çš„æ—¶é—´æ­¥é•¿å†…è¿›è¡Œé‡‡æ ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤å’Œå…¶ä»–åé—®é¢˜ä¸­çš„æ¡ä»¶é‡‡æ ·çš„æœ€æ–°è¿›å±•ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ— æ¡ä»¶è¯„åˆ†å‡½æ•°ç²¾ç¡®è®¡ç®—åéªŒè¯„åˆ†çš„è¡¨è¾¾å¼ï¼Œå¹¶æ®æ­¤åˆ†æäº†æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰åœ¨é™å™ªä»»åŠ¡ä¸­çš„æ—¶é—´ç›¸å…³è¯¯å·®ã€‚é€šè¿‡å®æ—¶è®¡ç®—æ­¥é•¿ï¼Œä»¥æœ€å°åŒ–æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„è¯¯å·®ã€‚æ­¤æ–¹æ³•å¯åº”ç”¨äºç›¸å…³é€†é—®é¢˜ï¼Œå¦‚å½©è‰²åŒ–ã€éšæœºè¡¥å…¨å’Œè¶…åˆ†è¾¨ç‡ç­‰ã€‚è¯¥æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æŠ€å·§ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ­¥éª¤ä¸­å®ç°é‡‡æ ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡è®­ç»ƒä¹‹å¤–çš„æŒ‡å¯¼æ¥å®ç°æ¡ä»¶é‡‡æ ·ï¼Œç”¨äºè§£å†³å›¾åƒä¿®å¤å’Œå…¶ä»–åé—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°è¡¨è¾¾åéªŒè¯„åˆ†çš„æ–¹å¼ï¼Œä¸“é—¨é’ˆå¯¹çº¯é™å™ªä»»åŠ¡ï¼Œå¹¶ç”¨æ— æ¡ä»¶è¯„åˆ†å‡½æ•°è¿›è¡Œè®¡ç®—ã€‚</li>
<li>é€šè¿‡å®æ—¶è®¡ç®—æ­¥é•¿ï¼Œä¼˜åŒ–äº†åŸºäºæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰çš„æ—¶é—´ç›¸å…³è¯¯å·®åˆ†æã€‚</li>
<li>è¯æ˜äº†è¿™ç§æ–¹æ³•å¯¹äºé™å™ªä»»åŠ¡çš„æ—¶é—´æ­¥é•¿è®¾ç½®çš„é€šç”¨æ€§ï¼Œå¯ä»¥åº”ç”¨äºé¢œè‰²åŒ–ã€éšæœºè¡¥å…¨å’Œè¶…åˆ†è¾¨ç‡ç­‰ä»»åŠ¡ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ­¥éª¤ä¸­å®ç°é‡‡æ ·ã€‚</li>
<li>æ­¤æ–¹æ³•ç®€åŒ–äº†æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜äº†å…¶åœ¨å›¾åƒä¿®å¤é¢†åŸŸçš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6e70da807713d012cb52ddbc5bf7eb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25471d2fd65bc067338a8d82d2bffeb8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space"><a href="#Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space" class="headerlink" title="Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space"></a>Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space</h2><p><strong>Authors:ClÃ©mentine Phung-Ngoc, Alexandre Bousse, Antoine De Paepe, Hong-Phuong Dang, Olivier Saut, Dimitris Visvikis</strong></p>
<p>Attenuation correction (AC) is necessary for accurate activity quantification in positron emission tomography (PET). Conventional reconstruction methods typically rely on attenuation maps derived from a co-registered computed tomography (CT) or magnetic resonance imaging scan. However, this additional scan may complicate the imaging workflow, introduce misalignment artifacts and increase radiation exposure. In this paper, we propose a joint reconstruction of activity and attenuation (JRAA) approach that eliminates the need for auxiliary anatomical imaging by relying solely on emission data. This framework combines wavelet diffusion model (WDM) and diffusion posterior sampling (DPS) to reconstruct fully three-dimensional (3-D) data. Experimental results show our method outperforms maximum likelihood activity and attenuation (MLAA) and MLAA with UNet-based post processing, and yields high-quality noise-free reconstructions across various count settings when time-of-flight (TOF) information is available. It is also able to reconstruct non-TOF data, although the reconstruction quality significantly degrades in low-count (LC) conditions, limiting its practical effectiveness in such settings. This approach represents a step towards stand-alone PET imaging by reducing the dependence on anatomical modalities while maintaining quantification accuracy, even in low-count scenarios when TOF information is available. </p>
<blockquote>
<p>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰åœ¨æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰çš„å‡†ç¡®æ´»åŠ¨é‡åŒ–ä¸­å¿…ä¸å¯å°‘ã€‚ä¼ ç»Ÿé‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºä»å…±æ³¨å†Œçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒæ‰«æä¸­å¾—å‡ºçš„è¡°å‡å›¾ã€‚ç„¶è€Œï¼Œè¿™ç§é¢å¤–çš„æ‰«æå¯èƒ½ä¼šä½¿æˆåƒå·¥ä½œæµç¨‹å¤æ‚åŒ–ï¼Œå¼•å…¥é”™ä½ä¼ªå½±å¹¶å¢åŠ è¾å°„æš´éœ²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆé‡å»ºæ´»åŠ¨å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä¾èµ–å‘å°„æ•°æ®ï¼Œæ¶ˆé™¤äº†å¯¹è¾…åŠ©è§£å‰–æˆåƒçš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰æ¥é‡å»ºå…¨ä¸‰ç»´ï¼ˆ3-Dï¼‰æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§è®¡æ•°è®¾ç½®ä¸‹çš„æ€§èƒ½ä¼˜äºæœ€å¤§å¯èƒ½æ€§æ´»åŠ¨å’Œè¡°å‡ï¼ˆMLAAï¼‰ä»¥åŠåŸºäºUNetçš„åå¤„ç†çš„MLAAï¼Œå¹¶äº§ç”Ÿé«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºã€‚è™½ç„¶å®ƒä¹Ÿèƒ½é‡å»ºéTOFæ•°æ®ï¼Œä½†åœ¨ä½è®¡æ•°ï¼ˆLCï¼‰æ¡ä»¶ä¸‹é‡å»ºè´¨é‡æ˜¾è‘—é™ä½ï¼Œè¿™åœ¨å®è·µä¸­é™åˆ¶äº†å…¶åœ¨è¿™ç§è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•æœç€ç‹¬ç«‹PETæˆåƒè¿ˆå‡ºäº†ä¸€æ­¥ï¼Œé€šè¿‡å‡å°‘å¯¹è§£å‰–æ¨¡æ€çš„ä¾èµ–ï¼Œå³ä½¿åœ¨æ—¶é—´é£è¡Œä¿¡æ¯å¯ç”¨æ—¶ä½è®¡æ•°åœºæ™¯ä¸­ä¹Ÿèƒ½ä¿æŒé‡åŒ–å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18782v2">PDF</a> 10 pages, 9 figures, 1 table</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§è”åˆé‡å»ºæ´»åŠ¨åº¦å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å‘å°„æ•°æ®ï¼Œæ— éœ€è¾…åŠ©çš„è§£å‰–å­¦æˆåƒã€‚é€šè¿‡ç»“åˆå°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œé‡å»ºå‡ºå…¨ä¸‰ç»´æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä¼˜äºæœ€å¤§å¯èƒ½æ€§æ´»åŠ¨åº¦å’Œè¡°å‡ï¼ˆMLAAï¼‰æ–¹æ³•åŠå…¶UNetåå¤„ç†ï¼Œèƒ½åœ¨å„ç§è®¡æ•°è®¾ç½®ä¸‹å®ç°é«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºã€‚å°½ç®¡åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹ï¼Œé‡å»ºè´¨é‡æœ‰æ‰€ä¸‹é™ï¼Œä½†åœ¨æœ‰TOFä¿¡æ¯çš„æƒ…å†µä¸‹ä»å…·æœ‰å®ç”¨ä»·å€¼ã€‚æ­¤æ–¹æ³•æ˜¯å®ç°ç‹¬ç«‹PETæˆåƒçš„é‡è¦ä¸€æ­¥ï¼Œå‡å°‘å¯¹è§£å‰–æ¨¡å¼çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒé‡åŒ–å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰åœ¨æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ä¸­çš„æ´»åŠ¨é‡åŒ–æ˜¯å¿…è¦çš„ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºæ–¹æ³•ä¾èµ–äºä»å…±æ³¨å†Œçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒæ‰«æå¾—åˆ°çš„è¡°å‡å›¾ã€‚</li>
<li>é¢å¤–çš„æ‰«æå¯èƒ½ä½¿æˆåƒå·¥ä½œæµç¨‹å¤æ‚åŒ–ï¼Œå¼•å…¥é”™ä½ä¼ªå½±å¹¶å¢åŠ è¾å°„æš´éœ²ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è”åˆé‡å»ºæ´»åŠ¨åº¦å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œä»…ä¾èµ–å‘å°„æ•°æ®ï¼Œæ— éœ€è¾…åŠ©çš„è§£å‰–å­¦æˆåƒã€‚</li>
<li>JRAAæ–¹æ³•ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œä»¥é‡å»ºå…¨ä¸‰ç»´æ•°æ®ã€‚</li>
<li>åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒJRAAæ–¹æ³•ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§è®¡æ•°è®¾ç½®ä¸‹å®ç°é«˜è´¨é‡æ— å™ªå£°é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c7d1178258ad1d7a8e607eed8446cbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44ede7683cddb537d59457ccedbd19ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d656d0ab49e0b73aebe3246052e0a482.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="BS-LDM-Effective-Bone-Suppression-in-High-Resolution-Chest-X-Ray-Images-with-Conditional-Latent-Diffusion-Models"><a href="#BS-LDM-Effective-Bone-Suppression-in-High-Resolution-Chest-X-Ray-Images-with-Conditional-Latent-Diffusion-Models" class="headerlink" title="BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images   with Conditional Latent Diffusion Models"></a>BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images   with Conditional Latent Diffusion Models</h2><p><strong>Authors:Yifei Sun, Zhanghao Chen, Hao Zheng, Wenming Deng, Jin Liu, Wenwen Min, Ahmed Elazab, Xiang Wan, Changmiao Wang, Ruiquan Ge</strong></p>
<p>Lung diseases represent a significant global health challenge, with Chest X-Ray (CXR) being a key diagnostic tool due to its accessibility and affordability. Nonetheless, the detection of pulmonary lesions is often hindered by overlapping bone structures in CXR images, leading to potential misdiagnoses. To address this issue, we develop an end-to-end framework called BS-LDM, designed to effectively suppress bone in high-resolution CXR images. This framework is based on conditional latent diffusion models and incorporates a multi-level hybrid loss-constrained vector-quantized generative adversarial network which is crafted for perceptual compression, ensuring the preservation of details. To further enhance the frameworkâ€™s performance, we utilize offset noise in the forward process, and a temporal adaptive thresholding strategy in the reverse process. These additions help minimize discrepancies in generating low-frequency information of soft tissue images. Additionally, we have compiled a high-quality bone suppression dataset named SZCH-X-Rays. This dataset includes 818 pairs of high-resolution CXR and soft tissue images collected from our partner hospital. Moreover, we processed 241 data pairs from the JSRT dataset into negative images, which are more commonly used in clinical practice. Our comprehensive experiments and downstream evaluations reveal that BS-LDM excels in bone suppression, underscoring its clinical value. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/diaoquesang/BS-LDM">https://github.com/diaoquesang/BS-LDM</a>. </p>
<blockquote>
<p>è‚ºéƒ¨ç–¾ç—…æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„é‡å¤§å¥åº·æŒ‘æˆ˜ï¼Œèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å› å…¶å¯è·å–æ€§å’Œå¯è´Ÿæ‹…æ€§æˆä¸ºå…³é”®çš„è¯Šæ–­å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºCXRå›¾åƒä¸­éª¨éª¼ç»“æ„çš„é‡å ï¼Œè‚ºéƒ¨ç—…å˜çš„æ£€æµ‹å¸¸å¸¸å—åˆ°é˜»ç¢ï¼Œå¯èƒ½å¯¼è‡´æ½œåœ¨è¯¯è¯Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œåä¸ºBS-LDMï¼Œæ—¨åœ¨åœ¨é«˜åˆ†è¾¨ç‡çš„CXRå›¾åƒä¸­æœ‰æ•ˆåœ°æŠ‘åˆ¶éª¨éª¼ã€‚è¯¥æ¡†æ¶åŸºäºæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶èå…¥äº†ä¸€ä¸ªå¤šå±‚æ¬¡æ··åˆæŸå¤±çº¦æŸå‘é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œè¯¥ç½‘ç»œä¸“ä¸ºæ„ŸçŸ¥å‹ç¼©è€Œè®¾è®¡ï¼Œå¯ç¡®ä¿ç»†èŠ‚ä¿ç•™ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„æ¡†æ¶çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨å‰å‘è¿‡ç¨‹ä¸­ä½¿ç”¨åç§»å™ªå£°ï¼Œå¹¶åœ¨åå‘è¿‡ç¨‹ä¸­é‡‡ç”¨ä¸´æ—¶è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥ã€‚è¿™äº›è¡¥å……æœ‰åŠ©äºå‡å°‘åœ¨ç”Ÿæˆè½¯ç»„ç»‡å›¾åƒçš„ä½é¢‘ä¿¡æ¯æ—¶çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¼–è¯‘äº†ä¸€ä¸ªé«˜è´¨é‡çš„å»éª¨æ•°æ®é›†ï¼Œåä¸ºSZCH-Xå°„çº¿ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»æˆ‘ä»¬çš„åˆä½œåŒ»é™¢æ”¶é›†çš„818å¯¹é«˜åˆ†è¾¨ç‡CXRå’Œè½¯ç»„ç»‡å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†JSRTæ•°æ®é›†ä¸­çš„241å¯¹æ•°æ®å¯¹å¤„ç†ä¸ºé˜´æ€§å›¾åƒï¼Œè¿™åœ¨ä¸´åºŠå®è·µä¸­æ›´ä¸ºå¸¸è§ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒå’Œä¸‹æ¸¸è¯„ä¼°æ˜¾ç¤ºï¼ŒBS-LDMåœ¨éª¨éª¼æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾äº†å…¶ä¸´åºŠä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/diaoquesang/BS-LDM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/diaoquesang/BS-LDMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15670v4">PDF</a> 12 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong><br>    å¼€å‘äº†ä¸€ç§åä¸ºBS-LDMçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºåœ¨é«˜æ¸…èƒ¸è…”Xå…‰ç‰‡ï¼ˆCXRï¼‰ä¸­æœ‰æ•ˆæŠ‘åˆ¶éª¨éª¼ï¼Œä»¥æé«˜è‚ºéƒ¨ç—…å˜æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŸºäºæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶èå…¥å¤šå±‚æ¬¡æ··åˆæŸå¤±çº¦æŸå‘é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œç”¨äºæ„ŸçŸ¥å‹ç¼©å¹¶ä¿ç•™ç»†èŠ‚ã€‚é‡‡ç”¨å‰å‘è¿‡ç¨‹åç§»å™ªå£°å’Œåå‘è¿‡ç¨‹æ—¶é—´è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥ï¼Œå‡å°‘è½¯ç»„ç»‡å›¾åƒä½é¢‘ä¿¡æ¯ç”Ÿæˆçš„å·®å¼‚ã€‚è¿˜ç¼–è¯‘äº†åä¸ºSZCH-Xå°„çº¿çš„é«˜è´¨é‡éª¨éª¼æŠ‘åˆ¶æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªåˆä½œåŒ»é™¢çš„818å¯¹é«˜æ¸…CXRå’Œè½¯ç»„ç»‡å›¾åƒã€‚å®éªŒå’Œä¸‹æ¸¸è¯„ä¼°æ˜¾ç¤ºBS-LDMåœ¨éª¨éª¼æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰ä¸´åºŠä»·å€¼ã€‚ä»£ç å…¬å¼€å¯è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>èƒ¸è…”Xå…‰ï¼ˆCXRï¼‰æ˜¯è‚ºéƒ¨ç—…å˜æ£€æµ‹çš„å…³é”®å·¥å…·ï¼Œä½†ç”±äºéª¨éª¼ç»“æ„çš„é‡å ï¼Œå…¶æ£€æµ‹å—åˆ°é˜»ç¢ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåä¸ºBS-LDMçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆæŠ‘åˆ¶é«˜æ¸…CXRå›¾åƒä¸­çš„éª¨éª¼ã€‚</li>
<li>BS-LDMåŸºäºæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šå±‚æ¬¡æ··åˆæŸå¤±çº¦æŸå‘é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚</li>
<li>BS-LDMé‡‡ç”¨å‰å‘è¿‡ç¨‹çš„åç§»å™ªå£°å’Œåå‘è¿‡ç¨‹çš„æ—¶é—´è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–è½¯ç»„ç»‡å›¾åƒçš„ç”Ÿæˆã€‚</li>
<li>ç ”ç©¶äººå‘˜ç¼–è¯‘äº†ä¸€ä¸ªåä¸ºSZCH-Xå°„çº¿çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°BS-LDMæ¡†æ¶ã€‚</li>
<li>BS-LDMåœ¨éª¨éª¼æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¿™é€šè¿‡ç»¼åˆå®éªŒå’Œä¸‹æ¸¸è¯„ä¼°å¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c138fc1074831cf1b4dd03b7e8722428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a07ec133501ee1432f9658f01f98e009.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-672191ace8b55fb0a070c27a025ecbfc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Representation-Alignment-for-Generation-Training-Diffusion-Transformers-Is-Easier-Than-You-Think"><a href="#Representation-Alignment-for-Generation-Training-Diffusion-Transformers-Is-Easier-Than-You-Think" class="headerlink" title="Representation Alignment for Generation: Training Diffusion Transformers   Is Easier Than You Think"></a>Representation Alignment for Generation: Training Diffusion Transformers   Is Easier Than You Think</h2><p><strong>Authors:Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie</strong></p>
<p>Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID&#x3D;1.42 using classifier-free guidance with the guidance interval. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œï¼ˆç”Ÿæˆå¼ï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„é™å™ªè¿‡ç¨‹å¯ä»¥åœ¨æ¨¡å‹å†…éƒ¨äº§ç”Ÿæœ‰æ„ä¹‰çš„ï¼ˆåˆ¤åˆ«å¼ï¼‰è¡¨ç¤ºï¼Œä½†è¿™äº›è¡¨ç¤ºçš„è´¨é‡ä»ç„¶è½åäºé€šè¿‡æœ€è¿‘è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å¾—åˆ°çš„è¡¨ç¤ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè®­ç»ƒç”¨äºç”Ÿæˆçš„å¤§å‹æ‰©æ•£æ¨¡å‹çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°å­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œé€šè¿‡èå…¥é«˜è´¨é‡çš„å¤–éƒ¨è§†è§‰è¡¨ç¤ºï¼Œè€Œéä»…ä¾èµ–æ‰©æ•£æ¨¡å‹ç‹¬ç«‹å­¦ä¹ ï¼Œå¯ä»¥ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§åä¸ºREPresentation Alignmentï¼ˆREPAï¼‰çš„ç®€å•æ­£åˆ™åŒ–æ–¹æ³•è¿›è¡Œç ”ç©¶ï¼Œè¯¥æ–¹æ³•å°†å»å™ªç½‘ç»œä¸­å™ªå£°è¾“å…¥éšè—çŠ¶æ€çš„æŠ•å½±ä¸ä»å¤–éƒ¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨è·å¾—çš„å¹²å‡€å›¾åƒè¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚ç»“æœä»¤äººå°è±¡æ·±åˆ»ï¼šå½“åº”ç”¨äºæµè¡Œçš„æ‰©æ•£å’ŒåŸºäºæµçš„å˜å‹å™¨ï¼ˆå¦‚DiTså’ŒSiTsï¼‰æ—¶ï¼Œæˆ‘ä»¬ç®€å•çš„ç­–ç•¥åœ¨è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½å¸¦æ¥äº†æ˜¾ç€æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†SiTè®­ç»ƒé€Ÿåº¦æé«˜17.5å€ä»¥ä¸Šï¼Œåœ¨ä¸åˆ°40ä¸‡æ­¥å†…è¾¾åˆ°ä¸7ç™¾ä¸‡æ­¥è®­ç»ƒçš„SiT-XLæ¨¡å‹ï¼ˆæ— éœ€åˆ†ç±»å™¨å¼•å¯¼ï¼‰ç›¸åŒ¹é…çš„æ€§èƒ½ã€‚åœ¨æœ€ç»ˆçš„ç”Ÿæˆè´¨é‡æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼çš„æŒ‡å¯¼ä¸‹é—´éš”å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒFID&#x3D;1.42ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06940v4">PDF</a> ICLR 2025 (Oral). Project page: <a target="_blank" rel="noopener" href="https://sihyun.me/REPA">https://sihyun.me/REPA</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹åœ¨å»å™ªè¿‡ç¨‹ä¸­å¯ä»¥å½¢æˆæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œä½†ç›¸è¾ƒäºé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è·å¾—çš„è¡¨ç¤ºè´¨é‡ä»æœ‰æ‰€ä¸è¶³ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè®­ç»ƒå¤§å‹ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå¦‚ä½•æœ‰æ•ˆå­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚é€šè¿‡å¼•å…¥é«˜è´¨é‡çš„å¤–éƒ¨è§†è§‰è¡¨ç¤ºï¼Œå¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºREPAï¼ˆREPresentation Alignmentï¼‰çš„ç®€å•æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å»å™ªç½‘ç»œä¸­å™ªå£°è¾“å…¥éšè—çŠ¶æ€çš„æŠ•å½±ä¸å¤–éƒ¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨è·å¾—çš„å¹²å‡€å›¾åƒè¡¨ç¤ºå¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œå¯¹äºæµè¡Œçš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºæµçš„è½¬æ¢å™¨ï¼Œå¦‚DiTså’ŒSiTsï¼Œä½¿ç”¨è¯¥æ–¹æ³•å¯ä»¥åŠ é€Ÿè®­ç»ƒï¼Œå¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å»å™ªè¿‡ç¨‹ä¸­èƒ½å½¢æˆæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œä½†è´¨é‡æœ‰å¾…æé«˜ã€‚</li>
<li>è®­ç»ƒå¤§å‹ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ä¸»è¦ç“¶é¢ˆåœ¨äºæœ‰æ•ˆå­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å¼•å…¥å¤–éƒ¨è§†è§‰è¡¨ç¤ºï¼Œå¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºREPAçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹é½å™ªå£°è¾“å…¥å’Œå¹²å‡€å›¾åƒè¡¨ç¤ºæ¥æé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>REPAæ–¹æ³•æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œä¾‹å¦‚å°†SiTè®­ç»ƒé€Ÿåº¦æé«˜17.5å€ã€‚</li>
<li>REPAæ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›çš„ç”Ÿæˆè´¨é‡ï¼Œä½¿ç”¨åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼çš„FIDä¸º1.42ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0be7be5cef958a6c93de091e25289856.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c6e26ab161a85109c401393e9e01f2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0acb44ce968a47a3fac81c18243170a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c347d9d0597ecbdc0251cdb81e4526be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33f86a298f407419a1b812c88535e044.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Dose-aware-Diffusion-Model-for-3D-PET-Image-Denoising-Multi-institutional-Validation-with-Reader-Study-and-Real-Low-dose-Data"><a href="#Dose-aware-Diffusion-Model-for-3D-PET-Image-Denoising-Multi-institutional-Validation-with-Reader-Study-and-Real-Low-dose-Data" class="headerlink" title="Dose-aware Diffusion Model for 3D PET Image Denoising:   Multi-institutional Validation with Reader Study and Real Low-dose Data"></a>Dose-aware Diffusion Model for 3D PET Image Denoising:   Multi-institutional Validation with Reader Study and Real Low-dose Data</h2><p><strong>Authors:Huidong Xie, Weijie Gan, Reimund Bayerlein, Bo Zhou, Ming-Kai Chen, Michal Kulon, Annemarie Boustani, Kuan-Yin Ko, Der-Shiun Wang, Benjamin A. Spencer, Wei Ji, Xiongchao Chen, Qiong Liu, Xueqi Guo, Menghua Xia, Yinchi Zhou, Hui Liu, Liang Guo, Hongyu An, Ulugbek S. Kamilov, Hanzhong Wang, Biao Li, Axel Rominger, Kuangyu Shi, Ge Wang, Ramsey D. Badawi, Chi Liu</strong></p>
<p>Reducing scan times, radiation dose, and enhancing image quality for lower-performance scanners, are critical in low-dose PET imaging. Deep learning techniques have been investigated for PET image denoising. However, existing models have often resulted in compromised image quality when achieving low-count&#x2F;low-dose PET and have limited generalizability to different image noise-levels, acquisition protocols, and patient populations. Recently, diffusion models have emerged as the new state-of-the-art generative model to generate high-quality samples and have demonstrated strong potential for medical imaging tasks. However, for low-dose PET imaging, existing diffusion models failed to generate consistent 3D reconstructions, unable to generalize across varying noise-levels, often produced visually-appealing but distorted image details, and produced images with biased tracer uptake. Here, we develop DDPET-3D, a dose-aware diffusion model for 3D low-dose PET imaging to address these challenges. Collected from 4 medical centers globally with different scanners and clinical protocols, we evaluated the proposed model using a total of 9,783 18F-FDG studies with low-dose levels ranging from 1% to 50%. With a cross-center, cross-scanner validation, the proposed DDPET-3D demonstrated its potential to generalize to different low-dose levels, different scanners, and different clinical protocols. As confirmed with reader studies performed by board-certified nuclear medicine physicians, experienced readers judged the images to be similar or superior to the full-dose images and previous DL baselines based on qualitative visual impression. Lesion-level quantitative accuracy was evaluated using a Monte Carlo simulation study and a lesion segmentation network. The presented results show the potential to achieve low-dose PET while maintaining image quality. Real low-dose scans was also included for evaluation. </p>
<blockquote>
<p>åœ¨ä½å‰‚é‡PETæˆåƒä¸­ï¼Œå‡å°‘æ‰«ææ—¶é—´ã€è¾å°„å‰‚é‡å¹¶å¢å¼ºä½æ€§èƒ½æ‰«æä»ªçš„å›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯å·²è¢«ç”¨äºPETå›¾åƒçš„é™å™ªå¤„ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨è¾¾åˆ°ä½è®¡æ•°&#x2F;ä½å‰‚é‡PETæ—¶å¾€å¾€ç‰ºç‰²äº†å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”åœ¨åº”å¯¹ä¸åŒå›¾åƒå™ªå£°æ°´å¹³ã€é‡‡é›†åè®®å’Œæ‚£è€…ç¾¤ä½“æ–¹é¢çš„é€šç”¨æ€§æœ‰é™ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºæ–°çš„æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å‡ºç°ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œå¹¶å·²åœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºä½å‰‚é‡PETæˆåƒï¼Œç°æœ‰æ‰©æ•£æ¨¡å‹æ— æ³•ç”Ÿæˆä¸€è‡´çš„3Dé‡å»ºå›¾åƒï¼Œæ— æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¹‹é—´é€šç”¨ï¼Œç»å¸¸äº§ç”Ÿè§†è§‰ä¸Šå¸å¼•äººä½†æ‰­æ›²çš„å›¾åƒç»†èŠ‚ï¼Œä»¥åŠå¸¦æœ‰åè§ç¤ºè¸ªç‰©æ‘„å–çš„å›¾åƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼€å‘äº†DDPET-3Dï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹3Dä½å‰‚é‡PETæˆåƒçš„å‰‚é‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹æ˜¯ä»å…¨çƒ4ä¸ªåŒ»ç–—ä¸­å¿ƒæ”¶é›†çš„ï¼Œä½¿ç”¨ä¸åŒçš„æ‰«æä»ªå’Œä¸´åºŠåè®®ã€‚æˆ‘ä»¬ä½¿ç”¨æ€»å…±9,783ä¾‹çš„18F-FDGç ”ç©¶è¯„ä¼°äº†æ‰€æå‡ºæ¨¡å‹ï¼Œä½å‰‚é‡æ°´å¹³èŒƒå›´ä»1%åˆ°50%ã€‚é€šè¿‡è·¨ä¸­å¿ƒã€è·¨æ‰«æä»ªéªŒè¯ï¼ŒDDPET-3Dæ¨¡å‹æ˜¾ç¤ºäº†å¯¹ä¸åŒä½å‰‚é‡æ°´å¹³ã€ä¸åŒæ‰«æä»ªå’Œä¸åŒä¸´åºŠåè®®çš„é€šç”¨æ€§æ½œåŠ›ã€‚ç»æ ¸åŒ»å­¦è‘£äº‹ä¼šè®¤è¯åŒ»å¸ˆè¿›è¡Œçš„è¯»è€…ç ”ç©¶è¯å®ï¼Œç»éªŒä¸°å¯Œçš„è¯»è€…è®¤ä¸ºå›¾åƒä¸å…¨å‰‚é‡å›¾åƒå’Œä¹‹å‰çš„æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•ç›¸ä¼¼æˆ–æ›´ä¼˜ç§€ï¼ŒåŸºäºå®šæ€§è§†è§‰å°è±¡ã€‚ç—…å˜æ°´å¹³çš„å®šé‡å‡†ç¡®æ€§æ˜¯é€šè¿‡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç ”ç©¶å’Œç—…å˜åˆ†å‰²ç½‘ç»œè¿›è¡Œè¯„ä¼°çš„ã€‚æ‰€å‘ˆç°çš„ç»“æœæ˜¾ç¤ºäº†åœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶å®ç°ä½å‰‚é‡PETçš„æ½œåŠ›ã€‚è¿˜åŒ…æ‹¬çœŸå®çš„ä½å‰‚é‡æ‰«æç”¨äºè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12996v3">PDF</a> 18 Pages, 16 Figures, 5 Tables. Paper under review. First-place Freek   J. Beekman Young Investigator Award at SNMMI 2024. Code available after paper   publication. arXiv admin note: substantial text overlap with arXiv:2311.04248</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é™ä½æ‰«ææ—¶é—´ã€å‡å°‘è¾å°„å‰‚é‡å¹¶æå‡å›¾åƒè´¨é‡å¯¹äºæ€§èƒ½è¾ƒä½çš„æ‰«æä»ªæ¥è¯´è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å‰‚é‡PETæˆåƒä¸­ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æŠ€æœ¯å·²è¢«ç ”ç©¶ç”¨äºPETå›¾åƒå»å™ªï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å®ç°ä½è®¡æ•°&#x2F;ä½å‰‚é‡PETæ—¶å¾€å¾€ç‰ºç‰²äº†å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”åœ¨åº”å¯¹ä¸åŒå›¾åƒå™ªå£°æ°´å¹³ã€é‡‡é›†åè®®å’Œæ‚£è€…ç¾¤ä½“æ—¶å…¶é€šç”¨æ€§æœ‰é™ã€‚æ‰©æ•£æ¨¡å‹ä½œä¸ºæœ€æ–°æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œå·²å±•ç°å‡ºç”¨äºåŒ»å­¦æˆåƒä»»åŠ¡çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºä½å‰‚é‡PETæˆåƒï¼Œç°æœ‰æ‰©æ•£æ¨¡å‹éš¾ä»¥ç”Ÿæˆä¸€è‡´çš„3Dé‡å»ºå›¾åƒï¼Œæ— æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸Šé€šç”¨ï¼Œä¸”å¸¸äº§ç”Ÿè§†è§‰æ•ˆæœå¥½ä½†ç»†èŠ‚å¤±çœŸçš„å›¾åƒï¼Œä»¥åŠå¸¦æœ‰åå·®çš„ç¤ºè¸ªç‰©æ‘„å–å›¾åƒã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†DDPET-3Dï¼Œä¸€ç§é¢å‘3Dä½å‰‚é‡PETæˆåƒçš„å‰‚é‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†æ¥è‡ªå…¨çƒ4ä¸ªåŒ»ç–—ä¸­å¿ƒçš„å…±è®¡9,783ä¸ªä½å‰‚é‡æ°´å¹³ä»‹äº1%è‡³50%çš„18F-FDGç ”ç©¶æ•°æ®ã€‚ç»è¿‡è·¨ä¸­å¿ƒã€è·¨æ‰«æä»ªéªŒè¯ï¼ŒDDPET-3Dæ˜¾ç¤ºå‡ºåœ¨ä¸åŒä½å‰‚é‡æ°´å¹³ã€ä¸åŒæ‰«æä»ªå’Œä¸åŒä¸´åºŠåè®®ä¸Šçš„é€šç”¨æ€§ã€‚æ ¸åŒ»å­¦ä¸“å®¶è¿›è¡Œçš„è¯»è€…ç ”ç©¶è¡¨æ˜ï¼Œç»éªŒä¸°å¯Œçš„è¯»è€…è®¤ä¸ºå›¾åƒä¸å…¨å‰‚é‡å›¾åƒå’Œä¹‹å‰çš„æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•ç›¸ä¼¼æˆ–æ›´ä¼˜è¶Šï¼ŒåŸºäºå®šæ€§è§†è§‰å°è±¡è¿›è¡Œè¯„ä»·ã€‚ç—…å˜æ°´å¹³çš„å®šé‡å‡†ç¡®æ€§é€šè¿‡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç ”ç©¶å’Œç—…å˜åˆ†å‰²ç½‘ç»œè¿›è¡Œè¯„ä¼°ã€‚æ‰€å‘ˆç°çš„ç»“æœå±•ç¤ºäº†å®ç°ä½å‰‚é‡PETçš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡çš„æ½œåŠ›ã€‚è¿˜åŒ…æ‹¬çœŸå®ä½å‰‚é‡æ‰«æç”¨äºè¯„ä¼°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½å‰‚é‡PETæˆåƒä¸­ï¼Œé™ä½æ‰«ææ—¶é—´ã€è¾å°„å‰‚é‡å’Œæå‡å›¾åƒè´¨é‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ€§èƒ½è¾ƒä½çš„æ‰«æä»ªã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä½å‰‚é‡PETæˆåƒä¸­å¸¸ç‰ºç‰²å›¾åƒè´¨é‡ï¼Œä¸”ç¼ºä¹å¯¹ä¸åŒå™ªå£°æ°´å¹³ã€é‡‡é›†åè®®å’Œæ‚£è€…ç¾¤ä½“çš„é€šç”¨æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºæœ€æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>DDPET-3Dæ¨¡å‹æ˜¯è§£å†³ä½å‰‚é‡PETæˆåƒæŒ‘æˆ˜çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å‰‚é‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>DDPET-3Dæ¨¡å‹åœ¨ä¸åŒä½å‰‚é‡æ°´å¹³ã€ä¸åŒæ‰«æä»ªå’Œä¸åŒä¸´åºŠåè®®ä¸Šå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
<li>è¯»è€…ç ”ç©¶è¡¨æ˜ï¼ŒDDPET-3Dç”Ÿæˆçš„å›¾åƒåœ¨è§†è§‰å°è±¡ä¸Šä¸å…¨å‰‚é‡å›¾åƒç›¸ä¼¼æˆ–æ›´ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.12996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d2662b93c62a4adf374b7daf9e2c75f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a79acdea2c28cbc5fe3cfff24748549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d006cf895a7f009d09dcc319062aaae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aae1d177ef2627737a0004bc405d7a2e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-646273fa036c24b95a127a7dc987a8f0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  RA-NeRF Robust Neural Radiance Field Reconstruction with Accurate   Camera Pose Estimation under Complex Trajectories
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
