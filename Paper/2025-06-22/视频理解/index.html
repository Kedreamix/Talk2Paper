<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  EVA02-AT Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-eb7ecb96a18e3cb871365015eeae8d4b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    30 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="EVA02-AT-Egocentric-Video-Language-Understanding-with-Spatial-Temporal-Rotary-Positional-Embeddings-and-Symmetric-Optimization"><a href="#EVA02-AT-Egocentric-Video-Language-Understanding-with-Spatial-Temporal-Rotary-Positional-Embeddings-and-Symmetric-Optimization" class="headerlink" title="EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization"></a>EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization</h2><p><strong>Authors:Xiaoqi Wang, Yi Wang, Lap-Pui Chau</strong></p>
<p>Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/xqwang14/EVA02-AT">https://github.com/xqwang14/EVA02-AT</a> . </p>
<blockquote>
<p>è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è¯­è¨€ç†è§£éœ€è¦é«˜æ•ˆç‡ä¸”ç²¾ç¡®çš„æ—¶ç©ºå»ºæ¨¡ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰ç”±äºå¤šé˜¶æ®µé¢„è®­ç»ƒç®¡é“è€Œäº§ç”Ÿçš„è¿‡é«˜çš„é¢„è®­ç»ƒæˆæœ¬ï¼›2ï¼‰ç”±äºæ‰‹åŠ¨åˆ†å‰²çš„3Dæ—‹è½¬ä½ç½®åµŒå…¥è€Œå¯¼è‡´çš„æ—¶ç©ºç¼–ç æ— æ•ˆï¼Œè¿™é˜»ç¢äº†ç‰¹å¾äº¤äº’ï¼›3ï¼‰è½¯æ ‡ç­¾å¤šå®ä¾‹æ£€ç´¢ä¸­çš„å­¦ä¹ ç›®æ ‡ä¸ç²¾ç¡®ï¼Œå¿½ç•¥äº†è´Ÿå¯¹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä»»åŠ¡çš„EVA02åŸºç¡€è§†é¢‘è¯­è¨€åŸºç¡€æ¨¡å‹å¥—ä»¶EVA02-ATã€‚EVA02-ATé¦–å…ˆé€šè¿‡å•é˜¶æ®µé¢„è®­ç»ƒæœ‰æ•ˆåœ°å°†åŸºäºå›¾åƒçš„CLIPæ¨¡å‹è½¬ç§»åˆ°ç»Ÿä¸€è§†é¢‘ç¼–ç å™¨ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ²¡æœ‰å°†æ—‹è½¬ä½ç½®åµŒå…¥åº”ç”¨äºå­¤ç«‹çš„ç»´åº¦ï¼Œè€Œæ˜¯å¼•å…¥äº†æ—¶ç©ºæ—‹è½¬ä½ç½®åµŒå…¥ä»¥åŠè”åˆæ³¨æ„åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¯¹æ•´ä¸ªéšè—ç»´åº¦è¿›è¡Œæ—¶ç©ºä¿¡æ¯ç¼–ç ã€‚è¿™ç§æ—¶ç©ºç‰¹å¾çš„è”åˆç¼–ç ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è·¨è½´å…³ç³»ï¼Œè¿™å¯¹äºå‡†ç¡®å»ºæ¨¡è§†é¢‘ä¸­çš„è¿åŠ¨å’Œäº¤äº’è‡³å…³é‡è¦ã€‚ç¬¬ä¸‰ï¼Œé’ˆå¯¹å¤šå®ä¾‹è§†é¢‘è¯­è¨€æ£€ç´¢ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹ç§°å¤šç›¸ä¼¼æ€§ï¼ˆSMSï¼‰æŸå¤±å’Œä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºæ­£å‘å’Œè´Ÿå‘å¯¹æ¨è¿›æ‰€æœ‰è½¯æ ‡ç­¾ï¼Œæä¾›æ›´ç²¾ç¡®çš„å­¦ä¹ ç›®æ ‡ã€‚åœ¨Ego4Dã€EPIC-Kitchens-100å’ŒCharades-Egoçš„é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEVA02-ATåœ¨å¤šç§è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è¯­è¨€ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ä¸”å‚æ•°æ›´å°‘ã€‚ä½¿ç”¨æˆ‘ä»¬SMSæŸå¤±çš„æ¨¡å‹åœ¨å¤šå®ä¾‹æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xqwang14/EVA02-AT%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xqwang14/EVA02-ATå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14356v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»»åŠ¡çš„EVA02-ATè§†é¢‘è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè¿‡é«˜çš„é¢„è®­ç»ƒæˆæœ¬ã€å› æ‰‹åŠ¨åˆ†å‰²çš„3Dæ—‹è½¬ä½ç½®åµŒå…¥è€Œå¯¼è‡´çš„ç©ºé—´æ—¶é—´ç¼–ç æ— æ•ˆä»¥åŠåœ¨è½¯æ ‡ç­¾å¤šå®ä¾‹æ£€ç´¢ä¸­å¿½ç•¥è´Ÿå¯¹å…³è”çš„ä¸ç²¾ç¡®å­¦ä¹ ç›®æ ‡ã€‚EVA02-ATé€šè¿‡å•é˜¶æ®µé¢„è®­ç»ƒï¼Œå°†å›¾åƒåŸºç¡€çš„CLIPæ¨¡å‹é«˜æ•ˆè½¬åŒ–ä¸ºç»Ÿä¸€è§†é¢‘ç¼–ç å™¨ã€‚åŒæ—¶å¼•å…¥æ—¶ç©ºæ—‹è½¬ä½ç½®åµŒå…¥å’Œè”åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆç¼–ç æ•´ä¸ªéšè—ç»´åº¦çš„æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶å­¦ä¹ è§†é¢‘ä¸­çš„è·¨è½´å…³ç³»ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹å¤šå®ä¾‹è§†é¢‘è¯­è¨€æ£€ç´¢ä»»åŠ¡ï¼Œå¼•å…¥å¯¹ç§°å¤šç›¸ä¼¼æ€§ï¼ˆSMSï¼‰æŸå¤±å’Œæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œä¸ºæ­£è´Ÿæ ·æœ¬æä¾›æ›´ç²¾ç¡®çš„å­¦ä¹ ç›®æ ‡ã€‚åœ¨Ego4Dã€EPIC-Kitchens-100å’ŒCharades-Egoæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®å®éªŒè¡¨æ˜ï¼ŒEVA02-ATåœ¨å¤šç§ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”å‚æ•°æ›´å°‘ã€‚ä½¿ç”¨æˆ‘ä»¬SMSæŸå¤±çš„æ¨¡å‹åœ¨å¤šå®ä¾‹æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ul>
<li>EVA02-ATæ¨¡å‹è§£å†³äº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å•é˜¶æ®µé¢„è®­ç»ƒï¼Œå°†å›¾åƒæ¨¡å‹è½¬åŒ–ä¸ºè§†é¢‘ç¼–ç å™¨ï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>å¼•å…¥æ—¶ç©ºæ—‹è½¬ä½ç½®åµŒå…¥å’Œè”åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆç¼–ç æ—¶ç©ºä¿¡æ¯å¹¶å­¦ä¹ è·¨è½´å…³ç³»ã€‚</li>
<li>é’ˆå¯¹å¤šå®ä¾‹è§†é¢‘è¯­è¨€æ£€ç´¢ä»»åŠ¡ï¼Œå¼•å…¥å¯¹ç§°å¤šç›¸ä¼¼æ€§ï¼ˆSMSï¼‰æŸå¤±å’Œæ–°çš„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEVA02-ATå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šå®ä¾‹æ£€ç´¢ä»»åŠ¡ä¸Šã€‚</li>
<li>æ¨¡å‹å’Œä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-12ae8b2c4cfe2017299f82086376fce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1838e129fadd3c9f74f376e8cd4e71d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eed7a005b153432459b7080a55e28758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae47a1ee6e3feb118e1c4bc36dba8dbc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AdaVideoRAG-Omni-Contextual-Adaptive-Retrieval-Augmented-Efficient-Long-Video-Understanding"><a href="#AdaVideoRAG-Omni-Contextual-Adaptive-Retrieval-Augmented-Efficient-Long-Video-Understanding" class="headerlink" title="AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long   Video Understanding"></a>AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long   Video Understanding</h2><p><strong>Authors:Zhucun Xue, Jiangning Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao</strong></p>
<p>Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos use static retrieval strategies, leading to inefficiencies for simple queries and information loss for complex tasks. To address this, we propose AdaVideoRAG, a novel framework that dynamically adapts retrieval granularity based on query complexity using a lightweight intent classifier. Our framework employs an Omni-Knowledge Indexing module to build hierarchical databases from text (captions, ASR, OCR), visual features, and semantic graphs, enabling optimal resource allocation across tasks. We also introduce the HiVU benchmark for comprehensive evaluation. Experiments demonstrate improved efficiency and accuracy for long-video understanding, with seamless integration into existing MLLMs. AdaVideoRAG establishes a new paradigm for adaptive retrieval in video analysis. Codes will be open-sourced at <a target="_blank" rel="noopener" href="https://github.com/xzc-zju/AdaVideoRAG">https://github.com/xzc-zju/AdaVideoRAG</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå›ºå®šçš„ä¸Šä¸‹æ–‡çª—å£å’Œè–„å¼±çš„é•¿æœŸä¾èµ–å»ºæ¨¡ï¼Œåœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´å›°éš¾ã€‚ç°æœ‰çš„è§†é¢‘æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ä½¿ç”¨é™æ€æ£€ç´¢ç­–ç•¥ï¼Œå¯¼è‡´ç®€å•æŸ¥è¯¢æ•ˆç‡ä½ä¸‹å’Œå¤æ‚ä»»åŠ¡ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdaVideoRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„æ„å›¾åˆ†ç±»å™¨æ¥æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ£€ç´¢çš„ç²’åº¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨Omni-Knowledgeç´¢å¼•æ¨¡å—ï¼Œä»æ–‡æœ¬ï¼ˆå­—å¹•ã€ASRã€OCRï¼‰ã€è§†è§‰ç‰¹å¾å’Œè¯­ä¹‰å›¾æ„å»ºåˆ†å±‚æ•°æ®åº“ï¼Œå®ç°ä»»åŠ¡é—´çš„æœ€ä½³èµ„æºé…ç½®ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†HiVUåŸºå‡†æµ‹è¯•ï¼Œä»¥ä¾¿è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æé«˜é•¿è§†é¢‘ç†è§£æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰MLLMsä¸­ã€‚AdaVideoRAGä¸ºè§†é¢‘åˆ†æä¸­è‡ªé€‚åº”æ£€ç´¢å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xzc-zju/AdaVideoRAG%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/xzc-zju/AdaVideoRAGä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13589v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´çš„å›ºå®šä¸Šä¸‹æ–‡çª—å£å’Œé•¿æœŸä¾èµ–å»ºæ¨¡å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶AdaVideoRAGã€‚è¯¥æ¡†æ¶èƒ½æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ£€ç´¢ç²’åº¦ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§æ„å›¾åˆ†ç±»å™¨ã€‚é€šè¿‡Omni-Knowledge Indexingæ¨¡å—å»ºç«‹æ–‡æœ¬ï¼ˆå­—å¹•ã€ASRã€OCRï¼‰ã€è§†è§‰ç‰¹å¾å’Œè¯­ä¹‰å›¾çš„åˆ†å±‚æ•°æ®åº“ï¼Œå®ç°ä»»åŠ¡é—´çš„æœ€ä½³èµ„æºé…ç½®ã€‚å¼•å…¥HiVUåŸºå‡†æµ‹è¯•è¿›è¡Œç»¼åˆè¯„ä»·ï¼Œå®éªŒè¡¨æ˜è¯¥æ¡†æ¶åœ¨æé«˜é•¿è§†é¢‘ç†è§£æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä¸ºè§†é¢‘åˆ†æä¸­çš„è‡ªé€‚åº”æ£€ç´¢æ ‘ç«‹äº†æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaVideoRAGè§£å†³äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´çš„å›ºå®šä¸Šä¸‹æ–‡çª—å£å’Œé•¿æœŸä¾èµ–å»ºæ¨¡éš¾é¢˜ã€‚</li>
<li>AdaVideoRAGèƒ½åŠ¨æ€é€‚åº”æŸ¥è¯¢å¤æ‚æ€§å¹¶è°ƒæ•´æ£€ç´¢ç²’åº¦ï¼Œä½¿ç”¨è½»é‡çº§æ„å›¾åˆ†ç±»å™¨å®ç°è¿™ä¸€åŠŸèƒ½ã€‚</li>
<li>Omni-Knowledge Indexingæ¨¡å—ç”¨äºå»ºç«‹åŒ…å«æ–‡æœ¬ã€è§†è§‰ç‰¹å¾å’Œè¯­ä¹‰å›¾çš„åˆ†å±‚æ•°æ®åº“ï¼Œå®ç°ä»»åŠ¡é—´çš„æœ€ä½³èµ„æºé…ç½®ã€‚</li>
<li>AdaVideoRAGå¼•å…¥HiVUåŸºå‡†æµ‹è¯•è¿›è¡Œç»¼åˆè¯„ä»·ï¼Œå±•ç¤ºå…¶åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„ä¼˜åŠ¿å’Œæ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>AdaVideoRAGä¸ºè§†é¢‘åˆ†æä¸­çš„è‡ªé€‚åº”æ£€ç´¢æ ‘ç«‹äº†æ–°èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9bd8aaed6a9307197e9320858b3cc43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a2bac9593576cdee77146a68ff7b078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee373f654d37052f7226d3f5c018fcb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MambaMia-A-State-Space-Model-Based-Compression-for-Efficient-Video-Understanding-in-Large-Multimodal-Models"><a href="#MambaMia-A-State-Space-Model-Based-Compression-for-Efficient-Video-Understanding-in-Large-Multimodal-Models" class="headerlink" title="MambaMia: A State-Space-Model-Based Compression for Efficient Video   Understanding in Large Multimodal Models"></a>MambaMia: A State-Space-Model-Based Compression for Efficient Video   Understanding in Large Multimodal Models</h2><p><strong>Authors:Geewook Kim, Minjoon Seo</strong></p>
<p>We propose an efficient framework to compress multiple video-frame features before feeding them into large multimodal models, thereby mitigating the severe token explosion arising from long or dense videos. Our design leverages a bidirectional state-space-based block equipped with a gated skip connection and a learnable weighted-average pooling mechanism applied to periodically inserted learned queries. This structure enables hierarchical downsampling across both spatial and temporal dimensions, preserving performance in a cost-effective manner. Across challenging long and dense video understanding tasks, our approach demonstrates competitive results against state-of-the-art models, while significantly reducing overall token budget. Notably, replacing our proposed state-space block with a conventional Transformer results in substantial performance degradation, highlighting the advantages of state-space modeling for effectively compressing multi-frame video data. Our framework emphasizes resource-conscious efficiency, making it practical for real-world deployments. We validate its scalability and generality across multiple benchmarks, achieving the dual objectives of efficient resource usage and comprehensive video understanding. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºåœ¨å°†å¤šå¸§è§†é¢‘ç‰¹å¾è¾“å…¥å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¹‹å‰å¯¹å…¶è¿›è¡Œå‹ç¼©ï¼Œä»è€Œç¼“è§£ç”±é•¿è§†é¢‘æˆ–å¯†é›†è§†é¢‘å¼•èµ·çš„ä¸¥é‡ä»¤ç‰Œçˆ†ç‚¸é—®é¢˜ã€‚æˆ‘ä»¬çš„è®¾è®¡åˆ©ç”¨äº†ä¸€ä¸ªåŸºäºåŒå‘çŠ¶æ€ç©ºé—´çš„å—ï¼Œè¯¥å—é…å¤‡äº†é—¨æ§è·³è¿‡è¿æ¥å’Œå¯å­¦ä¹ çš„åŠ æƒå¹³å‡æ± åŒ–æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶åº”ç”¨äºå®šæœŸæ’å…¥çš„å­¦ä¹ æŸ¥è¯¢ã€‚è¿™ç§ç»“æ„èƒ½å¤Ÿåœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šå®ç°åˆ†å±‚ä¸‹é‡‡æ ·ï¼Œä»¥ç»æµçš„æ–¹å¼ä¿æŒæ€§èƒ½ã€‚åœ¨é¢å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘å’Œå¯†é›†è§†é¢‘ç†è§£ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”è¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ€»ä½“ä»¤ç‰Œé¢„ç®—ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”¨ä¼ ç»Ÿçš„Transformeræ›¿æ¢æˆ‘ä»¬æå‡ºçš„çŠ¶æ€ç©ºé—´å—ä¼šå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œè¿™çªå‡ºäº†çŠ¶æ€ç©ºé—´å»ºæ¨¡åœ¨æœ‰æ•ˆå‹ç¼©å¤šå¸§è§†é¢‘æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ³¨é‡èµ„æºæ„è¯†æ•ˆç‡ï¼Œä½¿å…¶é€‚ç”¨äºå®é™…éƒ¨ç½²ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶å¯æ‰©å±•æ€§å’Œæ™®éæ€§ï¼Œå®ç°äº†æœ‰æ•ˆåˆ©ç”¨èµ„æºå’Œå…¨é¢ç†è§£è§†é¢‘çš„åŒé‡ç›®æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13564v1">PDF</a> 17 pages, 5 figures</p>
<p><strong>Summary</strong><br>æä¾›çš„æ–‡æœ¬æè¿°äº†ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå‹ç¼©å¤šå¸§è§†é¢‘ç‰¹å¾ï¼Œç„¶åå°†å…¶è¾“å…¥å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä»è€Œè§£å†³é•¿æˆ–å¯†é›†è§†é¢‘å¼•èµ·çš„ä¸¥é‡ä»¤ç‰Œçˆ†ç‚¸é—®é¢˜ã€‚è¯¥è®¾è®¡åˆ©ç”¨åŒå‘çŠ¶æ€ç©ºé—´å—ï¼Œé…å¤‡é—¨æ§è·³è·ƒè¿æ¥å’Œå¯å­¦ä¹ çš„åŠ æƒå¹³å‡æ± åŒ–æœºåˆ¶ï¼Œåº”ç”¨äºå®šæœŸæ’å…¥çš„å­¦ä¹ æŸ¥è¯¢ã€‚æ­¤ç»“æ„å¯åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šå®ç°åˆ†å±‚ä¸‹é‡‡æ ·ï¼Œä»¥èŠ‚çº¦æˆæœ¬çš„æ–¹å¼ä¿æŒæ€§èƒ½ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿å’Œå¯†é›†è§†é¢‘ç†è§£ä»»åŠ¡æ–¹é¢ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰ç«äº‰åŠ›çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†æ€»ä½“ä»¤ç‰Œé¢„ç®—ã€‚ä½¿ç”¨å¸¸è§„è½¬æ¢å™¨æ›¿æ¢å»ºè®®çš„çŠ¶æ€ç©ºé—´å—ä¼šå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œè¿™å‡¸æ˜¾äº†çŠ¶æ€ç©ºé—´å»ºæ¨¡åœ¨æœ‰æ•ˆå‹ç¼©å¤šå¸§è§†é¢‘æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¯¥æ¡†æ¶æ³¨é‡èµ„æºèŠ‚çº¦æ•ˆç‡ï¼Œé€‚åˆå®é™…åº”ç”¨éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºä¸€ä¸ªé«˜æ•ˆæ¡†æ¶å‹ç¼©å¤šå¸§è§†é¢‘ç‰¹å¾ï¼Œè§£å†³é•¿æˆ–å¯†é›†è§†é¢‘çš„ä»¤ç‰Œçˆ†ç‚¸é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨åŒå‘çŠ¶æ€ç©ºé—´å—ï¼Œé…å¤‡é—¨æ§è·³è·ƒè¿æ¥å’ŒåŠ æƒå¹³å‡æ± åŒ–æœºåˆ¶ã€‚</li>
<li>å®ç°ç©ºé—´å’Œæ—¶é—´çš„åˆ†å±‚ä¸‹é‡‡æ ·ï¼Œä»¥èŠ‚çº¦æˆæœ¬çš„æ–¹å¼ä¿æŒæ€§èƒ½ã€‚</li>
<li>åœ¨é•¿è€Œå¯†é›†çš„è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ç«äº‰åŠ›ï¼Œæ˜¾è‘—å‡å°‘ä»¤ç‰Œé¢„ç®—ã€‚</li>
<li>æ›¿æ¢çŠ¶æ€ç©ºé—´å—ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå‡¸æ˜¾å…¶ä¼˜åŠ¿ã€‚</li>
<li>æ¡†æ¶æ³¨é‡èµ„æºèŠ‚çº¦æ•ˆç‡ï¼Œé€‚åˆå®é™…åº”ç”¨éƒ¨ç½²ã€‚</li>
<li>éªŒè¯äº†å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¯æ‰©å±•æ€§å’Œæ™®éæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-177cedfc7d160942a59a5b6583c2e910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5930a3b6336dcf8d8cbdd75f7ff95e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4d380370868fcc4dc0d1151b7d6c288.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a230c035bc64b92761799ceb3c67bfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f2bd8c76ec06ca9281b5b3ab0c65772.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VideoDeepResearch-Long-Video-Understanding-With-Agentic-Tool-Using"><a href="#VideoDeepResearch-Long-Video-Understanding-With-Agentic-Tool-Using" class="headerlink" title="VideoDeepResearch: Long Video Understanding With Agentic Tool Using"></a>VideoDeepResearch: Long Video Understanding With Agentic Tool Using</h2><p><strong>Authors:Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Ji-Rong Wen, Zhicheng Dou</strong></p>
<p>Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the taskâ€™s inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰å¯¹å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè¯¥ä»»åŠ¡çš„å›ºæœ‰å¤æ‚æ€§å’Œä¸Šä¸‹æ–‡çª—å£çº¦æŸã€‚æ™®éè®¤ä¸ºï¼Œè§£å†³LVUä»»åŠ¡éœ€è¦å…·æœ‰æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ã€å¼ºå¤§è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å’Œä¸“ä¸šé¢†åŸŸçŸ¥è¯†çš„åŸºç¡€MLLMsã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥VideoDeepResearchï¼Œä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç†è§£çš„æ–°å‹ä»£ç†æ¡†æ¶ï¼Œæ¥æŒ‘æˆ˜è¿™ä¸€æ™®éä¿¡å¿µã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…ä¾èµ–äºæ–‡æœ¬å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰å’Œæ¨¡å—åŒ–å¤šæ¨¡æ€å·¥å…·åŒ…ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ£€ç´¢å™¨å’Œè§†è§‰æ„ŸçŸ¥å™¨ï¼Œæ‰€æœ‰è¿™äº›åœ¨å®è·µä¸­éƒ½å¾ˆå®¹æ˜“è·å¾—ã€‚å¯¹äºæ¯ä¸ªLVUä»»åŠ¡ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ¨ç†åˆ¶å®šé—®é¢˜è§£å†³ç­–ç•¥ï¼ŒåŒæ—¶æœ‰é€‰æ‹©åœ°è®¿é—®å’Œåˆ©ç”¨å·¥å…·ä¸­çš„å…³é”®è§†é¢‘å†…å®¹ã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„LVUåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬MLVUã€Video-MMEå’ŒLVBenchã€‚ç»“æœè¡¨æ˜ï¼ŒVideoDeepResearchåœ¨ç°æœ‰çš„MLLMåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨MLVUï¼ˆæµ‹è¯•ï¼‰ã€LVBenchå’ŒLongVideoBenchä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†9.6%ã€6.6%å’Œ3.9%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä»£ç†ç³»ç»Ÿåœ¨å…‹æœLVUé—®é¢˜ä¸­çš„å…³é”®æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10821v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘æ·±åº¦ç ”ç©¶æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬æ¨ç†æ¨¡å‹å’Œå¤šæ¨¡æ€å·¥å…·è¿›è¡Œé•¿è§†é¢‘ç†è§£ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰åŸºçº¿æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä»…ä¾èµ–äºæ–‡æœ¬å¤§å‹æ¨ç†æ¨¡å‹ä¸æ¨¡å—åŒ–å¤šæ¨¡æ€å·¥å…·ç»„åˆï¼Œä¸ºè§£å†³é•¿è§†é¢‘ç†è§£é—®é¢˜æä¾›äº†æ–°çš„è§†è§’ã€‚åœ¨ä¸»æµé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘æ·±åº¦ç ”ç©¶ï¼ˆVideoDeepResearchï¼‰æ˜¯ä¸€ä¸ªæ–°å‹çš„æ™ºèƒ½æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºé•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ä¾èµ–å…·æœ‰æ‰©å±•ä¸Šä¸‹æ–‡çª—å£çš„åŸºç¡€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œè€Œæ˜¯ç»“åˆæ–‡æœ¬æ¨ç†æ¨¡å‹å’Œå¤šæ¨¡æ€å·¥å…·è¿›è¡Œè§†é¢‘ç†è§£ã€‚</li>
<li>VideoDeepResearché€šè¿‡é€‰æ‹©æ€§è®¿é—®å’Œåˆ©ç”¨è§†é¢‘å†…å®¹ï¼Œä¸ºæ¯ä¸€ä¸ªLVUä»»åŠ¡åˆ¶å®šé—®é¢˜è§£å†³ç­–ç•¥ã€‚</li>
<li>åœ¨å¤šä¸ªæµè¡Œçš„LVUåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬MLVUã€Video-MMEå’ŒLVBenchã€‚</li>
<li>VideoDeepResearchå®ç°äº†å¯¹ç°æœ‰MLLMåŸºçº¿çš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æŸäº›æµ‹è¯•ä¸­çš„è¡¨ç°è¶…è¿‡äº†å…ˆå‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-465c9a9349dfcc6d537b919862df36b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7ecb96a18e3cb871365015eeae8d4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb709d90f75fcc0532562114865668f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMsâ€™ abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMsâ€™ abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git">https://github.com/zli12321/VideoHallu.git</a>. </p>
<blockquote>
<p>åˆæˆè§†é¢‘ç”Ÿæˆå› å…¶çœŸå®æ„Ÿå’Œå¹¿æ³›åº”ç”¨è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ä»å®¹æ˜“è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹ã€‚è¿™å¼ºè°ƒäº†å¯¹å¯é å¼‚å¸¸æ£€æµ‹å™¨çš„éœ€æ±‚ï¼Œè¿™äº›æ£€æµ‹å™¨éœ€è¦ç†è§£è¿™äº›åŸç†ï¼Œå¹¶å¯¹å¹»è§‰å…·æœ‰é²æ£’æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoHalluï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è¶…è¿‡3000ä¸ªè§†é¢‘é—®ç­”å¯¹ç»„æˆçš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›é—®ç­”å¯¹æ¥è‡ªç”±Veo2ã€Soraå’ŒKlingç­‰æ¨¡å‹ç”Ÿæˆçš„åˆæˆè§†é¢‘ï¼Œä»¥åŠä¸ä¸“å®¶ç²¾å¿ƒåˆ¶ä½œçš„åç›´è§‰é—®ç­”ç›¸ç»“åˆï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè¿™äº›å¼‚å¸¸å¯¹äººç±»æ¥è¯´åœ¨æ„ŸçŸ¥ä¸Šæ˜¯æ˜æ˜¾çš„ï¼Œä½†ç”±äºè¯­è¨€å…ˆéªŒçŸ¥è¯†å¾€å¾€ä¼šäº§ç”Ÿå¹»è§‰ã€‚VideoHalluåœ¨è·¨å¯¹é½ã€ä¸€è‡´æ€§ã€å¸¸è¯†å’Œç‰©ç†æ–¹é¢è¯„ä¼°MLLMsçš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹SOTA MLLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPT-4oã€Gemini-2.5-Proã€Qwen2.5-VLã€Video-R1å’ŒVideoChat-R1ã€‚æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨MVBenchå’ŒMovieChatç­‰ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘ä¸­çš„åŸºäºç‰©ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ˜¾ç¤ºï¼Œä½¿ç”¨ç»„åˆè§†é¢‘é—®ç­”ä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†çš„æ•°æ®é›†è¿›è¡Œè¯¾ç¨‹å­¦ä¹ çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åè®­ç»ƒï¼Œå¯ä»¥æ”¹å–„MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´ï¼Œè¯æ˜äº†æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒå¯¹äºæé«˜å®ƒä»¬å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£çš„ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zli12321/VideoHallu.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆæˆè§†é¢‘ç”ŸæˆæŠ€æœ¯åœ¨å¼•èµ·å…³æ³¨çš„åŒæ—¶ï¼Œä»å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦å¼€å‘å¯é çš„å¼‚å¸¸æ£€æµ‹å™¨æ¥ç†è§£è¿™äº›åŸç†å¹¶æŠµæŠ—å¹»è§‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoHalluåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡3000ä¸ªè§†é¢‘QAå¯¹ï¼Œç”±æ¨¡å‹ç”Ÿæˆçš„åˆæˆè§†é¢‘ä¸ä¸“å®¶è®¾è®¡çš„åç›´è§‰QAç»„æˆï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥æ˜æ˜¾ä½†å¯¹äººç±»è€Œè¨€å´ç»å¸¸å› è¯­è¨€å…ˆéªŒè€Œäº§ç”Ÿå¹»è§‰çš„å¼‚å¸¸æ–¹é¢çš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ã€‚VideoHalluåŸºå‡†æµ‹è¯•æ¶µç›–äº†å¯¹MLLMsåœ¨å®šä½ã€ä¸€è‡´æ€§ã€å¸¸è¯†å’Œç‰©ç†æ–¹é¢çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬GPT-4oã€Gemini-2.5-Proã€Qwen2.5-VLç­‰åœ¨å†…çš„é¡¶å°–MLLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨è®¸å¤šç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼ˆå¦‚MVBenchå’ŒMovieChatï¼‰ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘çš„åŸºäºç‰©ç†å’Œå¸¸è¯†çš„æ¨ç†æ–¹é¢ä»å­˜åœ¨é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨ç»„åˆè§†é¢‘QAä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†çš„æ•°æ®é›†è¿›è¡Œè¯¾ç¨‹å­¦ä¹ çš„é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰åè®­ç»ƒï¼Œèƒ½æé«˜MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè¯æ˜äº†æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒå¯¹äºæé«˜ä»–ä»¬å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£çš„ä»·å€¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆæˆè§†é¢‘ç”ŸæˆæŠ€æœ¯è™½ç„¶é€¼çœŸä¸”åº”ç”¨å¹¿æ³›ï¼Œä½†å¸¸å¸¸è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹ã€‚</li>
<li>VideoHalluåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«åˆæˆè§†é¢‘ä¸­çš„å¼‚å¸¸ç°è±¡çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰é¡¶å°–çš„è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ¶‰åŠç‰©ç†å’Œå¸¸è¯†æ¨ç†çš„åˆæˆè§†é¢‘ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒï¼Œå¦‚ä½¿ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œåè®­ç»ƒï¼Œå¯ä»¥æé«˜è¯­è¨€æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®é›†ç»“åˆäº†è§†é¢‘QAä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>VideoHalluçš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºç ”ç©¶å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0733746e310ac120e88dee7a0bb4e33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c0e565c33c36de724192494cf59dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd791c7ebf56cd412d8edaf50d4d8b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c07288c94070b78a2e6183c618ed95a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4f30a27d12edd17a705e7e98a27eba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6eba42bf105f2f69700259d4c77c4224.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-Video-Understanding-with-16-Frames-Per-Second"><a href="#Improving-LLM-Video-Understanding-with-16-Frames-Per-Second" class="headerlink" title="Improving LLM Video Understanding with 16 Frames Per Second"></a>Improving LLM Video Understanding with 16 Frames Per Second</h2><p><strong>Authors:Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang</strong></p>
<p>Human vision is dynamic and continuous. However, in video understanding with multimodal large language models (LLMs), existing methods primarily rely on static features extracted from images sampled at a fixed low frame rate of frame-per-second (FPS) $\leqslant$2, leading to critical visual information loss. In this paper, we introduce F-16, the first multimodal LLM designed for high-frame-rate video understanding. By increasing the frame rate to 16 FPS and compressing visual tokens within each 1-second clip, F-16 efficiently captures dynamic visual features while preserving key semantic information. Experimental results demonstrate that higher frame rates considerably enhance video understanding across multiple benchmarks, providing a new approach to improving video LLMs beyond scaling model size or training data. F-16 achieves state-of-the-art performance among 7-billion-parameter video LLMs on both general and fine-grained video understanding benchmarks, such as Video-MME and TemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks, including high-speed sports analysis (\textit{e.g.}, basketball, football, gymnastics, and diving), outperforming SOTA proprietary visual models like GPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method for F-16 that enables highly efficient low-frame-rate inference without requiring model retraining. We will release the source code, model checkpoints, and data at \href{<a target="_blank" rel="noopener" href="https://github.com/bytedance/F-16%7D%7Bhttps://github.com/bytedance/F-16%7D">https://github.com/bytedance/F-16}{https://github.com/bytedance/F-16}</a>. </p>
<blockquote>
<p>äººç±»è§†è§‰æ˜¯åŠ¨æ€ä¸”è¿ç»­çš„ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘ç†è§£ä¸­ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä»¥å›ºå®šä½å¸§ç‡ï¼ˆFPSï¼‰â‰¤2é‡‡æ ·çš„å›¾åƒä¸­æå–çš„é™æ€ç‰¹å¾ï¼Œè¿™å¯¼è‡´äº†å…³é”®è§†è§‰ä¿¡æ¯çš„ä¸¢å¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†F-16ï¼Œå®ƒæ˜¯ä¸“ä¸ºé«˜å¸§ç‡è§†é¢‘ç†è§£è®¾è®¡çš„ç¬¬ä¸€æ¬¾å¤šæ¨¡æ€LLMã€‚é€šè¿‡æé«˜åˆ°16 FPSçš„å¸§ç‡å’Œå‹ç¼©æ¯ç§’å‰ªè¾‘ä¸­çš„è§†è§‰ä»¤ç‰Œï¼ŒF-16èƒ½å¤Ÿé«˜æ•ˆåœ°æ•æ‰åŠ¨æ€è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™å…³é”®è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæé«˜å¸§ç‡å¯ä»¥æ˜¾è‘—å¢å¼ºå¤šä¸ªåŸºå‡†æµ‹è¯•çš„è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œä¸ºæ”¹è¿›è§†é¢‘LLMæä¾›äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè€Œæ— éœ€æ‰©å¤§æ¨¡å‹è§„æ¨¡æˆ–å¢åŠ è®­ç»ƒæ•°æ®ã€‚F-16åœ¨é€šç”¨å’Œç²¾ç»†ç²’åº¦è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚Video-MMEå’ŒTemporalBenchã€‚æ­¤å¤–ï¼ŒF-16åœ¨å¤æ‚çš„æ—¶ç©ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬é«˜é€Ÿè¿åŠ¨åˆ†æï¼ˆä¾‹å¦‚ç¯®çƒã€è¶³çƒã€ä½“æ“å’Œè·³æ°´ï¼‰ï¼Œè¶…è¶Šäº†å¦‚GPT-4oå’ŒGemini-1.5-proç­‰æœ€å…ˆè¿›çš„ä¸“ä¸šè§†è§‰æ¨¡å‹ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºF-16å¼•å…¥äº†ä¸€ç§æ–°å‹è§£ç æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°åœ¨æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆä½å¸§ç‡æ¨ç†ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/bytedance/F-16">https://github.com/bytedance/F-16</a>å‘å¸ƒæºä»£ç ã€æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13956v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é«˜å¸§ç‡è§†é¢‘ç†è§£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹F-16ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä»å›ºå®šä½å¸§ç‡é‡‡æ ·çš„å›¾åƒä¸­æå–çš„é™æ€ç‰¹å¾ï¼Œå¯¼è‡´å…³é”®è§†è§‰ä¿¡æ¯ä¸¢å¤±ã€‚F-16é€šè¿‡å°†å¸§ç‡æé«˜åˆ°æ¯ç§’16å¸§å¹¶å‹ç¼©æ¯ç§’å‰ªè¾‘ä¸­çš„è§†è§‰ä»¤ç‰Œï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•è·åŠ¨æ€è§†è§‰ç‰¹å¾å¹¶ä¿ç•™å…³é”®è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæé«˜å¸§ç‡æ˜¾è‘—æé«˜äº†å¤šä¸ªåŸºå‡†æµ‹è¯•çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒF-16åœ¨ä¸€èˆ¬å’Œç²¾ç»†ç²’åº¦è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨å¤æ‚æ—¶ç©ºä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ€åä»‹ç»äº†ä¸€ç§ç”¨äºF-16çš„æ–°å‹è§£ç æ–¹æ³•ï¼Œå¯å®ç°é«˜æ•ˆä½å¸§ç‡æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•ä¸»è¦ä¾èµ–ä½å¸§ç‡å›¾åƒæå–çš„é™æ€ç‰¹å¾ï¼Œå¯¼è‡´è§†è§‰ä¿¡æ¯æŸå¤±ã€‚</li>
<li>F-16æ¨¡å‹è®¾è®¡ç”¨äºé«˜å¸§ç‡è§†é¢‘ç†è§£ï¼Œé€šè¿‡æé«˜å¸§ç‡å’Œå‹ç¼©è§†è§‰ä»¤ç‰Œæ¥æ•è·åŠ¨æ€è§†è§‰ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºé«˜å¸§ç‡æ˜¾è‘—æé«˜è§†é¢‘ç†è§£æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>F-16åœ¨ä¸€èˆ¬å’Œç²¾ç»†ç²’åº¦è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ—¶ç©ºä»»åŠ¡ä¸­ã€‚</li>
<li>F-16å¼•å…¥æ–°å‹è§£ç æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆä½å¸§ç‡æ¨ç†ã€‚</li>
<li>F-16æ¨¡å‹ä¼˜äºå…¶ä»–å…ˆè¿›è§†è§‰æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒGemini-1.5-proã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c07cc78354862fbd1048114d6dae1850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b0a3a1df007791ffbdc08c470cae99e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e950075907c714fd64f1eb8c13e9b396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-794b19a4dd8756ff5559d01e245652ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-611b0b0308c235eeb547dbfb3240b790.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38f5929df68dba2278b777751e577005.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-Understanding-with-Large-Language-Models-A-Survey"><a href="#Video-Understanding-with-Large-Language-Models-A-Survey" class="headerlink" title="Video Understanding with Large Language Models: A Survey"></a>Video Understanding with Large Language Models: A Survey</h2><p><strong>Authors:Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu</strong></p>
<p>With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding</a>. </p>
<blockquote>
<p>éšç€åœ¨çº¿è§†é¢‘å¹³å°çš„è“¬å‹ƒå‘å±•å’Œè§†é¢‘å†…å®¹çš„ä¸æ–­å¢åŠ ï¼Œå¯¹ç†Ÿç»ƒçš„è§†é¢‘ç†è§£å·¥å…·çš„éœ€æ±‚æ˜¾è‘—å¢åŠ ã€‚é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­è¨€å’Œå¤šåª’ä½“ä»»åŠ¡ä¸­çš„å“è¶Šèƒ½åŠ›ï¼Œè¿™ç¯‡ç»¼è¿°æä¾›äº†å…³äºå¦‚ä½•åˆ©ç”¨LLMï¼ˆè§†é¢‘LLMï¼‰è¿›è¡Œè§†é¢‘ç†è§£çš„æœ€æ–°è¿›å±•çš„è¯¦ç»†ä»‹ç»ã€‚Vid-LLMçš„æ–°å…´èƒ½åŠ›ä»¤äººæƒŠè®¶åœ°å…ˆè¿›ï¼Œå°¤å…¶æ˜¯å®ƒä»¬ç»“åˆå¸¸è¯†çŸ¥è¯†è¿›è¡Œçš„å¼€æ”¾å¼å¤šç²’åº¦ï¼ˆä¸€èˆ¬ã€æ—¶é—´å’Œæ—¶ç©ºï¼‰æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸ºæœªæ¥çš„è§†é¢‘ç†è§£æŒ‡æ˜äº†æœ‰å¸Œæœ›çš„é“è·¯ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†Vid-LLMçš„ç‹¬ç‰¹ç‰¹å¾å’Œèƒ½åŠ›ï¼Œå¹¶å°†è¿™äº›æ–¹æ³•åˆ†ä¸ºä¸‰ç±»ï¼šè§†é¢‘åˆ†æå™¨x LLMã€è§†é¢‘åµŒå…¥å™¨x LLMå’Œï¼ˆåˆ†æå™¨+åµŒå…¥å™¨ï¼‰x LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ ¹æ®LLMåœ¨Vid-LLMä¸­çš„åŠŸèƒ½ç¡®å®šäº†äº”ç§äºšå‹ï¼šLLMä½œä¸ºæ‘˜è¦å™¨ã€LLMä½œä¸ºç®¡ç†å™¨ã€LLMä½œä¸ºæ–‡æœ¬è§£ç å™¨ã€LLMä½œä¸ºå›å½’å™¨å’ŒLLMä½œä¸ºéšè—å±‚ã€‚æ­¤å¤–ï¼Œè¿™ç¯‡ç»¼è¿°è¿˜å¯¹Vid-LLMçš„ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚å®ƒè¿˜æ¢è®¨äº†Vid-LLMåœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨å¹¿æ³›æ€§ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œçš„è§†é¢‘ç†è§£æŒ‘æˆ˜ä¸­çš„å‡ºè‰²å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚æœ€åï¼Œå®ƒæ€»ç»“äº†ç°æœ‰Vid-LLMçš„å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding%E4%BB%93%E5%BA%93%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understandingä»“åº“æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.17432v5">PDF</a> Accepted by IEEE TCSVT</p>
<p><strong>Summary</strong></p>
<p>éšç€åœ¨çº¿è§†é¢‘å¹³å°çš„è“¬å‹ƒå‘å±•å’Œè§†é¢‘å†…å®¹çš„ä¸æ–­å¢åŠ ï¼Œå¯¹ç†Ÿç»ƒçš„è§†é¢‘ç†è§£å·¥å…·çš„éœ€æ±‚æ€¥å‰§å¢é•¿ã€‚é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œè¿™ç¯‡ç»¼è¿°æä¾›äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVid-LLMsï¼‰è¿›è¡Œè§†é¢‘ç†è§£çš„æœ€æ–°è¿›å±•æ¦‚è¿°ã€‚Vid-LLMsçš„æ–°å…´èƒ½åŠ›ä»¤äººæƒŠè®¶åœ°å…ˆè¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆå¸¸è¯†çŸ¥è¯†è¿›è¡Œçš„å¼€æ”¾å¤šç«¯ã€å¤šç²’åº¦ï¼ˆä¸€èˆ¬ã€æ—¶é—´å’Œæ—¶ç©ºï¼‰æ¨ç†æ–¹é¢ï¼Œä¸ºæœªæ¥çš„è§†é¢‘ç†è§£æŒ‡æ˜äº†æœ‰å‰æ™¯çš„é“è·¯ã€‚æœ¬æ–‡æ€»ç»“äº†Vid-LLMsåœ¨è§†é¢‘åˆ†æã€åµŒå…¥ã€ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€è¯„ä¼°æ–¹æ³•å’Œåº”ç”¨é¢†åŸŸçš„ç‰¹ç‚¹å’Œèƒ½åŠ›ã€‚æ›´å¤šä¿¡æ¯å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding%E4%BA%86%E8%A7%A3%E3%80%82">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understandingäº†è§£ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åœ¨çº¿è§†é¢‘å¹³å°å’Œè§†é¢‘å†…å®¹çš„å¢é•¿å¯¼è‡´å¯¹ç†Ÿç»ƒè§†é¢‘ç†è§£å·¥å…·çš„éœ€æ±‚æ€¥å‰§å¢åŠ ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå½¢æˆçš„æ–°å…´æŠ€æœ¯ç§°ä¸ºVid-LLMsã€‚</li>
<li>Vid-LLMså…·æœ‰å¼€æ”¾å¤šç«¯ã€å¤šç²’åº¦çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç»“åˆå¸¸è¯†çŸ¥è¯†ï¼Œä¸ºæœªæ¥çš„è§†é¢‘ç†è§£æä¾›äº†å‰æ™¯ã€‚</li>
<li>Vid-LLMsæœ‰ä¸‰ç§ä¸»è¦ç±»å‹ï¼šVideo Analyzer x LLMã€Video Embedder x LLMå’Œï¼ˆAnalyzer + Embedderï¼‰x LLMã€‚</li>
<li>LLMåœ¨Vid-LLMsä¸­çš„åŠŸèƒ½å¯åˆ†ä¸ºäº”ç§äºšå‹ï¼šä½œä¸ºæ€»ç»“è€…ã€ç®¡ç†è€…ã€æ–‡æœ¬è§£ç å™¨ã€å›å½’å™¨å’Œéšè—å±‚ã€‚</li>
<li>è¯¥ç»¼è¿°å…¨é¢ç ”ç©¶äº†Vid-LLMsçš„ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ–¹æ³•ã€‚</li>
<li>Vid-LLMsåœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œè¡¨ç°å‡ºæƒŠäººçš„å¯æ‰©å±•æ€§å’Œåœ¨ç°å®ä¸–ç•Œè§†é¢‘ç†è§£æŒ‘æˆ˜ä¸­çš„é€šç”¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.17432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b47633ead27881529092be3c47142aed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-720012d99a98acdd8cb9eb270093a53f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-998dc54077c52aa56de1d1877831ddc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b009ab17f18a2cefda5c17ad7d53d4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57befebd00bd8025139e3ba9f9bafd2b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6be101dced6ae52c25293095f5eb1857.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  OpenPath Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e543d07694b38146aa2793cb90a02bf7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Beyond Black Boxes Enhancing Interpretability of Transformers Trained   on Neural Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
