<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-06-22  EVA02-AT Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-eb7ecb96a18e3cb871365015eeae8d4b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-22-更新"><a href="#2025-06-22-更新" class="headerlink" title="2025-06-22 更新"></a>2025-06-22 更新</h1><h2 id="EVA02-AT-Egocentric-Video-Language-Understanding-with-Spatial-Temporal-Rotary-Positional-Embeddings-and-Symmetric-Optimization"><a href="#EVA02-AT-Egocentric-Video-Language-Understanding-with-Spatial-Temporal-Rotary-Positional-Embeddings-and-Symmetric-Optimization" class="headerlink" title="EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization"></a>EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal   Rotary Positional Embeddings and Symmetric Optimization</h2><p><strong>Authors:Xiaoqi Wang, Yi Wang, Lap-Pui Chau</strong></p>
<p>Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/xqwang14/EVA02-AT">https://github.com/xqwang14/EVA02-AT</a> . </p>
<blockquote>
<p>自我中心视频语言理解需要高效率且精确的时空建模。现有方法面临三个主要挑战：1）由于多阶段预训练管道而产生的过高的预训练成本；2）由于手动分割的3D旋转位置嵌入而导致的时空编码无效，这阻碍了特征交互；3）软标签多实例检索中的学习目标不精确，忽略了负对之间的相关性。在本文中，我们介绍了针对自我中心视频理解任务的EVA02基础视频语言基础模型套件EVA02-AT。EVA02-AT首先通过单阶段预训练有效地将基于图像的CLIP模型转移到统一视频编码器。其次，我们没有将旋转位置嵌入应用于孤立的维度，而是引入了时空旋转位置嵌入以及联合注意力，可以有效地对整个隐藏维度进行时空信息编码。这种时空特征的联合编码使模型能够学习跨轴关系，这对于准确建模视频中的运动和交互至关重要。第三，针对多实例视频语言检索任务，我们引入了对称多相似性（SMS）损失和一个新的训练框架，该框架为正向和负向对推进所有软标签，提供更精确的学习目标。在Ego4D、EPIC-Kitchens-100和Charades-Ego的零样本和微调设置下的广泛实验表明，EVA02-AT在多种自我中心视频语言任务中实现了最先进的性能且参数更少。使用我们SMS损失的模型在多实例检索基准测试上也显示出显著的性能提升。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/xqwang14/EVA02-AT%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xqwang14/EVA02-AT公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14356v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了针对以自我为中心的视频理解任务的EVA02-AT视频语言基础模型。该模型解决了现有方法的三个主要挑战：过高的预训练成本、因手动分割的3D旋转位置嵌入而导致的空间时间编码无效以及在软标签多实例检索中忽略负对关联的不精确学习目标。EVA02-AT通过单阶段预训练，将图像基础的CLIP模型高效转化为统一视频编码器。同时引入时空旋转位置嵌入和联合注意力机制，有效编码整个隐藏维度的时空信息，并学习视频中的跨轴关系。此外，针对多实例视频语言检索任务，引入对称多相似性（SMS）损失和新的训练框架，为正负样本提供更精确的学习目标。在Ego4D、EPIC-Kitchens-100和Charades-Ego数据集上的零样本和微调设置实验表明，EVA02-AT在多种以自我为中心的视频语言任务上实现了最先进的性能，并且参数更少。使用我们SMS损失的模型在多实例检索基准测试上也显示出显著的性能提升。</p>
<p><strong>要点</strong></p>
<ul>
<li>EVA02-AT模型解决了以自我为中心的视频理解任务中的三个主要挑战。</li>
<li>通过单阶段预训练，将图像模型转化为视频编码器，提高效率。</li>
<li>引入时空旋转位置嵌入和联合注意力机制，有效编码时空信息并学习跨轴关系。</li>
<li>针对多实例视频语言检索任务，引入对称多相似性（SMS）损失和新的训练框架。</li>
<li>在多个数据集上的实验表明，EVA02-AT实现了先进的性能，特别是在多实例检索任务上。</li>
<li>模型和代码已公开可用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-12ae8b2c4cfe2017299f82086376fce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1838e129fadd3c9f74f376e8cd4e71d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eed7a005b153432459b7080a55e28758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae47a1ee6e3feb118e1c4bc36dba8dbc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AdaVideoRAG-Omni-Contextual-Adaptive-Retrieval-Augmented-Efficient-Long-Video-Understanding"><a href="#AdaVideoRAG-Omni-Contextual-Adaptive-Retrieval-Augmented-Efficient-Long-Video-Understanding" class="headerlink" title="AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long   Video Understanding"></a>AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long   Video Understanding</h2><p><strong>Authors:Zhucun Xue, Jiangning Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao</strong></p>
<p>Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos use static retrieval strategies, leading to inefficiencies for simple queries and information loss for complex tasks. To address this, we propose AdaVideoRAG, a novel framework that dynamically adapts retrieval granularity based on query complexity using a lightweight intent classifier. Our framework employs an Omni-Knowledge Indexing module to build hierarchical databases from text (captions, ASR, OCR), visual features, and semantic graphs, enabling optimal resource allocation across tasks. We also introduce the HiVU benchmark for comprehensive evaluation. Experiments demonstrate improved efficiency and accuracy for long-video understanding, with seamless integration into existing MLLMs. AdaVideoRAG establishes a new paradigm for adaptive retrieval in video analysis. Codes will be open-sourced at <a target="_blank" rel="noopener" href="https://github.com/xzc-zju/AdaVideoRAG">https://github.com/xzc-zju/AdaVideoRAG</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）由于固定的上下文窗口和薄弱的长期依赖建模，在处理长视频时面临困难。现有的视频检索增强生成（RAG）方法使用静态检索策略，导致简单查询效率低下和复杂任务信息丢失。为了解决这一问题，我们提出了AdaVideoRAG，这是一个新的框架，它使用一个轻量级的意图分类器来根据查询的复杂性动态调整检索的粒度。我们的框架采用Omni-Knowledge索引模块，从文本（字幕、ASR、OCR）、视觉特征和语义图构建分层数据库，实现任务间的最佳资源配置。我们还引入了HiVU基准测试，以便进行全面评估。实验表明，该框架在提高长视频理解效率和准确性方面具有优势，并能无缝集成到现有MLLMs中。AdaVideoRAG为视频分析中自适应检索建立了新的范式。代码将在<a target="_blank" rel="noopener" href="https://github.com/xzc-zju/AdaVideoRAG%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/xzc-zju/AdaVideoRAG上开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13589v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对多模态大型语言模型在处理长视频时面临的固定上下文窗口和长期依赖建模困难的问题，提出了一种新型框架AdaVideoRAG。该框架能根据查询的复杂性动态调整检索粒度，并使用轻量级意图分类器。通过Omni-Knowledge Indexing模块建立文本（字幕、ASR、OCR）、视觉特征和语义图的分层数据库，实现任务间的最佳资源配置。引入HiVU基准测试进行综合评价，实验表明该框架在提高长视频理解效率和准确性方面具有优势，并能无缝集成到现有多模态大型语言模型中，为视频分析中的自适应检索树立了新范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaVideoRAG解决了多模态大型语言模型处理长视频时面临的固定上下文窗口和长期依赖建模难题。</li>
<li>AdaVideoRAG能动态适应查询复杂性并调整检索粒度，使用轻量级意图分类器实现这一功能。</li>
<li>Omni-Knowledge Indexing模块用于建立包含文本、视觉特征和语义图的分层数据库，实现任务间的最佳资源配置。</li>
<li>AdaVideoRAG引入HiVU基准测试进行综合评价，展示其在长视频理解方面的优势和效率。</li>
<li>该框架能无缝集成到现有多模态大型语言模型中。</li>
<li>AdaVideoRAG为视频分析中的自适应检索树立了新范式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e9bd8aaed6a9307197e9320858b3cc43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a2bac9593576cdee77146a68ff7b078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee373f654d37052f7226d3f5c018fcb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MambaMia-A-State-Space-Model-Based-Compression-for-Efficient-Video-Understanding-in-Large-Multimodal-Models"><a href="#MambaMia-A-State-Space-Model-Based-Compression-for-Efficient-Video-Understanding-in-Large-Multimodal-Models" class="headerlink" title="MambaMia: A State-Space-Model-Based Compression for Efficient Video   Understanding in Large Multimodal Models"></a>MambaMia: A State-Space-Model-Based Compression for Efficient Video   Understanding in Large Multimodal Models</h2><p><strong>Authors:Geewook Kim, Minjoon Seo</strong></p>
<p>We propose an efficient framework to compress multiple video-frame features before feeding them into large multimodal models, thereby mitigating the severe token explosion arising from long or dense videos. Our design leverages a bidirectional state-space-based block equipped with a gated skip connection and a learnable weighted-average pooling mechanism applied to periodically inserted learned queries. This structure enables hierarchical downsampling across both spatial and temporal dimensions, preserving performance in a cost-effective manner. Across challenging long and dense video understanding tasks, our approach demonstrates competitive results against state-of-the-art models, while significantly reducing overall token budget. Notably, replacing our proposed state-space block with a conventional Transformer results in substantial performance degradation, highlighting the advantages of state-space modeling for effectively compressing multi-frame video data. Our framework emphasizes resource-conscious efficiency, making it practical for real-world deployments. We validate its scalability and generality across multiple benchmarks, achieving the dual objectives of efficient resource usage and comprehensive video understanding. </p>
<blockquote>
<p>我们提出了一个高效的框架，用于在将多帧视频特征输入大型多模态模型之前对其进行压缩，从而缓解由长视频或密集视频引起的严重令牌爆炸问题。我们的设计利用了一个基于双向状态空间的块，该块配备了门控跳过连接和可学习的加权平均池化机制，这些机制应用于定期插入的学习查询。这种结构能够在空间和时间维度上实现分层下采样，以经济的方式保持性能。在面对具有挑战性的长视频和密集视频理解任务时，我们的方法与最先进的模型相比表现出竞争力，同时显著减少了总体令牌预算。值得注意的是，用传统的Transformer替换我们提出的状态空间块会导致性能大幅下降，这突出了状态空间建模在有效压缩多帧视频数据方面的优势。我们的框架注重资源意识效率，使其适用于实际部署。我们在多个基准测试上验证了其可扩展性和普遍性，实现了有效利用资源和全面理解视频的双重目标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13564v1">PDF</a> 17 pages, 5 figures</p>
<p><strong>Summary</strong><br>提供的文本描述了一个高效的框架，用于压缩多帧视频特征，然后将其输入大型多模态模型，从而解决长或密集视频引起的严重令牌爆炸问题。该设计利用双向状态空间块，配备门控跳跃连接和可学习的加权平均池化机制，应用于定期插入的学习查询。此结构可在空间和时间上实现分层下采样，以节约成本的方式保持性能。在具有挑战性的长和密集视频理解任务方面，该方法在具有竞争力的同时，大大降低了总体令牌预算。使用常规转换器替换建议的状态空间块会导致性能大幅下降，这凸显了状态空间建模在有效压缩多帧视频数据方面的优势。该框架注重资源节约效率，适合实际应用部署。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出一个高效框架压缩多帧视频特征，解决长或密集视频的令牌爆炸问题。</li>
<li>利用双向状态空间块，配备门控跳跃连接和加权平均池化机制。</li>
<li>实现空间和时间的分层下采样，以节约成本的方式保持性能。</li>
<li>在长而密集的视频理解任务中表现竞争力，显著减少令牌预算。</li>
<li>替换状态空间块会导致性能显著下降，凸显其优势。</li>
<li>框架注重资源节约效率，适合实际应用部署。</li>
<li>验证了其在多个基准测试上的可扩展性和普遍性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-177cedfc7d160942a59a5b6583c2e910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5930a3b6336dcf8d8cbdd75f7ff95e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4d380370868fcc4dc0d1151b7d6c288.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a230c035bc64b92761799ceb3c67bfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f2bd8c76ec06ca9281b5b3ab0c65772.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VideoDeepResearch-Long-Video-Understanding-With-Agentic-Tool-Using"><a href="#VideoDeepResearch-Long-Video-Understanding-With-Agentic-Tool-Using" class="headerlink" title="VideoDeepResearch: Long Video Understanding With Agentic Tool Using"></a>VideoDeepResearch: Long Video Understanding With Agentic Tool Using</h2><p><strong>Authors:Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Ji-Rong Wen, Zhicheng Dou</strong></p>
<p>Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task’s inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems. </p>
<blockquote>
<p>长视频理解（LVU）对当前的多模态大型语言模型（MLLMs）提出了重大挑战，这主要是由于该任务的固有复杂性和上下文窗口约束。普遍认为，解决LVU任务需要具有扩展上下文窗口、强大视觉感知能力和专业领域知识的基础MLLMs。在这项工作中，我们通过引入VideoDeepResearch，一个用于长视频理解的新型代理框架，来挑战这一普遍信念。我们的方法仅依赖于文本大型推理模型（LRM）和模块化多模态工具包，包括多模态检索器和视觉感知器，所有这些在实践中都很容易获得。对于每个LVU任务，该系统通过推理制定问题解决策略，同时有选择地访问和利用工具中的关键视频内容。我们在流行的LVU基准测试上进行了大量实验，包括MLVU、Video-MME和LVBench。结果表明，VideoDeepResearch在现有的MLLM基准测试上实现了显著改进，在MLVU（测试）、LVBench和LongVideoBench上的性能分别提高了9.6%、6.6%和3.9%。这些发现突显了代理系统在克服LVU问题中的关键挑战方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10821v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视频深度研究框架，利用文本推理模型和多模态工具进行长视频理解，在多个数据集上超越现有基线方法。该框架仅依赖于文本大型推理模型与模块化多模态工具组合，为解决长视频理解问题提供了新的视角。在主流长视频理解基准测试中实现了显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>视频深度研究（VideoDeepResearch）是一个新型的智能框架，专门用于长视频理解（LVU）。</li>
<li>该框架不依赖具有扩展上下文窗口的基础多模态大型语言模型（MLLMs），而是结合文本推理模型和多模态工具进行视频理解。</li>
<li>VideoDeepResearch通过选择性访问和利用视频内容，为每一个LVU任务制定问题解决策略。</li>
<li>在多个流行的LVU基准测试中进行了广泛实验，包括MLVU、Video-MME和LVBench。</li>
<li>VideoDeepResearch实现了对现有MLLM基线的显著改进，在某些测试中的表现超过了先前最先进的水平。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10821">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-465c9a9349dfcc6d537b919862df36b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7ecb96a18e3cb871365015eeae8d4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb709d90f75fcc0532562114865668f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMs’ abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMs’ abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git">https://github.com/zli12321/VideoHallu.git</a>. </p>
<blockquote>
<p>合成视频生成因其真实感和广泛应用而受到广泛关注，但仍容易违反常识和物理定律。这强调了对可靠异常检测器的需求，这些检测器需要理解这些原理，并对幻觉具有鲁棒性。为了解决这一问题，我们引入了VideoHallu，这是一个由超过3000个视频问答对组成的基准测试，这些问答对来自由Veo2、Sora和Kling等模型生成的合成视频，以及与专家精心制作的反直觉问答相结合，以评估多模态大型语言模型（MLLMs）在异常检测方面的批判性思维能力，这些异常对人类来说在感知上是明显的，但由于语言先验知识往往会产生幻觉。VideoHallu在跨对齐、一致性、常识和物理方面评估MLLMs的异常检测能力。我们对SOTA MLLMs进行基准测试，包括GPT-4o、Gemini-2.5-Pro、Qwen2.5-VL、Video-R1和VideoChat-R1。我们发现这些模型在MVBench和MovieChat等现实世界基准测试中表现良好，但在合成视频中的基于物理和常识推理方面仍面临困难。我们还进一步显示，使用组合视频问答与反直觉常识和物理推理的数据集进行课程学习的集团相对策略优化（GRPO）后训练，可以改善MLLMs的异常检测和批判性思维，证明了有针对性的训练对于提高它们对常识和物理定律的理解的价值。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zli12321/VideoHallu.git上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了合成视频生成技术在引起关注的同时，仍存在违反常识和物理定律的问题。因此，需要开发可靠的异常检测器来理解这些原理并抵抗幻觉。为此，我们引入了VideoHallu基准测试，包含超过3000个视频QA对，由模型生成的合成视频与专家设计的反直觉QA组成，旨在评估多模态大型语言模型（MLLMs）在感知明显但对人类而言却经常因语言先验而产生幻觉的异常方面的批判性思维能力。VideoHallu基准测试涵盖了对MLLMs在定位、一致性、常识和物理方面的异常检测能力评估。我们对包括GPT-4o、Gemini-2.5-Pro、Qwen2.5-VL等在内的顶尖MLLMs进行了基准测试，发现它们在许多现实世界基准测试（如MVBench和MovieChat）上表现良好，但在合成视频的基于物理和常识的推理方面仍存在问题。此外，我们展示了使用组合视频QA与反直觉常识和物理推理的数据集进行课程学习的集团相对政策优化（GRPO）后训练，能提高MLLMs的异常检测和批判性思维能力，证明了有针对性的训练对于提高他们对常识和物理定律的理解的价值。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>合成视频生成技术虽然逼真且应用广泛，但常常违反常识和物理定律。</li>
<li>VideoHallu基准测试旨在评估多模态大型语言模型在识别合成视频中的异常现象的能力。</li>
<li>当前顶尖的语言模型在现实世界基准测试上表现良好，但在涉及物理和常识推理的合成视频上仍面临挑战。</li>
<li>通过有针对性的训练，如使用集团相对政策优化（GRPO）和后训练，可以提高语言模型在异常检测和批判性思维方面的能力。</li>
<li>有效的训练数据集结合了视频QA与反直觉常识和物理推理，有助于提高模型的理解能力。</li>
<li>VideoHallu的代码已公开发布，便于研究和进一步开发。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c0733746e310ac120e88dee7a0bb4e33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c0e565c33c36de724192494cf59dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd791c7ebf56cd412d8edaf50d4d8b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c07288c94070b78a2e6183c618ed95a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4f30a27d12edd17a705e7e98a27eba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6eba42bf105f2f69700259d4c77c4224.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-Video-Understanding-with-16-Frames-Per-Second"><a href="#Improving-LLM-Video-Understanding-with-16-Frames-Per-Second" class="headerlink" title="Improving LLM Video Understanding with 16 Frames Per Second"></a>Improving LLM Video Understanding with 16 Frames Per Second</h2><p><strong>Authors:Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang</strong></p>
<p>Human vision is dynamic and continuous. However, in video understanding with multimodal large language models (LLMs), existing methods primarily rely on static features extracted from images sampled at a fixed low frame rate of frame-per-second (FPS) $\leqslant$2, leading to critical visual information loss. In this paper, we introduce F-16, the first multimodal LLM designed for high-frame-rate video understanding. By increasing the frame rate to 16 FPS and compressing visual tokens within each 1-second clip, F-16 efficiently captures dynamic visual features while preserving key semantic information. Experimental results demonstrate that higher frame rates considerably enhance video understanding across multiple benchmarks, providing a new approach to improving video LLMs beyond scaling model size or training data. F-16 achieves state-of-the-art performance among 7-billion-parameter video LLMs on both general and fine-grained video understanding benchmarks, such as Video-MME and TemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks, including high-speed sports analysis (\textit{e.g.}, basketball, football, gymnastics, and diving), outperforming SOTA proprietary visual models like GPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method for F-16 that enables highly efficient low-frame-rate inference without requiring model retraining. We will release the source code, model checkpoints, and data at \href{<a target="_blank" rel="noopener" href="https://github.com/bytedance/F-16%7D%7Bhttps://github.com/bytedance/F-16%7D">https://github.com/bytedance/F-16}{https://github.com/bytedance/F-16}</a>. </p>
<blockquote>
<p>人类视觉是动态且连续的。然而，在多模态大型语言模型（LLM）的视频理解中，现有方法主要依赖于以固定低帧率（FPS）≤2采样的图像中提取的静态特征，这导致了关键视觉信息的丢失。在本文中，我们介绍了F-16，它是专为高帧率视频理解设计的第一款多模态LLM。通过提高到16 FPS的帧率和压缩每秒剪辑中的视觉令牌，F-16能够高效地捕捉动态视觉特征，同时保留关键语义信息。实验结果表明，提高帧率可以显著增强多个基准测试的视频理解能力，为改进视频LLM提供了一种新方法，而无需扩大模型规模或增加训练数据。F-16在通用和精细粒度视频理解基准测试上均实现了最先进的性能，如Video-MME和TemporalBench。此外，F-16在复杂的时空任务中表现出色，包括高速运动分析（例如篮球、足球、体操和跳水），超越了如GPT-4o和Gemini-1.5-pro等最先进的专业视觉模型。另外，我们还为F-16引入了一种新型解码方法，能够实现在无需重新训练模型的情况下进行高效低帧率推理。我们将在<a target="_blank" rel="noopener" href="https://github.com/bytedance/F-16">https://github.com/bytedance/F-16</a>发布源代码、模型检查点和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13956v2">PDF</a> </p>
<p><strong>摘要</strong><br>    本文介绍了针对高帧率视频理解的多模态大型语言模型F-16。现有方法主要依赖于从固定低帧率采样的图像中提取的静态特征，导致关键视觉信息丢失。F-16通过将帧率提高到每秒16帧并压缩每秒剪辑中的视觉令牌，能够高效捕获动态视觉特征并保留关键语义信息。实验结果表明，提高帧率显著提高了多个基准测试的视频理解能力。此外，F-16在一般和精细粒度视频理解基准测试中实现了最佳性能，并在复杂时空任务中表现出卓越性能。最后介绍了一种用于F-16的新型解码方法，可实现高效低帧率推理而无需重新训练模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>现有视频理解方法主要依赖低帧率图像提取的静态特征，导致视觉信息损失。</li>
<li>F-16模型设计用于高帧率视频理解，通过提高帧率和压缩视觉令牌来捕获动态视觉特征。</li>
<li>实验结果显示高帧率显著提高视频理解性能，在多个基准测试中表现最佳。</li>
<li>F-16在一般和精细粒度视频理解方面表现出卓越性能，特别是在复杂时空任务中。</li>
<li>F-16引入新型解码方法，实现高效低帧率推理。</li>
<li>F-16模型优于其他先进视觉模型，如GPT-4o和Gemini-1.5-pro。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c07cc78354862fbd1048114d6dae1850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b0a3a1df007791ffbdc08c470cae99e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e950075907c714fd64f1eb8c13e9b396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-794b19a4dd8756ff5559d01e245652ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-611b0b0308c235eeb547dbfb3240b790.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38f5929df68dba2278b777751e577005.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-Understanding-with-Large-Language-Models-A-Survey"><a href="#Video-Understanding-with-Large-Language-Models-A-Survey" class="headerlink" title="Video Understanding with Large Language Models: A Survey"></a>Video Understanding with Large Language Models: A Survey</h2><p><strong>Authors:Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu</strong></p>
<p>With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding</a>. </p>
<blockquote>
<p>随着在线视频平台的蓬勃发展和视频内容的不断增加，对熟练的视频理解工具的需求显著增加。鉴于大型语言模型（LLM）在语言和多媒体任务中的卓越能力，这篇综述提供了关于如何利用LLM（视频LLM）进行视频理解的最新进展的详细介绍。Vid-LLM的新兴能力令人惊讶地先进，尤其是它们结合常识知识进行的开放式多粒度（一般、时间和时空）推理能力，这为未来的视频理解指明了有希望的道路。我们考察了Vid-LLM的独特特征和能力，并将这些方法分为三类：视频分析器x LLM、视频嵌入器x LLM和（分析器+嵌入器）x LLM。此外，我们根据LLM在Vid-LLM中的功能确定了五种亚型：LLM作为摘要器、LLM作为管理器、LLM作为文本解码器、LLM作为回归器和LLM作为隐藏层。此外，这篇综述还对Vid-LLM的任务、数据集、基准测试和评估方法进行了全面的研究。它还探讨了Vid-LLM在各个领域的应用广泛性，突出了它们在现实世界的视频理解挑战中的出色可扩展性和通用性。最后，它总结了现有Vid-LLM的局限性，并指出了未来研究的方向。更多信息请访问<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding%E4%BB%93%E5%BA%93%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding仓库查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.17432v5">PDF</a> Accepted by IEEE TCSVT</p>
<p><strong>Summary</strong></p>
<p>随着在线视频平台的蓬勃发展和视频内容的不断增加，对熟练的视频理解工具的需求急剧增长。鉴于大型语言模型在多模态任务中的出色表现，这篇综述提供了利用大型语言模型（Vid-LLMs）进行视频理解的最新进展概述。Vid-LLMs的新兴能力令人惊讶地先进，特别是在结合常识知识进行的开放多端、多粒度（一般、时间和时空）推理方面，为未来的视频理解指明了有前景的道路。本文总结了Vid-LLMs在视频分析、嵌入、任务、数据集、基准测试、评估方法和应用领域的特点和能力。更多信息可访问<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding%E4%BA%86%E8%A7%A3%E3%80%82">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding了解。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>在线视频平台和视频内容的增长导致对熟练视频理解工具的需求急剧增加。</li>
<li>大型语言模型（LLMs）在视频理解方面表现出色，形成的新兴技术称为Vid-LLMs。</li>
<li>Vid-LLMs具有开放多端、多粒度的推理能力，并结合常识知识，为未来的视频理解提供了前景。</li>
<li>Vid-LLMs有三种主要类型：Video Analyzer x LLM、Video Embedder x LLM和（Analyzer + Embedder）x LLM。</li>
<li>LLM在Vid-LLMs中的功能可分为五种亚型：作为总结者、管理者、文本解码器、回归器和隐藏层。</li>
<li>该综述全面研究了Vid-LLMs的任务、数据集、基准测试和评估方法。</li>
<li>Vid-LLMs在多个领域有广泛的应用，表现出惊人的可扩展性和在现实世界视频理解挑战中的通用性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.17432">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b47633ead27881529092be3c47142aed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-720012d99a98acdd8cb9eb270093a53f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-998dc54077c52aa56de1d1877831ddc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b009ab17f18a2cefda5c17ad7d53d4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57befebd00bd8025139e3ba9f9bafd2b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6be101dced6ae52c25293095f5eb1857.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-06-22  OpenPath Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e543d07694b38146aa2793cb90a02bf7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-22  Beyond Black Boxes Enhancing Interpretability of Transformers Trained   on Neural Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
