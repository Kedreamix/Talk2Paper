<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0b3ebdc4821c4c59d09a146b320df3e3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-22-æ›´æ–°"><a href="#2025-06-22-æ›´æ–°" class="headerlink" title="2025-06-22 æ›´æ–°"></a>2025-06-22 æ›´æ–°</h1><h2 id="Exploring-Non-contrastive-Self-supervised-Representation-Learning-for-Image-based-Profiling"><a href="#Exploring-Non-contrastive-Self-supervised-Representation-Learning-for-Image-based-Profiling" class="headerlink" title="Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling"></a>Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling</h2><p><strong>Authors:Siran Dai, Qianqian Xu, Peisong Wen, Yang Liu, Qingming Huang</strong></p>
<p>Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025. </p>
<blockquote>
<p>åŸºäºå›¾åƒçš„ç»†èƒåˆ†ææ—¨åœ¨åˆ›å»ºç»†èƒå›¾åƒçš„ä¿¡æ¯åŒ–è¡¨ç¤ºã€‚è¿™ä¸€æŠ€æœ¯åœ¨è¯ç‰©å‘ç°ä¸­è‡³å…³é‡è¦ï¼Œå¹¶éšç€è®¡ç®—æœºè§†è§‰çš„æœ€è¿‘è¿›æ­¥è€Œå–å¾—äº†å·¨å¤§è¿›å±•ã€‚å—éå¯¹æ¯”æ€§è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æœ€æ–°å‘å±•çš„å¯å‘ï¼Œæœ¬æ–‡åˆæ­¥æ¢ç´¢äº†ä½¿ç”¨æ­¤ç±»æ–¹æ³•å¯¹ç»†èƒå›¾åƒè¿›è¡Œé€šç”¨ç‰¹å¾æå–å™¨çš„è®­ç»ƒã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰ç»†èƒå›¾åƒå’Œè‡ªç„¶å›¾åƒä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚å¾ˆå¤§ï¼Œå¯¼è‡´ç°æœ‰SSLæ–¹æ³•ä¸­çš„è§†å›¾ç”Ÿæˆè¿‡ç¨‹å¤±è´¥ï¼›2ï¼‰ä¸æ¯ä¸ªè¡¨ç¤ºåŸºäºå•å¼ å›¾åƒçš„å…¸å‹åœºæ™¯ä¸åŒï¼Œç»†èƒåˆ†æé€šå¸¸æ¶‰åŠå¤šå¼ è¾“å…¥å›¾åƒï¼Œè¿™ä½¿å¾—éš¾ä»¥æœ‰æ•ˆåœ°ç»“åˆæ‰€æœ‰å¯ç”¨ä¿¡æ¯ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SSLProfilerï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç»†èƒåˆ†æè®¾è®¡çš„éå¯¹æ¯”æ€§SSLæ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹ç»†èƒå›¾åƒçš„ä¸“ä¸šæ•°æ®å¢å¼ºå’Œè¡¨ç¤ºåå¤„ç†æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œå¹¶å¯¼è‡´äº†ä¸€ä¸ªç¨³å¥çš„ç‰¹å¾æå–å™¨ã€‚æœ‰äº†è¿™äº›æ”¹è¿›ï¼ŒSSLProfileråœ¨CVPR 2025çš„ç»†èƒç³»å¯è¿ç§»æ€§æŒ‘æˆ˜ä¸­è·èƒœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14265v1">PDF</a> CVPR 2025 Computer Vision for Drug Discovery</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå›¾åƒç»†èƒåˆ†æçš„å…³é”®æŠ€æœ¯åŠå…¶åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ã€‚å—åˆ°éå¯¹æ¯”æ€§è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§é’ˆå¯¹ç»†èƒå›¾åƒè®¾è®¡çš„é€šç”¨ç‰¹å¾æå–å™¨ã€‚é¢å¯¹ç»†èƒå›¾åƒä¸è‡ªç„¶å›¾åƒåˆ†å¸ƒå·®å¼‚å¤§åŠå¤šè¾“å…¥å›¾åƒçš„ä¿¡æ¯æ•´åˆæŒ‘æˆ˜ï¼Œå›¢é˜Ÿæå‡ºSSLProfileræ¡†æ¶ï¼ŒåŒ…å«é’ˆå¯¹ç»†èƒå›¾åƒçš„æ•°æ®å¢å¼ºå’Œç‰¹å¾è¡¨ç¤ºåå¤„ç†æ–¹æ³•ï¼Œè¿›è€Œæœ‰æ•ˆæå‡äº†ç‰¹å¾æå–çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨CVPR 2025çš„ç»†èƒçº¿å¯è¿ç§»æ€§æŒ‘æˆ˜ä¸­è·èƒœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å›¾åƒç»†èƒåˆ†ææ—¨åœ¨åˆ›å»ºç»†èƒå›¾åƒçš„ä¿¡æ¯åŒ–è¡¨ç¤ºï¼Œåœ¨è¯ç‰©å‘ç°ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰çš„æœ€æ–°æ”¹è¿›æ¨åŠ¨äº†å›¾åƒç»†èƒåˆ†æçš„å‘å±•ã€‚</li>
<li>éå¯¹æ¯”æ€§è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨å›¾åƒç»†èƒåˆ†æä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç»†èƒå›¾åƒä¸è‡ªç„¶å›¾åƒåˆ†å¸ƒå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´ç°æœ‰SSLæ–¹æ³•çš„è§†å›¾ç”Ÿæˆè¿‡ç¨‹å¤±æ•ˆã€‚</li>
<li>ç»†èƒåˆ†ææ¶‰åŠå¤šè¾“å…¥å›¾åƒï¼Œæ•´åˆæ‰€æœ‰å¯ç”¨ä¿¡æ¯æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>SSLProfileræ¡†æ¶é€šè¿‡é’ˆå¯¹ç»†èƒå›¾åƒçš„æ•°æ®å¢å¼ºå’Œç‰¹å¾è¡¨ç¤ºåå¤„ç†æ–¹æ³•è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>SSLProfileråœ¨CVPR 2025çš„ç»†èƒçº¿å¯è¿ç§»æ€§æŒ‘æˆ˜ä¸­èƒœå‡ºï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ef57b752a0f9102f00cae8f7279774af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72ee4cf68fe4479cdf57c41a5a7576e2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Contrastive-Self-Supervised-Learning-As-Neural-Manifold-Packing"><a href="#Contrastive-Self-Supervised-Learning-As-Neural-Manifold-Packing" class="headerlink" title="Contrastive Self-Supervised Learning As Neural Manifold Packing"></a>Contrastive Self-Supervised Learning As Neural Manifold Packing</h2><p><strong>Authors:Guanming Zhang, David J. Heeger, Stefano Martiniani</strong></p>
<p>Contrastive self-supervised learning based on point-wise comparisons has been widely studied for vision tasks. In the visual cortex of the brain, neuronal responses to distinct stimulus classes are organized into geometric structures known as neural manifolds. Accurate classification of stimuli can be achieved by effectively separating these manifolds, akin to solving a packing problem. We introduce Contrastive Learning As Manifold Packing (CLAMP), a self-supervised framework that recasts representation learning as a manifold packing problem. CLAMP introduces a loss function inspired by the potential energy of short-range repulsive particle systems, such as those encountered in the physics of simple liquids and jammed packings. In this framework, each class consists of sub-manifolds embedding multiple augmented views of a single image. The sizes and positions of the sub-manifolds are dynamically optimized by following the gradient of a packing loss. This approach yields interpretable dynamics in the embedding space that parallel jamming physics, and introduces geometrically meaningful hyperparameters within the loss function. Under the standard linear evaluation protocol, which freezes the backbone and trains only a linear classifier, CLAMP achieves competitive performance with state-of-the-art self-supervised models. Furthermore, our analysis reveals that neural manifolds corresponding to different categories emerge naturally and are effectively separated in the learned representation space, highlighting the potential of CLAMP to bridge insights from physics, neural science, and machine learning. </p>
<blockquote>
<p>åŸºäºç‚¹å¯¹æ¯”çš„å¯¹æ¯”å¼è‡ªç›‘ç£å­¦ä¹ åœ¨è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚åœ¨å¤§è„‘çš„è§†è§‰çš®å±‚ä¸­ï¼Œç¥ç»å…ƒå¯¹ä¸åŒåˆºæ¿€ç±»åˆ«çš„å“åº”è¢«ç»„ç»‡æˆç§°ä¸ºç¥ç»æµå½¢çš„å‡ ä½•ç»“æ„ã€‚é€šè¿‡æœ‰æ•ˆåœ°åˆ†ç¦»è¿™äº›æµå½¢ï¼Œç±»ä¼¼äºè§£å†³æ‰“åŒ…é—®é¢˜ï¼Œå¯ä»¥å®ç°åˆºæ¿€çš„å‡†ç¡®åˆ†ç±»ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ ä½œä¸ºæµå½¢æ‰“åŒ…ï¼ˆCLAMPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†è¡¨ç¤ºå­¦ä¹ é‡æ–°å®šä½ä¸ºæµå½¢æ‰“åŒ…é—®é¢˜çš„è‡ªç›‘ç£æ¡†æ¶ã€‚CLAMPå¼•å…¥äº†ä¸€ç§å—çŸ­ç¨‹æ’æ–¥ç²’å­ç³»ç»Ÿçš„åŠ¿èƒ½å¯å‘çš„æŸå¤±å‡½æ•°ï¼Œå¦‚ç®€å•æ¶²ä½“å’Œå µå¡æ‰“åŒ…çš„ç‰©ç†ä¸­æ‰€é‡åˆ°çš„ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæ¯ä¸ªç±»åˆ«ç”±åµŒå…¥å•ä¸ªå›¾åƒçš„å¤šä¸ªå¢å¼ºè§†å›¾çš„å­æµå½¢ç»„æˆã€‚å­æµå½¢çš„å¤§å°å’Œä½ç½®é€šè¿‡éµå¾ªæ‰“åŒ…æŸå¤±çš„æ¢¯åº¦è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•åœ¨åµŒå…¥ç©ºé—´ä¸­äº§ç”Ÿä¸å µå¡ç‰©ç†ç›¸å¹³è¡Œçš„å¯è§£é‡ŠåŠ¨æ€ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­è¾“å…¥äº†å…·æœ‰å‡ ä½•æ„ä¹‰çš„è¶…å‚æ•°ã€‚åœ¨æ ‡å‡†çº¿æ€§è¯„ä¼°åè®®ä¸‹ï¼Œå³å†»ç»“ä¸»å¹²å¹¶ä»…è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ï¼ŒCLAMPè¾¾åˆ°äº†ä¸æœ€æ–°è‡ªç›‘ç£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¯¹åº”äºä¸åŒç±»åˆ«çš„ç¥ç»æµå½¢è‡ªç„¶ä¼šå‡ºç°åœ¨å­¦ä¹ åˆ°çš„è¡¨ç¤ºç©ºé—´ä¸­ï¼Œå¹¶å¾—åˆ°æœ‰æ•ˆåˆ†ç¦»ï¼Œè¿™çªå‡ºäº†CLAMPåœ¨ç‰©ç†å­¦ã€ç¥ç»ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¹‹é—´çš„æ´å¯ŸåŠ›ä¹‹é—´çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13717v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºç‚¹å¯¹æ¯”çš„å¯¹æ¯”å¼è‡ªç›‘ç£å­¦ä¹ åœ¨è§†è§‰ä»»åŠ¡ä¸Šå—åˆ°å¹¿æ³›å…³æ³¨ã€‚æœ¬ç ”ç©¶å¼•å…¥Contrastive Learning As Manifold Packingï¼ˆCLAMPï¼‰æ¡†æ¶ï¼Œå°†è¡¨ç¤ºå­¦ä¹ é‡æ–°å®šä¹‰ä¸ºæµå½¢æ‰“åŒ…é—®é¢˜ã€‚è¯¥æ¡†æ¶å—åˆ°ç®€å•æ¶²ä½“å’Œå †ç§¯ç‰©ç†ä¸­åŠ¿èƒ½ç²’å­ç³»ç»Ÿçš„å¯å‘ï¼Œåˆ©ç”¨æµå½¢åŠ¨æ€ä¼˜åŒ–ç±»å†…çš„å¤šä¸ªå›¾åƒå¢å¼ºè§†å›¾ã€‚è¿™ç§æ–°æ–¹æ³•æä¾›äº†ä¸€ç§å¯è§†åŒ–åŠ¨æ€åµŒå…¥ç©ºé—´çš„æ–¹æ³•ï¼Œä¸é˜»å¡ç‰©ç†ç±»ä¼¼ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å‡ ä½•æ„ä¹‰ä¸Šçš„è¶…å‚æ•°ã€‚åœ¨æ ‡å‡†çº¿æ€§è¯„ä¼°åè®®ä¸‹ï¼ŒCLAMPä¸æœ€å…ˆè¿›çš„è‡ªç›‘ç£æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶æ­ç¤ºäº†ä¸åŒç±»åˆ«çš„ç¥ç»æµå½¢åœ¨è¡¨ç¤ºç©ºé—´ä¸­è‡ªç„¶å‡ºç°å¹¶å¾—åˆ°æœ‰æ•ˆåˆ†ç¦»ï¼Œæ˜¾ç¤ºå‡ºCLAMPåœ¨ç‰©ç†å­¦ã€ç¥ç»ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¹‹é—´å»ºç«‹è”ç³»çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯¹æ¯”å¼è‡ªç›‘ç£å­¦ä¹ é€šè¿‡ç‚¹å¯¹æ¯”å¹¿æ³›åº”ç”¨äºè§†è§‰ä»»åŠ¡ã€‚</li>
<li>CLAMPæ¡†æ¶å°†è¡¨ç¤ºå­¦ä¹ å®šä¹‰ä¸ºæµå½¢æ‰“åŒ…é—®é¢˜ã€‚</li>
<li>CLAMPå—ç®€å•æ¶²ä½“å’Œå †ç§¯ç‰©ç†ä¸­åŠ¿èƒ½ç²’å­ç³»ç»Ÿçš„å¯å‘ï¼Œåˆ©ç”¨æµå½¢ä¼˜åŒ–å›¾åƒå¢å¼ºè§†å›¾ã€‚</li>
<li>CLAMPæä¾›å¯è§†åŒ–åŠ¨æ€åµŒå…¥ç©ºé—´çš„æ–¹æ³•ï¼Œä¸é˜»å¡ç‰©ç†ç±»ä¼¼ã€‚</li>
<li>æŸå¤±å‡½æ•°ä¸­åŒ…å«å‡ ä½•æ„ä¹‰çš„è¶…å‚æ•°ã€‚</li>
<li>åœ¨æ ‡å‡†çº¿æ€§è¯„ä¼°åè®®ä¸‹ï¼ŒCLAMPè¡¨ç°ä¸æœ€å…ˆè¿›çš„è‡ªç›‘ç£æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-db0e214bbc1825e2ff5e322c902df400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ad4bca7a562948d334c0b21891f476.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3ebdc4821c4c59d09a146b320df3e3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TR2M-Transferring-Monocular-Relative-Depth-to-Metric-Depth-with-Language-Descriptions-and-Scale-Oriented-Contrast"><a href="#TR2M-Transferring-Monocular-Relative-Depth-to-Metric-Depth-with-Language-Descriptions-and-Scale-Oriented-Contrast" class="headerlink" title="TR2M: Transferring Monocular Relative Depth to Metric Depth with   Language Descriptions and Scale-Oriented Contrast"></a>TR2M: Transferring Monocular Relative Depth to Metric Depth with   Language Descriptions and Scale-Oriented Contrast</h2><p><strong>Authors:Beilei Cui, Yiming Huang, Long Bai, Hongliang Ren</strong></p>
<p>This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2Mâ€™s great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BeileiCui/TR2M">https://github.com/BeileiCui/TR2M</a>) </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œç”¨äºå°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ã€‚å½“å‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ä¸»è¦åˆ†ä¸ºåº¦é‡æ·±åº¦ä¼°è®¡ï¼ˆMMDEï¼‰å’Œç›¸å¯¹æ·±åº¦ä¼°è®¡ï¼ˆMRDEï¼‰ã€‚MMDEåœ¨åº¦é‡å°ºåº¦ä¸Šä¼°è®¡æ·±åº¦ï¼Œä½†é€šå¸¸ä»…é™äºç‰¹å®šé¢†åŸŸã€‚MRDEåœ¨ä¸åŒé¢†åŸŸé—´å…·æœ‰å¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å°ºåº¦ä¸ç¡®å®šï¼Œé˜»ç¢äº†ä¸‹æ¸¸åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ—¨åœ¨å»ºç«‹ä¸€ä¸ªæ¡†æ¶æ¥è§£å†³å°ºåº¦ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œå¹¶å°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ã€‚ä¹‹å‰çš„æ–¹æ³•ä½¿ç”¨è¯­è¨€ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¼°è®¡ä¸¤ä¸ªå› å­è¿›è¡Œé‡ç¼©æ”¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•TR2Måˆ©ç”¨æ–‡æœ¬æè¿°å’Œå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¼°è®¡ä¸¤ä¸ªé‡ç¼©æ”¾å›¾ï¼Œä»¥åƒç´ çº§åˆ«å°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ã€‚æ¥è‡ªä¸¤ç§æ¨¡æ€çš„ç‰¹å¾é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œèåˆï¼Œä»¥æ›´å¥½åœ°æ•è·å°ºåº¦ä¿¡æ¯ã€‚è®¾è®¡äº†ä¸€ç§ç­–ç•¥æ¥æ„å»ºå’Œè¿‡æ»¤å¯é çš„ä¼ªåº¦é‡æ·±åº¦ï¼Œä»¥å®ç°æ›´å…¨é¢çš„ç›‘ç£ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†é¢å‘å°ºåº¦çš„å¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨æ·±åº¦åˆ†å¸ƒä½œä¸ºæŒ‡å¯¼ï¼Œä»¥å¼ºåˆ¶æ¨¡å‹å­¦ä¹ å†…åœ¨çŸ¥è¯†ä¸å°ºåº¦åˆ†å¸ƒå¯¹é½ã€‚TR2Måªéœ€è¦å°‘é‡å¯è®­ç»ƒçš„å‚æ•°ï¼Œå°±å¯ä»¥åœ¨å„ç§é¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼›å®éªŒä¸ä»…è¯æ˜äº†TR2Måœ¨å¯è§æ•°æ®é›†ä¸Šçš„å‡ºè‰²æ€§èƒ½ï¼Œè¿˜å±•ç¤ºäº†åœ¨äº”ä¸ªæœªè§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬ä¼˜åŠ¿ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨åƒç´ çº§åˆ«ä½¿ç”¨è¯­è¨€è¾…åŠ©å°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦çš„å·¨å¤§æ½œåŠ›ã€‚ï¼ˆä»£ç å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/BeileiCui/TR2M%EF%BC%89">https://github.com/BeileiCui/TR2Mï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13387v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯å°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³å°ºåº¦ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œå¹¶å®ç°äº†ä»ç›¸å¯¹æ·±åº¦åˆ°åº¦é‡æ·±åº¦çš„è½¬æ¢ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„TR2Mæ–¹æ³•ä¸ä»…ä½¿ç”¨æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ï¼Œè¿˜ç»“åˆäº†å›¾åƒè¾“å…¥ï¼Œé€šè¿‡ä¼°è®¡ä¸¤ä¸ªé‡æ˜ å°„å…³ç³»æ¥å®ç°åƒç´ çº§åˆ«çš„æ·±åº¦è½¬æ¢ã€‚æ­¤å¤–ï¼ŒTR2Mè¿˜é‡‡ç”¨äº†è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—æ¥èåˆä¸¤ç§æ¨¡æ€çš„ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°æ•è·å°ºåº¦ä¿¡æ¯ã€‚é€šè¿‡æ„å»ºå’Œè¿‡æ»¤å¯é çš„ä¼ªåº¦é‡æ·±åº¦ï¼Œå®ç°äº†æ›´å…¨é¢çš„ç›‘ç£ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†é¢å‘å°ºåº¦çš„å¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨æ·±åº¦åˆ†å¸ƒä½œä¸ºæŒ‡å¯¼ï¼Œå¼ºåˆ¶æ‰§è¡Œæ¨¡å‹å­¦ä¹ ç¬¦åˆå°ºåº¦åˆ†å¸ƒçš„å†…åœ¨çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒTR2Mä¸ä»…åœ¨å¯è§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨äº”ä¸ªæœªè§æ•°æ®é›†ä¸Šå±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä½œæå‡ºä¸€ä¸ªé€šç”¨æ¡†æ¶ç”¨äºå°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ï¼Œä»¥è§£å†³å°ºåº¦ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>TR2Mæ–¹æ³•ç»“åˆæ–‡æœ¬æè¿°å’Œå›¾åƒä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ä¼°è®¡ä¸¤ä¸ªé‡æ˜ å°„å…³ç³»å®ç°åƒç´ çº§åˆ«çš„æ·±åº¦è½¬æ¢ã€‚</li>
<li>TR2Mé‡‡ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—èåˆä¸¤ç§æ¨¡æ€çš„ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°æ•è·å°ºåº¦ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ„å»ºå’Œè¿‡æ»¤å¯é çš„ä¼ªåº¦é‡æ·±åº¦ï¼Œå®ç°æ›´å…¨é¢çš„ç›‘ç£ã€‚</li>
<li>å¼•å…¥é¢å‘å°ºåº¦çš„å¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨æ·±åº¦åˆ†å¸ƒä½œä¸ºæŒ‡å¯¼ï¼Œä¿ƒè¿›æ¨¡å‹å­¦ä¹ å†…åœ¨çŸ¥è¯†ã€‚</li>
<li>TR2Måœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-48b7208321ed325302c5ded03d03c923.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-701a8edcb6ca1b500687741f333837f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a70ec9b62a185c5126a59a546b87f22.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SUSEP-Net-Simulation-Supervised-and-Contrastive-Learning-based-Deep-Neural-Networks-for-Susceptibility-Source-Separation"><a href="#SUSEP-Net-Simulation-Supervised-and-Contrastive-Learning-based-Deep-Neural-Networks-for-Susceptibility-Source-Separation" class="headerlink" title="SUSEP-Net: Simulation-Supervised and Contrastive Learning-based Deep   Neural Networks for Susceptibility Source Separation"></a>SUSEP-Net: Simulation-Supervised and Contrastive Learning-based Deep   Neural Networks for Susceptibility Source Separation</h2><p><strong>Authors:Min Li, Chen Chen, Zhenghao Li, Yin Liu, Shanshan Shan, Peng Wu, Pengfei Rong, Feng Liu, G. Bruce Pike, Alan H. Wilman, Hongfu Sun, Yang Gao</strong></p>
<p>Quantitative susceptibility mapping (QSM) provides a valuable tool for quantifying susceptibility distributions in human brains; however, two types of opposing susceptibility sources (i.e., paramagnetic and diamagnetic), may coexist in a single voxel, and cancel each other out in net QSM images. Susceptibility source separation techniques enable the extraction of sub-voxel information from QSM maps. This study proposes a novel SUSEP-Net for susceptibility source separation by training a dual-branch U-net with a simulation-supervised training strategy. In addition, a contrastive learning framework is included to explicitly impose similarity-based constraints between the branch-specific guidance features in specially-designed encoders and the latent features in the decoders. Comprehensive experiments were carried out on both simulated and in vivo data, including healthy subjects and patients with pathological conditions, to compare SUSEP-Net with three state-of-the-art susceptibility source separation methods (i.e., APART-QSM, \c{hi}-separation, and \c{hi}-sepnet). SUSEP-Net consistently showed improved results compared with the other three methods, with better numerical metrics, improved high-intensity hemorrhage and calcification lesion contrasts, and reduced artifacts in brains with pathological conditions. In addition, experiments on an agarose gel phantom data were conducted to validate the accuracy and the generalization capability of SUSEP-Net. </p>
<blockquote>
<p>å®šé‡ç£åŒ–ç‡æ˜ å°„ï¼ˆQSMï¼‰ä¸ºäººç±»å¤§è„‘ä¸­çš„ç£åŒ–ç‡åˆ†å¸ƒæä¾›äº†æœ‰ä»·å€¼çš„é‡åŒ–å·¥å…·ã€‚ç„¶è€Œï¼Œä¸¤ç§ç›¸åçš„ç£åŒ–ç‡æºï¼ˆå³é¡ºç£æ€§ç‰©è´¨å’Œåç£æ€§ç‰©è´¨ï¼‰å¯èƒ½å­˜åœ¨äºå•ä¸ªä½“ç´ å†…ï¼Œå¹¶åœ¨å‡€QSMå›¾åƒä¸­ç›¸äº’æŠµæ¶ˆã€‚ç£åŒ–ç‡æºåˆ†ç¦»æŠ€æœ¯èƒ½å¤Ÿä»QSMå›¾ä¸­æå–äºšä½“ç´ ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„SUSEP-Netç£åŒ–ç‡æºåˆ†ç¦»æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå¸¦æœ‰æ¨¡æ‹Ÿç›‘ç£è®­ç»ƒç­–ç•¥çš„åŒåˆ†æ”¯U-netç½‘ç»œå®ç°ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œä»¥åœ¨ä¸“é—¨è®¾è®¡çš„ç¼–ç å™¨ä¸­çš„åˆ†æ”¯ç‰¹å®šå¼•å¯¼ç‰¹å¾å’Œè§£ç å™¨ä¸­çš„æ½œåœ¨ç‰¹å¾ä¹‹é—´æ˜¾å¼æ–½åŠ åŸºäºç›¸ä¼¼æ€§çš„çº¦æŸã€‚å¯¹æ¨¡æ‹Ÿæ•°æ®å’Œæ´»ä½“æ•°æ®è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒåŒ…æ‹¬å¥åº·å—è¯•è€…å’Œç—…ç†æ¡ä»¶ä¸‹çš„æ‚£è€…ï¼Œå°†SUSEP-Netä¸ä¸‰ç§æœ€å…ˆè¿›çš„ç£åŒ–ç‡æºåˆ†ç¦»æ–¹æ³•ï¼ˆå³APART-QSMã€chiåˆ†ç¦»å’Œchiåˆ†ç¦»ç½‘ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚SUSEP-Netåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¡¨ç°æ›´å¥½ï¼Œæ˜¾ç¤ºå‡ºæ”¹è¿›çš„ç»“æœï¼›èƒ½å¤Ÿæé«˜é«˜çµæ•åº¦å‡ºè¡€å’Œé’™åŒ–ç—…ç¶å¯¹æ¯”åº¦ï¼›åœ¨ç—…ç†æ¡ä»¶ä¸‹çš„å¤§è„‘ä¸­å‡å°‘äº†ä¼ªå½±ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ç¼è„‚å‡èƒ¶å¹»å½±æ•°æ®çš„å®éªŒï¼Œä»¥éªŒè¯SUSEP-Netçš„å‡†ç¡®æ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13293v1">PDF</a> 8 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åä¸ºSUSEP-Netçš„æ–°æ–¹æ³•è¢«æå‡ºç”¨äºåˆ†ç¦»ç£åŒ–ç‡æºã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒå¸¦æœ‰ä»¿çœŸç›‘ç£è®­ç»ƒç­–ç•¥çš„åŒåˆ†æ”¯U-netç½‘ç»œæ¥å®ç°ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå¯¹åˆ†æ”¯ç‰¹å®šçš„æŒ‡å¯¼ç‰¹å¾å’Œè§£ç å™¨ä¸­çš„æ½œåœ¨ç‰¹å¾æ–½åŠ ç›¸ä¼¼æ€§çº¦æŸã€‚ä¸ä¸‰ç§å…ˆè¿›çš„ç£åŒ–ç‡æºåˆ†ç¦»æ–¹æ³•ç›¸æ¯”ï¼ŒSUSEP-Netåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸­å‡è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¥åº·å—è¯•è€…å’Œç—…ç†æ¡ä»¶ä¸‹çš„æ‚£è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SUSEP-Netè¢«æå‡ºç”¨äºç£åŒ–ç‡æºåˆ†ç¦»ã€‚</li>
<li>ä½¿ç”¨åŒåˆ†æ”¯U-netç»“æ„å’Œä»¿çœŸç›‘ç£è®­ç»ƒç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ¡†æ¶ç”¨äºæ–½åŠ åˆ†æ”¯ç‰¹å®šçš„æŒ‡å¯¼ç‰¹å¾å’Œæ½œåœ¨ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§çº¦æŸã€‚</li>
<li>SUSEP-Netåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å¥åº·äººå’Œç—…ç†æ¡ä»¶ä¸‹çš„ç—…äººã€‚</li>
<li>ä¸å…¶ä»–ä¸‰ç§å…ˆè¿›çš„ç£åŒ–ç‡æºåˆ†ç¦»æ–¹æ³•ç›¸æ¯”ï¼ŒSUSEP-Netåœ¨æ•°å€¼åº¦é‡ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
<li>SUSEP-Netèƒ½æé«˜é«˜å¼ºåº¦å‡ºè¡€å’Œé’™åŒ–ç—…å˜çš„å¯¹æ¯”åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90f84758d7cdc165588e65e3fd744531.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f24a4cb7fcf7679e76c18acbc003d37.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Contrastive-Learning-Using-Out-Of-Distribution-Data-for-Long-Tailed-Dataset"><a href="#Unsupervised-Contrastive-Learning-Using-Out-Of-Distribution-Data-for-Long-Tailed-Dataset" class="headerlink" title="Unsupervised Contrastive Learning Using Out-Of-Distribution Data for   Long-Tailed Dataset"></a>Unsupervised Contrastive Learning Using Out-Of-Distribution Data for   Long-Tailed Dataset</h2><p><strong>Authors:Cuong Manh Hoang, Yeejin Lee, Byeongkeun Kang</strong></p>
<p>This work addresses the task of self-supervised learning (SSL) on a long-tailed dataset that aims to learn balanced and well-separated representations for downstream tasks such as image classification. This task is crucial because the real world contains numerous object categories, and their distributions are inherently imbalanced. Towards robust SSL on a class-imbalanced dataset, we investigate leveraging a network trained using unlabeled out-of-distribution (OOD) data that are prevalently available online. We first train a network using both in-domain (ID) and sampled OOD data by back-propagating the proposed pseudo semantic discrimination loss alongside a domain discrimination loss. The OOD data sampling and loss functions are designed to learn a balanced and well-separated embedding space. Subsequently, we further optimize the network on ID data by unsupervised contrastive learning while using the previously trained network as a guiding network. The guiding network is utilized to select positive&#x2F;negative samples and to control the strengths of attractive&#x2F;repulsive forces in contrastive learning. We also distil and transfer its embedding space to the training network to maintain balancedness and separability. Through experiments on four publicly available long-tailed datasets, we demonstrate that the proposed method outperforms previous state-of-the-art methods. </p>
<blockquote>
<p>æœ¬æ–‡å…³æ³¨äº†åœ¨é•¿å°¾æ•°æ®é›†ä¸Šè¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„ä»»åŠ¡ï¼Œæ—¨åœ¨ä¸ºç›®æ ‡ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ï¼‰å­¦ä¹ å¹³è¡¡ä¸”è‰¯å¥½åˆ†ç¦»çš„è¡¨ç¤ºã€‚è¿™ä¸€ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå› ä¸ºç°å®ä¸–ç•ŒåŒ…å«ä¼—å¤šå¯¹è±¡ç±»åˆ«ï¼Œå®ƒä»¬çš„åˆ†å¸ƒæœ¬è´¨ä¸Šæ˜¯ä¸å¹³è¡¡çš„ã€‚ä¸ºäº†åœ¨ç±»åˆ«ä¸å¹³è¡¡æ•°æ®é›†ä¸Šå®ç°ç¨³å¥çš„SSLï¼Œæˆ‘ä»¬ç ”ç©¶åˆ©ç”¨ä½¿ç”¨æœªæ ‡è®°çš„åœ¨çº¿æ™®éå¯ç”¨çš„ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ•°æ®è¿›è¡Œè®­ç»ƒçš„ç½‘ç»œã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨åŸŸå†…ï¼ˆIDï¼‰å’Œé‡‡æ ·çš„OODæ•°æ®æ¥è®­ç»ƒç½‘ç»œï¼Œé€šè¿‡åå‘ä¼ æ’­æ‰€æå‡ºçš„ä¼ªè¯­ä¹‰åˆ¤åˆ«æŸå¤±å’ŒåŸŸåˆ¤åˆ«æŸå¤±ã€‚OODæ•°æ®é‡‡æ ·å’ŒæŸå¤±å‡½æ•°æ—¨åœ¨å­¦ä¹ å¹³è¡¡ä¸”è‰¯å¥½åˆ†ç¦»åµŒå…¥ç©ºé—´ã€‚éšåï¼Œæˆ‘ä»¬ä»¥å…ˆå‰è®­ç»ƒçš„ç½‘ç»œä½œä¸ºå¼•å¯¼ç½‘ç»œï¼Œé€šè¿‡æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ç½‘ç»œåœ¨IDæ•°æ®ä¸Šçš„è¡¨ç°ã€‚å¼•å¯¼ç½‘ç»œç”¨äºé€‰æ‹©æ­£&#x2F;è´Ÿæ ·æœ¬å¹¶æ§åˆ¶å¯¹æ¯”å­¦ä¹ ä¸­å¸å¼•&#x2F;æ’æ–¥åŠ›çš„å¼ºåº¦ã€‚æˆ‘ä»¬è¿˜è’¸é¦å¹¶è½¬ç§»å…¶åµŒå…¥ç©ºé—´åˆ°è®­ç»ƒç½‘ç»œï¼Œä»¥ç»´æŒå¹³è¡¡æ€§å’Œå¯åˆ†ç¦»æ€§ã€‚åœ¨å››ä¸ªå…¬å¼€å¯ç”¨çš„é•¿å°¾æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12698v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é•¿å°¾æ•°æ®é›†ä¸Šçš„è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œæ—¨åœ¨å­¦ä¹ å¹³è¡¡ä¸”è‰¯å¥½çš„è¡¨ç¤ºï¼Œä¸ºå›¾åƒåˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›æ”¯æŒã€‚é’ˆå¯¹ç°å®ä¸–ç•Œä¸­çš„å¤§é‡å¯¹è±¡ç±»åˆ«åŠå…¶å›ºæœ‰çš„ä¸å¹³è¡¡åˆ†å¸ƒé—®é¢˜ï¼Œç ”ç©¶åˆ©ç”¨åœ¨çº¿æ™®éå¯ç”¨çš„æ— æ ‡ç­¾çš„ç¦»ç¾¤æ•°æ®æ¥è®­ç»ƒç½‘ç»œã€‚é€šè¿‡åå‘ä¼ æ’­ä¼ªè¯­ä¹‰é‰´åˆ«æŸå¤±å’Œé¢†åŸŸé‰´åˆ«æŸå¤±æ¥è®­ç»ƒç½‘ç»œï¼Œå€ŸåŠ©ç¦»ç¾¤æ•°æ®çš„é‡‡æ ·å’ŒæŸå¤±å‡½æ•°æ¥å­¦ä¹ å¹³è¡¡ä¸”è‰¯å¥½çš„åµŒå…¥ç©ºé—´ã€‚æ¥ç€é€šè¿‡æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ç½‘ç»œï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒç½‘ç»œä½œä¸ºå¼•å¯¼ç½‘ç»œæ¥é€‰æ‹©æ­£&#x2F;è´Ÿæ ·æœ¬å’Œæ§åˆ¶å¯¹æ¯”å­¦ä¹ ä¸­çš„å¸å¼•åŠ›å’Œæ’æ–¥åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªå…¬å¼€çš„é•¿å°¾æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨è‡ªç›‘ç£å­¦ä¹ åœ¨é•¿å°¾æ•°æ®é›†ä¸Šçš„åº”ç”¨ï¼Œæ—¨åœ¨å­¦ä¹ å¹³è¡¡ä¸”è‰¯å¥½çš„è¡¨ç¤ºï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨åœ¨çº¿æ™®éå¯ç”¨çš„æ— æ ‡ç­¾ç¦»ç¾¤æ•°æ®æ¥è®­ç»ƒç½‘ç»œï¼Œä»¥åº”å¯¹ç°å®ä¸–ç•Œä¸­çš„å¯¹è±¡ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆé¢†åŸŸé‰´åˆ«æŸå¤±å’Œä¼ªè¯­ä¹‰é‰´åˆ«æŸå¤±æ¥è®­ç»ƒç½‘ç»œï¼Œä»¥å¢å¼ºç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›å¹¶ä¼˜åŒ–åµŒå…¥ç©ºé—´çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¼•å¯¼ç½‘ç»œè¿›è¡Œæ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œæé«˜ç½‘ç»œçš„æ€§èƒ½å¹¶å¢å¼ºç‰¹å¾è¡¨ç¤ºçš„åˆ†ç¦»æ€§ã€‚</li>
<li>å¼•å¯¼ç½‘ç»œç”¨äºé€‰æ‹©å¯¹æ¯”å­¦ä¹ ä¸­çš„æ­£&#x2F;è´Ÿæ ·æœ¬å’Œæ§åˆ¶å¸å¼•ä¸æ’æ–¥åŠ›ï¼Œä»¥ä¼˜åŒ–ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å››ä¸ªå…¬å¼€çš„é•¿å°¾æ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-68c0a4e5cf49a110d5768e150b065f2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee04cdc135325d24c417888ee2049d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd48185d7aefda7d68b07501b1c06664.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6040c48b1d1a84d30a6688e9a1dddad.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CellCLIP-â€“-Learning-Perturbation-Effects-in-Cell-Painting-via-Text-Guided-Contrastive-Learning"><a href="#CellCLIP-â€“-Learning-Perturbation-Effects-in-Cell-Painting-via-Text-Guided-Contrastive-Learning" class="headerlink" title="CellCLIP â€“ Learning Perturbation Effects in Cell Painting via   Text-Guided Contrastive Learning"></a>CellCLIP â€“ Learning Perturbation Effects in Cell Painting via   Text-Guided Contrastive Learning</h2><p><strong>Authors:Mingyu Lu, Ethan Weinberger, Chanwoo Kim, Su-In Lee</strong></p>
<p>High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cellsâ€™ morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time. </p>
<blockquote>
<p>åŸºäºé«˜é€šé‡æ˜¾å¾®é•œæŠ€æœ¯ï¼ˆå¦‚ç»†èƒæŸ“è‰²æ³•ï¼‰çš„é«˜å†…æ¶µç­›é€‰ï¼ˆHCSï¼‰æµ‹å®šæ³•ï¼Œå·²ç»èƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„è§„æ¨¡æ¢ç©¶ç»†èƒå¯¹æ‰°åŠ¨çš„å½¢æ€ååº”ã€‚æ”¶é›†æ­¤ç±»æ•°æ®æœ‰æœ›ä¿ƒè¿›å¯¹ä¸åŒæ‰°åŠ¨åŠå…¶ç»†èƒçŠ¶æ€å½±å“ä¹‹é—´å…³ç³»çš„æ›´å¥½ç†è§£ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæœ€è¿‘çš„è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ è¿›å±•ä»ç†è®ºä¸Šå¯ä»¥è¢«ç”¨æ¥å­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œä½¿æ‰°åŠ¨ä¸å…¶ç›¸åº”çš„å½¢æ€æ•ˆåº”å¯¹é½ã€‚ç„¶è€Œï¼Œå°†æ­¤ç±»æ–¹æ³•åº”ç”¨äºHCSæ•°æ®å¹¶ä¸ç®€å•ï¼Œå› ä¸ºä¸å¤©ç„¶å›¾åƒç›¸æ¯”ï¼Œç»†èƒæŸ“è‰²å›¾åƒçš„è¯­ä¹‰å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œè€Œä¸”éš¾ä»¥åœ¨å•ä¸ªæ½œåœ¨ç©ºé—´ä¸­è¡¨ç¤ºä¸åŒç±»å‹çš„æ‰°åŠ¨ï¼ˆä¾‹å¦‚å°åˆ†å­ä¸CRISPRåŸºå› æ•²é™¤ï¼‰ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥äº†CellCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºHCSæ•°æ®çš„è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚CellCLIPåˆ©ç”¨é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨ä¸æ–°å‹é€šé“ç¼–ç æ–¹æ¡ˆï¼Œä»¥æ›´å¥½åœ°æ•è·å›¾åƒåµŒå…¥ä¸­ä¸åŒæ˜¾å¾®é•œé€šé“ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠç”¨äºè¡¨ç¤ºæ‰°åŠ¨çš„è‡ªç„¶è¯­è¨€ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ€§èƒ½ä¼˜äºå½“å‰å¼€æºæ¨¡å‹ï¼Œåœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06290v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>é«˜å†…æ¶µç­›é€‰ï¼ˆHCSï¼‰å®éªŒé€šè¿‡é«˜é€šé‡æ˜¾å¾®é•œæŠ€æœ¯å¦‚ç»†èƒæŸ“è‰²æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„è§„æ¨¡æ¢ç©¶ç»†èƒå½¢æ€å¯¹å¹²æ‰°çš„å“åº”ã€‚åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£ä¸åŒå¹²æ‰°ä¹‹é—´çš„å…³ç³»åŠå…¶å¯¹ç»†èƒçŠ¶æ€çš„å½±å“ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œç†è®ºä¸Šå¯ä»¥åˆ©ç”¨è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„æœ€æ–°è¿›å±•æ¥å­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œä½¿å¹²æ‰°ä¸å…¶ç›¸åº”çš„å½¢æ€æ•ˆåº”ç›¸åŒ¹é…ã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ–¹æ³•åº”ç”¨äºHCSæ•°æ®å¹¶ä¸ç®€å•ï¼Œå› ä¸ºç»†èƒæŸ“è‰²å›¾åƒä¸è‡ªç„¶å›¾åƒåœ¨è¯­ä¹‰ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”éš¾ä»¥åœ¨å•ä¸€æ½œåœ¨ç©ºé—´å†…è¡¨ç¤ºä¸åŒç±»å‹çš„å¹²æ‰°ï¼ˆå¦‚å°åˆ†å­ä¸CRISPRåŸºå› æ•²é™¤ï¼‰ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†CellCLIPè·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚CellCLIPåˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ä¸æ–°å‹é€šé“ç¼–ç æ–¹æ¡ˆï¼Œæ›´å¥½åœ°æ•æ‰å›¾åƒåµŒå…¥ä¸­ä¸åŒæ˜¾å¾®é•œé€šé“ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠç”¨äºè¡¨ç¤ºå¹²æ‰°çš„è‡ªç„¶è¯­è¨€ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æ—¶é—´ä¹Ÿå¤§å¤§å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é«˜å†…æ¶µç­›é€‰ï¼ˆHCSï¼‰èƒ½å¤Ÿé€šè¿‡é«˜é€šé‡æ˜¾å¾®é•œæŠ€æœ¯å¤§è§„æ¨¡ç ”ç©¶ç»†èƒå½¢æ€å¯¹å¹²æ‰°çš„å“åº”ã€‚</li>
<li>æ”¶é›†è¿™äº›æ•°æ®æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£ä¸åŒå¹²æ‰°ä¸ç»†èƒçŠ¶æ€ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ å¯ç”¨äºå­¦ä¹ ä¸€ä¸ªç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼ŒåŒ¹é…å¹²æ‰°ä¸å½¢æ€æ•ˆåº”ã€‚</li>
<li>å°†è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•åº”ç”¨äºHCSæ•°æ®å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚è¯­ä¹‰å·®å¼‚å’Œä¸åŒç±»å‹å¹²æ‰°çš„è¡¨ç¤ºã€‚</li>
<li>CellCLIPæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œæ–°å‹é€šé“ç¼–ç æ–¹æ¡ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>CellCLIPæ¡†æ¶åœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œç”Ÿç‰©å­¦æ„ä¹‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e3fe46c22be2787deb2973acb2d969e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2aa7a1e33a6a1dd1412571628c9d47c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CAT-Contrastive-Adversarial-Training-for-Evaluating-the-Robustness-of-Protective-Perturbations-in-Latent-Diffusion-Models"><a href="#CAT-Contrastive-Adversarial-Training-for-Evaluating-the-Robustness-of-Protective-Perturbations-in-Latent-Diffusion-Models" class="headerlink" title="CAT: Contrastive Adversarial Training for Evaluating the Robustness of   Protective Perturbations in Latent Diffusion Models"></a>CAT: Contrastive Adversarial Training for Evaluating the Robustness of   Protective Perturbations in Latent Diffusion Models</h2><p><strong>Authors:Sen Peng, Mingyue Wang, Jianfei He, Jijia Yang, Xiaohua Jia</strong></p>
<p>Latent diffusion models have recently demonstrated superior capabilities in many downstream image synthesis tasks. However, customization of latent diffusion models using unauthorized data can severely compromise the privacy and intellectual property rights of data owners. Adversarial examples as protective perturbations have been developed to defend against unauthorized data usage by introducing imperceptible noise to customization samples, preventing diffusion models from effectively learning them. In this paper, we first reveal that the primary reason adversarial examples are effective as protective perturbations in latent diffusion models is the distortion of their latent representations, as demonstrated through qualitative and quantitative experiments. We then propose the Contrastive Adversarial Training (CAT) utilizing lightweight adapters as an adaptive attack against these protection methods, highlighting their lack of robustness. Extensive experiments demonstrate that our CAT method significantly reduces the effectiveness of protective perturbations in customization, urging the community to reconsider and improve the robustness of existing protective perturbations. The code is available at <a target="_blank" rel="noopener" href="https://github.com/senp98/CAT">https://github.com/senp98/CAT</a>. </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹å·²åœ¨è®¸å¤šä¸‹æ¸¸å›¾åƒåˆæˆä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœªç»æˆæƒçš„æ•°æ®å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå®šåˆ¶ä¼šä¸¥é‡æŸå®³æ•°æ®æ‰€æœ‰è€…çš„éšç§å’ŒçŸ¥è¯†äº§æƒã€‚å¯¹æŠ—æ€§æ ·æœ¬ä½œä¸ºä¿æŠ¤æ€§æ‰°åŠ¨å·²ç»å‘å±•èµ·æ¥ï¼Œé€šè¿‡å‘å®šåˆ¶æ ·æœ¬ä¸­åŠ å…¥å‡ ä¹ä¸å¯å¯Ÿè§‰çš„å™ªå£°æ¥é˜²å¾¡æœªç»æˆæƒçš„æ•°æ®ä½¿ç”¨ï¼Œé˜²æ­¢æ‰©æ•£æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ å®ƒä»¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ­ç¤ºå¯¹æŠ—æ€§æ ·æœ¬ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä¿æŠ¤æ€§æ‰°åŠ¨çš„ä¸»è¦åŸå› æ˜¯å…¶æ½œåœ¨è¡¨ç¤ºçš„å¤±çœŸï¼Œè¿™å·²é€šè¿‡å®šæ€§å’Œå®šé‡å®éªŒè¯æ˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è½»é‡çº§é€‚é…å™¨ä½œä¸ºé€‚åº”æ€§æ”»å‡»çš„å¯¹æ¯”å¯¹æŠ—è®­ç»ƒï¼ˆCATï¼‰ï¼Œçªå‡ºå…¶ç¼ºä¹ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CATæ–¹æ³•æ˜¾è‘—é™ä½äº†å®šåˆ¶ä¸­ä¿æŠ¤æ€§æ‰°åŠ¨çš„æœ‰æ•ˆæ€§ï¼Œæ•¦ä¿ƒç¤¾åŒºé‡æ–°è€ƒè™‘å¹¶æé«˜ç°æœ‰ä¿æŠ¤æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/senp98/CAT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/senp98/CATæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07225v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ä¸‹æ¸¸å›¾åƒåˆæˆä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶å®šåˆ¶åŒ–ä½¿ç”¨æœªç»æˆæƒæ•°æ®ä¼šä¸¥é‡ä¾µçŠ¯æ•°æ®æ‰€æœ‰è€…çš„éšç§å’ŒçŸ¥è¯†äº§æƒã€‚å¯¹æŠ—æ ·æœ¬ä½œä¸ºä¿æŠ¤æ€§æ‰°åŠ¨ï¼Œé€šè¿‡å¼•å…¥éš¾ä»¥å¯Ÿè§‰çš„å™ªå£°æ¥é˜²å¾¡æœªç»æˆæƒçš„æ•°æ®ä½¿ç”¨ï¼Œé˜»æ­¢æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆå­¦ä¹ ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡å®šæ€§å’Œå®šé‡å®éªŒæ­ç¤ºï¼Œå¯¹æŠ—æ ·æœ¬ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä¿æŠ¤æ€§æ‰°åŠ¨çš„å…³é”®åœ¨äºå…¶æ½œåœ¨è¡¨ç¤ºå½¢å¼çš„å¤±çœŸã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è½»é‡çº§é€‚é…å™¨çš„å¯¹æ¯”å¯¹æŠ—è®­ç»ƒï¼ˆCATï¼‰ä½œä¸ºä¸€ç§è‡ªé€‚åº”æ”»å‡»æ–¹æ³•ï¼Œæ¥çªæ˜¾ç°æœ‰ä¿æŠ¤æ–¹æ³•çš„ç¨³å¥æ€§ä¸è¶³ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CATæ–¹æ³•æ˜¾è‘—é™ä½äº†ä¿æŠ¤æ€§æ‰°åŠ¨åœ¨å®šåˆ¶ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¿ƒä½¿ç¤¾åŒºé‡æ–°è€ƒè™‘å¹¶æ”¹è¿›ç°æœ‰ä¿æŠ¤æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚ä»£ç å·²å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/senp98/CAT%E3%80%82">https://github.com/senp98/CATã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹æŠ—æ ·æœ¬å¯ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä¿æŠ¤æ€§æ‰°åŠ¨ï¼Œä¸»è¦é€šè¿‡æ½œåœ¨è¡¨ç¤ºçš„å¤±çœŸæ¥å‘æŒ¥ä½œç”¨ã€‚</li>
<li>å¯¹æ¯”å¯¹æŠ—è®­ç»ƒï¼ˆCATï¼‰æ–¹æ³•åˆ©ç”¨è½»é‡çº§é€‚é…å™¨ï¼Œé’ˆå¯¹ä¿æŠ¤æ–¹æ³•è¿›è¡Œè‡ªé€‚åº”æ”»å‡»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCATæ–¹æ³•æ˜¾è‘—é™ä½äº†ä¿æŠ¤æ€§æ‰°åŠ¨åœ¨å®šåˆ¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰ä¿æŠ¤æ–¹æ³•çš„ç¨³å¥æ€§ä¸è¶³ï¼Œéœ€è¦ç¤¾åŒºé‡æ–°è€ƒè™‘å’Œæ”¹è¿›ã€‚</li>
<li>è¯¥ç ”ç©¶æ­ç¤ºäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å®šåˆ¶åŒ–ä½¿ç”¨ä¸­çš„éšç§å’ŒçŸ¥è¯†äº§æƒé—®é¢˜ã€‚</li>
<li>å¯¹æŠ—æ ·æœ¬çš„å¼•å…¥å¯ä»¥é˜»æ­¢æ‰©æ•£æ¨¡å‹å¯¹æœªç»æˆæƒæ•°æ®çš„æœ‰æ•ˆå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cb68d5c5b68bf912c17ab578f59f9f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2963c3822eed2325dc3520b5ee9d9939.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b01da0fa78468b150ad189d3ad759fc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a18b5074aee9fa9442f40a8bded67cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d036de3d333b3651073832d6a030da2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Test-time-Contrastive-Concepts-for-Open-world-Semantic-Segmentation-with-Vision-Language-Models"><a href="#Test-time-Contrastive-Concepts-for-Open-world-Semantic-Segmentation-with-Vision-Language-Models" class="headerlink" title="Test-time Contrastive Concepts for Open-world Semantic Segmentation with   Vision-Language Models"></a>Test-time Contrastive Concepts for Open-world Semantic Segmentation with   Vision-Language Models</h2><p><strong>Authors:Monika WysoczaÅ„ska, Antonin Vobecky, Amaia Cardiel, Tomasz TrzciÅ„ski, Renaud Marlet, Andrei Bursuc, Oriane SimÃ©oni</strong></p>
<p>Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts of image-text pairs to align both modalities with a simple contrastive objective, have paved the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image pixels are assigned the closest query in feature space. However, this works well when a user exhaustively lists all possible visual concepts in an image that contrast against each other for the assignment. This corresponds to the current evaluation setup in the literature, which relies on having access to a list of in-domain relevant concepts, typically classes of a benchmark dataset. Here, we consider the more challenging (and realistic) scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic â€˜backgroundâ€™ text, we propose two different approaches to automatically generate, at test time, query-specific textual contrastive concepts. We do so by leveraging the distribution of text in the VLMâ€™s training set or crafted LLM prompts. We also propose a metric designed to evaluate this scenario and show the relevance of our approach on commonly used datasets. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒCLIPç±»è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å¤§é‡å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥ç®€å•çš„å¯¹æ¯”ç›®æ ‡å¯¹é½ä¸¤ç§æ¨¡æ€ï¼Œä¸ºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é“ºå¹³äº†é“è·¯ã€‚ç»™å®šä¸€ç»„ä»»æ„çš„æ–‡æœ¬æŸ¥è¯¢ï¼Œå›¾åƒåƒç´ ä¼šè¢«åˆ†é…åˆ°ç‰¹å¾ç©ºé—´ä¸­æœ€è¿‘çš„æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨ç”¨æˆ·è¯¦å°½åœ°åˆ—å‡ºå›¾åƒä¸­æ‰€æœ‰å¯èƒ½çš„è§†è§‰æ¦‚å¿µå¹¶ä¸”è¿™äº›æ¦‚å¿µä¹‹é—´è¿›è¡Œå¯¹æ¯”ä»¥è¿›è¡Œåˆ†é…æ—¶æ•ˆæœæœ€å¥½ã€‚è¿™å¯¹åº”äºæ–‡çŒ®ä¸­çš„å½“å‰è¯„ä¼°è®¾ç½®ï¼Œè¯¥è®¾ç½®ä¾èµ–äºè®¿é—®ç‰¹å®šé¢†åŸŸç›¸å…³æ¦‚å¿µçš„åˆ—è¡¨ï¼Œé€šå¸¸æ˜¯åŸºå‡†æ•°æ®é›†ç±»åˆ«ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§ï¼ˆå’Œæ›´ç°å®ï¼‰çš„åœºæ™¯ï¼Œå³åœ¨ç»™å®šæ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹å¯¹å•ä¸ªæ¦‚å¿µè¿›è¡Œåˆ†å‰²ï¼Œæ²¡æœ‰å…¶ä»–ä»»ä½•ä¿¡æ¯ã€‚é™¤äº†ä¸é€šç”¨çš„â€œèƒŒæ™¯â€æ–‡æœ¬è¿›è¡Œå¯¹æ¯”ä¹‹å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä¸åŒæ–¹æ³•æ¥åœ¨æµ‹è¯•æ—¶è‡ªåŠ¨ç”Ÿæˆç‰¹å®šæŸ¥è¯¢çš„å¯¹æ¯”æ¦‚å¿µã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨VLMè®­ç»ƒé›†ä¸­çš„æ–‡æœ¬åˆ†å¸ƒæˆ–ç²¾å¿ƒè®¾è®¡çš„LLMæç¤ºæ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°è¿™ç§æƒ…å†µçš„æŒ‡æ ‡ï¼Œå¹¶åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.05061v3">PDF</a> TMLR camera-ready</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºCLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„åº”ç”¨ã€‚æ¨¡å‹é€šè¿‡å¤§é‡å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥ç®€å•çš„å¯¹æ¯”ç›®æ ‡å¯¹é½ä¸¤ç§æ¨¡å¼ã€‚ç„¶è€Œï¼Œå½“å‰æ–‡çŒ®ä¸­çš„è¯„ä¼°è®¾ç½®ä¾èµ–äºè®¿é—®ä¸€ç³»åˆ—é¢†åŸŸç›¸å…³æ¦‚å¿µåˆ—è¡¨ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„åœºæ™¯ï¼Œå³ç»™å®šæ–‡æœ¬æç¤ºæ¥åˆ†å‰²å•ä¸ªæ¦‚å¿µï¼Œè€Œä¸ä¾èµ–é¢„å…ˆå®šä¹‰çš„æ¦‚å¿µåˆ—è¡¨ã€‚ä¸ºåº”å¯¹æ­¤åœºæ™¯ï¼Œé™¤äº†ä¸é€šç”¨èƒŒæ™¯æ–‡æœ¬è¿›è¡Œå¯¹æ¯”ï¼Œè¿˜æå‡ºäº†ä¸¤ç§åœ¨æµ‹è¯•æ—¶è‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢ç‰¹å®šæ–‡æœ¬å¯¹æ¯”æ¦‚å¿µçš„æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿™ä¸€åœºæ™¯çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šå±•ç¤ºäº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP-like Vision-Language Models (VLMs) å¼€å¯äº†å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ–°é€”å¾„ã€‚</li>
<li>VLMs é€šè¿‡å¤§é‡å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥ç®€å•çš„å¯¹æ¯”ç›®æ ‡å®ç°æ¨¡æ€å¯¹é½ã€‚</li>
<li>å½“å‰è¯„ä¼°è®¾ç½®å—é™äºéœ€è¦è®¿é—®é¢†åŸŸç›¸å…³æ¦‚å¿µåˆ—è¡¨ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„åœºæ™¯ï¼šä½¿ç”¨æ–‡æœ¬æç¤ºæ¥åˆ†å‰²å•ä¸ªæ¦‚å¿µã€‚</li>
<li>ä¸ºåº”å¯¹è¿™ä¸€åœºæ™¯ï¼Œé™¤äº†ä¸é€šç”¨èƒŒæ™¯æ–‡æœ¬è¿›è¡Œå¯¹æ¯”ï¼Œæå‡ºäº†ä¸¤ç§è‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢ç‰¹å®šæ–‡æœ¬å¯¹æ¯”æ¦‚å¿µçš„æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨VLMè®­ç»ƒé›†æ–‡æœ¬åˆ†å¸ƒæˆ–ç²¾å¿ƒè®¾è®¡çš„LLMæç¤ºæ¥ç”Ÿæˆå¯¹æ¯”æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.05061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1803c37ceca2a1bc39607ea6825e3980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ded018b0d58df0be1703db785e1765f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd01333f649f195da18d78c62390231.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Counterfactual-contrastive-learning-robust-representations-via-causal-image-synthesis"><a href="#Counterfactual-contrastive-learning-robust-representations-via-causal-image-synthesis" class="headerlink" title="Counterfactual contrastive learning: robust representations via causal   image synthesis"></a>Counterfactual contrastive learning: robust representations via causal   image synthesis</h2><p><strong>Authors:Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker</strong></p>
<p>Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training. </p>
<blockquote>
<p>å¯¹æ¯”é¢„è®­ç»ƒåœ¨æ”¹è¿›ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹æ³›åŒ–æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå®ƒå¯¹äºå¢å¼ºç®¡é“çš„é€‰æ‹©å¾ˆæ•æ„Ÿã€‚æ­£å‘å¯¹åº”è¯¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯åŒæ—¶ç ´åç‰¹å®šé¢†åŸŸçš„ä¿¡æ¯ã€‚æ ‡å‡†çš„å¢å¼ºç®¡é“é€šè¿‡é¢„å®šä¹‰çš„å…‰åº¦å˜æ¢æ¥æ¨¡æ‹Ÿç‰¹å®šé¢†åŸŸçš„æ”¹å˜ï¼Œä½†å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿæ¨¡æ‹ŸçœŸå®çš„é¢†åŸŸå˜åŒ–å‘¢ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å› æœå›¾åƒç”Ÿæˆçš„æœ€æ–°è¿›å±•æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†CF-SimCLRæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¿‘ä¼¼å› æœæ¨ç†çš„æ­£å‘å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚åœ¨èƒ¸éƒ¨æ”¾å°„æ‘„å½±å’Œä¹³è…ºæ‘„å½±äº”ä¸ªæ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒCF-SimCLRæå¤§åœ°æé«˜äº†å¯¹é‡‡é›†å˜åŒ–çš„ç¨³å¥æ€§ï¼Œåœ¨å†…éƒ¨å’Œå¤–éƒ¨åˆ†å¸ƒæ•°æ®çš„ä¸‹æ¸¸æ€§èƒ½è¡¨ç°å‡æœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè®­ç»ƒæœŸé—´è¡¨ç¤ºä¸è¶³çš„é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09605v3">PDF</a> Extended version available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.media.2025.103668">https://doi.org/10.1016/j.media.2025.103668</a>. This version was published in   the proceedings of the MICCAI 2024 Data Engineering in Medical Imaging   workshop. Code available at   <a target="_blank" rel="noopener" href="https://github.com/biomedia-mira/counterfactual-contrastive">https://github.com/biomedia-mira/counterfactual-contrastive</a></p>
<p><strong>Summary</strong><br>å¯¹æ¯”é¢„è®­ç»ƒåœ¨æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå…¶å¯¹äºå¢å¼ºç®¡é“çš„é€‰æ‹©éå¸¸æ•æ„Ÿã€‚æ­£å‘å¯¹éœ€è¦ä¿ç•™è¯­ä¹‰ä¿¡æ¯åŒæ—¶æ¶ˆé™¤ç‰¹å®šé¢†åŸŸä¿¡æ¯ã€‚ä¼ ç»Ÿçš„å¢å¼ºç®¡é“é€šè¿‡é¢„å®šä¹‰çš„å…‰åº¦å˜æ¢æ¨¡æ‹Ÿç‰¹å®šé¢†åŸŸçš„æ”¹å˜ï¼Œä½†å¦‚æœæˆ‘ä»¬èƒ½æ¨¡æ‹ŸçœŸå®çš„é¢†åŸŸå˜åŒ–ä¼šæ€æ ·ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å› æœå›¾åƒç”Ÿæˆçš„æœ€æ–°è¿›å±•æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†CF-SimCLRæ–¹æ³•ï¼Œä¸€ç§åˆ©ç”¨è¿‘ä¼¼å› æœæ¨ç†æ¥åˆ›å»ºæ­£å‘å¯¹çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬èƒ¸éƒ¨æ”¾å°„æ£€æŸ¥å’Œä¹³è…ºXå…‰æ£€æŸ¥ï¼Œè¯æ˜äº†CF-SimCLRåœ¨è·å–è½¬ç§»æ–¹é¢çš„ç¨³å¥æ€§ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œå¯¹è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„æ€§èƒ½éƒ½æœ‰æå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè®­ç»ƒæœŸé—´ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”é¢„è®­ç»ƒèƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>å¯¹æ¯”é¢„è®­ç»ƒçš„å¢å¼ºç®¡é“é€‰æ‹©å¯¹å…¶æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>æ­£å‘å¯¹éœ€è¦ä¿ç•™è¯­ä¹‰ä¿¡æ¯åŒæ—¶æ¶ˆé™¤ç‰¹å®šé¢†åŸŸä¿¡æ¯ã€‚</li>
<li>ä¼ ç»Ÿçš„å¢å¼ºç®¡é“é€šå¸¸æ¨¡æ‹Ÿé¢„å®šä¹‰çš„å…‰åº¦å˜æ¢æ¥æ¨¡æ‹Ÿç‰¹å®šé¢†åŸŸçš„æ”¹å˜ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ–¹æ³•CF-SimCLRï¼Œåˆ©ç”¨å› æœå›¾åƒç”ŸæˆæŠ€æœ¯æ¥æ¨¡æ‹ŸçœŸå®çš„é¢†åŸŸå˜åŒ–ã€‚</li>
<li>CF-SimCLRé€šè¿‡è¿‘ä¼¼å› æœæ¨ç†åˆ›å»ºæ­£å‘å¯¹è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-304c04009e8d5c8168468d4acddb8d44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f522181dc07a4877c2286361ec74aa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a7f972752cbac9f014183bddacaf3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01b07ddb8097087ab9877cdf884dace3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ee45cedcd6353bdb3f75d59119e8916e.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6efa04687e34bb5b76af8d48687f6128.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-22  Foundation Artificial Intelligence Models for Health Recognition Using   Face Photographs (FAHR-Face)
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
