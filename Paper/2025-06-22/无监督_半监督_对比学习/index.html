<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="无监督/半监督/对比学习">
    <meta name="description" content="无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-22  Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>无监督/半监督/对比学习 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0b3ebdc4821c4c59d09a146b320df3e3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">无监督/半监督/对比学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督/半监督/对比学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                无监督/半监督/对比学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-22-更新"><a href="#2025-06-22-更新" class="headerlink" title="2025-06-22 更新"></a>2025-06-22 更新</h1><h2 id="Exploring-Non-contrastive-Self-supervised-Representation-Learning-for-Image-based-Profiling"><a href="#Exploring-Non-contrastive-Self-supervised-Representation-Learning-for-Image-based-Profiling" class="headerlink" title="Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling"></a>Exploring Non-contrastive Self-supervised Representation Learning for   Image-based Profiling</h2><p><strong>Authors:Siran Dai, Qianqian Xu, Peisong Wen, Yang Liu, Qingming Huang</strong></p>
<p>Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025. </p>
<blockquote>
<p>基于图像的细胞分析旨在创建细胞图像的信息化表示。这一技术在药物发现中至关重要，并随着计算机视觉的最近进步而取得了巨大进展。受非对比性自监督学习（SSL）最新发展的启发，本文初步探索了使用此类方法对细胞图像进行通用特征提取器的训练。然而，存在两个主要挑战：1）细胞图像和自然图像之间的分布差异很大，导致现有SSL方法中的视图生成过程失败；2）与每个表示基于单张图像的典型场景不同，细胞分析通常涉及多张输入图像，这使得难以有效地结合所有可用信息。为了克服这些挑战，我们提出了SSLProfiler，这是一个专门为细胞分析设计的非对比性SSL框架。我们引入了针对细胞图像的专业数据增强和表示后处理方法，有效地解决了上述问题，并导致了一个稳健的特征提取器。有了这些改进，SSLProfiler在CVPR 2025的细胞系可迁移性挑战中获胜。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14265v1">PDF</a> CVPR 2025 Computer Vision for Drug Discovery</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于图像细胞分析的关键技术及其在药物发现中的应用。受到非对比性自监督学习（SSL）的启发，研究团队开发了一种针对细胞图像设计的通用特征提取器。面对细胞图像与自然图像分布差异大及多输入图像的信息整合挑战，团队提出SSLProfiler框架，包含针对细胞图像的数据增强和特征表示后处理方法，进而有效提升了特征提取的稳健性，并在CVPR 2025的细胞线可迁移性挑战中获胜。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>图像细胞分析旨在创建细胞图像的信息化表示，在药物发现中至关重要。</li>
<li>计算机视觉的最新改进推动了图像细胞分析的发展。</li>
<li>非对比性自监督学习（SSL）在图像细胞分析中具有潜力。</li>
<li>细胞图像与自然图像分布存在显著差异，导致现有SSL方法的视图生成过程失效。</li>
<li>细胞分析涉及多输入图像，整合所有可用信息是一大挑战。</li>
<li>SSLProfiler框架通过针对细胞图像的数据增强和特征表示后处理方法解决了上述问题。</li>
<li>SSLProfiler在CVPR 2025的细胞线可迁移性挑战中胜出，验证了其有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ef57b752a0f9102f00cae8f7279774af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72ee4cf68fe4479cdf57c41a5a7576e2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Contrastive-Self-Supervised-Learning-As-Neural-Manifold-Packing"><a href="#Contrastive-Self-Supervised-Learning-As-Neural-Manifold-Packing" class="headerlink" title="Contrastive Self-Supervised Learning As Neural Manifold Packing"></a>Contrastive Self-Supervised Learning As Neural Manifold Packing</h2><p><strong>Authors:Guanming Zhang, David J. Heeger, Stefano Martiniani</strong></p>
<p>Contrastive self-supervised learning based on point-wise comparisons has been widely studied for vision tasks. In the visual cortex of the brain, neuronal responses to distinct stimulus classes are organized into geometric structures known as neural manifolds. Accurate classification of stimuli can be achieved by effectively separating these manifolds, akin to solving a packing problem. We introduce Contrastive Learning As Manifold Packing (CLAMP), a self-supervised framework that recasts representation learning as a manifold packing problem. CLAMP introduces a loss function inspired by the potential energy of short-range repulsive particle systems, such as those encountered in the physics of simple liquids and jammed packings. In this framework, each class consists of sub-manifolds embedding multiple augmented views of a single image. The sizes and positions of the sub-manifolds are dynamically optimized by following the gradient of a packing loss. This approach yields interpretable dynamics in the embedding space that parallel jamming physics, and introduces geometrically meaningful hyperparameters within the loss function. Under the standard linear evaluation protocol, which freezes the backbone and trains only a linear classifier, CLAMP achieves competitive performance with state-of-the-art self-supervised models. Furthermore, our analysis reveals that neural manifolds corresponding to different categories emerge naturally and are effectively separated in the learned representation space, highlighting the potential of CLAMP to bridge insights from physics, neural science, and machine learning. </p>
<blockquote>
<p>基于点对比的对比式自监督学习在视觉任务中得到了广泛的研究。在大脑的视觉皮层中，神经元对不同刺激类别的响应被组织成称为神经流形的几何结构。通过有效地分离这些流形，类似于解决打包问题，可以实现刺激的准确分类。我们引入了对比学习作为流形打包（CLAMP），这是一种将表示学习重新定位为流形打包问题的自监督框架。CLAMP引入了一种受短程排斥粒子系统的势能启发的损失函数，如简单液体和堵塞打包的物理中所遇到的。在此框架中，每个类别由嵌入单个图像的多个增强视图的子流形组成。子流形的大小和位置通过遵循打包损失的梯度进行动态优化。这种方法在嵌入空间中产生与堵塞物理相平行的可解释动态，并在损失函数中输入了具有几何意义的超参数。在标准线性评估协议下，即冻结主干并仅训练线性分类器，CLAMP达到了与最新自监督模型相当的性能。此外，我们的分析表明，对应于不同类别的神经流形自然会出现在学习到的表示空间中，并得到有效分离，这突出了CLAMP在物理学、神经科学和机器学习之间的洞察力之间的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13717v1">PDF</a> </p>
<p><strong>Summary</strong>：基于点对比的对比式自监督学习在视觉任务上受到广泛关注。本研究引入Contrastive Learning As Manifold Packing（CLAMP）框架，将表示学习重新定义为流形打包问题。该框架受到简单液体和堆积物理中势能粒子系统的启发，利用流形动态优化类内的多个图像增强视图。这种新方法提供了一种可视化动态嵌入空间的方法，与阻塞物理类似，并在损失函数中引入了几何意义上的超参数。在标准线性评估协议下，CLAMP与最先进的自监督模型表现相当。此外，本研究揭示了不同类别的神经流形在表示空间中自然出现并得到有效分离，显示出CLAMP在物理学、神经科学和机器学习之间建立联系的潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>对比式自监督学习通过点对比广泛应用于视觉任务。</li>
<li>CLAMP框架将表示学习定义为流形打包问题。</li>
<li>CLAMP受简单液体和堆积物理中势能粒子系统的启发，利用流形优化图像增强视图。</li>
<li>CLAMP提供可视化动态嵌入空间的方法，与阻塞物理类似。</li>
<li>损失函数中包含几何意义的超参数。</li>
<li>在标准线性评估协议下，CLAMP表现与最先进的自监督模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-db0e214bbc1825e2ff5e322c902df400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ad4bca7a562948d334c0b21891f476.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3ebdc4821c4c59d09a146b320df3e3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TR2M-Transferring-Monocular-Relative-Depth-to-Metric-Depth-with-Language-Descriptions-and-Scale-Oriented-Contrast"><a href="#TR2M-Transferring-Monocular-Relative-Depth-to-Metric-Depth-with-Language-Descriptions-and-Scale-Oriented-Contrast" class="headerlink" title="TR2M: Transferring Monocular Relative Depth to Metric Depth with   Language Descriptions and Scale-Oriented Contrast"></a>TR2M: Transferring Monocular Relative Depth to Metric Depth with   Language Descriptions and Scale-Oriented Contrast</h2><p><strong>Authors:Beilei Cui, Yiming Huang, Long Bai, Hongliang Ren</strong></p>
<p>This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M’s great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BeileiCui/TR2M">https://github.com/BeileiCui/TR2M</a>) </p>
<blockquote>
<p>本文提出一个通用框架，用于将相对深度转换为度量深度。当前的单目深度估计方法主要分为度量深度估计（MMDE）和相对深度估计（MRDE）。MMDE在度量尺度上估计深度，但通常仅限于特定领域。MRDE在不同领域间具有很好的泛化能力，但尺度不确定，阻碍了下游应用。为此，我们旨在建立一个框架来解决尺度不确定性问题，并将相对深度转换为度量深度。之前的方法使用语言作为输入，并估计两个因子进行重缩放。我们的方法TR2M利用文本描述和图像作为输入，并估计两个重缩放图，以像素级别将相对深度转换为度量深度。来自两种模态的特征通过跨模态注意力模块进行融合，以更好地捕获尺度信息。设计了一种策略来构建和过滤可靠的伪度量深度，以实现更全面的监督。我们还开发了面向尺度的对比学习，利用深度分布作为指导，以强制模型学习内在知识与尺度分布对齐。TR2M只需要少量可训练的参数，就可以在各种领域的数据集上进行训练；实验不仅证明了TR2M在可见数据集上的出色性能，还展示了在五个未见数据集上的零样本优势。我们展示了在像素级别使用语言辅助将相对深度转换为度量深度的巨大潜力。（代码可用：<a target="_blank" rel="noopener" href="https://github.com/BeileiCui/TR2M%EF%BC%89">https://github.com/BeileiCui/TR2M）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13387v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个通用框架，可将相对深度转换为度量深度。该框架旨在解决尺度不确定性问题，并实现了从相对深度到度量深度的转换。与之前的方法相比，本文提出的TR2M方法不仅使用文本描述作为输入，还结合了图像输入，通过估计两个重映射关系来实现像素级别的深度转换。此外，TR2M还采用了跨模态注意力模块来融合两种模态的特征，以更好地捕获尺度信息。通过构建和过滤可靠的伪度量深度，实现了更全面的监督。此外，还开发了面向尺度的对比学习，利用深度分布作为指导，强制执行模型学习符合尺度分布的内在知识。实验表明，TR2M不仅在可见数据集上表现出良好的性能，而且在五个未见数据集上展现出卓越的零样本能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工作提出一个通用框架用于将相对深度转换为度量深度，以解决尺度不确定性问题。</li>
<li>TR2M方法结合文本描述和图像作为输入，通过估计两个重映射关系实现像素级别的深度转换。</li>
<li>TR2M采用跨模态注意力模块融合两种模态的特征，以更好地捕获尺度信息。</li>
<li>通过构建和过滤可靠的伪度量深度，实现更全面的监督。</li>
<li>引入面向尺度的对比学习，利用深度分布作为指导，促进模型学习内在知识。</li>
<li>TR2M在多个数据集上表现出卓越性能，具有强大的零样本能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-48b7208321ed325302c5ded03d03c923.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-701a8edcb6ca1b500687741f333837f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a70ec9b62a185c5126a59a546b87f22.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SUSEP-Net-Simulation-Supervised-and-Contrastive-Learning-based-Deep-Neural-Networks-for-Susceptibility-Source-Separation"><a href="#SUSEP-Net-Simulation-Supervised-and-Contrastive-Learning-based-Deep-Neural-Networks-for-Susceptibility-Source-Separation" class="headerlink" title="SUSEP-Net: Simulation-Supervised and Contrastive Learning-based Deep   Neural Networks for Susceptibility Source Separation"></a>SUSEP-Net: Simulation-Supervised and Contrastive Learning-based Deep   Neural Networks for Susceptibility Source Separation</h2><p><strong>Authors:Min Li, Chen Chen, Zhenghao Li, Yin Liu, Shanshan Shan, Peng Wu, Pengfei Rong, Feng Liu, G. Bruce Pike, Alan H. Wilman, Hongfu Sun, Yang Gao</strong></p>
<p>Quantitative susceptibility mapping (QSM) provides a valuable tool for quantifying susceptibility distributions in human brains; however, two types of opposing susceptibility sources (i.e., paramagnetic and diamagnetic), may coexist in a single voxel, and cancel each other out in net QSM images. Susceptibility source separation techniques enable the extraction of sub-voxel information from QSM maps. This study proposes a novel SUSEP-Net for susceptibility source separation by training a dual-branch U-net with a simulation-supervised training strategy. In addition, a contrastive learning framework is included to explicitly impose similarity-based constraints between the branch-specific guidance features in specially-designed encoders and the latent features in the decoders. Comprehensive experiments were carried out on both simulated and in vivo data, including healthy subjects and patients with pathological conditions, to compare SUSEP-Net with three state-of-the-art susceptibility source separation methods (i.e., APART-QSM, \c{hi}-separation, and \c{hi}-sepnet). SUSEP-Net consistently showed improved results compared with the other three methods, with better numerical metrics, improved high-intensity hemorrhage and calcification lesion contrasts, and reduced artifacts in brains with pathological conditions. In addition, experiments on an agarose gel phantom data were conducted to validate the accuracy and the generalization capability of SUSEP-Net. </p>
<blockquote>
<p>定量磁化率映射（QSM）为人类大脑中的磁化率分布提供了有价值的量化工具。然而，两种相反的磁化率源（即顺磁性物质和反磁性物质）可能存在于单个体素内，并在净QSM图像中相互抵消。磁化率源分离技术能够从QSM图中提取亚体素信息。本研究提出了一种新型的SUSEP-Net磁化率源分离方法，通过训练带有模拟监督训练策略的双分支U-net网络实现。此外，还包括一个对比学习框架，以在专门设计的编码器中的分支特定引导特征和解码器中的潜在特征之间显式施加基于相似性的约束。对模拟数据和活体数据进行了全面的实验，包括健康受试者和病理条件下的患者，将SUSEP-Net与三种最先进的磁化率源分离方法（即APART-QSM、chi分离和chi分离网）进行比较。SUSEP-Net在各项指标上均表现更好，显示出改进的结果；能够提高高灵敏度出血和钙化病灶对比度；在病理条件下的大脑中减少了伪影。此外，还进行了琼脂凝胶幻影数据的实验，以验证SUSEP-Net的准确性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13293v1">PDF</a> 8 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>一种名为SUSEP-Net的新方法被提出用于分离磁化率源。该方法通过训练带有仿真监督训练策略的双分支U-net网络来实现。此外，还包括一个对比学习框架，对分支特定的指导特征和解码器中的潜在特征施加相似性约束。与三种先进的磁化率源分离方法相比，SUSEP-Net在模拟和真实数据中均表现出更好的性能，包括健康受试者和病理条件下的患者。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SUSEP-Net被提出用于磁化率源分离。</li>
<li>使用双分支U-net结构和仿真监督训练策略进行训练。</li>
<li>对比学习框架用于施加分支特定的指导特征和潜在特征之间的相似性约束。</li>
<li>SUSEP-Net在模拟和真实数据中表现优越，包括健康人和病理条件下的病人。</li>
<li>与其他三种先进的磁化率源分离方法相比，SUSEP-Net在数值度量上表现更好。</li>
<li>SUSEP-Net能提高高强度出血和钙化病变的对比度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-90f84758d7cdc165588e65e3fd744531.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f24a4cb7fcf7679e76c18acbc003d37.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Contrastive-Learning-Using-Out-Of-Distribution-Data-for-Long-Tailed-Dataset"><a href="#Unsupervised-Contrastive-Learning-Using-Out-Of-Distribution-Data-for-Long-Tailed-Dataset" class="headerlink" title="Unsupervised Contrastive Learning Using Out-Of-Distribution Data for   Long-Tailed Dataset"></a>Unsupervised Contrastive Learning Using Out-Of-Distribution Data for   Long-Tailed Dataset</h2><p><strong>Authors:Cuong Manh Hoang, Yeejin Lee, Byeongkeun Kang</strong></p>
<p>This work addresses the task of self-supervised learning (SSL) on a long-tailed dataset that aims to learn balanced and well-separated representations for downstream tasks such as image classification. This task is crucial because the real world contains numerous object categories, and their distributions are inherently imbalanced. Towards robust SSL on a class-imbalanced dataset, we investigate leveraging a network trained using unlabeled out-of-distribution (OOD) data that are prevalently available online. We first train a network using both in-domain (ID) and sampled OOD data by back-propagating the proposed pseudo semantic discrimination loss alongside a domain discrimination loss. The OOD data sampling and loss functions are designed to learn a balanced and well-separated embedding space. Subsequently, we further optimize the network on ID data by unsupervised contrastive learning while using the previously trained network as a guiding network. The guiding network is utilized to select positive&#x2F;negative samples and to control the strengths of attractive&#x2F;repulsive forces in contrastive learning. We also distil and transfer its embedding space to the training network to maintain balancedness and separability. Through experiments on four publicly available long-tailed datasets, we demonstrate that the proposed method outperforms previous state-of-the-art methods. </p>
<blockquote>
<p>本文关注了在长尾数据集上进行自我监督学习（SSL）的任务，旨在为目标任务（如图像分类）学习平衡且良好分离的表示。这一任务至关重要，因为现实世界包含众多对象类别，它们的分布本质上是不平衡的。为了在类别不平衡数据集上实现稳健的SSL，我们研究利用使用未标记的在线普遍可用的离群分布（OOD）数据进行训练的网络。我们首先使用域内（ID）和采样的OOD数据来训练网络，通过反向传播所提出的伪语义判别损失和域判别损失。OOD数据采样和损失函数旨在学习平衡且良好分离嵌入空间。随后，我们以先前训练的网络作为引导网络，通过无监督对比学习进一步优化网络在ID数据上的表现。引导网络用于选择正&#x2F;负样本并控制对比学习中吸引&#x2F;排斥力的强度。我们还蒸馏并转移其嵌入空间到训练网络，以维持平衡性和可分离性。在四个公开可用的长尾数据集上的实验表明，所提出的方法优于先前最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12698v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>本文探讨了长尾数据集上的自监督学习任务，旨在学习平衡且良好的表示，为图像分类等下游任务提供支持。针对现实世界中的大量对象类别及其固有的不平衡分布问题，研究利用在线普遍可用的无标签的离群数据来训练网络。通过反向传播伪语义鉴别损失和领域鉴别损失来训练网络，借助离群数据的采样和损失函数来学习平衡且良好的嵌入空间。接着通过无监督对比学习进一步优化网络，并利用预训练网络作为引导网络来选择正&#x2F;负样本和控制对比学习中的吸引力和排斥力。实验结果表明，该方法在四个公开的长尾数据集上均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究关注自监督学习在长尾数据集上的应用，旨在学习平衡且良好的表示，适用于图像分类等任务。</li>
<li>研究利用在线普遍可用的无标签离群数据来训练网络，以应对现实世界中的对象类别不平衡问题。</li>
<li>通过结合领域鉴别损失和伪语义鉴别损失来训练网络，以增强网络的泛化能力并优化嵌入空间的性能。</li>
<li>利用预训练的引导网络进行无监督对比学习，提高网络的性能并增强特征表示的分离性。</li>
<li>引导网络用于选择对比学习中的正&#x2F;负样本和控制吸引与排斥力，以优化网络的训练过程。</li>
<li>通过实验验证了该方法在四个公开的长尾数据集上的优越性，证明了其在实际应用中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12698">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-68c0a4e5cf49a110d5768e150b065f2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee04cdc135325d24c417888ee2049d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd48185d7aefda7d68b07501b1c06664.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6040c48b1d1a84d30a6688e9a1dddad.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CellCLIP-–-Learning-Perturbation-Effects-in-Cell-Painting-via-Text-Guided-Contrastive-Learning"><a href="#CellCLIP-–-Learning-Perturbation-Effects-in-Cell-Painting-via-Text-Guided-Contrastive-Learning" class="headerlink" title="CellCLIP – Learning Perturbation Effects in Cell Painting via   Text-Guided Contrastive Learning"></a>CellCLIP – Learning Perturbation Effects in Cell Painting via   Text-Guided Contrastive Learning</h2><p><strong>Authors:Mingyu Lu, Ethan Weinberger, Chanwoo Kim, Su-In Lee</strong></p>
<p>High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells’ morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time. </p>
<blockquote>
<p>基于高通量显微镜技术（如细胞染色法）的高内涵筛选（HCS）测定法，已经能够以前所未有的规模探究细胞对扰动的形态反应。收集此类数据有望促进对不同扰动及其细胞状态影响之间关系的更好理解。为了实现这一目标，最近的跨模态对比学习进展从理论上可以被用来学习一个统一的潜在空间，使扰动与其相应的形态效应对齐。然而，将此类方法应用于HCS数据并不简单，因为与天然图像相比，细胞染色图像的语义存在很大差异，而且难以在单个潜在空间中表示不同类型的扰动（例如小分子与CRISPR基因敲除）。为了应对这些挑战，我们在这里引入了CellCLIP，这是一个用于HCS数据的跨模态对比学习框架。CellCLIP利用预训练图像编码器与新型通道编码方案，以更好地捕获图像嵌入中不同显微镜通道之间的关系，以及用于表示扰动的自然语言编码器。我们的框架性能优于当前开源模型，在跨模态检索和具有生物学意义的下游任务中表现出最佳性能，同时显著减少了计算时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06290v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>高内涵筛选（HCS）实验通过高通量显微镜技术如细胞染色技术，能够以前所未有的规模探究细胞形态对干扰的响应。利用这些数据，可以更好地理解不同干扰之间的关系及其对细胞状态的影响。为实现这一目标，理论上可以利用跨模态对比学习方法的最新进展来学习一个统一的潜在空间，使干扰与其相应的形态效应相匹配。然而，将这种方法应用于HCS数据并不简单，因为细胞染色图像与自然图像在语义上存在显著差异，且难以在单一潜在空间内表示不同类型的干扰（如小分子与CRISPR基因敲除）。为应对这些挑战，本文引入了CellCLIP跨模态对比学习框架。CellCLIP利用预训练的图像编码器与新型通道编码方案，更好地捕捉图像嵌入中不同显微镜通道之间的关系，以及用于表示干扰的自然语言编码器。我们的框架在跨模态检索和具有生物学意义的下游任务上表现出最佳性能，同时计算时间也大大减少。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>高内涵筛选（HCS）能够通过高通量显微镜技术大规模研究细胞形态对干扰的响应。</li>
<li>收集这些数据有助于更好地理解不同干扰与细胞状态之间的关系。</li>
<li>跨模态对比学习可用于学习一个统一潜在空间，匹配干扰与形态效应。</li>
<li>将跨模态对比学习方法应用于HCS数据存在挑战，如语义差异和不同类型干扰的表示。</li>
<li>CellCLIP框架利用预训练图像编码器和新型通道编码方案应对这些挑战。</li>
<li>CellCLIP框架在跨模态检索和生物学意义下游任务上表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e3fe46c22be2787deb2973acb2d969e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2aa7a1e33a6a1dd1412571628c9d47c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CAT-Contrastive-Adversarial-Training-for-Evaluating-the-Robustness-of-Protective-Perturbations-in-Latent-Diffusion-Models"><a href="#CAT-Contrastive-Adversarial-Training-for-Evaluating-the-Robustness-of-Protective-Perturbations-in-Latent-Diffusion-Models" class="headerlink" title="CAT: Contrastive Adversarial Training for Evaluating the Robustness of   Protective Perturbations in Latent Diffusion Models"></a>CAT: Contrastive Adversarial Training for Evaluating the Robustness of   Protective Perturbations in Latent Diffusion Models</h2><p><strong>Authors:Sen Peng, Mingyue Wang, Jianfei He, Jijia Yang, Xiaohua Jia</strong></p>
<p>Latent diffusion models have recently demonstrated superior capabilities in many downstream image synthesis tasks. However, customization of latent diffusion models using unauthorized data can severely compromise the privacy and intellectual property rights of data owners. Adversarial examples as protective perturbations have been developed to defend against unauthorized data usage by introducing imperceptible noise to customization samples, preventing diffusion models from effectively learning them. In this paper, we first reveal that the primary reason adversarial examples are effective as protective perturbations in latent diffusion models is the distortion of their latent representations, as demonstrated through qualitative and quantitative experiments. We then propose the Contrastive Adversarial Training (CAT) utilizing lightweight adapters as an adaptive attack against these protection methods, highlighting their lack of robustness. Extensive experiments demonstrate that our CAT method significantly reduces the effectiveness of protective perturbations in customization, urging the community to reconsider and improve the robustness of existing protective perturbations. The code is available at <a target="_blank" rel="noopener" href="https://github.com/senp98/CAT">https://github.com/senp98/CAT</a>. </p>
<blockquote>
<p>潜在扩散模型已在许多下游图像合成任务中展现出卓越的能力。然而，使用未经授权的数据对潜在扩散模型进行定制会严重损害数据所有者的隐私和知识产权。对抗性样本作为保护性扰动已经发展起来，通过向定制样本中加入几乎不可察觉的噪声来防御未经授权的数据使用，防止扩散模型有效地学习它们。在本文中，我们首先揭示对抗性样本作为潜在扩散模型中的保护性扰动的主要原因是其潜在表示的失真，这已通过定性和定量实验证明。然后，我们提出利用轻量级适配器作为适应性攻击的对比对抗训练（CAT），突出其缺乏稳健性。大量实验表明，我们的CAT方法显著降低了定制中保护性扰动的有效性，敦促社区重新考虑并提高现有保护性扰动的稳健性。代码可在<a target="_blank" rel="noopener" href="https://github.com/senp98/CAT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/senp98/CAT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07225v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>潜在扩散模型在下游图像合成任务中展现出卓越的能力，但其定制化使用未经授权数据会严重侵犯数据所有者的隐私和知识产权。对抗样本作为保护性扰动，通过引入难以察觉的噪声来防御未经授权的数据使用，阻止扩散模型的有效学习。本文首先通过定性和定量实验揭示，对抗样本作为潜在扩散模型中的保护性扰动的关键在于其潜在表示形式的失真。然后，我们提出利用轻量级适配器的对比对抗训练（CAT）作为一种自适应攻击方法，来突显现有保护方法的稳健性不足。大量实验表明，我们的CAT方法显著降低了保护性扰动在定制中的有效性，促使社区重新考虑并改进现有保护性扰动的稳健性。代码已公开于：<a target="_blank" rel="noopener" href="https://github.com/senp98/CAT%E3%80%82">https://github.com/senp98/CAT。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>对抗样本可作为潜在扩散模型中的保护性扰动，主要通过潜在表示的失真来发挥作用。</li>
<li>对比对抗训练（CAT）方法利用轻量级适配器，针对保护方法进行自适应攻击。</li>
<li>实验表明，CAT方法显著降低了保护性扰动在定制中的有效性。</li>
<li>现有保护方法的稳健性不足，需要社区重新考虑和改进。</li>
<li>该研究揭示了潜在扩散模型在定制化使用中的隐私和知识产权问题。</li>
<li>对抗样本的引入可以阻止扩散模型对未经授权数据的有效学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07225">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cb68d5c5b68bf912c17ab578f59f9f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2963c3822eed2325dc3520b5ee9d9939.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b01da0fa78468b150ad189d3ad759fc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a18b5074aee9fa9442f40a8bded67cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d036de3d333b3651073832d6a030da2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Test-time-Contrastive-Concepts-for-Open-world-Semantic-Segmentation-with-Vision-Language-Models"><a href="#Test-time-Contrastive-Concepts-for-Open-world-Semantic-Segmentation-with-Vision-Language-Models" class="headerlink" title="Test-time Contrastive Concepts for Open-world Semantic Segmentation with   Vision-Language Models"></a>Test-time Contrastive Concepts for Open-world Semantic Segmentation with   Vision-Language Models</h2><p><strong>Authors:Monika Wysoczańska, Antonin Vobecky, Amaia Cardiel, Tomasz Trzciński, Renaud Marlet, Andrei Bursuc, Oriane Siméoni</strong></p>
<p>Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts of image-text pairs to align both modalities with a simple contrastive objective, have paved the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image pixels are assigned the closest query in feature space. However, this works well when a user exhaustively lists all possible visual concepts in an image that contrast against each other for the assignment. This corresponds to the current evaluation setup in the literature, which relies on having access to a list of in-domain relevant concepts, typically classes of a benchmark dataset. Here, we consider the more challenging (and realistic) scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic ‘background’ text, we propose two different approaches to automatically generate, at test time, query-specific textual contrastive concepts. We do so by leveraging the distribution of text in the VLM’s training set or crafted LLM prompts. We also propose a metric designed to evaluate this scenario and show the relevance of our approach on commonly used datasets. </p>
<blockquote>
<p>最近，CLIP类视语言模型（VLMs）通过大量图像文本对进行预训练，以简单的对比目标对齐两种模态，为开放词汇语义分割铺平了道路。给定一组任意的文本查询，图像像素会被分配到特征空间中最近的查询。然而，这种方法在用户详尽地列出图像中所有可能的视觉概念并且这些概念之间进行对比以进行分配时效果最好。这对应于文献中的当前评估设置，该设置依赖于访问特定领域相关概念的列表，通常是基准数据集类别。在这里，我们考虑了一个更具挑战性（和更现实）的场景，即在给定文本提示的情况下对单个概念进行分割，没有其他任何信息。除了与通用的“背景”文本进行对比之外，我们提出了两种不同方法来在测试时自动生成特定查询的对比概念。我们通过利用VLM训练集中的文本分布或精心设计的LLM提示来实现这一点。我们还提出了一个用于评估这种情况的指标，并在常用数据集上展示了我们的方法的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.05061v3">PDF</a> TMLR camera-ready</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了基于CLIP的视觉语言模型（VLMs）在开放词汇语义分割领域的应用。模型通过大量图像文本对进行预训练，以简单的对比目标对齐两种模式。然而，当前文献中的评估设置依赖于访问一系列领域相关概念列表，这限制了模型的实用性。针对此问题，文本提出了一种更具挑战性和现实性的场景，即给定文本提示来分割单个概念，而不依赖预先定义的概念列表。为应对此场景，除了与通用背景文本进行对比，还提出了两种在测试时自动生成查询特定文本对比概念的方法。同时，还提出了一种针对这一场景的评估指标，并在常用数据集上展示了方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP-like Vision-Language Models (VLMs) 开启了开放词汇语义分割的新途径。</li>
<li>VLMs 通过大量图像文本对进行预训练，以简单的对比目标实现模态对齐。</li>
<li>当前评估设置受限于需要访问领域相关概念列表，限制了模型的实用性。</li>
<li>文本提出了一种更具挑战性和现实性的场景：使用文本提示来分割单个概念。</li>
<li>为应对这一场景，除了与通用背景文本进行对比，提出了两种自动生成查询特定文本对比概念的方法。</li>
<li>利用VLM训练集文本分布或精心设计的LLM提示来生成对比概念。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.05061">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1803c37ceca2a1bc39607ea6825e3980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ded018b0d58df0be1703db785e1765f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd01333f649f195da18d78c62390231.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Counterfactual-contrastive-learning-robust-representations-via-causal-image-synthesis"><a href="#Counterfactual-contrastive-learning-robust-representations-via-causal-image-synthesis" class="headerlink" title="Counterfactual contrastive learning: robust representations via causal   image synthesis"></a>Counterfactual contrastive learning: robust representations via causal   image synthesis</h2><p><strong>Authors:Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker</strong></p>
<p>Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training. </p>
<blockquote>
<p>对比预训练在改进下游任务性能和模型泛化方面表现良好，特别是在标签有限的情况下。然而，它对于增强管道的选择很敏感。正向对应该保留语义信息同时破坏特定领域的信息。标准的增强管道通过预定义的光度变换来模拟特定领域的改变，但如果我们能够模拟真实的领域变化呢？在这项工作中，我们展示了如何利用因果图像生成的最新进展来实现这一目标。我们提出了CF-SimCLR方法，这是一种基于近似因果推理的正向对比学习方法。在胸部放射摄影和乳腺摄影五个数据集上的全面评估表明，CF-SimCLR极大地提高了对采集变化的稳健性，在内部和外部分布数据的下游性能表现均有所提升，特别是对于训练期间表示不足的领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09605v3">PDF</a> Extended version available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.media.2025.103668">https://doi.org/10.1016/j.media.2025.103668</a>. This version was published in   the proceedings of the MICCAI 2024 Data Engineering in Medical Imaging   workshop. Code available at   <a target="_blank" rel="noopener" href="https://github.com/biomedia-mira/counterfactual-contrastive">https://github.com/biomedia-mira/counterfactual-contrastive</a></p>
<p><strong>Summary</strong><br>对比预训练在提高下游任务性能和模型泛化能力方面表现优秀，特别是在标签有限的情况下。然而，其对于增强管道的选择非常敏感。正向对需要保留语义信息同时消除特定领域信息。传统的增强管道通过预定义的光度变换模拟特定领域的改变，但如果我们能模拟真实的领域变化会怎样？在这项工作中，我们展示了如何利用因果图像生成的最新进展来实现这一目标。我们提出了CF-SimCLR方法，一种利用近似因果推理来创建正向对的对比学习方法。在五个数据集上的综合评估，包括胸部放射检查和乳腺X光检查，证明了CF-SimCLR在获取转移方面的稳健性上有显著提高，对训练和测试数据的性能都有提升，特别是对于训练期间代表性不足的领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比预训练能提高下游任务性能和模型泛化能力，尤其在标签有限的情况下。</li>
<li>对比预训练的增强管道选择对其性能有重要影响。</li>
<li>正向对需要保留语义信息同时消除特定领域信息。</li>
<li>传统的增强管道通常模拟预定义的光度变换来模拟特定领域的改变。</li>
<li>提出了新的方法CF-SimCLR，利用因果图像生成技术来模拟真实的领域变化。</li>
<li>CF-SimCLR通过近似因果推理创建正向对进行对比学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-304c04009e8d5c8168468d4acddb8d44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f522181dc07a4877c2286361ec74aa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a7f972752cbac9f014183bddacaf3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01b07ddb8097087ab9877cdf884dace3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-22/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督/半监督/对比学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ee45cedcd6353bdb3f75d59119e8916e.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech 方向最新论文已更新，请持续关注 Update in 2025-06-22  Exploiting Music Source Separation for Automatic Lyrics Transcription   with Whisper
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-22/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6efa04687e34bb5b76af8d48687f6128.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-06-22  Foundation Artificial Intelligence Models for Health Recognition Using   Face Photographs (FAHR-Face)
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
