<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  TomoSelfDEQ Self-Supervised Deep Equilibrium Learning for Sparse-Angle   CT Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ce43373886968d0e37d6a04148b8b272.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-04-æ›´æ–°"><a href="#2025-03-04-æ›´æ–°" class="headerlink" title="2025-03-04 æ›´æ–°"></a>2025-03-04 æ›´æ–°</h1><h2 id="TomoSelfDEQ-Self-Supervised-Deep-Equilibrium-Learning-for-Sparse-Angle-CT-Reconstruction"><a href="#TomoSelfDEQ-Self-Supervised-Deep-Equilibrium-Learning-for-Sparse-Angle-CT-Reconstruction" class="headerlink" title="TomoSelfDEQ: Self-Supervised Deep Equilibrium Learning for Sparse-Angle   CT Reconstruction"></a>TomoSelfDEQ: Self-Supervised Deep Equilibrium Learning for Sparse-Angle   CT Reconstruction</h2><p><strong>Authors:Tatiana A. Bubba, Matteo Santacesaria, Andrea Sebastiani</strong></p>
<p>Deep learning has emerged as a powerful tool for solving inverse problems in imaging, including computed tomography (CT). However, most approaches require paired training data with ground truth images, which can be difficult to obtain, e.g., in medical applications. We present TomoSelfDEQ, a self-supervised Deep Equilibrium (DEQ) framework for sparse-angle CT reconstruction that trains directly on undersampled measurements. We establish theoretical guarantees showing that, under suitable assumptions, our self-supervised updates match those of fully-supervised training with a loss including the (possibly non-unitary) forward operator like the CT forward map. Numerical experiments on sparse-angle CT data confirm this finding, also demonstrating that TomoSelfDEQ outperforms existing self-supervised methods, achieving state-of-the-art results with as few as 16 projection angles. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²æˆä¸ºè§£å†³æˆåƒä¸­çš„é€†é—®é¢˜ï¼ˆåŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼‰çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•éœ€è¦é…å¯¹è®­ç»ƒæ•°æ®ä¸çœŸå®å›¾åƒï¼Œè¿™åœ¨åŒ»ç–—åº”ç”¨ç­‰æƒ…å†µä¸‹å¯èƒ½éš¾ä»¥è·å¾—ã€‚æˆ‘ä»¬æå‡ºäº†TomoSelfDEQï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¨€ç–è§’åº¦CTé‡å»ºçš„è‡ªç›‘ç£æ·±åº¦å‡è¡¡ï¼ˆDEQï¼‰æ¡†æ¶ï¼Œå®ƒç›´æ¥åœ¨æ¬ é‡‡æ ·çš„æµ‹é‡å€¼ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œè¡¨æ˜åœ¨åˆé€‚çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬çš„è‡ªç›‘ç£æ›´æ–°ä¸åŒ…æ‹¬CTæ­£å‘æ˜ å°„åœ¨å†…çš„ï¼ˆå¯èƒ½æ˜¯éé…‰ï¼‰æ­£å‘æ“ä½œç¬¦çš„æŸå¤±çš„å…¨ç›‘ç£è®­ç»ƒç›¸åŒ¹é…ã€‚å¯¹ç¨€ç–è§’åº¦CTæ•°æ®çš„æ•°å€¼å®éªŒè¯å®äº†è¿™ä¸€å‘ç°ï¼Œè¿˜è¡¨æ˜TomoSelfDEQä¼˜äºç°æœ‰çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œåœ¨ä»…ä½¿ç”¨16ä¸ªæŠ•å½±è§’åº¦çš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21320v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ å·²æˆä¸ºè§£å†³æˆåƒä¸­çš„åé—®é¢˜ï¼ˆåŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼‰çš„æœ‰åŠ›å·¥å…·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•éœ€è¦é…å¯¹è®­ç»ƒæ•°æ®ä¸çœŸå®å›¾åƒï¼Œè¿™åœ¨åŒ»å­¦åº”ç”¨ä¸­å¯èƒ½éš¾ä»¥è·å¾—ã€‚æˆ‘ä»¬æå‡ºäº†TomoSelfDEQï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¨€ç–è§’åº¦CTé‡å»ºçš„è‡ªæˆ‘ç›‘ç£æ·±åº¦å‡è¡¡ï¼ˆDEQï¼‰æ¡†æ¶ï¼Œå¯ç›´æ¥åœ¨æ¬ é‡‡æ ·æµ‹é‡ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œè¡¨æ˜åœ¨åˆé€‚çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬çš„è‡ªæˆ‘ç›‘ç£æ›´æ–°ä¸åŒ…æ‹¬CTæ­£å‘æ˜ å°„åœ¨å†…çš„ï¼ˆå¯èƒ½éé…‰ï¼‰æ­£å‘ç®—å­çš„æŸå¤±çš„å…¨ç›‘ç£è®­ç»ƒç›¸åŒ¹é…ã€‚åœ¨ç¨€ç–è§’åº¦CTæ•°æ®ä¸Šçš„æ•°å€¼å®éªŒè¯å®äº†è¿™ä¸€ç‚¹ï¼Œä¹Ÿè¡¨æ˜TomoSelfDEQä¼˜äºç°æœ‰çš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•ï¼Œåœ¨ä»…ä½¿ç”¨16ä¸ªæŠ•å½±è§’åº¦çš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è§£å†³åŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨å†…çš„æˆåƒåé—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éœ€è¦é…å¯¹è®­ç»ƒæ•°æ®ä¸çœŸå®å›¾åƒï¼Œè¿™åœ¨åŒ»å­¦åº”ç”¨ä¸­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>TomoSelfDEQæ˜¯ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç¨€ç–è§’åº¦CTé‡å»ºï¼Œå¯ç›´æ¥åœ¨æ¬ é‡‡æ ·æµ‹é‡ä¸Šè®­ç»ƒã€‚</li>
<li>TomoSelfDEQçš„ç†è®ºä¿è¯è¡¨æ˜ï¼Œå…¶è‡ªæˆ‘ç›‘ç£æ›´æ–°ä¸å…¨ç›‘ç£è®­ç»ƒç›¸å½“ï¼Œå³ä½¿åŒ…æ‹¬å¯èƒ½éé…‰çš„æ­£å‘ç®—å­ã€‚</li>
<li>TomoSelfDEQåœ¨æ•°å€¼å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºç°æœ‰è‡ªæˆ‘ç›‘ç£æ–¹æ³•ã€‚</li>
<li>TomoSelfDEQåœ¨ä»…ä½¿ç”¨å°‘é‡æŠ•å½±è§’åº¦ï¼ˆå¦‚16ä¸ªï¼‰æ—¶å³å¯è¾¾åˆ°æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-689b46e011fdb6e336b51fcf8452c09e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Anatomically-guided-masked-autoencoder-pre-training-for-aneurysm-detection"><a href="#Anatomically-guided-masked-autoencoder-pre-training-for-aneurysm-detection" class="headerlink" title="Anatomically-guided masked autoencoder pre-training for aneurysm   detection"></a>Anatomically-guided masked autoencoder pre-training for aneurysm   detection</h2><p><strong>Authors:Alberto Mario Ceballos-Arroyo, Jisoo Kim, Chu-Hsuan Lin, Lei Qin, Geoffrey S. Young, Huaizu Jiang</strong></p>
<p>Intracranial aneurysms are a major cause of morbidity and mortality worldwide, and detecting them manually is a complex, time-consuming task. Albeit automated solutions are desirable, the limited availability of training data makes it difficult to develop such solutions using typical supervised learning frameworks. In this work, we propose a novel pre-training strategy using more widely available unannotated head CT scan data to pre-train a 3D Vision Transformer model prior to fine-tuning for the aneurysm detection task. Specifically, we modify masked auto-encoder (MAE) pre-training in the following ways: we use a factorized self-attention mechanism to make 3D attention computationally viable, we restrict the masked patches to areas near arteries to focus on areas where aneurysms are likely to occur, and we reconstruct not only CT scan intensity values but also artery distance maps, which describe the distance between each voxel and the closest artery, thereby enhancing the backboneâ€™s learned representations. Compared with SOTA aneurysm detection models, our approach gains +4-8% absolute Sensitivity at a false positive rate of 0.5. Code and weights will be released. </p>
<blockquote>
<p>é¢…å†…åŠ¨è„‰ç˜¤æ˜¯å…¨çƒå‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ‰‹åŠ¨æ£€æµ‹å®ƒä»¬æ˜¯ä¸€é¡¹å¤æ‚ä¸”è€—æ—¶çš„ä»»åŠ¡ã€‚å°½ç®¡å¸Œæœ›é‡‡ç”¨è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œä½¿ç”¨å…¸å‹çš„ç›‘ç£å­¦ä¹ æ¡†æ¶å¼€å‘æ­¤ç±»è§£å†³æ–¹æ¡ˆå˜å¾—å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨æ›´å¹¿æ³›å¯ç”¨çš„æœªæ³¨é‡Šå¤´éƒ¨CTæ‰«ææ•°æ®æ¥é¢„è®­ç»ƒä¸€ä¸ª3Dè§†è§‰Transformeræ¨¡å‹ï¼Œç„¶åå†å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥è¿›è¡ŒåŠ¨è„‰ç˜¤æ£€æµ‹ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒè¿›è¡Œäº†ä»¥ä¸‹ä¿®æ”¹ï¼šæˆ‘ä»¬ä½¿ç”¨å› å­åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶ä½¿3Dæ³¨æ„åŠ›åœ¨è®¡ç®—ä¸Šå¯è¡Œï¼Œæˆ‘ä»¬å°†æ©ç æ–‘å—é™åˆ¶åœ¨åŠ¨è„‰é™„è¿‘çš„åŒºåŸŸï¼Œä»¥å…³æ³¨åŠ¨è„‰ç˜¤å¯èƒ½å‘ç”Ÿçš„åŒºåŸŸï¼Œæˆ‘ä»¬é‡å»ºçš„ä¸ä»…æ˜¯CTæ‰«æå¼ºåº¦å€¼ï¼Œè¿˜æœ‰åŠ¨è„‰è·ç¦»å›¾ï¼Œæè¿°æ¯ä¸ªä½“ç´ ä¸æœ€è¿‘åŠ¨è„‰ä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œå¢å¼ºäº†ä¸»å¹²çš„å­¦ä¹ è¡¨ç¤ºã€‚ä¸æœ€å…ˆè¿›çš„åŠ¨è„‰ç˜¤æ£€æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡é˜³æ€§ç‡ä¸º0.5çš„æƒ…å†µä¸‹ï¼Œç»å¯¹çµæ•åº¦æé«˜äº†+4-8%ã€‚ä»£ç å’Œæƒé‡å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21244v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å¹¿æ³›å¯ç”¨çš„æœªæ ‡æ³¨å¤´éƒ¨CTæ‰«ææ•°æ®ï¼Œå¯¹3Dè§†è§‰è½¬æ¢å™¨æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥ç”¨äºåŠ¨è„‰ç˜¤æ£€æµ‹ä»»åŠ¡ã€‚é€šè¿‡ä¿®æ”¹æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†å¤šç§åˆ›æ–°æ‰‹æ®µä»¥æå‡æ£€æµ‹ç²¾åº¦å’Œå®ç”¨æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹ä¸å½“å‰æœ€ä½³åŠ¨è„‰ç˜¤æ£€æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å‡é˜³æ€§ç‡ä¸º0.5çš„æƒ…å†µä¸‹ï¼Œçµæ•åº¦æé«˜äº†+4-8%ã€‚è¯¥æ¨¡å‹çš„ä»£ç å’Œæƒé‡å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢…å†…åŠ¨è„‰ç˜¤æ‰‹åŠ¨æ£€æµ‹å¤æ‚ä¸”è€—æ—¶ï¼Œå­˜åœ¨è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ã€‚</li>
<li>ç”±äºè®­ç»ƒæ•°æ®æœ‰é™ï¼Œä½¿ç”¨å…¸å‹ç›‘ç£å­¦ä¹ æ¡†æ¶å¼€å‘è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨ä¸€ç§æ–°å‹é¢„è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨æœªæ ‡æ³¨çš„å¤´éƒ¨CTæ‰«ææ•°æ®è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿä¿®æ”¹äº†æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬é‡‡ç”¨å› å­åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€é™åˆ¶æ©ç åŒºåŸŸèšç„¦äºåŠ¨è„‰é™„è¿‘ä»¥åŠé‡å»ºCTæ‰«æå¼ºåº¦å€¼å’ŒåŠ¨è„‰è·ç¦»å›¾ã€‚</li>
<li>åŠ¨è„‰è·ç¦»å›¾çš„é‡å»ºå¢å¼ºäº†æ¨¡å‹å¯¹åŠ¨è„‰ç˜¤å‘ç”ŸåŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ä¸å½“å‰æœ€ä½³åŠ¨è„‰ç˜¤æ£€æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨çµæ•åº¦ä¸Šæœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a899e824bb6cd28bdb4273d29843e9d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f5e9994d1811963da99c2fcabb1c5f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c8d90e656f5b9d72766b5aa82eb8a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9c127e9d760065a123bd00f76b0e36f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf565ceb402e071b8e027c8639cdab3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="â€œNo-negatives-neededâ€-weakly-supervised-regression-for-interpretable-tumor-detection-in-whole-slide-histopathology-images"><a href="#â€œNo-negatives-neededâ€-weakly-supervised-regression-for-interpretable-tumor-detection-in-whole-slide-histopathology-images" class="headerlink" title="â€œNo negatives neededâ€: weakly-supervised regression for interpretable   tumor detection in whole-slide histopathology images"></a>â€œNo negatives neededâ€: weakly-supervised regression for interpretable   tumor detection in whole-slide histopathology images</h2><p><strong>Authors:Marina Dâ€™Amato, Jeroen van der Laak, Francesco Ciompi</strong></p>
<p>Accurate tumor detection in digital pathology whole-slide images (WSIs) is crucial for cancer diagnosis and treatment planning. Multiple Instance Learning (MIL) has emerged as a widely used approach for weakly-supervised tumor detection with large-scale data without the need for manual annotations. However, traditional MIL methods often depend on classification tasks that require tumor-free cases as negative examples, which are challenging to obtain in real-world clinical workflows, especially for surgical resection specimens. We address this limitation by reformulating tumor detection as a regression task, estimating tumor percentages from WSIs, a clinically available target across multiple cancer types. In this paper, we provide an analysis of the proposed weakly-supervised regression framework by applying it to multiple organs, specimen types and clinical scenarios. We characterize the robustness of our framework to tumor percentage as a noisy regression target, and introduce a novel concept of amplification technique to improve tumor detection sensitivity when learning from small tumor regions. Finally, we provide interpretable insights into the modelâ€™s predictions by analyzing visual attention and logit maps. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/tumor-percentage-mil-regression">https://github.com/DIAGNijmegen/tumor-percentage-mil-regression</a>. </p>
<blockquote>
<p>åœ¨æ•°å­—ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­è¿›è¡Œå‡†ç¡®çš„è‚¿ç˜¤æ£€æµ‹å¯¹äºç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ä½œä¸ºä¸€ç§å¹¿æ³›ä½¿ç”¨çš„å¼±ç›‘ç£è‚¿ç˜¤æ£€æµ‹æ–¹æ³•ï¼Œå¯ä»¥åœ¨å¤§è§„æ¨¡æ•°æ®çš„æƒ…å†µä¸‹æ— éœ€æ‰‹åŠ¨æ³¨é‡Šå°±èƒ½å®ç°åº”ç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„MILæ–¹æ³•é€šå¸¸ä¾èµ–äºéœ€è¦æ— è‚¿ç˜¤ç—…ä¾‹ä½œä¸ºè´Ÿä¾‹çš„åˆ†ç±»ä»»åŠ¡ï¼Œè¿™åœ¨ç°å®çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­éš¾ä»¥è·å¾—ï¼Œå°¤å…¶æ˜¯åœ¨æ‰‹æœ¯åˆ‡é™¤æ ‡æœ¬ä¸­ã€‚æˆ‘ä»¬é€šè¿‡å°†è‚¿ç˜¤æ£€æµ‹é‡æ–°åˆ¶å®šä¸ºå›å½’ä»»åŠ¡æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä»WSIä¸­ä¼°è®¡è‚¿ç˜¤ç™¾åˆ†æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨å¤šç§ç™Œç—‡ç±»å‹çš„ä¸´åºŠå¯ç”¨ç›®æ ‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ‰€æå‡ºçš„å¼±ç›‘ç£å›å½’æ¡†æ¶åº”ç”¨äºå¤šä¸ªå™¨å®˜ã€æ ‡æœ¬ç±»å‹å’Œä¸´åºŠåœºæ™¯ï¼Œå¯¹å…¶è¿›è¡Œäº†åˆ†æã€‚æˆ‘ä»¬å¯¹æ¡†æ¶å¯¹è‚¿ç˜¤ç™¾åˆ†æ¯”ä½œä¸ºå™ªå£°å›å½’ç›®æ ‡çš„ç¨³å¥æ€§è¿›è¡Œäº†æè¿°ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ”¾å¤§æŠ€æœ¯çš„æ–°æ¦‚å¿µï¼Œä»¥æé«˜ä»å°è‚¿ç˜¤åŒºåŸŸå­¦ä¹ æ—¶çš„è‚¿ç˜¤æ£€æµ‹çµæ•åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æè§†è§‰æ³¨æ„åŠ›å’Œé€»è¾‘å›¾ï¼Œæä¾›äº†å¯¹æ¨¡å‹é¢„æµ‹çš„ç›´è§‚è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/tumor-percentage-mil-regression%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DIAGNijmegen/tumor-percentage-mil-regressionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21109v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›å½’æ¡†æ¶çš„å¼±ç›‘ç£è‚¿ç˜¤æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡å¯¹å…¨åˆ‡ç‰‡æ•°å­—ç—…ç†å›¾åƒï¼ˆWSIsï¼‰ä¸­çš„è‚¿ç˜¤æ¯”ä¾‹è¿›è¡Œä¼°è®¡ï¼Œå®ç°å¯¹å¤šç§ç™Œç—‡ç±»å‹çš„è‚¿ç˜¤æ£€æµ‹ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ä¾èµ–è‚¿ç˜¤ç—…ä¾‹ä½œä¸ºè´Ÿæ ·æœ¬çš„é—®é¢˜ï¼Œé€šè¿‡å°†è‚¿ç˜¤æ£€æµ‹é‡æ–°å®šä¹‰ä¸ºå›å½’ä»»åŠ¡ï¼Œæé«˜äº†æ¨¡å‹å¯¹è‚¿ç˜¤æ¯”ä¾‹çš„é²æ£’æ€§ï¼Œå¹¶å¼•å…¥æ”¾å¤§æŠ€æœ¯æé«˜å¯¹å°è‚¿ç˜¤åŒºåŸŸçš„æ£€æµ‹çµæ•åº¦ã€‚åŒæ—¶ï¼Œé€šè¿‡å¯è§†åŒ–åˆ†æå’Œé€»è¾‘å›¾è§£ææä¾›æ¨¡å‹é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¿ç˜¤æ£€æµ‹æ˜¯æ•°å­—ç—…ç†ä¸­é‡è¦çš„ä»»åŠ¡ï¼Œå¯¹äºç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ä¾èµ–äºåˆ†ç±»ä»»åŠ¡ï¼Œéœ€è¦è‚¿ç˜¤ç—…ä¾‹ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œè¿™åœ¨ç°å®çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­éš¾ä»¥è·å–ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å°†è‚¿ç˜¤æ£€æµ‹é‡æ–°å®šä¹‰ä¸ºå›å½’ä»»åŠ¡ï¼Œä¼°è®¡WSIsä¸­çš„è‚¿ç˜¤æ¯”ä¾‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ä¸´åºŠä¸­å¯è·å¾—çš„ã€é€‚ç”¨äºå¤šç§ç™Œç—‡ç±»å‹çš„ç›®æ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ¨¡å‹å¯¹è‚¿ç˜¤æ¯”ä¾‹çš„é²æ£’æ€§ï¼Œå¹¶å¼•å…¥æ”¾å¤§æŠ€æœ¯ä»¥æé«˜å¯¹å°è‚¿ç˜¤åŒºåŸŸçš„æ£€æµ‹çµæ•åº¦ã€‚</li>
<li>é€šè¿‡å¯è§†åŒ–åˆ†æå’Œé€»è¾‘å›¾è§£æï¼Œæä¾›äº†æ¨¡å‹é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§å™¨å®˜ã€æ ‡æœ¬ç±»å‹å’Œä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨å¾—åˆ°äº†åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-20d4c922e3c555748874ef6533c7bb27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a57911fc298882cd3d224bdc9b31deab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bce3b961165ccfaa3a47ad3619f8bbe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb8e82ebc2efb0591e3e81c39021249.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce43373886968d0e37d6a04148b8b272.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9551552126a67f15a60b6f8e391fcbad.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Individualized-Aging-Brains-in-Health-and-Disease-with-Generative-Models-and-Parallel-Transport"><a href="#Synthesizing-Individualized-Aging-Brains-in-Health-and-Disease-with-Generative-Models-and-Parallel-Transport" class="headerlink" title="Synthesizing Individualized Aging Brains in Health and Disease with   Generative Models and Parallel Transport"></a>Synthesizing Individualized Aging Brains in Health and Disease with   Generative Models and Parallel Transport</h2><p><strong>Authors:Jingru Fu, Yuqi Zheng, Neel Dey, Daniel Ferreira, Rodrigo Moreno</strong></p>
<p>Simulating prospective magnetic resonance imaging (MRI) scans from a given individual brain image is challenging, as it requires accounting for canonical changes in aging and&#x2F;or disease progression while also considering the individual brainâ€™s current status and unique characteristics. While current deep generative models can produce high-resolution anatomically accurate templates for population-wide studies, their ability to predict future aging trajectories for individuals remains limited, particularly in capturing subject-specific neuroanatomical variations over time. In this study, we introduce Individualized Brain Synthesis (InBrainSyn), a framework for synthesizing high-resolution subject-specific longitudinal MRI scans that simulate neurodegeneration in both Alzheimerâ€™s disease (AD) and normal aging. InBrainSyn uses a parallel transport algorithm to adapt the population-level aging trajectories learned by a generative deep template network, enabling individualized aging synthesis. As InBrainSyn uses diffeomorphic transformations to simulate aging, the synthesized images are topologically consistent with the original anatomy by design. We evaluated InBrainSyn both quantitatively and qualitatively on AD and healthy control cohorts from the Open Access Series of Imaging Studies - version 3 dataset. Experimentally, InBrainSyn can also model neuroanatomical transitions between normal aging and AD. An evaluation of an external set supports its generalizability. Overall, with only a single baseline scan, InBrainSyn synthesizes realistic 3D spatiotemporal T1w MRI scans, producing personalized longitudinal aging trajectories. The code for InBrainSyn is available at: <a target="_blank" rel="noopener" href="https://github.com/Fjr9516/InBrainSyn">https://github.com/Fjr9516/InBrainSyn</a>. </p>
<blockquote>
<p>æ ¹æ®ç»™å®šçš„ä¸ªäººè„‘å›¾åƒæ¨¡æ‹Ÿæœªæ¥ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™éœ€è¦åœ¨è€ƒè™‘å…¸å‹çš„è¡°è€å’Œï¼ˆæˆ–ï¼‰ç–¾ç—…è¿›å±•å˜åŒ–çš„åŒæ—¶ï¼Œè¿˜è¦è€ƒè™‘ä¸ªäººå¤§è„‘çš„å½“å‰çŠ¶å†µå’Œç‹¬ç‰¹ç‰¹å¾ã€‚è™½ç„¶ç›®å‰çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹å¯ä»¥ä¸ºç¾¤ä½“ç ”ç©¶äº§ç”Ÿé«˜åˆ†è¾¨ç‡ä¸”è§£å‰–ç»“æ„å‡†ç¡®çš„æ¨¡æ¿ï¼Œä½†å®ƒä»¬é¢„æµ‹ä¸ªäººæœªæ¥è¡°è€è½¨è¿¹çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•æ‰å—è¯•è€…ç‰¹å®šçš„ç¥ç»è§£å‰–ç»“æ„éšæ—¶é—´å˜åŒ–æ–¹é¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸ªæ€§åŒ–è„‘åˆæˆï¼ˆInBrainSynï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ç”¨äºåˆæˆé«˜åˆ†è¾¨ç‡çš„å—è¯•è€…ç‰¹å®šçºµå‘MRIæ‰«æï¼Œæ¨¡æ‹Ÿé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰å’Œæ­£å¸¸è¡°è€ä¸­çš„ç¥ç»å˜æ€§ã€‚InBrainSynä½¿ç”¨å¹¶è¡Œä¼ è¾“ç®—æ³•ï¼Œé€‚åº”ç”±ç”Ÿæˆæ·±åº¦æ¨¡æ¿ç½‘ç»œå­¦ä¹ çš„äººç¾¤æ°´å¹³è¡°è€è½¨è¿¹ï¼Œä»è€Œå®ç°ä¸ªæ€§åŒ–çš„è¡°è€åˆæˆã€‚ç”±äºInBrainSynä½¿ç”¨å¾®åˆ†åŒèƒšå˜æ¢æ¥æ¨¡æ‹Ÿè¡°è€ï¼Œå› æ­¤åˆæˆå›¾åƒåœ¨è®¾è®¡ä¸Šä¸åŸå§‹è§£å‰–ç»“æ„åœ¨æ‹“æ‰‘ä¸Šæ˜¯ä¸€è‡´çš„ã€‚æˆ‘ä»¬åœ¨å¼€æ”¾è®¿é—®æˆåƒç ”ç©¶ç¬¬ä¸‰ç‰ˆæ•°æ®é›†ä¸Šå¯¹ADå’Œå¥åº·å¯¹ç…§ç»„çš„InBrainSynè¿›è¡Œäº†å®šé‡å’Œå®šæ€§çš„è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒInBrainSynè¿˜å¯ä»¥æ¨¡æ‹Ÿæ­£å¸¸è¡°è€å’ŒADä¹‹é—´çš„ç¥ç»è§£å‰–ç»“æ„è¿‡æ¸¡ã€‚å¯¹å¤–éƒ¨æ•°æ®é›†çš„è¯„ä¼°æ”¯æŒäº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼Œåªéœ€ä¸€æ¬¡åŸºçº¿æ‰«æï¼ŒInBrainSynå°±å¯ä»¥åˆæˆé€¼çœŸçš„3Dæ—¶ç©ºT1w MRIæ‰«æï¼Œäº§ç”Ÿä¸ªæ€§åŒ–çš„çºµå‘è¡°è€è½¨è¿¹ã€‚InBrainSynçš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Fjr9516/InBrainSyn">https://github.com/Fjr9516/InBrainSyn</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21049v1">PDF</a> 20 pages, 9 figures, 6 tables, diffeomorphic registration, parallel   transport, brain aging, medical image generation, Alzheimerâ€™s disease</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºIndividualized Brain Synthesisï¼ˆInBrainSynï¼‰çš„æ¡†æ¶ï¼Œå®ƒèƒ½åˆæˆé«˜åˆ†è¾¨ç‡çš„ä¸ªä½“ç‰¹å®šçºµå‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æï¼Œæ¨¡æ‹Ÿé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰å’Œæ­£å¸¸è¡°è€è¿‡ç¨‹ä¸­çš„ç¥ç»é€€åŒ–ã€‚InBrainSynä½¿ç”¨å¹¶è¡Œä¼ è¾“ç®—æ³•ï¼Œå°†ç¾¤ä½“æ°´å¹³çš„è¡°è€è½¨è¿¹ä¸æ·±åº¦ç”Ÿæˆæ¨¡æ¿ç½‘ç»œç›¸ç»“åˆï¼Œå®ç°ä¸ªæ€§åŒ–çš„è¡°è€åˆæˆã€‚å®éªŒè¯æ˜ï¼ŒInBrainSynèƒ½å¤Ÿæ¨¡æ‹Ÿæ­£å¸¸è¡°è€å’ŒADä¹‹é—´çš„ç¥ç»è§£å‰–ç»“æ„è¿‡æ¸¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InBrainSynæ¡†æ¶èƒ½å¤Ÿåˆæˆé«˜åˆ†è¾¨ç‡çš„ä¸ªä½“ç‰¹å®šçºµå‘MRIæ‰«æï¼Œæ¨¡æ‹ŸADå’Œæ­£å¸¸è¡°è€è¿‡ç¨‹ä¸­çš„ç¥ç»é€€åŒ–ã€‚</li>
<li>InBrainSynä½¿ç”¨å¹¶è¡Œä¼ è¾“ç®—æ³•å’Œå¾®åˆ†åŒèƒšå˜æ¢ï¼Œä½¿åˆæˆçš„å›¾åƒä¸åŸå§‹è§£å‰–ç»“æ„ä¸€è‡´ã€‚</li>
<li>è¯¥æ¡†æ¶å¯ä»¥å°†ç¾¤ä½“æ°´å¹³çš„è¡°è€è½¨è¿¹ä¸æ·±åº¦ç”Ÿæˆæ¨¡æ¿ç½‘ç»œç»“åˆï¼Œå®ç°ä¸ªæ€§åŒ–çš„è¡°è€åˆæˆã€‚</li>
<li>InBrainSynèƒ½å¤Ÿåœ¨å•ä¸€åŸºçº¿æ‰«æçš„æƒ…å†µä¸‹ï¼ŒåˆæˆçœŸå®çš„3Dæ—¶ç©ºT1w MRIæ‰«æã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒInBrainSynå¯ä»¥æ¨¡æ‹Ÿæ­£å¸¸è¡°è€å’ŒADä¹‹é—´çš„ç¥ç»è§£å‰–ç»“æ„è¿‡æ¸¡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…¬å¼€è®¿é—®æˆåƒç ”ç©¶ç¬¬ä¸‰ç‰ˆæ•°æ®é›†ä¸Šçš„ADå’Œå¥åº·å¯¹ç…§ç»„ä¸­è¿›è¡Œäº†å®šé‡å’Œå®šæ€§çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edaf550ddd2464c6ca006f596fcdbe23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f847e0f760e624523e1eca1bd40b589.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-382f1e871d472f2b11e314a30e38b66c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LesionLocator-Zero-Shot-Universal-Tumor-Segmentation-and-Tracking-in-3D-Whole-Body-Imaging"><a href="#LesionLocator-Zero-Shot-Universal-Tumor-Segmentation-and-Tracking-in-3D-Whole-Body-Imaging" class="headerlink" title="LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D   Whole-Body Imaging"></a>LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D   Whole-Body Imaging</h2><p><strong>Authors:Maximilian Rokuss, Yannick Kirchhoff, Seval Akbal, Balint Kovacs, Saikat Roy, Constantin Ulrich, Tassilo Wald, Lukas T. Rotkopf, Heinz-Peter Schlemmer, Klaus Maier-Hein</strong></p>
<p>In this work, we present LesionLocator, a framework for zero-shot longitudinal lesion tracking and segmentation in 3D medical imaging, establishing the first end-to-end model capable of 4D tracking with dense spatial prompts. Our model leverages an extensive dataset of 23,262 annotated medical scans, as well as synthesized longitudinal data across diverse lesion types. The diversity and scale of our dataset significantly enhances model generalizability to real-world medical imaging challenges and addresses key limitations in longitudinal data availability. LesionLocator outperforms all existing promptable models in lesion segmentation by nearly 10 dice points, reaching human-level performance, and achieves state-of-the-art results in lesion tracking, with superior lesion retrieval and segmentation accuracy. LesionLocator not only sets a new benchmark in universal promptable lesion segmentation and automated longitudinal lesion tracking but also provides the first open-access solution of its kind, releasing our synthetic 4D dataset and model to the community, empowering future advancements in medical imaging. Code is available at: <a target="_blank" rel="noopener" href="http://www.github.com/MIC-DKFZ/LesionLocator">www.github.com/MIC-DKFZ/LesionLocator</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LesionLocatoræ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä¸‰ç»´åŒ»å­¦æˆåƒä¸­çš„é›¶æ ·æœ¬çºµå‘ç—…å˜è·Ÿè¸ªå’Œåˆ†å‰²çš„æ¡†æ¶ï¼Œå»ºç«‹äº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œå››ç»´è·Ÿè¸ªçš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå…·æœ‰å¯†é›†çš„ç©ºé—´æç¤ºã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„23262ä¸ªæ ‡æ³¨åŒ»å­¦æ‰«ææ•°æ®é›†ï¼Œä»¥åŠåˆæˆå„ç§ç—…å˜ç±»å‹çš„çºµå‘æ•°æ®ã€‚æ•°æ®é›†çš„å¤šæ ·æ€§å’Œè§„æ¨¡æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ç°å®ä¸–ç•ŒåŒ»å­¦å½±åƒæŒ‘æˆ˜çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è§£å†³äº†çºµå‘æ•°æ®å¯ç”¨æ€§æ–¹é¢çš„å…³é”®é™åˆ¶ã€‚LesionLocatoråœ¨ç—…å˜åˆ†å‰²æ–¹é¢ä¼˜äºæ‰€æœ‰ç°æœ‰çš„å¯æç¤ºæ¨¡å‹ï¼Œç‹„æ°æŒ‡æ•°æé«˜äº†è¿‘10ä¸ªç‚¹ï¼Œè¾¾åˆ°äº†äººç±»æ°´å¹³æ€§èƒ½ï¼Œå¹¶åœ¨ç—…å˜è·Ÿè¸ªæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå…·æœ‰å‡ºè‰²çš„ç—…å˜æ£€ç´¢å’Œåˆ†å‰²å‡†ç¡®æ€§ã€‚LesionLocatorä¸ä»…ä¸ºé€šç”¨å¯æç¤ºç—…å˜åˆ†å‰²å’Œè‡ªåŠ¨çºµå‘ç—…å˜è·Ÿè¸ªè®¾å®šäº†æ–°çš„åŸºå‡†çº¿ï¼Œè€Œä¸”è¿˜æä¾›äº†æ­¤ç±»é¦–ä¸ªå¼€æºè§£å†³æ–¹æ¡ˆï¼Œå‘ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„åˆæˆå››ç»´æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸ºåŒ»å­¦å½±åƒçš„æœªæ¥è¿›æ­¥æä¾›æ”¯æŒã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://www.github.com/MIC-DKFZ/LesionLocator%E6%89%BE%E5%88%B0%E3%80%82">www.github.com/MIC-DKFZ/LesionLocatoræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20985v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ–°ç ”ç©¶æ¡†æ¶LesionLocatorå®ç°äº†é›¶æ ·æœ¬çºµå‘ç—…å˜è¿½è¸ªå’Œåˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è§„æ¨¡æ ‡æ³¨åŒ»å­¦æ‰«ææ•°æ®å’Œåˆæˆçºµå‘æ•°æ®ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹çœŸå®ä¸–ç•ŒåŒ»å­¦æˆåƒæŒ‘æˆ˜çš„é€šç”¨æ€§ï¼Œå¹¶åœ¨ç—…å˜è¿½è¸ªå’Œåˆ†å‰²æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚LesionLocatorä¸ä»…è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œè€Œä¸”æä¾›é¦–ä¸ªå¼€æ”¾è®¿é—®çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å…¬å¼€åˆæˆ4Dæ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸ºåŒ»å­¦æˆåƒçš„æœªæ¥è¿›æ­¥æä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LesionLocatoræ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬çºµå‘ç—…å˜è¿½è¸ªå’Œåˆ†å‰²çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨å¤§è§„æ¨¡çš„æ ‡æ³¨åŒ»å­¦æ‰«ææ•°æ®å’Œåˆæˆçºµå‘æ•°æ®ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåº”å¯¹çœŸå®ä¸–ç•ŒåŒ»å­¦æˆåƒæŒ‘æˆ˜ã€‚</li>
<li>LesionLocatoråœ¨ç—…å˜è¿½è¸ªå’Œåˆ†å‰²æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œè¶…è¶Šäº†ç°æœ‰å¯æç¤ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å®ƒè¾¾åˆ°äº†äººç±»æ°´å¹³çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…å˜åˆ†å‰²æ–¹é¢ã€‚</li>
<li>LesionLocatoræä¾›äº†é¦–ä¸ªå¼€æ”¾è®¿é—®çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å…¬å¼€åˆæˆ4Dæ•°æ®é›†å’Œæ¨¡å‹èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-653868b55d0fa7a82bf59a608130facb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b4dfcb83a314166e1bf2841547a6a34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdafa3883664cc92ef4ea4518ef2ee7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b22b5c4d7459270b80503d06852222.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20829a55ae2c20ceabebf81895c46c3c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Guiding-Quantitative-MRI-Reconstruction-with-Phase-wise-Uncertainty"><a href="#Guiding-Quantitative-MRI-Reconstruction-with-Phase-wise-Uncertainty" class="headerlink" title="Guiding Quantitative MRI Reconstruction with Phase-wise Uncertainty"></a>Guiding Quantitative MRI Reconstruction with Phase-wise Uncertainty</h2><p><strong>Authors:Haozhong Sun, Zhongsen Li, Chenlin Du, Haokun Li, Yajie Wang, Huijun Chen</strong></p>
<p>Quantitative magnetic resonance imaging (qMRI) requires multi-phase acqui-sition, often relying on reduced data sampling and reconstruction algorithms to accelerate scans, which inherently poses an ill-posed inverse problem. While many studies focus on measuring uncertainty during this process, few explore how to leverage it to enhance reconstruction performance. In this paper, we in-troduce PUQ, a novel approach that pioneers the use of uncertainty infor-mation for qMRI reconstruction. PUQ employs a two-stage reconstruction and parameter fitting framework, where phase-wise uncertainty is estimated during reconstruction and utilized in the fitting stage. This design allows uncertainty to reflect the reliability of different phases and guide information integration during parameter fitting. We evaluated PUQ on in vivo T1 and T2 mapping datasets from healthy subjects. Compared to existing qMRI reconstruction methods, PUQ achieved the state-of-the-art performance in parameter map-pings, demonstrating the effectiveness of uncertainty guidance. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PUQ-75B2/">https://anonymous.4open.science/r/PUQ-75B2/</a>. </p>
<blockquote>
<p>å®šé‡ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰éœ€è¦å¤šé˜¶æ®µé‡‡é›†ï¼Œé€šå¸¸ä¾èµ–äºå‡å°‘æ•°æ®é‡‡æ ·å’Œé‡å»ºç®—æ³•æ¥åŠ é€Ÿæ‰«æï¼Œè¿™æœ¬è´¨ä¸Šæ„æˆäº†ä¸€ä¸ªä¸é€‚å®šçš„åé—®é¢˜ã€‚å°½ç®¡è®¸å¤šç ”ç©¶å…³æ³¨äºæ­¤è¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§æµ‹é‡ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶æ¢ç´¢å¦‚ä½•åˆ©ç”¨ä¸ç¡®å®šæ€§æ¥æé«˜é‡å»ºæ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PUQï¼Œè¿™æ˜¯ä¸€ç§å¼€åˆ›æ€§åœ°å°†ä¸ç¡®å®šæ€§ä¿¡æ¯ç”¨äºqMRIé‡å»ºçš„æ–°æ–¹æ³•ã€‚PUQé‡‡ç”¨ä¸¤é˜¶æ®µé‡å»ºå’Œå‚æ•°æ‹Ÿåˆæ¡†æ¶ï¼Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­ä¼°è®¡é˜¶æ®µæ€§ä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨æ‹Ÿåˆé˜¶æ®µåŠ ä»¥åˆ©ç”¨ã€‚è¿™ç§è®¾è®¡å…è®¸ä¸ç¡®å®šæ€§åæ˜ ä¸åŒé˜¶æ®µçš„å¯é æ€§ï¼Œå¹¶åœ¨å‚æ•°æ‹Ÿåˆè¿‡ç¨‹ä¸­æŒ‡å¯¼ä¿¡æ¯æ•´åˆã€‚æˆ‘ä»¬å¯¹å¥åº·å—è¯•è€…çš„æ´»ä½“T1å’ŒT2æ˜ å°„æ•°æ®é›†è¯„ä¼°äº†PUQã€‚ä¸ç°æœ‰çš„qMRIé‡å»ºæ–¹æ³•ç›¸æ¯”ï¼ŒPUQåœ¨å‚æ•°æ˜ å°„æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œè¯æ˜äº†ä¸ç¡®å®šæ€§æŒ‡å¯¼çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PUQ-75B2/]%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/r/PUQ-75B2/]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20877v1">PDF</a> Submitted to MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å®šé‡ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰é‡å»ºæ–¹æ³•PUQï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸ç¡®å®šæ€§ä¿¡æ¯æ¥æé«˜é‡å»ºæ€§èƒ½ã€‚PUQé‡‡ç”¨ä¸¤é˜¶æ®µé‡å»ºå’Œå‚æ•°æ‹Ÿåˆæ¡†æ¶ï¼Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­ä¼°è®¡é˜¶æ®µä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨æ‹Ÿåˆé˜¶æ®µåŠ ä»¥åˆ©ç”¨ã€‚PUQåœ¨å¥åº·å—è¯•è€…çš„ä½“å†…T1å’ŒT2æ˜ å°„æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰qMRIé‡å»ºæ–¹æ³•ï¼Œè¯æ˜äº†ä¸ç¡®å®šæ€§æŒ‡å¯¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PUQæ˜¯ä¸€ç§æ–°å‹çš„å®šé‡ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰é‡å»ºæ–¹æ³•ã€‚</li>
<li>PUQåˆ©ç”¨ä¸ç¡®å®šæ€§ä¿¡æ¯æ¥æé«˜é‡å»ºæ€§èƒ½ã€‚</li>
<li>PUQé‡‡ç”¨ä¸¤é˜¶æ®µé‡å»ºå’Œå‚æ•°æ‹Ÿåˆæ¡†æ¶ã€‚</li>
<li>åœ¨é‡å»ºè¿‡ç¨‹ä¸­ï¼ŒPUQä¼°è®¡é˜¶æ®µä¸ç¡®å®šæ€§ã€‚</li>
<li>é˜¶æ®µä¸ç¡®å®šæ€§åœ¨å‚æ•°æ‹Ÿåˆé˜¶æ®µå¾—åˆ°åˆ©ç”¨ã€‚</li>
<li>PUQåœ¨å¥åº·å—è¯•è€…çš„ä½“å†…T1å’ŒT2æ˜ å°„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-430916e9060e2cacbb5f448c35d3260f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-455ea381393ac39789c3966392970f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-650365ee8ee9ab80b6dad8fc7a3cf91f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-312f7ba06cb36e02a116228fe35b6fa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4b4c1f6cb90d59d7dbd2d16524c4bfc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Delta-WKV-A-Novel-Meta-in-Context-Learner-for-MRI-Super-Resolution"><a href="#Delta-WKV-A-Novel-Meta-in-Context-Learner-for-MRI-Super-Resolution" class="headerlink" title="Delta-WKV: A Novel Meta-in-Context Learner for MRI Super-Resolution"></a>Delta-WKV: A Novel Meta-in-Context Learner for MRI Super-Resolution</h2><p><strong>Authors:Rongchang Lu, Bingcheng Liao, Haowen Hou, Jiahang Lv, Xin Hai</strong></p>
<p>Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the challenges such as long scan times and expensive equipment by enhancing image resolution from low-quality inputs acquired in shorter scan times in clinical settings. However, current SR techniques still have problems such as limited ability to capture both local and global static patterns effectively and efficiently. To address these limitations, we propose Delta-WKV, a novel MRI super-resolution model that combines Meta-in-Context Learning (MiCL) with the Delta rule to better recognize both local and global patterns in MRI images. This approach allows Delta-WKV to adjust weights dynamically during inference, improving pattern recognition with fewer parameters and less computational effort, without using state-space modeling. Additionally, inspired by Receptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional scanning mechanism with time-mixing and channel-mixing structures to capture long-range dependencies while maintaining high-frequency details. Tests on the IXI and fastMRI datasets show that Delta-WKV outperforms existing methods, improving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and inference times by over 15%. These results demonstrate its efficiency and potential for clinical use with large datasets and high-resolution imaging. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯é€šè¿‡å¢å¼ºä¸´åºŠç¯å¢ƒä¸­è¾ƒçŸ­æ‰«ææ—¶é—´å†…è·å–çš„å›¾åƒåˆ†è¾¨ç‡æ¥è§£å†³æ‰«ææ—¶é—´é•¿å’Œæ˜‚è´µçš„è®¾å¤‡æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç›®å‰çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯ä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œä¾‹å¦‚æœ‰é™çš„æ•è·å±€éƒ¨å’Œå…¨å±€é™æ€æ¨¡å¼çš„èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆä¸”é«˜æ•ˆåœ°å®Œæˆã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Delta-WKVè¿™ä¸€æ–°å‹MRIè¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†Meta-in-Context Learningï¼ˆMiCLï¼‰å’ŒDeltaè§„åˆ™ï¼Œå¯ä»¥æ›´å¥½åœ°è¯†åˆ«MRIå›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•å…è®¸Delta-WKVåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä½¿ç”¨æ›´å°‘çš„å‚æ•°å’Œè®¡ç®—èµ„æºæé«˜æ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨çŠ¶æ€ç©ºé—´å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œå—Receptance Weighted Key Valueï¼ˆRWKVï¼‰çš„å¯å‘ï¼ŒDelta-WKVé‡‡ç”¨å››å‘æ‰«ææœºåˆ¶ï¼Œå…·æœ‰æ—¶é—´æ··åˆå’Œé€šé“æ··åˆç»“æ„ï¼Œä»¥æ•æ‰é•¿æœŸä¾èµ–å…³ç³»çš„åŒæ—¶ä¿æŒé«˜é¢‘ç»†èŠ‚ã€‚åœ¨IXIå’ŒfastMRIæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒDelta-WKVä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰0.06åˆ†è´å’Œç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰0.001ï¼ŒåŒæ—¶è®­ç»ƒå’Œæ¨ç†æ—¶é—´ç¼©çŸ­äº†è¶…è¿‡15%ã€‚è¿™äº›ç»“æœè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡æ•°æ®é›†å’Œé«˜åˆ†è¾¨ç‡æˆåƒä¸­çš„æ•ˆç‡å’Œä¸´åºŠåº”ç”¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20852v1">PDF</a> This paper has been published to MICCAI 2025. Feel free to contact on   <a href="mailto:&#110;&#111;&#x6d;&#111;&#100;&#x65;&#115;&#101;&#x74;&#x40;&#x71;&#113;&#46;&#x63;&#x6f;&#109;">&#110;&#111;&#x6d;&#111;&#100;&#x65;&#115;&#101;&#x74;&#x40;&#x71;&#113;&#46;&#x63;&#x6f;&#109;</a></p>
<p><strong>Summary</strong></p>
<p>MRIè¶…åˆ†è¾¨ç‡æŠ€æœ¯é€šè¿‡æé«˜ä½è´¨é‡è¾“å…¥çš„å›¾åƒåˆ†è¾¨ç‡æ¥è§£å†³æ‰«ææ—¶é—´é•¿å’Œæ˜‚è´µçš„è®¾å¤‡é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ï¼ŒDelta-WKVæ¨¡å‹ç»“åˆäº†Meta-in-Context Learningï¼ˆMiCLï¼‰å’ŒDeltaè§„åˆ™ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰MRIå›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ã€‚è¯¥æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œæé«˜äº†æ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†å‚æ•°å’Œè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒDelta-WKVé‡‡ç”¨å››æ–¹å‘æ‰«ææœºåˆ¶ï¼Œæ•æ‰é•¿æœŸä¾èµ–å…³ç³»å¹¶ä¿æŒé«˜é¢‘ç»†èŠ‚ï¼Œæé«˜äº†æ€§èƒ½ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒDelta-WKVåœ¨IXIå’ŒfastMRIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIè¶…åˆ†è¾¨ç‡æŠ€æœ¯æ—¨åœ¨è§£å†³æ‰«ææ—¶é—´é•¿å’Œæ˜‚è´µè®¾å¤‡çš„é—®é¢˜ï¼Œé€šè¿‡æé«˜ä½è´¨é‡å›¾åƒçš„åˆ†è¾¨ç‡ã€‚</li>
<li>å½“å‰MRIè¶…åˆ†è¾¨ç‡æŠ€æœ¯é¢ä¸´æ•æ‰å±€éƒ¨å’Œå…¨å±€æ¨¡å¼çš„æŒ‘æˆ˜ã€‚</li>
<li>Delta-WKVæ¨¡å‹ç»“åˆäº†Meta-in-Context Learningï¼ˆMiCLï¼‰å’ŒDeltaè§„åˆ™ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¯†åˆ«MRIå›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ã€‚</li>
<li>Delta-WKVèƒ½åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œæé«˜æ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘å‚æ•°å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>Delta-WKVé‡‡ç”¨å››æ–¹å‘æ‰«ææœºåˆ¶ï¼Œç»“åˆäº†æ—¶é—´æ··åˆå’Œé€šé“æ··åˆç»“æ„ï¼Œä»¥æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å¹¶ä¿æŒé«˜é¢‘ç»†èŠ‚ã€‚</li>
<li>åœ¨IXIå’ŒfastMRIæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒDelta-WKVçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94c38313784b3458ae83c734b618b892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2272a3df362ff1d2f4029bab4a3ec98a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58ebf75fd24c5b5bf5ddac5d16b8e048.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Cyanothiazole-Copper-I-Complexes-Uncharted-Materials-with-Exceptional-Optical-and-Conductive-Properties"><a href="#Cyanothiazole-Copper-I-Complexes-Uncharted-Materials-with-Exceptional-Optical-and-Conductive-Properties" class="headerlink" title="Cyanothiazole Copper(I) Complexes: Uncharted Materials with Exceptional   Optical and Conductive Properties"></a>Cyanothiazole Copper(I) Complexes: Uncharted Materials with Exceptional   Optical and Conductive Properties</h2><p><strong>Authors:Karolina GutmaÅ„ska, Agnieszka Podborska, Tomasz Mazur, Andrzej SÅ‚awek, Ramesh Sivasamy, Alexey Maximenko, Åukasz OrzeÅ‚, Janusz Oszajca, GraÅ¼yna Stochel, Konrad SzaciÅ‚owski, Anna DoÅ‚Ä™ga</strong></p>
<p>Cyanothiazoles, small and quite overlooked molecules, possess remarkable optical properties that can be fine-tuned through coordination with transition metals. In this study, we investigate a promising application of cyanothiazoles, where their combination with copper(I) iodide forms a new class of complexes exhibiting outstanding optical properties. X-ray crystallography of copper(I) iodide complexes with isomeric cyanothiazoles revealed key structural features, such as {\pi}-{\pi} stacking, hydrogen bonding, and rare halogen-chalcogen I-S interactions, enhancing stability and reactivity. Advanced spectroscopy and computational modeling allowed precise identification of spectral signatures in FTIR, NMR, and UV-Vis spectra. Fluorescence studies, along with XANES synchrotron analyses, highlighted their unique thermal and electronic properties, providing a solid foundation for further research in the field. </p>
<blockquote>
<p>æ°°åŸºå™»å”‘æ˜¯ä¸€ç§å°è€Œå¸¸è¢«å¿½è§†çš„åˆ†å­ï¼Œå…·æœ‰æ˜¾è‘—çš„å…‰å­¦ç‰¹æ€§ï¼Œå¯ä»¥é€šè¿‡ä¸è¿‡æ¸¡é‡‘å±çš„é…åˆè¿›è¡Œå¾®è°ƒã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ°°åŸºå™»å”‘çš„ä¸€ç§æœ‰å‰é€”çš„åº”ç”¨ï¼Œå³å…¶ä¸ç¢˜åŒ–äºšé“œç»“åˆå½¢æˆä¸€ç±»æ–°çš„å¤åˆç‰©ï¼Œå±•ç°å‡ºå“è¶Šçš„å…‰å­¦ç‰¹æ€§ã€‚é€šè¿‡Xå°„çº¿æ™¶ä½“å­¦åˆ†æç¢˜åŒ–äºšé“œä¸å¼‚æ„æ°°åŸºå™»å”‘çš„å¤åˆç‰©ï¼Œæ­ç¤ºäº†å…³é”®çš„ç»“æ„ç‰¹å¾ï¼Œå¦‚Ï€-Ï€å †ç§¯ã€æ°¢é”®å’Œç½•è§çš„å¤ç´ -ç¡«æ—å…ƒç´ I-Sç›¸äº’ä½œç”¨ï¼Œå¢å¼ºäº†å…¶ç¨³å®šæ€§å’Œååº”æ€§ã€‚å…ˆè¿›çš„å…‰è°±å­¦å’Œè®¡ç®—å»ºæ¨¡å…è®¸åœ¨çº¢å¤–å…‰è°±ã€æ ¸ç£å…±æŒ¯å’Œç´«å¤–å¯è§å…‰è°±ä¸­ç²¾ç¡®è¯†åˆ«å…‰è°±ç‰¹å¾ã€‚è§å…‰ç ”ç©¶ä»¥åŠXANESåŒæ­¥åŠ é€Ÿå™¨åˆ†æçªå‡ºäº†å…¶ç‹¬ç‰¹çš„çƒ­å­¦å’Œç”µå­ç‰¹æ€§ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20848v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Cyanothiazolesä¸é“œï¼ˆIï¼‰ç¢˜åŒ–ç‰©ç»“åˆå½¢æˆçš„æ–°å‹å¤åˆç‰©çš„å…‰å­¦æ€§è´¨ã€‚é€šè¿‡Xå°„çº¿æ™¶ä½“å­¦çš„ç ”ç©¶ï¼Œå‘ç°è¿™äº›å¤åˆç‰©å…·æœ‰å…³é”®çš„Ï€-Ï€å †ç§¯ã€æ°¢é”®å’Œç½•è§çš„å¤ç´ -ç¡«æ—å…ƒç´ I-Sç›¸äº’ä½œç”¨ç­‰ç»“æ„ç‰¹å¾ã€‚é«˜çº§å…‰è°±å­¦å’Œè®¡ç®—å»ºæ¨¡æŠ€æœ¯ç²¾ç¡®é‰´å®šäº†çº¢å¤–ã€æ ¸ç£å’Œç´«å¤–å¯è§å…‰è°±ä¸­çš„å…‰è°±ç‰¹å¾ã€‚è§å…‰ç ”ç©¶ä»¥åŠåŒæ­¥åŠ é€Ÿå™¨åˆ†æçªå‡ºäº†å…¶ç‹¬ç‰¹çš„çƒ­å’Œç”µå­æ€§è´¨ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cyanothiazolesä¸é“œï¼ˆIï¼‰ç¢˜åŒ–ç‰©ç»“åˆå½¢æˆæ–°å‹å¤åˆç‰©ã€‚</li>
<li>é€šè¿‡Xå°„çº¿æ™¶ä½“å­¦æ­ç¤ºäº†å¤åˆç‰©çš„å…³é”®ç»“æ„ç‰¹å¾ï¼Œå¦‚Ï€-Ï€å †ç§¯ã€æ°¢é”®å’Œç½•è§çš„I-Sç›¸äº’ä½œç”¨ã€‚</li>
<li>é«˜çº§å…‰è°±å­¦å’Œè®¡ç®—å»ºæ¨¡æŠ€æœ¯ç”¨äºç²¾ç¡®é‰´å®šå…‰è°±ç‰¹å¾ã€‚</li>
<li>è§å…‰ç ”ç©¶è¯å®äº†è¿™äº›å¤åˆç‰©çš„ç‹¬ç‰¹çƒ­å’Œç”µå­æ€§è´¨ã€‚</li>
<li>è¿™äº›å¤åˆç‰©å±•ç°å‡ºå“è¶Šçš„å…‰å­¦æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡ä¸ºè¿™ä¸€é¢†åŸŸæœªæ¥çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bff32385d3d04962ed77207a04a98bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e4cef89613b2c9ec4540e8c62b39325.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd70c74538cf6ab06ba8f5d3eb0c7f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64dd982a3523a60e65a5aca633ae1c2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c95ce71e99a1b97129d14a18b872b18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76c1c08d320b1ad22e29843dc9be5cc3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Medical-Image-Segmentation-via-Next-Scale-Mask-Prediction"><a href="#Autoregressive-Medical-Image-Segmentation-via-Next-Scale-Mask-Prediction" class="headerlink" title="Autoregressive Medical Image Segmentation via Next-Scale Mask Prediction"></a>Autoregressive Medical Image Segmentation via Next-Scale Mask Prediction</h2><p><strong>Authors:Tao Chen, Chenhui Wang, Zhihao Chen, Hongming Shan</strong></p>
<p>While deep learning has significantly advanced medical image segmentation, most existing methods still struggle with handling complex anatomical regions. Cascaded or deep supervision-based approaches attempt to address this challenge through multi-scale feature learning but fail to establish sufficient inter-scale dependencies, as each scale relies solely on the features of the immediate predecessor. To this end, we propose the AutoRegressive Segmentation framework via next-scale mask prediction, termed AR-Seg, which progressively predicts the next-scale mask by explicitly modeling dependencies across all previous scales within a unified architecture. AR-Seg introduces three innovations: (1) a multi-scale mask autoencoder that quantizes the mask into multi-scale token maps to capture hierarchical anatomical structures, (2) a next-scale autoregressive mechanism that progressively predicts next-scale masks to enable sufficient inter-scale dependencies, and (3) a consensus-aggregation strategy that combines multiple sampled results to generate a more accurate mask, further improving segmentation robustness. Extensive experimental results on two benchmark datasets with different modalities demonstrate that AR-Seg outperforms state-of-the-art methods while explicitly visualizing the intermediate coarse-to-fine segmentation process. </p>
<blockquote>
<p>è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚è§£å‰–åŒºåŸŸæ—¶ä»é¢ä¸´å›°éš¾ã€‚çº§è”æˆ–åŸºäºæ·±åº¦ç›‘ç£çš„æ–¹æ³•è¯•å›¾é€šè¿‡å¤šå°ºåº¦ç‰¹å¾å­¦ä¹ æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†æœªèƒ½å»ºç«‹è¶³å¤Ÿçš„è·¨å°ºåº¦ä¾èµ–å…³ç³»ï¼Œå› ä¸ºæ¯ä¸ªå°ºåº¦åªä¾èµ–äºç›´æ¥å‰åºå°ºåº¦çš„ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡ä¸‹ä¸€å°ºåº¦æ©è†œé¢„æµ‹çš„è‡ªå›å½’åˆ†å‰²æ¡†æ¶ï¼Œç§°ä¸ºAR-Segã€‚AR-Segåœ¨ç»Ÿä¸€æ¶æ„å†…æ˜¾å¼å»ºæ¨¡æ‰€æœ‰å…ˆå‰å°ºåº¦ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œé€æ­¥é¢„æµ‹ä¸‹ä¸€å°ºåº¦æ©è†œã€‚AR-Segå¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°ï¼š1ï¼‰å¤šå°ºåº¦æ©è†œè‡ªç¼–ç å™¨å°†æ©è†œé‡åŒ–ä¸ºå¤šå°ºåº¦ä»¤ç‰Œå›¾ï¼Œä»¥æ•è·å±‚æ¬¡åŒ–çš„è§£å‰–ç»“æ„ï¼›2ï¼‰ä¸‹ä¸€å°ºåº¦è‡ªå›å½’æœºåˆ¶é€æ­¥é¢„æµ‹ä¸‹ä¸€å°ºåº¦æ©è†œï¼Œä»¥å®ç°è¶³å¤Ÿçš„è·¨å°ºåº¦ä¾èµ–å…³ç³»ï¼›3ï¼‰å…±è¯†èšåˆç­–ç•¥ç»“åˆäº†å¤šä¸ªé‡‡æ ·ç»“æœï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„æ©è†œï¼Œè¿›ä¸€æ­¥æé«˜åˆ†å‰²çš„ç¨³å¥æ€§ã€‚åœ¨ä¸¤ä¸ªä¸åŒæ¨¡æ€çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒAR-Segä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶æ˜ç¡®å¯è§†åŒ–ä»ç²—åˆ°ç»†çš„ä¸­é—´åˆ†å‰²è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20784v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‡ªå›å½’åˆ†å‰²æ¡†æ¶çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œåä¸ºAR-Segã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šå°ºåº¦æ©è†œè‡ªç¼–ç å™¨æ•æ‰å±‚æ¬¡åŒ–çš„è§£å‰–ç»“æ„ï¼Œå¹¶é‡‡ç”¨è‡ªå›å½’æœºåˆ¶é€æ­¥é¢„æµ‹ä¸‹ä¸€å°ºåº¦æ©è†œï¼Œå®ç°å……åˆ†çš„è·¨å°ºåº¦ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼ŒAR-Segè¿˜é‡‡ç”¨å…±è¯†èšåˆç­–ç•¥ï¼Œç»“åˆå¤šä¸ªé‡‡æ ·ç»“æœç”Ÿæˆæ›´å‡†ç¡®çš„æ©è†œï¼Œæé«˜äº†åˆ†å‰²çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAR-Segåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR-Segæ¡†æ¶é€šè¿‡å¤šå°ºåº¦ç‰¹å¾å­¦ä¹ å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å¤æ‚è§£å‰–åŒºåŸŸé—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨å°ºåº¦ä¾èµ–å…³ç³»æ—¶å­˜åœ¨ä¸è¶³ï¼Œè€ŒAR-Segé€šè¿‡è‡ªå›å½’æœºåˆ¶é€æ­¥é¢„æµ‹ä¸‹ä¸€å°ºåº¦æ©è†œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>AR-Segå¼•å…¥å¤šå°ºåº¦æ©è†œè‡ªç¼–ç å™¨ï¼Œå°†æ©è†œé‡åŒ–ä¸ºå¤šå°ºåº¦ä»¤ç‰Œå›¾ï¼Œä»¥æ•æ‰å±‚æ¬¡åŒ–çš„è§£å‰–ç»“æ„ã€‚</li>
<li>AR-Segé‡‡ç”¨å…±è¯†èšåˆç­–ç•¥ï¼Œç»“åˆå¤šä¸ªé‡‡æ ·ç»“æœç”Ÿæˆæ›´å‡†ç¡®æ©è†œï¼Œæé«˜åˆ†å‰²é²æ£’æ€§ã€‚</li>
<li>AR-Segåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
<li>AR-Segèƒ½å¤Ÿå¯è§†åŒ–ä¸­é—´ç²—åˆ°ç»†çš„åˆ†å‰²è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bf7de293592bf96e76e349770b6515c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b10e915ffd79e2a17a1be6daf859223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b3dd483541163f805782ca274fd04f0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CADDreamer-CAD-object-Generation-from-Single-view-Images"><a href="#CADDreamer-CAD-object-Generation-from-Single-view-Images" class="headerlink" title="CADDreamer: CAD object Generation from Single-view Images"></a>CADDreamer: CAD object Generation from Single-view Images</h2><p><strong>Authors:Yuan Li, Cheng Lin, Yuan Liu, Xiaoxiao Long, Chenxu Zhang, Ningna Wang, Xin Li, Wenping Wang, Xiaohu Guo</strong></p>
<p>Diffusion-based 3D generation has made remarkable progress in recent years. However, existing 3D generative models often produce overly dense and unstructured meshes, which stand in stark contrast to the compact, structured, and sharply-edged Computer-Aided Design (CAD) models crafted by human designers. To address this gap, we introduce CADDreamer, a novel approach for generating boundary representations (B-rep) of CAD objects from a single image. CADDreamer employs a primitive-aware multi-view diffusion model that captures both local geometric details and high-level structural semantics during the generation process. By encoding primitive semantics into the color domain, the method leverages the strong priors of pre-trained diffusion models to align with well-defined primitives. This enables the inference of multi-view normal maps and semantic maps from a single image, facilitating the reconstruction of a mesh with primitive labels. Furthermore, we introduce geometric optimization techniques and topology-preserving extraction methods to mitigate noise and distortion in the generated primitives. These enhancements result in a complete and seamless B-rep of the CAD model. Experimental results demonstrate that our method effectively recovers high-quality CAD objects from single-view images. Compared to existing 3D generation techniques, the B-rep models produced by CADDreamer are compact in representation, clear in structure, sharp in edges, and watertight in topology. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„3Dç”ŸæˆæŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dç”Ÿæˆæ¨¡å‹å¾€å¾€äº§ç”Ÿè¿‡äºå¯†é›†ä¸”æ— ç»“æ„çš„ç½‘æ ¼ï¼Œè¿™ä¸äººç±»è®¾è®¡å¸ˆç²¾å¿ƒåˆ¶ä½œçš„ç´§å‡‘ã€ç»“æ„åŒ–ã€è¾¹ç¼˜æ¸…æ™°çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CADDreamerï¼Œè¿™æ˜¯ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡å¯¹è±¡è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰çš„æ–°æ–¹æ³•ã€‚CADDreameré‡‡ç”¨äº†ä¸€ç§åŸå§‹æ„ŸçŸ¥å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ•æ‰å±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜çº§ç»“æ„è¯­ä¹‰ã€‚é€šè¿‡å°†åŸå§‹è¯­ä¹‰ç¼–ç åˆ°é¢œè‰²åŸŸä¸­ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ä¸å®šä¹‰è‰¯å¥½çš„åŸå§‹å…ƒç´ è¿›è¡Œå¯¹é½ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä»å•å¼ å›¾åƒæ¨æ–­å¤šè§†è§’æ³•çº¿è´´å›¾å’Œè¯­ä¹‰è´´å›¾ï¼Œä¿ƒè¿›å¸¦æœ‰åŸå§‹æ ‡ç­¾çš„ç½‘æ ¼é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œä»¥å‡è½»ç”ŸæˆåŸå§‹å…ƒç´ ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚è¿™äº›å¢å¼ºåŠŸèƒ½å¯¼è‡´äº†ä¸€ä¸ªå®Œæ•´ä¸”æ— ç¼éš™çš„CADæ¨¡å‹çš„B-repã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»å•è§†å›¾å›¾åƒä¸­æœ‰æ•ˆåœ°æ¢å¤äº†é«˜è´¨é‡çš„CADå¯¹è±¡ã€‚ä¸ç°æœ‰çš„3Dç”ŸæˆæŠ€æœ¯ç›¸æ¯”ï¼ŒCADDreameräº§ç”Ÿçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºä¸Šæ›´ç´§å‡‘ã€åœ¨ç»“æ„ä¸Šæ›´æ¸…æ™°ã€åœ¨è¾¹ç¼˜ä¸Šæ›´é”‹åˆ©ã€åœ¨æ‹“æ‰‘ä¸Šæ›´æ— ç¼éš™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20732v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„3Dç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æ¨¡å‹å­˜åœ¨çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCADDreamerçš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•ä¸€å›¾åƒç”ŸæˆCADå¯¹è±¡çš„è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åŸå§‹æ„ŸçŸ¥çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜çº§ç»“æ„è¯­ä¹‰è¿›è¡Œç”Ÿæˆã€‚é€šè¿‡ç¼–ç åŸå§‹è¯­ä¹‰åˆ°é¢œè‰²åŸŸï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„å¼ºå…ˆéªŒçŸ¥è¯†ï¼Œå®ç°äº†ä¸é¢„å®šä¹‰åŸå§‹å›¾å½¢çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œå‡å°‘äº†ç”ŸæˆåŸå§‹å›¾å½¢ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä»å•è§†å›¾å›¾åƒä¸­æ¢å¤é«˜è´¨é‡çš„CADå¯¹è±¡ï¼Œç”Ÿæˆçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºã€ç»“æ„ã€è¾¹ç¼˜å’Œæ‹“æ‰‘æ–¹é¢å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰3Dç”Ÿæˆæ¨¡å‹ä¼šäº§ç”Ÿè¿‡äºå¯†é›†å’Œæ— ç»“æ„çš„ç½‘æ ¼ï¼Œä¸äººç±»çš„CADæ¨¡å‹è®¾è®¡å­˜åœ¨å·®è·ã€‚</li>
<li>CADDreameræ–¹æ³•èƒ½å¤Ÿä»å•ä¸€å›¾åƒç”ŸæˆCADå¯¹è±¡çš„è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰ã€‚</li>
<li>CADDreameré‡‡ç”¨åŸå§‹æ„ŸçŸ¥çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜çº§ç»“æ„è¯­ä¹‰è¿›è¡Œç”Ÿæˆã€‚</li>
<li>é€šè¿‡ç¼–ç åŸå§‹è¯­ä¹‰åˆ°é¢œè‰²åŸŸï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå…ˆéªŒçŸ¥è¯†ï¼Œå®ç°ä¸é¢„å®šä¹‰åŸå§‹å›¾å½¢çš„å¯¹é½ã€‚</li>
<li>å¼•å…¥å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œå‡å°‘ç”ŸæˆåŸå§‹å›¾å½¢ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚</li>
<li>å®éªŒç»“æœè¯æ˜CADDreameræ–¹æ³•èƒ½ä»å•è§†å›¾å›¾åƒä¸­æœ‰æ•ˆæ¢å¤é«˜è´¨é‡çš„CADå¯¹è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14453b9ed8a69591b1a6ab794f2ca1be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7b67d84798689f08d8b98bb15badb03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-daf5a0d3dee0506d349493f6eebb2faa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7e9ff7f89b5b57ed6168d8196c65314.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfdf93393cc68bbbf03807bc8f30c8cc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-AI-Powered-Medical-Image-Synthesis-Insights-from-MedVQA-GI-Challenge-Using-CLIP-Fine-Tuned-Stable-Diffusion-and-Dream-Booth-LoRA"><a href="#Advancing-AI-Powered-Medical-Image-Synthesis-Insights-from-MedVQA-GI-Challenge-Using-CLIP-Fine-Tuned-Stable-Diffusion-and-Dream-Booth-LoRA" class="headerlink" title="Advancing AI-Powered Medical Image Synthesis: Insights from MedVQA-GI   Challenge Using CLIP, Fine-Tuned Stable Diffusion, and Dream-Booth + LoRA"></a>Advancing AI-Powered Medical Image Synthesis: Insights from MedVQA-GI   Challenge Using CLIP, Fine-Tuned Stable Diffusion, and Dream-Booth + LoRA</h2><p><strong>Authors:Ojonugwa Oluwafemi Ejiga Peter, Md Mahmudur Rahman, Fahmi Khalifa</strong></p>
<p>The MEDVQA-GI challenge addresses the integration of AI-driven text-to-image generative models in medical diagnostics, aiming to enhance diagnostic capabilities through synthetic image generation. Existing methods primarily focus on static image analysis and lack the dynamic generation of medical imagery from textual descriptions. This study intends to partially close this gap by introducing a novel approach based on fine-tuned generative models to generate dynamic, scalable, and precise images from textual descriptions. Particularly, our system integrates fine-tuned Stable Diffusion and DreamBooth models, as well as Low-Rank Adaptation (LORA), to generate high-fidelity medical images. The problem is around two sub-tasks namely: image synthesis (IS) and optimal prompt production (OPG). The former creates medical images via verbal prompts, whereas the latter provides prompts that produce high-quality images in specified categories. The study emphasizes the limitations of traditional medical image generation methods, such as hand sketching, constrained datasets, static procedures, and generic models. Our evaluation measures showed that Stable Diffusion surpasses CLIP and DreamBooth + LORA in terms of producing high-quality, diversified images. Specifically, Stable Diffusion had the lowest Fr&#39;echet Inception Distance (FID) scores (0.099 for single center, 0.064 for multi-center, and 0.067 for combined), indicating higher image quality. Furthermore, it had the highest average Inception Score (2.327 across all datasets), indicating exceptional diversity and quality. This advances the field of AI-powered medical diagnosis. Future research will concentrate on model refining, dataset augmentation, and ethical considerations for efficiently implementing these advances into clinical practice </p>
<blockquote>
<p>MEDVQA-GIæŒ‘æˆ˜è‡´åŠ›äºå°†äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹æ•´åˆåˆ°åŒ»å­¦è¯Šæ–­ä¸­ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆå›¾åƒç”Ÿæˆå¢å¼ºè¯Šæ–­èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºé™æ€å›¾åƒåˆ†æï¼Œç¼ºä¹ä»æ–‡æœ¬æè¿°ä¸­åŠ¨æ€ç”ŸæˆåŒ»å­¦å›¾åƒçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§åŸºäºç²¾ç»†è°ƒæ•´ç”Ÿæˆæ¨¡å‹çš„æ–°å‹æ–¹æ³•ï¼Œæ¥éƒ¨åˆ†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä»¥ä»æ–‡æœ¬æè¿°ä¸­ç”ŸæˆåŠ¨æ€ã€å¯ä¼¸ç¼©å’Œç²¾ç¡®çš„å›¾åƒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé›†æˆäº†ç²¾ç»†è°ƒæ•´çš„Stable Diffusionå’ŒDreamBoothæ¨¡å‹ï¼Œä»¥åŠä½ç§©é€‚åº”ï¼ˆLORAï¼‰ï¼Œä»¥ç”Ÿæˆé«˜ä¿çœŸåŒ»å­¦å›¾åƒã€‚è¯¥é—®é¢˜æ¶‰åŠä¸¤ä¸ªå­ä»»åŠ¡ï¼šå›¾åƒåˆæˆï¼ˆISï¼‰å’Œæœ€ä½³æç¤ºç”Ÿæˆï¼ˆOPGï¼‰ã€‚å‰è€…é€šè¿‡æ–‡å­—æç¤ºåˆ›å»ºåŒ»å­¦å›¾åƒï¼Œè€Œåè€…æä¾›åœ¨æŒ‡å®šç±»åˆ«ä¸­äº§ç”Ÿé«˜è´¨é‡å›¾åƒçš„æç¤ºã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¼ ç»ŸåŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚æ‰‹ç»˜ã€å—é™æ•°æ®é›†ã€é™æ€ç¨‹åºå’Œé€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–å›¾åƒæ–¹é¢ï¼ŒStable Diffusionè¶…è¶Šäº†CLIPå’ŒDreamBooth + LORAã€‚å…·ä½“æ¥è¯´ï¼ŒStable Diffusionçš„FrÃ©chet Inception Distanceï¼ˆFIDï¼‰åˆ†æ•°æœ€ä½ï¼ˆå•ä¸­å¿ƒä¸º0.099ï¼Œå¤šä¸­å¿ƒä¸º0.064ï¼Œç»„åˆä¸º0.067ï¼‰ï¼Œè¡¨æ˜å›¾åƒè´¨é‡è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œå®ƒå…·æœ‰æœ€é«˜çš„å¹³å‡Inception Scoreï¼ˆæ‰€æœ‰æ•°æ®é›†ä¸Šä¸º2.327ï¼‰ï¼Œè¡¨æ˜å…·æœ‰å‡ºè‰²çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚è¿™æ¨åŠ¨äº†äººå·¥æ™ºèƒ½é©±åŠ¨åŒ»å­¦è¯Šæ–­é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ç ”ç©¶å°†é›†ä¸­åœ¨æ¨¡å‹ç²¾ç‚¼ã€æ•°æ®é›†å¢å¼ºä»¥åŠå°†è¿™äº›è¿›æ­¥æœ‰æ•ˆåœ°åº”ç”¨äºä¸´åºŠå®è·µçš„ä¼¦ç†è€ƒè™‘ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20667v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>MEDVQA-GIæŒ‘æˆ˜æ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ•´åˆé—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆå›¾åƒç”Ÿæˆå¢å¼ºè¯Šæ–­èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é™æ€å›¾åƒåˆ†æï¼Œç¼ºä¹ä»æ–‡æœ¬æè¿°ä¸­åŠ¨æ€ç”ŸæˆåŒ»å­¦å›¾åƒçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼•å…¥åŸºäºç²¾ç»†è°ƒæ•´çš„ç”Ÿæˆæ¨¡å‹çš„æ–°å‹æ–¹æ³•ï¼Œéƒ¨åˆ†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä»¥ä»æ–‡æœ¬æè¿°ä¸­ç”ŸæˆåŠ¨æ€ã€å¯ä¼¸ç¼©å’Œç²¾ç¡®çš„å›¾åƒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé›†æˆäº†å¾®è°ƒåçš„Stable Diffusionå’ŒDreamBoothæ¨¡å‹ï¼Œä»¥åŠä½ç§©é€‚é…ï¼ˆLORAï¼‰ï¼Œä»¥ç”Ÿæˆé«˜ä¿çœŸåŒ»å­¦å›¾åƒã€‚é—®é¢˜ä¸»è¦å›´ç»•ä¸¤ä¸ªå­ä»»åŠ¡ï¼šå›¾åƒåˆæˆï¼ˆISï¼‰å’Œæœ€ä½³æç¤ºç”Ÿæˆï¼ˆOPGï¼‰ã€‚å‰è€…é€šè¿‡æ–‡å­—æç¤ºåˆ›å»ºåŒ»å­¦å›¾åƒï¼Œè€Œåè€…æä¾›äº§ç”Ÿé«˜è´¨é‡å›¾åƒçš„æç¤ºã€‚ç ”ç©¶å¼ºè°ƒäº†ä¼ ç»ŸåŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚æ‰‹ç»˜ã€å—é™æ•°æ®é›†ã€é™æ€ç¨‹åºå’Œé€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒStable Diffusionåœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢è¶…è¿‡äº†CLIPå’ŒDreamBooth + LORAã€‚ç‰¹åˆ«æ˜¯ï¼ŒStable Diffusionçš„FrÃ©chet Inception Distanceï¼ˆFIDï¼‰åˆ†æ•°æœ€ä½ï¼ˆå•ä¸­å¿ƒä¸º0.099ï¼Œå¤šä¸­å¿ƒä¸º0.064ï¼Œç»„åˆä¸º0.067ï¼‰ï¼Œè¡¨æ˜å›¾åƒè´¨é‡è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œå®ƒå…·æœ‰æœ€é«˜çš„å¹³å‡Inception Scoreï¼ˆæ‰€æœ‰æ•°æ®é›†ä¸Šä¸º2.327ï¼‰ï¼Œè¡¨æ˜å…¶å¤šæ ·æ€§å’Œè´¨é‡å‡ºè‰²ã€‚è¿™æ¨åŠ¨äº†äººå·¥æ™ºèƒ½é©±åŠ¨åŒ»å­¦è¯Šæ–­é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ç ”ç©¶å°†é›†ä¸­åœ¨æ¨¡å‹ç²¾ç‚¼ã€æ•°æ®é›†å¢å¼ºå’Œä¼¦ç†è€ƒè™‘ç­‰æ–¹é¢ï¼Œä»¥æœ‰æ•ˆåœ°å°†è¿™äº›è¿›æ­¥åº”ç”¨äºä¸´åºŠå®è·µã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MEDVQA-GIæŒ‘æˆ˜æ—¨åœ¨æ•´åˆAIé©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œä»¥æé«˜åŒ»å­¦è¯Šæ–­èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨é™æ€å›¾åƒåˆ†æï¼Œç¼ºä¹åŠ¨æ€ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé›†æˆå¾®è°ƒåçš„Stable Diffusionå’ŒDreamBoothæ¨¡å‹ä»¥åŠLow-Rank Adaptationï¼ˆLORAï¼‰æ¥ç”ŸæˆåŒ»å­¦å›¾åƒã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ä¼ ç»ŸåŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>Stable Diffusionåœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Future researchå°†é›†ä¸­åœ¨æ¨¡å‹ä¼˜åŒ–ã€æ•°æ®é›†å¢å¼ºå’Œä¼¦ç†è€ƒè™‘ç­‰æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54450c132b4fae46a9f4fc8da67578e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c5bfa2b0b05d6056dda3a142d835e33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50a2cce7d3e3a0aa3131dab8cd646d30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1be8fb3e391cba78170d864c204774.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Style-Content-Decomposition-based-Data-Augmentation-for-Domain-Generalizable-Medical-Image-Segmentation"><a href="#Style-Content-Decomposition-based-Data-Augmentation-for-Domain-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Style Content Decomposition-based Data Augmentation for Domain   Generalizable Medical Image Segmentation"></a>Style Content Decomposition-based Data Augmentation for Domain   Generalizable Medical Image Segmentation</h2><p><strong>Authors:Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane, Zhaolin Chen</strong></p>
<p>Due to the domain shifts between training and testing medical images, learned segmentation models often experience significant performance degradation during deployment. In this paper, we first decompose an image into its style code and content map and reveal that domain shifts in medical images involve: \textbf{style shifts} (\emph{i.e.}, differences in image appearance) and \textbf{content shifts} (\emph{i.e.}, variations in anatomical structures), the latter of which has been largely overlooked. To this end, we propose \textbf{StyCona}, a \textbf{sty}le \textbf{con}tent decomposition-based data \textbf{a}ugmentation method that innovatively augments both image style and content within the rank-one space, for domain generalizable medical image segmentation. StyCona is a simple yet effective plug-and-play module that substantially improves model generalization without requiring additional training parameters or modifications to the segmentation model architecture. Experiments on cross-sequence, cross-center, and cross-modality medical image segmentation settings with increasingly severe domain shifts, demonstrate the effectiveness of StyCona and its superiority over state-of-the-arts. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Senyh/StyCona">https://github.com/Senyh/StyCona</a>. </p>
<blockquote>
<p>ç”±äºè®­ç»ƒä¸æµ‹è¯•åŒ»å­¦å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œå·²å­¦ä¹ çš„åˆ†å‰²æ¨¡å‹åœ¨éƒ¨ç½²æ—¶é€šå¸¸ä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å›¾åƒåˆ†è§£ä¸ºé£æ ¼ä»£ç å’Œå†…å®¹æ˜ å°„ï¼Œå¹¶æ­ç¤ºåŒ»å­¦å›¾åƒçš„é¢†åŸŸå˜åŒ–æ¶‰åŠï¼šé£æ ¼å˜åŒ–ï¼ˆå³å›¾åƒå¤–è§‚çš„å·®å¼‚ï¼‰å’Œå†…å®¹å˜åŒ–ï¼ˆå³è§£å‰–ç»“æ„çš„å·®å¼‚ï¼‰ï¼Œåè€…åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†äº†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé£æ ¼å†…å®¹åˆ†è§£çš„æ•°æ®å¢å¼ºæ–¹æ³•StyConaï¼Œè¯¥æ–¹æ³•åœ¨ç§©ä¸€ç©ºé—´å†…åˆ›æ–°æ€§åœ°å¢å¼ºäº†å›¾åƒçš„é£æ ¼å’Œå†…å®¹ï¼Œç”¨äºé¢†åŸŸé€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚StyConaæ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒå‚æ•°æˆ–å¯¹åˆ†å‰²æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œå³å¯æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è·¨åºåˆ—ã€è·¨ä¸­å¿ƒå’Œè·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²è®¾ç½®ä¸‹çš„å®éªŒï¼Œæ˜¾ç¤ºäº†StyConaçš„æœ‰æ•ˆæ€§åŠå…¶è¶…è¶Šæœ€æ–°æŠ€æœ¯çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Senyh/StyCona">https://github.com/Senyh/StyCona</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨è®­ç»ƒå’Œæµ‹è¯•æ—¶é‡åˆ°é¢†åŸŸåç§»çš„é—®é¢˜ï¼Œæå‡ºäº†Style-Contentåˆ†è§£çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼ˆStyConaï¼‰ã€‚è¯¥æ–¹æ³•åœ¨rank-oneç©ºé—´å†…åŒæ—¶å¢å¼ºå›¾åƒé£æ ¼å’Œå†…å®¹ï¼Œä»¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒStyConaåœ¨è·¨åºåˆ—ã€è·¨ä¸­å¿ƒå’Œè·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²è®¾ç½®ä¸­ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹æ—¥ç›Šä¸¥é‡çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œå¹¶ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨éƒ¨ç½²æ—¶ï¼Œç”±äºè®­ç»ƒå’Œæµ‹è¯•å›¾åƒé¢†åŸŸçš„åç§»ï¼Œæ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚</li>
<li>é¢†åŸŸåç§»åŒ…æ‹¬é£æ ¼åç§»ï¼ˆå›¾åƒå¤–è§‚çš„å·®å¼‚ï¼‰å’Œå†…å®¹åç§»ï¼ˆè§£å‰–ç»“æ„çš„å˜åŒ–ï¼‰ï¼Œå…¶ä¸­å†…å®¹åç§»ä¸€ç›´è¢«å¿½è§†ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé£æ ¼å†…å®¹åˆ†è§£çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼ˆStyConaï¼‰ï¼Œåœ¨rank-oneç©ºé—´å†…åŒæ—¶å¢å¼ºå›¾åƒé£æ ¼å’Œå†…å®¹ã€‚</li>
<li>StyConaæ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å³æ’å³ç”¨æ¨¡å—ï¼Œèƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€å¢åŠ è®­ç»ƒå‚æ•°æˆ–å¯¹åˆ†å‰²æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒStyConaåœ¨è·¨åºåˆ—ã€è·¨ä¸­å¿ƒå’Œè·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½æœ‰æ•ˆåº”å¯¹é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>StyConaçš„æ–¹æ³•ç›¸è¾ƒäºå½“å‰å…ˆè¿›æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a228b5d0e034bb86da3a5b2d259e5748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f633bd5e8ce5753bacad84622e36c152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36fb7cb8ac6705bd83e5f945bcfbb629.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation"><a href="#AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation" class="headerlink" title="AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation"></a>AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation</h2><p><strong>Authors:Rui Li, Xiaowei Zhao</strong></p>
<p>As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer">https://github.com/lironui/AeroReformer</a>. </p>
<blockquote>
<p>ä½œä¸ºä¸€æ–°é¢–ä¸”å……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œå‚è€ƒåˆ†å‰²ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒåŸºäºæ–‡æœ¬æè¿°è¿›è¡Œå¯¹è±¡å®šä½å’Œåˆ†å‰²ã€‚è™½ç„¶å‚è€ƒå›¾åƒåˆ†å‰²ï¼ˆRISï¼‰åœ¨è‡ªç„¶å›¾åƒä¸­å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†å¯¹èˆªç©ºå›¾åƒï¼ˆå°¤å…¶æ˜¯æ¥è‡ªæ— äººæœºçš„å›¾åƒï¼‰çš„å…³æ³¨å¾ˆå°‘ã€‚æ— äººæœºå›¾åƒå…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œä¸åŒçš„å¯¹è±¡æ–¹å‘ï¼Œè¿™ä½¿å¾—ç°æœ‰çš„RISæ–¹æ³•æ— æ•ˆã€‚ä¸€ä¸ªå…³é”®çš„å±€é™æ€§åœ¨äºç¼ºä¹é’ˆå¯¹æ— äººæœºçš„ç‰¹å®šæ•°æ®é›†ï¼Œå› ä¸ºæ‰‹åŠ¨æ³¨é‡Šåƒç´ çº§æ©è†œå’Œç”Ÿæˆæ–‡æœ¬æè¿°æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ä¸”è€—æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨ç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ— äººæœºå‚è€ƒå›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰çš„ç©ºä¸­å‚è€ƒå˜å‹å™¨ï¼ˆAeroReformerï¼‰æ–°å‹æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹åœ¨äºå…·å¤‡è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰ï¼Œå¯æœ‰æ•ˆè¿›è¡Œè·¨æ¨¡æ€ç†è§£ï¼Œä»¥åŠå…·å¤‡æ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ï¼Œå¯æé«˜ç©ºä¸­åœºæ™¯çš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸¤ä¸ªæ–°å¼€å‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAeroReformerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºUAV-RISå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/lironui/AeroReformerå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16680v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶å…³æ³¨äº†ä¸€ç§æ–°å‹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡â€”â€”å¼•ç”¨åˆ†å‰²ï¼ˆReferring Segmentationï¼‰ï¼Œè¿™ä¸€ä»»åŠ¡ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ¥åŸºäºæ–‡æœ¬æè¿°å®šä½å¹¶åˆ†å‰²ç‰©ä½“ã€‚è™½ç„¶å¼•ç”¨å›¾åƒåˆ†å‰²åœ¨è‡ªç„¶å›¾åƒä¸Šå¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨èˆªç©ºå›¾åƒä¸Šï¼Œç‰¹åˆ«æ˜¯æ— äººæœºå›¾åƒä¸Šï¼Œç›¸å…³ç ”ç©¶å´å¾ˆå°‘ã€‚é’ˆå¯¹æ— äººæœºå›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œç‰©ä½“æ–¹å‘å˜åŒ–ç­‰ï¼Œæœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œåˆ©ç”¨ç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§æ–°çš„æ— äººæœºå¼•ç”¨å›¾åƒåˆ†å‰²æ¡†æ¶â€”â€”Aerial Referring Transformerï¼ˆAeroReformerï¼‰ï¼Œå…¶ä¸­åŒ…å«äº†ç”¨äºæœ‰æ•ˆè·¨æ¨¡æ€ç†è§£çš„Vision-Language Cross-Attention Moduleï¼ˆVLCAMï¼‰ä»¥åŠç”¨äºæé«˜èˆªç©ºåœºæ™¯åˆ†å‰²å‡†ç¡®æ€§çš„Rotation-Aware Multi-Scale Fusionï¼ˆRAMSFï¼‰è§£ç å™¨ã€‚åœ¨ä¸¤é¡¹æ–°å¼€å‘çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAeroReformeråœ¨æ— äººæœºå¼•ç”¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè¿™ä¸€é¢†åŸŸå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•ç”¨åˆ†å‰²ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒåŸºäºæ–‡æœ¬æè¿°è¿›è¡Œç‰©ä½“å®šä½å’Œåˆ†å‰²ã€‚</li>
<li>è™½ç„¶è‡ªç„¶å›¾åƒçš„å¼•ç”¨å›¾åƒåˆ†å‰²å·²å—å¹¿æ³›å…³æ³¨ï¼Œä½†æ— äººæœºå›¾åƒçš„å¼•ç”¨å›¾åƒåˆ†å‰²ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚</li>
<li>æ— äººæœºå›¾åƒå­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œç‰©ä½“æ–¹å‘å˜åŒ–ç­‰ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹æ— äººæœºçš„ç‰¹å®šæ•°æ®é›†æ˜¯è¿™ä¸€é¢†åŸŸçš„ä¸€ä¸ªå…³é”®é™åˆ¶ã€‚</li>
<li>ç ”ç©¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œç»“åˆç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶Aerial Referring Transformerï¼ˆAeroReformerï¼‰æ¥å¤„ç†æ— äººæœºå¼•ç”¨å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>AeroReformeråŒ…å«äº†ç”¨äºè·¨æ¨¡æ€ç†è§£çš„VLCAMå’Œç”¨äºæé«˜åˆ†å‰²å‡†ç¡®æ€§çš„RAMSFè§£ç å™¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3af0fd5647598b30f82615da85c7b669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b7d314935fb91f7fe70e77e43b75f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67069d33f329753b9e872c1c21852e95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1647f55d62f3f04f918b88a51f5b1d47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ede21416761cb84b00dec2f94df09139.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3f7d09675111a9665bb7cb179655c66.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Diagnosing-COVID-19-Severity-from-Chest-X-Ray-Images-Using-ViT-and-CNN-Architectures"><a href="#Diagnosing-COVID-19-Severity-from-Chest-X-Ray-Images-Using-ViT-and-CNN-Architectures" class="headerlink" title="Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN   Architectures"></a>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN   Architectures</h2><p><strong>Authors:Luis Lara, Lucia Eve Berger, Rajesh Raju</strong></p>
<p>The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patientâ€™s condition from CXRs. In this study, we produce a large COVID severity dataset by merging three sources and investigate the efficacy of transfer learning using ImageNet- and CXR-pretrained models and vision transformers (ViTs) in both severity regression and classification tasks. A pretrained DenseNet161 model performed the best on the three class severity prediction problem, reaching 80% accuracy overall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases, respectively. The ViT had the best regression results, with a mean absolute error of 0.5676 compared to radiologist-predicted severity scores. The projectâ€™s source code is publicly available. </p>
<blockquote>
<p>COVID-19å¤§æµè¡Œä½¿åŒ»ç–—èµ„æºæ‰¿å—å·¨å¤§å‹åŠ›ï¼Œå¹¶å¼•å‘äº†å…³äºæœºå™¨å­¦ä¹ å¦‚ä½•å‡è½»åŒ»ç”Ÿè´Ÿæ‹…å¹¶ä¸ºè¯Šæ–­åšå‡ºè´¡çŒ®çš„è®¨è®ºã€‚èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRsï¼‰è¢«ç”¨äºè¯Šæ–­COVID-19ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶ä»CXRsé¢„æµ‹æ‚£è€…ç—…æƒ…çš„ä¸¥é‡ç¨‹åº¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆå¹¶ä¸‰ä¸ªæ¥æºåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹çš„COVIDä¸¥é‡ç¨‹åº¦æ•°æ®é›†ï¼Œå¹¶ç ”ç©¶äº†ä½¿ç”¨ImageNetå’ŒCXRé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰åœ¨ä¸¥é‡æ€§å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡Œè¿ç§»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚åœ¨ä¸‰çº§ä¸¥é‡æ€§é¢„æµ‹é—®é¢˜ä¸Šï¼Œé¢„è®­ç»ƒçš„DenseNet14æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°ç™¾åˆ†ä¹‹å…«åï¼Œåœ¨è½»åº¦ã€ä¸­åº¦å’Œé‡åº¦ç—…ä¾‹ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸ºç™¾åˆ†ä¹‹ä¸ƒåä¸ƒç‚¹ä¸‰ã€ç™¾åˆ†ä¹‹å…«åä¸‰ç‚¹ä¹å’Œç™¾åˆ†ä¹‹ä¸ƒåã€‚ViTåœ¨å›å½’æ–¹é¢çš„è¡¨ç°æœ€ä½³ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿé¢„æµ‹çš„ä¸¥é‡ç¨‹åº¦è¯„åˆ†ç›¸æ¯”ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º0.5676ã€‚è¯¥é¡¹ç›®çš„æºä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16622v3">PDF</a> Upon reflection, the final version of this work does not meet the   authorâ€™s personal standards for thoroughness and clarity. As a result, the   authors have chosen to withdraw the paper to prevent the dissemination of   work that may not fully reflect the level of quality they strive to maintain</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†COVID-19ç–«æƒ…ä¸­æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨è¯Šæ–­åŠç—…æƒ…ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡åˆå¹¶ä¸‰ä¸ªæ•°æ®æºåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹COVIDä¸¥é‡ç¨‹åº¦æ•°æ®é›†ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨ImageNetå’ŒCXRé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¿›è¡Œè¿ç§»å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ç—…æƒ…ä¸¥é‡ç¨‹åº¦å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚å…¶ä¸­ï¼Œé¢„è®­ç»ƒçš„DenseNet161æ¨¡å‹åœ¨ä¸‰ç±»ä¸¥é‡ç¨‹åº¦é¢„æµ‹é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º80%ï¼Œåœ¨è½»åº¦ã€ä¸­åº¦å’Œé‡åº¦ç—…ä¾‹ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º77.3%ã€83.9%å’Œ70%ã€‚ViTåœ¨å›å½’é¢„æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿé¢„æµ‹çš„ä¸¥é‡ç¨‹åº¦åˆ†æ•°çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º0.5676ã€‚è¯¥é¡¹ç›®çš„æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COVID-19ç–«æƒ…åŠ å‰§äº†åŒ»ç–—èµ„æºç´§å¼ ï¼Œä¿ƒä½¿ç ”ç©¶å¦‚ä½•åˆ©ç”¨æœºå™¨å­¦ä¹ è¾…åŠ©åŒ»ç”Ÿè¯Šæ–­åŠé¢„æµ‹ç—…æƒ…ä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åˆå¹¶ä¸‰ä¸ªæ•°æ®æºåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹COVIDä¸¥é‡ç¨‹åº¦æ•°æ®é›†ã€‚</li>
<li>è¿ç§»å­¦ä¹ ä½¿ç”¨ImageNetå’ŒCXRé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸­è¿›è¡Œäº†æ¢ç´¢ã€‚</li>
<li>é¢„è®­ç»ƒçš„DenseNet161æ¨¡å‹åœ¨ä¸‰ç±»ä¸¥é‡ç¨‹åº¦é¢„æµ‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º80%ã€‚</li>
<li>ViTåœ¨å›å½’é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿé¢„æµ‹çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º0.5676ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„æ¨¡å‹åœ¨é¢„æµ‹COVID-19ç—…æƒ…ä¸¥é‡ç¨‹åº¦æ–¹é¢å…·æœ‰è¾ƒé«˜çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4084c5588aeaba05e941b2733c359fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-864285972ea183e55ca4b5eb4ff1bfe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee189a4eba4b697ea4e614bce8bc44b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f409dde1cb252694bda231d8c806d4ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67dcf86b3ee347124aa6a19496966b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f883ca57a32061673d1225346c59395.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Kinetic-Diffusion-Rotation-Algorithm-for-Dose-Estimation-in-Electron-Beam-Therapy"><a href="#Kinetic-Diffusion-Rotation-Algorithm-for-Dose-Estimation-in-Electron-Beam-Therapy" class="headerlink" title="Kinetic-Diffusion-Rotation Algorithm for Dose Estimation in Electron   Beam Therapy"></a>Kinetic-Diffusion-Rotation Algorithm for Dose Estimation in Electron   Beam Therapy</h2><p><strong>Authors:Klaas Willems, Vince Maes, Zhirui Tang, Giovanni Samaey</strong></p>
<p>Monte Carlo methods are state-of-the-art when it comes to dosimetric computations in radiotherapy. However, the execution time of these methods suffers in high-collisional regimes. We address this problem by introducing a kinetic-diffusion particle tracing scheme. This algorithm, first proposed in the context of neutral transport in fusion energy, relies on explicit simulation of the kinetic motion in low-collisional regimes and dynamically switches to motion based on a random walk in high-collisional regimes. The random walk motion maintains the first two moments (mean and variance) of the kinetic motion. We derive an analytic formula for the mean kinetic motion and discuss the addition of a multiple scattering distribution to the algorithm. In contrast to neutral transport, the radiation transfer setting does not readily admit to an analytical expression for the variance of the kinetic motion, and we therefore resort to the use of a lookup table. We test the algorithm for dosimetric computations in radiation therapy on a 2D CT scan of a lung patient. Using a simple particle model, our Python implementation of the algorithm is nearly 33 times faster than an equivalent kinetic simulation at the cost of a small modeling error. </p>
<blockquote>
<p>è’™ç‰¹å¡æ´›æ–¹æ³•åœ¨æ”¾å°„æ²»ç–—å‰‚é‡è®¡ç®—æ–¹é¢å±äºæœ€å…ˆè¿›æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨é«˜ç¢°æ’çŠ¶æ€ä¸‹å­˜åœ¨æ‰§è¡Œæ—¶é—´è¾ƒé•¿çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥åŠ¨åŠ›å­¦æ‰©æ•£ç²’å­è¿½è¸ªæ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥ç®—æ³•æœ€åˆåœ¨æ ¸èšå˜èƒ½é‡ä¸­çš„ä¸­æ€§ç‰©è´¨ä¼ è¾“ä¸Šä¸‹æ–‡ä¸­æå‡ºï¼Œä¾èµ–äºåœ¨ä½ç¢°æ’çŠ¶æ€ä¸‹å¯¹åŠ¨åŠ›å­¦è¿åŠ¨çš„æ˜¾å¼æ¨¡æ‹Ÿï¼Œå¹¶åœ¨é«˜ç¢°æ’çŠ¶æ€ä¸‹åŠ¨æ€åˆ‡æ¢åˆ°åŸºäºéšæœºæ¸¸èµ°çš„è¿åŠ¨ã€‚éšæœºæ¸¸èµ°è¿åŠ¨ä¿æŒäº†åŠ¨åŠ›å­¦è¿åŠ¨çš„å‰ä¸¤ä¸ªæ—¶åˆ»ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰ã€‚æˆ‘ä»¬ä¸ºå¹³å‡åŠ¨åŠ›å­¦è¿åŠ¨æ¨å¯¼äº†ä¸€ä¸ªåˆ†æå…¬å¼ï¼Œå¹¶è®¨è®ºäº†å‘ç®—æ³•ä¸­æ·»åŠ å¤šé‡æ•£å°„åˆ†å¸ƒçš„é—®é¢˜ã€‚ä¸ä¸­æ€§ç‰©è´¨ä¼ è¾“ä¸åŒï¼Œè¾å°„ä¼ è¾“è®¾ç½®ä¸ä¾¿äºæ¥å—åŠ¨åŠ›å­¦è¿åŠ¨çš„æ–¹å·®çš„åˆ†æè¡¨è¾¾å¼ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æŸ¥æ‰¾è¡¨ã€‚æˆ‘ä»¬åœ¨äºŒç»´è‚ºéƒ¨æ‚£è€…CTæ‰«æä¸Šå¯¹ç®—æ³•è¿›è¡Œæ”¾å°„æ²»ç–—å‰‚é‡è®¡ç®—æµ‹è¯•ã€‚ä½¿ç”¨ç®€å•çš„ç²’å­æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç®—æ³•Pythonå®ç°æ¯”ç­‰æ•ˆåŠ¨åŠ›å­¦æ¨¡æ‹Ÿé€Ÿåº¦å¿«è¿‘33å€ï¼Œä½†å­˜åœ¨è¾ƒå°çš„å»ºæ¨¡è¯¯å·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05063v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ”¾ç–—ä¸­çš„å‰‚é‡è®¡ç®—ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºåŠ¨åŠ›å­¦æ‰©æ•£ç²’å­è¿½è¸ªæ–¹æ¡ˆæ¥è§£å†³Monte Carloæ–¹æ³•åœ¨é«˜ç¢°æ’çŠ¶æ€ä¸‹çš„è¿è¡Œæ—¶é—´è¿‡é•¿çš„é—®é¢˜ã€‚è¯¥ç®—æ³•åœ¨ä¸­ä½ç¢°æ’çŠ¶æ€ä¸‹æ¨¡æ‹Ÿç²’å­åŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶åœ¨é«˜ç¢°æ’çŠ¶æ€ä¸‹è½¬æ¢ä¸ºéšæœºè¿åŠ¨æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•é€šè¿‡è§£æå…¬å¼è®¡ç®—å¹³å‡åŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶è®¨è®ºäº†å¤šé‡æ•£å°„åˆ†å¸ƒç®—æ³•çš„æ”¹è¿›ã€‚é€šè¿‡è‚ºéƒ¨æ‚£è€…äºŒç»´CTæ‰«ææµ‹è¯•ï¼Œè¯¥ç®—æ³•è¾ƒç­‰ä»·åŠ¨åŠ›å­¦æ¨¡æ‹Ÿé€Ÿåº¦æå‡è¿‘33å€ï¼Œä½†æœ‰å¾®å°å»ºæ¨¡è¯¯å·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Monte Carloæ–¹æ³•åœ¨æ”¾å°„æ²»ç–—ä¸­å‰‚é‡è®¡ç®—æ–¹é¢å¤„äºå‰æ²¿åœ°ä½ï¼Œä½†åœ¨é«˜ç¢°æ’çŠ¶æ€ä¸‹æ‰§è¡Œæ—¶é—´è¾ƒé•¿ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºåŠ¨åŠ›å­¦æ‰©æ•£ç²’å­è¿½è¸ªæ–¹æ¡ˆæ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨ä½ç¢°æ’çŠ¶æ€ä¸‹æ¨¡æ‹Ÿç²’å­çš„åŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶åœ¨é«˜ç¢°æ’çŠ¶æ€ä¸‹é‡‡ç”¨éšæœºè¿åŠ¨æ¨¡å¼ã€‚</li>
<li>ç®—æ³•é‡‡ç”¨è§£æå…¬å¼è®¡ç®—å¹³å‡åŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶è€ƒè™‘äº†å¤šé‡æ•£å°„åˆ†å¸ƒã€‚</li>
<li>è¾å°„ä¼ è¾“åœºæ™¯ä¸‹çš„åŠ¨åŠ›å­¦è¿åŠ¨æ–¹å·®æ²¡æœ‰ç°æˆçš„è§£æè¡¨è¾¾å¼ï¼Œå› æ­¤é‡‡ç”¨æŸ¥æ‰¾è¡¨è¿›è¡Œè®¡ç®—ã€‚</li>
<li>åœ¨è‚ºç™Œæ‚£è€…äºŒç»´CTæ‰«æä¸Šæµ‹è¯•ç®—æ³•ï¼Œå‘ç°è¾ƒç­‰ä»·åŠ¨åŠ›å­¦æ¨¡æ‹Ÿé€Ÿåº¦æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53419d6a9555c30b49d24eb1f88cd75c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e52ba49507459c17776c937f58064b2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Hybrid-deep-learning-based-strategy-for-the-hepatocellular-carcinoma-cancer-grade-classification-of-H-E-stained-liver-histopathology-images"><a href="#Hybrid-deep-learning-based-strategy-for-the-hepatocellular-carcinoma-cancer-grade-classification-of-H-E-stained-liver-histopathology-images" class="headerlink" title="Hybrid deep learning-based strategy for the hepatocellular carcinoma   cancer grade classification of H&amp;E stained liver histopathology images"></a>Hybrid deep learning-based strategy for the hepatocellular carcinoma   cancer grade classification of H&amp;E stained liver histopathology images</h2><p><strong>Authors:Ajinkya Deshpande, Deep Gupta, Ankit Bhurane, Nisha Meshram, Sneha Singh, Petia Radeva</strong></p>
<p>Hepatocellular carcinoma (HCC) is a common type of liver cancer whose early-stage diagnosis is a common challenge, mainly due to the manual assessment of hematoxylin and eosin-stained whole slide images, which is a time-consuming process and may lead to variability in decision-making. For accurate detection of HCC, we propose a hybrid deep learning-based architecture that uses transfer learning to extract the features from pre-trained convolutional neural network (CNN) models and a classifier made up of a sequence of fully connected layers. This study uses a publicly available The Cancer Genome Atlas Hepatocellular Carcinoma (TCGA-LIHC)database (n&#x3D;491) for model development and database of Kasturba Gandhi Medical College (KMC), India for validation. The pre-processing step involves patch extraction, colour normalization, and augmentation that results in 3920 patches for the TCGA dataset. The developed hybrid deep neural network consisting of a CNN-based pre-trained feature extractor and a customized artificial neural network-based classifier is trained using five-fold cross-validation. For this study, eight different state-of-the-art models are trained and tested as feature extractors for the proposed hybrid model. The proposed hybrid model with ResNet50-based feature extractor provided the sensitivity, specificity, F1-score, accuracy, and AUC of 100.00%, 100.00%, 100.00%, 100.00%, and 1.00, respectively on the TCGA database. On the KMC database, EfficientNetb3 resulted in the optimal choice of the feature extractor giving sensitivity, specificity, F1-score, accuracy, and AUC of 96.97, 98.85, 96.71, 96.71, and 0.99, respectively. The proposed hybrid models showed improvement in accuracy of 2% and 4% over the pre-trained models in TCGA-LIHC and KMC databases. </p>
<blockquote>
<p>è‚ç»†èƒç™Œï¼ˆHCCï¼‰æ˜¯ä¸€ç§å¸¸è§çš„è‚ç™Œç±»å‹ï¼Œæ—©æœŸé˜¶æ®µçš„è¯Šæ–­æ˜¯ä¸€ä¸ªå¸¸è§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºéœ€è¦å¯¹è‹æœ¨ç²¾å’Œä¼Šçº¢æŸ“è‰²çš„å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œæ‰‹åŠ¨è¯„ä¼°ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆå¯èƒ½å¯¼è‡´å†³ç­–ä¸Šçš„å·®å¼‚ã€‚ä¸ºäº†å‡†ç¡®æ£€æµ‹è‚ç»†èƒç™Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨è¿ç§»å­¦ä¹ ä»é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ä¸­æå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ç”±å…¨è¿æ¥å±‚ç»„æˆçš„åˆ†ç±»å™¨ã€‚æœ¬ç ”ç©¶ä½¿ç”¨å…¬å¼€å¯ç”¨çš„ç™Œç—‡åŸºå› ç»„å›¾è°±è‚ç»†èƒç™Œï¼ˆTCGA-LIHCï¼‰æ•°æ®åº“ï¼ˆn&#x3D;491ï¼‰è¿›è¡Œæ¨¡å‹å¼€å‘ï¼Œå¹¶ä½¿ç”¨å°åº¦å¡æ–¯ç‰¹é²å·´ç”˜åœ°åŒ»å­¦é™¢ï¼ˆKMCï¼‰æ•°æ®åº“è¿›è¡ŒéªŒè¯ã€‚é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬è¡¥ä¸æå–ã€é¢œè‰²å½’ä¸€åŒ–å’Œæ•°æ®å¢å¼ºï¼Œç»“æœäº§ç”Ÿé’ˆå¯¹TCGAæ•°æ®é›†çš„3920ä¸ªè¡¥ä¸ã€‚å¼€å‘çš„æ··åˆæ·±åº¦ç¥ç»ç½‘ç»œç”±åŸºäºCNNçš„é¢„è®­ç»ƒç‰¹å¾æå–å™¨å’ŒåŸºäºå®šåˆ¶äººå·¥ç¥ç»ç½‘ç»œçš„åˆ†ç±»å™¨ç»„æˆï¼Œé‡‡ç”¨äº”æŠ˜äº¤å‰éªŒè¯è¿›è¡Œè®­ç»ƒã€‚æœ¬ç ”ç©¶å¯¹å…«ç§æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒå’Œæµ‹è¯•ï¼Œä½œä¸ºæ‰€æå‡ºæ··åˆæ¨¡å‹çš„ç‰¹å¾æå–å™¨ã€‚é‡‡ç”¨åŸºäºResNet50çš„ç‰¹å¾æå–å™¨çš„æ··åˆæ¨¡å‹åœ¨TCGAæ•°æ®åº“ä¸Šæä¾›äº†æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§ã€F1åˆ†æ•°ã€å‡†ç¡®åº¦å’ŒAUCåˆ†åˆ«ä¸º100.00%ã€100.00%ã€100.00%ã€100.00%å’Œ1.00ã€‚åœ¨KMCæ•°æ®åº“ä¸­ï¼ŒEfficientNetb3ä½œä¸ºç‰¹å¾æå–å™¨çš„æœ€ä½³é€‰æ‹©ï¼Œå…¶æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§ã€F1åˆ†æ•°ã€å‡†ç¡®åº¦å’ŒAUCåˆ†åˆ«ä¸º96.97%ã€98.85%ã€96.71%ã€96.71%å’Œ0.99ã€‚ä¸TCGA-LIHCå’ŒKMCæ•°æ®åº“ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºæ··åˆæ¨¡å‹çš„å‡†ç¡®åº¦æé«˜äº†2%å’Œ4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03084v2">PDF</a> 14 figure, 9 tables</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹è‚ç»†èƒç™Œæ—©æœŸè¯Šæ–­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œåˆ©ç”¨è¿ç§»å­¦ä¹ ä»é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡å…¨è¿æ¥å±‚ç»„æˆçš„åˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶ä½¿ç”¨å…¬å¼€å¯ç”¨çš„ç™Œç—‡åŸºå› ç»„å›¾è°±è‚ç»†èƒç™Œæ•°æ®åº“è¿›è¡Œæ¨¡å‹å¼€å‘ï¼Œå¹¶ä½¿ç”¨å°åº¦å¡æ–¯ç‰¹å°”å·´ç”˜åœ°åŒ»ç–—å­¦é™¢æ•°æ®åº“è¿›è¡ŒéªŒè¯ã€‚ç»è¿‡é¢„å¤„ç†åï¼Œåˆ©ç”¨å¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æå–å™¨ä¸å®šåˆ¶çš„äººå·¥ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ç»„æˆçš„æ··åˆæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚å…¶ä¸­åŸºäºResNet50çš„ç‰¹å¾æå–å™¨åœ¨ç™Œç—‡åŸºå› ç»„å›¾è°±æ•°æ®åº“ä¸Šçš„è¡¨ç°æœ€ä½³ï¼Œè€ŒEfficientNetb3åœ¨å¡æ–¯ç‰¹å°”å·´ç”˜åœ°åŒ»ç–—å­¦é™¢æ•°æ®åº“ä¸Šè¡¨ç°æœ€ä¼˜ã€‚æ··åˆæ¨¡å‹ç›¸è¾ƒäºé¢„è®­ç»ƒæ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šæœ‰æ‰€æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‚ç»†èƒç™Œçš„æ—©æœŸè¯Šæ–­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦å› ä¸ºæ‰‹åŠ¨è¯„ä¼°æŸ“è‰²å…¨ç‰‡å›¾åƒçš„æ—¶é—´æ¶ˆè€—å’Œå†³ç­–å˜é‡æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç»“åˆè¿ç§»å­¦ä¹ ã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå®šåˆ¶äººå·¥ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ã€‚</li>
<li>ä½¿ç”¨ç™Œç—‡åŸºå› ç»„å›¾è°±è‚ç»†èƒç™Œæ•°æ®åº“è¿›è¡Œæ¨¡å‹å¼€å‘ï¼Œå°åº¦å¡æ–¯ç‰¹å°”å·´ç”˜åœ°åŒ»ç–—å­¦é™¢æ•°æ®åº“ç”¨äºéªŒè¯ã€‚</li>
<li>é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬è¡¥ä¸æå–ã€é¢œè‰²å½’ä¸€åŒ–å’Œå¢å¼ºã€‚</li>
<li>åŸºäºResNet50çš„ç‰¹å¾æå–å™¨åœ¨TCGAæ•°æ®åº“ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒEfficientNetb3åœ¨KMCæ•°æ®åº“ä¸Šè¡¨ç°æœ€ä¼˜ã€‚</li>
<li>æ··åˆæ¨¡å‹ç›¸è¾ƒäºé¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚</li>
<li>æ··åˆæ¨¡å‹åœ¨è¯Šæ–­æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§ã€F1åˆ†æ•°ã€å‡†ç¡®ç‡å’ŒAUCç­‰æ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9b1d056411fa9b2c4432512f957eb6c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbc470520f3abb5900d7d6e96e68c969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84725744000767db47f3dd86025fc54d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4aa6c97fce1fc7df601ec8730f6045b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a7e94f0add4ae9cceea5162ade8afd4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-225b59eae1bfbcf6671330df28741c52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fedea3b007a52732db2c1c3fa0ab5799.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1086c7c0ada318e21639c433f83ec765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa6b9f2ac53f8d08396381d0a3417a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93676b757d4d51f90e2b3a7647b445ca.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Progressive-Curriculum-Learning-with-Scale-Enhanced-U-Net-for-Continuous-Airway-Segmentation"><a href="#Progressive-Curriculum-Learning-with-Scale-Enhanced-U-Net-for-Continuous-Airway-Segmentation" class="headerlink" title="Progressive Curriculum Learning with Scale-Enhanced U-Net for Continuous   Airway Segmentation"></a>Progressive Curriculum Learning with Scale-Enhanced U-Net for Continuous   Airway Segmentation</h2><p><strong>Authors:Bingyu Yang, Qingyao Tian, Huai Liao, Xinyan Huang, Jinlin Wu, Jingdi Hu, Hongbin Liu</strong></p>
<p>Continuous and accurate segmentation of airways in chest CT images is essential for preoperative planning and real-time bronchoscopy navigation. Despite advances in deep learning for medical image segmentation, maintaining airway continuity remains a challenge, particularly due to intra-class imbalance between large and small branches and blurred CT scan details. To address these challenges, we propose a progressive curriculum learning pipeline and a Scale-Enhanced U-Net (SE-UNet) to enhance segmentation continuity. Specifically, our progressive curriculum learning pipeline consists of three stages: extracting main airways, identifying small airways, and repairing discontinuities. The cropping sampling strategy in each stage reduces feature interference between airways of different scales, effectively addressing the challenge of intra-class imbalance. In the third training stage, we present an Adaptive Topology-Responsive Loss (ATRL) to guide the network to focus on airway continuity. The progressive training pipeline shares the same SE-UNet, integrating multi-scale inputs and Detail Information Enhancers (DIEs) to enhance information flow and effectively capture the intricate details of small airways. Additionally, we propose a robust airway tree parsing method and hierarchical evaluation metrics to provide more clinically relevant and precise analysis. Experiments on both in-house and public datasets demonstrate that our method outperforms existing approaches, significantly improving the accuracy of small airways and the completeness of the airway tree. The code will be released upon publication. </p>
<blockquote>
<p>åœ¨èƒ¸éƒ¨CTå›¾åƒä¸­ï¼Œæ°”é“çš„è¿ç»­å’Œå‡†ç¡®åˆ†å‰²å¯¹äºæœ¯å‰è§„åˆ’å’Œå®æ—¶æ”¯æ°”ç®¡é•œæ£€æŸ¥å¯¼èˆªè‡³å…³é‡è¦ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä¿æŒæ°”é“çš„è¿ç»­æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºå¤§åˆ†æ”¯å’Œå°åˆ†æ”¯ä¹‹é—´çš„ç±»åˆ«å†…ä¸å¹³è¡¡ä»¥åŠCTæ‰«æç»†èŠ‚æ¨¡ç³Šã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ æµç¨‹å’Œå°ºåº¦å¢å¼ºU-Netï¼ˆSE-UNetï¼‰æ¥æé«˜åˆ†å‰²è¿ç»­æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ æµç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šæå–ä¸»æ°”é“ã€è¯†åˆ«å°æ°”é“å’Œä¿®å¤ä¸è¿ç»­å¤„ã€‚æ¯ä¸ªé˜¶æ®µçš„è£å‰ªé‡‡æ ·ç­–ç•¥å‡å°‘äº†ä¸åŒè§„æ¨¡æ°”é“ä¹‹é—´çš„ç‰¹å¾å¹²æ‰°ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç±»å†…ä¸å¹³è¡¡çš„æŒ‘æˆ˜ã€‚åœ¨ç¬¬ä¸‰ä¸ªè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ‹“æ‰‘å“åº”æŸå¤±ï¼ˆATRLï¼‰æ¥æŒ‡å¯¼ç½‘ç»œå…³æ³¨æ°”é“è¿ç»­æ€§ã€‚æ¸è¿›å¼è®­ç»ƒæµç¨‹ä½¿ç”¨ç›¸åŒçš„SE-UNetï¼Œå®ƒç»“åˆäº†å¤šå°ºåº¦è¾“å…¥å’Œç»†èŠ‚ä¿¡æ¯å¢å¼ºå™¨ï¼ˆDIEsï¼‰ï¼Œä»¥åŠ å¼ºä¿¡æ¯æµå¹¶æœ‰æ•ˆåœ°æ•æ‰å°æ°”é“çš„å¤æ‚ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ°”é“æ ‘è§£ææ–¹æ³•å’Œåˆ†å±‚è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æä¾›æ›´ä¸ä¸´åºŠç›¸å…³å’Œç²¾ç¡®çš„åˆ†æã€‚åœ¨å†…éƒ¨å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜å°æ°”é“å‡†ç¡®æ€§å’Œæ°”é“æ ‘çš„å®Œæ•´æ€§ã€‚ä»£ç å°†åœ¨å‘å¸ƒæ—¶å…¬å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18456v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ å’ŒScale-Enhanced U-Netï¼ˆSE-UNetï¼‰çš„ç®¡é“ï¼Œç”¨äºæé«˜èƒ¸éƒ¨CTå›¾åƒä¸­æ°”é“åˆ†å‰²çš„è¿ç»­æ€§ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆæå–ä¸»è¦æ°”é“ã€è¯†åˆ«å°æ°”é“å¹¶ä¿®å¤æ–­è£‚å¤„ã€‚åˆ©ç”¨è£å‰ªé‡‡æ ·ç­–ç•¥å’Œå¤šå°ºåº¦è¾“å…¥ï¼ŒSE-UNetè§£å†³äº†ç±»å†…ä¸å¹³è¡¡å’Œæ¨¡ç³Šç»†èŠ‚çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªé€‚åº”æ‹“æ‰‘å“åº”æŸå¤±ï¼ˆATRLï¼‰ä»¥åŠæ°”é“æ ‘è§£ææ–¹æ³•å’Œå±‚æ¬¡è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æé«˜åˆ†æçš„ç²¾ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜å°æ°”é“å‡†ç¡®æ€§å’Œæ°”é“æ ‘çš„å®Œæ•´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°”é“åœ¨èƒ¸éƒ¨CTå›¾åƒä¸­çš„è¿ç»­å‡†ç¡®åˆ†å‰²å¯¹æœ¯å‰è§„åˆ’å’Œå®æ—¶æ”¯æ°”ç®¡é•œæ£€æŸ¥å¯¼èˆªè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´æ°”é“è¿ç»­æ€§ç»´æŒã€ç±»å†…ä¸å¹³è¡¡å’Œæ¨¡ç³Šç»†èŠ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºæ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ çš„ç®¡é“ï¼ŒåŒ…æ‹¬æå–ä¸»æ°”é“ã€è¯†åˆ«å°æ°”é“å’Œä¿®å¤æ–­è£‚ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>é‡‡ç”¨è£å‰ªé‡‡æ ·ç­–ç•¥æ¥å‡å°‘ä¸åŒè§„æ¨¡æ°”é“ä¹‹é—´çš„ç‰¹å¾å¹²æ‰°ï¼Œè§£å†³ç±»å†…ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>åœ¨ç¬¬ä¸‰é˜¶æ®µå¼•å…¥è‡ªé€‚åº”æ‹“æ‰‘å“åº”æŸå¤±ï¼ˆATRLï¼‰ï¼Œå¼•å¯¼ç½‘ç»œå…³æ³¨æ°”é“è¿ç»­æ€§ã€‚</li>
<li>åˆ©ç”¨Scale-Enhanced U-Netï¼ˆSE-UNetï¼‰è¿›è¡Œå¤šå°ºåº¦è¾“å…¥å’Œä¿¡æ¯å¢å¼ºï¼Œæœ‰æ•ˆæ•æ‰å°æ°”é“ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b77fe8af8e3589652eeebedea672dcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21d69f67342781a3a8f2a2e785372aab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a2110c14799cad0b3adec62be8af646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c3e64ba1deadf37d507c38c21e36519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3ef4e571a4fe74003dc5604abdccc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-744e38a0b3b0f14f01ea27f0f680652f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="High-Precision-Dichotomous-Image-Segmentation-via-Probing-Diffusion-Capacity"><a href="#High-Precision-Dichotomous-Image-Segmentation-via-Probing-Diffusion-Capacity" class="headerlink" title="High-Precision Dichotomous Image Segmentation via Probing Diffusion   Capacity"></a>High-Precision Dichotomous Image Segmentation via Probing Diffusion   Capacity</h2><p><strong>Authors:Qian Yu, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu</strong></p>
<p>In the realm of high-resolution (HR), fine-grained image segmentation, the primary challenge is balancing broad contextual awareness with the precision required for detailed object delineation, capturing intricate details and the finest edges of objects. Diffusion models, trained on vast datasets comprising billions of image-text pairs, such as SD V2.1, have revolutionized text-to-image synthesis by delivering exceptional quality, fine detail resolution, and strong contextual awareness, making them an attractive solution for high-resolution image segmentation. To this end, we propose DiffDIS, a diffusion-driven segmentation model that taps into the potential of the pre-trained U-Net within diffusion models, specifically designed for high-resolution, fine-grained object segmentation. By leveraging the robust generalization capabilities and rich, versatile image representation prior of the SD models, coupled with a task-specific stable one-step denoising approach, we significantly reduce the inference time while preserving high-fidelity, detailed generation. Additionally, we introduce an auxiliary edge generation task to not only enhance the preservation of fine details of the object boundaries, but reconcile the probabilistic nature of diffusion with the deterministic demands of segmentation. With these refined strategies in place, DiffDIS serves as a rapid object mask generation model, specifically optimized for generating detailed binary maps at high resolutions, while demonstrating impressive accuracy and swift processing. Experiments on the DIS5K dataset demonstrate the superiority of DiffDIS, achieving state-of-the-art results through a streamlined inference process. The source code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/qianyu-dlut/DiffDIS">https://github.com/qianyu-dlut/DiffDIS</a>. </p>
<blockquote>
<p>åœ¨é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰ç²¾ç»†ç²’åº¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå¹³è¡¡å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ„è¯†å’Œè¿›è¡Œè¯¦å°½ç‰©ä½“è½®å»“æç»˜æ‰€éœ€çš„ç²¾åº¦ï¼ŒåŒæ—¶æ•æ‰å¤æ‚ç»†èŠ‚å’Œç‰©ä½“çš„æœ€ç»†å¾®è¾¹ç¼˜ã€‚æ‰©æ•£æ¨¡å‹ç»è¿‡åœ¨åŒ…å«æ•°åäº¿å›¾åƒæ–‡æœ¬å¯¹çš„åºå¤§æ•°æ®é›†ï¼ˆå¦‚SD V2.1ï¼‰ä¸Šçš„è®­ç»ƒï¼Œä»¥å…¶å“è¶Šçš„è´¨é‡ã€ç²¾ç»†çš„ç»†èŠ‚åˆ†è¾¨ç‡å’Œå¼ºå¤§çš„ä¸Šä¸‹æ–‡æ„è¯†ï¼Œå·²ç»å½»åº•æ”¹å˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆæ–¹å¼ï¼Œä½¿å…¶æˆä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²çš„å¸å¼•åŠ›è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DiffDISæ‰©æ•£é©±åŠ¨åˆ†å‰²æ¨¡å‹ï¼Œå®ƒé€šè¿‡æŒ–æ˜æ‰©æ•£æ¨¡å‹å†…é¢„è®­ç»ƒU-Netçš„æ½œåŠ›ï¼Œç‰¹åˆ«é’ˆå¯¹é«˜åˆ†è¾¨ç‡ç²¾ç»†ç²’åº¦å¯¹è±¡åˆ†å‰²è¿›è¡Œè®¾è®¡ã€‚é€šè¿‡åˆ©ç”¨SDæ¨¡å‹çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›å’Œä¸°å¯Œçš„é€šç”¨å›¾åƒè¡¨ç¤ºå…ˆéªŒçŸ¥è¯†ï¼Œç»“åˆé’ˆå¯¹ä»»åŠ¡çš„ç¨³å®šä¸€æ­¥å»å™ªæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å‡å°‘æ¨ç†æ—¶é—´çš„åŒæ—¶ä¿æŒé«˜åº¦ä¿çœŸå’Œè¯¦ç»†çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€é¡¹è¾…åŠ©è¾¹ç¼˜ç”Ÿæˆä»»åŠ¡ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºå¢å¼ºå¯¹è±¡è¾¹ç•Œç²¾ç»†ç»†èŠ‚çš„ä¿æŠ¤ï¼Œè¿˜èƒ½åè°ƒæ‰©æ•£çš„æ¦‚ç‡æ€§ä¸åˆ†å‰²çš„ç¡®å®šæ€§éœ€æ±‚ã€‚å‡­å€Ÿè¿™äº›æ”¹è¿›çš„ç­–ç•¥ï¼ŒDiffDISä½œä¸ºä¸€ä¸ªå¿«é€Ÿå¯¹è±¡æ©è†œç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«ä¼˜åŒ–äº†é«˜åˆ†è¾¨ç‡ä¸‹è¯¦ç»†äºŒè¿›åˆ¶åœ°å›¾çš„ç”Ÿæˆï¼Œå±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å‡†ç¡®æ€§å’Œå¿«é€Ÿå¤„ç†ã€‚åœ¨DIS5Kæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†DiffDISçš„ä¼˜è¶Šæ€§ï¼Œé€šè¿‡ç®€åŒ–çš„æ¨ç†è¿‡ç¨‹å®ç°äº†æœ€æ–°ç»“æœã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qianyu-dlut/DiffDIS">https://github.com/qianyu-dlut/DiffDIS</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10105v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨é«˜åˆ†è¾¨ç‡ç²¾ç»†ç²’åº¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„ä¸»è¦æŒ‘æˆ˜ï¼Œå³å¹³è¡¡å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ„è¯†å’Œç²¾ç»†å¯¹è±¡è½®å»“çš„ç²¾ç¡®æç»˜ã€‚æå‡ºä¸€ç§åä¸ºDiffDISçš„æ‰©æ•£é©±åŠ¨åˆ†å‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„U-Netç»“æ„ï¼Œä¸“ä¸ºé«˜åˆ†è¾¨ç‡ç²¾ç»†ç²’åº¦å¯¹è±¡åˆ†å‰²è®¾è®¡ã€‚é€šè¿‡ç»“åˆSDæ¨¡å‹çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›å’Œä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºå…ˆéªŒçŸ¥è¯†ï¼Œä»¥åŠç‰¹å®šçš„ç¨³å®šå•æ­¥å»å™ªæ–¹æ³•ï¼ŒDiffDISåœ¨ä¿æŒé«˜ä¿çœŸåº¦ç»†èŠ‚ç”Ÿæˆçš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚å¼•å…¥è¾…åŠ©è¾¹ç¼˜ç”Ÿæˆä»»åŠ¡ï¼Œä¸ä»…æé«˜äº†å¯¹è±¡è¾¹ç•Œç²¾ç»†ç»†èŠ‚çš„ä¿ç•™ï¼Œè¿˜åè°ƒäº†æ‰©æ•£çš„æ¦‚ç‡æ€§ä¸åˆ†å‰²çš„ç¡®å®šæ€§éœ€æ±‚ã€‚åœ¨DIS5Kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffDISå…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†ç®€æ´çš„æ¨ç†è¿‡ç¨‹ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡ç²¾ç»†ç²’åº¦å›¾åƒåˆ†å‰²é¢ä¸´å¹³è¡¡ä¸Šä¸‹æ–‡æ„è¯†å’Œå¯¹è±¡è½®å»“ç²¾ç»†æç»˜çš„æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„è´¨é‡å’Œç²¾ç»†çš„ç»†èŠ‚åˆ†è¾¨ç‡ï¼Œå·²æˆä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²çš„å¸å¼•åŠ›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>DiffDISæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å‰²æ¨¡å‹ï¼Œä¸“ä¸ºé«˜åˆ†è¾¨ç‡ç²¾ç»†ç²’åº¦å¯¹è±¡åˆ†å‰²è®¾è®¡ã€‚</li>
<li>DiffDISåˆ©ç”¨SDæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºå…ˆéªŒï¼Œç»“åˆç¨³å®šçš„ä¸€æ­¥å»å™ªæ–¹æ³•ï¼Œå‡å°‘æ¨ç†æ—¶é—´å¹¶ä¿æŒé«˜ä¿çœŸåº¦ç»†èŠ‚ç”Ÿæˆã€‚</li>
<li>å¼•å…¥è¾…åŠ©è¾¹ç¼˜ç”Ÿæˆä»»åŠ¡ä»¥æé«˜å¯¹è±¡è¾¹ç•Œçš„ç²¾ç»†ç»†èŠ‚ä¿ç•™ï¼Œå¹¶åè°ƒæ‰©æ•£çš„æ¦‚ç‡æ€§ä¸åˆ†å‰²çš„ç¡®å®šæ€§ã€‚</li>
<li>DiffDISåœ¨DIS5Kæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>DiffDISå°†å…¬å¼€å¯ç”¨æºä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-857183236a8451ccde94fee519dc17f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6982a7382beeb93eefa5c2e3b5a585e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31edc1e1b196ba928312dac2e7a0ffaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be5bd53de8fda17fc629d352c267e818.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Goal-Oriented-Semantic-Communication-for-Wireless-Image-Transmission-via-Stable-Diffusion"><a href="#Goal-Oriented-Semantic-Communication-for-Wireless-Image-Transmission-via-Stable-Diffusion" class="headerlink" title="Goal-Oriented Semantic Communication for Wireless Image Transmission via   Stable Diffusion"></a>Goal-Oriented Semantic Communication for Wireless Image Transmission via   Stable Diffusion</h2><p><strong>Authors:Nan Li, Yansha Deng</strong></p>
<p>Efficient image transmission is essential for seamless communication and collaboration within the visually-driven digital landscape. To achieve low latency and high-quality image reconstruction over a bandwidth-constrained noisy wireless channel, we propose a stable diffusion (SD)-based goal-oriented semantic communication (GSC) framework. In this framework, we design a semantic autoencoder that effectively extracts semantic information (SI) from images to reduce the transmission data size while ensuring high-quality reconstruction. Recognizing the impact of wireless channel noise on SI transmission, we propose an SD-based denoiser for GSC (SD-GSC) conditional on an instantaneous channel gain to remove the channel noise from the received noisy SI under known channel. For scenarios with unknown channel, we further propose a parallel SD denoiser for GSC (PSD-GSC) to jointly learn the distribution of channel gains and denoise the received SI. It is shown that, with the known channel, our SD-GSC outperforms state-of-the-art ADJSCC and Latent-Diff DNSC, improving Peak Signal-to-Noise Ratio (PSNR) by 32% and 21%, and reducing Fr&#39;echet Inception Distance (FID) by 40% and 35%, respectively. With the unknown channel, our PSD-GSC improves PSNR by 8% and reduces FID by 17% compared to MMSE equalizer-enhanced SD-GSC. </p>
<blockquote>
<p>åœ¨è§†è§‰é©±åŠ¨çš„æ•°å­—åŒ–æ™¯è§‚ä¸­ï¼Œé«˜æ•ˆçš„å›¾åƒä¼ è¾“å¯¹äºæ— ç¼é€šä¿¡å’Œåä½œè‡³å…³é‡è¦ã€‚ä¸ºäº†åœ¨å¸¦å®½å—é™çš„å˜ˆæ‚æ— çº¿é€šé“ä¸Šå®ç°ä½å»¶è¿Ÿå’Œé«˜è´¨é‡çš„å›¾åƒé‡å»ºï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç›®æ ‡å¯¼å‘è¯­ä¹‰é€šä¿¡ï¼ˆGSCï¼‰æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰è‡ªåŠ¨ç¼–ç å™¨ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°ä»å›¾åƒä¸­æå–è¯­ä¹‰ä¿¡æ¯ï¼ˆSIï¼‰ï¼Œä»¥å‡å°ä¼ è¾“æ•°æ®å¤§å°ï¼ŒåŒæ—¶ç¡®ä¿é«˜è´¨é‡çš„é‡å»ºã€‚è€ƒè™‘åˆ°æ— çº¿ä¿¡é“å™ªå£°å¯¹SIä¼ è¾“çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºSDçš„GSCå»å™ªå™¨ï¼ˆSD-GSCï¼‰ï¼Œå®ƒä¾èµ–äºç¬æ—¶ä¿¡é“å¢ç›Šï¼Œä»¥ä»æ¥æ”¶åˆ°çš„å¸¦å™ªå£°çš„SIä¸­æ¶ˆé™¤ä¿¡é“å™ªå£°ã€‚å¯¹äºæœªçŸ¥ä¿¡é“åœºæ™¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å¹¶è¡ŒSDå»å™ªå™¨GSCï¼ˆPSD-GSCï¼‰ï¼Œä»¥è”åˆå­¦ä¹ ä¿¡é“å¢ç›Šçš„åˆ†å¸ƒå¹¶å¯¹æ¥æ”¶åˆ°çš„SIè¿›è¡Œå»å™ªã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å·²çŸ¥ä¿¡é“çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„SD-GSCä¼˜äºæœ€å…ˆè¿›ADJSCCå’Œéšå¼å·®åˆ†DNSCï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†32%å’Œ21%ï¼ŒFrÃ©chet Inception Distanceï¼ˆFIDï¼‰åˆ†åˆ«é™ä½äº†40%å’Œ35%ã€‚åœ¨æœªçŸ¥ä¿¡é“çš„æƒ…å†µä¸‹ï¼Œä¸MMSEå‡è¡¡å¢å¼ºå‹SD-GSCç›¸æ¯”ï¼Œæˆ‘ä»¬çš„PSD-GSCæé«˜äº†PSNRçš„8%ï¼Œå¹¶é™ä½äº†FIDçš„17%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00428v3">PDF</a> Accepted by IEEE ICC 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å›¾åƒçš„é«˜æ•ˆä¼ è¾“åœ¨è§†è§‰é©±åŠ¨çš„æ•°å­—åŒ–æ™¯è§‚ä¸­å¯¹äºæ— ç¼é€šä¿¡å’Œåä½œè‡³å…³é‡è¦ã€‚ä¸ºå®ç°ä½å»¶è¿Ÿã€é«˜è´¨é‡å›¾åƒåœ¨å¸¦å®½å—é™çš„å™ªå£°æ— çº¿é€šé“ä¸Šçš„é‡å»ºï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç›®æ ‡å¯¼å‘è¯­ä¹‰é€šä¿¡ï¼ˆGSCï¼‰æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è¯­ä¹‰è‡ªåŠ¨ç¼–ç å™¨ï¼Œæœ‰æ•ˆæå–å›¾åƒè¯­ä¹‰ä¿¡æ¯ï¼ˆSIï¼‰ï¼Œä»¥å‡å°‘ä¼ è¾“æ•°æ®é‡å¹¶ç¡®ä¿é«˜è´¨é‡é‡å»ºã€‚è€ƒè™‘åˆ°æ— çº¿ä¿¡é“å™ªå£°å¯¹SIä¼ è¾“çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç¬æ—¶ä¿¡é“å¢ç›Šçš„SD GSCå»å™ªå™¨ï¼ˆSD-GSCï¼‰ï¼Œä»¥ä»æ¥æ”¶åˆ°çš„å™ªå£°SIä¸­æ¶ˆé™¤ä¿¡é“å™ªå£°ã€‚å¯¹äºæœªçŸ¥ä¿¡é“åœºæ™¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å¹¶è¡ŒSD GSCå»å™ªå™¨ï¼ˆPSD-GSCï¼‰ï¼Œä»¥è”åˆå­¦ä¹ ä¿¡é“å¢ç›Šåˆ†å¸ƒå¹¶å¯¹æ¥æ”¶åˆ°çš„SIè¿›è¡Œå»å™ªã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å·²çŸ¥ä¿¡é“ä¸Šï¼Œæˆ‘ä»¬çš„SD-GSCä¼˜äºæœ€å…ˆè¿›çš„ADJSCCå’Œæ½œåœ¨å·®å¼‚DNSCï¼Œå³°å€¼ä¿¡å·ä¸å™ªå£°æ¯”ï¼ˆPSNRï¼‰æé«˜äº†32%å’Œ21%ï¼ŒFrâ€™echet inceptionè·ç¦»ï¼ˆFIDï¼‰åˆ†åˆ«é™ä½äº†40%å’Œ35%ã€‚åœ¨æœªçŸ¥ä¿¡é“ä¸Šï¼Œä¸MMSEå‡è¡¡å¢å¼ºSD-GSCç›¸æ¯”ï¼Œæˆ‘ä»¬çš„PSD-GSCæé«˜äº†PSNRçš„8%ï¼Œå¹¶é™ä½äº†FIDçš„17%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰ç›®æ ‡å¯¼å‘è¯­ä¹‰é€šä¿¡ï¼ˆGSCï¼‰æ¡†æ¶è¢«æå‡ºä»¥å®ç°é«˜æ•ˆå›¾åƒä¼ è¾“ã€‚</li>
<li>è®¾è®¡äº†è¯­ä¹‰è‡ªåŠ¨ç¼–ç å™¨ä»¥æœ‰æ•ˆæå–å¹¶ä¼ è¾“å›¾åƒè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>é’ˆå¯¹æ— çº¿é€šé“å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†åŸºäºç¬æ—¶ä¿¡é“å¢ç›Šçš„SD-GSCå»å™ªå™¨ã€‚</li>
<li>å¯¹äºæœªçŸ¥ä¿¡é“ï¼Œæå‡ºäº†å¹¶è¡ŒSD GSCå»å™ªå™¨ï¼ˆPSD-GSCï¼‰è¿›è¡Œè”åˆå­¦ä¹ å¹¶å»å™ªã€‚</li>
<li>åœ¨å·²çŸ¥ä¿¡é“åœºæ™¯ä¸‹ï¼ŒSD-GSCæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨PSNRå’ŒFIDæŒ‡æ ‡ä¸Šæœ‰æ˜æ˜¾æé«˜ã€‚</li>
<li>åœ¨æœªçŸ¥ä¿¡é“åœºæ™¯ä¸‹ï¼ŒPSD-GSCç›¸æ¯”å¢å¼ºå‹SD-GSCæœ‰æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºå®ç°ä½å»¶è¿Ÿã€é«˜è´¨é‡å›¾åƒåœ¨å¸¦å®½å—é™ã€å™ªå£°å¹²æ‰°çš„æ— çº¿é€šé“ä¸Šçš„ä¼ è¾“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62f7fce37a18bafd9d7cf4e946705ed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1f7fb5f19aa8f64232645a8d1d08b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8330e97243aa09de401f538f59c019d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bf770107e16180969c301dd870a0f2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d66c95468477b329d478174776be86b7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c6cd6aa7d48fc834b882a37029a98766.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  Persuasion Should be Double-Blind A Multi-Domain Dialogue Dataset With   Faithfulness Based on Causal Theory of Mind
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f384f1b9f2ad51b7ac6f1f4c3487f2f7.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  Does Generation Require Memorization? Creative Diffusion Models using   Ambient Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
