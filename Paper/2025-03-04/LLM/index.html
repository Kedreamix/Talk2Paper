<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  LLM Post-Training A Deep Dive into Reasoning Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c168f75afe910ba118b14fe1fde0cbb6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-04-æ›´æ–°"><a href="#2025-03-04-æ›´æ–°" class="headerlink" title="2025-03-04 æ›´æ–°"></a>2025-03-04 æ›´æ–°</h1><h2 id="LLM-Post-Training-A-Deep-Dive-into-Reasoning-Large-Language-Models"><a href="#LLM-Post-Training-A-Deep-Dive-into-Reasoning-Large-Language-Models" class="headerlink" title="LLM Post-Training: A Deep Dive into Reasoning Large Language Models"></a>LLM Post-Training: A Deep Dive into Reasoning Large Language Models</h2><p><strong>Authors:Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now increasingly shifting focus toward post-training techniques to achieve further breakthroughs. While pretraining provides a broad linguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and align more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have emerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various real-world tasks. This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. We highlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research directions. We also provide a public repository to continually track developments in this fast-evolving field: <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/Awesome-LLM-Post-training">https://github.com/mbzuai-oryx/Awesome-LLM-Post-training</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ï¼Œå¹¶å¸¦æ¥äº†å¤šæ ·åŒ–çš„åº”ç”¨ã€‚åœ¨å¤§é‡ç½‘ç»œè§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒä¸ºè¿™äº›æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œç„¶è€Œç ”ç©¶ç•Œç°åœ¨è¶Šæ¥è¶Šå°†é‡ç‚¹è½¬å‘åè®­ç»ƒæŠ€æœ¯ï¼Œä»¥å®ç°è¿›ä¸€æ­¥çš„çªç ´ã€‚è™½ç„¶é¢„è®­ç»ƒæä¾›äº†å¹¿æ³›çš„è¯­è¨€åŸºç¡€ï¼Œä½†åè®­ç»ƒæ–¹æ³•ä½¿LLMèƒ½å¤Ÿç²¾ç‚¼å…¶çŸ¥è¯†ï¼Œæé«˜æ¨ç†èƒ½åŠ›ï¼Œå¢å¼ºäº‹å®å‡†ç¡®æ€§ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°ä¸ç”¨æˆ·æ„å›¾å’Œé“å¾·è€ƒé‡å¯¹é½ã€‚å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾å·²æˆä¸ºä¼˜åŒ–LLMæ€§èƒ½ã€ç¡®ä¿ç¨³å¥æ€§å’Œæé«˜åœ¨å„ç§ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›çš„å…³é”®ç­–ç•¥ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†åè®­ç»ƒæ–¹æ³•è®ºï¼Œåˆ†æäº†å…¶åœ¨é¢„è®­ç»ƒä¹‹å¤–ç²¾ç‚¼LLMçš„ä½œç”¨ï¼Œè§£å†³äº†å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ç¾éš¾æ€§é—å¿˜ã€å¥–åŠ±é»‘å®¢å’Œæ¨ç†æ—¶é—´æƒè¡¡ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æ¨¡å‹å¯¹é½ã€å¯æ‰©å±•é€‚åº”å’Œæ¨ç†æ—¶é—´æ¨ç†çš„æ–°å…´æ–¹å‘ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¬å…±ä»“åº“ï¼Œä»¥æŒç»­è·Ÿè¸ªè¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„å‘å±•ï¼š<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/Awesome-LLM-Post-training">https://github.com/mbzuai-oryx/Awesome-LLM-Post-training</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21321v1">PDF</a> 31 pages, 7 figures, 3 tables, 375 references</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥äº†å˜é©ï¼Œå¹¶å‚¬ç”Ÿäº†å¤šæ ·åŒ–çš„åº”ç”¨ã€‚ç ”ç©¶ç•Œç°åœ¨è¶Šæ¥è¶Šå…³æ³¨å¦‚ä½•é€šè¿‡åè®­ç»ƒæŠ€æœ¯å®ç°è¿›ä¸€æ­¥çš„çªç ´ã€‚è™½ç„¶é¢„è®­ç»ƒæä¾›äº†å¹¿æ³›çš„è¯­è¨€åŸºç¡€ï¼Œä½†åè®­ç»ƒæ–¹æ³•ä½¿LLMèƒ½å¤Ÿç²¾ç‚¼çŸ¥è¯†ã€æé«˜æ¨ç†èƒ½åŠ›ã€å¢å¼ºäº‹å®å‡†ç¡®æ€§ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°ä¸ç”¨æˆ·æ„å›¾å’Œé“å¾·è€ƒé‡å¯¹é½ã€‚å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ç­‰ç­–ç•¥å¯¹äºä¼˜åŒ–LLMæ€§èƒ½ã€ç¡®ä¿ç¨³å¥æ€§å’Œæé«˜é€‚åº”å„ç§ç°å®ä»»åŠ¡çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†åè®­ç»ƒæ–¹æ³•è®ºï¼Œåˆ†æäº†å…¶åœ¨ç²¾ç‚¼LLMæ–¹é¢è¶…è¶Šé¢„è®­ç»ƒçš„ä½œç”¨ï¼Œå¹¶è§£å†³äº†è¯¸å¦‚ç¾éš¾æ€§é—å¿˜ã€å¥–åŠ±é»‘å®¢å’Œæ¨ç†æ—¶é—´æƒè¡¡ç­‰å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æ¨¡å‹å¯¹é½ã€å¯ä¼¸ç¼©é€‚åº”å’Œæ¨ç†æ—¶é—´æ¨ç†ç­‰æ–°å…´æ–¹å‘ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ï¼Œå¹¶å‚¬ç”Ÿäº†å„ç§åº”ç”¨ã€‚</li>
<li>é¢„è®­ç»ƒä¸ºLLMsæä¾›äº†å¹¿æ³›çš„è¯­è¨€åŸºç¡€ï¼Œè€Œåè®­ç»ƒæ–¹æ³•ç”¨äºç²¾ç‚¼çŸ¥è¯†ã€æé«˜æ¨ç†èƒ½åŠ›å’Œäº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>åè®­ç»ƒæŠ€æœ¯ä½¿LLMä¸ç”¨æˆ·æ„å›¾å’Œé“å¾·è€ƒé‡æ›´åŠ å¯¹é½ã€‚</li>
<li>å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ˜¯ä¼˜åŒ–LLMæ€§èƒ½çš„å…³é”®ç­–ç•¥ã€‚</li>
<li>åè®­ç»ƒæ–¹æ³•è®ºåœ¨è§£å†³ç¾éš¾æ€§é—å¿˜ã€å¥–åŠ±é»‘å®¢å’Œæ¨ç†æ—¶é—´æƒè¡¡ç­‰æŒ‘æˆ˜æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ã€‚</li>
<li>æ¨¡å‹å¯¹é½ã€å¯ä¼¸ç¼©é€‚åº”å’Œæ¨ç†æ—¶é—´æ¨ç†æ˜¯æ–°å…´çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a41e6f30e5612aa1c10ca45a52c0ea1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6a34b4d9e21d303cd94e341e1d14e97.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FANformer-Improving-Large-Language-Models-Through-Effective-Periodicity-Modeling"><a href="#FANformer-Improving-Large-Language-Models-Through-Effective-Periodicity-Modeling" class="headerlink" title="FANformer: Improving Large Language Models Through Effective Periodicity   Modeling"></a>FANformer: Improving Large Language Models Through Effective Periodicity   Modeling</h2><p><strong>Authors:Yihong Dong, Ge Li, Xue Jiang, Yongding Tao, Kechi Zhang, Hao Zhu, Huanyu Liu, Jiazheng Ding, Jia Li, Jinliang Deng, Hong Mei</strong></p>
<p>Periodicity, as one of the most important basic characteristics, lays the foundation for facilitating structured knowledge acquisition and systematic cognitive processes within human learning paradigms. However, the potential flaws of periodicity modeling in Transformer affect the learning efficiency and establishment of underlying principles from data for large language models (LLMs) built upon it. In this paper, we demonstrate that integrating effective periodicity modeling can improve the learning efficiency and performance of LLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN) into attention mechanism to achieve efficient periodicity modeling, by modifying the feature projection process of attention mechanism. Extensive experimental results on language modeling show that FANformer consistently outperforms Transformer when scaling up model size and training tokens, underscoring its superior learning efficiency. To further validate the effectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens. FANformer-1B exhibits marked improvements on downstream tasks compared to open-source LLMs with similar model parameters or training tokens. The results position FANformer as an effective and promising architecture for advancing LLMs. </p>
<blockquote>
<p>å‘¨æœŸæ€§ä½œä¸ºæœ€é‡è¦çš„åŸºæœ¬ç‰¹å¾ä¹‹ä¸€ï¼Œä¸ºäººç±»å­¦ä¹ èŒƒå¼ä¸­çš„ç»“æ„åŒ–çŸ¥è¯†è·å–å’Œç³»ç»Ÿè®¤çŸ¥è¿‡ç¨‹å¥ å®šäº†åŸºç¡€ã€‚ç„¶è€Œï¼ŒTransformerä¸­çš„å‘¨æœŸæ€§å»ºæ¨¡çš„æ½œåœ¨ç¼ºé™·å½±å“äº†å…¶ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å­¦ä¹ æ•ˆç‡å’ŒåŸºç¡€åŸç†çš„å»ºç«‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†é›†æˆæœ‰æ•ˆçš„å‘¨æœŸæ€§å»ºæ¨¡å¯ä»¥æé«˜LLMçš„å­¦ä¹ æ•ˆç‡å’Œå·¥ä½œæ€§èƒ½ã€‚æˆ‘ä»¬ä»‹ç»äº†FANformerï¼Œå®ƒé€šè¿‡ä¿®æ”¹æ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾æŠ•å½±è¿‡ç¨‹ï¼Œå°†å‚…é‡Œå¶åˆ†æç½‘ç»œï¼ˆFANï¼‰é›†æˆåˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å‘¨æœŸæ€§å»ºæ¨¡ã€‚åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒä»¤ç‰Œæ—¶ï¼ŒFANformeræŒç»­ä¼˜äºTransformerï¼Œçªæ˜¾äº†å…¶è¾ƒé«˜çš„å­¦ä¹ æ•ˆç‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯FANformerçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨FANformer-1Bå¯¹ä¸‡äº¿ä»¤ç‰Œè¿›è¡Œé¢„è®­ç»ƒã€‚ä¸å…·æœ‰ç±»ä¼¼æ¨¡å‹å‚æ•°æˆ–è®­ç»ƒä»¤ç‰Œçš„å¼€æºLLMç›¸æ¯”ï¼ŒFANformer-1Båœ¨ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚è¿™ä¸ºFANformerä½œä¸ºä¸€ç§æœ‰æ•ˆä¸”å‰æ™¯å¹¿é˜”çš„LLMæ¶æ„çš„å®šä½æä¾›äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21309v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†å‘¨æœŸæ€§ä½œä¸ºé‡è¦çš„åŸºæœ¬ç‰¹å¾ï¼Œåœ¨ä¿ƒè¿›äººç±»å­¦ä¹ æ¨¡å¼ä¸­çš„ç»“æ„åŒ–çŸ¥è¯†è·å–å’Œç³»ç»Ÿæ€§è®¤çŸ¥è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚æ–‡ç« æŒ‡å‡ºTransformerä¸­çš„å‘¨æœŸæ€§å»ºæ¨¡å­˜åœ¨æ½œåœ¨ç¼ºé™·ï¼Œå½±å“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å­¦ä¹ æ•ˆç‡å’ŒåŸç†å»ºç«‹ã€‚ä¸ºæ”¹å–„è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å±•ç¤ºäº†é›†æˆæœ‰æ•ˆçš„å‘¨æœŸæ€§å»ºæ¨¡èƒ½æé«˜LLMçš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†FANformerï¼Œå®ƒé€šè¿‡ä¿®æ”¹æ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾æŠ•å½±è¿‡ç¨‹ï¼Œå°†å‚…é‡Œå¶åˆ†æç½‘ç»œï¼ˆFANï¼‰èå…¥æ³¨æ„åŠ›æœºåˆ¶ä¸­å®ç°é«˜æ•ˆçš„å‘¨æœŸæ€§å»ºæ¨¡ã€‚åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢çš„å¹¿æ³›å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFANformeråœ¨æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒä»¤ç‰Œæ—¶å§‹ç»ˆä¼˜äºTransformerï¼Œæ˜¾ç¤ºå‡ºå…¶å“è¶Šçš„å­¦ä¹ æ•ˆç‡ã€‚é€šè¿‡é¢„è®­ç»ƒFANformer-1Bæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºå…·æœ‰ç›¸ä¼¼æ¨¡å‹å‚æ•°æˆ–è®­ç»ƒä»¤ç‰Œçš„å¼€æºLLMè¡¨ç°æ›´ä½³ï¼Œè¿™è¯æ˜äº†FANformeræ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”æœ‰å‰æ™¯çš„æ¶æ„ï¼Œå¯ä¿ƒè¿›LLMçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘¨æœŸæ€§åœ¨ä¿ƒè¿›äººç±»å­¦ä¹ ä¸­çš„ç»“æ„åŒ–çŸ¥è¯†è·å–å’Œç³»ç»Ÿæ€§è®¤çŸ¥è¿‡ç¨‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>Transformerä¸­çš„å‘¨æœŸæ€§å»ºæ¨¡å­˜åœ¨æ½œåœ¨ç¼ºé™·ï¼Œå¯èƒ½å½±å“LLMçš„å­¦ä¹ æ•ˆç‡å’ŒåŸç†å»ºç«‹ã€‚</li>
<li>FANformeré€šè¿‡é›†æˆæœ‰æ•ˆçš„å‘¨æœŸæ€§å»ºæ¨¡ï¼Œæé«˜äº†LLMçš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>FANformerå¼•å…¥äº†å‚…é‡Œå¶åˆ†æç½‘ç»œï¼ˆFANï¼‰ï¼Œå®ç°äº†é«˜æ•ˆçš„å‘¨æœŸæ€§å»ºæ¨¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFANformeråœ¨æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒä»¤ç‰Œæ—¶è¡¨ç°ä¼˜äºTransformerã€‚</li>
<li>FANformer-1Bæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç›¸ä¼¼å‚æ•°æˆ–è®­ç»ƒä»¤ç‰Œçš„å¼€æºLLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3df0e5a5fd8ad0c4c57f5aefca96cb40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8237f4a01f07391879aeaa10a351f387.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f5e72b070011412fa1164509b60e0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff746befe013c7df3e2a1b84da8e9d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-906e37f6ea64447f5c53fe936f751a46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c168f75afe910ba118b14fe1fde0cbb6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contextualizing-biological-perturbation-experiments-through-language"><a href="#Contextualizing-biological-perturbation-experiments-through-language" class="headerlink" title="Contextualizing biological perturbation experiments through language"></a>Contextualizing biological perturbation experiments through language</h2><p><strong>Authors:Menghua Wu, Russell Littman, Jacob Levine, Lin Qiu, Tommaso Biancalani, David Richmond, Jan-Christian Huetter</strong></p>
<p>High-content perturbation experiments allow scientists to probe biomolecular systems at unprecedented resolution, but experimental and analysis costs pose significant barriers to widespread adoption. Machine learning has the potential to guide efficient exploration of the perturbation space and extract novel insights from these data. However, current approaches neglect the semantic richness of the relevant biology, and their objectives are misaligned with downstream biological analyses. In this paper, we hypothesize that large language models (LLMs) present a natural medium for representing complex biological relationships and rationalizing experimental outcomes. We propose PerturbQA, a benchmark for structured reasoning over perturbation experiments. Unlike current benchmarks that primarily interrogate existing knowledge, PerturbQA is inspired by open problems in perturbation modeling: prediction of differential expression and change of direction for unseen perturbations, and gene set enrichment. We evaluate state-of-the-art machine learning and statistical approaches for modeling perturbations, as well as standard LLM reasoning strategies, and we find that current methods perform poorly on PerturbQA. As a proof of feasibility, we introduce Summer (SUMMarize, retrievE, and answeR, a simple, domain-informed LLM framework that matches or exceeds the current state-of-the-art. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/genentech/PerturbQA">https://github.com/genentech/PerturbQA</a>. </p>
<blockquote>
<p>é«˜å†…æ¶µæ‰°åŠ¨å®éªŒè®©ç§‘å­¦å®¶èƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„åˆ†è¾¨ç‡æ¢ç©¶ç”Ÿç‰©åˆ†å­ç³»ç»Ÿï¼Œä½†å®éªŒå’Œåˆ†ææˆæœ¬æ„æˆäº†å¹¿æ³›é‡‡çº³çš„é‡å¤§éšœç¢ã€‚æœºå™¨å­¦ä¹ æœ‰å¯èƒ½å¼•å¯¼é«˜æ•ˆæ¢ç´¢æ‰°åŠ¨ç©ºé—´ï¼Œå¹¶ä»è¿™äº›æ•°æ®ä¸­æå–æ–°é¢–è§è§£ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¿½è§†äº†ç›¸å…³ç”Ÿç‰©å­¦çš„è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œå¹¶ä¸”ä»–ä»¬çš„ç›®æ ‡ä¸ä¸‹æ¸¸ç”Ÿç‰©åˆ†æå­˜åœ¨ä¸åŒ¹é…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‡è®¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯è¡¨ç¤ºå¤æ‚ç”Ÿç‰©å…³ç³»å¹¶å¯¹å®éªŒç»“æœè¿›è¡Œåˆç†åŒ–è§£é‡Šçš„å¤©ç„¶åª’ä»‹ã€‚æˆ‘ä»¬æå‡ºäº†PerturbQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºæ‰°åŠ¨å®éªŒçš„ç»“æ„åŒ–æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä¸»è¦è¯¢é—®ç°æœ‰çŸ¥è¯†çš„å½“å‰åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒPerturbQAçš„çµæ„Ÿæ¥æºäºæ‰°åŠ¨å»ºæ¨¡ä¸­çš„å¼€æ”¾é—®é¢˜ï¼šé¢„æµ‹æœªè§æ‰°åŠ¨çš„å·®å¼‚è¡¨è¾¾å’Œå˜åŒ–æ–¹å‘ï¼Œä»¥åŠåŸºå› é›†å¯Œé›†ã€‚æˆ‘ä»¬è¯„ä¼°äº†ç”¨äºå»ºæ¨¡æ‰°åŠ¨çš„æœ€æ–°æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡æ–¹æ³•ï¼Œä»¥åŠæ ‡å‡†LLMæ¨ç†ç­–ç•¥ï¼Œæˆ‘ä»¬å‘ç°å½“å‰æ–¹æ³•åœ¨PerturbQAä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚ä½œä¸ºå¯è¡Œæ€§çš„è¯æ˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†Summerï¼ˆSUMMarizeï¼ŒretrievEï¼Œand answeRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€é¢å‘é¢†åŸŸçš„LLMæ¡†æ¶ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡äº†å½“å‰æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/genentech/PerturbQA%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/genentech/PerturbQAä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21290v1">PDF</a> The Thirteenth International Conference on Learning Representations   (2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é«˜é€šé‡æ‰°åŠ¨å®éªŒåœ¨æ¢ç©¶ç”Ÿç‰©åˆ†å­ç³»ç»Ÿæ–¹é¢çš„ä¼˜åŠ¿åŠæŒ‘æˆ˜ã€‚æœºå™¨å­¦ä¹ å¯æŒ‡å¯¼æœ‰æ•ˆæ¢ç´¢æ‰°åŠ¨ç©ºé—´å¹¶ä»æ•°æ®ä¸­æå–æ–°è§è§£ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•å¿½ç•¥äº†ç›¸å…³ç”Ÿç‰©å­¦çš„è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œä¸”ä¸ä¸‹æ¸¸ç”Ÿç‰©åˆ†æçš„ç›®æ ‡ä¸åŒ¹é…ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¡¨ç¤ºå¤æ‚çš„ç”Ÿç‰©å…³ç³»å¹¶è§£é‡Šå®éªŒç»“æœã€‚æå‡ºäº†æ‰°åŠ¨å®éªŒçš„æ–°åŸºå‡†æµ‹è¯•PerturbQAï¼Œç”¨äºç»“æ„æ¨ç†ã€‚è¯„ä¼°äº†å½“å‰å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ–¹æ³•ä»¥åŠæ ‡å‡†LLMæ¨ç†ç­–ç•¥ï¼Œå‘ç°å®ƒä»¬åœ¨PerturbQAä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºè¯æ˜å¯è¡Œæ€§ï¼Œä»‹ç»äº†ä¸€ä¸ªç®€å•ä¸”åŸºäºé¢†åŸŸçŸ¥è¯†çš„LLMæ¡†æ¶Summerã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜é€šé‡æ‰°åŠ¨å®éªŒèƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„åˆ†è¾¨ç‡æ¢ç©¶ç”Ÿç‰©åˆ†å­ç³»ç»Ÿï¼Œä½†å®éªŒå’Œåˆ†ææˆæœ¬æ„æˆé‡è¦éšœç¢ã€‚</li>
<li>æœºå™¨å­¦ä¹ å¯æŒ‡å¯¼é«˜æ•ˆæ¢ç´¢æ‰°åŠ¨ç©ºé—´å¹¶ä»ä¸­æå–æ–°è§è§£ï¼Œä½†å½“å‰æ–¹æ³•å¿½ç•¥äº†ç”Ÿç‰©å­¦çš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯è‡ªç„¶è¡¨ç¤ºå¤æ‚çš„ç”Ÿç‰©å…³ç³»å¹¶è§£é‡Šå®éªŒç»“æœã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•PerturbQAï¼Œç”¨äºè¯„ä¼°å¯¹æ‰°åŠ¨å®éªŒçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å…ˆè¿›æ–¹æ³•åœ¨PerturbQAä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>Summeræ˜¯ä¸€ä¸ªç®€å•ä¸”åŸºäºé¢†åŸŸçŸ¥è¯†çš„LLMæ¡†æ¶ï¼Œå…¶æ€§èƒ½åŒ¹é…æˆ–è¶…è¿‡äº†å½“å‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-19fe501840c1240c846ecd1e43632aae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3e0259bcd98fc4f9582a2c275bd1de9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62149897bb06049a92cca8df91be0802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b88fc5a5c7d481b94390c792daa12d6a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Keyframe-Sampling-for-Long-Video-Understanding"><a href="#Adaptive-Keyframe-Sampling-for-Long-Video-Understanding" class="headerlink" title="Adaptive Keyframe Sampling for Long Video Understanding"></a>Adaptive Keyframe Sampling for Long Video Understanding</h2><p><strong>Authors:Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, Qixiang Ye</strong></p>
<p>Multimodal large language models (MLLMs) have enabled open-world visual understanding by injecting visual input as extra tokens into large language models (LLMs) as contexts. However, when the visual input changes from a single image to a long video, the above paradigm encounters difficulty because the vast amount of video tokens has significantly exceeded the maximal capacity of MLLMs. Therefore, existing video-based MLLMs are mostly established upon sampling a small portion of tokens from input data, which can cause key information to be lost and thus produce incorrect answers. This paper presents a simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It inserts a plug-and-play module known as keyframe selection, which aims to maximize the useful information with a fixed number of video tokens. We formulate keyframe selection as an optimization involving (1) the relevance between the keyframes and the prompt, and (2) the coverage of the keyframes over the video, and present an adaptive algorithm to approximate the best solution. Experiments on two long video understanding benchmarks validate that Adaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines) upon selecting informative keyframes. Our study reveals the importance of information pre-filtering in video-based MLLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ncTimTang/AKS">https://github.com/ncTimTang/AKS</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡å°†è§†è§‰è¾“å…¥ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸Šä¸‹æ–‡ä¸­çš„é¢å¤–ä»¤ç‰Œï¼Œå®ç°äº†å¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£ã€‚ç„¶è€Œï¼Œå½“è§†è§‰è¾“å…¥ä»å•å¼ å›¾åƒå˜ä¸ºé•¿è§†é¢‘æ—¶ï¼Œä¸Šè¿°æ¨¡å¼ä¼šé‡åˆ°å›°éš¾ï¼Œå› ä¸ºå¤§é‡è§†é¢‘ä»¤ç‰Œå·²ç»å¤§å¤§è¶…è¿‡äº†MLLMsçš„æœ€å¤§å®¹é‡ã€‚å› æ­¤ï¼Œç°æœ‰çš„åŸºäºè§†é¢‘çš„MLLMså¤§å¤šå»ºç«‹åœ¨ä»è¾“å…¥æ•°æ®ä¸­é‡‡æ ·ä¸€å°éƒ¨åˆ†ä»¤ç‰Œçš„åŸºç¡€ä¸Šï¼Œè¿™å¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±ï¼Œä»è€Œäº§ç”Ÿé”™è¯¯çš„ç­”æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç®—æ³•ï¼Œåä¸ºè‡ªé€‚åº”å…³é”®å¸§é‡‡æ ·ï¼ˆAKSï¼‰ã€‚å®ƒæ’å…¥äº†ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œç§°ä¸ºå…³é”®å¸§é€‰æ‹©ï¼Œæ—¨åœ¨ç”¨å›ºå®šæ•°é‡çš„è§†é¢‘ä»¤ç‰Œæœ€å¤§åŒ–æœ‰ç”¨ä¿¡æ¯ã€‚æˆ‘ä»¬å°†å…³é”®å¸§é€‰æ‹©ä¸æç¤ºä¹‹é—´çš„ç›¸å…³æ€§ä»¥åŠå…³é”®å¸§å¯¹è§†é¢‘çš„è¦†ç›–ä½œä¸ºä¼˜åŒ–çš„ä¸¤ä¸ªæ–¹é¢ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç®—æ³•æ¥è¿‘ä¼¼æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚åœ¨ä¸¤ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†è‡ªé€‚åº”å…³é”®å¸§é‡‡æ ·åœ¨æé«˜é€‰æ‹©æœ‰ä¿¡æ¯çš„å…³é”®å¸§çš„è§†é¢‘é—®ç­”å‡†ç¡®æ€§ï¼ˆè¶…è¶Šå¼ºåŸºçº¿ï¼‰æ–¹é¢çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¿¡æ¯åŸºäºè§†é¢‘çš„MLLMsä¸­ï¼Œä¿¡æ¯é¢„è¿‡æ»¤çš„é‡è¦æ€§ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ncTimTang/AKS%E3%80%82">https://github.com/ncTimTang/AKSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21271v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªé€‚åº”å…³é”®å¸§é‡‡æ ·æŠ€æœ¯ï¼ˆAKSï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§ç®€å•çš„ç®—æ³•ï¼Œé’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é•¿è§†é¢‘è¾“å…¥æ—¶çš„é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚AKSé€šè¿‡æ’å…¥ä¸€ä¸ªæ’ä»¶æ¨¡å—è¿›è¡Œå…³é”®å¸§é€‰æ‹©ï¼Œæ—¨åœ¨ä»¥å›ºå®šæ•°é‡çš„è§†é¢‘ä»¤ç‰Œæœ€å¤§åŒ–æœ‰ç”¨ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼ŒAdaptive Keyframe SamplingæŠ€æœ¯åœ¨é€‰æ‹©æœ‰ä¿¡æ¯é‡çš„å…³é”®å¸§åï¼Œèƒ½æé«˜è§†é¢‘é—®ç­”çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜è§†é¢‘ä¿¡æ¯é¢„è¿‡æ»¤åœ¨è§†é¢‘åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ååˆ†é‡è¦ã€‚è¯¥æ–¹æ³•çš„ä»£ç å¯ä»¥åœ¨æŒ‡å®šçš„GitHubåœ°å€æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘è¾“å…¥æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè§†é¢‘ä»¤ç‰Œæ•°é‡è¶…å‡ºäº†æ¨¡å‹çš„æœ€å¤§å®¹é‡ã€‚</li>
<li>å½“å‰åŸºäºè§†é¢‘çš„MLLMsä¸»è¦é€šè¿‡ä»è¾“å…¥æ•°æ®ä¸­é‡‡æ ·ä¸€å°éƒ¨åˆ†ä»¤ç‰Œæ¥å»ºç«‹æ¨¡å‹ï¼Œè¿™å¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯çš„ä¸¢å¤±å¹¶äº§ç”Ÿé”™è¯¯çš„ç­”æ¡ˆã€‚</li>
<li>æå‡ºäº†è‡ªé€‚åº”å…³é”®å¸§é‡‡æ ·ï¼ˆAKSï¼‰ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç®—æ³•åŒ…å«ä¸€ä¸ªæ’ä»¶æ¨¡å—ç”¨äºå…³é”®å¸§é€‰æ‹©ï¼Œä»¥æœ€å¤§åŒ–å›ºå®šæ•°é‡çš„è§†é¢‘ä»¤ç‰Œä¸­çš„æœ‰ç”¨ä¿¡æ¯ã€‚</li>
<li>å…³é”®å¸§é€‰æ‹©æ˜¯é€šè¿‡ä¸¤ä¸ªä¼˜åŒ–é—®é¢˜æ¥è§£å†³çš„ï¼šå…³é”®å¸§ä¸æç¤ºçš„ç›¸å…³æ€§ï¼Œä»¥åŠå…³é”®å¸§å¯¹è§†é¢‘çš„è¦†ç›–ç¨‹åº¦ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè‡ªé€‚åº”å…³é”®å¸§é‡‡æ ·èƒ½æé«˜è§†é¢‘é—®ç­”çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†ä¸€äº›å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†è§†é¢‘ä¿¡æ¯é¢„è¿‡æ»¤åœ¨åŸºäºè§†é¢‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b0c3cfeacb950e497f8c2aa683e6d882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f66fed94a8de05e1df24b9e51370ddd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24fa052a94e97c5529773f4c39687e9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-409faecd413ce1bb8499b2f9e1519282.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4189048ef1851c636f311451aa718954.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RoboBrain-A-Unified-Brain-Model-for-Robotic-Manipulation-from-Abstract-to-Concrete"><a href="#RoboBrain-A-Unified-Brain-Model-for-Robotic-Manipulation-from-Abstract-to-Concrete" class="headerlink" title="RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract   to Concrete"></a>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract   to Concrete</h2><p><strong>Authors:Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang</strong></p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various multimodal contexts. However, their application in robotic scenarios, particularly for long-horizon manipulation tasks, reveals significant limitations. These limitations arise from the current MLLMs lacking three essential robotic brain capabilities: Planning Capability, which involves decomposing complex manipulation instructions into manageable sub-tasks; Affordance Perception, the ability to recognize and interpret the affordances of interactive objects; and Trajectory Prediction, the foresight to anticipate the complete manipulation trajectory necessary for successful execution. To enhance the robotic brainâ€™s core capabilities from abstract to concrete, we introduce ShareRobot, a high-quality heterogeneous dataset that labels multi-dimensional information such as task planning, object affordance, and end-effector trajectory. ShareRobotâ€™s diversity and accuracy have been meticulously refined by three human annotators. Building on this dataset, we developed RoboBrain, an MLLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities. Extensive experiments demonstrate that RoboBrain achieves state-of-the-art performance across various robotic tasks, highlighting its potential to advance robotic brain capabilities. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›å±•åœ¨å„ç§å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœºå™¨äººåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æœŸæ“ä½œä»»åŠ¡ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„å±€é™æ€§ã€‚è¿™äº›å±€é™æ€§æºäºå½“å‰MLLMç¼ºä¹ä¸‰é¡¹é‡è¦çš„æœºå™¨äººæ ¸å¿ƒåŠŸèƒ½ï¼šè§„åˆ’èƒ½åŠ›ï¼Œæ¶‰åŠå°†å¤æ‚çš„æ“ä½œæŒ‡ä»¤åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼›è´Ÿæ‹…æ„ŸçŸ¥èƒ½åŠ›ï¼Œå³è¯†åˆ«å’Œè§£é‡Šäº¤äº’å¯¹è±¡çš„è´Ÿæ‹…çš„èƒ½åŠ›ï¼›ä»¥åŠè½¨è¿¹é¢„æµ‹èƒ½åŠ›ï¼Œå³å¯¹æˆåŠŸæ‰§è¡Œæ‰€éœ€å®Œæ•´æ“ä½œè½¨è¿¹çš„é¢„è§æ€§ã€‚ä¸ºäº†ä»æŠ½è±¡åˆ°å…·ä½“æå‡æœºå™¨äººçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ShareRobotï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€å¼‚è´¨æ€§çš„æ•°æ®é›†ï¼Œå®ƒæ ‡æ³¨äº†ä»»åŠ¡è§„åˆ’ã€å¯¹è±¡è´Ÿæ‹…å’Œæœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹ç­‰å¤šç»´ä¿¡æ¯ã€‚ShareRobotçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§å·²ç»ç”±ä¸‰åäººå·¥æ³¨é‡Šè€…ä»”ç»†å®¡æ ¸å’Œæ”¹è¿›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ„å»ºäº†RoboBrainï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMLLMçš„æ¨¡å‹ï¼Œç»“åˆäº†æœºå™¨äººå’Œä¸€èˆ¬å¤šæ¨¡æ€æ•°æ®ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶èå…¥é•¿è§†é¢‘å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä»¥æé«˜å…¶æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRoboBrainåœ¨å„ç§æœºå™¨äººä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œçªæ˜¾äº†å…¶åœ¨æå‡æœºå™¨äººæ ¸å¿ƒèƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21257v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨å¤šç§æ¨¡æ€åœºæ™¯ä¸­çš„æœ€æ–°è¿›å±•å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨æœºå™¨äººåœºæ™¯ã€ç‰¹åˆ«æ˜¯é•¿æœŸæ“ä½œä»»åŠ¡ä¸­çš„åº”ç”¨å­˜åœ¨æ˜¾è‘—å±€é™ã€‚ä¸ºæå‡æœºå™¨äººæ ¸å¿ƒä»æŠ½è±¡åˆ°å…·ä½“çš„èƒ½åŠ›ï¼Œå¼•å…¥äº†ShareRobotæ•°æ®é›†å’ŒRoboBrainæ¨¡å‹ã€‚ShareRobotæ•°æ®é›†æ ‡æ³¨äº†å¤šç»´ä¿¡æ¯å¦‚ä»»åŠ¡è§„åˆ’ã€ç‰©ä½“å¯ç”¨æ€§å’Œæœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹ç­‰ã€‚RoboBrainç»“åˆäº†æœºå™¨äººå’Œé€šç”¨å¤šæ¨¡æ€æ•°æ®ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶èå…¥é•¿è§†é¢‘å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œå®ç°å¤šé¡¹æœºå™¨äººä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šç§æ¨¡æ€åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨æœºå™¨äººé•¿æœŸæ“ä½œä»»åŠ¡ä¸­çš„åº”ç”¨å­˜åœ¨å±€é™ã€‚</li>
<li>æœºå™¨äººåœºæ™¯ä¸­çš„MLLMsç¼ºä¹è§„åˆ’èƒ½åŠ›ã€å¯ç”¨æ€§æ„ŸçŸ¥èƒ½åŠ›å’Œè½¨è¿¹é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>ShareRobotæ•°æ®é›†æ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€å¼‚æ„çš„æ•°æ®é›†ï¼Œç”¨äºæ ‡æ³¨ä»»åŠ¡è§„åˆ’ã€ç‰©ä½“å¯ç”¨æ€§å’Œæœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹ç­‰å¤šç»´ä¿¡æ¯ã€‚</li>
<li>ShareRobotæ•°æ®é›†çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ç»è¿‡ä¸‰ä½äººç±»æ ‡æ³¨è€…çš„ç²¾å¿ƒä¼˜åŒ–ã€‚</li>
<li>RoboBrainæ˜¯åŸºäºMLLMçš„æ¨¡å‹ï¼Œç»“åˆäº†æœºå™¨äººå’Œé€šç”¨å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>RoboBrainé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶èå…¥é•¿è§†é¢‘å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä»¥æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-453ee24e51a81cfc9c461aa10a6713f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecadd0383c9c4b68a17f77bd5adbf510.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63ebf6849bc35eba6f7229ea82ca84c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a84e2227a91f706ff8cf545495dc042.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32625e8462c13e84ad47a77321438384.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semantic-Volume-Quantifying-and-Detecting-both-External-and-Internal-Uncertainty-in-LLMs"><a href="#Semantic-Volume-Quantifying-and-Detecting-both-External-and-Internal-Uncertainty-in-LLMs" class="headerlink" title="Semantic Volume: Quantifying and Detecting both External and Internal   Uncertainty in LLMs"></a>Semantic Volume: Quantifying and Detecting both External and Internal   Uncertainty in LLMs</h2><p><strong>Authors:Xiaomin Li, Zhou Yu, Ziji Zhang, Yingying Zhuang, Swair Shah, Anurag Beniwal</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable performance across diverse tasks by encoding vast amounts of factual knowledge. However, they are still prone to hallucinations, generating incorrect or misleading information, often accompanied by high uncertainty. Existing methods for hallucination detection primarily focus on quantifying internal uncertainty, which arises from missing or conflicting knowledge within the model. However, hallucinations can also stem from external uncertainty, where ambiguous user queries lead to multiple possible interpretations. In this work, we introduce Semantic Volume, a novel mathematical measure for quantifying both external and internal uncertainty in LLMs. Our approach perturbs queries and responses, embeds them in a semantic space, and computes the determinant of the Gram matrix of the embedding vectors, capturing their dispersion as a measure of uncertainty. Our framework provides a generalizable and unsupervised uncertainty detection method without requiring white-box access to LLMs. We conduct extensive experiments on both external and internal uncertainty detection, demonstrating that our Semantic Volume method consistently outperforms existing baselines in both tasks. Additionally, we provide theoretical insights linking our measure to differential entropy, unifying and extending previous sampling-based uncertainty measures such as the semantic entropy. Semantic Volume is shown to be a robust and interpretable approach to improving the reliability of LLMs by systematically detecting uncertainty in both user queries and model responses. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç¼–ç å¤§é‡äº‹å®çŸ¥è¯†ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œç”Ÿæˆé”™è¯¯æˆ–è¯¯å¯¼æ€§çš„ä¿¡æ¯ï¼Œå¹¶ä¸”é€šå¸¸ä¼´éšç€è¾ƒé«˜çš„ä¸ç¡®å®šæ€§ã€‚ç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ä¸»è¦é›†ä¸­äºé‡åŒ–å†…éƒ¨ä¸ç¡®å®šæ€§ï¼Œè¿™ç§ä¸ç¡®å®šæ€§æºäºæ¨¡å‹ä¸­çš„çŸ¥è¯†ç¼ºå¤±æˆ–å†²çªã€‚ç„¶è€Œï¼Œå¹»è§‰ä¹Ÿå¯èƒ½æºäºå¤–éƒ¨ä¸ç¡®å®šæ€§ï¼Œå…¶ä¸­æ¨¡ç³Šçš„ç”¨æˆ·æŸ¥è¯¢å¯¼è‡´å¤šç§å¯èƒ½çš„è§£é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰ä½“ç§¯ï¼ˆSemantic Volumeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ•°å­¦åº¦é‡æ–¹æ³•ï¼Œå¯ä»¥é‡åŒ–LLMä¸­çš„å¤–éƒ¨å’Œå†…éƒ¨ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ‰°åŠ¨æŸ¥è¯¢å’Œå“åº”ï¼Œå°†å®ƒä»¬åµŒå…¥åˆ°è¯­ä¹‰ç©ºé—´ä¸­ï¼Œå¹¶è®¡ç®—åµŒå…¥å‘é‡çš„GramçŸ©é˜µçš„è¡Œåˆ—å¼ï¼Œæ•æ‰å…¶åˆ†æ•£ç¨‹åº¦ä½œä¸ºä¸ç¡®å®šæ€§çš„åº¦é‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†ä¸€ç§é€šç”¨ä¸”æ— éœ€ç›‘ç£çš„ä¸ç¡®å®šæ€§æ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€è®¿é—®LLMçš„ç™½ç›’ã€‚æˆ‘ä»¬å¯¹å¤–éƒ¨å’Œå†…éƒ¨ä¸ç¡®å®šæ€§æ£€æµ‹è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„è¯­ä¹‰ä½“ç§¯æ³•åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šæ·±å…¥æ¢è®¨äº†æˆ‘ä»¬çš„åº¦é‡æ–¹æ³•ä¸å¾®åˆ†ç†µä¹‹é—´çš„è”ç³»ï¼Œç»Ÿä¸€å¹¶æ‰©å±•äº†ä¹‹å‰çš„åŸºäºé‡‡æ ·çš„ä¸ç¡®å®šæ€§åº¦é‡æ–¹æ³•ï¼Œå¦‚è¯­ä¹‰ç†µã€‚è¯­ä¹‰ä½“ç§¯è¢«è¯æ˜æ˜¯ä¸€ç§å¯é ä¸”å¯è§£é‡Šçš„æ–¹æ³•ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ£€æµ‹ç”¨æˆ·æŸ¥è¯¢å’Œæ¨¡å‹å“åº”ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¯ä»¥æé«˜LLMçš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21239v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶èƒ½ç¼–ç å¤§é‡äº‹å®çŸ¥è¯†å¹¶åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä»æ˜“äº§ç”Ÿå¹»è§‰ï¼Œç”Ÿæˆé”™è¯¯æˆ–è¯¯å¯¼ä¿¡æ¯ï¼Œä¸”å¸¸ä¼´éšé«˜ä¸ç¡®å®šæ€§ã€‚ç°æœ‰å¹»è§‰æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨é‡åŒ–æ¨¡å‹å†…éƒ¨çš„ä¸ç¡®å®šæ€§ï¼Œä½†å¹»è§‰ä¹Ÿå¯èƒ½æºäºå¤–éƒ¨ä¸ç¡®å®šæ€§ï¼Œå¦‚ç”¨æˆ·æŸ¥è¯¢çš„æ¨¡ç³Šæ€§å¯¼è‡´å¤šé‡å¯èƒ½è§£é‡Šã€‚æœ¬ç ”ç©¶å¼•å…¥è¯­ä¹‰ä½“ç§¯è¿™ä¸€æ–°å‹æ•°å­¦åº¦é‡ï¼Œå¯é‡åŒ–LLMçš„å†…å¤–ä¸ç¡®å®šæ€§ã€‚é€šè¿‡æ‰°åŠ¨æŸ¥è¯¢å’Œå“åº”ã€åµŒå…¥è¯­ä¹‰ç©ºé—´ï¼Œå¹¶è®¡ç®—åµŒå…¥å‘é‡çš„æ ¼æ‹‰å§†çŸ©é˜µè¡Œåˆ—å¼ï¼Œæ•æ‰å…¶åˆ†æ•£ç¨‹åº¦ä½œä¸ºä¸ç¡®å®šæ€§çš„åº¦é‡ã€‚è¯¥æ–¹æ³•æä¾›é€šç”¨ã€æ— éœ€ç›‘ç£çš„ä¸ç¡®å®šæ€§æ£€æµ‹æ‰‹æ®µï¼Œæ— éœ€è®¿é—®LLMçš„ç™½ç›’ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å†…å¤–ä¸ç¡®å®šæ€§æ£€æµ‹æ–¹é¢ï¼Œè¯­ä¹‰ä½“ç§¯æ–¹æ³•å‡è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸å¾®åˆ†ç†µçš„ç†è®ºè”ç³»ä¹Ÿè¢«æ­ç¤ºï¼Œç»Ÿä¸€å¹¶æ‰©å±•äº†å…ˆå‰çš„é‡‡æ ·ä¸ç¡®å®šæ€§åº¦é‡ã€‚è¯­ä¹‰ä½“ç§¯æ˜¯ä¸€ç§å¯é ã€å¯è§£é‡Šçš„æ–¹æ³•ï¼Œèƒ½ç³»ç»Ÿåœ°æ£€æµ‹ç”¨æˆ·æŸ¥è¯¢å’Œæ¨¡å‹å“åº”ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œæé«˜LLMçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè™½ç„¶èƒ½å®Œæˆå¤šç§ä»»åŠ¡ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆé”™è¯¯æˆ–è¯¯å¯¼ä¿¡æ¯çš„é—®é¢˜ï¼Œå³â€œå¹»è§‰â€ï¼Œä¸”ä¼´éšé«˜ä¸ç¡®å®šæ€§ã€‚</li>
<li>ç°æœ‰å¹»è§‰æ£€æµ‹ä¸»è¦å…³æ³¨æ¨¡å‹å†…éƒ¨çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>å¹»è§‰ä¹Ÿå¯æºäºå¤–éƒ¨ä¸ç¡®å®šæ€§ï¼Œå¦‚ç”¨æˆ·æŸ¥è¯¢çš„æ¨¡ç³Šæ€§å¯¼è‡´çš„å¤šé‡è§£é‡Šã€‚</li>
<li>å¼•å…¥è¯­ä¹‰ä½“ç§¯è¿™ä¸€æ–°å‹æ•°å­¦åº¦é‡ï¼Œå¯é‡åŒ–LLMçš„å†…å¤–ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯­ä¹‰ä½“ç§¯æ–¹æ³•é€šè¿‡åµŒå…¥è¯­ä¹‰ç©ºé—´ã€è®¡ç®—æ ¼æ‹‰å§†çŸ©é˜µè¡Œåˆ—å¼æ¥æ•æ‰ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯­ä¹‰ä½“ç§¯æ–¹æ³•æä¾›é€šç”¨ã€æ— éœ€ç›‘ç£çš„ä¸ç¡®å®šæ€§æ£€æµ‹æ‰‹æ®µï¼Œå®éªŒè¯æ˜å…¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d14ae64a22b5444e0ca2120b532ca392.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a39ec34f9fe6f996ae25b5b39d233f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45aab17308e9278f0aa2bc55e11ab70d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Transforming-Tuberculosis-Care-Optimizing-Large-Language-Models-For-Enhanced-Clinician-Patient-Communication"><a href="#Transforming-Tuberculosis-Care-Optimizing-Large-Language-Models-For-Enhanced-Clinician-Patient-Communication" class="headerlink" title="Transforming Tuberculosis Care: Optimizing Large Language Models For   Enhanced Clinician-Patient Communication"></a>Transforming Tuberculosis Care: Optimizing Large Language Models For   Enhanced Clinician-Patient Communication</h2><p><strong>Authors:Daniil Filienko, Mahek Nizar, Javier Roberti, Denise Galdamez, Haroon Jakher, Sarah Iribarren, Weichao Yuwen, Martine De Cock</strong></p>
<p>Tuberculosis (TB) is the leading cause of death from an infectious disease globally, with the highest burden in low- and middle-income countries. In these regions, limited healthcare access and high patient-to-provider ratios impede effective patient support, communication, and treatment completion. To bridge this gap, we propose integrating a specialized Large Language Model into an efficacious digital adherence technology to augment interactive communication with treatment supporters. This AI-powered approach, operating within a human-in-the-loop framework, aims to enhance patient engagement and improve TB treatment outcomes. </p>
<blockquote>
<p>ç»“æ ¸ç—…ï¼ˆTBï¼‰æ˜¯å…¨çƒä¼ æŸ“ç—…æ­»äº¡çš„ä¸»è¦åŸå› ï¼Œä½æ”¶å…¥å’Œä¸­æ”¶å…¥å›½å®¶è´Ÿæ‹…æœ€é‡ã€‚åœ¨è¿™äº›åœ°åŒºï¼Œæœ‰é™çš„åŒ»ç–—æŠ¤ç†èµ„æºå’Œé«˜æ‚£è€…ä¸åŒ»æŠ¤äººå‘˜çš„æ¯”ä¾‹é˜»ç¢äº†æœ‰æ•ˆçš„æ‚£è€…æ”¯æŒã€æ²Ÿé€šå’Œæ²»ç–—å®Œæˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æè®®å°†ä¸“ä¸šåŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°æœ‰æ•ˆçš„æ•°å­—ä¾ä»æ€§æŠ€æœ¯ä¸­ï¼Œä»¥å¢å¼ºä¸æ²»ç–—æ”¯æŒè€…ä¹‹é—´çš„äº¤äº’é€šä¿¡ã€‚è¿™ä¸€äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–¹æ³•ï¼Œåœ¨äººç±»å¾ªç¯çš„æ¡†æ¶å†…è¿è¡Œï¼Œæ—¨åœ¨æé«˜æ‚£è€…çš„å‚ä¸åº¦å¹¶æ”¹å–„ç»“æ ¸ç—…çš„æ²»ç–—ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21236v1">PDF</a> GenAI4Health at AAAI-25</p>
<p><strong>Summary</strong><br>å…¨çƒèŒƒå›´å†…ï¼Œç»“æ ¸ç—…æ˜¯ä¼ æŸ“æ€§è‡´æ­»çš„ä¸»è¦åŸå› ï¼Œä½æ”¶å…¥å’Œä¸­æ”¶å…¥å›½å®¶è´Ÿæ‹…æœ€é‡ã€‚é’ˆå¯¹è¿™äº›åœ°åŒºåŒ»ç–—èµ„æºæœ‰é™ã€æ‚£è€…ä¸æœåŠ¡æä¾›è€…æ¯”ä¾‹å¤±è¡¡çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†ä¸“ä¸šçš„å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°æœ‰æ•ˆçš„æ•°å­—ä¾ä»æ€§æŠ€æœ¯ä¸­ï¼Œå¢å¼ºä¸æ²»ç–—æ”¯æŒè€…ä¹‹é—´çš„äº¤äº’é€šä¿¡ã€‚è¿™ä¸€äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–¹æ³•æ—¨åœ¨æå‡æ‚£è€…å‚ä¸åº¦å¹¶æ”¹å–„ç»“æ ¸ç—…æ²»ç–—ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒç»“æ ¸ç—…è‡´æ­»ç‡å±…é«˜ä¸ä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ”¶å…¥å’Œä¸­ç­‰æ”¶å…¥å›½å®¶ã€‚</li>
<li>è¿™äº›å›½å®¶é¢ä¸´åŒ»ç–—èµ„æºæœ‰é™å’Œæ‚£è€…ä¸æœåŠ¡æä¾›è€…æ¯”ä¾‹å¤±è¡¡çš„æŒ‘æˆ˜ã€‚</li>
<li>å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆçš„æ•°å­—ä¾ä»æ€§æŠ€æœ¯èƒ½å¤Ÿå¢å¼ºä¸æ²»ç–—æ”¯æŒè€…çš„äº¤äº’é€šä¿¡ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨è¿™ä¸€é¢†åŸŸçš„åº”ç”¨æ—¨åœ¨æé«˜æ‚£è€…çš„å‚ä¸åº¦å’Œæ²»ç–—æ•ˆæœã€‚</li>
<li>è¿™ç§AIæ–¹æ³•æ˜¯ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¾ªç¯æ¡†æ¶çš„ä¸€éƒ¨åˆ†ã€‚</li>
<li>é€šè¿‡è¿™ç§æ•´åˆæ–¹æ³•ï¼Œæœ‰æœ›æå‡ç»“æ ¸ç—…æ²»ç–—çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f0cc95df50fe61018410c2510556e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4174a584982b2f4123a81025d54805da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9cec9047126b21a8751dc3be21544bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a35fdc9996e4c749969c84ae73edc043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a4470e78dc759542e1ef5a85e57aaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73cb546cf85b5bf0fd20f2cd51cfa155.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ByteScale-Efficient-Scaling-of-LLM-Training-with-a-2048K-Context-Length-on-More-Than-12-000-GPUs"><a href="#ByteScale-Efficient-Scaling-of-LLM-Training-with-a-2048K-Context-Length-on-More-Than-12-000-GPUs" class="headerlink" title="ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length   on More Than 12,000 GPUs"></a>ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length   on More Than 12,000 GPUs</h2><p><strong>Authors:Hao Ge, Junda Feng, Qi Huang, Fangcheng Fu, Xiaonan Nie, Lei Zuo, Haibin Lin, Bin Cui, Xin Liu</strong></p>
<p>Scaling long-context ability is essential for Large Language Models (LLMs). To amortize the memory consumption across multiple devices in long-context training, inter-data partitioning (a.k.a. Data Parallelism) and intra-data partitioning (a.k.a. Context Parallelism) are commonly used. Current training frameworks predominantly treat the two techniques as orthogonal, and establish static communication groups to organize the devices as a static mesh (e.g., a 2D mesh). However, the sequences for LLM training typically vary in lengths, no matter for texts, multi-modalities or reinforcement learning. The mismatch between data heterogeneity and static mesh causes redundant communication and imbalanced computation, degrading the training efficiency.   In this work, we introduce ByteScale, an efficient, flexible, and scalable LLM training framework for large-scale mixed training of long and short sequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid Data Parallelism (HDP), which unifies the inter- and intra-data partitioning with a dynamic mesh design. In particular, we build a communication optimizer, which eliminates the redundant communication for short sequences by data-aware sharding and dynamic communication, and further compresses the communication cost for long sequences by selective offloading. Besides, we also develop a balance scheduler to mitigate the imbalanced computation by parallelism-aware data assignment. We evaluate ByteScale with the model sizes ranging from 7B to 141B, context lengths from 256K to 2048K, on a production cluster with more than 12,000 GPUs. Experiment results show that ByteScale outperforms the state-of-the-art training system by up to 7.89x. </p>
<blockquote>
<p>æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿æ–‡ä¸Šä¸‹æ–‡èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¸ºäº†åœ¨é•¿æ–‡æœ¬è®­ç»ƒçš„å¤šä¸ªè®¾å¤‡ä¹‹é—´è¿›è¡Œå†…å­˜æ¶ˆè€—è¡¥å¿ï¼Œé€šå¸¸ä½¿ç”¨æ•°æ®é—´åˆ†å‰²ï¼ˆä¹Ÿç§°ä¸ºæ•°æ®å¹¶è¡Œæ€§ï¼‰å’Œæ•°æ®å†…åˆ†å‰²ï¼ˆä¹Ÿç§°ä¸ºä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼‰ã€‚å½“å‰çš„è®­ç»ƒæ¡†æ¶ä¸»è¦å°†è¿™ä¸¤ç§æŠ€æœ¯è§†ä¸ºæ­£äº¤çš„ï¼Œå¹¶å»ºç«‹é™æ€é€šä¿¡ç»„æ¥ç»„ç»‡è®¾å¤‡å½¢æˆé™æ€ç½‘æ ¼ï¼ˆä¾‹å¦‚äºŒç»´ç½‘æ ¼ï¼‰ã€‚ç„¶è€Œï¼Œæ— è®ºæ–‡æœ¬ã€å¤šæ¨¡å¼è¿˜æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ŒLLMè®­ç»ƒåºåˆ—çš„é•¿åº¦é€šå¸¸éƒ½ä¼šæœ‰æ‰€å˜åŒ–ã€‚æ•°æ®å¼‚è´¨æ€§ä¸é™æ€ç½‘æ ¼ä¹‹é—´çš„ä¸åŒ¹é…å¯¼è‡´äº†å†—ä½™é€šä¿¡å’Œè®¡ç®—ä¸å¹³è¡¡ï¼Œé™ä½äº†è®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21231v1">PDF</a> 12 pages, 21 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡å¤„ç†è‡³å…³é‡è¦ã€‚ä¸ºäº†é™ä½è·¨è®¾å¤‡å†…å­˜æ¶ˆè€—ï¼Œé€šå¸¸é‡‡ç”¨æ•°æ®å¹¶è¡Œï¼ˆinter-data partitioningï¼‰å’Œä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆintra-data partitioningï¼‰ã€‚å½“å‰è®­ç»ƒæ¡†æ¶ä¸»è¦å°†ä¸¤è€…è§†ä¸ºæ­£äº¤æŠ€æœ¯å¹¶ä»¥é™æ€ç½‘æ ¼ç»„ç»‡è®¾å¤‡ï¼Œå¦‚äºŒç»´ç½‘æ ¼ã€‚ç„¶è€Œï¼ŒLLMè®­ç»ƒåºåˆ—é•¿åº¦å„å¼‚ï¼Œå¯¼è‡´æ•°æ®å¼‚è´¨æ€§ä¸é™æ€ç½‘æ ¼ä¸åŒ¹é…ï¼Œå¼•å‘å†—ä½™é€šä¿¡å’Œè®¡ç®—ä¸å¹³è¡¡é—®é¢˜ï¼Œå½±å“è®­ç»ƒæ•ˆç‡ã€‚æœ¬ç ”ç©¶æå‡ºByteScaleè®­ç»ƒæ¡†æ¶ï¼Œèåˆæ•°æ®å¹¶è¡Œå’Œä¸Šä¸‹æ–‡å¹¶è¡Œç­–ç•¥çš„æ··åˆæ•°æ®å¹¶è¡Œï¼ˆHDPï¼‰ä¸ºæ ¸å¿ƒã€‚é€šè¿‡é€šä¿¡ä¼˜åŒ–å™¨æ¶ˆé™¤çŸ­åºåˆ—çš„å†—ä½™é€šä¿¡å¹¶å®ç°é€‰æ‹©æ€§å¸è½½å‡å°‘é•¿åºåˆ—é€šä¿¡æˆæœ¬ï¼Œå¹³è¡¡è°ƒåº¦å™¨è§£å†³è®¡ç®—ä¸å¹³è¡¡é—®é¢˜ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒByteScaleåœ¨æ¨¡å‹è§„æ¨¡ä»7Båˆ°141Bã€ä¸Šä¸‹æ–‡é•¿åº¦ä»256Kåˆ°2048Kçš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºç°æœ‰è®­ç»ƒç³»ç»Ÿæ€§èƒ½æå‡æœ€é«˜è¾¾7.89å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡å¤„ç†åœ¨LLMè®­ç»ƒä¸­è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è®­ç»ƒæ¡†æ¶ä»¥é™æ€ç½‘æ ¼ç»„ç»‡è®¾å¤‡ï¼Œå¯¼è‡´æ•°æ®å¼‚è´¨æ€§ä¸é™æ€ç½‘æ ¼ä¸åŒ¹é…ã€‚</li>
<li>ByteScaleæ¡†æ¶å¼•å…¥æ··åˆæ•°æ®å¹¶è¡Œï¼ˆHDPï¼‰ç­–ç•¥ï¼Œèåˆæ•°æ®å¹¶è¡Œå’Œä¸Šä¸‹æ–‡å¹¶è¡Œã€‚</li>
<li>é€šä¿¡ä¼˜åŒ–å™¨æ¶ˆé™¤çŸ­åºåˆ—å†—ä½™é€šä¿¡ï¼Œé€‰æ‹©æ€§å¸è½½å‡å°‘é•¿åºåˆ—é€šä¿¡æˆæœ¬ã€‚</li>
<li>å¹³è¡¡è°ƒåº¦å™¨è§£å†³è®¡ç®—ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ByteScaleåœ¨æ¨¡å‹è§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦å¹¿æ³›çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85fa987b456de7d3107f6cfe61ce8299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b831b827cbc263256b8b8dc929e7ad46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67d09d5494608f319f4db6f4636f4a4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3419e67c269fa7576fcfcba4c2a6c912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334f98275a6414c905cadaa321300702.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ffb4f37d7dc551b0acce32e1b27acf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bc71b7aef1067269cd0d0dc6581fa3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2a44b6a14ebf1fc086939e235fb9b5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ECLeKTic-a-Novel-Challenge-Set-for-Evaluation-of-Cross-Lingual-Knowledge-Transfer"><a href="#ECLeKTic-a-Novel-Challenge-Set-for-Evaluation-of-Cross-Lingual-Knowledge-Transfer" class="headerlink" title="ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual   Knowledge Transfer"></a>ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual   Knowledge Transfer</h2><p><strong>Authors:Omer Goldman, Uri Shaham, Dan Malkin, Sivan Eiger, Avinatan Hassidim, Yossi Matias, Joshua Maynez, Adi Mayrav Gilady, Jason Riesa, Shruti Rijhwani, Laura Rimell, Idan Szpektor, Reut Tsarfaty, Matan Eyal</strong></p>
<p>To achieve equitable performance across languages, multilingual large language models (LLMs) must be able to abstract knowledge beyond the language in which it was acquired. However, the current literature lacks reliable ways to measure LLMsâ€™ capability of cross-lingual knowledge transfer. To that end, we present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We detected information with uneven coverage across languages by controlling for presence and absence of Wikipedia articles in 12 languages. We generated knowledge-seeking questions in a source language, for which the answer appears in a relevant Wikipedia article and translated them to all other 11 languages, for which the respective Wikipedias lack equivalent articles. Assuming that Wikipedia reflects the prominent knowledge in the LLMâ€™s training data, to solve ECLeKTicâ€™s CBQA task the model is required to transfer knowledge between languages. Experimenting with 8 LLMs, we show that SOTA models struggle to effectively share knowledge across, languages even if they can predict the answer well for queries in the same language the knowledge was acquired in. </p>
<blockquote>
<p>ä¸ºäº†å®ç°ä¸åŒè¯­è¨€çš„å…¬å¹³æ€§èƒ½ï¼Œå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¿…é¡»èƒ½å¤ŸæŠ½è±¡è·å–è¯­è¨€ä¹‹å¤–çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®ç¼ºä¹å¯é çš„æ–¹æ³•æ¥è¡¡é‡LLMè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ECLeKTicï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€å°é—­é—®ç­”ï¼ˆCBQAï¼‰æ•°æ®é›†ï¼Œä»¥ç®€å•ã€é»‘ç®±çš„æ–¹å¼è¯„ä¼°è·¨è¯­è¨€çŸ¥è¯†çš„è½¬ç§»ã€‚æˆ‘ä»¬é€šè¿‡æ§åˆ¶12ç§è¯­è¨€çš„ç»´åŸºç™¾ç§‘æ–‡ç« çš„æœ‰æ— ï¼Œæ£€æµ‹äº†è¯­è¨€é—´ä¿¡æ¯è¦†ç›–çš„ä¸å‡åŒ€æ€§ã€‚æˆ‘ä»¬åœ¨æºè¯­è¨€ä¸­ç”Ÿæˆå¯»æ±‚çŸ¥è¯†çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜çš„ç­”æ¡ˆå‡ºç°åœ¨ç›¸å…³çš„ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­ï¼Œç„¶åå°†å…¶ç¿»è¯‘åˆ°å…¶ä»–æ‰€æœ‰11ç§è¯­è¨€ï¼Œè¿™äº›è¯­è¨€çš„ç»´åŸºç™¾ç§‘éƒ½æ²¡æœ‰ç›¸åº”çš„æ–‡ç« ã€‚å‡è®¾ç»´åŸºç™¾ç§‘åæ˜ äº†LLMè®­ç»ƒæ•°æ®ä¸­çš„çªå‡ºçŸ¥è¯†ï¼Œè¦è§£å†³ECLeKTicçš„CBQAä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦åœ¨è¯­è¨€é—´è½¬ç§»çŸ¥è¯†ã€‚é€šè¿‡å¯¹8ä¸ªLLMè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿å¯¹äºåœ¨è·å–çŸ¥è¯†çš„åŒä¸€è¯­è¨€ä¸­çš„æŸ¥è¯¢èƒ½å¤Ÿå¾ˆå¥½åœ°é¢„æµ‹ç­”æ¡ˆï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è·¨è¯­è¨€å…±äº«çŸ¥è¯†æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21228v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå®ç°è·¨è¯­è¨€æ€§èƒ½å…¬å¹³ï¼Œå¿…é¡»èƒ½å¤ŸæŠ½å–è¶…è¶Šè·å–è¯­è¨€çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå½“å‰æ–‡çŒ®ç¼ºä¹è¡¡é‡LLMè·¨è¯­è¨€çŸ¥è¯†è¿ç§»èƒ½åŠ›çš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ECLeKTicï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­ç§å°é—­é—®ç­”ï¼ˆCBQAï¼‰æ•°æ®é›†ï¼Œä»¥ç®€å•ã€é»‘ç®±çš„æ–¹å¼è¯„ä¼°è·¨è¯­è¨€çŸ¥è¯†è¿ç§»ã€‚æˆ‘ä»¬é€šè¿‡æ§åˆ¶12ç§è¯­è¨€çš„ç»´åŸºç™¾ç§‘æ–‡ç« çš„æœ‰æ— ï¼Œå‘ç°äº†è¯­è¨€é—´ä¿¡æ¯è¦†ç›–ä¸å‡çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨æºè¯­è¨€ç”Ÿæˆå¯»æ±‚çŸ¥è¯†çš„é—®ç­”ï¼Œå…¶ç­”æ¡ˆå‡ºç°åœ¨ç›¸å…³ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­ï¼Œå¹¶å°†å…¶ç¿»è¯‘è‡³å…¶ä»–11ç§è¯­è¨€ï¼Œè¿™äº›è¯­è¨€çš„ç»´åŸºç™¾ç§‘ç¼ºä¹ç›¸åº”æ–‡ç« ã€‚ä¸ºè§£å†³ECLeKTicçš„CBQAä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦åœ¨è¯­è¨€é—´è½¬ç§»çŸ¥è¯†ã€‚é€šè¿‡å¯¹8ç§LLMçš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿å®ƒä»¬èƒ½åœ¨åŒä¸€è¯­è¨€çš„æŸ¥è¯¢ä¸­è·å¾—è‰¯å¥½çš„é¢„æµ‹ç»“æœï¼Œä½†åœ¨è·¨è¯­è¨€çš„çŸ¥è¯†å…±äº«æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­ç§å¤§å‹è¯­è¨€æ¨¡å‹éœ€å…·å¤‡è·¨è¯­è¨€çŸ¥è¯†è¿ç§»èƒ½åŠ›ä»¥å®ç°æ€§èƒ½å…¬å¹³ã€‚</li>
<li>å½“å‰ç¼ºä¹è¡¡é‡LLMè·¨è¯­è¨€çŸ¥è¯†è¿ç§»èƒ½åŠ›çš„æ–¹æ³•ã€‚</li>
<li>ECLeKTicæ•°æ®é›†ç”¨äºè¯„ä¼°å¤šè¯­ç§LLMçš„è·¨è¯­è¨€çŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚</li>
<li>è¯­è¨€é—´çš„ä¿¡æ¯è¦†ç›–å­˜åœ¨ä¸å‡è¡¡ç°è±¡ã€‚</li>
<li>LLMåœ¨è·¨è¯­è¨€çŸ¥è¯†å…±äº«æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå³ä½¿åœ¨åŒä¸€è¯­è¨€çš„æŸ¥è¯¢ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>å°é—­é—®ç­”ï¼ˆCBQAï¼‰ä»»åŠ¡æ˜¯è¯„ä¼°LLMè·¨è¯­è¨€èƒ½åŠ›çš„é‡è¦æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7fef17c858d08e97a0eb40311ce1885c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8fd04be3270256fe636720ea0fae062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc819cc8565e76808159d5cbd05fcf09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76518c26e33f3dc0b0527b001749fa3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd7bfbfce8508a172797f8f691c3762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bdbe0d6627a32d0f7372675b707af60.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Transformers-Learn-to-Implement-Multi-step-Gradient-Descent-with-Chain-of-Thought"><a href="#Transformers-Learn-to-Implement-Multi-step-Gradient-Descent-with-Chain-of-Thought" class="headerlink" title="Transformers Learn to Implement Multi-step Gradient Descent with Chain   of Thought"></a>Transformers Learn to Implement Multi-step Gradient Descent with Chain   of Thought</h2><p><strong>Authors:Jianhao Huang, Zixuan Wang, Jason D. Lee</strong></p>
<p>Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on an in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. With our technique, we also show that looped transformers significantly improve final performance compared to transformers without looping in the in-context learning of linear regression. Empirically, we demonstrate that CoT prompting yields substantial performance improvements. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºè¢«è¯æ˜å¯ä»¥æ˜¾è‘—åœ°æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç®—æœ¯å’Œæ¨ç†ä»»åŠ¡ä¸­ï¼Œé€šè¿‡æŒ‡å¯¼æ¨¡å‹äº§ç”Ÿä¸­é—´æ¨ç†æ­¥éª¤ã€‚å°½ç®¡CoTçš„æ˜¾è‘—å®è¯æˆåŠŸåŠå…¶åœ¨å¢å¼ºè¡¨ç°åŠ›æ–¹é¢çš„ç†è®ºä¼˜åŠ¿ï¼Œä½†CoTè®­ç»ƒçš„åŸºç¡€æœºåˆ¶ä»ç„¶å¤§éƒ¨åˆ†æœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ä¸Šä¸‹æ–‡æƒé‡é¢„æµ‹ä»»åŠ¡çš„çº¿æ€§å›å½’ä¸Šï¼Œå˜æ¢å™¨å¯¹CoTç›®æ ‡çš„è®­ç»ƒåŠ¨æ€ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè™½ç„¶ä¸€å±‚çº¿æ€§å˜æ¢å™¨åœ¨æ²¡æœ‰CoTçš„æƒ…å†µä¸‹åªèƒ½å®ç°ä¸€æ­¥æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ï¼Œå¹¶ä¸”æ— æ³•æ¢å¤çœŸå®çš„æƒé‡å‘é‡ï¼Œä½†å¸¦æœ‰CoTæç¤ºçš„å˜æ¢å™¨å¯ä»¥å­¦ä¹ æ‰§è¡Œå¤šæ­¥GDçš„è‡ªå›å½’ï¼Œå®ç°è¿‘ä¹ç²¾ç¡®çš„å›æ”¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ç»è¿‡è®­ç»ƒçš„å˜æ¢å™¨å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ä¸æ²¡æœ‰å¾ªç¯çš„å˜æ¢å™¨ç›¸æ¯”ï¼Œå¾ªç¯å˜æ¢å™¨åœ¨çº¿æ€§å›å½’çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ˜¾è‘—æé«˜äº†æœ€ç»ˆæ€§èƒ½ã€‚ä»å®è¯æ¥çœ‹ï¼Œæˆ‘ä»¬è¯æ˜äº†CoTæç¤ºäº§ç”Ÿäº†å®è´¨æ€§çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21212v1">PDF</a> ICLR 2025 Spotlight</p>
<p><strong>Summary</strong><br>     é“¾å¼æ€ç»´ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æç¤ºå¯æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç®—æœ¯å’Œæ¨ç†ä»»åŠ¡ä¸­ã€‚é€šè¿‡æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼ŒCoTä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¹Ÿåœ¨ç†è®ºä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚æœ¬æ–‡ç ”ç©¶å˜å‹å™¨åœ¨é“¾å¼æ€ç»´ç›®æ ‡ä¸‹çš„è®­ç»ƒåŠ¨æ€ï¼Œé’ˆå¯¹çº¿æ€§å›å½’è¿›è¡Œä¸Šä¸‹æ–‡æƒé‡é¢„æµ‹ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œæ— é“¾å¼æ€ç»´æç¤ºçš„å•å±‚çº¿æ€§å˜å‹å™¨ä»…èƒ½å®ç°æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰çš„å•æ­¥æ“ä½œï¼Œæ— æ³•æ¢å¤çœŸå®æƒé‡å‘é‡ï¼›è€Œå¸¦æœ‰CoTæç¤ºçš„å˜å‹å™¨å¯ä»¥å­¦ä¹ æ‰§è¡Œå¤šæ­¥GDçš„è‡ªåŠ¨å›å½’ï¼Œå®ç°è¿‘ä¼¼ç²¾ç¡®æ¢å¤ï¼Œå¹¶åœ¨æœªè§æ•°æ®ä¸Šå®ç°æœ‰æ•ˆæ³›åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨å¾ªç¯å˜å‹å™¨æŠ€æœ¯ï¼Œä¸æ— å¾ªç¯çš„å˜å‹å™¨ç›¸æ¯”ï¼Œåœ¨çº¿æ€§å›å½’çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å¯æ˜¾è‘—æé«˜æœ€ç»ˆæ€§èƒ½ã€‚ç»éªŒè¯ï¼ŒCoTæç¤ºå¯å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºèƒ½æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æœ¯å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>CoTé€šè¿‡æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>åœ¨çº¿æ€§å›å½’çš„ä¸Šä¸‹æ–‡æƒé‡é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œå¸¦æœ‰CoTæç¤ºçš„å˜å‹å™¨å¯ä»¥å­¦ä¹ æ‰§è¡Œå¤šæ­¥æ¢¯åº¦ä¸‹é™çš„è‡ªåŠ¨å›å½’ã€‚</li>
<li>CoTæç¤ºå¸®åŠ©å˜å‹å™¨å®ç°è¿‘ä¼¼ç²¾ç¡®æ¢å¤çœŸå®æƒé‡å‘é‡ï¼Œå¹¶åœ¨æœªè§æ•°æ®ä¸Šå®ç°æœ‰æ•ˆæ³›åŒ–ã€‚</li>
<li>é‡‡ç”¨å¾ªç¯å˜å‹å™¨æŠ€æœ¯å¯è¿›ä¸€æ­¥æé«˜åœ¨çº¿æ€§å›å½’çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœ€ç»ˆæ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜äº†CoTæç¤ºå¸¦æ¥çš„æ€§èƒ½æ”¹è¿›æ˜¯æ˜¾è‘—çš„ã€‚</li>
<li>CoTåœ¨ç†è®ºä¸Šçš„ä¼˜åŠ¿å’Œåœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-272ed9d447783baeb7af2b599730539e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Chronologically-Consistent-Large-Language-Models"><a href="#Chronologically-Consistent-Large-Language-Models" class="headerlink" title="Chronologically Consistent Large Language Models"></a>Chronologically Consistent Large Language Models</h2><p><strong>Authors:Songrun He, Linying Lv, Asaf Manela, Jimmy Wu</strong></p>
<p>Large language models are increasingly used in social sciences, but their training data can introduce lookahead bias and training leakage. A good chronologically consistent language model requires efficient use of training data to maintain accuracy despite time-restricted data. Here, we overcome this challenge by training chronologically consistent large language models timestamped with the availability date of their training data, yet accurate enough that their performance is comparable to state-of-the-art open-weight models. Lookahead bias is model and application-specific because even if a chronologically consistent language model has poorer language comprehension, a regression or prediction model applied on top of the language model can compensate. In an asset pricing application, we compare the performance of news-based portfolio strategies that rely on chronologically consistent versus biased language models and estimate a modest lookahead bias. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å…¶è®­ç»ƒæ•°æ®å¯èƒ½ä¼šå¼•å…¥å‰ç»åè§å’Œè®­ç»ƒæ³„éœ²ã€‚ä¸€ä¸ªè‰¯å¥½çš„æ—¶é—´è¿è´¯æ€§è¯­è¨€æ¨¡å‹éœ€è¦æœ‰æ•ˆåˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œå³ä½¿åœ¨æ—¶é—´å—é™çš„æ•°æ®ä¸‹ä¹Ÿèƒ½ä¿æŒå‡†ç¡®æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒå¸¦æœ‰è®­ç»ƒæ•°æ®å¯ç”¨æ—¥æœŸçš„æ—¶é—´è¿è´¯çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹çš„å‡†ç¡®æ€§è¶³å¤Ÿé«˜ï¼Œå…¶æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„å¼€æ”¾æƒé‡æ¨¡å‹ç›¸åª²ç¾ã€‚å‰ç»åè§æ˜¯æ¨¡å‹å’Œåº”ç”¨ç¨‹åºç‰¹å®šçš„ï¼Œå› ä¸ºå³ä½¿æ—¶é—´è¿è´¯çš„è¯­è¨€æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›è¾ƒå·®ï¼Œåº”ç”¨äºè¯­è¨€æ¨¡å‹ä¹‹ä¸Šçš„å›å½’æˆ–é¢„æµ‹æ¨¡å‹ä¹Ÿå¯ä»¥è¿›è¡Œè¡¥å¿ã€‚åœ¨èµ„äº§å®šä»·åº”ç”¨ç¨‹åºä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ–°é—»çš„ç»„åˆç­–ç•¥çš„æ€§èƒ½ï¼Œè¿™äº›ç­–ç•¥ä¾èµ–äºæ—¶é—´è¿è´¯ä¸æœ‰åè§çš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¼°è®¡äº†ä¸€ä¸ªé€‚åº¦çš„å‰ç»åè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21206v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†å…¶è®­ç»ƒæ•°æ®å¯èƒ½å¼•å…¥å‰ç»åè§å’Œè®­ç»ƒæ³„éœ²é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨æ—¶é—´æˆ³è®­ç»ƒæ•°æ®è®­ç»ƒæ—¶åºä¸€è‡´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸æœ€æ–°å…¬å¼€æƒé‡æ¨¡å‹ç›¸å½“ã€‚å‰ç»åè§æ˜¯æ¨¡å‹å’Œåº”ç”¨ç‰¹å®šçš„ï¼Œå› ä¸ºå³ä½¿æ—¶åºä¸€è‡´çš„è¯­è¨€æ¨¡å‹è¯­è¨€ç†è§£èƒ½åŠ›è¾ƒå·®ï¼Œåœ¨è¯­è¨€æ¨¡å‹ä¹‹ä¸Šåº”ç”¨çš„å›å½’æˆ–é¢„æµ‹æ¨¡å‹ä¹Ÿå¯ä»¥è¿›è¡Œè¡¥å¿ã€‚åœ¨èµ„äº§å®šä»·åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ—¶åºä¸€è‡´è¯­è¨€æ¨¡å‹å’Œå¸¦æœ‰åè§è¯­è¨€æ¨¡å‹çš„æ–°é—»ç»„åˆç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶ä¼°è®¡å‡ºè½»å¾®çš„å‰ç»åè§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†å­˜åœ¨è®­ç»ƒæ•°æ®å¼•å…¥çš„å‰ç»åè§å’Œè®­ç»ƒæ³„éœ²é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æ—¶é—´æˆ³è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥è®­ç»ƒå‡ºæ—¶åºä¸€è‡´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸æœ€æ–°å…¬å¼€æƒé‡æ¨¡å‹ç›¸å½“ã€‚</li>
<li>å‰ç»åè§æ˜¯æ¨¡å‹å’Œåº”ç”¨ç‰¹å®šçš„ï¼Œå¯ä»¥é€šè¿‡åœ¨åº”ç”¨å±‚è¿›è¡Œè¡¥å¿æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨èµ„äº§å®šä»·åº”ç”¨ä¸­ï¼ŒåŸºäºæ—¶åºä¸€è‡´è¯­è¨€æ¨¡å‹çš„æ–°é—»ç»„åˆç­–ç•¥æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>ç›¸æ¯”å¸¦æœ‰åè§çš„è¯­è¨€æ¨¡å‹ï¼Œæ—¶åºä¸€è‡´çš„æ¨¡å‹å…·æœ‰è½»å¾®çš„å‰ç»åè§ã€‚</li>
<li>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶éœ€è¦æ³¨æ„ä½¿ç”¨æ•°æ®çš„æ•ˆç‡ï¼Œä»¥ç»´æŒå‡†ç¡®æ€§å¹¶é¿å…æ—¶é—´é™åˆ¶æ•°æ®çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1228dd455f18273f01ae7db4332fda11.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Disentangling-Feature-Structure-A-Mathematically-Provable-Two-Stage-Training-Dynamics-in-Transformers"><a href="#Disentangling-Feature-Structure-A-Mathematically-Provable-Two-Stage-Training-Dynamics-in-Transformers" class="headerlink" title="Disentangling Feature Structure: A Mathematically Provable Two-Stage   Training Dynamics in Transformers"></a>Disentangling Feature Structure: A Mathematically Provable Two-Stage   Training Dynamics in Transformers</h2><p><strong>Authors:Zixuan Gong, Jiaye Teng, Yong Liu</strong></p>
<p>Transformers may exhibit two-stage training dynamics during the real-world training process. For instance, when training GPT-2 on the Counterfact dataset, the answers progress from syntactically incorrect to syntactically correct to semantically correct. However, existing theoretical analyses hardly account for this two-stage phenomenon. In this paper, we theoretically demonstrate how such two-stage training dynamics occur in transformers. Specifically, we analyze the dynamics of transformers using feature learning techniques under in-context learning regimes, based on a disentangled two-type feature structure. Such disentanglement of feature structure is general in practice, e.g., natural languages contain syntax and semantics, and proteins contain primary and secondary structures. To our best known, this is the first rigorous result regarding a two-stage optimization process in transformers. Additionally, a corollary indicates that such a two-stage process is closely related to the spectral properties of the attention weights, which accords well with empirical findings. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œä¸­çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒTransformerå¯èƒ½ä¼šå±•ç°å‡ºä¸¤é˜¶æ®µçš„è®­ç»ƒåŠ¨æ€ã€‚ä¾‹å¦‚ï¼Œåœ¨Counterfactæ•°æ®é›†ä¸Šè®­ç»ƒGPT-2æ—¶ï¼Œç­”æ¡ˆçš„è¿›å±•ä»è¯­æ³•ä¸æ­£ç¡®åˆ°è¯­æ³•æ­£ç¡®å†åˆ°è¯­ä¹‰æ­£ç¡®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç†è®ºåˆ†æå¾ˆå°‘æ¶‰åŠè¿™ç§ä¸¤é˜¶æ®µç°è±¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†Transformerä¸­è¿™ç§ä¸¤é˜¶æ®µè®­ç»ƒåŠ¨æ€æ˜¯å¦‚ä½•å‘ç”Ÿçš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ç‰¹å¾å­¦ä¹ æŠ€æœ¯åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ä¸‹åˆ†æTransformerçš„åŠ¨æ€å˜åŒ–ï¼ŒåŸºäºè§£è€¦çš„ä¸¤ç±»ç‰¹å¾ç»“æ„ã€‚åœ¨å®è·µä¸­ï¼Œç‰¹å¾ç»“æ„çš„è§£è€¦æ˜¯æ™®éçš„ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€åŒ…å«è¯­æ³•å’Œè¯­ä¹‰ï¼Œè›‹ç™½è´¨åŒ…å«ä¸€çº§ç»“æ„å’ŒäºŒçº§ç»“æ„ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯å…³äºTransformerä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹çš„é¦–ä¸ªä¸¥æ ¼ç»“æœã€‚æ­¤å¤–ï¼Œæ¨è®ºè¡¨æ˜è¿™ç§ä¸¤é˜¶æ®µè¿‡ç¨‹ä¸æ³¨æ„åŠ›æƒé‡çš„è°±å±æ€§å¯†åˆ‡ç›¸å…³ï¼Œè¿™ä¸ç»éªŒå‘ç°ç›¸å»åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20681v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åœ¨çœŸå®ä¸–ç•Œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒTransformerå¯èƒ½ä¼šè¡¨ç°å‡ºä¸¤é˜¶æ®µè®­ç»ƒåŠ¨æ€ã€‚æœ¬è®ºæ–‡ä»¥GPT-2åœ¨Counterfactæ•°æ®é›†ä¸Šçš„è®­ç»ƒä¸ºä¾‹ï¼Œå±•ç¤ºäº†è¿™ç§ä»è¯­æ³•ä¸æ­£ç¡®åˆ°è¯­æ³•æ­£ç¡®å†åˆ°è¯­ä¹‰æ­£ç¡®çš„ç­”æ¡ˆè¿›æ­¥è¿‡ç¨‹ã€‚æœ¬æ–‡é€šè¿‡ç‰¹å¾å­¦ä¹ æŠ€æœ¯å’Œä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶å¯¹è¿™ç§ç°è±¡è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶åŸºäºä¸¤ç§ç±»å‹ç‰¹å¾çš„è§£è€¦ç»“æ„è¿›è¡Œäº†åˆ†æã€‚æ­¤ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ä¸è‡ªç„¶è¯­è¨€ä¸­çš„è¯­æ³•å’Œè¯­ä¹‰ã€è›‹ç™½è´¨ä¸­çš„ä¸€çº§å’ŒäºŒçº§ç»“æ„çš„è§£è€¦ç›¸å¯¹åº”ã€‚æœ¬æ–‡çš„ç»“æœå…³äºTransformerçš„ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹æ˜¯å·²çŸ¥çš„é¦–ä¸ªä¸¥è°¨ç»“æœï¼Œä¸”ä¸æ³¨æ„åŠ›æƒé‡çš„è°±å±æ€§å¯†åˆ‡ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformeråœ¨çœŸå®ä¸–ç•Œè®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½è¡¨ç°å‡ºä¸¤é˜¶æ®µè®­ç»ƒåŠ¨æ€ã€‚</li>
<li>GPT-2åœ¨Counterfactæ•°æ®é›†ä¸Šçš„è®­ç»ƒä¸ºä¾‹ï¼Œå±•ç¤ºäº†ç­”æ¡ˆä»è¯­æ³•ä¸æ­£ç¡®åˆ°è¯­æ³•æ­£ç¡®å†åˆ°è¯­ä¹‰æ­£ç¡®çš„è¿›æ­¥è¿‡ç¨‹ã€‚</li>
<li>æœ¬æ–‡ä½¿ç”¨ç‰¹å¾å­¦ä¹ æŠ€æœ¯å’Œä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶å¯¹è¿™ç§ç°è±¡è¿›è¡Œäº†ç†è®ºåˆ†æã€‚</li>
<li>Transformerçš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹åŸºäºä¸¤ç§ç±»å‹ç‰¹å¾çš„è§£è€¦ç»“æ„ã€‚</li>
<li>è¿™ç§è§£è€¦ä¸è‡ªç„¶è¯­è¨€ä¸­çš„è¯­æ³•å’Œè¯­ä¹‰ã€è›‹ç™½è´¨ä¸­çš„ä¸€çº§å’ŒäºŒçº§ç»“æ„çš„è§£è€¦ç›¸å¯¹åº”ã€‚</li>
<li>æœ¬è®ºæ–‡çš„ç»“æœå…³äºTransformerçš„ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹æ˜¯å·²çŸ¥çš„é¦–ä¸ªä¸¥è°¨ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99f56536f429b00b1c5bfec41b22d982.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1f392c407b6776cc39f1a0a0c0aa8a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93a4f5007c5487969ee1228db8a79d76.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RTGen-Real-Time-Generative-Detection-Transformer"><a href="#RTGen-Real-Time-Generative-Detection-Transformer" class="headerlink" title="RTGen: Real-Time Generative Detection Transformer"></a>RTGen: Real-Time Generative Detection Transformer</h2><p><strong>Authors:Chi Ruan</strong></p>
<p>While open-vocabulary object detectors require predefined categories during inference, generative object detectors overcome this limitation by endowing the model with text generation capabilities. However, existing generative object detection methods directly append an autoregressive language model to an object detector to generate texts for each detected object. This straightforward design leads to structural redundancy and increased processing time. In this paper, we propose a Real-Time GENerative Detection Transformer (RTGen), a real-time generative object detector with a succinct encoder-decoder architecture. Specifically, we introduce a novel Region-Language Decoder (RL-Decoder), which innovatively integrates a non-autoregressive language model into the detection decoder, enabling concurrent processing of object and text information. With these efficient designs, RTGen achieves a remarkable inference speed of 60.41 FPS. Moreover, RTGen obtains 18.6 mAP on the LVIS dataset, outperforming the previous SOTA method by 3.5 mAP. </p>
<blockquote>
<p>è™½ç„¶å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦é¢„å…ˆå®šä¹‰ç±»åˆ«ï¼Œä½†ç”Ÿæˆå¼å¯¹è±¡æ£€æµ‹å™¨é€šè¿‡èµ‹äºˆæ¨¡å‹æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›æ¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼å¯¹è±¡æ£€æµ‹æ–¹æ³•ç›´æ¥å°†è‡ªå›å½’è¯­è¨€æ¨¡å‹é™„åŠ åˆ°å¯¹è±¡æ£€æµ‹å™¨ä¸Šï¼Œä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„å¯¹è±¡ç”Ÿæˆæ–‡æœ¬ã€‚è¿™ç§ç›´æ¥çš„è®¾è®¡å¯¼è‡´äº†ç»“æ„å†—ä½™å’Œå¢åŠ çš„å¤„ç†æ—¶é—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®æ—¶ç”Ÿæˆæ£€æµ‹è½¬æ¢å™¨ï¼ˆRTGenï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç®€æ´ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å®æ—¶ç”Ÿæˆå¼å¯¹è±¡æ£€æµ‹å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŒºåŸŸ-è¯­è¨€è§£ç å™¨ï¼ˆRL-Decoderï¼‰ï¼Œå®ƒåˆ›æ–°åœ°å°†éè‡ªå›å½’è¯­è¨€æ¨¡å‹é›†æˆåˆ°æ£€æµ‹è§£ç å™¨ä¸­ï¼Œå®ç°å¯¹å¯¹è±¡å’Œæ–‡æœ¬ä¿¡æ¯çš„å¹¶è¡Œå¤„ç†ã€‚å‡­å€Ÿè¿™äº›é«˜æ•ˆçš„è®¾è®¡ï¼ŒRTGenå®ç°äº†60.41 FPSçš„æƒŠäººæ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒRTGenåœ¨LVISæ•°æ®é›†ä¸Šè·å¾—äº†18.6 mAPï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º3.5 mAPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20622v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å®æ—¶ç”Ÿæˆæ£€æµ‹å˜å‹å™¨ï¼ˆRTGenï¼‰çš„å®æ—¶ç”Ÿæˆç›®æ ‡æ£€æµ‹å™¨ï¼Œé‡‡ç”¨ç®€æ´çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚é€šè¿‡å¼•å…¥æ–°çš„Region-Language Decoderï¼ˆRL-Decoderï¼‰ï¼Œå°†éè‡ªå›å½’è¯­è¨€æ¨¡å‹åˆ›æ–°åœ°é›†æˆåˆ°æ£€æµ‹è§£ç å™¨ä¸­ï¼Œå®ç°ç›®æ ‡å’Œæ–‡æœ¬ä¿¡æ¯çš„å¹¶è¡Œå¤„ç†ã€‚RTGenå…·æœ‰å‡ºè‰²çš„æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°æ¯ç§’60.41å¸§ï¼Œä¸”åœ¨LVISæ•°æ®é›†ä¸Šè·å¾—è¾ƒé«˜çš„å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆç›®æ ‡æ£€æµ‹å™¨å…‹æœäº†å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹å™¨éœ€è¦åœ¨æ¨ç†æœŸé—´è¿›è¡Œé¢„å®šä¹‰ç±»åˆ«çš„é™åˆ¶ã€‚</li>
<li>ç°æœ‰ç”Ÿæˆç›®æ ‡æ£€æµ‹æ–¹æ³•ç›´æ¥åœ¨ç›®æ ‡æ£€æµ‹å™¨ä¸Šé™„åŠ è‡ªå›å½’è¯­è¨€æ¨¡å‹æ¥ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„å¯¹è±¡ç”Ÿæˆæ–‡æœ¬ï¼Œå¯¼è‡´ç»“æ„å†—ä½™å’Œå¢åŠ å¤„ç†æ—¶é—´ã€‚</li>
<li>RTGené‡‡ç”¨ç®€æ´çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå®ç°å®æ—¶ç”Ÿæˆç›®æ ‡æ£€æµ‹ã€‚</li>
<li>RTGenå¼•å…¥äº†æ–°çš„Region-Language Decoderï¼ˆRL-Decoderï¼‰ï¼Œå°†éè‡ªå›å½’è¯­è¨€æ¨¡å‹é›†æˆåˆ°æ£€æµ‹è§£ç å™¨ä¸­ã€‚</li>
<li>RL-Decoderä½¿å¯¹è±¡å’Œæ–‡æœ¬ä¿¡æ¯èƒ½å¤Ÿå¹¶è¡Œå¤„ç†ã€‚</li>
<li>RTGenå…·æœ‰å‡ºè‰²çš„æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°æ¯ç§’60.41å¸§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a74035f3996578153bb6db2e493d007a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddffcfd5cb57ccf270e81ad0bbb4cae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1500e514bd345c5804b60872d00fa226.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98b812284488b58ce80d0b3be3c97b5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef20373eb35c45b9e510d720ce959658.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Visual-Reasoning-at-Urban-Intersections-FineTuning-GPT-4o-for-Traffic-Conflict-Detection"><a href="#Visual-Reasoning-at-Urban-Intersections-FineTuning-GPT-4o-for-Traffic-Conflict-Detection" class="headerlink" title="Visual Reasoning at Urban Intersections: FineTuning GPT-4o for Traffic   Conflict Detection"></a>Visual Reasoning at Urban Intersections: FineTuning GPT-4o for Traffic   Conflict Detection</h2><p><strong>Authors:Sari Masri, Huthaifa I. Ashqar, Mohammed Elhenawy</strong></p>
<p>Traffic control in unsignalized urban intersections presents significant challenges due to the complexity, frequent conflicts, and blind spots. This study explores the capability of leveraging Multimodal Large Language Models (MLLMs), such as GPT-4o, to provide logical and visual reasoning by directly using birds-eye-view videos of four-legged intersections. In this proposed method, GPT-4o acts as intelligent system to detect conflicts and provide explanations and recommendations for the drivers. The fine-tuned model achieved an accuracy of 77.14%, while the manual evaluation of the true predicted values of the fine-tuned GPT-4o showed significant achievements of 89.9% accuracy for model-generated explanations and 92.3% for the recommended next actions. These results highlight the feasibility of using MLLMs for real-time traffic management using videos as inputs, offering scalable and actionable insights into intersections traffic management and operation. Code used in this study is available at <a target="_blank" rel="noopener" href="https://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git">https://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git</a>. </p>
<blockquote>
<p>åœ¨æ— ä¿¡å·çš„åŸå¸‚äº¤å‰å£çš„äº¤é€šæ§åˆ¶å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼Œå› ä¸ºå…¶å¤æ‚æ€§ã€é¢‘ç¹çš„å†²çªå’Œç›²ç‚¹ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„èƒ½åŠ›ï¼Œå¦‚GPT-4oï¼Œé€šè¿‡ç›´æ¥ä½¿ç”¨é¸Ÿç°å›¾è§†é¢‘è¿›è¡Œé€»è¾‘å’Œè§†è§‰æ¨ç†ã€‚åœ¨æå‡ºçš„æ–¹æ³•ä¸­ï¼ŒGPT-4oå……å½“æ™ºèƒ½ç³»ç»Ÿï¼Œæ£€æµ‹å†²çªå¹¶ä¸ºé©¾é©¶å‘˜æä¾›è§£é‡Šå’Œå»ºè®®ã€‚ç»è¿‡å¾®è°ƒçš„æ¨¡å‹è¾¾åˆ°äº†77.14%çš„å‡†ç¡®ç‡ï¼Œè€Œå¯¹å¾®è°ƒåçš„GPT-4oçš„çœŸå®é¢„æµ‹å€¼è¿›è¡Œæ‰‹åŠ¨è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹ç”Ÿæˆçš„è§£é‡Šå‡†ç¡®ç‡ä¸º89.9%ï¼Œæ¨èè¡ŒåŠ¨çš„å‡†ç¡®ç‡ä¸º92.3%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä½¿ç”¨MLLMsè¿›è¡Œå®æ—¶äº¤é€šç®¡ç†çš„å¯è¡Œæ€§ï¼Œä»¥è§†é¢‘ä¸ºè¾“å…¥ï¼Œä¸ºäº¤å‰å£çš„äº¤é€šç®¡ç†å’Œè¿è¥æä¾›å¯ä¼¸ç¼©å’Œå¯æ“ä½œçš„è§è§£ã€‚æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oæ¥è§£å†³æ— ä¿¡å·åŸå¸‚äº¤å‰å£çš„äº¤é€šæ§åˆ¶é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡ç›´æ¥ä½¿ç”¨é¸Ÿç°å›¾è§†é¢‘ï¼Œå¯¹äº¤å‰è·¯å£çš„äº¤é€šæƒ…å†µè¿›è¡Œé€»è¾‘å’Œè§†è§‰æ¨ç†ï¼Œæ£€æµ‹å†²çªå¹¶ä¸ºé©¾é©¶å‘˜æä¾›è§£é‡Šå’Œå»ºè®®ã€‚ç»è¿‡å¾®è°ƒï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°77.14%ã€‚æ­¤å¤–ï¼Œå¯¹GPT-4oç”Ÿæˆçš„è§£é‡Šå’Œæ¨èè¡ŒåŠ¨çš„å‡†ç¡®æ€§è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶å‡†ç¡®ç‡ä¸º89.9%å’Œ92.3%ã€‚è¿™è¡¨æ˜ä½¿ç”¨MLLMsé€šè¿‡è§†é¢‘è¿›è¡Œå®æ—¶äº¤é€šç®¡ç†çš„å¯è¡Œæ€§ï¼Œä¸ºäº¤å‰å£äº¤é€šç®¡ç†å’Œè¿è¥æä¾›å¯ä¼¸ç¼©å’Œå¯æ“ä½œçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oå¯ç”¨äºè§£å†³æ— ä¿¡å·åŸå¸‚äº¤å‰å£çš„äº¤é€šæ§åˆ¶æŒ‘æˆ˜ã€‚</li>
<li>GPT-4oèƒ½å¤Ÿç›´æ¥ä½¿ç”¨é¸Ÿç°å›¾è§†é¢‘è¿›è¡Œé€»è¾‘å’Œè§†è§‰æ¨ç†ã€‚</li>
<li>GPT-4oå¯ä»¥æ£€æµ‹äº¤é€šå†²çªå¹¶ä¸ºé©¾é©¶å‘˜æä¾›è§£é‡Šå’Œå»ºè®®ã€‚</li>
<li>ç»è¿‡å¾®è°ƒçš„æ¨¡å‹å®ç°è¾ƒé«˜å‡†ç¡®ç‡ï¼ˆ77.14%ï¼‰ã€‚</li>
<li>GPT-4oåœ¨ç”Ÿæˆè§£é‡Šå’Œæ¨èè¡ŒåŠ¨æ–¹é¢çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º89.9%å’Œ92.3%ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ä½¿ç”¨MLLMsè¿›è¡Œå®æ—¶äº¤é€šç®¡ç†çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e40aca14316b69c2c47171d1ec6a4628.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f2d780532aad0cf70bf9d94fc50e0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e2ebba892589638a7174aff2654abd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd096a7a09903cf3e2408b1e1e5d5f67.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level â€œnovelty.â€ Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. </p>
<blockquote>
<p>æ•°æ®çš„å¤šæ ·æ€§å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§æ„è¯†åˆ°çš„æ•°æ®é€‰æ‹©æ–¹æ³•æ¥æ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„é—®é¢˜å°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ï¼Œè¿™é™åˆ¶äº†æ•°æ®å·¥ç¨‹çš„æ˜ç¡®æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å¾®è°ƒå®éªŒè¯„ä¼°äº†ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ï¼Œç³»ç»Ÿåœ°åˆ†æäº†ç°æœ‰çš„1 1ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§åº¦é‡åº”é€‚å½“åœ°è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†NovelSumï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„æ–°å¤šæ ·æ€§åº¦é‡æ ‡å‡†ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumå‡†ç¡®åœ°æ•æ‰äº†å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.9 7ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬åº¦é‡çš„æœ‰æ•ˆæ€§å’Œå®é™…æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v4">PDF</a> 16 pages. The related codes and resources will be released later.   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong></p>
<p>æ•°æ®å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²æ¢ç´¢äº†å¤šç§æ„è¯†æ•°æ®é€‰æ‹©æ–¹æ³•æ¥æ„å»ºé«˜è´¨é‡æ•°æ®é›†å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ•°æ®å¤šæ ·æ€§çš„ç²¾ç¡®å®šä¹‰å’Œæµ‹é‡åŸºç¡€é—®é¢˜å°šæœªå¾—åˆ°è¶³å¤Ÿç ”ç©¶ï¼Œè¿™é™åˆ¶äº†æ•°æ®å·¥ç¨‹çš„æ˜ç¡®æŒ‡å¯¼ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°åˆ†æäº†ç°æœ‰çš„11ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œé€šè¿‡å¹¿æ³›çš„å¾®è°ƒå®éªŒè¯„ä¼°å…¶ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§æµ‹é‡åº”é€‚å½“åœ°è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬ç ”ç©¶æå‡ºäº†åŸºäºæ ·æœ¬çº§åˆ«â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumå‡†ç¡®æ•æ‰äº†å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚åˆ©ç”¨NovelSumä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´ªå©ªçš„ã€é¢å‘å¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®é™…æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¤šæ ·æ€§å¯¹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤šç§æ•°æ®é€‰æ‹©æ–¹æ³•æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ•°æ®å¤šæ ·æ€§çš„ç²¾ç¡®å®šä¹‰å’Œæµ‹é‡ä»ç„¶æ˜¯ä¸€ä¸ªåŸºç¡€é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶ç³»ç»Ÿåœ°åˆ†æäº†11ç§ç°æœ‰çš„å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ã€‚</li>
<li>å¯é çš„å¤šæ ·æ€§æµ‹é‡åº”ç»¼åˆè€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚å’Œæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†åŸºäºæ ·æœ¬çº§åˆ«â€œæ–°é¢–æ€§â€çš„æ–°å¤šæ ·æ€§æŒ‡æ ‡NovelSumã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44a696b50e219614ff2dc7c180d1e176.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54e13d1819d42fc8227602d59168d40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73708b42657d862618f4d0f17d41eb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e43cf27f6eac82e9bd09d3c6ce4f768.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation"><a href="#AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation" class="headerlink" title="AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation"></a>AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation</h2><p><strong>Authors:Rui Li, Xiaowei Zhao</strong></p>
<p>As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer">https://github.com/lironui/AeroReformer</a>. </p>
<blockquote>
<p>ä½œä¸ºä¸€é¡¹ç›®æ–°å‹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¼•ç”¨åˆ†å‰²ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæ ¹æ®æ–‡æœ¬æè¿°å®šä½å¹¶åˆ†å‰²å¯¹è±¡ã€‚è™½ç„¶å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰åœ¨è‡ªç„¶å›¾åƒä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¯¹èˆªç©ºå›¾åƒçš„å…³æ³¨è¾ƒå°‘ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªæ— äººæœºçš„å›¾åƒã€‚æ— äººæœºå›¾åƒå…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œå¯¹è±¡æ–¹å‘çš„å˜åŒ–ï¼Œä½¿å¾—ç°æœ‰çš„RISæ–¹æ³•æ•ˆæœä¸ä½³ã€‚ä¸€ä¸ªå…³é”®çš„é™åˆ¶æ˜¯ç¼ºä¹é’ˆå¯¹æ— äººæœºçš„ç‰¹å®šæ•°æ®é›†ï¼Œå› ä¸ºæ‰‹åŠ¨æ³¨é‡Šåƒç´ çº§è’™ç‰ˆå’Œç”Ÿæˆæ–‡æœ¬æè¿°æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œä¸”è€—æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨ç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ— äººæœºå¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰çš„æ–°å‹æ¡†æ¶â€”â€”AerialReferring Transformerï¼ˆAeroReformerï¼‰ï¼Œå®ƒå…·æœ‰è§†è§‰è¯­è¨€è·¨æ³¨æ„æ¨¡å—ï¼ˆVLCAMï¼‰ç”¨äºæœ‰æ•ˆçš„è·¨æ¨¡æ€ç†è§£ï¼Œä»¥åŠæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ï¼Œä»¥æé«˜èˆªç©ºåœºæ™¯ä¸­çš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸¤ä¸ªæ–°å¼€å‘çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAeroReformerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºUAV-RISå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/lironui/AeroReformerå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16680v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ— äººæœºå›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰ä»»åŠ¡ï¼Œå­˜åœ¨å¤æ‚ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œç‰©ä½“æ–¹å‘å˜åŒ–ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåº”å¯¹ï¼Œå› æ­¤è®¾è®¡è‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œå¹¶åˆ©ç”¨ç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚åŒæ—¶æå‡ºåä¸ºAerial Referring Transformerï¼ˆAeroReformerï¼‰çš„æ–°æ¡†æ¶ï¼ŒåŒ…å«è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰å’Œæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ï¼Œä»¥æé«˜æ— äººæœºåœºæ™¯ä¸­çš„åˆ†å‰²å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºUAV-RISä»»åŠ¡æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€åˆ†äº«äº[<a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer]%E3%80%82">https://github.com/lironui/AeroReformer]ã€‚</a> </p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ— äººæœºå›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œç‰©ä½“æ–¹å‘å˜åŒ–ç­‰ã€‚</li>
<li>ç›®å‰ç¼ºä¹é’ˆå¯¹æ— äººæœºçš„æ•°æ®é›†æ˜¯ä¸€å¤§é™åˆ¶ï¼Œå› æ­¤æ‰‹åŠ¨æ ‡æ³¨åƒç´ çº§æ©è†œå’Œç”Ÿæˆæ–‡æœ¬æè¿°è¾ƒä¸ºå›°éš¾ä¸”è€—æ—¶ã€‚</li>
<li>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨ç°æœ‰æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºåä¸ºAerial Referring Transformerï¼ˆAeroReformerï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºæ— äººæœºå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŒ…å«è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰å’Œæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆè§£ç å™¨ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAeroReformeråœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºUAV-RISä»»åŠ¡æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚ </li>
<li>æ•°æ®é›†å’Œä»£ç å°†åœ¨å…¬å¼€å¹³å°ä¸Šåˆ†äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3af0fd5647598b30f82615da85c7b669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b7d314935fb91f7fe70e77e43b75f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67069d33f329753b9e872c1c21852e95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1647f55d62f3f04f918b88a51f5b1d47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede21416761cb84b00dec2f94df09139.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3f7d09675111a9665bb7cb179655c66.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cache-Me-If-You-Must-Adaptive-Key-Value-Quantization-for-Large-Language-Models"><a href="#Cache-Me-If-You-Must-Adaptive-Key-Value-Quantization-for-Large-Language-Models" class="headerlink" title="Cache Me If You Must: Adaptive Key-Value Quantization for Large Language   Models"></a>Cache Me If You Must: Adaptive Key-Value Quantization for Large Language   Models</h2><p><strong>Authors:Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh</strong></p>
<p>Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key &amp; Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) high-compression mechanisms for internal network states. We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to â€œoptimallyâ€ compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œçš„æœ‰æ•ˆéƒ¨ç½²ä¾èµ–äºé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ¥å¤„ç†å¹¶ç”Ÿæˆé•¿è¾“å‡ºï¼Œå‡å°‘é‡å¤è®¡ç®—çš„éœ€æ±‚ã€‚å¯¹äºå¤§å‹ä¸Šä¸‹æ–‡ï¼Œé”®å€¼ç¼“å­˜ä¼šå ç”¨æ•°åGBçš„è®¾å¤‡å†…å­˜ï¼Œå› ä¸ºå®ƒä»¬å­˜å‚¨æ¯ä¸ªæ ‡è®°å’Œå±‚çš„å‘é‡è¡¨ç¤ºã€‚è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡é‡åŒ–ã€ä¿®å‰ªæˆ–åˆå¹¶æ¥å‹ç¼©ç¼“å­˜çš„å‘é‡ï¼Œä½†è¿™äº›æŠ€æœ¯å¾€å¾€ä¸ºäº†æ›´é«˜çš„å‹ç¼©ç‡è€Œç‰ºç‰²äº†è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡ä¸¤ä¸ªè§‚å¯Ÿç»“æœæ”¹è¿›é”®å’Œå€¼çš„å‹ç¼©ï¼š1ï¼‰ä¸åŒå±‚ä¹‹é—´é”®å’Œå€¼ä¹‹é—´çš„å›ºæœ‰ä¾èµ–æ€§ï¼›2ï¼‰å†…éƒ¨ç½‘ç»œçŠ¶æ€çš„é«˜å‹ç¼©æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†AQUA-KVï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é”®å€¼ç¼“å­˜çš„è‡ªé€‚åº”é‡åŒ–ï¼Œå®ƒä¾èµ–äºç´§å‡‘çš„é€‚é…å™¨æ¥åˆ©ç”¨é”®å’Œå€¼ä¹‹é—´çš„ç°æœ‰ä¾èµ–æ€§ï¼Œæ—¨åœ¨â€œæœ€ä¼˜â€åœ°å‹ç¼©é‚£äº›æ— æ³•é¢„æµ‹çš„ä¿¡æ¯ã€‚AQUA-KVæ˜¾è‘—æé«˜äº†å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹æœ€æ–°LLMå®¶æ—çš„é«˜å‡†ç¡®æ€§ã€‚åœ¨Llama 3.2 LLMä¸Šï¼Œæˆ‘ä»¬åœ¨å›°æƒ‘åº¦å’ŒLongBenchå¾—åˆ†ç›¸å¯¹è¯¯å·®ä½äº1%çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ¯å€¼2-2.5æ¯”ç‰¹çš„æ— æŸæ¨ç†ã€‚AQUA-KVæ˜¯ä¸€æ¬¡æ€§çš„ã€ç®€å•çš„å’Œé«˜æ•ˆçš„ï¼šå³ä½¿å¯¹äº70Bæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥åœ¨å•ä¸ªGPUä¸Š1-6å°æ—¶å†…è¿›è¡Œæ ¡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19392v4">PDF</a> Preprint, under review</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„æ•ˆç‡é—®é¢˜ï¼ŒæŒ‡å‡ºKey-Valueï¼ˆKVï¼‰ç¼“å­˜å¯¹äºå¤„ç†å’Œç”Ÿæˆé•¿è¾“å‡ºè‡³å…³é‡è¦ã€‚ä¸ºäº†å‡å°‘å†…å­˜å ç”¨å’Œæé«˜æ€§èƒ½ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡åŒ–æŠ€æœ¯â€”â€”AQUA-KVï¼Œç”¨äºå‹ç¼©KVç¼“å­˜ä¸­çš„é”®å’Œå€¼å‘é‡ã€‚AQUA-KVèƒ½å¤Ÿæ˜¾è‘—æé«˜å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—è€Œè¨€å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨Llama 3.2æ¨¡å‹ä¸Šï¼ŒAQUA-KVå®ç°äº†è¿‘æ— æŸæ¨ç†ï¼Œå…·æœ‰é«˜æ•ˆçš„æ€§èƒ½å’Œç®€å•çš„æ ¡å‡†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­ä¾èµ–Key-Valueï¼ˆKVï¼‰ç¼“å­˜å¤„ç†é•¿è¾“å‡ºï¼Œå‡å°‘é‡å¤è®¡ç®—ã€‚</li>
<li>KVç¼“å­˜å ç”¨å¤§é‡è®¾å¤‡å†…å­˜ï¼Œå› æ­¤éœ€è¦å‹ç¼©æŠ€æœ¯æ¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚</li>
<li>è‡ªé€‚åº”é‡åŒ–æŠ€æœ¯AQUA-KVè¢«æå‡ºç”¨äºå‹ç¼©KVç¼“å­˜ä¸­çš„é”®å’Œå€¼å‘é‡ã€‚</li>
<li>AQUA-KVé€šè¿‡åˆ©ç”¨é”®å’Œå€¼ä¹‹é—´çš„å†…åœ¨ä¾èµ–å…³ç³»ä»¥åŠå†…éƒ¨ç½‘ç»œçŠ¶æ€çš„é«˜å‹ç¼©æœºåˆ¶æ¥æ”¹è¿›å‹ç¼©æ•ˆæœã€‚</li>
<li>AQUA-KVåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å‹ç¼©ç‡ã€‚</li>
<li>åœ¨Llama 3.2æ¨¡å‹ä¸Šï¼ŒAQUA-KVå®ç°äº†è¿‘æ— æŸæ¨ç†ï¼Œå…·æœ‰é«˜æ•ˆçš„æ€§èƒ½å’Œç®€å•çš„æ ¡å‡†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c08338e8c632f93327b8af479e1c3bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf6eabf0877e8c4b1736f37b24667433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c933df7c70ce35ce904ab8e398a3125.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models"><a href="#TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models" class="headerlink" title="TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models"></a>TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models</h2><p><strong>Authors:Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba</strong></p>
<p>Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the studentâ€™s initial distribution towards the teacherâ€™s distribution. We provide a theoretical analysis demonstrating TAIDâ€™s ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAIDâ€™s superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAIDâ€™s practical impact by developing two state-of-the-art compact foundation models: $\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAIDâ€™s effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies. </p>
<blockquote>
<p>å› æœè¯­è¨€æ¨¡å‹å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„è§„æ¨¡å¯¹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥ä»å¤§å‹æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™ä¸ºæ¨¡å‹å‹ç¼©æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ä¸€ä¸ªä¸»è¦çš„å‰©ä½™é—®é¢˜åœ¨äºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„å·¨å¤§å·®å¼‚ï¼Œå³å·¨å¤§çš„èƒ½åŠ›å·®è·ã€æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒï¼Œå®ƒä»¬åœ¨è’¸é¦è¿‡ç¨‹ä¸­æ„æˆäº†éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œæ—¶é—´è‡ªé€‚åº”æ’å€¼è’¸é¦ï¼ˆTAIDï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”ä¸­é—´åˆ†å¸ƒåŠ¨æ€æ’å€¼å­¦ç”Ÿå’Œæ•™å¸ˆçš„åˆ†å¸ƒï¼Œä»å­¦ç”Ÿæœ€åˆçš„åˆ†å¸ƒé€æ¸è½¬å‘æ•™å¸ˆçš„åˆ†å¸ƒã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºåˆ†æï¼Œè¯æ˜äº†TAIDé˜²æ­¢æ¨¡å¼å´©æºƒçš„èƒ½åŠ›ï¼Œå¹¶å®è¯è¯æ˜äº†å®ƒåœ¨è§£å†³èƒ½åŠ›å·®è·ã€å¹³è¡¡æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒTAIDåœ¨å„ç§æ¨¡å‹å’Œæ¶æ„çš„å¤§å°ã€æŒ‡ä»¤è°ƒæ•´å’Œé¢„è®­ç»ƒåœºæ™¯ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å‘ä¸¤ä¸ªæœ€å…ˆè¿›çš„ç´§å‡‘åŸºç¡€æ¨¡å‹ï¼šç”¨äºè¯­è¨€ä»»åŠ¡çš„â€œTAID-LLM-1.5Bâ€å’Œç”¨äºè§†è§‰è¯­è¨€ä»»åŠ¡çš„â€œTAID-VLM-2Bâ€ï¼Œå±•ç¤ºäº†TAIDçš„å®é™…å½±å“ã€‚è¿™äº›ç»“æœè¯æ˜äº†TAIDåœ¨åˆ›å»ºé«˜æ€§èƒ½å’Œé«˜æ•ˆæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæ¨åŠ¨äº†æ›´å¯è®¿é—®çš„AIæŠ€æœ¯çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16937v4">PDF</a> To appear at the 13th International Conference on Learning   Representations (ICLR 2025) as a Spotlight presentation</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å› æœè¯­è¨€æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶è§„æ¨¡å¯¹èµ„æºå—é™ç¯å¢ƒçš„éƒ¨ç½²å¸¦æ¥æŒ‘æˆ˜ã€‚çŸ¥è¯†è’¸é¦æŠ€æœ¯å¯ä»¥ä»å¤§è§„æ¨¡æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œä¸ºè§£å†³æ¨¡å‹å‹ç¼©é—®é¢˜æä¾›äº†å¸Œæœ›ã€‚ç„¶è€Œï¼Œæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œå¦‚å®¹é‡å·®è·ã€æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒï¼Œä»ç„¶å­˜åœ¨é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ—¶é—´è‡ªé€‚åº”æ’å€¼è’¸é¦â€ï¼ˆTAIDï¼‰çš„æ–°çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”ä¸­é—´åˆ†å¸ƒåŠ¨æ€æ’å€¼å­¦ç”Ÿå’Œæ•™å¸ˆçš„åˆ†å¸ƒï¼Œä»å­¦ç”Ÿçš„åˆå§‹åˆ†å¸ƒé€æ¸è½¬å‘æ•™å¸ˆçš„åˆ†å¸ƒã€‚ç†è®ºåˆ†æå’Œå®éªŒè¡¨æ˜ï¼ŒTAIDèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å®¹é‡å·®è·é—®é¢˜ï¼ŒåŒæ—¶å¹³è¡¡æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒã€‚æ­¤å¤–ï¼ŒTAIDåœ¨å¤šç§æ¨¡å‹å’Œæ¶æ„ã€æŒ‡ä»¤è°ƒä¼˜å’Œé¢„è®­ç»ƒåœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸¤ç§å…ˆè¿›çš„ç´§å‡‘åŸºç¡€æ¨¡å‹TAID-LLM-1.5Bï¼ˆç”¨äºè¯­è¨€ä»»åŠ¡ï¼‰å’ŒTAID-VLM-2Bï¼ˆç”¨äºè§†è§‰è¯­è¨€ä»»åŠ¡ï¼‰ï¼Œå±•ç¤ºTAIDåœ¨åˆ›å»ºé«˜æ€§èƒ½ã€é«˜æ•ˆæ¨¡å‹æ–¹é¢çš„å®é™…æ•ˆæœï¼Œæ¨åŠ¨æ›´æ™®åŠçš„AIæŠ€æœ¯å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å› æœè¯­è¨€æ¨¡å‹è™½è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ˜¯å‹ç¼©æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼ˆå¦‚å®¹é‡å·®è·ã€æ¨¡å¼å¹³å‡å’Œæ¨¡å¼å´©æºƒï¼‰ä»ç„¶å­˜åœ¨é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°çš„çŸ¥è¯†è’¸é¦æ–¹æ³•â€”â€”æ—¶é—´è‡ªé€‚åº”æ’å€¼è’¸é¦ï¼ˆTAIDï¼‰ï¼Œé€šè¿‡åŠ¨æ€æ’å€¼å­¦ç”Ÿå’Œæ•™å¸ˆçš„åˆ†å¸ƒæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TAIDèƒ½å¤Ÿé¢„é˜²æ¨¡å¼å´©æºƒï¼Œå¹¶æœ‰æ•ˆè§£å†³å®¹é‡å·®è·é—®é¢˜ã€‚</li>
<li>TAIDåœ¨å¤šç§æ¨¡å‹å’Œæ¶æ„ã€æŒ‡ä»¤è°ƒä¼˜å’Œé¢„è®­ç»ƒåœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>TAIDåœ¨åˆ›å»ºé«˜æ€§èƒ½ã€é«˜æ•ˆæ¨¡å‹æ–¹é¢å…·å¤‡å®é™…æ•ˆæœï¼Œæ¨åŠ¨æ›´æ™®åŠçš„AIæŠ€æœ¯å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ec63585dbeacc2d4bead44f51a5f022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d19b6b370cd647f9e7364ec50c82a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65bc89ef8169783c6875b2aeb18c91a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Predict-the-Outcome-of-Judicial-Decisions"><a href="#Can-Large-Language-Models-Predict-the-Outcome-of-Judicial-Decisions" class="headerlink" title="Can Large Language Models Predict the Outcome of Judicial Decisions?"></a>Can Large Language Models Predict the Outcome of Judicial Decisions?</h2><p><strong>Authors:Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Amani Al-Ghraibah</strong></p>
<p>Large Language Models (LLMs) have shown exceptional capabilities in Natural Language Processing (NLP) across diverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource languages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a comprehensive evaluation framework that integrates both quantitative metrics (such as BLEU, ROUGE, and BERT) and qualitative assessments (including Coherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the impact of fine-tuning the model on a diverse set of instructions, offering valuable insights into the development of a more human-centric and adaptable LLM. We have made the dataset, code, and models publicly available to provide a solid foundation for future research in Arabic legal NLP. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¦‚åœ¨é˜¿æ‹‰ä¼¯è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸Šè¿›è¡Œæ³•å¾‹åˆ¤å†³é¢„æµ‹ï¼ˆLJPï¼‰ï¼Œä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å‘ä¸€ä¸ªé˜¿æ‹‰ä¼¯è¯­LJPæ•°æ®é›†æ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œè¯¥æ•°æ®é›†æ˜¯ä»æ²™ç‰¹å•†ä¸šæ³•é™¢çš„åˆ¤å†³ä¸­æ”¶é›†å’Œé¢„å¤„ç†çš„ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„å¼€æºLLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬LLaMA-3.2-3Bå’ŒLLaMA-3.1-8Bï¼Œåœ¨ä¸åŒçš„é…ç½®ä¸‹ï¼Œå¦‚é›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œé‡‡ç”¨LoRAçš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å®šé‡æŒ‡æ ‡ï¼ˆå¦‚BLEUã€ROUGEå’ŒBERTï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆåŒ…æ‹¬è¿è´¯æ€§ã€æ³•å¾‹è¯­è¨€ã€æ¸…æ™°åº¦ç­‰ï¼‰ï¼Œä½¿ç”¨LLMè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šä¸‹æ–‡ä¸­ï¼Œç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹å¯ä»¥è¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ˜¾è‘—çš„èµ„æºæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†åœ¨å¤šç§æŒ‡ä»¤é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å½±å“ï¼Œä¸ºå¼€å‘æ›´ä»¥äººç±»ä¸ºä¸­å¿ƒå’Œé€‚åº”æ€§æ›´å¼ºçš„LLMæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬å·²ç»å…¬å¼€æä¾›äº†æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹ï¼Œä¸ºæœªæ¥åœ¨é˜¿æ‹‰ä¼¯è¯­æ³•å¾‹NLPé¢†åŸŸçš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09768v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é˜¿æ‹‰ä¼¯æ³•å¾‹åˆ¤æ–­é¢„æµ‹ï¼ˆLJPï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³ä½èµ„æºè¯­è¨€é˜¿æ‹‰ä¼¯è¯­çš„LJPé—®é¢˜ï¼Œä½œè€…å¼€å‘äº†åŸºäºæ²™ç‰¹å•†äº‹åˆ¤å†³çš„é˜¿æ‹‰ä¼¯è¯­LJPæ•°æ®é›†ã€‚æ–‡ç« è¯„ä¼°äº†åŒ…æ‹¬LLaMA-3.2-3Bå’ŒLLaMA-3.1-8Båœ¨å†…çš„å¼€æºLLMçš„æ€§èƒ½ï¼Œå¹¶é‡‡ç”¨äº†é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹å’Œå¾®è°ƒç­‰å¤šç§é…ç½®æ–¹æ³•ã€‚é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºå¾®è°ƒåçš„è¾ƒå°æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸è¾ƒå¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ›´å…·èµ„æºæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–æŒ‡ä»¤ä¸‹çš„å¾®è°ƒæ•ˆæœï¼Œä¸ºæœªæ¥é˜¿æ‹‰ä¼¯æ³•å¾‹NLPç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨é˜¿æ‹‰ä¼¯è¯­LJPä»»åŠ¡çš„åº”ç”¨ä»ç„¶å¤„äºè¢«ä½ä¼°çš„çŠ¶æ€ã€‚</li>
<li>æœ¬æ–‡ä¸ºé˜¿æ‹‰ä¼¯è¯­LJPå¼€å‘äº†ä¸€ä¸ªåŸºäºæ²™ç‰¹å•†äº‹åˆ¤å†³çš„æ•°æ®é›†ã€‚</li>
<li>è¯„ä¼°äº†LLMåœ¨å¤šç§é…ç½®æ–¹æ³•ä¸‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹å’Œå¾®è°ƒç­‰ã€‚</li>
<li>é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°æ¡†æ¶å‘ç°å¾®è°ƒåçš„è¾ƒå°æ¨¡å‹å…·æœ‰ä¼˜è¶Šçš„èµ„æºæ•ˆç‡å’Œç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šæ ·åŒ–æŒ‡ä»¤ä¸‹çš„å¾®è°ƒæ•ˆæœç ”ç©¶ä¸ºå‘å±•æ›´äººæ€§åŒ–ã€é€‚åº”æ€§æ›´å¼ºçš„LLMæä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡å…¬å¼€äº†æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹ï¼Œä¸ºæœªæ¥é˜¿æ‹‰ä¼¯æ³•å¾‹NLPç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7674b041487a5c6e53a5b42f47d85f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0a8c80c87232743e44f363ddf4b54a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6dc8e3055e34ab41ce04ab17ce714d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1571b171c94c8977a2ec47b669d0c290.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SPAM-Spike-Aware-Adam-with-Momentum-Reset-for-Stable-LLM-Training"><a href="#SPAM-Spike-Aware-Adam-with-Momentum-Reset-for-Stable-LLM-Training" class="headerlink" title="SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"></a>SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training</h2><p><strong>Authors:Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, Shiwei Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource-intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to $1000\times$ larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across various tasks, including (1) LLM pre-training from 60M to 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time Series Forecasting. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our work underscores the importance of mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TianjinYellow/SPAM-Optimizer.git">https://github.com/TianjinYellow/SPAM-Optimizer.git</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç„¶è€Œï¼Œå®ƒä»¬çš„è®­ç»ƒä»ç„¶éœ€è¦æ¶ˆè€—å¤§é‡èµ„æºï¼Œå¹¶å®¹æ˜“å—åˆ°è®­ç»ƒä¸ç¨³å®šç­‰å…³é”®æŒ‘æˆ˜ã€‚è¿™ç§ä¸ç¨³å®šæ€§çš„ä¸»è¦æ¥æºæ˜¯æ¢¯åº¦å’ŒæŸå¤±å³°å€¼ï¼Œå®ƒä»¬ä¼šç ´åå­¦ä¹ è¿‡ç¨‹ï¼Œç»å¸¸å¯¼è‡´è¯¸å¦‚æ£€æŸ¥ç‚¹æ¢å¤å’Œå®éªŒé‡å¯ç­‰æ˜‚è´µå¹²é¢„ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†åœ¨LLMè®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„æ¢¯åº¦å³°å€¼ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å¤šä¸ªæ¶æ„å’Œæ•°æ®é›†ä¸­çš„æ™®éæ€§ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™äº›å³°å€¼å¯èƒ½é«˜è¾¾å…¸å‹æ¢¯åº¦çš„$1000\times$ï¼Œä»è€Œæ˜¾è‘—æ¶åŒ–æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰åŠ¨é‡é‡ç½®SPAMçš„Spikeæ„ŸçŸ¥Adamä¼˜åŒ–å™¨ã€‚è¿™æ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–å™¨ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨é‡é‡ç½®å’Œå³°å€¼æ„ŸçŸ¥æ¢¯åº¦è£å‰ªæ¥æŠµæ¶ˆæ¢¯åº¦å³°å€¼ã€‚åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒåœ¨å†…çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPAMåœ¨å„ç§ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºAdamåŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰LLMä»60Måˆ°1Bçš„é¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰4ä½LLMé¢„è®­ç»ƒï¼›ï¼ˆ3ï¼‰å¼ºåŒ–å­¦ä¹ ï¼›ï¼ˆ4ï¼‰æ—¶é—´åºåˆ—é¢„æµ‹ã€‚æ­¤å¤–ï¼ŒSPAMé€šè¿‡å®ç°ç¨€ç–åŠ¨é‡æ¥ä¿ƒè¿›å†…å­˜é«˜æ•ˆè®­ç»ƒï¼Œå…¶ä¸­ä»…ç»´æŠ¤å¹¶æ›´æ–°ä¸€å°éƒ¨åˆ†åŠ¨é‡é¡¹ã€‚åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹ï¼ŒSPAMä¼˜äºæœ€æ–°çš„å†…å­˜ä¼˜åŒ–å™¨ï¼Œå¦‚GaLoreå’ŒAdam-Miniã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å‡è½»LLMè®­ç»ƒä¸­æ¢¯åº¦å³°å€¼çš„é‡è¦æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œèµ„æºæ•ˆç‡ã€‚ä»£ç å¯ç”¨<a target="_blank" rel="noopener" href="https://github.com/TianjinYellow/SPAM-Optimizer.git%E3%80%82">https://github.com/TianjinYellow/SPAM-Optimizer.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06842v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œèµ„æºæ¶ˆè€—å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¯¥è®ºæ–‡æ¢è®¨äº†LLMè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ³¢åŠ¨ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–å™¨SPAMï¼ˆSpike-Aware Adam with Momentum Resetï¼‰ã€‚SPAMé€šè¿‡åŠ¨é‡é‡ç½®å’Œæ¢¯åº¦è£å‰ªæ¥å¯¹æŠ—æ¢¯åº¦æ³¢åŠ¨ï¼Œèƒ½åœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶ŠAdamåŠå…¶å˜ä½“ã€‚æ­¤å¤–ï¼ŒSPAMè¿˜èƒ½å®ç°å†…å­˜é«˜æ•ˆçš„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè®­ç»ƒå­˜åœ¨æ¢¯åº¦æ³¢åŠ¨é—®é¢˜ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¢¯åº¦æ³¢åŠ¨å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œèµ„æºæ¶ˆè€—å·¨å¤§ã€‚</li>
<li>SPAMæ˜¯ä¸€ç§é’ˆå¯¹LLMè®­ç»ƒçš„æ–°ä¼˜åŒ–å™¨ï¼Œé€šè¿‡åŠ¨é‡é‡ç½®å’Œæ¢¯åº¦è£å‰ªå¯¹æŠ—æ¢¯åº¦æ³¢åŠ¨ã€‚</li>
<li>SPAMåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶ŠAdamåŠå…¶å˜ä½“ã€‚</li>
<li>SPAMå¯å®ç°å†…å­˜é«˜æ•ˆçš„è®­ç»ƒã€‚</li>
<li>SPAMé€šè¿‡åªç»´æŠ¤å¹¶æ›´æ–°ä¸€å°éƒ¨åˆ†åŠ¨é‡é¡¹æ¥å®ç°å†…å­˜ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-210baf103ee8a4877a5154b36814321b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffbeee516b0cef3a562ac95ba2ac267e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547d574ccab1a87f95d351eb2c7395ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016d0350ebacff202767dada5499b3f0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-13767a2602b68de874417b9b5e162439.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  Hybrid Team Tetris A New Platform For Hybrid Multi-Agent, Multi-Human   Teaming
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-02/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e2092ad58bfbfc3cc1ba032c97a32bc5.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-02  Future-Proofing Class-Incremental Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
