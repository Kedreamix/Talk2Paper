<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  Does Generation Require Memorization? Creative Diffusion Models using   Ambient Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f384f1b9f2ad51b7ac6f1f4c3487f2f7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-04-æ›´æ–°"><a href="#2025-03-04-æ›´æ–°" class="headerlink" title="2025-03-04 æ›´æ–°"></a>2025-03-04 æ›´æ–°</h1><h2 id="Does-Generation-Require-Memorization-Creative-Diffusion-Models-using-Ambient-Diffusion"><a href="#Does-Generation-Require-Memorization-Creative-Diffusion-Models-using-Ambient-Diffusion" class="headerlink" title="Does Generation Require Memorization? Creative Diffusion Models using   Ambient Diffusion"></a>Does Generation Require Memorization? Creative Diffusion Models using   Ambient Diffusion</h2><p><strong>Authors:Kulin Shah, Alkis Kalavasis, Adam R. Klivans, Giannis Daras</strong></p>
<p>There is strong empirical evidence that the state-of-the-art diffusion modeling paradigm leads to models that memorize the training set, especially when the training set is small. Prior methods to mitigate the memorization problem often lead to a decrease in image quality. Is it possible to obtain strong and creative generative models, i.e., models that achieve high generation quality and low memorization? Despite the current pessimistic landscape of results, we make significant progress in pushing the trade-off between fidelity and memorization. We first provide theoretical evidence that memorization in diffusion models is only necessary for denoising problems at low noise scales (usually used in generating high-frequency details). Using this theoretical insight, we propose a simple, principled method to train the diffusion models using noisy data at large noise scales. We show that our method significantly reduces memorization without decreasing the image quality, for both text-conditional and unconditional models and for a variety of data availability settings. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„æ‰©æ•£å»ºæ¨¡èŒƒå¼ä¼šå¯¼è‡´æ¨¡å‹å¯¹è®­ç»ƒé›†è¿›è¡Œè®°å¿†ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒé›†è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œè¿™å·²æœ‰å¤§é‡å®è¯è¯æ®æ”¯æŒã€‚ä¸ºäº†å‡è½»è®°å¿†åŒ–é—®é¢˜è€Œé‡‡ç”¨ä¹‹å‰çš„æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬èƒ½å¦è·å¾—å¼ºå¤§ä¸”æœ‰åˆ›é€ åŠ›çš„ç”Ÿæˆæ¨¡å‹ï¼Œå³èƒ½å¤Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡å†…å®¹çš„åŒæ—¶åˆå®ç°ä½è®°å¿†åŒ–çš„æ¨¡å‹å‘¢ï¼Ÿå°½ç®¡å½“å‰ç»“æœå¹¶ä¸ä¹è§‚ï¼Œæˆ‘ä»¬åœ¨æ¨åŠ¨ä¿çœŸåº¦å’Œè®°å¿†åŒ–ä¹‹é—´çš„æƒè¡¡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚æˆ‘ä»¬é¦–å…ˆæä¾›äº†ç†è®ºè¯æ®ï¼Œè¯æ˜æ‰©æ•£æ¨¡å‹ä¸­çš„è®°å¿†åŒ–ä»…å¯¹äºä½å™ªå£°å°ºåº¦ä¸‹çš„å»å™ªé—®é¢˜ï¼ˆé€šå¸¸ç”¨äºç”Ÿæˆé«˜é¢‘ç»†èŠ‚ï¼‰æ˜¯å¿…è¦çš„ã€‚åˆ©ç”¨è¿™ä¸€ç†è®ºè§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰åŸåˆ™çš„æ–¹æ³•ï¼Œä½¿ç”¨å¤§å™ªå£°å°ºåº¦çš„å™ªå£°æ•°æ®æ¥è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—é™ä½äº†è®°å¿†åŒ–ï¼ŒåŒæ—¶å¹¶ä¸é™ä½å›¾åƒè´¨é‡ï¼Œæ—¢é€‚ç”¨äºåŸºäºæ–‡æœ¬çš„æ¡ä»¶æ¨¡å‹ï¼Œä¹Ÿé€‚ç”¨äºæ— æ¡ä»¶çš„æ¨¡å‹ï¼Œå¹¶ä¸”é€‚ç”¨äºå„ç§æ•°æ®å¯ç”¨æ€§è®¾ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21278v1">PDF</a> 33 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å½“å‰æœ€å…ˆè¿›çš„æ‰©æ•£å»ºæ¨¡èŒƒå¼å­˜åœ¨çš„é—®é¢˜ï¼Œå³å½“è®­ç»ƒé›†è¾ƒå°æ—¶ï¼Œæ¨¡å‹å®¹æ˜“è®°ä½è®­ç»ƒé›†å†…å®¹ã€‚ä¸ºè§£å†³è®°å¿†åŒ–é—®é¢˜è€Œé‡‡ç”¨çš„å‰ç½®æ–¹æ³•å¾€å¾€ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å› æ­¤ï¼Œæ–‡ç« æ—¨åœ¨å¯»æ‰¾å¹³è¡¡ç”Ÿæˆè´¨é‡ä¸è®°å¿†åŒ–çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†ç†è®ºè¯æ®ï¼Œè¯æ˜äº†åªæœ‰åœ¨å»å™ªé—®é¢˜ä¸­å¤„ç†ä½å™ªå£°å°ºåº¦æ—¶æ‰éœ€è¦è®°å¿†åŒ–ã€‚åŸºäºæ­¤ç†è®ºæ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å¤§å™ªå£°å°ºåº¦ä¸‹ä½¿ç”¨å™ªå£°æ•°æ®è¿›è¡Œè®­ç»ƒæ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è®°å¿†åŒ–é—®é¢˜ï¼ŒåŒæ—¶ä¿è¯äº†å›¾åƒè´¨é‡åœ¨å„ç§æ•°æ®è®¾ç½®ä¸‹çš„æ–‡æœ¬æ¡ä»¶å’Œæ— æ¡ä»¶æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å–å¾—çš„æˆæœæ˜¯åœ¨é«˜ç”Ÿæˆè´¨é‡å’Œä½è®°å¿†åŒ–ä¹‹é—´æ‰¾åˆ°äº†æ–°çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ‰©æ•£å»ºæ¨¡å­˜åœ¨çš„é—®é¢˜æ˜¯æ¨¡å‹å®¹æ˜“è®°ä½å°è§„æ¨¡çš„è®­ç»ƒé›†å†…å®¹ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚</li>
<li>ç°æœ‰çš„è§£å†³ç­–ç•¥å¾€å¾€ä¼šå¯¼è‡´å›¾åƒè´¨é‡çš„é™ä½ï¼Œå› æ­¤éœ€è¦å¯»æ‰¾ä¸€ç§èƒ½å¤ŸåŒæ—¶æé«˜ç”Ÿæˆè´¨é‡å’Œé™ä½è®°å¿†åŒ–çš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–‡ç« ç»™å‡ºäº†ç†è®ºè¯æ®ï¼Œè¡¨æ˜è®°å¿†åŒ–åœ¨å»å™ªé—®é¢˜å¤„ç†ä½å™ªå£°å°ºåº¦æ—¶æ‰å¿…è¦ã€‚</li>
<li>åŸºäºä¸Šè¿°ç†è®ºæ´å¯Ÿï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å¤§å™ªå£°å°ºåº¦ä¸‹ä½¿ç”¨å™ªå£°æ•°æ®è¿›è¡Œè®­ç»ƒæ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è®°å¿†åŒ–é—®é¢˜ï¼ŒåŒæ—¶ä¿è¯äº†åœ¨å„ç§æ•°æ®è®¾ç½®ä¸‹çš„æ–‡æœ¬æ¡ä»¶å’Œæ— æ¡ä»¶æ¨¡å‹çš„å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•çš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨é«˜ç”Ÿæˆè´¨é‡å’Œä½è®°å¿†åŒ–ä¹‹é—´æ‰¾åˆ°äº†æ–°çš„å¹³è¡¡ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c84a4f2f763e8b0a02591b0cc8f816a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741bd9ac252315aa411f6570230f0336.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39805b1d79132dffea11e7d7b0f6e33b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f840d4f790c78f288202cd6dc123177.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Review-on-Generative-AI-For-Text-To-Image-and-Image-To-Image-Generation-and-Implications-To-Scientific-Images"><a href="#A-Review-on-Generative-AI-For-Text-To-Image-and-Image-To-Image-Generation-and-Implications-To-Scientific-Images" class="headerlink" title="A Review on Generative AI For Text-To-Image and Image-To-Image   Generation and Implications To Scientific Images"></a>A Review on Generative AI For Text-To-Image and Image-To-Image   Generation and Implications To Scientific Images</h2><p><strong>Authors:Zineb Sordo, Eric Chagnon, Daniela Ushizima</strong></p>
<p>This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI. We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding. Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field. </p>
<blockquote>
<p>æœ¬æ–‡ç»¼è¿°äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸå†…æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å¯¹ä¸‰ç§ä¸»æµæ¶æ„ï¼šå˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚å¯¹äºæ¯ä¸€ç§æ¶æ„ï¼Œæˆ‘ä»¬éƒ½é˜è¿°äº†æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„åˆ›æ–°ä»¥åŠå®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦å›¾åƒç†è§£æ–¹é¢çš„åº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†è¿™ä¸€å¿«é€Ÿæ¼”è¿›é¢†åŸŸä¸­çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜å’Œæ½œåœ¨æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21151v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç»¼è¿°äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸä¸­æ–‡å­—åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼Œå¯¹æ¯”åˆ†æäº†å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ä¸‰ç§ä¸»æµæ¶æ„çš„æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„åˆ›æ–°åŠå®é™…åº”ç”¨ä¸­çš„ä¼˜ç¼ºç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦å›¾åƒç†è§£æ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« è¿˜è®¨è®ºäº†è¯¥é¢†åŸŸçš„å…³é”®å¼€æ”¾æŒ‘æˆ˜å’Œæ½œåœ¨æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æœ€æ–°è¿›å±•ã€‚</li>
<li>å¯¹æ¯”åˆ†æäº†å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ä¸‰ç§ä¸»æµæ¶æ„ã€‚</li>
<li>é˜è¿°äº†æ¯ç§æ¶æ„çš„æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„åˆ›æ–°ã€‚</li>
<li>å¼ºè°ƒäº†è¿™äº›æ¶æ„åœ¨å›¾åƒç”Ÿæˆä¸­çš„å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦å›¾åƒç†è§£æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>è¯¥é¢†åŸŸå­˜åœ¨è®¸å¤šå¼€æ”¾æŒ‘æˆ˜ï¼Œå¦‚æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5f37b7d4f6c46e5692c43d69b1f16d94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08a3fde9f8beb6307c3a56d9cf378a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ec1a6c0c058e7221fbda31c8d9a6cff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0e5e74d50c9c3b0d37d983305d8a0cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-327c0037e6d032c58ceb3ad708a2f6d7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CADDreamer-CAD-object-Generation-from-Single-view-Images"><a href="#CADDreamer-CAD-object-Generation-from-Single-view-Images" class="headerlink" title="CADDreamer: CAD object Generation from Single-view Images"></a>CADDreamer: CAD object Generation from Single-view Images</h2><p><strong>Authors:Yuan Li, Cheng Lin, Yuan Liu, Xiaoxiao Long, Chenxu Zhang, Ningna Wang, Xin Li, Wenping Wang, Xiaohu Guo</strong></p>
<p>Diffusion-based 3D generation has made remarkable progress in recent years. However, existing 3D generative models often produce overly dense and unstructured meshes, which stand in stark contrast to the compact, structured, and sharply-edged Computer-Aided Design (CAD) models crafted by human designers. To address this gap, we introduce CADDreamer, a novel approach for generating boundary representations (B-rep) of CAD objects from a single image. CADDreamer employs a primitive-aware multi-view diffusion model that captures both local geometric details and high-level structural semantics during the generation process. By encoding primitive semantics into the color domain, the method leverages the strong priors of pre-trained diffusion models to align with well-defined primitives. This enables the inference of multi-view normal maps and semantic maps from a single image, facilitating the reconstruction of a mesh with primitive labels. Furthermore, we introduce geometric optimization techniques and topology-preserving extraction methods to mitigate noise and distortion in the generated primitives. These enhancements result in a complete and seamless B-rep of the CAD model. Experimental results demonstrate that our method effectively recovers high-quality CAD objects from single-view images. Compared to existing 3D generation techniques, the B-rep models produced by CADDreamer are compact in representation, clear in structure, sharp in edges, and watertight in topology. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„3Dç”ŸæˆæŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dç”Ÿæˆæ¨¡å‹å¾€å¾€äº§ç”Ÿè¿‡äºå¯†é›†å’Œæ— ç»“æ„çš„ç½‘æ ¼ï¼Œè¿™ä¸äººç±»è®¾è®¡å¸ˆç²¾å¿ƒåˆ¶ä½œçš„ç´§å‡‘ã€ç»“æ„åŒ–ã€è¾¹ç¼˜æ¸…æ™°çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CADDreamerï¼Œè¿™æ˜¯ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡å¯¹è±¡è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰çš„æ–°æ–¹æ³•ã€‚CADDreameré‡‡ç”¨äº†ä¸€ç§åŸå§‹æ„ŸçŸ¥çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ•æ‰å±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜å±‚æ¬¡çš„ç»“æ„è¯­ä¹‰ã€‚é€šè¿‡å°†åŸå§‹è¯­ä¹‰ç¼–ç åˆ°é¢œè‰²åŸŸä¸­ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ä¸å®šä¹‰çš„åŸå§‹å¯¹é½ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä»å•å¼ å›¾åƒæ¨æ–­å¤šè§†è§’æ³•çº¿å›¾å’Œè¯­ä¹‰å›¾ï¼Œä¿ƒè¿›å¸¦æœ‰åŸå§‹æ ‡ç­¾çš„ç½‘æ ¼é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œä»¥å‡è½»ç”ŸæˆåŸå§‹æ¨¡å‹ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚è¿™äº›å¢å¼ºåŠŸèƒ½å¯¼è‡´äº†ä¸€ä¸ªå®Œæ•´ä¸”æ— ç¼éš™çš„CADæ¨¡å‹çš„B-repã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»å•è§†å›¾å›¾åƒä¸­æœ‰æ•ˆåœ°æ¢å¤äº†é«˜è´¨é‡çš„CADå¯¹è±¡ã€‚ä¸ç°æœ‰çš„3Dç”ŸæˆæŠ€æœ¯ç›¸æ¯”ï¼ŒCADDreameräº§ç”Ÿçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºä¸Šæ›´ç´§å‡‘ã€ç»“æ„ä¸Šæ›´æ¸…æ™°ã€è¾¹ç¼˜æ›´é”‹åˆ©ã€æ‹“æ‰‘æ›´é˜²æ°´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20732v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„3Dç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆè¿‡äºå¯†é›†ã€ç»“æ„æ··ä¹±çš„ç½‘æ ¼æ¨¡å‹çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CADDreamerï¼Œä¸€ç§èƒ½ä»å•å¼ å›¾åƒç”ŸæˆCADå¯¹è±¡è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰çš„æ–°æ–¹æ³•ã€‚CADDreameré‡‡ç”¨æ„ŸçŸ¥åŸå§‹ç‰¹å¾çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ•æ‰å±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜å±‚æ¬¡çš„ç»“æ„è¯­ä¹‰ã€‚é€šè¿‡ç¼–ç åŸå§‹è¯­ä¹‰åˆ°é¢œè‰²åŸŸï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå…ˆéªŒçŸ¥è¯†ä¸å®šä¹‰çš„åŸå§‹ç‰¹å¾å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ä»å•ä¸€è§†è§’çš„å›¾åƒä¸­æœ‰æ•ˆæ¢å¤é«˜è´¨é‡çš„CADå¯¹è±¡ã€‚ç›¸æ¯”ç°æœ‰çš„3Dç”ŸæˆæŠ€æœ¯ï¼ŒCADDreamerç”Ÿæˆçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºä¸Šæ›´ç´§å‡‘ã€ç»“æ„ä¸Šæ›´æ¸…æ™°ã€è¾¹ç¼˜æ›´é”åˆ©ã€æ‹“æ‰‘æ›´å®Œæ•´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨3Dç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆç½‘æ ¼æ¨¡å‹è¿‡äºå¯†é›†å’Œç»“æ„æ··ä¹±çš„é—®é¢˜ã€‚</li>
<li>CADDreameræ˜¯ä¸€ç§èƒ½ä»å•å¼ å›¾åƒç”ŸæˆCADå¯¹è±¡è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰çš„æ–°æ–¹æ³•ã€‚</li>
<li>CADDreameré‡‡ç”¨æ„ŸçŸ¥åŸå§‹ç‰¹å¾çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œæ•æ‰å±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜å±‚æ¬¡çš„ç»“æ„è¯­ä¹‰ã€‚</li>
<li>é€šè¿‡ç¼–ç åŸå§‹è¯­ä¹‰åˆ°é¢œè‰²åŸŸï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå…ˆéªŒçŸ¥è¯†ï¼Œå®ç°ä¸å®šä¹‰çš„åŸå§‹ç‰¹å¾å¯¹é½ã€‚</li>
<li>CADDreamerèƒ½æ¨ç†å‡ºå¤šè§†è§’æ³•çº¿è´´å›¾å’Œè¯­ä¹‰è´´å›¾ï¼Œä»å•å¼ å›¾åƒé‡å»ºå¸¦åŸå§‹æ ‡ç­¾çš„ç½‘æ ¼ã€‚</li>
<li>å¼•å…¥å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œå‡å°‘ç”ŸæˆåŸå§‹ç‰¹å¾çš„å™ªå£°å’Œå¤±çœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14453b9ed8a69591b1a6ab794f2ca1be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b67d84798689f08d8b98bb15badb03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daf5a0d3dee0506d349493f6eebb2faa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7e9ff7f89b5b57ed6168d8196c65314.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfdf93393cc68bbbf03807bc8f30c8cc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Gungnir-Exploiting-Stylistic-Features-in-Images-for-Backdoor-Attacks-on-Diffusion-Models"><a href="#Gungnir-Exploiting-Stylistic-Features-in-Images-for-Backdoor-Attacks-on-Diffusion-Models" class="headerlink" title="Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on   Diffusion Models"></a>Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on   Diffusion Models</h2><p><strong>Authors:Yu Pan, Bingrong Dai, Jiahao Chen, Lin Wang, Yi Du, Jiao Liu</strong></p>
<p>In recent years, Diffusion Models (DMs) have demonstrated significant advances in the field of image generation. However, according to current research, DMs are vulnerable to backdoor attacks, which allow attackers to control the modelâ€™s output by inputting data containing covert triggers, such as a specific patch or phrase. Existing defense strategies are well equipped to thwart such attacks through backdoor detection and trigger inversion because previous attack methods are constrained by limited input spaces and triggers defined by low-dimensional features. To bridge these gaps, we propose Gungnir, a novel method that enables attackers to activate the backdoor in DMs through hidden style triggers within input images. Our approach proposes using stylistic features as triggers for the first time and implements backdoor attacks successfully in image2image tasks by utilizing Reconstructing-Adversarial Noise (RAN) and Short-Term-Timesteps-Retention (STTR) of DMs. Meanwhile, experiments demonstrate that our method can easily bypass existing defense methods. Among existing DM main backdoor defense frameworks, our approach achieves a 0% backdoor detection rate (BDR). Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/paoche11/Gungnir">https://github.com/paoche11/Gungnir</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ ¹æ®å½“å‰çš„ç ”ç©¶ï¼Œæ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡è¾“å…¥åŒ…å«éšè”½è§¦å‘å™¨çš„æ•°æ®æ¥æ§åˆ¶æ¨¡å‹çš„è¾“å‡ºï¼Œä¾‹å¦‚ç‰¹å®šçš„è¡¥ä¸æˆ–çŸ­è¯­ã€‚ç°æœ‰çš„é˜²å¾¡ç­–ç•¥èƒ½å¤Ÿé€šè¿‡åé—¨æ£€æµ‹å’Œè§¦å‘åè½¬æ¥æœ‰æ•ˆåœ°é˜»æ­¢è¿™ç±»æ”»å‡»ï¼Œå› ä¸ºä»¥å‰çš„æ”»å‡»æ–¹æ³•å—åˆ°æœ‰é™è¾“å…¥ç©ºé—´å’Œç”±ä½ç»´ç‰¹å¾å®šä¹‰è§¦å‘å™¨çš„åˆ¶çº¦ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†Gungnirè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒèƒ½ä½¿æ”»å‡»è€…é€šè¿‡è¾“å…¥å›¾åƒä¸­çš„éšè—æ ·å¼è§¦å‘å™¨åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ¿€æ´»åé—¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–æ¬¡æå‡ºä½¿ç”¨é£æ ¼ç‰¹å¾ä½œä¸ºè§¦å‘å™¨ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é‡å»ºå¯¹æŠ—å™ªå£°ï¼ˆRANï¼‰å’ŒçŸ­æœŸæ—¶é—´æ­¥ä¿ç•™ï¼ˆSTTRï¼‰åœ¨å›¾åƒåˆ°å›¾åƒçš„ä»»åŠ¡ä¸­æˆåŠŸå®æ–½åé—¨æ”»å‡»ã€‚åŒæ—¶ï¼Œå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ˜“åœ°ç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ–¹æ³•ã€‚åœ¨ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸»è¦åé—¨é˜²å¾¡æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†0%çš„åé—¨æ£€æµ‹ç‡ï¼ˆBDRï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/paoche11/Gungnir">https://github.com/paoche11/Gungnir</a> ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20650v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¿‘æœŸç ”ç©¶å‘ç°å…¶æ˜“å—åé—¨æ”»å‡»å½±å“ã€‚æ–°æå‡ºçš„Gungniræ–¹æ³•åˆ©ç”¨éšè—çš„é£æ ¼è§¦å‘å› ç´ ï¼Œé€šè¿‡è¾“å…¥å›¾åƒä¸­çš„éšè—é£æ ¼è§¦å‘å› ç´ åœ¨DMsä¸­æ¿€æ´»åé—¨ã€‚è¯¥æ–¹æ³•æˆåŠŸåœ¨å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä»»åŠ¡ä¸­å®ç°åé—¨æ”»å‡»ï¼Œå¹¶å¯ä»¥ç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µã€‚ç›®å‰é˜²å¾¡æ¡†æ¶å¯¹Gungniræ–¹æ³•çš„åé—¨æ£€æµ‹ç‡ä¸ºé›¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>DMsæ˜“å—åé—¨æ”»å‡»å½±å“ï¼Œæ”»å‡»è€…å¯é€šè¿‡è¾“å…¥åŒ…å«éšè”½è§¦å‘çš„æ•°æ®æ¥æ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚</li>
<li>ç°æœ‰é˜²å¾¡ç­–ç•¥èƒ½é€šè¿‡åé—¨æ£€æµ‹å’Œè§¦å‘åè½¬æ¥æŠµå¾¡æ­¤ç±»æ”»å‡»ã€‚</li>
<li>Gungniræ–¹æ³•é¦–æ¬¡åˆ©ç”¨é£æ ¼ç‰¹å¾ä½œä¸ºè§¦å‘å› ç´ ï¼Œåœ¨å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä»»åŠ¡ä¸­å®ç°æˆåŠŸçš„åé—¨æ”»å‡»ã€‚</li>
<li>Gungniræ–¹æ³•é€šè¿‡åˆ©ç”¨DMsçš„é‡å»ºå¯¹æŠ—å™ªå£°å’ŒçŸ­æœŸæ—¶é—´æ­¥ä¿ç•™æ¥å®ç°åé—¨æ¿€æ´»ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒGungniræ–¹æ³•å¯ä»¥è½»æ˜“ç»•è¿‡ç°æœ‰é˜²å¾¡æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-518fd7ff8407359f0270f12dad8181a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7197cb36d2564cdb3be0a0c4df24a681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df8a5324ccf6a032482cf3ef8e0049f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4974a4acc1df89f5368784994c9e03ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b5388f8f57cb39d8bf20b561fbd426b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed0f13b4349ed2bb925a68d05eeb703d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77f4c9031eba41fd2bd02b5b9bd888e0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="T2ICount-Enhancing-Cross-modal-Understanding-for-Zero-Shot-Counting"><a href="#T2ICount-Enhancing-Cross-modal-Understanding-for-Zero-Shot-Counting" class="headerlink" title="T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting"></a>T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting</h2><p><strong>Authors:Yifei Qian, Zhongliang Guo, Bowen Deng, Chun Tong Lei, Shuai Zhao, Chun Pong Lau, Xiaopeng Hong, Michael P. Pound</strong></p>
<p>Zero-shot object counting aims to count instances of arbitrary object categories specified by text descriptions. Existing methods typically rely on vision-language models like CLIP, but often exhibit limited sensitivity to text prompts. We present T2ICount, a diffusion-based framework that leverages rich prior knowledge and fine-grained visual understanding from pretrained diffusion models. While one-step denoising ensures efficiency, it leads to weakened text sensitivity. To address this challenge, we propose a Hierarchical Semantic Correction Module that progressively refines text-image feature alignment, and a Representational Regional Coherence Loss that provides reliable supervision signals by leveraging the cross-attention maps extracted from the denosing U-Net. Furthermore, we observe that current benchmarks mainly focus on majority objects in images, potentially masking modelsâ€™ text sensitivity. To address this, we contribute a challenging re-annotated subset of FSC147 for better evaluation of text-guided counting ability. Extensive experiments demonstrate that our method achieves superior performance across different benchmarks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/cha15yq/T2ICount">https://github.com/cha15yq/T2ICount</a>. </p>
<blockquote>
<p>é›¶é•œå¤´ç›®æ ‡è®¡æ•°æ—¨åœ¨é€šè¿‡æ–‡æœ¬æè¿°è®¡æ•°ä»»æ„å¯¹è±¡ç±»åˆ«çš„å®ä¾‹æ•°é‡ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºCLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½†å¯¹æ–‡æœ¬æç¤ºçš„æ•æ„Ÿæ€§æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†T2ICountï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å’Œæ¥è‡ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†è§†è§‰ç†è§£ã€‚ä¸€æ­¥å»å™ªè™½ç„¶ä¿è¯äº†æ•ˆç‡ï¼Œä½†å¯¼è‡´äº†æ–‡æœ¬æ•æ„Ÿæ€§ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è¯­ä¹‰æ ¡æ­£æ¨¡å—ï¼Œé€æ­¥æ”¹è¿›æ–‡æœ¬-å›¾åƒç‰¹å¾å¯¹é½ï¼Œå¹¶æå‡ºä»£è¡¨æ€§åŒºåŸŸä¸€è‡´æ€§æŸå¤±ï¼Œé€šè¿‡åˆ©ç”¨å»å™ªU-Netæå–çš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œæä¾›å¯é çš„ç›‘ç£ä¿¡å·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å½“å‰çš„ä¸»è¦åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å›¾åƒä¸­çš„ä¸»è¦å¯¹è±¡ä¸Šï¼Œå¯èƒ½ä¼šæ©ç›–æ¨¡å‹çš„æ–‡æœ¬æ•æ„Ÿæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹FSC147è¿›è¡Œäº†é‡æ–°æ ‡æ³¨çš„å­é›†æŒ‘æˆ˜ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ–‡æœ¬å¼•å¯¼è®¡æ•°èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cha15yq/T2ICount%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cha15yq/T2ICountæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20625v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„T2ICountæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å’Œå¯¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†è§†è§‰ç†è§£æ¥å®ç°é›¶å°„å‡»å¯¹è±¡è®¡æ•°ã€‚ä¸ºè§£å†³å•æ­¥å»å™ªå¸¦æ¥çš„æ–‡æœ¬æ•æ„Ÿæ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºåˆ†å±‚è¯­ä¹‰æ ¡æ­£æ¨¡å—å’Œä»£è¡¨æ€§åŒºåŸŸä¸€è‡´æ€§æŸå¤±æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å›¾åƒä¸­çš„å¤šæ•°å¯¹è±¡çš„é—®é¢˜è¿›è¡Œå…³æ³¨ï¼Œé‡æ–°æ³¨é‡Šäº†FSC147çš„æ›´å…·æŒ‘æˆ˜æ€§çš„å­é›†ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ–‡æœ¬å¼•å¯¼è®¡æ•°èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2ICountæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°é›¶å°„å‡»å¯¹è±¡è®¡æ•°ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†è§†è§‰ç†è§£ï¼Œæé«˜è®¡æ•°å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºåˆ†å±‚è¯­ä¹‰æ ¡æ­£æ¨¡å—æ¥è§£å†³å•æ­¥å»å™ªå¯¼è‡´çš„æ–‡æœ¬æ•æ„Ÿæ€§ä¸è¶³é—®é¢˜ã€‚</li>
<li>æå‡ºä»£è¡¨æ€§åŒºåŸŸä¸€è‡´æ€§æŸå¤±æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å»å™ªU-Netçš„äº¤å‰æ³¨æ„åŠ›å›¾æä¾›å¯é çš„ç›‘ç£ä¿¡å·ã€‚</li>
<li>å…³æ³¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å›¾åƒä¸­çš„å¤šæ•°å¯¹è±¡é—®é¢˜ï¼Œé‡æ–°æ³¨é‡ŠFSC147æ•°æ®é›†ä»¥æ›´å¥½åœ°è¯„ä¼°æ–‡æœ¬å¼•å¯¼è®¡æ•°èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>å…¬å¼€å¯ç”¨ä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1785505f5fe4e8b5907a071864722df3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba17b602a1566e2d37a06e944ff38515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17e64c6d37d1a08999382c190ab7a570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb67c46b044f63736c7198d39466e52c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Language-Informed-Hyperspectral-Image-Synthesis-for-Imbalanced-Small-Sample-Classification-via-Semi-Supervised-Conditional-Diffusion-Model"><a href="#Language-Informed-Hyperspectral-Image-Synthesis-for-Imbalanced-Small-Sample-Classification-via-Semi-Supervised-Conditional-Diffusion-Model" class="headerlink" title="Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small   Sample Classification via Semi-Supervised Conditional Diffusion Model"></a>Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small   Sample Classification via Semi-Supervised Conditional Diffusion Model</h2><p><strong>Authors:Yimin Zhu, Linlin Xu</strong></p>
<p>Data augmentation effectively addresses the imbalanced-small sample data (ISSD) problem in hyperspectral image classification (HSIC). While most methodologies extend features in the latent space, few leverage text-driven generation to create realistic and diverse samples. Recently, text-guided diffusion models have gained significant attention due to their ability to generate highly diverse and high-quality images based on text prompts in natural image synthesis. Motivated by this, this paper proposes Txt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis method to address the ISSD in HSIC. The proposed approach uses a denoising diffusion model, which iteratively removes Gaussian noise to generate hyperspectral samples conditioned on textual descriptions. First, to address the high-dimensionality of hyperspectral data, a universal variational autoencoder (VAE) is designed to map the data into a low-dimensional latent space, which provides stable features and reduces the inference complexity of diffusion model. Second, a semi-supervised diffusion model is designed to fully take advantage of unlabeled data. Random polygon spatial clipping (RPSC) and uncertainty estimation of latent feature (LF-UE) are used to simulate the varying degrees of mixing. Third, the VAE decodes HSI from latent space generated by the diffusion model with the language conditions as input. In our experiments, we fully evaluate synthetic samplesâ€™ effectiveness from statistical characteristics and data distribution in 2D-PCA space. Additionally, visual-linguistic cross-attention is visualized on the pixel level to prove that our proposed model can capture the spatial layout and geometry of the generated data. Experiments demonstrate that the performance of the proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models, state-of-the-art CNNs, and semi-supervised methods. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºæœ‰æ•ˆè§£å†³äº†é«˜å…‰è°±å›¾åƒåˆ†ç±»ï¼ˆHSICï¼‰ä¸­çš„ä¸å¹³è¡¡å°æ ·æœ¬æ•°æ®ï¼ˆISSDï¼‰é—®é¢˜ã€‚è™½ç„¶å¤§å¤šæ•°æ–¹æ³•éƒ½åœ¨æ½œåœ¨ç©ºé—´æ‰©å±•ç‰¹å¾ï¼Œä½†å¾ˆå°‘æœ‰æ–¹æ³•åˆ©ç”¨æ–‡æœ¬é©±åŠ¨ç”Ÿæˆæ¥åˆ›å»ºç°å®å’Œå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚æœ€è¿‘ï¼Œæ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„å›¾åƒçš„èƒ½åŠ›è€Œåœ¨è‡ªç„¶å›¾åƒåˆæˆä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†Txt2HSI-LDMï¼ˆVAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¯­è¨€ä¿¡æ¯å¼•å¯¼çš„é«˜å…‰è°±å›¾åƒåˆæˆæ–¹æ³•æ¥è§£å†³HSICä¸­çš„ISSDé—®é¢˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä½¿ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£å»é™¤é«˜æ–¯å™ªå£°ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜å…‰è°±æ ·æœ¬ã€‚é¦–å…ˆï¼Œä¸ºäº†è§£å†³é«˜å…‰è°±æ•°æ®çš„é«˜ç»´æ€§é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªé€šç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå°†æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œè¿™æä¾›äº†ç¨³å®šçš„ç‰¹å¾å¹¶é™ä½äº†æ‰©æ•£æ¨¡å‹çš„æ¨ç†å¤æ‚æ€§ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†åŠç›‘ç£æ‰©æ•£æ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨æœªæ ‡è®°æ•°æ®ã€‚ä½¿ç”¨éšæœºå¤šè¾¹å½¢ç©ºé—´è£å‰ªï¼ˆRPSCï¼‰å’Œæ½œåœ¨ç‰¹å¾çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆLF-UEï¼‰æ¥æ¨¡æ‹Ÿä¸åŒç¨‹åº¦çš„æ··åˆã€‚ç¬¬ä¸‰ï¼ŒVAEæ ¹æ®æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ä¸­çš„è¯­è¨€æ¡ä»¶å¯¹HSIè¿›è¡Œè§£ç ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å…¨é¢è¯„ä¼°äº†åˆæˆæ ·æœ¬åœ¨ç»Ÿè®¡ç‰¹æ€§å’Œæ•°æ®åˆ†å¸ƒæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œåœ¨äºŒç»´ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç©ºé—´ä¸­ã€‚æ­¤å¤–ï¼Œè§†è§‰è¯­è¨€äº¤å‰æ³¨æ„åŠ›åœ¨åƒç´ çº§åˆ«å¯è§†åŒ–ï¼Œè¯æ˜äº†æˆ‘ä»¬æ‰€æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç”Ÿæˆæ•°æ®çš„ç©ºé—´å¸ƒå±€å’Œå‡ ä½•ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Txt2HSI-LDMï¼ˆVAEï¼‰çš„æ€§èƒ½è¶…è¿‡äº†ç»å…¸çš„åå¤‡æ¨¡å‹ã€æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŠç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19700v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬æå‡ºä¸€ç§åä¸ºTxt2HSI-LDMï¼ˆVAEï¼‰çš„æ–°å‹è¯­è¨€å¼•å¯¼çš„é¥æ„Ÿå›¾åƒåˆæˆæ–¹æ³•ï¼Œç”¨äºè§£å†³é¥æ„Ÿå›¾åƒåˆ†ç±»ä¸­çš„æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é™å™ªæ‰©æ•£æ¨¡å‹å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé¥æ„Ÿå›¾åƒæ ·æœ¬ã€‚å®éªŒè¯æ˜å…¶æ€§èƒ½è¶…è¶Šä¼ ç»Ÿæ¨¡å‹å’Œæœ€æ–°å·ç§¯ç¥ç»ç½‘ç»œä»¥åŠåŠç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºæ˜¯è§£å†³é¥æ„Ÿå›¾åƒåˆ†ç±»ä¸­å°æ ·æœ¬æ•°æ®ä¸å¹³è¡¡é—®é¢˜çš„æœ‰æ•ˆæ‰‹æ®µã€‚</li>
<li>å¤§å¤šæ•°æ–¹æ³•é€šè¿‡æ‰©å±•æ½œåœ¨ç©ºé—´ç‰¹å¾æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†æ–‡æœ¬é©±åŠ¨ç”Ÿæˆçš„æ–¹æ³•èƒ½åˆ›å»ºçœŸå®ä¸”å¤šæ ·çš„æ ·æœ¬ã€‚</li>
<li>Txt2HSI-LDMï¼ˆVAEï¼‰æ˜¯ä¸€ç§æ–°å‹è¯­è¨€å¼•å¯¼çš„æ–¹æ³•ï¼Œç”¨äºåˆæˆé¥æ„Ÿå›¾åƒï¼Œæ—¨åœ¨è§£å†³é¥æ„Ÿå›¾åƒåˆ†ç±»ä¸­çš„æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆé™å™ªæ‰©æ•£æ¨¡å‹å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼Œé€šè¿‡è¿­ä»£å»é™¤é«˜æ–¯å™ªå£°ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé¥æ„Ÿå›¾åƒæ ·æœ¬ã€‚</li>
<li>å˜åˆ†è‡ªç¼–ç å™¨ç”¨äºå°†é«˜å…‰è°±æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œæä¾›ç¨³å®šç‰¹å¾å¹¶é™ä½æ‰©æ•£æ¨¡å‹çš„æ¨ç†å¤æ‚æ€§ã€‚</li>
<li>å®éªŒè¯æ˜Txt2HSI-LDMï¼ˆVAEï¼‰æ€§èƒ½ä¼˜äºç»å…¸æ¨¡å‹ã€æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŠç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d894a477738274020c7b57a510a9a85c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93a4f7e67b755915d12a4a198ef4077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cf90141d6f010df9eda2249d6020435.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d78aaf678d47a35338cbab026a4eb3a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78c00c24c5ca6e139925e2ab61f08a70.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UIBDiffusion-Universal-Imperceptible-Backdoor-Attack-for-Diffusion-Models"><a href="#UIBDiffusion-Universal-Imperceptible-Backdoor-Attack-for-Diffusion-Models" class="headerlink" title="UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion   Models"></a>UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion   Models</h2><p><strong>Authors:Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao</strong></p>
<p>Recent studies show that diffusion models (DMs) are vulnerable to backdoor attacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray box and eyeglasses) that contain evident patterns, rendering remarkable attack effects yet easy detection upon human inspection and defensive algorithms. While it is possible to improve stealthiness by reducing the strength of the backdoor, doing so can significantly compromise its generality and effectiveness. In this paper, we propose UIBDiffusion, the universal imperceptible backdoor attack for diffusion models, which allows us to achieve superior attack and generation performance while evading state-of-the-art defenses. We propose a novel trigger generation approach based on universal adversarial perturbations (UAPs) and reveal that such perturbations, which are initially devised for fooling pre-trained discriminative models, can be adapted as potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on multiple types of DMs with different kinds of samplers across various datasets and targets. Experimental results demonstrate that UIBDiffusion brings three advantages: 1) Universality, the imperceptible trigger is universal (i.e., image and model agnostic) where a single trigger is effective to any images and all diffusion models with different samplers; 2) Utility, it achieves comparable generation quality (e.g., FID) and even better attack success rate (i.e., ASR) at low poison rates compared to the prior works; and 3) Undetectability, UIBDiffusion is plausible to human perception and can bypass Elijah and TERD, the SOTA defenses against backdoors for DMs. We will release our backdoor triggers and code. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚ç°æœ‰çš„åé—¨æ”»å‡»é‡‡ç”¨æœªåŠ æ©é¥°çš„è§¦å‘å™¨ï¼ˆä¾‹å¦‚ï¼Œç°ç›’å’Œçœ¼é•œï¼‰ï¼Œè¿™äº›è§¦å‘å™¨åŒ…å«æ˜æ˜¾çš„æ¨¡å¼ï¼Œè™½ç„¶æ”»å‡»æ•ˆæœæ˜¾è‘—ï¼Œä½†å¾ˆå®¹æ˜“è¢«äººç±»æ£€æŸ¥å’Œé˜²å¾¡ç®—æ³•æ£€æµ‹å‡ºæ¥ã€‚è™½ç„¶é€šè¿‡å‡å¼±åé—¨å¼ºåº¦å¯ä»¥æé«˜éšè”½æ€§ï¼Œä½†è¿™æ ·åšå¯èƒ½ä¼šæå¤§åœ°æŸå®³å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„é€šç”¨éšè”½åé—¨æ”»å‡»UIBDiffusionï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨èº²é¿æœ€æ–°é˜²å¾¡çš„åŒæ—¶å®ç°å‡ºè‰²çš„æ”»å‡»å’Œç”Ÿæˆæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€šç”¨å¯¹æŠ—æ‰°åŠ¨ï¼ˆUAPsï¼‰çš„æ–°å‹è§¦å‘å™¨ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶æ­ç¤ºè¿™ç§æœ€åˆè¢«è®¾è®¡ç”¨æ¥æ¬ºéª—é¢„è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹çš„æ‰°åŠ¨ï¼Œå¯ä»¥é€‚åº”æˆä¸ºé’ˆå¯¹DMsçš„å¼ºå¤§éšè”½åé—¨è§¦å‘å™¨ã€‚æˆ‘ä»¬åœ¨å¤šç§ç±»å‹çš„æ‰©æ•£æ¨¡å‹ä¸Šè¯„ä¼°äº†UIBDiffusionï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨äº†å¤šç§é‡‡æ ·å™¨ã€è·¨å„ç§æ•°æ®é›†å’Œç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUIBDiffusionå…·æœ‰ä¸‰ä¸ªä¼˜åŠ¿ï¼š1ï¼‰é€šç”¨æ€§ï¼Œéšè”½è§¦å‘å™¨æ˜¯é€šç”¨çš„ï¼ˆå³å›¾åƒå’Œæ¨¡å‹æ— å…³ï¼‰ï¼Œå•ä¸ªè§¦å‘å™¨å¯¹ä»»ä½•å›¾åƒå’Œæ‰€æœ‰ä½¿ç”¨ä¸åŒé‡‡æ ·å™¨çš„æ‰©æ•£æ¨¡å‹éƒ½æœ‰æ•ˆï¼›2ï¼‰å®ç”¨æ€§ï¼Œå®ƒåœ¨ä½ä¸­æ¯’ç‡ä¸‹å®ç°äº†ç›¸å½“çš„ç”Ÿæˆè´¨é‡ï¼ˆä¾‹å¦‚FIDï¼‰ç”šè‡³æ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ˆå³ASRï¼‰ï¼›3ï¼‰ä¸å¯æ£€æµ‹æ€§ï¼ŒUIBDiffusionå¯¹äººç±»æ„ŸçŸ¥æ¥è¯´æ˜¯åˆç†çš„ï¼Œå¹¶ä¸”èƒ½å¤Ÿç»•è¿‡Elijahå’ŒTERDâ€”â€”é’ˆå¯¹DMsåé—¨çš„æœ€æ–°é˜²å¾¡æ‰‹æ®µã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„åé—¨è§¦å‘å™¨å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11441v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¿‘æœŸç ”ç©¶æ˜¾ç¤ºæ˜“å—åé—¨æ”»å‡»å½±å“ã€‚ç°æœ‰åé—¨æ”»å‡»é€šå¸¸ä½¿ç”¨æ˜æ˜¾çš„è§¦å‘å› ç´ ï¼ˆå¦‚ç°è‰²æ–¹å—å’Œçœ¼é•œï¼‰ï¼Œè¿™äº›è§¦å‘å› ç´ æ˜“è¢«äººçœ¼å’Œé˜²å¾¡ç®—æ³•è¯†åˆ«ã€‚å°½ç®¡å¯ä»¥é€šè¿‡é™ä½åé—¨å¼ºåº¦æ¥æé«˜éšè”½æ€§ï¼Œä½†è¿™å¾€å¾€ä¼šæŸå®³å…¶æ™®éæ€§å’Œæ•ˆæœã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„é€šç”¨éšè”½åé—¨æ”»å‡»æ–¹æ³•UIBDiffusionï¼Œå¯åœ¨é¿å¼€æœ€æ–°é˜²å¾¡æ‰‹æ®µçš„åŒæ—¶å®ç°æ›´å‡ºè‰²çš„æ”»å‡»å’Œç”Ÿæˆæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€šç”¨å¯¹æŠ—æ‰°åŠ¨ï¼ˆUAPsï¼‰çš„æ–°å‹è§¦å‘å› ç´ ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶å‘ç°è¿™ç§åŸæœ¬ç”¨äºæ¬ºéª—é¢„è®­ç»ƒåˆ¤åˆ«æ¨¡å‹çš„æ‰°åŠ¨ï¼Œå¯ä½œä¸ºé’ˆå¯¹DMsçš„å¼ºå¤§éšè”½åé—¨è§¦å‘å› ç´ ã€‚æˆ‘ä»¬åœ¨å¤šç§ç±»å‹çš„æ‰©æ•£æ¨¡å‹ã€é‡‡æ ·å™¨ä»¥åŠæ•°æ®é›†ä¸Šè¯„ä¼°äº†UIBDiffusionã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUIBDiffusionå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼š1ï¼‰é€šç”¨æ€§ï¼Œéšè”½è§¦å‘å› ç´ å…·æœ‰å›¾åƒå’Œæ¨¡å‹é€šç”¨æ€§ï¼Œå•ä¸ªè§¦å‘å› ç´ å¯¹æ‰€æœ‰å›¾åƒå’Œå„ç§æ‰©æ•£æ¨¡å‹åŠé‡‡æ ·å™¨å‡æœ‰æ•ˆï¼›2ï¼‰å®ç”¨æ€§ï¼Œå®ƒåœ¨ä½æ¯’ç‡ä¸‹å®ç°äº†ç›¸å½“çš„ç”Ÿæˆè´¨é‡ï¼ˆå¦‚FIDï¼‰ç”šè‡³æ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼›3ï¼‰ä¸å¯æ£€æµ‹æ€§ï¼ŒUIBDiffusionå¯¹äººçœ¼æ¥è¯´å¾ˆéš¾å¯Ÿè§‰ï¼Œå¯ä»¥ç»•è¿‡Elijahå’ŒTERDç­‰é’ˆå¯¹æ‰©æ•£æ¨¡å‹åé—¨çš„æœ€æ–°é˜²å¾¡æ‰‹æ®µã€‚æˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„åé—¨è§¦å‘å› ç´ å’Œä»£ç ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰é¢ä¸´åé—¨æ”»å‡»é£é™©ï¼Œç°æœ‰è§¦å‘å› ç´ æ˜æ˜¾ä¸”æ˜“æ£€æµ‹ã€‚</li>
<li>æå‡ºUIBDiffusionï¼Œä¸€ç§é€šç”¨éšè”½åé—¨æ”»å‡»æ–¹æ³•ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>UIBDiffusionåŸºäºé€šç”¨å¯¹æŠ—æ‰°åŠ¨ï¼ˆUAPsï¼‰ç”Ÿæˆæ–°å‹è§¦å‘å› ç´ ã€‚</li>
<li>UIBDiffusionå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šé€šç”¨æ€§ã€å®ç”¨æ€§å’Œä¸å¯æ£€æµ‹æ€§ã€‚</li>
<li>UIBDiffusioné€‚ç”¨äºå¤šç§ç±»å‹çš„æ‰©æ•£æ¨¡å‹å’Œé‡‡æ ·å™¨ï¼Œä»¥åŠå„ç§æ•°æ®é›†ã€‚</li>
<li>UIBDiffusionå¯å®ç°é«˜æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒä½æ¯’ç‡ä¸‹çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>UIBDiffusionå¯ç»•è¿‡ç°æœ‰é’ˆå¯¹æ‰©æ•£æ¨¡å‹åé—¨æ”»å‡»çš„é˜²å¾¡æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-edeca587cdedc0770facb73f4a4d2d6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36b14c527c95beef92764064544e8764.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af3d4d2ec217161864bef09a506f5eac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56817607714bf3c008636666384176f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce0c2c85ec53de7058db6bc8a2dd9993.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="High-Precision-Dichotomous-Image-Segmentation-via-Probing-Diffusion-Capacity"><a href="#High-Precision-Dichotomous-Image-Segmentation-via-Probing-Diffusion-Capacity" class="headerlink" title="High-Precision Dichotomous Image Segmentation via Probing Diffusion   Capacity"></a>High-Precision Dichotomous Image Segmentation via Probing Diffusion   Capacity</h2><p><strong>Authors:Qian Yu, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu</strong></p>
<p>In the realm of high-resolution (HR), fine-grained image segmentation, the primary challenge is balancing broad contextual awareness with the precision required for detailed object delineation, capturing intricate details and the finest edges of objects. Diffusion models, trained on vast datasets comprising billions of image-text pairs, such as SD V2.1, have revolutionized text-to-image synthesis by delivering exceptional quality, fine detail resolution, and strong contextual awareness, making them an attractive solution for high-resolution image segmentation. To this end, we propose DiffDIS, a diffusion-driven segmentation model that taps into the potential of the pre-trained U-Net within diffusion models, specifically designed for high-resolution, fine-grained object segmentation. By leveraging the robust generalization capabilities and rich, versatile image representation prior of the SD models, coupled with a task-specific stable one-step denoising approach, we significantly reduce the inference time while preserving high-fidelity, detailed generation. Additionally, we introduce an auxiliary edge generation task to not only enhance the preservation of fine details of the object boundaries, but reconcile the probabilistic nature of diffusion with the deterministic demands of segmentation. With these refined strategies in place, DiffDIS serves as a rapid object mask generation model, specifically optimized for generating detailed binary maps at high resolutions, while demonstrating impressive accuracy and swift processing. Experiments on the DIS5K dataset demonstrate the superiority of DiffDIS, achieving state-of-the-art results through a streamlined inference process. The source code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/qianyu-dlut/DiffDIS">https://github.com/qianyu-dlut/DiffDIS</a>. </p>
<blockquote>
<p>åœ¨é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰ç²¾ç»†ç²’åº¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå¹³è¡¡å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ„è¯†å’Œè¯¦ç»†å¯¹è±¡æç”»çš„ç²¾ç¡®åº¦ï¼Œæ•æ‰å¯¹è±¡çš„å¤æ‚ç»†èŠ‚å’Œæœ€ç²¾ç»†çš„è¾¹ç¼˜ã€‚æ‰©æ•£æ¨¡å‹åœ¨SD V2.1ç­‰åŒ…å«æ•°åäº¿å›¾åƒæ–‡æœ¬å¯¹çš„åºå¤§æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œé€šè¿‡æä¾›å‡ºè‰²çš„è´¨é‡ã€ç²¾ç»†çš„ç»†èŠ‚åˆ†è¾¨ç‡å’Œå¼ºå¤§çš„ä¸Šä¸‹æ–‡æ„è¯†ï¼Œå½»åº•æ”¹å˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆï¼Œä½¿å…¶æˆä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²çš„å¸å¼•è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DiffDISï¼Œä¸€ä¸ªç”±æ‰©æ•£é©±åŠ¨çš„åˆ†å‰²æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„é¢„è®­ç»ƒU-Netçš„æ½œåŠ›ï¼Œä¸“é—¨è®¾è®¡ç”¨äºé«˜åˆ†è¾¨ç‡ç²¾ç»†ç²’åº¦å¯¹è±¡åˆ†å‰²ã€‚é€šè¿‡åˆ©ç”¨SDæ¨¡å‹çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›å’Œä¸°å¯Œçš„é€šç”¨å›¾åƒè¡¨ç¤ºå…ˆéªŒï¼Œç»“åˆç‰¹å®šçš„ç¨³å®šå•æ­¥å»å™ªæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å‡å°‘æ¨ç†æ—¶é—´çš„åŒæ—¶ä¿æŒäº†é«˜ä¿çœŸå’Œè¯¦ç»†çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©è¾¹ç¼˜ç”Ÿæˆä»»åŠ¡ï¼Œä¸ä»…æœ‰åŠ©äºå¢å¼ºå¯¹è±¡è¾¹ç•Œçš„ç²¾ç»†ç»†èŠ‚çš„ä¿ç•™ï¼Œè€Œä¸”åè°ƒäº†æ‰©æ•£çš„æ¦‚ç‡æ€§ä¸åˆ†å‰²çš„ç¡®å®šæ€§éœ€æ±‚ã€‚é€šè¿‡å®æ–½è¿™äº›ç²¾ç‚¼ç­–ç•¥ï¼ŒDiffDISä½œä¸ºä¸€ä¸ªå¿«é€Ÿå¯¹è±¡æ©è†œç”Ÿæˆæ¨¡å‹ï¼Œä¸“é—¨ä¼˜åŒ–ç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è¯¦ç»†äºŒè¿›åˆ¶åœ°å›¾ï¼ŒåŒæ—¶å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å‡†ç¡®æ€§å’Œå¿«é€Ÿå¤„ç†ã€‚åœ¨DIS5Kæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†DiffDISçš„ä¼˜è¶Šæ€§ï¼Œé€šè¿‡ç®€åŒ–çš„æ¨ç†è¿‡ç¨‹å®ç°äº†æœ€å…ˆè¿›çš„æˆæœã€‚æºä»£ç å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/qianyu-dlut/DiffDIS%E3%80%82">https://github.com/qianyu-dlut/DiffDISã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10105v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ç»è¿‡å¤§é‡å›¾åƒæ–‡æœ¬å¯¹æ•°æ®çš„è®­ç»ƒï¼Œå¦‚SD V2.1ï¼Œå·²ç»å®ç°äº†é«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡çš„æ–‡æœ¬å›¾åƒåˆæˆã€‚é’ˆå¯¹é«˜åˆ†è¾¨ç‡ç²¾ç»†å›¾åƒåˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffDISæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒU-Netç»“æ„ï¼Œå¹¶å¼•å…¥è¾…åŠ©è¾¹ç¼˜ç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆè¯¦ç»†çš„äºŒè¿›åˆ¶åœ°å›¾ã€‚åœ¨DIS5Kæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†DiffDISçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²ç»åœ¨æ–‡æœ¬å›¾åƒåˆæˆé¢†åŸŸå±•ç°å‡ºé«˜è´¨é‡å’Œé«˜åˆ†è¾¨ç‡çš„èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹é«˜åˆ†è¾¨ç‡ç²¾ç»†å›¾åƒåˆ†å‰²çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ„è¯†å’Œç²¾ç»†çš„å¯¹è±¡è½®å»“æ¸…æ™°åº¦ã€‚</li>
<li>DiffDISæ¨¡å‹æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºçš„ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡ç²¾ç»†å›¾åƒåˆ†å‰²ã€‚</li>
<li>DiffDISåˆ©ç”¨é¢„è®­ç»ƒçš„U-Netç»“æ„å’ŒSDæ¨¡å‹çš„ä¸°å¯Œå›¾åƒè¡¨ç¤ºå…ˆéªŒçŸ¥è¯†ï¼Œç»“åˆä¸€æ­¥å»å™ªæ–¹æ³•ï¼Œå‡å°‘äº†æ¨ç†æ—¶é—´å¹¶ä¿æŒé«˜è´¨é‡ç”Ÿæˆã€‚</li>
<li>å¼•å…¥çš„è¾…åŠ©è¾¹ç¼˜ç”Ÿæˆä»»åŠ¡ä¸ä»…æé«˜äº†å¯¹è±¡è¾¹ç•Œçš„ç²¾ç»†ç»†èŠ‚ä¿ç•™ï¼Œè¿˜è°ƒå’Œäº†æ‰©æ•£çš„æ¦‚ç‡æ€§ä¸åˆ†å‰²çš„ç¡®å®šæ€§éœ€æ±‚ã€‚</li>
<li>DiffDISè¢«ä¼˜åŒ–ä¸ºå¿«é€Ÿç”Ÿæˆå¯¹è±¡æ©è†œæ¨¡å‹ï¼Œç‰¹åˆ«é€‚ç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è¯¦ç»†äºŒè¿›åˆ¶åœ°å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-857183236a8451ccde94fee519dc17f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6982a7382beeb93eefa5c2e3b5a585e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31edc1e1b196ba928312dac2e7a0ffaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be5bd53de8fda17fc629d352c267e818.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Representation-Alignment-for-Generation-Training-Diffusion-Transformers-Is-Easier-Than-You-Think"><a href="#Representation-Alignment-for-Generation-Training-Diffusion-Transformers-Is-Easier-Than-You-Think" class="headerlink" title="Representation Alignment for Generation: Training Diffusion Transformers   Is Easier Than You Think"></a>Representation Alignment for Generation: Training Diffusion Transformers   Is Easier Than You Think</h2><p><strong>Authors:Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie</strong></p>
<p>Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID&#x3D;1.42 using classifier-free guidance with the guidance interval. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œï¼ˆç”Ÿæˆå¼ï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„é™å™ªè¿‡ç¨‹å¯ä»¥åœ¨æ¨¡å‹å†…éƒ¨äº§ç”Ÿæœ‰æ„ä¹‰çš„ï¼ˆåˆ¤åˆ«å¼ï¼‰è¡¨ç¤ºï¼Œä½†è¿™äº›è¡¨ç¤ºçš„è´¨é‡ä»ç„¶è½åäºé€šè¿‡æœ€è¿‘çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å­¦ä¹ å¾—åˆ°çš„è¡¨ç¤ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè®­ç»ƒç”¨äºç”Ÿæˆçš„å¤§å‹æ‰©æ•£æ¨¡å‹çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°å­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œé€šè¿‡èå…¥é«˜è´¨é‡çš„å¤–éƒ¨è§†è§‰è¡¨ç¤ºï¼Œè€Œéä»…ä¾èµ–æ‰©æ•£æ¨¡å‹ç‹¬ç«‹å­¦ä¹ ï¼Œå¯ä»¥ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§åä¸ºREPresentation Alignmentï¼ˆREPAï¼‰çš„ç›´è§‚æ­£åˆ™åŒ–æ–¹æ³•è¿›è¡Œç ”ç©¶ï¼Œè¯¥æ–¹æ³•å°†é™å™ªç½‘ç»œä¸­å™ªå£°è¾“å…¥éšè—çŠ¶æ€çš„æŠ•å½±ä¸ä»å¤–éƒ¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨è·å¾—çš„å¹²å‡€å›¾åƒè¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚ç»“æœä»¤äººç©ç›®ï¼šå½“æˆ‘ä»¬æŠŠè¿™ä¸€ç®€å•ç­–ç•¥åº”ç”¨åˆ°æµè¡Œçš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºæµçš„å˜å‹å™¨ï¼ˆå¦‚DiTså’ŒSiTsï¼‰ä¸Šæ—¶ï¼Œä¸ä»…åœ¨è®­ç»ƒæ•ˆç‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¿˜åœ¨ç”Ÿæˆè´¨é‡ä¸Šå–å¾—äº†æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†SiTè®­ç»ƒé€Ÿåº¦æé«˜17.5å€ä»¥ä¸Šï¼Œåœ¨ä¸åˆ°40ä¸‡æ­¥çš„è®­ç»ƒä¸­ï¼Œå°±èƒ½è¾¾åˆ°åœ¨æ²¡æœ‰åˆ†ç±»å™¨å¼•å¯¼çš„æƒ…å†µä¸‹è®­ç»ƒäº†7ç™¾ä¸‡æ­¥çš„SiT-XLæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨æœ€ç»ˆçš„ç”Ÿæˆè´¨é‡æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼çš„æŒ‡å¯¼é—´éš”ä¸‹ï¼Œè¾¾åˆ°äº†FID&#x3D;1.42çš„æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06940v3">PDF</a> ICLR 2025 (Oral). Project page: <a target="_blank" rel="noopener" href="https://sihyun.me/REPA">https://sihyun.me/REPA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­çš„é™å™ªè¿‡ç¨‹å¯ä»¥äº§ç”Ÿæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œä½†è¿™äº›è¡¨ç¤ºçš„è´¨é‡ä»ç„¶è½åäºé€šè¿‡æœ€æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å­¦åˆ°çš„è¡¨ç¤ºã€‚æ–‡ç« æŒ‡å‡ºè®­ç»ƒå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¸»è¦ç“¶é¢ˆåœ¨äºæœ‰æ•ˆå­¦ä¹ è¿™äº›è¡¨ç¤ºï¼Œè€Œé€šè¿‡èå…¥é«˜è´¨é‡çš„å¤–éƒ¨è§†è§‰è¡¨ç¤ºï¼Œå¯ä»¥ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ç§åä¸ºREPAçš„ç›´è§‚æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å»å™ªç½‘ç»œä¸­å™ªå£°è¾“å…¥éšè—çŠ¶æ€çš„æŠ•å½±ä¸æ¥è‡ªå¤–éƒ¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„æ¸…æ´å›¾åƒè¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚æ­¤æ–¹æ³•åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºæµè¡Œçš„æ‰©æ•£å’ŒåŸºäºæµçš„è½¬æ¢å™¨ï¼Œå¦‚DiTså’ŒSiTsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„é™å™ªè¿‡ç¨‹å¯ä»¥äº§ç”Ÿæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œä½†è´¨é‡æœ‰å¾…æé«˜ã€‚</li>
<li>è®­ç»ƒå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„ä¸»è¦ç“¶é¢ˆåœ¨äºæœ‰æ•ˆå­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚</li>
<li>èå…¥é«˜è´¨é‡çš„å¤–éƒ¨è§†è§‰è¡¨ç¤ºå¯ä»¥ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºREPAçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¯¹é½å»å™ªç½‘ç»œä¸­å™ªå£°è¾“å…¥éšè—çŠ¶æ€çš„æŠ•å½±ä¸æ¸…æ´å›¾åƒè¡¨ç¤ºã€‚</li>
<li>REPAæ–¹æ³•åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ­¤æ–¹æ³•é€‚ç”¨äºæµè¡Œçš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºæµçš„è½¬æ¢å™¨ï¼Œå¦‚DiTså’ŒSiTsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6cd62471ab6369a11a5117fea28c92d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c6e26ab161a85109c401393e9e01f2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0acb44ce968a47a3fac81c18243170a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c347d9d0597ecbdc0251cdb81e4526be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33f86a298f407419a1b812c88535e044.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DKDM-Data-Free-Knowledge-Distillation-for-Diffusion-Models-with-Any-Architecture"><a href="#DKDM-Data-Free-Knowledge-Distillation-for-Diffusion-Models-with-Any-Architecture" class="headerlink" title="DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any   Architecture"></a>DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any   Architecture</h2><p><strong>Authors:Qianlong Xiang, Miao Zhang, Yuzhang Shang, Jianlong Wu, Yan Yan, Liqiang Nie</strong></p>
<p>Diffusion models (DMs) have demonstrated exceptional generative capabilities across various domains, including image, video, and so on. A key factor contributing to their effectiveness is the high quantity and quality of data used during training. However, mainstream DMs now consume increasingly large amounts of data. For example, training a Stable Diffusion model requires billions of image-text pairs. This enormous data requirement poses significant challenges for training large DMs due to high data acquisition costs and storage expenses. To alleviate this data burden, we propose a novel scenario: using existing DMs as data sources to train new DMs with any architecture. We refer to this scenario as Data-Free Knowledge Distillation for Diffusion Models (DKDM), where the generative ability of DMs is transferred to new ones in a data-free manner. To tackle this challenge, we make two main contributions. First, we introduce a DKDM objective that enables the training of new DMs via distillation, without requiring access to the data. Second, we develop a dynamic iterative distillation method that efficiently extracts time-domain knowledge from existing DMs, enabling direct retrieval of training data without the need for a prolonged generative process. To the best of our knowledge, we are the first to explore this scenario. Experimental results demonstrate that our data-free approach not only achieves competitive generative performance but also, in some instances, outperforms models trained with the entire dataset. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒã€è§†é¢‘ç­‰å„ç§é¢†åŸŸéƒ½å±•ç°å‡ºäº†å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ã€‚å…¶æœ‰æ•ˆæ€§çš„ä¸€ä¸ªé‡è¦å› ç´ æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„é«˜è´¨é‡å’Œå¤§é‡æ•°æ®ã€‚ç„¶è€Œï¼Œä¸»æµæ‰©æ•£æ¨¡å‹ç°åœ¨æ¶ˆè€—çš„æ•°æ®é‡è¶Šæ¥è¶Šå¤§ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒä¸€ä¸ªç¨³å®šæ‰©æ•£æ¨¡å‹éœ€è¦æ•°åäº¿å¼ å›¾åƒæ–‡æœ¬å¯¹ã€‚è¿™ç§å·¨å¤§çš„æ•°æ®éœ€æ±‚ç»™è®­ç»ƒå¤§å‹æ‰©æ•£æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®è·å–å’Œå­˜å‚¨æˆæœ¬éƒ½å¾ˆé«˜ã€‚ä¸ºäº†å‡è½»è¿™ä¸€æ•°æ®è´Ÿæ‹…ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°åœºæ™¯ï¼šä»¥ç°æœ‰æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•°æ®æºï¼Œä»¥ä»»ä½•æ¶æ„è®­ç»ƒæ–°çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†è¿™ç§åœºæ™¯ç§°ä¸ºæ— æ•°æ®çŸ¥è¯†è’¸é¦æ‰©æ•£æ¨¡å‹ï¼ˆDKDMï¼‰ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä»¥ä¸€ç§æ— éœ€æ•°æ®çš„æ–¹å¼è½¬ç§»åˆ°æ–°æ¨¡å‹ä¸Šã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªDKDMç›®æ ‡ï¼Œé€šè¿‡è’¸é¦è®­ç»ƒæ–°çš„æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ— éœ€è®¿é—®æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŠ¨æ€è¿­ä»£è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°ä»ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­æå–æ—¶é—´åŸŸçŸ¥è¯†ï¼Œå®ç°æ— éœ€æ¼«é•¿ç”Ÿæˆè¿‡ç¨‹å³å¯ç›´æ¥æ£€ç´¢è®­ç»ƒæ•°æ®ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢è¿™ç§åœºæ™¯çš„äººã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— æ•°æ®æ–¹æ³•ä¸ä»…å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç”Ÿæˆæ€§èƒ½ï¼Œè€Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¿˜ä¼˜äºä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03550v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒã€è§†é¢‘ç­‰é¢†åŸŸçš„å‡ºè‰²ç”Ÿæˆèƒ½åŠ›ï¼Œå…¶å…³é”®æˆåŠŸå› ç´ åœ¨äºè®­ç»ƒæ—¶ä½¿ç”¨çš„æ•°æ®é‡å¤§ä¸”è´¨é‡é«˜ã€‚ç„¶è€Œï¼Œä¸»æµDMséœ€è¦å¤§é‡æ•°æ®ï¼Œå¯¼è‡´æ•°æ®è·å–å’Œå­˜å‚¨æˆæœ¬é«˜æ˜‚ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼šåˆ©ç”¨ç°æœ‰DMsä½œä¸ºæ•°æ®æºï¼Œä»¥ä»»ä½•æ¶æ„è®­ç»ƒæ–°çš„DMsï¼Œç§°ä¸ºæ— æ•°æ®çŸ¥è¯†è’¸é¦æ‰©æ•£æ¨¡å‹ï¼ˆDKDMï¼‰ã€‚è¯¥æ–¹æ³•ä»¥æ— éœ€è®¿é—®æ•°æ®çš„æ–¹å¼å°†DMsçš„ç”Ÿæˆèƒ½åŠ›è½¬ç§»åˆ°æ–°æ¨¡å‹ä¸Šã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šä¸€æ˜¯å¼•å…¥DKDMç›®æ ‡ï¼Œä½¿æ–°DMså¯ä»¥é€šè¿‡è’¸é¦è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€è®¿é—®æ•°æ®ï¼›äºŒæ˜¯å¼€å‘äº†ä¸€ç§åŠ¨æ€è¿­ä»£è’¸é¦æ–¹æ³•ï¼Œä»ç°æœ‰DMsä¸­æœ‰æ•ˆåœ°æå–æ—¶é—´åŸŸçŸ¥è¯†ï¼Œå®ç°æ— éœ€æ¼«é•¿ç”Ÿæˆè¿‡ç¨‹å³å¯ç›´æ¥æ£€ç´¢è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ— æ•°æ®çš„æ–¹æ³•ä¸ä»…å…·æœ‰ç«äº‰åŠ›çš„ç”Ÿæˆæ€§èƒ½ï¼Œè€Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¾—ç›Šäºå…¶è®­ç»ƒæ—¶ä½¿ç”¨çš„å¤§é‡é«˜è´¨é‡æ•°æ®ã€‚</li>
<li>ä¸»æµDMsé¢ä¸´å·¨å¤§çš„æ•°æ®éœ€æ±‚æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜æ•°æ®è·å–å’Œå­˜å‚¨æˆæœ¬ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ— æ•°æ®çŸ¥è¯†è’¸é¦æ‰©æ•£æ¨¡å‹ï¼ˆDKDMï¼‰ï¼Œåˆ©ç”¨ç°æœ‰DMsä½œä¸ºæ•°æ®æºè®­ç»ƒæ–°æ¨¡å‹ã€‚</li>
<li>DKDMæ–¹æ³•ä»¥æ— éœ€è®¿é—®æ•°æ®çš„æ–¹å¼è½¬ç§»DMsçš„ç”Ÿæˆèƒ½åŠ›åˆ°æ–°çš„æ¨¡å‹ä¸Šã€‚</li>
<li>å¼•å…¥äº†DKDMç›®æ ‡ï¼Œä½¿æ–°DMsé€šè¿‡è’¸é¦è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼€å‘äº†åŠ¨æ€è¿­ä»£è’¸é¦æ–¹æ³•ï¼Œæœ‰æ•ˆæå–ç°æœ‰DMsçš„æ—¶é—´åŸŸçŸ¥è¯†ï¼Œæ— éœ€æ¼«é•¿ç”Ÿæˆè¿‡ç¨‹å³å¯ç›´æ¥æ£€ç´¢è®­ç»ƒæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0aa0bc3de649fd69a8cafd455ae53759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b260b9ba656d50c851783bc419c329.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1938860f73432542665af0c33401bbf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f384f1b9f2ad51b7ac6f1f4c3487f2f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66efe38e8ee889745f6afb2240d21c99.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce43373886968d0e37d6a04148b8b272.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  TomoSelfDEQ Self-Supervised Deep Equilibrium Learning for Sparse-Angle   CT Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-db677ba3cf80a36a7d7b6435dc4dff0f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  SV4D Dynamic 3D Content Generation with Multi-Frame and Multi-View   Consistency
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19758k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
