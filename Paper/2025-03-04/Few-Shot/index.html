<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  Improving Open-world Continual Learning under the Constraints of Scarce   Labeled Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-551c1277524d61403fce9c8d396b3de8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-04-æ›´æ–°"><a href="#2025-03-04-æ›´æ–°" class="headerlink" title="2025-03-04 æ›´æ–°"></a>2025-03-04 æ›´æ–°</h1><h2 id="Improving-Open-world-Continual-Learning-under-the-Constraints-of-Scarce-Labeled-Data"><a href="#Improving-Open-world-Continual-Learning-under-the-Constraints-of-Scarce-Labeled-Data" class="headerlink" title="Improving Open-world Continual Learning under the Constraints of Scarce   Labeled Data"></a>Improving Open-world Continual Learning under the Constraints of Scarce   Labeled Data</h2><p><strong>Authors:Yujie Li, Xiangkun Wang, Xin Yang, Marcello Bonsangue, Junbo Zhang, Tianrui Li</strong></p>
<p>Open-world continual learning (OWCL) adapts to sequential tasks with open samples, learning knowledge incrementally while preventing forgetting. However, existing OWCL still requires a large amount of labeled data for training, which is often impractical in real-world applications. Given that new categories&#x2F;entities typically come with limited annotations and are in small quantities, a more realistic situation is OWCL with scarce labeled data, i.e., few-shot training samples. Hence, this paper investigates the problem of open-world few-shot continual learning (OFCL), challenging in (i) learning unbounded tasks without forgetting previous knowledge and avoiding overfitting, (ii) constructing compact decision boundaries for open detection with limited labeled data, and (iii) transferring knowledge about knowns and unknowns and even update the unknowns to knowns once the labels of open samples are learned. In response, we propose a novel OFCL framework that integrates three key components: (1) an instance-wise token augmentation (ITA) that represents and enriches sample representations with additional knowledge, (2) a margin-based open boundary (MOB) that supports open detection with new tasks emerge over time, and (3) an adaptive knowledge space (AKS) that endows unknowns with knowledge for the updating from unknowns to knowns. Finally, extensive experiments show the proposed OFCL framework outperforms all baselines remarkably with practical importance and reproducibility. The source code is released at <a target="_blank" rel="noopener" href="https://github.com/liyj1201/OFCL">https://github.com/liyj1201/OFCL</a>. </p>
<blockquote>
<p>å¼€æ”¾ä¸–ç•ŒæŒç»­å­¦ä¹ ï¼ˆOWCLï¼‰é€‚åº”å…·æœ‰å¼€æ”¾æ ·æœ¬çš„è¿ç»­ä»»åŠ¡ï¼Œèƒ½å¤Ÿé€æ­¥å­¦ä¹ æ–°çŸ¥è¯†ï¼ŒåŒæ—¶é˜²æ­¢é—å¿˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„OWCLä»ç„¶éœ€è¦å¤§é‡æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­å¾€å¾€ä¸åˆ‡å®é™…ã€‚è€ƒè™‘åˆ°æ–°ç±»åˆ«&#x2F;å®ä½“é€šå¸¸å¸¦æœ‰æœ‰é™çš„æ³¨é‡Šå¹¶ä¸”æ•°é‡è¾ƒå°ï¼Œæ›´ç°å®çš„æƒ…å†µæ˜¯åœ¨ç¨€ç¼ºæ ‡è®°æ•°æ®ä¸‹çš„OWCLï¼Œå³å°‘æ•°è®­ç»ƒæ ·æœ¬ã€‚å› æ­¤ï¼Œæœ¬æ–‡ç ”ç©¶äº†å¼€æ”¾ä¸–ç•Œå°‘æ•°æ ·æœ¬æŒç»­å­¦ä¹ ï¼ˆOFCLï¼‰çš„é—®é¢˜ï¼Œé¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ï¼ˆiï¼‰å­¦ä¹ æ— ç•Œä»»åŠ¡ï¼Œä¸å¿˜æ‰ä»¥å‰çš„çŸ¥è¯†ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆï¼›ï¼ˆiiï¼‰ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®æ„å»ºç´§å‡‘çš„å†³ç­–è¾¹ç•Œï¼Œè¿›è¡Œå¼€æ”¾æ£€æµ‹ï¼›ï¼ˆiiiï¼‰è½¬ç§»å…³äºå·²çŸ¥å’ŒæœªçŸ¥çš„çŸ¥è¯†ï¼Œç”šè‡³ä¸€æ—¦å­¦ä¹ åˆ°å¼€æ”¾æ ·æœ¬çš„æ ‡ç­¾ï¼Œå°±å°†æœªçŸ¥æ›´æ–°ä¸ºå·²çŸ¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„OFCLæ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰å®ä¾‹çº§ä»¤ç‰Œå¢å¼ºï¼ˆITAï¼‰ï¼Œå®ƒç”¨é¢å¤–çš„çŸ¥è¯†è¡¨ç¤ºå’Œä¸°å¯Œæ ·æœ¬è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰åŸºäºè¾¹è·çš„å¼€æ”¾è¾¹ç•Œï¼ˆMOBï¼‰ï¼Œå®ƒæ”¯æŒéšæ—¶é—´å‡ºç°çš„æ–°ä»»åŠ¡çš„å¼€æ”¾æ£€æµ‹ï¼›ï¼ˆ3ï¼‰è‡ªé€‚åº”çŸ¥è¯†ç©ºé—´ï¼ˆAKSï¼‰ï¼Œå®ƒèµ‹äºˆæœªçŸ¥çŸ¥è¯†ï¼Œä»¥ä¾¿ä»æœªçŸ¥åˆ°å·²çŸ¥è¿›è¡Œæ›´æ–°ã€‚æœ€åï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„OFCLæ¡†æ¶åœ¨å®ç”¨æ€§å’Œå¯é‡å¤æ€§æ–¹é¢å‡ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚æºä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/liyj1201/OFCL%E3%80%82">https://github.com/liyj1201/OFCLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼€æ”¾ä¸–ç•Œå°æ ·æœ¬æŒç»­å­¦ä¹ ï¼ˆOFCLï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶çš„OFCLæ¡†æ¶ï¼ŒåŒ…æ‹¬å®ä¾‹çº§ä»¤ç‰Œå¢å¼ºï¼ˆITAï¼‰ã€åŸºäºè¾¹ç•Œçš„å¼€æ”¾è¾¹ç•Œï¼ˆMOBï¼‰å’Œè‡ªé€‚åº”çŸ¥è¯†ç©ºé—´ï¼ˆAKSï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸‹ï¼Œé€‚åº”ä¸æ–­æ¶Œç°çš„æ–°ä»»åŠ¡ï¼Œå®ç°å¼€æ”¾æ£€æµ‹ï¼ŒåŒæ—¶æ›´æ–°æœªçŸ¥çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å®é™…åº”ç”¨å’Œå¯é‡å¤æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾ä¸–ç•Œå°æ ·æœ¬æŒç»­å­¦ä¹ ï¼ˆOFCLï¼‰æ˜¯ä¸€ä¸ªç°å®ä¸­çš„é—®é¢˜ï¼Œå› ä¸ºæ–°ç±»åˆ«&#x2F;å®ä½“é€šå¸¸å¸¦æœ‰æœ‰é™çš„æ³¨é‡Šå¹¶ä¸”æ•°é‡è¾ƒå°ã€‚</li>
<li>OFCLé¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ï¼šåœ¨ä¸é—å¿˜å…ˆå‰çŸ¥è¯†çš„æƒ…å†µä¸‹å­¦ä¹ æ— ç•Œä»»åŠ¡ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆï¼›ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®æ„å»ºç”¨äºå¼€æ”¾æ£€æµ‹çš„æ›´ç´§å‡‘çš„å†³ç­–è¾¹ç•Œï¼›ä»¥åŠè½¬ç§»å…³äºå·²çŸ¥å’ŒæœªçŸ¥çš„çŸ¥è¯†ï¼Œç”šè‡³ä¸€æ—¦å­¦ä¹ åˆ°å¼€æ”¾çš„æ ·æœ¬æ ‡ç­¾ï¼Œå°±èƒ½å°†æœªçŸ¥æ›´æ–°ä¸ºå·²çŸ¥ã€‚</li>
<li>æå‡ºçš„OFCLæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå®ä¾‹çº§ä»¤ç‰Œå¢å¼ºï¼ˆITAï¼‰ï¼ŒåŸºäºè¾¹ç•Œçš„å¼€æ”¾è¾¹ç•Œï¼ˆMOBï¼‰å’Œè‡ªé€‚åº”çŸ¥è¯†ç©ºé—´ï¼ˆAKSï¼‰ã€‚</li>
<li>ITAç”¨äºè¡¨ç¤ºå’Œä¸°å¯Œæ ·æœ¬è¡¨ç¤ºï¼Œå¸¦æœ‰é¢å¤–çš„çŸ¥è¯†ã€‚</li>
<li>MOBæ”¯æŒéšæ—¶é—´å‡ºç°çš„æ–°ä»»åŠ¡çš„å¼€æ”¾æ£€æµ‹ã€‚</li>
<li>AKSèµ‹äºˆæœªçŸ¥çŸ¥è¯†æ›´æ–°çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ba66d84ab1cbabb870aece91596d3ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85bafc9583d05ab29b01e4bf0d98a9dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ac2bb1d27726631436171fbbcfeeea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57259b2c56458cd781901f2cf09676d7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-to-Substitute-Components-for-Compositional-Generalization"><a href="#Learning-to-Substitute-Components-for-Compositional-Generalization" class="headerlink" title="Learning to Substitute Components for Compositional Generalization"></a>Learning to Substitute Components for Compositional Generalization</h2><p><strong>Authors:Zhaoyi Li, Gangwei Jiang, Chenwang Wu, Ying Wei, Defu Lian, Enhong Chen</strong></p>
<p>Despite the rising prevalence of neural language models, recent empirical evidence suggests their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, which aims to introduce additional compositional inductive bias. However, existing handcrafted augmentation strategies offer limited improvement when systematic generalization of neural language models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases alone) or when training sentences have an imbalanced difficulty distribution. To address these challenges, we first propose a novel compositional augmentation strategy called Component Substitution (CompSub), which enables multi-grained composition of substantial substructures across the entire training set. Furthermore, we introduce the Learning Component Substitution (LCS) framework. This framework empowers the learning of component substitution probabilities in CompSub in an end-to-end manner by maximizing the loss of neural language models, thereby prioritizing challenging compositions with elusive concepts and novel contexts. We extend the key ideas of CompSub and LCS to the recently emerging in-context learning scenarios of pre-trained large language models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot compositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we provide insights into why applying our algorithms to language models can improve compositional generalization performance. Empirically, our results on four standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and COGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with improvements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively. </p>
<blockquote>
<p>å°½ç®¡ç¥ç»è¯­è¨€æ¨¡å‹çš„æ™®åŠç¨‹åº¦ä¸æ–­ä¸Šå‡ï¼Œä½†æœ€è¿‘çš„å®è¯è¯æ®è¡¨æ˜å®ƒä»¬åœ¨ç»„åˆæ³›åŒ–æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚ç›®å‰è§£å†³è¿™ä¸ªé—®é¢˜çš„å®é™…è§£å†³æ–¹æ¡ˆä¹‹ä¸€æ˜¯ç»„åˆæ•°æ®å¢å¼ºï¼Œå…¶æ—¨åœ¨å¼•å…¥é¢å¤–çš„ç»„åˆå½’çº³åè§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰‹å·¥å¢å¼ºç­–ç•¥ä»…åœ¨ç³»ç»Ÿæ³›åŒ–éœ€è¦å¤šç²’åº¦ç»„åˆåè§ï¼ˆå³ä¸é™äºè¯æ±‡æˆ–ç»“æ„åè§ï¼‰æˆ–è®­ç»ƒå¥å­éš¾åº¦åˆ†å¸ƒä¸å¹³è¡¡æ—¶æä¾›æœ‰é™çš„æ”¹è¿›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°çš„ç»„åˆå¢å¼ºç­–ç•¥ï¼Œç§°ä¸ºç»„ä»¶æ›¿æ¢ï¼ˆCompSubï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸­ç»„åˆå¤§é‡çš„é‡è¦å­ç»“æ„çš„å¤šç²’åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­¦ä¹ ç»„ä»¶æ›¿æ¢ï¼ˆLCSï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å¤§åŒ–ç¥ç»è¯­è¨€æ¨¡å‹çš„æŸå¤±æ¥èµ‹èƒ½ç»„ä»¶æ›¿æ¢æ¦‚ç‡çš„å­¦ä¹ ï¼Œä»è€Œä¼˜å…ˆå¤„ç†å…·æœ‰éš¾ä»¥æ‰æ‘¸çš„æ¦‚å¿µå’Œæ–°é¢–èƒŒæ™¯çš„å›°éš¾ç»„åˆã€‚æˆ‘ä»¬å°†CompSubå’ŒLCSçš„å…³é”®æ€æƒ³æ‰©å±•åˆ°æœ€è¿‘å‡ºç°çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯ï¼Œæå‡ºLCS-ICLç®—æ³•ï¼Œä»¥æé«˜æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰LLMçš„å°‘é‡ç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬æä¾›äº†å°†æˆ‘ä»¬çš„ç®—æ³•åº”ç”¨äºè¯­è¨€æ¨¡å‹å¯ä»¥æé«˜ç»„åˆæ³›åŒ–æ€§èƒ½çš„åŸå› ã€‚åœ¨å››ä¸ªæ ‡å‡†ç»„åˆæ³›åŒ–åŸºå‡†æµ‹è¯•ï¼ˆSCANã€COGSã€GeoQueryå’ŒCOGS-QLï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCompSubã€LCSå’ŒLCS-ICLçš„ä¼˜è¶Šæ€§ï¼Œæ”¹è¿›å¹…åº¦é«˜è¾¾66.5%ã€10.3%ã€1.4%å’Œ8.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20834v1">PDF</a> 23 pages, 9 figures, preprint, the extension paper of the paper   (arXiv:2306.02840)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºå°½ç®¡ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šæµè¡Œï¼Œä½†åœ¨ç»„åˆæ³›åŒ–æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚ç°æœ‰çš„æ‰‹å·¥è‰ºå“å¢å¼ºç­–ç•¥æä¾›çš„æ”¹è¿›æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šç²’åº¦ç»„åˆåå·®æˆ–è®­ç»ƒå¥å­éš¾åº¦åˆ†å¸ƒä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç»„åˆå¢å¼ºç­–ç•¥â€”â€”ç»„ä»¶æ›¿æ¢ï¼ˆCompSubï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸­ç»„åˆå¤šç²’åº¦çš„å®è´¨æ€§å­ç»“æ„ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å­¦ä¹ ç»„ä»¶æ›¿æ¢ï¼ˆLCSï¼‰æ¡†æ¶ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ ç»„ä»¶æ›¿æ¢æ¦‚ç‡ï¼Œé€šè¿‡æœ€å¤§åŒ–ç¥ç»ç½‘ç»œæ¨¡å‹çš„æŸå¤±æ¥ä¼˜å…ˆå¤„ç†å…·æœ‰éš¾ä»¥æ‰æ‘¸çš„æ¦‚å¿µå’Œæ–°é¢–ä¸Šä¸‹æ–‡çš„ç»„åˆã€‚æœ€åï¼Œå°†CompSubå’ŒLCSçš„å…³é”®æ€æƒ³æ‰©å±•åˆ°æ–°å…´çš„ä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯ï¼Œæå‡ºLCS-ICLç®—æ³•ï¼Œä»¥æé«˜æœ€å…ˆè¿›çš„LLMçš„å°‘é‡ç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚ç†è®ºåˆ†æå’Œå®è¯ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ç®—æ³•èƒ½æœ‰æ•ˆæé«˜è¯­è¨€æ¨¡å‹çš„ç»„åˆæ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ³›åŒ–æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šç²’åº¦ç»„åˆåå·®æˆ–è®­ç»ƒå¥å­éš¾åº¦åˆ†å¸ƒä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç»„åˆå¢å¼ºç­–ç•¥â€”â€”ç»„ä»¶æ›¿æ¢ï¼ˆCompSubï¼‰ï¼Œèƒ½æ•´åˆå¤šç²’åº¦çš„å­ç»“æ„ã€‚</li>
<li>å¼•å…¥äº†å­¦ä¹ ç»„ä»¶æ›¿æ¢ï¼ˆLCSï¼‰æ¡†æ¶ï¼Œä»¥æœ€å¤§åŒ–ç¥ç»ç½‘ç»œæ¨¡å‹çš„æŸå¤±æ¥å­¦ä¹ ç»„ä»¶æ›¿æ¢æ¦‚ç‡ã€‚</li>
<li>å°†CompSubå’ŒLCSçš„æ€æƒ³æ‰©å±•åˆ°ä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯ï¼Œæå‡ºLCS-ICLç®—æ³•ï¼Œæé«˜LLMçš„å°‘é‡ç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æä¾›çš„ç®—æ³•èƒ½æœ‰æ•ˆæé«˜è¯­è¨€æ¨¡å‹çš„ç»„åˆæ³›åŒ–æ€§èƒ½ï¼Œè¿™åœ¨å››ä¸ªæ ‡å‡†ç»„åˆæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>é€šè¿‡ç†è®ºåˆ†æï¼Œè§£é‡Šäº†ä¸ºä½•å°†è¿™äº›ç®—æ³•åº”ç”¨äºè¯­è¨€æ¨¡å‹èƒ½æé«˜ç»„åˆæ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb38ead256155fbbefa6628164b97e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75ae7d0ffe5be5b48a67b1e8a47affac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec3f7ebd01c47ba5a65593040c54d8d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Can-We-Simplify-Slide-level-Fine-tuning-of-Pathology-Foundation-Models"><a href="#Can-We-Simplify-Slide-level-Fine-tuning-of-Pathology-Foundation-Models" class="headerlink" title="Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?"></a>Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</h2><p><strong>Authors:Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He</strong></p>
<p>The emergence of foundation models in computational pathology has transformed histopathological image analysis, with whole slide imaging (WSI) diagnosis being a core application. Traditionally, weakly supervised fine-tuning via multiple instance learning (MIL) has been the primary method for adapting foundation models to WSIs. However, in this work we present a key experimental finding: a simple nonlinear mapping strategy combining mean pooling and a multilayer perceptron, called SiMLP, can effectively adapt patch-level foundation models to slide-level tasks without complex MIL-based learning. Through extensive experiments across diverse downstream tasks, we demonstrate the superior performance of SiMLP with state-of-the-art methods. For instance, on a large-scale pan-cancer classification task, SiMLP surpasses popular MIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in few-shot classification and remaining highly competitive with slide-level foundation models pretrained on tens of thousands of slides. Finally, SiMLP exhibits remarkable robustness and transferability in lung cancer subtyping. Overall, our findings challenge the conventional MIL-based fine-tuning paradigm, demonstrating that a task-agnostic representation strategy alone can effectively adapt foundation models to WSI analysis. These insights offer a unique and meaningful perspective for future research in digital pathology, paving the way for more efficient and broadly applicable methodologies. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦ä¸­çš„åŸºç¡€æ¨¡å‹çš„å…´èµ·å·²ç»æ”¹å˜äº†ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†æçš„å±€é¢ï¼Œå…¶ä¸­å…¨å¹»ç¯ç‰‡æˆåƒï¼ˆWSIï¼‰è¯Šæ–­æ˜¯æ ¸å¿ƒåº”ç”¨ä¹‹ä¸€ã€‚ä¼ ç»Ÿä¸Šï¼Œé€šè¿‡å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰è¿›è¡Œå¼±ç›‘ç£å¾®è°ƒæ˜¯ä½¿åŸºç¡€æ¨¡å‹é€‚åº”WSIçš„ä¸»è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé‡è¦çš„å®éªŒå‘ç°ï¼šä¸€ç§ç»“åˆå‡å€¼æ± åŒ–å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„ç®€å•éçº¿æ€§æ˜ å°„ç­–ç•¥ï¼Œç§°ä¸ºSiMLPï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è¡¥ä¸çº§åˆ«çš„åŸºç¡€æ¨¡å‹é€‚åº”åˆ°å¹»ç¯ç‰‡çº§åˆ«çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€å¤æ‚çš„åŸºäºMILçš„å­¦ä¹ ã€‚é€šè¿‡åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†SiMLPåœ¨æœ€æ–°æŠ€æœ¯æ–¹æ³•ä¸­çš„å“è¶Šæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå¤§è§„æ¨¡æ³›ç™Œåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSiMLPåœ¨æµè¡Œçš„åŸºäºMILçš„æ–¹æ³•çš„åŸºç¡€ä¸Šæé«˜äº†3.52%ã€‚æ­¤å¤–ï¼ŒSiMLPåœ¨å°‘æ ·æœ¬åˆ†ç±»ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ•°ä»¥ä¸‡è®¡å¹»ç¯ç‰‡ä¸Šé¢„è®­ç»ƒè¿‡çš„å¹»ç¯ç‰‡çº§åˆ«åŸºç¡€æ¨¡å‹ä¸­ä¿æŒç«äº‰åŠ›ã€‚æœ€åï¼ŒSiMLPåœ¨è‚ºç™Œåˆ†å‹ä¸­è¡¨ç°å‡ºä»¤äººç©ç›®çš„é²æ£’æ€§å’Œå¯è¿ç§»æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„åŸºäºMILçš„å¾®è°ƒæ¨¡å¼ï¼Œè¯æ˜äº†å•ä¸€çš„ä»»åŠ¡ä¸å¯çŸ¥è¡¨ç¤ºç­–ç•¥å°±å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”åŸºç¡€æ¨¡å‹åˆ°WSIåˆ†æã€‚è¿™äº›è§è§£ä¸ºæ•°å­—ç—…ç†å­¦çš„æœªæ¥ç ”ç©¶æä¾›äº†ç‹¬ç‰¹è€Œæœ‰æ„ä¹‰çš„è§†è§’ï¼Œä¸ºæ›´æœ‰æ•ˆç‡ä¸”æ›´å¹¿æ³›é€‚ç”¨çš„æ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20823v1">PDF</a> 11 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—ç—…ç†å­¦ä¸­åŸºç¡€æ¨¡å‹çš„å…´èµ·ï¼Œå…¨å¹»ç¯ç‰‡æˆåƒï¼ˆWSIï¼‰è¯Šæ–­æˆä¸ºå…¶æ ¸å¿ƒåº”ç”¨ä¹‹ä¸€ã€‚ä¼ ç»Ÿä¸Šï¼Œé€šè¿‡å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰è¿›è¡Œå¼±ç›‘ç£å¾®è°ƒæ˜¯é€‚åº”WSIçš„åŸºç¡€æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„éçº¿æ€§æ˜ å°„ç­–ç•¥â€”â€”SiMLPï¼ˆç»“åˆå‡å€¼æ± åŒ–å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è¡¥ä¸çº§åˆ«çš„åŸºç¡€æ¨¡å‹é€‚åº”äºå¹»ç¯ç‰‡çº§åˆ«çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€å¤æ‚çš„åŸºäºMILçš„å­¦ä¹ ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œæˆ‘ä»¬è¯æ˜äº†SiMLPçš„ä¼˜è¶Šæ€§ï¼Œä¾‹å¦‚åœ¨å¤§å‹æ³›ç™Œåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSiMLPè¶…è¶Šäº†æµè¡Œçš„åŸºäºMILçš„æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†3.52%ã€‚æ­¤å¤–ï¼ŒSiMLPåœ¨å°‘æ ·æœ¬åˆ†ç±»ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨å¹»ç¯ç‰‡çº§åˆ«é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ä¸Šä»ç„¶å…·æœ‰ç«äº‰åŠ›ã€‚æœ€åï¼ŒSiMLPåœ¨è‚ºç™Œåˆ†å‹ä¸­è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§å’Œå¯è¿ç§»æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„åŸºäºMILçš„å¾®è°ƒèŒƒå¼ï¼Œè¯æ˜äº†ä»»åŠ¡æ— å…³çš„è¡¨ç¤ºç­–ç•¥å¯ä»¥å•ç‹¬æœ‰æ•ˆåœ°é€‚åº”WSIåˆ†æçš„åŸºç¡€æ¨¡å‹ã€‚è¿™äº›è§è§£ä¸ºæ•°å­—ç—…ç†å­¦æœªæ¥çš„ç ”ç©¶æä¾›äº†ç‹¬ç‰¹å’Œæœ‰æ„ä¹‰çš„è§†è§’ï¼Œä¸ºæ›´æœ‰æ•ˆç‡ä¸”æ›´å¹¿æ³›é€‚ç”¨çš„æ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥åŸºç¡€æ¨¡å‹æ”¹å˜äº†è®¡ç®—ç—…ç†å­¦ä¸­çš„å…¨å¹»ç¯ç‰‡æˆåƒï¼ˆWSIï¼‰è¯Šæ–­æ–¹å¼ã€‚</li>
<li>ä¼ ç»Ÿä¸Šï¼ŒåŸºäºå¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰çš„å¼±ç›‘ç£å¾®è°ƒæ˜¯é€‚åº”WSIçš„ä¸»è¦æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•çš„éçº¿æ€§æ˜ å°„ç­–ç•¥SiMLPï¼Œæ— éœ€å¤æ‚çš„åŸºäºMILçš„å­¦ä¹ å³å¯é€‚åº”æ¨¡å‹ã€‚</li>
<li>åœ¨å¤§å‹æ³›ç™Œåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSiMLPè¶…è¶Šäº†åŸºäºMILçš„æ–¹æ³•ã€‚</li>
<li>SiMLPåœ¨å°‘æ ·æœ¬åˆ†ç±»æ–¹é¢å…·æœ‰å¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶åœ¨å¹»ç¯ç‰‡çº§åˆ«é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ä¸Šè¡¨ç°ç«äº‰åŠ›ã€‚</li>
<li>SiMLPåœ¨è‚ºç™Œåˆ†å‹ä¸­æ˜¾ç¤ºå‡ºç¨³å¥æ€§å’Œå¯è¿ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6354e1485dd08a57eb4d4d98041243ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55944973048a2f1d9c8df35af60410d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c728361a91890f71be2d97440b490920.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e50fa95993bd9a2337de62c3c664704.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-728bf60ef3ebb8f4dc48804aeb776663.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Few-Shot-No-Problem-Descriptive-Continual-Relation-Extraction"><a href="#Few-Shot-No-Problem-Descriptive-Continual-Relation-Extraction" class="headerlink" title="Few-Shot, No Problem: Descriptive Continual Relation Extraction"></a>Few-Shot, No Problem: Descriptive Continual Relation Extraction</h2><p><strong>Authors:Nguyen Xuan Thanh, Anh Duc Le, Quyen Tran, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen</strong></p>
<p>Few-shot Continual Relation Extraction is a crucial challenge for enabling AI systems to identify and adapt to evolving relationships in dynamic real-world domains. Traditional memory-based approaches often overfit to limited samples, failing to reinforce old knowledge, with the scarcity of data in few-shot scenarios further exacerbating these issues by hindering effective data augmentation in the latent space. In this paper, we propose a novel retrieval-based solution, starting with a large language model to generate descriptions for each relation. From these descriptions, we introduce a bi-encoder retrieval training paradigm to enrich both sample and class representation learning. Leveraging these enhanced representations, we design a retrieval-based prediction method where each sample â€œretrievesâ€ the best fitting relation via a reciprocal rank fusion score that integrates both relation description vectors and class prototypes. Extensive experiments on multiple datasets demonstrate that our method significantly advances the state-of-the-art by maintaining robust performance across sequential tasks, effectively addressing catastrophic forgetting. </p>
<blockquote>
<p>å°‘æ ·æœ¬æŒç»­å…³ç³»æŠ½å–æ˜¯ä½¿AIç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å’Œé€‚åº”åŠ¨æ€ç°å®é¢†åŸŸä¸­çš„ä¸æ–­å˜åŒ–å…³ç³»çš„å…³é”®æŒ‘æˆ˜ã€‚åŸºäºä¼ ç»Ÿçš„è®°å¿†æ–¹æ³•å¾€å¾€ä¼šå¯¹æœ‰é™çš„æ ·æœ¬è¿‡åº¦æ‹Ÿåˆï¼Œæ— æ³•å·©å›ºæ—§çŸ¥è¯†ï¼Œè€Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œæ•°æ®çš„ç¨€ç¼ºæ€§è¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›é—®é¢˜ï¼Œé˜»ç¢äº†æ½œåœ¨ç©ºé—´ä¸­çš„æœ‰æ•ˆæ•°æ®å¢å¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ£€ç´¢çš„è§£å†³æ–¹æ¡ˆï¼Œé¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªå…³ç³»ç”Ÿæˆæè¿°ã€‚ä»è¿™äº›æè¿°ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒç¼–ç å™¨æ£€ç´¢è®­ç»ƒæ¨¡å¼ï¼Œä»¥ä¸°å¯Œæ ·æœ¬å’Œç±»åˆ«è¡¨ç¤ºå­¦ä¹ ã€‚åˆ©ç”¨è¿™äº›å¢å¼ºçš„è¡¨ç¤ºå½¢å¼ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ£€ç´¢çš„é¢„æµ‹æ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬é€šè¿‡èåˆå…³ç³»æè¿°å‘é‡å’Œç±»åˆ«åŸå‹çš„åå‘æ’åèåˆå¾—åˆ†æ¥æ£€ç´¢æœ€é€‚åˆçš„å…³ç³»ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åºåˆ—ä»»åŠ¡ä¸Šä¿æŒäº†ç¨³å¥çš„æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæ˜¾è‘—åœ°æé«˜äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20596v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°‘æ ·æœ¬æŒç»­å…³ç³»æŠ½å–çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¯ä¸ªå…³ç³»çš„æè¿°ï¼Œå¹¶å¼•å…¥åŒç¼–ç å™¨æ£€ç´¢è®­ç»ƒèŒƒå¼ï¼Œä¸°å¯Œæ ·æœ¬å’Œç±»åˆ«è¡¨ç¤ºå­¦ä¹ ã€‚åŸºäºè¿™äº›å¢å¼ºè¡¨ç¤ºï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ£€ç´¢çš„é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡èåˆå…³ç³»æè¿°å‘é‡å’Œç±»åˆ«åŸå‹ï¼Œå®ç°æ ·æœ¬ä¸æœ€ä½³æ‹Ÿåˆå…³ç³»çš„åŒ¹é…ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åºè´¯ä»»åŠ¡ä¸Šæ€§èƒ½ç¨³å¥ï¼Œæœ‰æ•ˆè§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»å°‘æ ·æœ¬æŒç»­å…³ç³»æŠ½å–çš„é‡è¦æ€§ï¼Œä½¿AIç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å’Œé€‚åº”åŠ¨æ€ç°å®ä¸–ç•Œé¢†åŸŸä¸­ä¸æ–­å˜åŒ–çš„å…³ç³»ã€‚</li>
<li>ä¼ ç»ŸåŸºäºè®°å¿†çš„æ–¹æ³•åœ¨æœ‰é™æ ·æœ¬ä¸Šè¿‡æ‹Ÿåˆï¼Œæ— æ³•å·©å›ºæ—§çŸ¥è¯†ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œæ•°æ®ç¨€ç¼ºæ€§åŠ å‰§äº†è¿™äº›é—®é¢˜ï¼Œé˜»ç¢äº†æ½œåœ¨ç©ºé—´ä¸­çš„æœ‰æ•ˆæ•°æ®å¢å¼ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå…³ç³»æè¿°ã€‚</li>
<li>å¼•å…¥åŒç¼–ç å™¨æ£€ç´¢è®­ç»ƒèŒƒå¼ï¼Œä¸°å¯Œæ ·æœ¬å’Œç±»åˆ«çš„è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºæ£€ç´¢çš„é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡èåˆå…³ç³»æè¿°å‘é‡å’Œç±»åˆ«åŸå‹ï¼Œå®ç°æ ·æœ¬ä¸æœ€ä½³æ‹Ÿåˆå…³ç³»çš„åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab4938e7e0c7a226fea603e65b3c5a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58bf6acc8315dbf6c75aace37fa51103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9a5f77096c14420f8268791d9236586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f1f6e7bb5734d9011b59222cf458f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb4a65d02d237f54882e9504e357289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bed4b8df45b80b96c72987ee93fdf65.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Training-Elicits-Concise-Reasoning-in-Large-Language-Models"><a href="#Self-Training-Elicits-Concise-Reasoning-in-Large-Language-Models" class="headerlink" title="Self-Training Elicits Concise Reasoning in Large Language Models"></a>Self-Training Elicits Concise Reasoning in Large Language Models</h2><p><strong>Authors:Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun</strong></p>
<p>Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TergelMunkhbat/concise-reasoning">https://github.com/TergelMunkhbat/concise-reasoning</a> </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ä¸­é—´ä»¤ç‰Œåˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥è§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºå…¸å‹çš„æ¨ç†è½¨è¿¹åŒ…å«è®¸å¤šå†—ä½™çš„ä»¤ç‰Œï¼Œäº§ç”Ÿäº†é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚åœ¨æ£€æŸ¥å½“å‰LLMçš„è¾“å‡ºåˆ†å¸ƒæ—¶ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬ç›¸å¯¹äºé»˜è®¤è¡Œä¸ºå…·æœ‰æ›´ç®€æ´æ¨ç†çš„æ½œåœ¨èƒ½åŠ›ã€‚ä¸ºäº†æ¿€å‘è¿™ä¸€èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ç®€å•çš„å¾®è°ƒæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åˆ©ç”¨é€šè¿‡æœ€ä½³Né‡‡æ ·å’Œå°‘æ ·æœ¬æ¡ä»¶åœ¨ç‰¹å®šä»»åŠ¡ç¯å¢ƒä¸­è·å¾—çš„è‡ªæˆ‘ç”Ÿæˆçš„ç®€æ´æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬çš„ç»„åˆæ–¹æ³•åœ¨GSM8Kå’ŒMATHä¸Šå¹³å‡å‡å°‘äº†è¾“å‡ºä»¤ç‰Œæ•°çš„30%ï¼ŒåŒæ—¶ä¿æŒå¹³å‡å‡†ç¡®ç‡ã€‚é€šè¿‡åˆ©ç”¨LLMçš„åŸºæœ¬éšæœºæ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„è‡ªè®­ç»ƒæ–¹æ³•åœ¨å„ç§æ¨¡å‹ä¸Šéƒ½èƒ½ç¨³å¥åœ°å¼•å‘ç®€æ´æ¨ç†ï¼ŒåŒ…æ‹¬é‚£äº›ç»è¿‡å¹¿æ³›è®­ç»ƒåçš„æ¨¡å‹ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/TergelMunkhbat/concise-reasoning">https://github.com/TergelMunkhbat/concise-reasoning</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20122v2">PDF</a> 23 pages, 10 figures, 18 tables</p>
<p><strong>Summary</strong></p>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ä¸­é—´æ ‡è®°åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é€šå¸¸çš„æ¨ç†è¿‡ç¨‹åŒ…å«è®¸å¤šå†—ä½™æ ‡è®°ï¼Œäº§ç”Ÿäº†é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ç®€å•å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªæˆ‘ç”Ÿæˆçš„ç®€æ´æ¨ç†è·¯å¾„ï¼Œé€šè¿‡æœ€ä½³Né‡‡æ ·å’Œå°‘æ ·æœ¬æ¡ä»¶ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ç¯å¢ƒä¸­å®ç°å¹³å‡è¾“å‡ºæ ‡è®°å‡å°‘30%ï¼ŒåŒæ—¶åœ¨GSM8Kå’ŒMATHä¸Šç»´æŒå¹³å‡å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„è‡ªæˆ‘è®­ç»ƒæ–¹æ³•åˆ©ç”¨LLMçš„åŸºæœ¬éšæœºæ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½åœ¨å„ç§æ¨¡å‹ä¸Šç¨³å¥åœ°å®ç°ç®€æ´æ¨ç†ï¼ŒåŒ…æ‹¬é‚£äº›ç»è¿‡å¤§é‡è®­ç»ƒåçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³å¤æ‚ä»»åŠ¡æ—¶åˆ©ç”¨é¢å¤–è®¡ç®—ã€‚</li>
<li>å…¸å‹çš„æ¨ç†è¿‡ç¨‹åŒ…å«è®¸å¤šå†—ä½™æ ‡è®°ï¼Œå¯¼è‡´é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚</li>
<li>æå‡ºç®€å•å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªæˆ‘ç”Ÿæˆçš„ç®€æ´æ¨ç†è·¯å¾„ã€‚</li>
<li>é€šè¿‡æœ€ä½³Né‡‡æ ·å’Œå°‘æ ·æœ¬æ¡ä»¶ï¼Œå®ç°äº†å¹³å‡è¾“å‡ºæ ‡è®°å‡å°‘30%ã€‚</li>
<li>åœ¨ç‰¹å®šä»»åŠ¡ç¯å¢ƒä¸­ç»´æŒäº†å¹³å‡å‡†ç¡®ç‡ã€‚</li>
<li>åˆ©ç”¨LLMçš„åŸºæœ¬éšæœºæ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæ–¹æ³•èƒ½åœ¨å„ç§æ¨¡å‹ä¸Šå®ç°ç®€æ´æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4d211e5f506dc17daa0eda94c8969b14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf4df0a272bce5a46a6179100a08a64a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4f49fd89aedf26a88de006267ac2a9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52a205504d20d5833915ab13c9833aa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a0742ff26a61b535f9435f3053d2465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5718763a438227758f20955cf1036cf1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="3D-Prior-is-All-You-Need-Cross-Task-Few-shot-2D-Gaze-Estimation"><a href="#3D-Prior-is-All-You-Need-Cross-Task-Few-shot-2D-Gaze-Estimation" class="headerlink" title="3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation"></a>3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation</h2><p><strong>Authors:Yihua Cheng, Hengfei Wang, Zhongqun Zhang, Yang Yue, Bo Eun Kim, Feng Lu, Hyung Jin Chang</strong></p>
<p>3D and 2D gaze estimation share the fundamental objective of capturing eye movements but are traditionally treated as two distinct research domains. In this paper, we introduce a novel cross-task few-shot 2D gaze estimation approach, aiming to adapt a pre-trained 3D gaze estimation network for 2D gaze prediction on unseen devices using only a few training images. This task is highly challenging due to the domain gap between 3D and 2D gaze, unknown screen poses, and limited training data. To address these challenges, we propose a novel framework that bridges the gap between 3D and 2D gaze. Our framework contains a physics-based differentiable projection module with learnable parameters to model screen poses and project 3D gaze into 2D gaze. The framework is fully differentiable and can integrate into existing 3D gaze networks without modifying their original architecture. Additionally, we introduce a dynamic pseudo-labelling strategy for flipped images, which is particularly challenging for 2D labels due to unknown screen poses. To overcome this, we reverse the projection process by converting 2D labels to 3D space, where flipping is performed. Notably, this 3D space is not aligned with the camera coordinate system, so we learn a dynamic transformation matrix to compensate for this misalignment. We evaluate our method on MPIIGaze, EVE, and GazeCapture datasets, collected respectively on laptops, desktop computers, and mobile devices. The superior performance highlights the effectiveness of our approach, and demonstrates its strong potential for real-world applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è·¨ä»»åŠ¡å°æ ·æœ¬2Dçœ¼åŠ¨ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨é¢„è®­ç»ƒçš„3Dçœ¼åŠ¨ä¼°è®¡ç½‘ç»œå¯¹æœªè§è¿‡çš„è®¾å¤‡è¿›è¡ŒåŸºäºå°‘é‡è®­ç»ƒå›¾åƒçš„2Dçœ¼åŠ¨é¢„æµ‹ã€‚ç”±äº3Då’Œ2Dçœ¼åŠ¨ä¹‹é—´çš„é¢†åŸŸå·®è·ã€æœªçŸ¥çš„å±å¹•å§¿æ€ä»¥åŠæœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œæ­¤ä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶æ¥å¼¥è¡¥3Då’Œ2Dçœ¼åŠ¨ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸€ä¸ªåŸºäºç‰©ç†çš„å¯å¾®åˆ†æŠ•å½±æ¨¡å—ï¼Œå…·æœ‰å¯å­¦ä¹ å‚æ•°æ¥æ¨¡æ‹Ÿå±å¹•å§¿æ€å¹¶å°†3Dçœ¼åŠ¨æŠ•å½±åˆ°2Dçœ¼åŠ¨ã€‚è¯¥æ¡†æ¶æ˜¯å®Œå…¨å¯å¾®åˆ†çš„ï¼Œå¹¶ä¸”å¯ä»¥é›†æˆåˆ°ç°æœ‰çš„3Dçœ¼åŠ¨ç½‘ç»œä¸­ï¼Œæ— éœ€ä¿®æ”¹å…¶åŸå§‹æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€ä¼ªæ ‡ç­¾ç­–ç•¥æ¥å¤„ç†ç¿»è½¬å›¾åƒï¼Œè¿™å¯¹äºå…·æœ‰æœªçŸ¥å±å¹•å§¿æ€çš„2Dæ ‡ç­¾æ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é€šè¿‡å°†2Dæ ‡ç­¾è½¬æ¢ä¸ºä¸‰ç»´ç©ºé—´æ¥åè½¬æŠ•å½±è¿‡ç¨‹ï¼Œå¹¶åœ¨é‚£é‡Œæ‰§è¡Œç¿»è½¬æ“ä½œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªä¸‰ç»´ç©ºé—´ä¸ä¸ç›¸æœºåæ ‡ç³»å¯¹é½ï¼Œå› æ­¤æˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªåŠ¨æ€è½¬æ¢çŸ©é˜µæ¥è¡¥å¿è¿™ç§ä¸å¯¹é½ã€‚æˆ‘ä»¬åœ¨MPIIGazeã€EVEå’ŒGazeCaptureæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›æ•°æ®é›†åˆ†åˆ«åœ¨ç¬”è®°æœ¬ç”µè„‘ã€å°å¼ç”µè„‘å’Œç§»åŠ¨è®¾å¤‡ä¸Šæ”¶é›†ã€‚å‡ºè‰²çš„æ€§èƒ½å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04074v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨ä»»åŠ¡å°æ ·æœ¬2Dçœ¼åŠ¨æ³¨è§†ç‚¹ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„3Dçœ¼åŠ¨æ³¨è§†ç‚¹ä¼°è®¡ç½‘ç»œå¯¹æœªè§è¿‡çš„è®¾å¤‡è¿›è¡Œå°‘é‡è®­ç»ƒå›¾åƒå³å¯è¿›è¡Œ2Dçœ¼åŠ¨æ³¨è§†ç‚¹é¢„æµ‹ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†å¯å¾®åˆ†æŠ•å½±æ¨¡å—å»ºæ¨¡å±å¹•å§¿æ€å¹¶å°†3Dçœ¼åŠ¨æ³¨è§†ç‚¹æŠ•å½±åˆ°2Dç©ºé—´ï¼ŒåŒæ—¶é‡‡ç”¨åŠ¨æ€ä¼ªæ ‡ç­¾ç­–ç•¥åº”å¯¹ç¿»è½¬å›¾åƒçš„æŒ‘æˆ˜ã€‚åœ¨MPIIGazeã€EVEå’ŒGazeCaptureæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°å…¶çœŸå®ä¸–ç•Œåº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†å°†é¢„è®­ç»ƒçš„3Dçœ¼åŠ¨æ³¨è§†ç‚¹ä¼°è®¡ç½‘ç»œåº”ç”¨äºæœªè§è®¾å¤‡çš„å°æ ·æœ¬ä¸‹çš„2Dçœ¼åŠ¨æ³¨è§†ç‚¹é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨ä»»åŠ¡æ–¹æ³•ï¼Œä½¿ç”¨å¯å¾®åˆ†æŠ•å½±æ¨¡å—å°†3Då’Œ2Dçœ¼åŠ¨æ³¨è§†ç‚¹è”ç³»ç´§å¯†èµ·æ¥ã€‚è¯¥æ¨¡å—èƒ½å»ºæ¨¡å±å¹•å§¿æ€å¹¶å®ç°ä»3Dåˆ°2Dçš„æŠ•å½±ã€‚</li>
<li>å¼•å…¥äº†åŠ¨æ€ä¼ªæ ‡ç­¾ç­–ç•¥å¤„ç†ç¿»è½¬å›¾åƒå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å°†2Dæ ‡ç­¾è½¬åŒ–ä¸ºç›¸åº”çš„ç‰©ç†ç©ºé—´çš„åšæ³•æ¥å¤„ç†è¿™ä¸€æŒ‘æˆ˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯è½¬æ¢åçš„ç©ºé—´ä¸ç›¸æœºåæ ‡ç³»ä¸ä¸€è‡´ï¼Œå› æ­¤å¼•å…¥äº†åŠ¨æ€è½¬æ¢çŸ©é˜µæ¥ä¿®æ­£è¿™ä¸€å·®å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35e8eb992b5c0d1f607702759e87750e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8b9a8c24828382d605e144d5c6eb903.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8ef542cda50013ea21d98b66fc923a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7889d33c9089358d01601dc03b0178ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76b6fa6144fa1463bf28fd443033cfe9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Context-Aware-Label-Propagation-for-Zero-Few-Shot-Training-Free-Adaptation-of-Vision-Language-Model"><a href="#Efficient-and-Context-Aware-Label-Propagation-for-Zero-Few-Shot-Training-Free-Adaptation-of-Vision-Language-Model" class="headerlink" title="Efficient and Context-Aware Label Propagation for Zero-&#x2F;Few-Shot   Training-Free Adaptation of Vision-Language Model"></a>Efficient and Context-Aware Label Propagation for Zero-&#x2F;Few-Shot   Training-Free Adaptation of Vision-Language Model</h2><p><strong>Authors:Yushu Li, Yongyi Su, Adam Goodge, Kui Jia, Xun Xu</strong></p>
<p>Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Although label, training, and data efficiency have improved, many state-of-the-art VLMs still require task-specific hyperparameter tuning and fail to fully exploit test samples. To overcome these challenges, we propose a graph-based approach for label-efficient adaptation and inference. Our method dynamically constructs a graph over text prompts, few-shot examples, and test samples, using label propagation for inference without task-specific tuning. Unlike existing zero-shot label propagation techniques, our approach requires no additional unlabeled support set and effectively leverages the test sample manifold through dynamic graph expansion. We further introduce a context-aware feature re-weighting mechanism to improve task adaptation accuracy. Additionally, our method supports efficient graph expansion, enabling real-time inductive inference. Extensive evaluations on downstream tasks, such as fine-grained categorization and out-of-distribution generalization, demonstrate the effectiveness of our approach. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/Yushu-Li/ECALP">https://github.com/Yushu-Li/ECALP</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ¥è§£å†³å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»è€Œé©æ–°äº†æœºå™¨å­¦ä¹ ã€‚å°½ç®¡æ ‡ç­¾ã€è®­ç»ƒå’Œæ•°æ®çš„æ•ˆç‡å·²ç»æé«˜ï¼Œä½†è®¸å¤šæœ€å…ˆè¿›çš„VLMsä»ç„¶éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è¶…å‚æ•°è°ƒæ•´ï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨æµ‹è¯•æ ·æœ¬ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ ‡ç­¾é«˜æ•ˆé€‚åº”å’Œæ¨ç†æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€åœ°åœ¨æ–‡æœ¬æç¤ºã€å°‘é‡ç¤ºä¾‹å’Œæµ‹è¯•æ ·æœ¬ä¸Šæ„å»ºå›¾ï¼Œä½¿ç”¨æ ‡ç­¾ä¼ æ’­è¿›è¡Œæ¨ç†ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è°ƒæ•´ã€‚ä¸ç°æœ‰çš„é›¶æ ·æœ¬æ ‡ç­¾ä¼ æ’­æŠ€æœ¯ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„æ— æ ‡ç­¾æ”¯æŒé›†ï¼Œå¹¶é€šè¿‡åŠ¨æ€å›¾æ‰©å±•æœ‰æ•ˆåœ°åˆ©ç”¨æµ‹è¯•æ ·æœ¬æµå½¢ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç‰¹å¾é‡æ–°åŠ æƒæœºåˆ¶ï¼Œä»¥æé«˜ä»»åŠ¡é€‚åº”çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒé«˜æ•ˆçš„å›¾æ‰©å±•ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶å½’çº³æ¨ç†ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ç²¾ç»†åˆ†ç±»å’Œåˆ†å¸ƒå¤–æ³›åŒ–ï¼‰ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yushu-Li/ECALP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yushu-Li/ECALPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18303v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹å¤„ç†ä¸‹æ¸¸ä»»åŠ¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚å°½ç®¡æ ‡ç­¾ã€è®­ç»ƒå’Œæ•°æ®çš„æ•ˆç‡æœ‰æ‰€æå‡ï¼Œä½†ç°æœ‰çš„å…ˆè¿›VLMsä»éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è¶…å‚æ•°è°ƒæ•´ï¼Œä¸”æ— æ³•å……åˆ†åˆ©ç”¨æµ‹è¯•æ ·æœ¬ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ ‡ç­¾é«˜æ•ˆé€‚åº”å’Œæ¨ç†æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŠ¨æ€æ„å»ºæ–‡æœ¬æç¤ºã€å°‘é‡ç¤ºä¾‹å’Œæµ‹è¯•æ ·æœ¬çš„å›¾ï¼Œåˆ©ç”¨æ ‡ç­¾ä¼ æ’­è¿›è¡Œæ¨ç†ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è°ƒæ•´ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„æ— æ ‡ç­¾æ”¯æŒé›†ï¼Œé€šè¿‡åŠ¨æ€å›¾æ‰©å±•æœ‰æ•ˆåˆ©ç”¨äº†æµ‹è¯•æ ·æœ¬æµå½¢ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡ç‰¹å¾çš„é‡åŠ æƒæœºåˆ¶ï¼Œä»¥æé«˜ä»»åŠ¡é€‚åº”çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•æ”¯æŒæœ‰æ•ˆçš„å›¾æ‰©å±•ï¼Œå¯å®ç°å®æ—¶å½’çº³æ¨ç†ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMså·²ç»åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹å¤„ç†å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰VLMséœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è¶…å‚æ•°è°ƒæ•´ï¼Œä¸”æ— æ³•å……åˆ†åˆ©ç”¨æµ‹è¯•æ ·æœ¬ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ ‡ç­¾é«˜æ•ˆé€‚åº”å’Œæ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€æ„å»ºæ–‡æœ¬æç¤ºã€å°‘é‡ç¤ºä¾‹å’Œæµ‹è¯•æ ·æœ¬çš„å›¾è¿›è¡Œæ¨ç†ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ ‡ç­¾ä¼ æ’­è¿›è¡Œæ¨ç†ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è°ƒæ•´ï¼Œä¸”æ— éœ€é¢å¤–çš„æ— æ ‡ç­¾æ”¯æŒé›†ã€‚</li>
<li>é€šè¿‡åŠ¨æ€å›¾æ‰©å±•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨äº†æµ‹è¯•æ ·æœ¬æµå½¢ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡ç‰¹å¾çš„é‡åŠ æƒæœºåˆ¶ï¼Œæé«˜ä»»åŠ¡é€‚åº”çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒå®æ—¶å½’çº³æ¨ç†ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06b0fae190ba2c37f2b3125b672cdda7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c135842bf9d4d897b65735a9cef63ae.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Limited-Impact-of-Medical-Adaptation-of-Large-Language-and-Vision-Language-Models"><a href="#The-Limited-Impact-of-Medical-Adaptation-of-Large-Language-and-Vision-Language-Models" class="headerlink" title="The Limited Impact of Medical Adaptation of Large Language and   Vision-Language Models"></a>The Limited Impact of Medical Adaptation of Large Language and   Vision-Language Models</h2><p><strong>Authors:Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst</strong></p>
<p>Several recent works seek to adapt general-purpose large language models (LLMs) and vision-language models (VLMs) for medical applications through continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining improves performance on various downstream medical tasks, such as answering medical exam questions. In this paper, we compare ten â€œmedicalâ€ LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-&#x2F;few-shot prompting and supervised fine-tuning regimes for medical question answering (QA). For instance, on clinical-note-based QA tasks in the 3-shot setting, medical LLMs outperform their base models in only 26.7% of cases, reach a (statistical) tie in 16.7% of cases, and perform significantly worse in the remaining 56.7% of cases. Our conclusions are based on (i) comparing each medical model directly against its base model; (ii) optimizing the prompts for each model separately in zero-&#x2F;few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies. </p>
<blockquote>
<p>è¿‘æœŸæœ‰å‡ é¡¹ç ”ç©¶å°è¯•é€šè¿‡å¯¹å…¬å¼€å¯ç”¨çš„ç”Ÿç‰©åŒ»å­¦è¯­æ–™åº“è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œå°†é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€‚åº”äºåŒ»ç–—åº”ç”¨ã€‚è¿™äº›ç ”ç©¶é€šå¸¸å£°ç§°ï¼Œè¿™ç§é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒèƒ½æé«˜åœ¨å„ç§ä¸‹æ¸¸åŒ»ç–—ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¦‚å›ç­”åŒ»å­¦è€ƒè¯•é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†åç§â€œåŒ»ç–—â€LLMså’Œä¸¤ç§VLMsä¸å®ƒä»¬çš„åŸºç¡€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œå¾—å‡ºäº†ä¸åŒçš„ç»“è®ºï¼šæ‰€æœ‰çš„åŒ»ç–—VLMså’Œå‡ ä¹æ‰€æœ‰çš„åŒ»ç–—LLMsåœ¨é›¶&#x2F;å°‘é•œå¤´æç¤ºå’Œç›‘ç£å¾®è°ƒåˆ¶åº¦ä¸‹ï¼Œéƒ½æ— æ³•æŒç»­æé«˜å…¶åŸºç¡€æ¨¡å‹åœ¨åŒ»ç–—é—®ç­”ï¼ˆQAï¼‰ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨åŸºäºä¸´åºŠç¬”è®°çš„QAä»»åŠ¡çš„3é•œå¤´è®¾ç½®ä¸­ï¼ŒåŒ»ç–—LLMsä»…åœ¨26.7%çš„æƒ…å†µä¸‹è¶…è¶Šå…¶åŸºç¡€æ¨¡å‹ï¼Œåœ¨16.7%çš„æƒ…å†µä¸‹ä¸åŸºç¡€æ¨¡å‹è¡¨ç°ç›¸å½“ï¼ˆç»Ÿè®¡å­¦ä¸Šçš„å¹³æ‰‹ï¼‰ï¼Œå¹¶åœ¨å…¶ä½™56.7%çš„æƒ…å†µä¸‹è¡¨ç°æ˜¾è‘—æ›´å·®ã€‚æˆ‘ä»¬çš„ç»“è®ºæ˜¯åŸºäºï¼ˆiï¼‰å°†æ¯ä¸ªåŒ»ç–—æ¨¡å‹ç›´æ¥ä¸å…¶åŸºç¡€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼›ï¼ˆiiï¼‰åœ¨é›¶&#x2F;å°‘é•œå¤´æç¤ºä¸­åˆ†åˆ«ä¸ºæ¯ä¸ªæ¨¡å‹ä¼˜åŒ–æç¤ºï¼›ï¼ˆiiiï¼‰æ¯”è¾ƒä¸­è€ƒè™‘åˆ°ç»Ÿè®¡ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„é€šç”¨é¢†åŸŸæ¨¡å‹å¯èƒ½å·²ç»å±•ç°å‡ºå¼ºå¤§çš„åŒ»ç–—çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶åŠ å¼ºç»“è®ºæä¾›äº†å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08870v2">PDF</a> Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes   additional results on clinical note QA tasks and supervised fine-tuning   evaluations</p>
<p><strong>Summary</strong></p>
<p>é€‚åº”åŒ»ç–—åº”ç”¨é¢†åŸŸçš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç ”ç©¶å·¥ä½œæ™®éè®¤ä¸ºï¼Œé€šè¿‡åœ¨å…¬å¼€å¯ç”¨çš„ç”Ÿç‰©åŒ»å­¦è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œå¯ä»¥æé«˜åœ¨å„ç§ä¸‹æ¸¸åŒ»ç–—ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å¯¹æ¯”äº†åç§â€œåŒ»ç–—â€LLMså’Œä¸¤ç§VLMsåŠå…¶åŸºç¡€æ¨¡å‹ï¼Œå¾—å‡ºä¸åŒç»“è®ºï¼šåœ¨é›¶&#x2F;å°‘é•œå¤´æç¤ºå’Œç›‘ç£å¾®è°ƒåˆ¶åº¦ä¸‹ï¼Œå‡ ä¹æ‰€æœ‰åŒ»ç–—VLMså’Œè¿‘å¤§éƒ¨åˆ†åŒ»ç–—LLMsåœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸Šå¹¶æœªæ¯”åŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚ä¾‹å¦‚ï¼Œåœ¨åŸºäºä¸´åºŠç¬”è®°çš„é—®ç­”ä»»åŠ¡çš„3ä¸ªæ ·æœ¬ä¸­ï¼ŒåŒ»ç–—LLMsä»…åœ¨26.7%çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œåœ¨16.7%çš„æƒ…å†µä¸‹è¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨å‰©ä¸‹çš„56.7%çš„æƒ…å†µä¸‹è¡¨ç°æ›´å·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤šé¡¹ç ”ç©¶å°è¯•é€šè¿‡æŒç»­åœ¨å…¬å¼€ç”Ÿç‰©åŒ»å­¦è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå°†é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”äºåŒ»ç–—åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡å¯¹æ¯”äº†åŒ»ç–—LLMså’ŒVLMsåŠå…¶åŸºç¡€æ¨¡å‹ï¼Œå‘ç°åœ¨é›¶&#x2F;å°‘é•œå¤´æç¤ºå’Œç›‘ç£å¾®è°ƒç¯å¢ƒä¸‹ï¼Œè¿™äº›åŒ»ç–—æ¨¡å‹åœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸Šæœªæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚</li>
<li>åœ¨åŸºäºä¸´åºŠç¬”è®°çš„3ä¸ªæ ·æœ¬é—®ç­”ä»»åŠ¡ä¸­ï¼ŒåŒ»ç–—LLMsä»…åœ¨å°‘æ•°æƒ…å†µä¸‹è¡¨ç°è¾ƒå¥½ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ä¸åŸºç¡€æ¨¡å‹è¡¨ç°ç›¸å½“æˆ–æ›´å·®ã€‚</li>
<li>æœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ç›´æ¥å¯¹æ¯”åŒ»ç–—æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€é’ˆå¯¹æ¯ä¸ªæ¨¡å‹åˆ†åˆ«ä¼˜åŒ–æç¤ºï¼Œå¹¶è€ƒè™‘æ¯”è¾ƒä¸­çš„ç»Ÿè®¡ä¸ç¡®å®šæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„é€šç”¨é¢†åŸŸæ¨¡å‹å¯èƒ½å·²ç»å…·å¤‡å¼ºå¤§çš„åŒ»ç–—çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å…³äºå¦‚ä½•å¼ºåŒ–æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸè¡¨ç°çš„å»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a24737e4bfdc36089904acd76dce88cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f1047aadd0e30c124594ed99621eb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cefc72433c24a1dd4c5f8fa57bc79534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b88a07ecb9923d34a1255ce614e3c0e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ARIC-An-Activity-Recognition-Dataset-in-Classroom-Surveillance-Images"><a href="#ARIC-An-Activity-Recognition-Dataset-in-Classroom-Surveillance-Images" class="headerlink" title="ARIC: An Activity Recognition Dataset in Classroom Surveillance Images"></a>ARIC: An Activity Recognition Dataset in Classroom Surveillance Images</h2><p><strong>Authors:Linfeng Xu, Fanman Meng, Qingbo Wu, Lili Pan, Heqian Qiu, Lanxiao Wang, Kailong Chen, Kanglei Geng, Yilei Qian, Haojie Wang, Shuchang Zhou, Shimou Ling, Zejia Liu, Nanlin Chen, Yingjie Xu, Shaoxu Cheng, Bowen Tan, Ziyong Xu, Hongliang Li</strong></p>
<p>The application of activity recognition in the &#96;&#96;AI + Educationâ€ field is gaining increasing attention. However, current work mainly focuses on the recognition of activities in manually captured videos and a limited number of activity types, with little attention given to recognizing activities in surveillance images from real classrooms. Activity recognition in classroom surveillance images faces multiple challenges, such as class imbalance and high activity similarity. To address this gap, we constructed a novel multimodal dataset focused on classroom surveillance image activity recognition called ARIC (Activity Recognition In Classroom). The ARIC dataset has advantages of multiple perspectives, 32 activity categories, three modalities, and real-world classroom scenarios. In addition to the general activity recognition tasks, we also provide settings for continual learning and few-shot continual learning. We hope that the ARIC dataset can act as a facilitator for future analysis and research for open teaching scenarios. You can download preliminary data from <a target="_blank" rel="noopener" href="https://ivipclab.github.io/publication_ARIC/ARIC">https://ivipclab.github.io/publication_ARIC/ARIC</a>. </p>
<blockquote>
<p>â€œAI +æ•™è‚²â€é¢†åŸŸä¸­æ´»åŠ¨è¯†åˆ«çš„åº”ç”¨è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ‰‹åŠ¨æ•è·è§†é¢‘ä¸­çš„æ´»åŠ¨è¯†åˆ«ä»¥åŠæœ‰é™çš„æ´»åŠ¨ç±»å‹ä¸Šï¼Œå¯¹äºä»çœŸå®è¯¾å ‚ç›‘æ§å›¾åƒä¸­è¯†åˆ«æ´»åŠ¨çš„å…³æ³¨è¾ƒå°‘ã€‚è¯¾å ‚ç›‘æ§å›¾åƒä¸­çš„æ´»åŠ¨è¯†åˆ«é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼Œå¦‚ç±»åˆ«ä¸å¹³è¡¡å’Œæ´»åŠ¨é«˜åº¦ç›¸ä¼¼ç­‰ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸“æ³¨äºè¯¾å ‚ç›‘æ§å›¾åƒæ´»åŠ¨è¯†åˆ«çš„æ–°å‹å¤šæ¨¡å¼æ•°æ®é›†ï¼Œåä¸ºARICï¼ˆè¯¾å ‚æ´»åŠ¨è¯†åˆ«ï¼‰ã€‚ARICæ•°æ®é›†å…·æœ‰å¤šè§’åº¦ã€32ç±»æ´»åŠ¨ã€ä¸‰ç§æ¨¡æ€å’ŒçœŸå®è¯¾å ‚åœºæ™¯çš„ä¼˜åŠ¿ã€‚é™¤äº†ä¸€èˆ¬çš„æ´»åŠ¨è¯†åˆ«ä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†æŒç»­å­¦ä¹ å’Œå°‘é‡æ ·æœ¬æŒç»­å­¦ä¹ çš„è®¾ç½®ã€‚æˆ‘ä»¬å¸Œæœ›ARICæ•°æ®é›†èƒ½ä¸ºå¼€æ”¾æ•™å­¦åœºæ™¯çš„æœªæ¥åˆ†æå’Œç ”ç©¶æä¾›å¸®åŠ©ã€‚æ‚¨å¯ä»¥ä»<a target="_blank" rel="noopener" href="https://ivipclab.github.io/publication_ARIC/ARIC">https://ivipclab.github.io/publication_ARIC/ARIC</a>ä¸‹è½½åˆæ­¥æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12337v2">PDF</a> arXiv admin note: text overlap with arXiv:2409.03354. Updated the   description for ARIC supplement</p>
<p><strong>Summary</strong><br>æ´»åŠ¨è¯†åˆ«åœ¨â€œäººå·¥æ™ºèƒ½+æ•™è‚²â€é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ‰‹åŠ¨é‡‡é›†è§†é¢‘çš„æ´»åŠ¨è¯†åˆ«åŠæœ‰é™çš„æ´»åŠ¨ç±»å‹ä¸Šï¼Œè€Œé’ˆå¯¹æ•™å®¤ç›‘æ§å›¾åƒçš„æ´»åŠ¨è¯†åˆ«ç ”ç©¶è¾ƒå°‘ã€‚æ•™å®¤ç›‘æ§å›¾åƒçš„æ´»åŠ¨è¯†åˆ«é¢ä¸´ç±»ä¸å¹³è¡¡ã€æ´»åŠ¨ç›¸ä¼¼æ€§é«˜ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸“æ³¨äºæ•™å®¤ç›‘æ§å›¾åƒæ´»åŠ¨è¯†åˆ«çš„å¤šæ¨¡æ€æ•°æ®é›†ARICã€‚ARICæ•°æ®é›†å…·æœ‰å¤šè§’åº¦ã€32ç±»æ´»åŠ¨ã€ä¸‰æ¨¡æ€å’ŒçœŸå®è¯¾å ‚åœºæ™¯ä¼˜åŠ¿ï¼Œå¹¶è®¾ç½®äº†æŒç»­å­¦ä¹ å’Œå°æ ·æœ¬æŒç»­å­¦ä¹ çš„åœºæ™¯ã€‚æˆ‘ä»¬å¸Œæœ›ARICæ•°æ®é›†èƒ½ä¸ºå¼€æ”¾æ•™å­¦ç¯å¢ƒæœªæ¥çš„åˆ†æå’Œç ”ç©¶æä¾›ä¾¿åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ´»åŠ¨è¯†åˆ«åœ¨â€œäººå·¥æ™ºèƒ½+æ•™è‚²â€é¢†åŸŸçš„åº”ç”¨æ­£é€æ¸å—åˆ°å…³æ³¨ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ‰‹åŠ¨é‡‡é›†è§†é¢‘çš„æ´»åŠ¨è¯†åˆ«ï¼Œå¯¹æ•™å®¤ç›‘æ§å›¾åƒçš„æ´»åŠ¨è¯†åˆ«ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>æ•™å®¤ç›‘æ§å›¾åƒçš„æ´»åŠ¨è¯†åˆ«é¢ä¸´ç±»ä¸å¹³è¡¡å’Œæ´»åŠ¨ç›¸ä¼¼æ€§é«˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ARICæ•°æ®é›†æ˜¯ä¸€ä¸ªä¸“æ³¨äºæ•™å®¤ç›‘æ§å›¾åƒæ´»åŠ¨è¯†åˆ«çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚</li>
<li>ARICæ•°æ®é›†å…·æœ‰å¤šè§’åº¦ã€32ç±»æ´»åŠ¨ã€ä¸‰æ¨¡æ€å’ŒçœŸå®è¯¾å ‚åœºæ™¯çš„ä¼˜åŠ¿ã€‚</li>
<li>ARICæ•°æ®é›†è®¾ç½®äº†æŒç»­å­¦ä¹ å’Œå°æ ·æœ¬æŒç»­å­¦ä¹ çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-552fc82ca5647a4f9c660d8c8e9f4865.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-265cc5d493afdd38516dabfd4f26479a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f446ed88066b7b5d8eccdb33b8f1be9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-394b0d23c50fa5ac8386b5807ff4a38e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c650cc82bebd8aa4e2b6c61197c85bd1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Training-Free-Exponential-Context-Extension-via-Cascading-KV-Cache"><a href="#Training-Free-Exponential-Context-Extension-via-Cascading-KV-Cache" class="headerlink" title="Training-Free Exponential Context Extension via Cascading KV Cache"></a>Training-Free Exponential Context Extension via Cascading KV Cache</h2><p><strong>Authors:Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang</strong></p>
<p>The transformerâ€™s context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill&#x2F;prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency. </p>
<blockquote>
<p>å˜å‹å™¨çš„ä¸Šä¸‹æ–‡çª—å£å¯¹äºå°‘æ ·æœ¬å­¦ä¹ å’Œæ¡ä»¶ç”Ÿæˆç­‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥ä¿ç•™å…ˆå‰çš„ä»¤ç‰Œä½œä¸ºæ´»åŠ¨å†…å­˜ã€‚ç„¶è€Œï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œè®¡ç®—æˆæœ¬å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œé˜»ç¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œä¸­çš„é•¿åºåˆ—åœºæ™¯éƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„ä¸€äº›é”®å€¼ç¼“å­˜ï¼ˆKV Cacheï¼‰æ–¹æ³•æä¾›äº†çº¿æ€§çš„æ¨ç†å¤æ‚æ€§ï¼Œä½†å®ƒä»¬ç®€å•åœ°ç®¡ç†å­˜å‚¨çš„ä¸Šä¸‹æ–‡ï¼Œè¿‡æ—©åœ°é€å‡ºä»¤ç‰Œå¹¶ä¸¢å¤±æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç¼ºä¹ä¼˜åŒ–çš„é¢„å¡«å……&#x2F;æç¤ºé˜¶æ®µç­–ç•¥ï¼Œå¯¼è‡´åœ¨å®é™…ä¸Šä¸‹æ–‡å¤§å°ä¸‹çš„å»¶è¿Ÿç”šè‡³é«˜äºäºŒæ¬¡æ³¨æ„åŠ›ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨çº§è”çš„å­ç¼“å­˜ç¼“å†²åŒºæ¥æœ‰é€‰æ‹©åœ°ä¿ç•™æœ€ç›¸å…³çš„ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸å¢åŠ ç¼“å­˜å¤§å°çš„æƒ…å†µä¸‹ä¿æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å†å²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†çº¿æ€§ç¼“å­˜åŸºçº¿ï¼ŒåŒ…æ‹¬æµå¼å›°æƒ‘åº¦ã€é—®ç­”ã€ä¹¦ç±æ‘˜è¦å’Œé€šè¡Œè¯æ£€ç´¢ç­‰ã€‚åœ¨ç¼“å­˜å¤§å°ç¿»å€å››æ¬¡è¾¾åˆ°65Kåï¼Œæˆ‘ä»¬åœ¨1Mä»¤ç‰Œä¸Šä¿æŒäº†æ›´å¥½çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨1Mä»¤ç‰Œä¸Šä¸é—ªå…‰æ³¨æ„åŠ›ç›¸æ¯”ï¼Œé¢„å¡«å……é˜¶æ®µå»¶è¿Ÿé™ä½äº†6.8å€ã€‚è¿™äº›åˆ›æ–°ä¸ä»…æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œè¿˜ä¸ºå®ƒä»¬åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æœ‰æ•ˆéƒ¨ç½²é“ºå¹³äº†é“è·¯ï¼Œå®ç°äº†å¤§è§„æ¨¡ã€å®æ—¶çš„åº”ç”¨ç¨‹åºï¼Œå¹¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17808v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ–‡ä¸­æ¢è®¨äº†åœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œæ¡ä»¶ç”Ÿæˆç­‰ä»»åŠ¡ä¸­ï¼Œtransformerçš„ä¸Šä¸‹æ–‡çª—å£çš„é‡è¦æ€§åŠå…¶å¸¦æ¥çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨çº§è”å­ç¼“å­˜ç¼“å†²åŒºçš„æ–°æœºåˆ¶ï¼Œæœ‰é€‰æ‹©åœ°ä¿ç•™æœ€ç›¸å…³çš„ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¿æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å†å²ï¼ŒåŒæ—¶ä¸å¢åŠ ç¼“å­˜å¤§å°ã€‚æ­¤æ–¹æ³•åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºçº¿æ€§ç¼“å­˜åŸºçº¿ï¼ŒåŒ…æ‹¬æµå¼å›°æƒ‘åº¦ã€é—®ç­”ã€ä¹¦ç±æ‘˜è¦å’Œå¯†é’¥æ£€ç´¢ç­‰ï¼Œå¹¶ä¸”åœ¨ç¼“å­˜å¤§å°ç¿»å€æ—¶ä»èƒ½ä¿æŒè¾ƒå¥½çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é™ä½äº†å¡«å……é˜¶æ®µçš„å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformerçš„ä¸Šä¸‹æ–‡çª—å£å¯¹äºå°‘æ ·æœ¬å­¦ä¹ å’Œæ¡ä»¶ç”Ÿæˆç­‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†è®¡ç®—æˆæœ¬éšä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚</li>
<li>ç°æœ‰çš„KV Cacheæ–¹æ³•è™½ç„¶æä¾›çº¿æ€§æ¨ç†å¤æ‚æ€§ï¼Œä½†ç®¡ç†å­˜å‚¨çš„ä¸Šä¸‹æ–‡çš„æ–¹å¼è¿‡äºç®€å•ï¼Œå¯èƒ½å¯¼è‡´è¿‡æ—©æ·˜æ±°ä»¤ç‰Œå’Œä¸¢å¤±æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„çº§è”å­ç¼“å­˜ç¼“å†²åŒºæœºåˆ¶èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°ä¿ç•™æœ€ç›¸å…³çš„ä»¤ç‰Œï¼Œç»´æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å†å²ï¼ŒåŒæ—¶ä¸å¢åŠ ç¼“å­˜å¤§å°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æµå¼å›°æƒ‘åº¦ã€é—®ç­”ã€ä¹¦ç±æ‘˜è¦å’Œå¯†é’¥æ£€ç´¢ç­‰ä»»åŠ¡ã€‚</li>
<li>åœ¨ç¼“å­˜å¤§å°ç¿»å€æ—¶ï¼Œè¯¥æ–¹æ³•ä»ç„¶èƒ½ä¿æŒè¾ƒå¥½çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚</li>
<li>ä¸flash attentionç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨1Mä»¤ç‰Œçš„æƒ…å†µä¸‹å°†å¡«å……é˜¶æ®µå»¶è¿Ÿé™ä½äº†6.8å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef7f6a90ecb9ef49b8af5dc5f5b2a1d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9d4f27f40553795118d8097a2645c5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b56923c4a88ce9e07d2338dec266d002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3291149c952ca7ee58168dcb7cbfd42b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea92457bc0a6b5619b47df496a2e8083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-551c1277524d61403fce9c8d396b3de8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LTSM-Bundle-A-Toolbox-and-Benchmark-on-Large-Language-Models-for-Time-Series-Forecasting"><a href="#LTSM-Bundle-A-Toolbox-and-Benchmark-on-Large-Language-Models-for-Time-Series-Forecasting" class="headerlink" title="LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time   Series Forecasting"></a>LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time   Series Forecasting</h2><p><strong>Authors:Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Songyuan Sui, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, Xia Hu</strong></p>
<p>Time Series Forecasting (TSF) has long been a challenge in time series analysis. Inspired by the success of Large Language Models (LLMs), researchers are now developing Large Time Series Models (LTSMs)-universal transformer-based models that use autoregressive prediction-to improve TSF. However, training LTSMs on heterogeneous time series data poses unique challenges, including diverse frequencies, dimensions, and patterns across datasets. Recent endeavors have studied and evaluated various design choices aimed at enhancing LTSM training and generalization capabilities. However, these design choices are typically studied and evaluated in isolation and are not benchmarked collectively. In this work, we introduce LTSM-Bundle, a comprehensive toolbox, and benchmark for training LTSMs, spanning pre-processing techniques, model configurations, and dataset configuration. It modularized and benchmarked LTSMs from multiple dimensions, encompassing prompting strategies, tokenization approaches, training paradigms, base model selection, data quantity, and dataset diversity. Furthermore, we combine the most effective design choices identified in our study. Empirical results demonstrate that this combination achieves superior zero-shot and few-shot performances compared to state-of-the-art LTSMs and traditional TSF methods on benchmark datasets. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸€ç›´æ˜¯æ—¶é—´åºåˆ—åˆ†æä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜ç°åœ¨æ­£åœ¨å¼€å‘åŸºäºé€šç”¨å˜å‹å™¨çš„å¤§å‹æ—¶é—´åºåˆ—æ¨¡å‹ï¼ˆLTSMï¼‰ï¼Œé‡‡ç”¨è‡ªå›å½’é¢„æµ‹æ¥æé«˜TSFã€‚ç„¶è€Œï¼Œåœ¨å¼‚è´¨æ—¶é—´åºåˆ—æ•°æ®ä¸Šè®­ç»ƒLTSMé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†é—´çš„é¢‘ç‡ã€ç»´åº¦å’Œæ¨¡å¼çš„å¤šæ ·æ€§ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»ç ”ç©¶å’Œè¯„ä¼°äº†æ—¨åœ¨æé«˜LTSMè®­ç»ƒå’Œæ³›åŒ–èƒ½åŠ›çš„å„ç§è®¾è®¡é€‰æ‹©ã€‚ç„¶è€Œï¼Œè¿™äº›è®¾è®¡é€‰æ‹©é€šå¸¸å­¤ç«‹åœ°ç ”ç©¶å’Œè¯„ä¼°ï¼Œå¹¶æœªé›†ä½“è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LTSM-Bundleï¼Œä¸€ä¸ªå…¨é¢çš„å·¥å…·åŒ…å’ŒåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè®­ç»ƒLTSMï¼Œæ¶µç›–é¢„å¤„ç†æŠ€æœ¯ã€æ¨¡å‹é…ç½®å’Œæ•°æ®é›†é…ç½®ã€‚å®ƒä»å¤šä¸ªç»´åº¦å¯¹LTSMè¿›è¡Œæ¨¡å—åŒ–å’ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ ‡è®°åŒ–æ–¹æ³•ã€è®­ç»ƒèŒƒå¼ã€åŸºç¡€æ¨¡å‹é€‰æ‹©ã€æ•°æ®é‡ä»¥åŠæ•°æ®é›†å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç»“åˆç ”ç©¶ä¸­ç¡®å®šçš„æœ€æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ•°æ®é›†ä¸Šçš„æœ€æ–°LTSMå’Œä¼ ç»ŸTSFæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§ç»„åˆå®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14045v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>ç ”ç©¶äººå‘˜åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç†å¿µï¼Œå¼€å‘äº†ä¸€ç§åŸºäºé€šç”¨è½¬æ¢å™¨çš„å¤§å‹æ—¶é—´åºåˆ—æ¨¡å‹ï¼ˆLTSMï¼‰ï¼Œä»¥æ”¹è¿›æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ã€‚ç„¶è€Œï¼Œåœ¨å¼‚è´¨æ—¶é—´åºåˆ—æ•°æ®ä¸Šè®­ç»ƒLTSMé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†çš„é¢‘ç‡ã€ç»´åº¦å’Œæ¨¡å¼çš„å¤šæ ·æ€§ã€‚è¿‘æœŸç ”ç©¶ç€çœ¼äºå¢å¼ºLTSMè®­ç»ƒå’Œæ³›åŒ–èƒ½åŠ›çš„è®¾è®¡é€‰æ‹©ï¼Œä½†é€šå¸¸æ˜¯å­¤ç«‹åœ°ç ”ç©¶å¹¶è¯„ä¼°çš„ï¼Œå¹¶æ²¡æœ‰ç»Ÿä¸€çš„æ ‡å‡†è¿›è¡Œè¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†LTSMæ†ç»‘åŒ…ï¼Œä¸€ä¸ªå…¨é¢çš„å·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–é¢„å¤„ç†æŠ€æœ¯ã€æ¨¡å‹é…ç½®å’Œæ•°æ®é›†é…ç½®ã€‚å®ƒä»å¤šä¸ªç»´åº¦æ¨¡å—åŒ–å¹¶è¯„ä¼°LTSMï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ ‡è®°åŒ–æ–¹æ³•ã€è®­ç»ƒæ¨¡å¼ã€åŸºç¡€æ¨¡å‹é€‰æ‹©ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ç»“åˆäº†ç ”ç©¶ä¸­å‘ç°çš„æœ€æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ï¼Œå®è¯ç»“æœè¡¨æ˜è¯¥ç»„åˆåœ¨åŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›LTSMå’Œä¼ ç»ŸTSFæ–¹æ³•å®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½çš„ä¼˜è¶Šè¡¨ç°ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LTSMsè¢«å¼€å‘ä¸ºæ”¹è¿›æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ã€‚</li>
<li>LTSMsåœ¨è®­ç»ƒå¼‚è´¨æ—¶é—´åºåˆ—æ•°æ®æ—¶é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰å¯¹LTSMçš„è®¾è®¡é€‰æ‹©é€šå¸¸æ˜¯å­¤ç«‹ç ”ç©¶å’Œè¯„ä¼°çš„ï¼Œç¼ºä¹ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>LTSM-Bundleå·¥å…·ç®±å›Šæ‹¬äº†å¤šç§è®¾è®¡é€‰æ‹©çš„æ¨¡å—åŒ–è¯„ä¼°ï¼ŒåŒ…æ‹¬æç¤ºç­–ç•¥ã€æ ‡è®°åŒ–æ–¹æ³•ç­‰ã€‚</li>
<li>ç»“åˆæœ€æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ï¼Œå®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ä¸Šçš„ä¼˜è¶Šè¡¨ç°ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºLTSMsçš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨æä¾›äº†å®è´µçš„å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0dd0dca60a8e70ba2b084adda7da81c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc2b037f6d512ad805734e98d29828d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03d3d6b45c616ab3fb75c31f2853b29b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4ac2b9d50c996daeee1129ac0205ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f427b7582c1565b6ba41288c75ff0c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aef481bd9d1497a07b27f308d454f1a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-643c558eb5764a2680b38874de069f06.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ab7dbd6993c351be6253d325bfce3a31.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  GUIDE LLM-Driven GUI Generation Decomposition for Automated Prototyping
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c82cda3fc8611445c36642fe3497feb1.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  Chitranuvad Adapting Multi-Lingual LLMs for Multimodal Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
