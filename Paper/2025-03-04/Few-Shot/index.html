<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-04  Improving Open-world Continual Learning under the Constraints of Scarce   Labeled Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-551c1277524d61403fce9c8d396b3de8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-04-更新"><a href="#2025-03-04-更新" class="headerlink" title="2025-03-04 更新"></a>2025-03-04 更新</h1><h2 id="Improving-Open-world-Continual-Learning-under-the-Constraints-of-Scarce-Labeled-Data"><a href="#Improving-Open-world-Continual-Learning-under-the-Constraints-of-Scarce-Labeled-Data" class="headerlink" title="Improving Open-world Continual Learning under the Constraints of Scarce   Labeled Data"></a>Improving Open-world Continual Learning under the Constraints of Scarce   Labeled Data</h2><p><strong>Authors:Yujie Li, Xiangkun Wang, Xin Yang, Marcello Bonsangue, Junbo Zhang, Tianrui Li</strong></p>
<p>Open-world continual learning (OWCL) adapts to sequential tasks with open samples, learning knowledge incrementally while preventing forgetting. However, existing OWCL still requires a large amount of labeled data for training, which is often impractical in real-world applications. Given that new categories&#x2F;entities typically come with limited annotations and are in small quantities, a more realistic situation is OWCL with scarce labeled data, i.e., few-shot training samples. Hence, this paper investigates the problem of open-world few-shot continual learning (OFCL), challenging in (i) learning unbounded tasks without forgetting previous knowledge and avoiding overfitting, (ii) constructing compact decision boundaries for open detection with limited labeled data, and (iii) transferring knowledge about knowns and unknowns and even update the unknowns to knowns once the labels of open samples are learned. In response, we propose a novel OFCL framework that integrates three key components: (1) an instance-wise token augmentation (ITA) that represents and enriches sample representations with additional knowledge, (2) a margin-based open boundary (MOB) that supports open detection with new tasks emerge over time, and (3) an adaptive knowledge space (AKS) that endows unknowns with knowledge for the updating from unknowns to knowns. Finally, extensive experiments show the proposed OFCL framework outperforms all baselines remarkably with practical importance and reproducibility. The source code is released at <a target="_blank" rel="noopener" href="https://github.com/liyj1201/OFCL">https://github.com/liyj1201/OFCL</a>. </p>
<blockquote>
<p>开放世界持续学习（OWCL）适应具有开放样本的连续任务，能够逐步学习新知识，同时防止遗忘。然而，现有的OWCL仍然需要大量标记数据进行训练，这在现实世界的应用中往往不切实际。考虑到新类别&#x2F;实体通常带有有限的注释并且数量较小，更现实的情况是在稀缺标记数据下的OWCL，即少数训练样本。因此，本文研究了开放世界少数样本持续学习（OFCL）的问题，面临的挑战包括（i）学习无界任务，不忘掉以前的知识，避免过度拟合；（ii）用有限的标记数据构建紧凑的决策边界，进行开放检测；（iii）转移关于已知和未知的知识，甚至一旦学习到开放样本的标签，就将未知更新为已知。为此，我们提出了一种新的OFCL框架，它包括三个关键组成部分：（1）实例级令牌增强（ITA），它用额外的知识表示和丰富样本表示；（2）基于边距的开放边界（MOB），它支持随时间出现的新任务的开放检测；（3）自适应知识空间（AKS），它赋予未知知识，以便从未知到已知进行更新。最后，大量实验表明，所提出的OFCL框架在实用性和可重复性方面均优于所有基线。源代码已发布在<a target="_blank" rel="noopener" href="https://github.com/liyj1201/OFCL%E3%80%82">https://github.com/liyj1201/OFCL。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了开放世界小样本持续学习（OFCL）的问题，提出了一个包含三个关键组件的OFCL框架，包括实例级令牌增强（ITA）、基于边界的开放边界（MOB）和自适应知识空间（AKS）。该框架能够在有限的标注数据下，适应不断涌现的新任务，实现开放检测，同时更新未知知识。实验表明，该框架在实际应用和可重复性方面显著优于所有基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开放世界小样本持续学习（OFCL）是一个现实中的问题，因为新类别&#x2F;实体通常带有有限的注释并且数量较小。</li>
<li>OFCL面临的挑战包括：在不遗忘先前知识的情况下学习无界任务，避免过度拟合；用有限的标注数据构建用于开放检测的更紧凑的决策边界；以及转移关于已知和未知的知识，甚至一旦学习到开放的样本标签，就能将未知更新为已知。</li>
<li>提出的OFCL框架包含三个关键组件：实例级令牌增强（ITA），基于边界的开放边界（MOB）和自适应知识空间（AKS）。</li>
<li>ITA用于表示和丰富样本表示，带有额外的知识。</li>
<li>MOB支持随时间出现的新任务的开放检测。</li>
<li>AKS赋予未知知识更新的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20974">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7ba66d84ab1cbabb870aece91596d3ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85bafc9583d05ab29b01e4bf0d98a9dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ac2bb1d27726631436171fbbcfeeea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57259b2c56458cd781901f2cf09676d7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-to-Substitute-Components-for-Compositional-Generalization"><a href="#Learning-to-Substitute-Components-for-Compositional-Generalization" class="headerlink" title="Learning to Substitute Components for Compositional Generalization"></a>Learning to Substitute Components for Compositional Generalization</h2><p><strong>Authors:Zhaoyi Li, Gangwei Jiang, Chenwang Wu, Ying Wei, Defu Lian, Enhong Chen</strong></p>
<p>Despite the rising prevalence of neural language models, recent empirical evidence suggests their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, which aims to introduce additional compositional inductive bias. However, existing handcrafted augmentation strategies offer limited improvement when systematic generalization of neural language models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases alone) or when training sentences have an imbalanced difficulty distribution. To address these challenges, we first propose a novel compositional augmentation strategy called Component Substitution (CompSub), which enables multi-grained composition of substantial substructures across the entire training set. Furthermore, we introduce the Learning Component Substitution (LCS) framework. This framework empowers the learning of component substitution probabilities in CompSub in an end-to-end manner by maximizing the loss of neural language models, thereby prioritizing challenging compositions with elusive concepts and novel contexts. We extend the key ideas of CompSub and LCS to the recently emerging in-context learning scenarios of pre-trained large language models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot compositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we provide insights into why applying our algorithms to language models can improve compositional generalization performance. Empirically, our results on four standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and COGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with improvements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively. </p>
<blockquote>
<p>尽管神经语言模型的普及程度不断上升，但最近的实证证据表明它们在组合泛化方面存在缺陷。目前解决这个问题的实际解决方案之一是组合数据增强，其旨在引入额外的组合归纳偏见。然而，现有的手工增强策略仅在系统泛化需要多粒度组合偏见（即不限于词汇或结构偏见）或训练句子难度分布不平衡时提供有限的改进。为了应对这些挑战，我们首先提出了一种新的组合增强策略，称为组件替换（CompSub），它能够在整个训练集中组合大量的重要子结构的多粒度。此外，我们引入了学习组件替换（LCS）框架。该框架通过最大化神经语言模型的损失来赋能组件替换概率的学习，从而优先处理具有难以捉摸的概念和新颖背景的困难组合。我们将CompSub和LCS的关键思想扩展到最近出现的预训练大型语言模型（LLM）的上下文学习场景，提出LCS-ICL算法，以提高最新技术（SOTA）LLM的少量组合泛化能力。从理论上讲，我们提供了将我们的算法应用于语言模型可以提高组合泛化性能的原因。在四个标准组合泛化基准测试（SCAN、COGS、GeoQuery和COGS-QL）上的实验结果表明，CompSub、LCS和LCS-ICL的优越性，改进幅度高达66.5%、10.3%、1.4%和8.8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20834v1">PDF</a> 23 pages, 9 figures, preprint, the extension paper of the paper   (arXiv:2306.02840)</p>
<p><strong>摘要</strong></p>
<p>该文指出尽管神经网络语言模型越来越流行，但在组合泛化方面存在缺陷。现有的手工艺品增强策略提供的改进有限，特别是在需要多粒度组合偏差或训练句子难度分布不平衡的情况下。为此，提出了一种新的组合增强策略——组件替换（CompSub），能够在整个训练集中组合多粒度的实质性子结构。此外，还引入了学习组件替换（LCS）框架，以端到端的方式学习组件替换概率，通过最大化神经网络模型的损失来优先处理具有难以捉摸的概念和新颖上下文的组合。最后，将CompSub和LCS的关键思想扩展到新兴的上下文学习场景，提出LCS-ICL算法，以提高最先进的LLM的少量组合泛化能力。理论分析和实证结果表明，本文提出的算法能有效提高语言模型的组合泛化性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>神经网络语言模型在组合泛化方面存在缺陷，特别是在需要多粒度组合偏差或训练句子难度分布不平衡的情况下。</li>
<li>提出了一种新的组合增强策略——组件替换（CompSub），能整合多粒度的子结构。</li>
<li>引入了学习组件替换（LCS）框架，以最大化神经网络模型的损失来学习组件替换概率。</li>
<li>将CompSub和LCS的思想扩展到上下文学习场景，提出LCS-ICL算法，提高LLM的少量组合泛化能力。</li>
<li>本文提供的算法能有效提高语言模型的组合泛化性能，这在四个标准组合泛化基准测试中得到了验证。</li>
<li>通过理论分析，解释了为何将这些算法应用于语言模型能提高组合泛化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb38ead256155fbbefa6628164b97e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75ae7d0ffe5be5b48a67b1e8a47affac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec3f7ebd01c47ba5a65593040c54d8d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Can-We-Simplify-Slide-level-Fine-tuning-of-Pathology-Foundation-Models"><a href="#Can-We-Simplify-Slide-level-Fine-tuning-of-Pathology-Foundation-Models" class="headerlink" title="Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?"></a>Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</h2><p><strong>Authors:Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He</strong></p>
<p>The emergence of foundation models in computational pathology has transformed histopathological image analysis, with whole slide imaging (WSI) diagnosis being a core application. Traditionally, weakly supervised fine-tuning via multiple instance learning (MIL) has been the primary method for adapting foundation models to WSIs. However, in this work we present a key experimental finding: a simple nonlinear mapping strategy combining mean pooling and a multilayer perceptron, called SiMLP, can effectively adapt patch-level foundation models to slide-level tasks without complex MIL-based learning. Through extensive experiments across diverse downstream tasks, we demonstrate the superior performance of SiMLP with state-of-the-art methods. For instance, on a large-scale pan-cancer classification task, SiMLP surpasses popular MIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in few-shot classification and remaining highly competitive with slide-level foundation models pretrained on tens of thousands of slides. Finally, SiMLP exhibits remarkable robustness and transferability in lung cancer subtyping. Overall, our findings challenge the conventional MIL-based fine-tuning paradigm, demonstrating that a task-agnostic representation strategy alone can effectively adapt foundation models to WSI analysis. These insights offer a unique and meaningful perspective for future research in digital pathology, paving the way for more efficient and broadly applicable methodologies. </p>
<blockquote>
<p>计算病理学中的基础模型的兴起已经改变了组织病理学图像分析的局面，其中全幻灯片成像（WSI）诊断是核心应用之一。传统上，通过多重实例学习（MIL）进行弱监督微调是使基础模型适应WSI的主要方法。然而，在这项工作中，我们提出了一个重要的实验发现：一种结合均值池化和多层感知器（MLP）的简单非线性映射策略，称为SiMLP，可以有效地将补丁级别的基础模型适应到幻灯片级别的任务，而无需复杂的基于MIL的学习。通过在不同下游任务上的大量实验，我们证明了SiMLP在最新技术方法中的卓越性能。例如，在一个大规模泛癌分类任务中，SiMLP在流行的基于MIL的方法的基础上提高了3.52%。此外，SiMLP在少样本分类中显示出强大的学习能力，并且在数以万计幻灯片上预训练过的幻灯片级别基础模型中保持竞争力。最后，SiMLP在肺癌分型中表现出令人瞩目的鲁棒性和可迁移性。总的来说，我们的发现挑战了传统的基于MIL的微调模式，证明了单一的任务不可知表示策略就可以有效地适应基础模型到WSI分析。这些见解为数字病理学的未来研究提供了独特而有意义的视角，为更有效率且更广泛适用的方法铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20823v1">PDF</a> 11 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>基于计算病理学中基础模型的兴起，全幻灯片成像（WSI）诊断成为其核心应用之一。传统上，通过多重实例学习（MIL）进行弱监督微调是适应WSI的基础模型的主要方法。然而，本文提出了一种简单的非线性映射策略——SiMLP（结合均值池化和多层感知器），可以有效地将补丁级别的基础模型适应于幻灯片级别的任务，而无需复杂的基于MIL的学习。通过广泛的实验和多种下游任务，我们证明了SiMLP的优越性，例如在大型泛癌分类任务中，SiMLP超越了流行的基于MIL的方法，准确率提高了3.52%。此外，SiMLP在少样本分类中显示出强大的学习能力，并且在幻灯片级别预训练的基础模型上仍然具有竞争力。最后，SiMLP在肺癌分型中表现出惊人的稳健性和可迁移性。总的来说，我们的研究挑战了传统的基于MIL的微调范式，证明了任务无关的表示策略可以单独有效地适应WSI分析的基础模型。这些见解为数字病理学未来的研究提供了独特和有意义的视角，为更有效率且更广泛适用的方法铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入基础模型改变了计算病理学中的全幻灯片成像（WSI）诊断方式。</li>
<li>传统上，基于多重实例学习（MIL）的弱监督微调是适应WSI的主要方法。</li>
<li>提出了一种简单的非线性映射策略SiMLP，无需复杂的基于MIL的学习即可适应模型。</li>
<li>在大型泛癌分类任务中，SiMLP超越了基于MIL的方法。</li>
<li>SiMLP在少样本分类方面具有强大的学习能力，并在幻灯片级别预训练的基础模型上表现竞争力。</li>
<li>SiMLP在肺癌分型中显示出稳健性和可迁移性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6354e1485dd08a57eb4d4d98041243ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55944973048a2f1d9c8df35af60410d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c728361a91890f71be2d97440b490920.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e50fa95993bd9a2337de62c3c664704.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-728bf60ef3ebb8f4dc48804aeb776663.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Few-Shot-No-Problem-Descriptive-Continual-Relation-Extraction"><a href="#Few-Shot-No-Problem-Descriptive-Continual-Relation-Extraction" class="headerlink" title="Few-Shot, No Problem: Descriptive Continual Relation Extraction"></a>Few-Shot, No Problem: Descriptive Continual Relation Extraction</h2><p><strong>Authors:Nguyen Xuan Thanh, Anh Duc Le, Quyen Tran, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen</strong></p>
<p>Few-shot Continual Relation Extraction is a crucial challenge for enabling AI systems to identify and adapt to evolving relationships in dynamic real-world domains. Traditional memory-based approaches often overfit to limited samples, failing to reinforce old knowledge, with the scarcity of data in few-shot scenarios further exacerbating these issues by hindering effective data augmentation in the latent space. In this paper, we propose a novel retrieval-based solution, starting with a large language model to generate descriptions for each relation. From these descriptions, we introduce a bi-encoder retrieval training paradigm to enrich both sample and class representation learning. Leveraging these enhanced representations, we design a retrieval-based prediction method where each sample “retrieves” the best fitting relation via a reciprocal rank fusion score that integrates both relation description vectors and class prototypes. Extensive experiments on multiple datasets demonstrate that our method significantly advances the state-of-the-art by maintaining robust performance across sequential tasks, effectively addressing catastrophic forgetting. </p>
<blockquote>
<p>少样本持续关系抽取是使AI系统能够识别和适应动态现实领域中的不断变化关系的关键挑战。基于传统的记忆方法往往会对有限的样本过度拟合，无法巩固旧知识，而在少样本场景中，数据的稀缺性进一步加剧了这些问题，阻碍了潜在空间中的有效数据增强。在本文中，我们提出了一种新型的基于检索的解决方案，首先使用大型语言模型为每个关系生成描述。从这些描述中，我们引入了一种双编码器检索训练模式，以丰富样本和类别表示学习。利用这些增强的表示形式，我们设计了一种基于检索的预测方法，其中每个样本通过融合关系描述向量和类别原型的反向排名融合得分来检索最适合的关系。在多个数据集上的大量实验表明，我们的方法在序列任务上保持了稳健的性能，有效地解决了灾难性遗忘问题，显著地提高了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20596v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了少样本持续关系抽取的挑战，并提出了一种基于检索的解决方案。该方案利用大型语言模型生成每个关系的描述，并引入双编码器检索训练范式，丰富样本和类别表示学习。基于这些增强表示，设计了一种基于检索的预测方法，通过融合关系描述向量和类别原型，实现样本与最佳拟合关系的匹配。在多个数据集上的实验表明，该方法在序贯任务上性能稳健，有效解决了灾难性遗忘问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍少样本持续关系抽取的重要性，使AI系统能够识别和适应动态现实世界领域中不断变化的关系。</li>
<li>传统基于记忆的方法在有限样本上过拟合，无法巩固旧知识。</li>
<li>在少样本场景中，数据稀缺性加剧了这些问题，阻碍了潜在空间中的有效数据增强。</li>
<li>提出了一种基于检索的解决方案，利用大型语言模型生成关系描述。</li>
<li>引入双编码器检索训练范式，丰富样本和类别的表示学习。</li>
<li>设计了一种基于检索的预测方法，通过融合关系描述向量和类别原型，实现样本与最佳拟合关系的匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab4938e7e0c7a226fea603e65b3c5a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58bf6acc8315dbf6c75aace37fa51103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9a5f77096c14420f8268791d9236586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f1f6e7bb5734d9011b59222cf458f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb4a65d02d237f54882e9504e357289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bed4b8df45b80b96c72987ee93fdf65.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Training-Elicits-Concise-Reasoning-in-Large-Language-Models"><a href="#Self-Training-Elicits-Concise-Reasoning-in-Large-Language-Models" class="headerlink" title="Self-Training Elicits Concise Reasoning in Large Language Models"></a>Self-Training Elicits Concise Reasoning in Large Language Models</h2><p><strong>Authors:Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun</strong></p>
<p>Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TergelMunkhbat/concise-reasoning">https://github.com/TergelMunkhbat/concise-reasoning</a> </p>
<blockquote>
<p>链式思维（CoT）推理使得大型语言模型（LLM）能够通过中间令牌利用额外的计算来解决复杂的任务。然而，我们认为典型的推理轨迹包含许多冗余的令牌，产生了额外的推理成本。在检查当前LLM的输出分布时，我们发现它们相对于默认行为具有更简洁推理的潜在能力。为了激发这一能力，我们提出了简单的微调方法，这些方法利用通过最佳N采样和少样本条件在特定任务环境中获得的自我生成的简洁推理路径。我们的组合方法在GSM8K和MATH上平均减少了输出令牌数的30%，同时保持平均准确率。通过利用LLM的基本随机性和上下文学习能力，我们的自训练方法在各种模型上都能稳健地引发简洁推理，包括那些经过广泛训练后的模型。代码可通过以下网址获得：<a target="_blank" rel="noopener" href="https://github.com/TergelMunkhbat/concise-reasoning">https://github.com/TergelMunkhbat/concise-reasoning</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20122v2">PDF</a> 23 pages, 10 figures, 18 tables</p>
<p><strong>Summary</strong></p>
<p>链式思维（CoT）推理使大型语言模型（LLM）能够通过中间标记利用额外的计算来解决复杂任务。然而，我们观察到通常的推理过程包含许多冗余标记，产生了额外的推理成本。我们提出了简单微调方法，利用自我生成的简洁推理路径，通过最佳N采样和少样本条件，在特定任务环境中实现平均输出标记减少30%，同时在GSM8K和MATH上维持平均准确率。我们的自我训练方法利用LLM的基本随机性和上下文学习能力，能在各种模型上稳健地实现简洁推理，包括那些经过大量训练后的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>链式思维（CoT）推理允许大型语言模型（LLM）解决复杂任务时利用额外计算。</li>
<li>典型的推理过程包含许多冗余标记，导致额外的推理成本。</li>
<li>提出简单微调方法，利用自我生成的简洁推理路径。</li>
<li>通过最佳N采样和少样本条件，实现了平均输出标记减少30%。</li>
<li>在特定任务环境中维持了平均准确率。</li>
<li>利用LLM的基本随机性和上下文学习能力，方法能在各种模型上实现简洁推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4d211e5f506dc17daa0eda94c8969b14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf4df0a272bce5a46a6179100a08a64a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4f49fd89aedf26a88de006267ac2a9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52a205504d20d5833915ab13c9833aa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a0742ff26a61b535f9435f3053d2465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5718763a438227758f20955cf1036cf1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="3D-Prior-is-All-You-Need-Cross-Task-Few-shot-2D-Gaze-Estimation"><a href="#3D-Prior-is-All-You-Need-Cross-Task-Few-shot-2D-Gaze-Estimation" class="headerlink" title="3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation"></a>3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation</h2><p><strong>Authors:Yihua Cheng, Hengfei Wang, Zhongqun Zhang, Yang Yue, Bo Eun Kim, Feng Lu, Hyung Jin Chang</strong></p>
<p>3D and 2D gaze estimation share the fundamental objective of capturing eye movements but are traditionally treated as two distinct research domains. In this paper, we introduce a novel cross-task few-shot 2D gaze estimation approach, aiming to adapt a pre-trained 3D gaze estimation network for 2D gaze prediction on unseen devices using only a few training images. This task is highly challenging due to the domain gap between 3D and 2D gaze, unknown screen poses, and limited training data. To address these challenges, we propose a novel framework that bridges the gap between 3D and 2D gaze. Our framework contains a physics-based differentiable projection module with learnable parameters to model screen poses and project 3D gaze into 2D gaze. The framework is fully differentiable and can integrate into existing 3D gaze networks without modifying their original architecture. Additionally, we introduce a dynamic pseudo-labelling strategy for flipped images, which is particularly challenging for 2D labels due to unknown screen poses. To overcome this, we reverse the projection process by converting 2D labels to 3D space, where flipping is performed. Notably, this 3D space is not aligned with the camera coordinate system, so we learn a dynamic transformation matrix to compensate for this misalignment. We evaluate our method on MPIIGaze, EVE, and GazeCapture datasets, collected respectively on laptops, desktop computers, and mobile devices. The superior performance highlights the effectiveness of our approach, and demonstrates its strong potential for real-world applications. </p>
<blockquote>
<p>本文介绍了一种新颖的跨任务小样本2D眼动估计方法，旨在使用预训练的3D眼动估计网络对未见过的设备进行基于少量训练图像的2D眼动预测。由于3D和2D眼动之间的领域差距、未知的屏幕姿态以及有限的训练数据，此任务极具挑战性。为了应对这些挑战，我们提出了一种新颖的框架来弥补3D和2D眼动之间的差距。我们的框架包含一个基于物理的可微分投影模块，具有可学习参数来模拟屏幕姿态并将3D眼动投影到2D眼动。该框架是完全可微分的，并且可以集成到现有的3D眼动网络中，无需修改其原始架构。此外，我们引入了一种动态伪标签策略来处理翻转图像，这对于具有未知屏幕姿态的2D标签来说尤其具有挑战性。为了克服这一点，我们通过将2D标签转换为三维空间来反转投影过程，并在那里执行翻转操作。值得注意的是，这个三维空间不与相机坐标系对齐，因此我们学习一个动态转换矩阵来补偿这种不对齐。我们在MPIIGaze、EVE和GazeCapture数据集上评估了我们的方法，这些数据集分别在笔记本电脑、台式电脑和移动设备上收集。出色的性能凸显了我们方法的有效性，并展示了其在现实世界应用中的强大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04074v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的跨任务小样本2D眼动注视点估计方法，旨在利用预训练的3D眼动注视点估计网络对未见过的设备进行少量训练图像即可进行2D眼动注视点预测。该研究引入了可微分投影模块建模屏幕姿态并将3D眼动注视点投影到2D空间，同时采用动态伪标签策略应对翻转图像的挑战。在MPIIGaze、EVE和GazeCapture数据集上的实验结果证明了方法的有效性，展现其真实世界应用的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了将预训练的3D眼动注视点估计网络应用于未见设备的小样本下的2D眼动注视点预测方法。</li>
<li>提出了一种新颖的跨任务方法，使用可微分投影模块将3D和2D眼动注视点联系紧密起来。该模块能建模屏幕姿态并实现从3D到2D的投影。</li>
<li>引入了动态伪标签策略处理翻转图像带来的挑战，通过将2D标签转化为相应的物理空间的做法来处理这一挑战。值得注意的是转换后的空间与相机坐标系不一致，因此引入了动态转换矩阵来修正这一差异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-35e8eb992b5c0d1f607702759e87750e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8b9a8c24828382d605e144d5c6eb903.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8ef542cda50013ea21d98b66fc923a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7889d33c9089358d01601dc03b0178ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76b6fa6144fa1463bf28fd443033cfe9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Context-Aware-Label-Propagation-for-Zero-Few-Shot-Training-Free-Adaptation-of-Vision-Language-Model"><a href="#Efficient-and-Context-Aware-Label-Propagation-for-Zero-Few-Shot-Training-Free-Adaptation-of-Vision-Language-Model" class="headerlink" title="Efficient and Context-Aware Label Propagation for Zero-&#x2F;Few-Shot   Training-Free Adaptation of Vision-Language Model"></a>Efficient and Context-Aware Label Propagation for Zero-&#x2F;Few-Shot   Training-Free Adaptation of Vision-Language Model</h2><p><strong>Authors:Yushu Li, Yongyi Su, Adam Goodge, Kui Jia, Xun Xu</strong></p>
<p>Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Although label, training, and data efficiency have improved, many state-of-the-art VLMs still require task-specific hyperparameter tuning and fail to fully exploit test samples. To overcome these challenges, we propose a graph-based approach for label-efficient adaptation and inference. Our method dynamically constructs a graph over text prompts, few-shot examples, and test samples, using label propagation for inference without task-specific tuning. Unlike existing zero-shot label propagation techniques, our approach requires no additional unlabeled support set and effectively leverages the test sample manifold through dynamic graph expansion. We further introduce a context-aware feature re-weighting mechanism to improve task adaptation accuracy. Additionally, our method supports efficient graph expansion, enabling real-time inductive inference. Extensive evaluations on downstream tasks, such as fine-grained categorization and out-of-distribution generalization, demonstrate the effectiveness of our approach. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/Yushu-Li/ECALP">https://github.com/Yushu-Li/ECALP</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）通过利用大型预训练模型来解决各种下游任务，从而革新了机器学习。尽管标签、训练和数据的效率已经提高，但许多最先进的VLMs仍然需要针对特定任务的超参数调整，并且未能充分利用测试样本。为了克服这些挑战，我们提出了一种基于图的标签高效适应和推理方法。我们的方法动态地在文本提示、少量示例和测试样本上构建图，使用标签传播进行推理，无需特定任务的调整。与现有的零样本标签传播技术不同，我们的方法不需要额外的无标签支持集，并通过动态图扩展有效地利用测试样本流形。我们还引入了一种上下文感知的特征重新加权机制，以提高任务适应的准确性。此外，我们的方法支持高效的图扩展，能够实现实时归纳推理。在下游任务（如精细分类和分布外泛化）上的广泛评估证明了我们的方法的有效性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/Yushu-Li/ECALP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yushu-Li/ECALP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18303v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用大型预训练模型处理下游任务的视觉语言模型（VLMs）。尽管标签、训练和数据的效率有所提升，但现有的先进VLMs仍需要针对特定任务的超参数调整，且无法充分利用测试样本。为应对这些挑战，提出了一种基于图的标签高效适应和推理方法。该方法动态构建文本提示、少量示例和测试样本的图，利用标签传播进行推理，无需特定任务调整。该方法无需额外的无标签支持集，通过动态图扩展有效利用了测试样本流形。此外，还引入了一种基于上下文特征的重加权机制，以提高任务适应的准确性。该方法支持有效的图扩展，可实现实时归纳推理。在下游任务上的广泛评估证明了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMs已经利用大型预训练模型处理各种下游任务。</li>
<li>现有VLMs需要针对特定任务的超参数调整，且无法充分利用测试样本。</li>
<li>提出了一种基于图的标签高效适应和推理方法，通过动态构建文本提示、少量示例和测试样本的图进行推理。</li>
<li>该方法利用标签传播进行推理，无需特定任务调整，且无需额外的无标签支持集。</li>
<li>通过动态图扩展，该方法有效利用了测试样本流形。</li>
<li>引入了一种基于上下文特征的重加权机制，提高任务适应的准确性。</li>
<li>该方法支持实时归纳推理，并在下游任务上的广泛评估证明了其有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18303">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-06b0fae190ba2c37f2b3125b672cdda7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c135842bf9d4d897b65735a9cef63ae.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Limited-Impact-of-Medical-Adaptation-of-Large-Language-and-Vision-Language-Models"><a href="#The-Limited-Impact-of-Medical-Adaptation-of-Large-Language-and-Vision-Language-Models" class="headerlink" title="The Limited Impact of Medical Adaptation of Large Language and   Vision-Language Models"></a>The Limited Impact of Medical Adaptation of Large Language and   Vision-Language Models</h2><p><strong>Authors:Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst</strong></p>
<p>Several recent works seek to adapt general-purpose large language models (LLMs) and vision-language models (VLMs) for medical applications through continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining improves performance on various downstream medical tasks, such as answering medical exam questions. In this paper, we compare ten “medical” LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-&#x2F;few-shot prompting and supervised fine-tuning regimes for medical question answering (QA). For instance, on clinical-note-based QA tasks in the 3-shot setting, medical LLMs outperform their base models in only 26.7% of cases, reach a (statistical) tie in 16.7% of cases, and perform significantly worse in the remaining 56.7% of cases. Our conclusions are based on (i) comparing each medical model directly against its base model; (ii) optimizing the prompts for each model separately in zero-&#x2F;few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies. </p>
<blockquote>
<p>近期有几项研究尝试通过对公开可用的生物医学语料库进行持续预训练，将通用的大型语言模型（LLMs）和视觉语言模型（VLMs）适应于医疗应用。这些研究通常声称，这种领域自适应预训练能提高在各种下游医疗任务上的性能，如回答医学考试问题。在本文中，我们将十种“医疗”LLMs和两种VLMs与它们的基础模型进行比较，得出了不同的结论：所有的医疗VLMs和几乎所有的医疗LLMs在零&#x2F;少镜头提示和监督微调制度下，都无法持续提高其基础模型在医疗问答（QA）上的性能。例如，在基于临床笔记的QA任务的3镜头设置中，医疗LLMs仅在26.7%的情况下超越其基础模型，在16.7%的情况下与基础模型表现相当（统计学上的平手），并在其余56.7%的情况下表现显著更差。我们的结论是基于（i）将每个医疗模型直接与其基础模型进行比较；（ii）在零&#x2F;少镜头提示中分别为每个模型优化提示；（iii）比较中考虑到统计不确定性。我们的研究结果表明，最先进的通用领域模型可能已经展现出强大的医疗知识和推理能力，并为未来研究加强结论提供了建议。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08870v2">PDF</a> Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes   additional results on clinical note QA tasks and supervised fine-tuning   evaluations</p>
<p><strong>Summary</strong></p>
<p>适应医疗应用领域的通用大型语言模型（LLMs）和视觉语言模型（VLMs）的研究工作普遍认为，通过在公开可用的生物医学语料库上进行领域自适应预训练，可以提高在各种下游医疗任务上的性能。然而，本文对比了十种“医疗”LLMs和两种VLMs及其基础模型，得出不同结论：在零&#x2F;少镜头提示和监督微调制度下，几乎所有医疗VLMs和近大部分医疗LLMs在医疗问答任务上并未比基础模型有显著改善。例如，在基于临床笔记的问答任务的3个样本中，医疗LLMs仅在26.7%的情况下表现优于基础模型，在16.7%的情况下表现相当，并在剩下的56.7%的情况下表现更差。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期多项研究尝试通过持续在公开生物医学语料库上进行预训练，将通用大型语言模型和视觉语言模型适应于医疗应用。</li>
<li>本文对比了医疗LLMs和VLMs及其基础模型，发现在零&#x2F;少镜头提示和监督微调环境下，这些医疗模型在医疗问答任务上未显著优于基础模型。</li>
<li>在基于临床笔记的3个样本问答任务中，医疗LLMs仅在少数情况下表现较好，大部分情况下与基础模型表现相当或更差。</li>
<li>本文的研究方法包括直接对比医疗模型和基础模型、针对每个模型分别优化提示，并考虑比较中的统计不确定性。</li>
<li>研究结果表明，现有的通用领域模型可能已经具备强大的医疗知识和推理能力。</li>
<li>本文为未来的研究提供了关于如何强化模型在医疗领域表现的建议。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08870">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a24737e4bfdc36089904acd76dce88cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f1047aadd0e30c124594ed99621eb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cefc72433c24a1dd4c5f8fa57bc79534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b88a07ecb9923d34a1255ce614e3c0e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ARIC-An-Activity-Recognition-Dataset-in-Classroom-Surveillance-Images"><a href="#ARIC-An-Activity-Recognition-Dataset-in-Classroom-Surveillance-Images" class="headerlink" title="ARIC: An Activity Recognition Dataset in Classroom Surveillance Images"></a>ARIC: An Activity Recognition Dataset in Classroom Surveillance Images</h2><p><strong>Authors:Linfeng Xu, Fanman Meng, Qingbo Wu, Lili Pan, Heqian Qiu, Lanxiao Wang, Kailong Chen, Kanglei Geng, Yilei Qian, Haojie Wang, Shuchang Zhou, Shimou Ling, Zejia Liu, Nanlin Chen, Yingjie Xu, Shaoxu Cheng, Bowen Tan, Ziyong Xu, Hongliang Li</strong></p>
<p>The application of activity recognition in the &#96;&#96;AI + Education” field is gaining increasing attention. However, current work mainly focuses on the recognition of activities in manually captured videos and a limited number of activity types, with little attention given to recognizing activities in surveillance images from real classrooms. Activity recognition in classroom surveillance images faces multiple challenges, such as class imbalance and high activity similarity. To address this gap, we constructed a novel multimodal dataset focused on classroom surveillance image activity recognition called ARIC (Activity Recognition In Classroom). The ARIC dataset has advantages of multiple perspectives, 32 activity categories, three modalities, and real-world classroom scenarios. In addition to the general activity recognition tasks, we also provide settings for continual learning and few-shot continual learning. We hope that the ARIC dataset can act as a facilitator for future analysis and research for open teaching scenarios. You can download preliminary data from <a target="_blank" rel="noopener" href="https://ivipclab.github.io/publication_ARIC/ARIC">https://ivipclab.github.io/publication_ARIC/ARIC</a>. </p>
<blockquote>
<p>“AI +教育”领域中活动识别的应用越来越受到关注。然而，当前的研究工作主要集中在手动捕获视频中的活动识别以及有限的活动类型上，对于从真实课堂监控图像中识别活动的关注较少。课堂监控图像中的活动识别面临多重挑战，如类别不平衡和活动高度相似等。为了弥补这一空白，我们构建了一个专注于课堂监控图像活动识别的新型多模式数据集，名为ARIC（课堂活动识别）。ARIC数据集具有多角度、32类活动、三种模态和真实课堂场景的优势。除了一般的活动识别任务外，我们还提供了持续学习和少量样本持续学习的设置。我们希望ARIC数据集能为开放教学场景的未来分析和研究提供帮助。您可以从<a target="_blank" rel="noopener" href="https://ivipclab.github.io/publication_ARIC/ARIC">https://ivipclab.github.io/publication_ARIC/ARIC</a>下载初步数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12337v2">PDF</a> arXiv admin note: text overlap with arXiv:2409.03354. Updated the   description for ARIC supplement</p>
<p><strong>Summary</strong><br>活动识别在“人工智能+教育”领域的应用越来越受到关注。当前研究主要集中在手动采集视频的活动识别及有限的活动类型上，而针对教室监控图像的活动识别研究较少。教室监控图像的活动识别面临类不平衡、活动相似性高等挑战。为解决此问题，我们构建了一个专注于教室监控图像活动识别的多模态数据集ARIC。ARIC数据集具有多角度、32类活动、三模态和真实课堂场景优势，并设置了持续学习和小样本持续学习的场景。我们希望ARIC数据集能为开放教学环境未来的分析和研究提供便利。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>活动识别在“人工智能+教育”领域的应用正逐渐受到关注。</li>
<li>当前研究主要集中在手动采集视频的活动识别，对教室监控图像的活动识别研究较少。</li>
<li>教室监控图像的活动识别面临类不平衡和活动相似性高等挑战。</li>
<li>ARIC数据集是一个专注于教室监控图像活动识别的多模态数据集。</li>
<li>ARIC数据集具有多角度、32类活动、三模态和真实课堂场景的优势。</li>
<li>ARIC数据集设置了持续学习和小样本持续学习的场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-552fc82ca5647a4f9c660d8c8e9f4865.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-265cc5d493afdd38516dabfd4f26479a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f446ed88066b7b5d8eccdb33b8f1be9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-394b0d23c50fa5ac8386b5807ff4a38e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c650cc82bebd8aa4e2b6c61197c85bd1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Training-Free-Exponential-Context-Extension-via-Cascading-KV-Cache"><a href="#Training-Free-Exponential-Context-Extension-via-Cascading-KV-Cache" class="headerlink" title="Training-Free Exponential Context Extension via Cascading KV Cache"></a>Training-Free Exponential Context Extension via Cascading KV Cache</h2><p><strong>Authors:Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang</strong></p>
<p>The transformer’s context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill&#x2F;prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency. </p>
<blockquote>
<p>变压器的上下文窗口对于少样本学习和条件生成等任务至关重要，因为它可以保留先前的令牌作为活动内存。然而，随着上下文长度的增加，计算成本呈二次方增长，阻碍了大型语言模型（LLM）在现实世界中的长序列场景部署。尽管最近的一些键值缓存（KV Cache）方法提供了线性的推理复杂性，但它们简单地管理存储的上下文，过早地逐出令牌并丢失有价值的信息。此外，它们缺乏优化的预填充&#x2F;提示阶段策略，导致在实际上下文大小下的延迟甚至高于二次注意力。作为回应，我们引入了一种新型机制，该机制利用级联的子缓存缓冲区来有选择地保留最相关的令牌，使模型能够在不增加缓存大小的情况下保持更长的上下文历史。我们的方法在关键基准测试上超越了线性缓存基线，包括流式困惑度、问答、书籍摘要和通行证检索等。在缓存大小翻倍四次达到65K后，我们在1M令牌上保持了更好的检索准确性。此外，我们的方法在1M令牌上与闪光注意力相比，预填充阶段延迟降低了6.8倍。这些创新不仅提高了大型语言模型的计算效率，还为它们在资源受限环境中的有效部署铺平了道路，实现了大规模、实时的应用程序，并显著降低了延迟。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17808v3">PDF</a> </p>
<p><strong>Summary</strong>：<br>文中探讨了在少样本学习和条件生成等任务中，transformer的上下文窗口的重要性及其带来的计算成本问题。为解决这一问题，提出了利用级联子缓存缓冲区的新机制，有选择地保留最相关的令牌，使模型能够保持更长的上下文历史，同时不增加缓存大小。此方法在关键基准测试中优于线性缓存基线，包括流式困惑度、问答、书籍摘要和密钥检索等，并且在缓存大小翻倍时仍能保持较好的检索准确性。此外，该方法还降低了填充阶段的延迟。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Transformer的上下文窗口对于少样本学习和条件生成等任务至关重要，但计算成本随上下文长度的增加而增加。</li>
<li>现有的KV Cache方法虽然提供线性推理复杂性，但管理存储的上下文的方式过于简单，可能导致过早淘汰令牌和丢失有价值的信息。</li>
<li>提出的级联子缓存缓冲区机制能够有选择地保留最相关的令牌，维持更长的上下文历史，同时不增加缓存大小。</li>
<li>该方法在关键基准测试中表现优异，包括流式困惑度、问答、书籍摘要和密钥检索等任务。</li>
<li>在缓存大小翻倍时，该方法仍然能保持较好的检索准确性。</li>
<li>与flash attention相比，该方法在1M令牌的情况下将填充阶段延迟降低了6.8倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17808">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef7f6a90ecb9ef49b8af5dc5f5b2a1d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9d4f27f40553795118d8097a2645c5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b56923c4a88ce9e07d2338dec266d002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3291149c952ca7ee58168dcb7cbfd42b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea92457bc0a6b5619b47df496a2e8083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-551c1277524d61403fce9c8d396b3de8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LTSM-Bundle-A-Toolbox-and-Benchmark-on-Large-Language-Models-for-Time-Series-Forecasting"><a href="#LTSM-Bundle-A-Toolbox-and-Benchmark-on-Large-Language-Models-for-Time-Series-Forecasting" class="headerlink" title="LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time   Series Forecasting"></a>LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time   Series Forecasting</h2><p><strong>Authors:Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Songyuan Sui, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, Xia Hu</strong></p>
<p>Time Series Forecasting (TSF) has long been a challenge in time series analysis. Inspired by the success of Large Language Models (LLMs), researchers are now developing Large Time Series Models (LTSMs)-universal transformer-based models that use autoregressive prediction-to improve TSF. However, training LTSMs on heterogeneous time series data poses unique challenges, including diverse frequencies, dimensions, and patterns across datasets. Recent endeavors have studied and evaluated various design choices aimed at enhancing LTSM training and generalization capabilities. However, these design choices are typically studied and evaluated in isolation and are not benchmarked collectively. In this work, we introduce LTSM-Bundle, a comprehensive toolbox, and benchmark for training LTSMs, spanning pre-processing techniques, model configurations, and dataset configuration. It modularized and benchmarked LTSMs from multiple dimensions, encompassing prompting strategies, tokenization approaches, training paradigms, base model selection, data quantity, and dataset diversity. Furthermore, we combine the most effective design choices identified in our study. Empirical results demonstrate that this combination achieves superior zero-shot and few-shot performances compared to state-of-the-art LTSMs and traditional TSF methods on benchmark datasets. </p>
<blockquote>
<p>时间序列预测（TSF）一直是时间序列分析中的一大挑战。受大型语言模型（LLM）成功的启发，研究人员现在正在开发基于通用变压器的大型时间序列模型（LTSM），采用自回归预测来提高TSF。然而，在异质时间序列数据上训练LTSM面临独特挑战，包括数据集间的频率、维度和模式的多样性。近期的研究工作已经研究和评估了旨在提高LTSM训练和泛化能力的各种设计选择。然而，这些设计选择通常孤立地研究和评估，并未集体进行基准测试。在这项工作中，我们介绍了LTSM-Bundle，一个全面的工具包和基准测试平台，用于训练LTSM，涵盖预处理技术、模型配置和数据集配置。它从多个维度对LTSM进行模块化和基准测试，包括提示策略、标记化方法、训练范式、基础模型选择、数据量以及数据集多样性。此外，我们将结合研究中确定的最有效的设计选择。实证结果表明，与基准数据集上的最新LTSM和传统TSF方法相比，这种组合实现了零样本和少样本性能上的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14045v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>研究人员利用大型语言模型（LLM）的理念，开发了一种基于通用转换器的大型时间序列模型（LTSM），以改进时间序列预测（TSF）。然而，在异质时间序列数据上训练LTSM面临独特挑战，包括数据集的频率、维度和模式的多样性。近期研究着眼于增强LTSM训练和泛化能力的设计选择，但通常是孤立地研究并评估的，并没有统一的标准进行评估。为此，本文引入了LTSM捆绑包，一个全面的工具箱和基准测试平台，涵盖预处理技术、模型配置和数据集配置。它从多个维度模块化并评估LTSM，包括提示策略、标记化方法、训练模式、基础模型选择等。此外，本文结合了研究中发现的最有效的设计选择，实证结果表明该组合在基准数据集上相较于最先进LTSM和传统TSF方法实现了零样本和少样本性能的优越表现。
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LTSMs被开发为改进时间序列预测（TSF）。</li>
<li>LTSMs在训练异质时间序列数据时面临独特挑战。</li>
<li>目前对LTSM的设计选择通常是孤立研究和评估的，缺乏统一的基准测试。</li>
<li>LTSM-Bundle工具箱囊括了多种设计选择的模块化评估，包括提示策略、标记化方法等。</li>
<li>结合最有效的设计选择，实现了零样本和少样本性能上的优越表现。</li>
<li>该研究为LTSMs的进一步发展和应用提供了宝贵的参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d0dd0dca60a8e70ba2b084adda7da81c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc2b037f6d512ad805734e98d29828d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03d3d6b45c616ab3fb75c31f2853b29b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4ac2b9d50c996daeee1129ac0205ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f427b7582c1565b6ba41288c75ff0c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aef481bd9d1497a07b27f308d454f1a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-643c558eb5764a2680b38874de069f06.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ab7dbd6993c351be6253d325bfce3a31.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-04  GUIDE LLM-Driven GUI Generation Decomposition for Automated Prototyping
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c82cda3fc8611445c36642fe3497feb1.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-03-04  Chitranuvad Adapting Multi-Lingual LLMs for Multimodal Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
