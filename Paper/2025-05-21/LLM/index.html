<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-05-21  Optimizing Anytime Reasoning via Budget Relative Policy Optimization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1103a4e20d6934d949a1b818d06d3b19.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-21-更新"><a href="#2025-05-21-更新" class="headerlink" title="2025-05-21 更新"></a>2025-05-21 更新</h1><h2 id="Optimizing-Anytime-Reasoning-via-Budget-Relative-Policy-Optimization"><a href="#Optimizing-Anytime-Reasoning-via-Budget-Relative-Policy-Optimization" class="headerlink" title="Optimizing Anytime Reasoning via Budget Relative Policy Optimization"></a>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</h2><p><strong>Authors:Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin</strong></p>
<p>Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency. </p>
<blockquote>
<p>在大型语言模型（LLM）中，提升测试时间的计算能力对于增强推理能力至关重要。现有方法通常采用强化学习（RL）来最大化推理轨迹末尾的可验证奖励。然而，这些方法仅针对固定标记预算下的最终性能进行优化，这阻碍了训练和部署的效率。在本研究中，我们提出了一种新的框架AnytimeReasoner，旨在优化任意时间的推理性能，旨在提高标记效率和在不同标记预算约束下的推理灵活性。为了实现这一点，我们将完整的思考过程截断以适应从先验分布中采样的标记预算，迫使模型为每次截断的思考过程总结最佳答案以进行验证。这为推理过程引入了可验证的密集奖励，有助于更有效的强化学习优化中的奖励分配。然后，我们以解耦的方式优化思考和总结策略以最大化累积奖励。此外，我们引入了一种新的方差减少技术——预算相对策略优化（BRPO），以提高在强化思考策略时学习过程的稳健性和效率。在数学推理任务的实证结果表明，我们的方法在所有思考预算下均优于GRPO在各种先验分布的表现，提高了训练和标记的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13438v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在推理过程中需要处理大量计算任务，为此必须扩大测试时间计算规模以提升模型推理能力。传统的强化学习（RL）方法仅在最终阶段最大化奖励得分，而这无法满足持续变化的标记需求和提高模型的灵活性及令牌效率的需求。为此，本研究提出了一种名为AnytimeReasoner的新框架，以优化任意时间的推理性能，提升模型的令牌效率和在不同令牌预算约束下的灵活性。其实现过程包括对完整推理过程的截断处理以适应特定采样令牌预算需求，要求模型必须汇总不同截断情况下的最优答案以便进行验证，引入了一种即时验证机制并产生了密集型奖励信息以增强学习的可靠性及调整任务的动态执行路径以提升执行效率和避免错过获取中间阶段正确的回答而导致失控问题。此外，本研究还提出了一种新的方差缩减技术——预算相对策略优化（BRPO），以增强策略学习过程时的稳健性和效率。在实证的数学推理任务中，该方法在多种先验分布下各思考预算均表现出优于GRPO的效果，表明此方法能够在提高推理性能和增强计算效率两方面提供稳健表现。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是基于文本提取的七个关键见解：</p>
<ol>
<li>测试时间的计算扩展对于增强大型语言模型（LLM）的推理能力至关重要。</li>
<li>传统强化学习（RL）方法在处理大型语言模型时存在不足，因为它们主要关注最终性能的优化。</li>
<li>AnytimeReasoner框架旨在优化任意时间的推理性能，提高模型的令牌效率和灵活性。</li>
<li>AnytimeReasoner通过截断完整的推理过程以适应不同的令牌预算约束，并引入即时验证和奖励信息机制提升性能。</li>
<li>该框架要求对模型在不同截断思考下的最优答案进行汇总以验证输出准确性并改善动态任务路径的调整过程。</li>
<li>提出了一种新的方差缩减技术——预算相对策略优化（BRPO），以增强学习过程的稳健性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13438">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab7f0514cd9aaee07b0e579d019548a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e18da88445987aa06e1873ce9ae3403.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3854d94a83e2188024a13add244d982e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-088a2ec05312ed15c4112a44fdcb572f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-PRM-Enhancing-Multimodal-Mathematical-Reasoning-with-Scalable-Step-Level-Supervision"><a href="#MM-PRM-Enhancing-Multimodal-Mathematical-Reasoning-with-Scalable-Step-Level-Supervision" class="headerlink" title="MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable   Step-Level Supervision"></a>MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable   Step-Level Supervision</h2><p><strong>Authors:Lingxiao Du, Fanqing Meng, Zongkai Liu, Zhixiang Zhou, Ping Luo, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-PRM">https://github.com/ModalMinds/MM-PRM</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLM）在视觉语言理解方面取得了令人印象深刻的进展，但在处理复杂的多步骤推理时仍面临挑战，常常产生逻辑不一致或部分正确的解决方案。一个关键的局限性在于中间推理步骤缺乏精细的监督。为了解决这一问题，我们提出了MM-PRM，这是一个在全自动化可扩展框架中训练的流程奖励模型。我们首先建立了一个强大的多模态模型MM-Policy，该模型在多样化的数学推理数据上进行训练。然后，我们构建了MM-K12数据集，这是一个包含一万道可验证答案的多模态数学问题集，作为种子数据。通过利用基于蒙特卡洛树搜索（MCTS）的管道，我们生成了超过70万步级别的注释，无需人工标注。所得的PRM用于在Best-of-N推理设置中评分候选推理路径，并在内部领域（MM-K12测试集）和外部领域（OlympiadBench、MathVista等）的基准测试中实现了显著改进。进一步的分析证实了软标签、较小学习率和路径多样性在优化PRM性能方面的有效性。MM-PRM证明过程监督是提高多模态推理系统逻辑稳健性的有力工具。我们已将所有代码和数据发布在<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-PRM%E3%80%82">https://github.com/ModalMinds/MM-PRM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对多模态大型语言模型（MLLMs）在复杂多步骤推理方面存在的不足，提出了一种名为MM-PRM的过程奖励模型。该模型通过构建MM-Policy和MM-K12，利用蒙特卡洛树搜索（MCTS）生成超过70万步的注释数据，实现了对候选推理路径的评分，显著提高了在域内和域外基准测试上的性能。研究证实了过程监督在增强多模态推理系统的逻辑稳健性方面的强大作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在视觉语言理解方面取得了显著进展，但在复杂多步骤推理方面仍存在挑战。</li>
<li>当前模型在推理过程中常产生逻辑不一致或部分正确的解决方案。</li>
<li>MM-PRM模型通过构建MM-Policy和MM-K12数据集来解决这一问题。</li>
<li>MM-K12数据集包含1万个多模态数学问题及其可验证的答案，作为种子数据。</li>
<li>利用蒙特卡洛树搜索（MCTS）生成了超过70万步的注释数据，实现了全自动、可扩展的过程奖励模型训练。</li>
<li>PRM在Best-of-N推理设置中对候选推理路径进行评分，显著提高了在多种基准测试上的性能。</li>
<li>研究结果证实了过程监督在增强多模态推理系统的逻辑稳健性方面的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c915a603cf50a0a534b2d84ce3fd7677.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ad800df424f2c31542099f4e1ec67b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learnware-of-Language-Models-Specialized-Small-Language-Models-Can-Do-Big"><a href="#Learnware-of-Language-Models-Specialized-Small-Language-Models-Can-Do-Big" class="headerlink" title="Learnware of Language Models: Specialized Small Language Models Can Do   Big"></a>Learnware of Language Models: Specialized Small Language Models Can Do   Big</h2><p><strong>Authors:Zhi-Hao Tan, Zi-Chen Zhao, Hao-Yu Shi, Xin-Yu Zhang, Peng Tan, Yang Yu, Zhi-Hua Zhou</strong></p>
<p>The learnware paradigm offers a novel approach to machine learning by enabling users to reuse a set of well-trained models for tasks beyond the models’ original purposes. It eliminates the need to build models from scratch, instead relying on specifications (representations of a model’s capabilities) to identify and leverage the most suitable models for new tasks. While learnware has proven effective in many scenarios, its application to language models has remained largely unexplored. At the same time, large language models (LLMs) have demonstrated remarkable universal question-answering abilities, yet they face challenges in specialized scenarios due to data scarcity, privacy concerns, and high computational costs, thus more and more specialized small language models (SLMs) are being trained for specific domains. To address these limitations systematically, the learnware paradigm provides a promising solution by enabling maximum utilization of specialized SLMs, and allowing users to identify and reuse them in a collaborative and privacy-preserving manner.   This paper presents a preliminary attempt to apply the learnware paradigm to language models. We simulated a learnware system comprising approximately 100 learnwares of specialized SLMs with 8B parameters, fine-tuned across finance, healthcare, and mathematics domains. Each learnware contains an SLM and a specification, which enables users to identify the most relevant models without exposing their own data. Experimental results demonstrate promising performance: by selecting one suitable learnware for each task-specific inference, the system outperforms the base SLMs on all benchmarks. Compared to LLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and Llama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses Flan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical domain tasks. </p>
<blockquote>
<p>学习软件范式通过使用户能够重复使用一组经过良好训练的模型来完成超出模型原始目的的的任务，为机器学习提供了一种新颖的方法。它消除了从头构建模型的需求，而是依赖于规范（模型的能力的表示）来识别和利用最适合新任务的模型。虽然学习软件在许多场景中已被证明是有效的，但其对语言模型的应用却鲜有探索。与此同时，大型语言模型（LLM）已经显示出惊人的通用问答能力，但由于数据稀缺、隐私担忧和计算成本高昂等挑战，它们在特定场景中的表现受到限制。因此，越来越多的针对特定领域的小型语言模型（SLM）正在接受训练。为了系统地解决这些局限性，学习软件范式通过最大限度地利用专门的SLM并允许用户以协作和隐私保护的方式识别和重复使用它们，提供了一个有前景的解决方案。本文是尝试将学习软件范式应用于语言模型的初步尝试。我们模拟了一个学习软件系统，其中包括大约100个SLM的组件（具有跨金融、医疗和数学领域的微调参数），每个学习软件中都包含一个SLM和一个规范，使用户能够识别最相关的模型而不暴露自己的数据。实验结果表明，通过为每个特定任务选择适当的学习软件来进行推理，该系统在所有基准测试中均优于基础SLM的性能。与LLM相比，该系统在金融领域任务中至少优于Qwen 1.5-110B、Qwen 2.5-72B和Llama 3.1-70B-Instruct 14%，并在医疗领域任务中超越了在开放医学LLM排行榜上排名第七的Flan-PaLM-540B。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了学习软件范式在机器学习领域的新应用，特别是在语言模型中的应用。学习软件范式允许用户重用已训练好的模型来完成超出其原始目的的任务。文章尝试将学习软件范式应用于语言模型，模拟了一个包含约100个学习软件的专业小型语言模型系统，并在金融、医疗和数学领域进行了微调。实验结果表明，该系统在选择适合每个任务的学习软件时，其性能优于基础小型语言模型和大语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学习软件范式允许用户重用已训练好的模型，避免了从头开始构建模型的繁琐过程。</li>
<li>该范式通过规格（模型的表示）来识别和利用最适合新任务的模型。</li>
<li>大型语言模型（LLMs）虽然在通用问答方面表现出色，但在特定领域面临数据稀缺、隐私关注和计算成本高等挑战。</li>
<li>学习软件范式为解决这些挑战提供了有希望的解决方案，通过最大化利用专业小型语言模型（SLMs）并允许用户以协作和隐私保护的方式识别和重用它们。</li>
<li>模拟系统包括大约100个学习软件的专业小型语言模型，涵盖金融、医疗和数学等领域。</li>
<li>实验结果表明，该系统在选择适合每个任务的学习软件时，性能优于基础小型语言模型和大语言模型在特定领域的任务表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13425">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-366e059a335a61a793726536673d75ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9be61d689eb52adbfade6bd104b5c37b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931ceb28429bec55bf1fd30df7461f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2949c55dd9f9705a34d004f0d57a9594.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-714127920524533565d156965e89c4ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Make-Still-Further-Progress-Chain-of-Thoughts-for-Tabular-Data-Leaderboard"><a href="#Make-Still-Further-Progress-Chain-of-Thoughts-for-Tabular-Data-Leaderboard" class="headerlink" title="Make Still Further Progress: Chain of Thoughts for Tabular Data   Leaderboard"></a>Make Still Further Progress: Chain of Thoughts for Tabular Data   Leaderboard</h2><p><strong>Authors:Si-Yang Liu, Qile Zhou, Han-Jia Ye</strong></p>
<p>Tabular data, a fundamental data format in machine learning, is predominantly utilized in competitions and real-world applications. The performance of tabular models–such as gradient boosted decision trees and neural networks–can vary significantly across datasets due to differences in feature distributions and task characteristics. Achieving top performance on each dataset often requires specialized expert knowledge. To address this variability, practitioners often aggregate the predictions of multiple models. However, conventional aggregation strategies typically rely on static combination rules and lack instance-level adaptability. In this work, we propose an in-context ensemble framework for tabular prediction that leverages large language models (LLMs) to perform dynamic, instance-specific integration of external model predictions. Without access to raw tabular features or semantic information, our method constructs a context around each test instance using its nearest neighbors and the predictions from a pool of external models. Within this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$), a prompting strategy that guides LLMs through multi-step, interpretable reasoning, making still further progress toward expert-level decision-making. Experimental results show that our method outperforms well-tuned baselines and standard ensemble techniques across a wide range of tabular datasets. </p>
<blockquote>
<p>表格数据是机器学习中的基本数据格式，在竞赛和实际应用中得到了广泛应用。由于特征分布和任务特性的差异，梯度增强决策树和神经网络等表格模型的性能在不同数据集上可能会有很大差异。在每个数据集上实现最佳性能通常需要专业的专家知识。为了应对这种差异，从业者通常会聚合多个模型的预测结果。然而，传统的聚合策略通常依赖于静态的组合规则，缺乏实例级别的适应性。在这项工作中，我们提出了一种用于表格预测的上下文集成框架，该框架利用大型语言模型（LLM）执行动态、针对特定实例的外部模型预测集成。我们的方法无需访问原始表格特征或语义信息，而是围绕每个测试实例构建上下文环境，使用其最近邻居和来自外部模型池中的预测结果。在这个丰富的上下文环境中，我们引入了“表格思维链”（CoT$^2$），这是一种提示策略，通过多步骤的可解释推理来引导LLM，进一步推动向专家级决策制定迈进。实验结果表明，我们的方法在多种表格数据集上优于经过良好调整的基准模型和标准的集成技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13421v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文中主要探讨了表格数据在机器学习领域的重要性及其在竞赛和实际应用中的广泛应用。文章指出，由于特征分布和任务特性的差异，梯度增强决策树和神经网络等表格模型的性能在不同数据集上可能会有很大差异。为了应对这种差异并实现最佳性能，研究人员通常采用多种模型的预测聚合。然而，传统的聚合策略通常依赖于静态组合规则，缺乏实例级别的适应性。因此，本文提出了一种基于大型语言模型（LLM）的上下文集成框架，用于动态、特定实例地整合外部模型预测。该方法通过构建每个测试实例的上下文环境（使用其最近邻居和外部模型的预测），在不需要原始表格特征或语义信息的情况下，引入了一种名为Chain of Tabular Thoughts（CoT$^2$）的提示策略，通过多步骤、可解释性的推理引导LLM进行决策。实验结果表明，该方法在多种表格数据集上的性能优于精心调整的基准模型和标准的集成技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>表格数据在机器学习和实际应用中广泛应用。</li>
<li>表格模型的性能在不同数据集上可能会有显著差异，需考虑特征分布和任务特性。</li>
<li>为了提高性能，研究人员常采用多模型预测聚合的方法。</li>
<li>传统聚合策略缺乏实例级别的适应性。</li>
<li>本文提出了一种基于大型语言模型的上下文集成框架，用于动态整合外部模型预测。</li>
<li>该方法通过构建测试实例的上下文环境，引入Chain of Tabular Thoughts（CoT$^2$）提示策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2693a9fe345d5ffc5aaa7d8e4a5e0400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0b1ebad1e917b7ca7779ff49ed7c839.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e122b0d730ae7ea0818fff68d54ba38e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FEALLM-Advancing-Facial-Emotion-Analysis-in-Multimodal-Large-Language-Models-with-Emotional-Synergy-and-Reasoning"><a href="#FEALLM-Advancing-Facial-Emotion-Analysis-in-Multimodal-Large-Language-Models-with-Emotional-Synergy-and-Reasoning" class="headerlink" title="FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language   Models with Emotional Synergy and Reasoning"></a>FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language   Models with Emotional Synergy and Reasoning</h2><p><strong>Authors:Zhuozhao Hu, Kaishen Yuan, Xin Liu, Zitong Yu, Yuan Zong, Jingang Shi, Huanjing Yue, Jingyu Yang</strong></p>
<p>Facial Emotion Analysis (FEA) plays a crucial role in visual affective computing, aiming to infer a person’s emotional state based on facial data. Scientifically, facial expressions (FEs) result from the coordinated movement of facial muscles, which can be decomposed into specific action units (AUs) that provide detailed emotional insights. However, traditional methods often struggle with limited interpretability, constrained generalization and reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have shown exceptional performance in various visual tasks, while they still face significant challenges in FEA due to the lack of specialized datasets and their inability to capture the intricate relationships between FEs and AUs. To address these issues, we introduce a novel FEA Instruction Dataset that provides accurate and aligned FE and AU descriptions and establishes causal reasoning relationships between them, followed by constructing a new benchmark, FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to capture more detailed facial information, enhancing its capability in FEA tasks. Our model demonstrates strong performance on FEABench and impressive generalization capability through zero-shot evaluation on various datasets, including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and effectiveness in FEA tasks. The dataset and code will be available at <a target="_blank" rel="noopener" href="https://github.com/953206211/FEALLM">https://github.com/953206211/FEALLM</a>. </p>
<blockquote>
<p>面部情感分析（FEA）在视觉情感计算中扮演着至关重要的角色，其旨在基于面部数据推断一个人的情感状态。科学上，面部表情（FE）是面部肌肉协调运动的结果，可以分解为特定的动作单元（AU），从而提供详细的情感洞察。然而，传统的方法常常在解释性、通用性和推理能力方面存在局限。最近，多模态大型语言模型（MLLM）在各种视觉任务中表现出了出色的性能，而在FEA中仍然面临重大挑战，主要是由于缺乏专业数据集和无法捕捉面部表情和动作单元之间复杂的关系。为了解决这些问题，我们引入了一个新的面部情感分析指令数据集，提供准确和对齐的面部表情和动作单元描述，并建立它们之间的因果推理关系，然后构建了一个新的基准测试FEABench。此外，我们提出了FEALLM，这是一种新的MLLM架构，旨在捕捉更详细的面部信息，提高其在面部情感分析任务中的能力。我们的模型在FEABench上表现出强大的性能，在各种数据集（包括RAF-DB、AffectNet、BP4D和DISFA）上进行零样本评估时，展现出令人印象深刻的泛化能力，证明了其在面部情感分析任务中的稳健性和有效性。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/953206211/FEALLM">https://github.com/953206211/FEALLM</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13419v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>面部表情分析（FEA）在视觉情感计算中起着关键作用，旨在基于面部数据推断人的情感状态。面部表达是由面部肌肉协调运动产生的，可以分解为特定的动作单元（AU）以提供详细的情感见解。然而，传统方法在解释、推广和推理方面存在局限性。最近，多模态大型语言模型（MLLM）在各种视觉任务中表现出卓越性能，但在FEA方面仍面临缺乏专用数据集和无法捕捉面部表情与动作单元之间复杂关系的挑战。为解决这些问题，我们引入了新型的FEA指令数据集，提供了准确对齐的面部表情和动作单元描述，建立了它们之间的因果关系。我们还构建了新的基准测试FEABench，并提出了FEALLM，一种新型MLLM架构，能够捕捉更详细的面部信息，增强FEA任务的能力。模型在FEABench上表现出强大的性能，并在多个数据集上通过零样本评估展示了其在FEA任务中的稳健性和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面部表情分析（FEA）是视觉情感计算中的关键领域，旨在通过面部数据推断人的情感状态。</li>
<li>面部表达是由面部肌肉的协调运动产生的，可以分解为动作单元（AU）以获得详细的情感理解。</li>
<li>传统方法在FEA方面存在解释性、推广和推理能力的局限性。</li>
<li>多模态大型语言模型（MLLM）在视觉任务中表现出卓越性能，但在FEA中仍面临挑战，主要由于缺乏专用数据集和捕捉复杂关系的能力不足。</li>
<li>引入新型的FEA指令数据集，提供准确对齐的面部表情和动作单元描述，并建立它们之间的因果关系。</li>
<li>构建了新的FEA基准测试FEABench以评估模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-927f8f88ba3523bc7da087f24df358bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf72b786d27e40d66f8592056978fe79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717e8a18a3d6ada6d61fed5bbde2bb92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fc32467b9d87e5770b4942ecb1169ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c94279218da334e770e450bd610a88eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MR-Judge-Multimodal-Reasoner-as-a-Judge"><a href="#MR-Judge-Multimodal-Reasoner-as-a-Judge" class="headerlink" title="MR. Judge: Multimodal Reasoner as a Judge"></a>MR. Judge: Multimodal Reasoner as a Judge</h2><p><strong>Authors:Renjie Pi, Felix Bai, Qibin Chen, Simon Wang, Jiulong Shan, Kieran Liu, Meng Cao</strong></p>
<p>The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%. </p>
<blockquote>
<p>使用大型语言模型（LLM）和多模态大型语言模型（MLLM）作为评价者的范式，在RLHF和推理时间缩放中已成为一种有效的方法。在这项工作中，我们提出了“多模态推理者作为评判员”（MR Judge）这一范式，旨在赋予通用MLLM评判员强大的推理能力。我们并不直接为每种回应打分，而是将评判过程制定为受推理启发的多项选择题。具体来说，评判模型首先进行涵盖回应不同方面的深思熟虑的推理，并最终从中选择最佳的回应。这种推理过程不仅提高了评判的可解释性，还大大提高了MLLM评判员的表现。为了应对缺乏带评分回应的问题，我们提出以下策略来实现自动标注：1）反向回应候选合成：从监督微调（SFT）数据集开始，我们将原始回应视为最佳候选，并提示MLLM生成合理的但存在缺陷的负面候选；2）基于文本的推理提取：我们精心设计了一个数据合成管道，以从基于文本的推理模型中提炼推理能力，采用该模型使MLLM评委能够通过热身的监督微调恢复复杂的推理能力。实验表明，我们的MR Judge在多种任务上均表现出良好的效果。具体来说，我们的MR Judge-7B在VL-RewardBench上的表现优于GPT-4o，超过9.9%，并在推理时间缩放期间MM-Vet上的性能提高了高达7.7%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本摘要中，提出了一种基于多模态大语言模型（MLLMs）的通用评价法官模型——多模态推理法官（MR. Judge）。该模型将评价过程形式化为一个基于推理的多选问题，提高了评估的可靠性和模型的性能。为了解决缺少带有得分的响应问题，提出通过合成逆向响应候选和基于文本的推理提取策略实现自动标注。实验表明，MR. Judge在不同任务上均表现出优异的效果，如MR. Judge-7B在VL-RewardBench上的性能优于GPT-4o达9.9%，并在推理时间尺度上在MM-Vet上的性能提升高达7.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用多模态大语言模型（MLLMs）作为评价法官已成为有效方法。</li>
<li>提出多模态推理法官（MR. Judge）模型，将评价过程形式化为基于推理的多选问题。</li>
<li>通过合成逆向响应候选和基于文本的推理提取策略实现自动标注。</li>
<li>MR. Judge模型提高了评估的可靠性和模型的性能。</li>
<li>MR. Judge在多种任务上表现优异，特别是在VL-RewardBench和MM-Vet上的性能提升显著。</li>
<li>MR. Judge模型通过精细设计的合成数据管道实现复杂推理能力的提炼。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1809fb6603c738bee2dc5c105dec5e04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e36a0cfef9d9b2ae0fc9bdfd994aa76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19169a84e543cd10a04e8de471ccc9b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f422a2127e996d925673863d8a00ebad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CompeteSMoE-–-Statistically-Guaranteed-Mixture-of-Experts-Training-via-Competition"><a href="#CompeteSMoE-–-Statistically-Guaranteed-Mixture-of-Experts-Training-via-Competition" class="headerlink" title="CompeteSMoE – Statistically Guaranteed Mixture of Experts Training via   Competition"></a>CompeteSMoE – Statistically Guaranteed Mixture of Experts Training via   Competition</h2><p><strong>Authors:Nam V. Nguyen, Huy Nguyen, Quang Pham, Van Nguyen, Savitha Ramasamy, Nhat Ho</strong></p>
<p>Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network’s depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: <a target="_blank" rel="noopener" href="https://github.com/Fsoft-AIC/CompeteSMoE">https://github.com/Fsoft-AIC/CompeteSMoE</a>. This work is an improved version of the previous study at arXiv:2402.02526 </p>
<blockquote>
<p>稀疏混合专家网络（SMoE）提供了一种吸引人的解决方案，可以扩大模型复杂度，而不仅仅是增加网络的深度或宽度。然而，我们认为有效的SMoE训练仍然具有挑战性，因为次优的路由过程并不能使执行计算的专家直接贡献于路由过程。在这项工作中，我们提出了竞争机制，这是一种将令牌路由到具有最高神经响应的专家的新机制。从理论上讲，我们证明了竞争机制的样本效率高于传统的softmax路由。此外，我们开发了CompeteSMoE，这是一种简单有效的算法，通过部署路由器来学习竞争策略，从而以较低的训练开销享受强大的性能。我们在视觉指令调整和语言预训练任务上的广泛实证评估表明，与最新的SMoE策略相比，CompeteSMoE具有高效性、稳健性和可扩展性。我们的实现可在[<a target="_blank" rel="noopener" href="https://github.com/Fsoft-AIC/CompeteSMoE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E6%98%AFarXiv:2402.02526%E4%B9%8B%E5%89%8D%E7%A0%94%E7%A9%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E6%94%B9%E8%BF%9B%E7%89%88%E6%9C%AC%E3%80%82">https://github.com/Fsoft-AIC/CompeteSMoE上获得。这项工作是arXiv:2402.02526之前研究的一个改进版本。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13380v1">PDF</a> 52 pages. This work is an improved version of the previous study at   arXiv:2402.02526</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Sparse Mixture of Experts (SMoE)模型面临的挑战，并提出了竞争机制来解决这一问题。竞争机制通过将令牌路由到响应最高的专家来提高样本效率。为此，文章开发了一种名为CompeteSMoE的新算法，通过部署路由器来学习竞争策略，实现了强大的性能且训练开销较低。文章在视觉指令调整和语言预训练任务上的大量实证评估表明，相较于最新SMoE策略，CompeteSMoE更有效、稳健且可扩展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMoE模型面临专家路由过程的挑战，其中进行计算的专家并未直接对路由过程做出贡献。</li>
<li>竞争机制被提出作为一种解决方案，该机制能将令牌路由到响应最高的专家。</li>
<li>竞争机制的理论优势在于其样本效率高于传统的softmax路由。</li>
<li>CompeteSMoE算法简单有效，通过部署路由器学习竞争策略，实现高性能和低训练开销。</li>
<li>实证评估显示CompeteSMoE在视觉指令调整和语言预训练任务上的表现优于其他最新SMoE策略。</li>
<li>该研究是对arXiv上之前研究的一种改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0658908f77581756a2b8f5d0b2ca77f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-069d6d43404dd591aafe6ad3cfd962d0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Thinkless-LLM-Learns-When-to-Think"><a href="#Thinkless-LLM-Learns-When-to-Think" class="headerlink" title="Thinkless: LLM Learns When to Think"></a>Thinkless: LLM Learns When to Think</h2><p><strong>Authors:Gongfan Fang, Xinyin Ma, Xinchao Wang</strong></p>
<p>Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model’s ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/VainF/Thinkless">https://github.com/VainF/Thinkless</a> </p>
<blockquote>
<p>推理语言模型具备扩展的链式思维推理能力，在需要复杂逻辑推断的任务中表现出卓越的性能。然而，对所有查询进行精细推理通常会导致大量的计算效率低下，特别是当许多问题都有简单解决方案时。这引发了一个开放性的问题：LLM能否学会何时思考？为了回答这个问题，我们提出了Thinkless，这是一个可学习的框架，能够赋予LLM根据任务复杂度和模型能力自适应选择短形式和长形式推理。Thinkless是在强化学习模式下进行训练的，并采用两个控制令牌，<short>用于简洁的回答和<think>用于详细的推理。我们的方法的核心是解耦组相对策略优化（DeGRPO）算法，该算法将混合推理的学习目标分解为两个部分：（1）控制令牌损失，它控制推理模式的选择；（2）响应损失，它提高生成答案的准确性。这种解耦的公式化表达能够精细控制每个目标的贡献，稳定训练，并有效防止了原始GRPO中观察到的崩溃。在Minerva Algebra、MATH-500和GSM8K等多个基准测试中，Thinkless能够减少长链思考的使用率50%~90%，显著提高推理语言模型的效率。代码可用在<a target="_blank" rel="noopener" href="https://github.com/VainF/Thinkless%E3%80%82">https://github.com/VainF/Thinkless。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了Thinkless框架，该框架让大型语言模型（LLM）能够自适应地选择简洁回答和详细推理的模式，基于任务复杂度和模型能力。Thinkless采用强化学习模式训练，使用两个控制符号：<short>用于简洁回答，<think>用于详细推理。核心方法是Decoupled Group Relative Policy Optimization（DeGRPO）算法，该算法将混合推理的学习目标分解为两个组成部分：控制符号损失和响应损失。在多个基准测试中，Thinkless能够减少大型语言模型的长链思考使用，显著提高推理效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Thinkless框架让LLM能够自适应选择简洁回答和详细推理的模式。</li>
<li>Thinkless采用强化学习模式训练，使用控制符号来控制推理模式。</li>
<li>Decoupled Group Relative Policy Optimization（DeGRPO）算法是Thinkless的核心方法，它将混合推理的学习目标分解为两个组成部分。</li>
<li>控制符号损失负责选择推理模式，响应损失则提高生成答案的准确性。</li>
<li>Thinkless能够在多个基准测试中减少LLM的长链思考使用。</li>
<li>Thinkless显著提高了推理效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33aa4bcf4653260e5a0e7235af8c9221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35c050fb0ce9a3ab8e6166d621f4af7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608781e2d2769d48b179ad8b99ab3c63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-604da8dfe31b0c8525b7ab0caf713e98.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Sense-and-Sensitivity-Examining-the-Influence-of-Semantic-Recall-on-Long-Context-Code-Reasoning"><a href="#Sense-and-Sensitivity-Examining-the-Influence-of-Semantic-Recall-on-Long-Context-Code-Reasoning" class="headerlink" title="Sense and Sensitivity: Examining the Influence of Semantic Recall on   Long Context Code Reasoning"></a>Sense and Sensitivity: Examining the Influence of Semantic Recall on   Long Context Code Reasoning</h2><p><strong>Authors:Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</strong></p>
<p>Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information. </p>
<blockquote>
<p>尽管现代的大型语言模型（LLM）支持极大的上下文环境，但它们利用长上下文进行代码推理的有效性仍不明确。本文调查了LLM在大仓库中的代码片段推理能力及其与回忆能力的关系。具体来说，我们区分了词汇代码回忆（逐字检索）和语义代码回忆（记住代码的功能）。为了衡量语义回忆，我们提出了SemTrace，这是一种代码推理技术，可以追溯特定语句对输出的影响，且具有不可预测性。我们还介绍了一种在现有基准测试中量化语义回忆敏感度的方法。对最新LLM的评估显示，当代码片段接近输入上下文的中部时，特别是需要使用高语义回忆的技术（如SemTrace），代码推理的准确性会显著下降。此外，我们发现词汇回忆的粒度不同，模型在函数检索方面表现出色，但在逐行回忆方面却遇到困难。值得注意的是，词汇回忆和语义回忆之间存在断层，这表明了不同的潜在机制。最后，我们的研究结果表明，当前的代码推理基准测试可能表现出较低的语义回忆敏感性，可能会低估LLM在利用上下文信息方面的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13353v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文探讨了大型语言模型（LLM）在代码片段推理方面的能力，及其与代码召回能力的关联。研究区分了词汇代码召回（逐字检索）和语义代码召回（记住代码的功能）。为衡量语义召回能力，提出了SemTrace代码推理技术，并介绍了在现有基准测试中量化语义召回敏感度的方法。评估显示，随着代码片段位于输入上下文中的位置逐渐居中，特别是需要高度语义召回的技术如SemTrace，代码推理的准确性会显著下降。此外，还发现词汇召回能力因粒度而异，模型擅长函数检索，但在逐行召回方面遇到困难。重要的是，词汇召回和语义召回之间存在断层，说明存在不同的内在机制。最后，研究表明现有的代码推理基准测试可能表现出较低的语义召回敏感性，可能会低估LLM在利用上下文信息方面的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在代码片段推理方面的能力尚不清楚。</li>
<li>区分了词汇代码召回和语义代码召回。</li>
<li>提出了SemTrace技术来衡量语义召回能力。</li>
<li>LLM在代码推理方面存在准确性下降的问题，特别是在需要高语义召回的技术上。</li>
<li>词汇召回能力因粒度而异，模型在函数检索方面表现较好，但在逐行召回方面存在困难。</li>
<li>词汇召回和语义召回之间存在断层，表明它们有不同的内在机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d8350a705151525f12bec85cbdffcb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c51a82f8b477aa0475d656a8c1ac0fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-990fb901fd5afe98db09ada9a957fd2b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Preference-Optimization"><a href="#J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Preference-Optimization" class="headerlink" title="J4R: Learning to Judge with Equivalent Initial State Group Relative   Preference Optimization"></a>J4R: Learning to Judge with Equivalent Initial State Group Relative   Preference Optimization</h2><p><strong>Authors:Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</strong></p>
<p>To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench. </p>
<blockquote>
<p>随着大型语言模型（LLM）发展步伐的加快，模型输出评估已经从耗时的人类评估转变为自动评估。在自动评估中，大型语言模型本身被用来评估和批判其他模型的输出。LLM作为评判者的模型是一类生成评估器，擅长评估相对简单的领域，如聊天质量，但在需要推理的复杂领域中表现挣扎，这些领域的模型回应包含更多实质性的挑战内容。为了弥补现有评判者的不足，我们探索使用强化学习（RL）来训练评判者。我们做出了三个关键贡献：（1）我们提出了等价初始状态组相对策略优化（EIS-GRPO）算法，该算法使我们能够训练评判者对更复杂评估环境中出现的定位偏差具有鲁棒性。（2）我们引入了ReasoningJudgeBench，这是一个评估在以前的工作中没有涵盖的各种推理环境中的评判者。（3）我们使用EIS-GRPO训练了J4R（推理评判者），这是一个7B的评判者，其性能超过了GPT-4o和下一个最佳小型评判者6.7%和9%，在JudgeBench和ReasoningJudgeBench上的表现与更大的GRPO训练过的评判者相匹配或更好。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13346v1">PDF</a> 25 pages, 4 figures, 6 tables. To be updated with links for   code&#x2F;benchmark</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的发展推动了模型输出评估的转变，从耗时的人力评估逐渐转向自动评估。现在，LLM自身被用于评估和批判其他模型的输出。本文探讨了利用强化学习（RL）训练评价模型的方案，提出一种名为EIS-GRPO的新算法，增强了评价模型在复杂环境下的稳健性。同时，文章引入了一个名为ReasoningJudgeBench的新基准测试，用于评估评价模型在不同推理场景下的表现。最终，通过EIS-GRPO训练的J4R模型在ReasoningJudgeBench上的表现超过了GPT-4o和其他小型评价模型，甚至与大型GRPO训练的评价模型相匹配或表现更好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的发展推动了模型输出评估的转变，现在更多地依赖于自动评估和LLM自身的评价。</li>
<li>LLM-as-judge模型在评估相对简单的领域（如聊天质量）表现良好，但在需要大量推理的复杂领域表现欠佳。</li>
<li>为了改进现有评价模型的不足，研究者采用强化学习（RL）进行训练，并提出了一种新的算法EIS-GRPO，增强了评价模型在复杂环境下的稳健性。</li>
<li>引入了一个新的基准测试ReasoningJudgeBench，用于评估评价模型在不同推理场景下的表现。</li>
<li>通过EIS-GRPO训练的J4R模型在ReasoningJudgeBench上的表现超过了GPT-4o和其他小型评价模型。</li>
<li>J4R模型的表现与大型GRPO训练的评价模型相匹配或表现更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13346">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c206c38f5dddb3041de5540b90c0f44b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93315927c8300339a387f090d719d795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10c1657f1be23f37719d032298207ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99c75154d32ee65f20210787204ec610.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JNLP-at-SemEval-2025-Task-11-Cross-Lingual-Multi-Label-Emotion-Detection-Using-Generative-Models"><a href="#JNLP-at-SemEval-2025-Task-11-Cross-Lingual-Multi-Label-Emotion-Detection-Using-Generative-Models" class="headerlink" title="JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion   Detection Using Generative Models"></a>JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion   Detection Using Generative Models</h2><p><strong>Authors:Jieying Xue, Phuong Minh Nguyen, Minh Le Nguyen, Xin Liu</strong></p>
<p>With the rapid advancement of global digitalization, users from different countries increasingly rely on social media for information exchange. In this context, multilingual multi-label emotion detection has emerged as a critical research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task: (1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity. To tackle multilingual challenges, we leverage pre-trained multilingual models and focus on two architectures: (1) a fine-tuned BERT-based classification model and (2) an instruction-tuned generative LLM. Additionally, we propose two methods for handling multi-label classification: the base method, which maps an input directly to all its corresponding emotion labels, and the pairwise method, which models the relationship between the input text and each emotion category individually. Experimental results demonstrate the strong generalization ability of our approach in multilingual emotion recognition. In Track A, our method achieved Top 4 performance across 10 languages, ranking 1st in Hindi. In Track B, our approach also secured Top 5 performance in 7 languages, highlighting its simplicity and effectiveness\footnote{Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yingjie7/mlingual_multilabel_emo_detection">https://github.com/yingjie7/mlingual_multilabel_emo_detection</a>. </p>
<blockquote>
<p>随着全球数字化的快速发展，来自不同国家的用户越来越依赖社交媒体进行信息交流。在此背景下，多语言多标签情感检测已成为一个关键研究领域。本研究旨在解决SemEval-2025任务11：文本情感检测中的鸿沟问题。我们的论文重点关注该任务中的两个子领域：（1）赛道A：多标签情感检测；（2）赛道B：情感强度。为了应对多语言挑战，我们利用预训练的多语言模型，并专注于两种架构：（1）微调后的基于BERT的分类模型；（2）经过指令训练的生成式大型语言模型。此外，我们提出了两种处理多标签分类的方法：基础方法直接将输入映射到其对应的所有情感标签上；配对方法则分别建模输入文本与每个情感类别之间的关系。实验结果表明，我们的方法在多种语言的情感识别中具有强大的泛化能力。在赛道A中，我们的方法在10种语言中取得了前4名的成绩，并在印地语中排名第1。在赛道B中，我们的方法也在7种语言中取得了前5名的成绩，突显了其简单性和有效性【我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/yingjie7/mlingual_multilabel_emo_detection%E6%89%BE%E5%88%B0%E3%80%91%E3%80%82">https://github.com/yingjie7/mlingual_multilabel_emo_detection找到】。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13244v1">PDF</a> Published in The 19th International Workshop on Semantic Evaluation   (SemEval-2025)</p>
<p><strong>Summary</strong></p>
<p>随着全球数字化进程加快，不同国家用户日益依赖社交媒体进行信息交流，多语言多标签情感检测成为重要研究领域。本研究关注SemEval-2025 Task 11：文本情感检测中的鸿沟问题。针对该任务的两个子课题：（1）多标签情感检测和（2）情感强度进行深入研究。为应对多语言挑战，研究利用预训练多语言模型和两种架构，即微调BERT分类模型和指令微调生成式大型语言模型。同时提出两种处理多标签分类的方法：基础方法直接映射输入到其对应的情感标签，配对方法则独立建模输入文本与每种情感类别之间的关系。实验结果显示，该方法在多语言情感识别中具有强大的泛化能力，在A赛道10种语言中位列第4，并在印度语中排名第1；在B赛道7种语言中位列第5，凸显其简洁性和有效性^[注：代码可访问<a target="_blank" rel="noopener" href="https://github.com/yingjie7/mlingual_multilabel_emo_detection]%E3%80%82">https://github.com/yingjie7/mlingual_multilabel_emo_detection]。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全球数字化进程中，社交媒体成为信息交流的关键渠道，多语言多标签情感检测成为研究热点。</li>
<li>本研究关注SemEval-2025 Task 11中的两个子课题：多标签情感检测和情感强度。</li>
<li>为应对多语言挑战，研究利用预训练多语言模型和两种架构：微调BERT分类模型和指令微调生成式大型语言模型。</li>
<li>提出两种处理多标签分类的方法：基础方法和配对方法。</li>
<li>实验结果显示，该方法在多语言情感识别中具有强大的泛化能力。</li>
<li>在特定赛道中取得优异成绩，如A赛道印度语排名第1，B赛道7种语言中位列第5。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1103a4e20d6934d949a1b818d06d3b19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea062cf4fe4c2fdd85ed4c0a8da0cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f78b67c18ff0fd0ed1cb79977430218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4ea68b68328ea8ae91177235864f54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c00e94753e5c0c297de4c7c5441472b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FLASH-Latent-Aware-Semi-Autoregressive-Speculative-Decoding-for-Multimodal-Tasks"><a href="#FLASH-Latent-Aware-Semi-Autoregressive-Speculative-Decoding-for-Multimodal-Tasks" class="headerlink" title="FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for   Multimodal Tasks"></a>FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for   Multimodal Tasks</h2><p><strong>Authors:Zihua Wang, Ruibo Li, Haozhe Du, Joey Tianyi Zhou, Yu Zhang, Xu Yang</strong></p>
<p>Large language and multimodal models (LLMs and LMMs) exhibit strong inference capabilities but are often limited by slow decoding speeds. This challenge is especially acute in LMMs, where visual inputs typically comprise more tokens with lower information density than text – an issue exacerbated by recent trends toward finer-grained visual tokenizations to boost performance. Speculative decoding has been effective in accelerating LLM inference by using a smaller draft model to generate candidate tokens, which are then selectively verified by the target model, improving speed without sacrificing output quality. While this strategy has been extended to LMMs, existing methods largely overlook the unique properties of visual inputs and depend solely on text-based draft models. In this work, we propose \textbf{FLASH} (Fast Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework designed specifically for LMMs, which leverages two key properties of multimodal data to design the draft model. First, to address redundancy in visual tokens, we propose a lightweight latent-aware token compression mechanism. Second, recognizing that visual objects often co-occur within a scene, we employ a semi-autoregressive decoding strategy to generate multiple tokens per forward pass. These innovations accelerate draft decoding while maintaining high acceptance rates, resulting in faster overall inference. Experiments show that FLASH significantly outperforms prior speculative decoding approaches in both unimodal and multimodal settings, achieving up to \textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on visual instruction tuning tasks compared to the original LMM. </p>
<blockquote>
<p>大型语言和多模态模型（LLM 和 LMM）展现出强大的推理能力，但往往受到解码速度较慢的限制。这一挑战在 LMM 中尤为突出，因为视觉输入通常包含比文本更多的标记，但信息密度较低——这一问题因近期为提升性能而出现的更精细的视觉标记化趋势而加剧。投机解码通过使用较小的草稿模型生成候选标记来加速 LLM 推理，然后目标模型会进行选择性验证，从而在提高速度的同时不牺牲输出质量。虽然这一策略已扩展到 LMM，但现有方法大多忽略了视觉输入的独特属性，仅依赖于文本草稿模型。在这项工作中，我们提出一种名为 FLASH（快速潜在感知半自回归启发式方法）的投机解码框架，它是专门针对 LMM 设计的，利用多模态数据的两个关键属性来构建草稿模型。首先，为了解决视觉标记的冗余问题，我们提出了一种轻量级的潜在感知标记压缩机制。其次，我们认识到视觉对象通常会在场景中共现，因此采用半自回归解码策略，每次前向传递生成多个标记。这些创新加速了草稿解码，同时保持了较高的接受率，从而实现了更快的整体推理。实验表明，无论是在单模态还是多模态环境中，FLASH 都显著优于之前的投机解码方法。在与原始 LMM 的比较中，FLASH 在视频描述和视觉指令调整任务上分别实现了高达 2.68 倍和 2.55 倍的加速。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12728v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型语言和多媒体模型（LLMs和LMMs）的推理速度限制问题，提出了一种名为FLASH的投机解码框架。该框架针对LMMs的特性进行设计，通过利用视觉数据的冗余性和对象共现性，实现了快速解码。实验表明，FLASH在单模态和多模态场景下均显著优于传统投机解码方法，视频描述和视觉指令调整任务的速度提升最高可达2.68倍和2.55倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs和LMMs具有强大的推理能力，但解码速度较慢，特别是在处理视觉输入时更为明显。</li>
<li>现有投机解码方法主要面向LLMs，忽略了视觉输入的独特性。</li>
<li>FLASH框架针对LMMs设计，利用视觉数据的冗余性和对象共现性来加速解码。</li>
<li>FLASH采用轻量级的潜在感知令牌压缩机制和半自动递归解码策略。</li>
<li>实验表明，FLASH在单模态和多模态场景下均显著提高了解码速度。</li>
<li>FLASH在视频描述和视觉指令调整任务中的速度提升最高可达2.68倍和2.55倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d854458d913abb813efecb4f2562fdcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc6281967c26711d3e845409d935d024.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e91764fab59dc4f5bd237bd597e63970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f9491fe52a15a8252f32211d376a17e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Shadow-FT-Tuning-Instruct-via-Base"><a href="#Shadow-FT-Tuning-Instruct-via-Base" class="headerlink" title="Shadow-FT: Tuning Instruct via Base"></a>Shadow-FT: Tuning Instruct via Base</h2><p><strong>Authors:Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</strong></p>
<p>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%7D%7BGithub%7D">https://github.com/wutaiqiang/Shadow-FT}{Github}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各种任务上通过进一步的微调持续受益。然而，我们观察到直接调整INSTRUCT（即指令调整）模型通常只会带来微小的改进，甚至会导致性能下降。值得注意的是，这些INSTRUCT变体的基础配套BASE模型包含高度相似的权重值（例如Llama 3.1  8B的平均值小于2%）。因此，我们提出了一种新的Shadow-FT框架，利用相应的BASE模型来调整INSTRUCT模型。关键思路是微调BASE模型，然后将学习到的权重更新直接移植到INSTRUCT模型。我们提出的Shadow-FT无需添加额外的参数，易于实现，并能显著提高性能。我们在主流的LLM上进行了广泛的实验，如Qwen 3和Llama 3系列，并在涵盖编码、推理和数学任务的19个基准测试上对其进行了评估。实验结果表明，Shadow-FT持续优于传统的全参数和参数高效的调整方法。进一步的分析表明，Shadow-FT可应用于多模态大型语言模型（MLLM），并与直接偏好优化（DPO）相结合。相关代码和权重可在<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT">Github</a>上获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12716v1">PDF</a> Under review</p>
<p><strong>摘要</strong></p>
<p>大语言模型通过进一步的微调在各种任务上持续受益，但直接对INSTRUCT模型进行微调往往导致性能提升有限甚至出现退化。本文提出了Shadow-FT框架，利用相应的BASE模型对INSTRUCT模型进行微调。Shadow-FT的核心思想是先对BASE模型进行微调，然后将学习到的权重更新直接应用到INSTRUCT模型上。该方法无需添加额外的参数，易于实现，且能显著提高性能。实验结果表明，Shadow-FT在主流LLM上的性能优于传统的全参数和参数高效调整方法。同时，Shadow-FT可应用于多模态大型语言模型（MLLMs）并与直接偏好优化（DPO）结合。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>直接微调INSTRUCT模型常常导致性能提升有限或性能退化。</li>
<li>提出的Shadow-FT框架利用相应的BASE模型进行微调。</li>
<li>Shadow-FT通过对BASE模型进行微调，然后将学习到的权重更新应用到INSTRUCT模型上。</li>
<li>Shadow-FT方法无需额外的参数，易于实现。</li>
<li>实验表明，Shadow-FT在主流LLM上的性能优于传统方法。</li>
<li>Shadow-FT可应用于多模态大型语言模型（MLLMs）。</li>
<li>Shadow-FT可与直接偏好优化（DPO）结合使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17fb9242662645175e05b7135628e9df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db1cd1fb6974c657fb8c9fe5a97dbd7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a6a60633b13a1a8a5fe06c4d09ce7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c483fe6239f6a33b6b24849362c7879c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3578d59fe55ea0c10fb3aafa78028437.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Latent-Computation-in-Transformers-with-Latent-Tokens"><a href="#Enhancing-Latent-Computation-in-Transformers-with-Latent-Tokens" class="headerlink" title="Enhancing Latent Computation in Transformers with Latent Tokens"></a>Enhancing Latent Computation in Transformers with Latent Tokens</h2><p><strong>Authors:Yuchang Sun, Yanxi Chen, Yaliang Li, Bolin Ding</strong></p>
<p>Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs. </p>
<blockquote>
<p>通过辅助标记增强大型语言模型（LLM）已成为提高模型性能的一种有前途的策略。在这项工作中，我们提出了一种轻量级的方法，称为潜在标记。这些是非自然语言中不可解释的虚拟标记，但通过注意力机制引导基于Transformer的LLM的自回归解码过程。所提出的潜在标记可以无缝地集成到预训练的Transformer中，以高效参数的方式进行训练，并在推理时间灵活应用，同时给现有标准Transformer基础设施增加最小的复杂性开销。我们对潜在标记的潜在机制提出了一些假设，并设计了相应的合成任务来验证它们。数值结果表明，该方法明显优于基线，特别是在超出分布范围的泛化场景中表现尤为突出，突显其在提高LLM适应性方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>潜藏令牌是一种增强大型语言模型（LLM）性能的有效策略。该策略通过引入非自然语言理解的潜藏令牌，利用注意力机制引导基于Transformer的LLM的自回归解码过程。这些令牌可无缝集成到预训练的Transformer中，以高效的方式训练，并在推理时灵活应用，同时为现有的标准Transformer架构增加极低的复杂性开销。实验结果表明，该方法在特别是非分布泛化场景中显著优于基线方法，显示出提高LLM适应性的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜藏令牌是一种增强LLM性能的有效策略。</li>
<li>潜藏令牌通过注意力机制引导基于Transformer的LLM的自回归解码过程。</li>
<li>潜藏令牌可以无缝集成到预训练的Transformer中，并以参数高效的方式进行训练。</li>
<li>潜藏令牌在推理时具有灵活性，并增加了极低的复杂性开销。</li>
<li>实验结果表明，潜藏令牌在特别是非分布泛化场景中显著提高了模型性能。</li>
<li>潜藏令牌提高了LLM的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-12a2f38ba075e74979091c88c4fbed1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b83879ed557c6b655a699d65a3ce38ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01701b1030c2f63f41017206d65f7609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f952ffc52515e252f4663859f72954ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44ad86e37e8e6901b5362c7e2042ce7d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Video-GPT-via-Next-Clip-Diffusion"><a href="#Video-GPT-via-Next-Clip-Diffusion" class="headerlink" title="Video-GPT via Next Clip Diffusion"></a>Video-GPT via Next Clip Diffusion</h2><p><strong>Authors:Shaobin Zhuang, Zhipeng Huang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Binxin Yang, Chong Sun, Chen Li, Yali Wang</strong></p>
<p>GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream. The project page is at <a target="_blank" rel="noopener" href="https://video-gpt.github.io/">https://Video-GPT.github.io</a>. </p>
<blockquote>
<p>GPT在自然语言处理方面取得了显著的成功。然而，仅仅依靠语言序列不足以描述视觉世界中的时空细节。相比之下，视频序列更擅长捕捉这些细节。基于这一事实，我们在本文中提出了一种简洁的视频GPT，将视频视为视觉世界建模的新语言。通过模拟GPT中的下一个令牌预测，我们引入了一种新的下一个片段扩散模式来预训练视频GPT。与以前的工作不同，这种独特的模式允许视频GPT解决短期生成和长期预测问题，根据历史中的干净片段自回归地对嘈杂片段进行降噪。大量实验表明，我们的视频GPT在视频预测方面达到了最新水平的技术性能，这是实现世界建模的关键因素（Physics-IQ Benchmark：视频GPT 34.97对Kling 23.64对Wan 20.89）。此外，它还可以很好地适应视频生成和理解方面的6个主流任务，显示出其在下游任务中强大的泛化能力。项目页面位于<a target="_blank" rel="noopener" href="https://video-gpt.github.io./">https://Video-GPT.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12489v1">PDF</a> 22 pages, 12 figures, 18 tables</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个新颖的Video-GPT模型，将视频视为一种新的语言来进行视觉世界建模。通过借鉴GPT中的下一个词预测，引入了下一个片段扩散范式来预训练Video-GPT。与以往的工作不同，这种独特的范式使得Video-GPT能够同时进行短期生成和长期预测。通过自适应去除噪声片段中的噪声，它达到了视频预测方面的最佳性能。此外，Video-GPT在视频生成和理解方面的六个主流任务上具有良好的适应性，显示出其出色的下游任务泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-GPT模型将视频视为一种新语言进行视觉世界建模。</li>
<li>借鉴GPT中的下一个词预测，引入了下一个片段扩散范式进行预训练。</li>
<li>这种新的范式使得Video-GPT能够同时进行短期生成和长期预测。</li>
<li>Video-GPT通过自适应去除噪声片段中的噪声，实现了视频预测的最佳性能。</li>
<li>Video-GPT在多个视频任务上表现优秀，具有良好的泛化能力。</li>
<li>Video-GPT在Physics-IQ Benchmark上的得分远高于其他模型，体现了其优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12489">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5880e8761bc9d693cfd70962ec12448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6159bc24c2e0e6f729f8be17cf4d5d87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522041452fd6400da1ad1c1ec2eba23b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edc96a36afedd6103677e14942e896ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35976ec655681789cdc85f7756300682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d9d368cc7938690b3efbcc92c3eb9a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89ebf858d5221e519d9e99e8f8c4547d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts"><a href="#Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts" class="headerlink" title="Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts"></a>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h2><p><strong>Authors:Qi Feng, Hidetoshi Shimodaira</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在一般的视觉语言任务上表现出色，但对于空间布局、关系和动态的空间认知仍然是一个巨大的挑战。现有模型通常缺乏必要的架构组件和精细空间理解所需的专门训练数据。我们推出了ViCA2（视觉空间认知助手2），这是一款旨在增强空间推理能力的新型MLLM。ViCA2采用双视觉编码器架构，集成SigLIP进行语义分析，结合Hiera进行空间结构分析，同时采用令牌比率控制机制以提高效率。我们还开发了ViCA-322K，这是一个新的大规模数据集，包含超过32万对空间定位的问题和答案，用于针对性的指令调整。在具有挑战性的VSI-Bench基准测试中，我们的ViCA2-7B模型达到了平均得分56.8分的业界最佳水平，显著超过了更大的开源模型（例如LLaVA-NeXT-Video-72B的40.9分）和领先的专有模型（Gemini-1.5 Pro的45.4分）。这证明了我们的方法在实现强大空间智能时的有效性，且采用了紧凑的模型。我们发布ViCA2、其代码库和ViCA-322K数据集，以促进进一步的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12363v1">PDF</a> 26 pages, 19 figures, 4 tables. Code, models, and dataset are   available at our project page: <a target="_blank" rel="noopener" href="https://github.com/nkkbr/ViCA">https://github.com/nkkbr/ViCA</a></p>
<p><strong>Summary</strong></p>
<p>MLLM在一般视觉语言任务上表现出色，但在处理空间布局、关系和动态的视觉空间认知上仍面临挑战。针对这一问题，我们推出了ViCA2（视觉空间认知助手2），这是一款设计用于增强空间推理的新型Multimodal大型语言模型。ViCA2采用双视觉编码器架构，集成SigLIP进行语义分析并Hiera进行空间结构分析，同时采用令牌比率控制机制以提高效率。此外，我们还开发了大型数据集ViCA-322K，包含超过32万组空间定位问题答案对，用于针对性指导调整。在具有挑战性的VSI-Bench基准测试中，我们的ViCA2-7B模型平均得分达到56.8，显著超越了其他开源大型模型（如LLaVA-NeXT-Video-72B的40.9分）和领先的专有模型（Gemini-1.5 Pro的45.4分）。这证明了我们的方法在实现强大的视觉空间智能方面的有效性，同时模型较为紧凑。我们公开发布了ViCA2、其代码库和ViCA-322K数据集，以促进进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Multimodal Large Language Models (MLLMs)虽擅长一般视觉语言任务，但在视觉空间认知方面存在挑战。</li>
<li>ViCA2是一款新型MLLM，旨在增强空间推理能力，具有双视觉编码器架构、语义分析、空间结构分析以及高效的令牌比率控制机制。</li>
<li>开发了大型数据集ViCA-322K，用于针对性指导调整模型。</li>
<li>ViCA2在VSI-Bench基准测试中表现优异，平均得分56.8，显著超越其他模型。</li>
<li>ViCA2实现了强大的视觉空间智能，同时模型较为紧凑。</li>
<li>ViCA2、其代码库和ViCA-322K数据集已公开发布，以促进进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab70a243d7dd1133839c489e87bd7071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73408c0690e6babe23ec2f59c9dcd46c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b49a1d2f8faf93399797eb2a9195519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94de87aa0b44a40c568d427a06ae798c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TechniqueRAG-Retrieval-Augmented-Generation-for-Adversarial-Technique-Annotation-in-Cyber-Threat-Intelligence-Text"><a href="#TechniqueRAG-Retrieval-Augmented-Generation-for-Adversarial-Technique-Annotation-in-Cyber-Threat-Intelligence-Text" class="headerlink" title="TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique   Annotation in Cyber Threat Intelligence Text"></a>TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique   Annotation in Cyber Threat Intelligence Text</h2><p><strong>Authors:Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, Md Rizwan Parvez</strong></p>
<p>Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights. </p>
<blockquote>
<p>准确识别安全文本中的对抗技术是有效网络防御的关键。然而，现有方法面临基本权衡：它们要么依赖于具有有限领域精度的通用模型，要么需要依赖大量标记数据和特定任务优化（如自定义的硬负样本挖掘和去噪）的资源密集型管道，这些资源在特定领域很少可用。我们提出了TechniqueRAG，这是一个领域特定的检索增强生成（RAG）框架，它通过集成现成的检索器、指令调优的LLM和少量的文本技术配对来弥补这一差距。我们的方法通过仅对领域内的有限示例进行生成组件的微调来解决数据稀缺问题，从而避免了资源密集型的检索训练需求。虽然传统的RAG通过耦合检索和生成来缓解虚构问题，但它对通用检索器的依赖往往会引入嘈杂的候选对象，从而限制了领域特定的精度。为了解决这一问题，我们通过零样本LLM重新排序来提高检索质量和领域特异性，这将明确地将检索到的候选对象与对抗技术对齐。在多个安全基准测试上的实验表明，TechniqueRAG在不进行大量特定任务优化或标记数据的情况下实现了最佳性能，而综合分析提供了进一步的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11988v1">PDF</a> Accepted at ACL (Findings) 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在安全文本中准确识别对抗技术的重要性，并指出了现有方法的局限性。为此，提出了一种基于检索增强生成（RAG）的域特定框架TechniqueRAG，通过集成现成的检索器、指令微调的大型语言模型和少量的文本技术对数据对来弥补数据稀缺的缺陷。该框架通过提高检索质量和领域特异性来解决传统RAG方法的问题，实现了在安全文本领域的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>准确识别安全文本中的对抗技术是有效网络防御的关键。</li>
<li>现有方法面临通用模型与领域精度有限的权衡问题。</li>
<li>TechniqueRAG是一个域特定的RAG框架，集成了现成的检索器、指令微调的大型语言模型和少量文本技术对数据对。</li>
<li>TechniqueRAG解决了数据稀缺的问题，通过只对生成组件进行微调，而无需大量领域特定的数据集和任务特定优化。</li>
<li>传统RAG方法中的检索和生成耦合有助于缓解虚构问题，但依赖于通用检索器，可能导致引入噪声候选者，限制了领域特定的精度。</li>
<li>TechniqueRAG通过零样本大型语言模型重新排名增强检索质量和领域特异性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-334934d0876049f5dc08c8ac14038116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64105975f73050a0d216b0acb495578e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0449a68ec7301665b6daf4c9bd9acd7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6006da90d7c9ba6c535904fde0b31051.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedSG-Bench-A-Benchmark-for-Medical-Image-Sequences-Grounding"><a href="#MedSG-Bench-A-Benchmark-for-Medical-Image-Sequences-Grounding" class="headerlink" title="MedSG-Bench: A Benchmark for Medical Image Sequences Grounding"></a>MedSG-Bench: A Benchmark for Medical Image Sequences Grounding</h2><p><strong>Authors:Jingkun Yue, Siqi Zhang, Zinan Jia, Huihuan Xu, Zongbo Han, Xiaohong Liu, Guangyu Wang</strong></p>
<p>Visual grounding is essential for precise perception and reasoning in multimodal large language models (MLLMs), especially in medical imaging domains. While existing medical visual grounding benchmarks primarily focus on single-image scenarios, real-world clinical applications often involve sequential images, where accurate lesion localization across different modalities and temporal tracking of disease progression (e.g., pre- vs. post-treatment comparison) require fine-grained cross-image semantic alignment and context-aware reasoning. To remedy the underrepresentation of image sequences in existing medical visual grounding benchmarks, we propose MedSG-Bench, the first benchmark tailored for Medical Image Sequences Grounding. It comprises eight VQA-style tasks, formulated into two paradigms of the grounding tasks, including 1) Image Difference Grounding, which focuses on detecting change regions across images, and 2) Image Consistency Grounding, which emphasizes detection of consistent or shared semantics across sequential images. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities, and a wide spectrum of anatomical structures and diseases, totaling 9,630 question-answer pairs. We benchmark both general-purpose MLLMs (e.g., Qwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision), observing that even the advanced models exhibit substantial limitations in medical sequential grounding tasks. To advance this field, we construct MedSG-188K, a large-scale instruction-tuning dataset tailored for sequential visual grounding, and further develop MedSeq-Grounder, an MLLM designed to facilitate future research on fine-grained understanding across medical sequential images. The benchmark, dataset, and model are available at <a target="_blank" rel="noopener" href="https://huggingface.co/MedSG-Bench">https://huggingface.co/MedSG-Bench</a> </p>
<blockquote>
<p>视觉定位在多模态大型语言模型（MLLMs）中，特别是在医学影像领域，对于精确感知和推理至关重要。尽管现有的医学视觉定位基准测试主要关注单图像场景，但现实世界临床应用通常涉及图像序列，其中不同模态之间的精确病灶定位和疾病进展的时空追踪（例如，治疗前后的比较）需要精细的跨图像语义对齐和上下文感知推理。为了弥补现有医学视觉定位基准测试中图像序列表示不足的问题，我们提出了MedSG-Bench，这是专门为医学图像序列定位量身定制的第一个基准测试。它包含了八个VQA风格的任务，制定为两种定位任务的模式，包括1）图像差异定位，专注于检测图像之间的变化区域；2）图像一致性定位，强调检测序列图像中一致或共享的语义。MedSG-Bench涵盖了76个公共数据集、10种医学成像模态以及广泛的解剖结构和疾病，总共9630个问答对。我们对通用MLLMs（例如Qwen2.5-VL）和医疗领域专用MLLMs（例如HuatuoGPT-vision）进行了基准测试，发现即使在医学序列定位任务中，先进模型也表现出相当大的局限性。为了推动这一领域的发展，我们构建了MedSG-188K，这是一个专门用于序列视觉定位的大型指令调整数据集，并进一步发展了MedSeq-Grounder，这是一个MLLM，旨在促进未来对医学序列图像的精细理解研究。该基准测试、数据集和模型均可在<a target="_blank" rel="noopener" href="https://huggingface.co/MedSG-Bench%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/MedSG-Bench找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11852v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对多模态大型语言模型（MLLMs）在医疗成像领域中的精确感知和推理，视觉基础至关重要。现有医疗视觉基础基准测试主要集中在单图像场景上，而现实世界中的临床应用通常涉及图像序列，要求精细的跨图像语义对齐和上下文感知推理。为此，我们提出MedSG-Bench，首个针对医疗图像序列基础的基准测试。它包括八种VQA风格的任务，分为两大基础任务范式，包括1）图像差异基础，专注于检测图像间的变化区域；2）图像一致性基础，强调检测序列图像中一致或共享语义。MedSG-Bench覆盖76个公共数据集、10种医学影像模态和广泛的结构与疾病，共包含9630个问答对。我们对通用MLLMs（如Qwen2.5-VL）和医疗领域专用MLLMs（如HuatuoGPT-vision）进行了基准测试，发现即使是高级模型在医疗序列基础任务上也存在显著局限性。为了推动这一领域的发展，我们构建了针对序列视觉基础的大规模指令调整数据集MedSG-188K，并开发了MLLM——MedSeq-Grounder，以促进对医疗序列图像的精细理解研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉基础在多模态大型语言模型（MLLMs）的精确感知和推理中至关重要，特别是在医疗成像领域。</li>
<li>现有医疗视觉基础基准测试主要关注单图像场景，但实际应用涉及图像序列。</li>
<li>MedSG-Bench首次为医疗图像序列基础提供基准测试，涵盖多种任务和数据集。</li>
<li>基准测试显示，即使是高级MLLMs在医疗序列基础任务上仍存在局限性。</li>
<li>为了改进模型性能，构建了针对序列视觉基础的大规模指令调整数据集MedSG-188K。</li>
<li>开发了MedSeq-Grounder这一MLLM，以促进对医疗序列图像的精细理解研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-df99f3638ead56b1e1cd929e2b247099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ffd97d31a9dfb91511a6c136abbe53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bafc1b02070a60b3b67918fae5d6a710.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ada4dfa1c0cb817a1adbfdfebbe7fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663a71799a93facc5427bc539094574d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe7af8fd9425def2e33349952de04f2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="REI-Bench-Can-Embodied-Agents-Understand-Vague-Human-Instructions-in-Task-Planning"><a href="#REI-Bench-Can-Embodied-Agents-Understand-Vague-Human-Instructions-in-Task-Planning" class="headerlink" title="REI-Bench: Can Embodied Agents Understand Vague Human Instructions in   Task Planning?"></a>REI-Bench: Can Embodied Agents Understand Vague Human Instructions in   Task Planning?</h2><p><strong>Authors:Chenxi Jiang, Chuhao Zhou, Jianfei Yang</strong></p>
<p>Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children. </p>
<blockquote>
<p>机器人任务规划能够将人类指令分解为可执行的行动序列，从而使机器人能够完成一系列复杂任务。尽管最近基于大型语言模型的任务规划器取得了惊人的表现，但它们假设人类指令是清晰直接的。然而，真实世界的用户并非专家，他们对机器人的指令通常包含大量的模糊性。语言学家认为，这种模糊性往往源于指代表达式（RE），其含义在很大程度上依赖于对话上下文和环境。这种模糊性在老年人和儿童中更为普遍，而机器人应该更多地为他们服务。这篇论文研究了人类指令中的指代表达式（RE）的模糊性是如何影响基于大型语言模型的机器人任务规划的，以及如何解决这一问题。为此，我们提出了首个具有模糊指代表达式的机器人任务规划基准测试（REI-Bench），在该基准测试中我们发现，指代表达式的模糊性会严重降低机器人规划的性能，导致成功率下降高达77.9%。我们还观察到，大多数失败的情况源于规划中的目标对象缺失。为了缓解指代表达式的问题，我们提出了一种简单有效的方法：面向任务的上下文认知，它为机器人生成清晰的指令，与有意识的提示和思维链相比，取得了最先进的性能。这项工作通过使机器人任务规划更加实用，特别是针对非专业用户（如老年人和儿童），为人工智能与机器人交互研究社区做出了贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10872v2">PDF</a> Under Review</p>
<p><strong>摘要</strong></p>
<p>机器人任务规划将人类指令分解成可执行的动作序列，使机器人能够完成一系列复杂的任务。虽然基于大型语言模型（LLM）的任务规划器取得了令人瞩目的性能，但它们假设人类指令是清晰和直接的。然而，真实世界的用户并非专家，他们对机器人的指令通常包含大量的模糊性。本文研究了人类指令中的参照表达式（REs）的模糊性如何影响基于LLM的机器人任务规划，并探讨了如何解决这一问题。为此，我们提出了首个含有模糊参照表达式的机器人任务规划基准测试（REI-Bench），发现参照表达式的模糊性会严重降低机器人规划的性能，成功率下降高达77.9%。我们还发现大多数失败的情况是由于规划中的目标缺失。为了缓解参照表达式的问题，我们提出了一种简单有效的方法：面向任务的上下文认知，为机器人生成清晰的指令，实现了与意识提示和思考链相比的先进性能。本研究为机器人与人类互动的研究群体做出了贡献，使机器人任务规划更加实用，特别是面向非专业用户，如老年人和儿童。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>机器人任务规划能将复杂的人类指令转化为可执行动作序列。</li>
<li>基于LLM的机器人任务规划器在假设人类指令清晰直接时表现最佳。</li>
<li>真实世界的用户指令常包含模糊性，主要源于参照表达式的使用。</li>
<li>参照表达式的模糊性对机器人任务规划性能产生重大影响，成功率下降可高达77.9%。</li>
<li>大多数规划失败的情况是因为目标在指令中的缺失。</li>
<li>提出了一种新的方法——面向任务的上下文认知，以处理模糊的参照表达式，为机器人生成清晰指令。</li>
<li>此方法实现了卓越的性能，特别是在处理非专业用户的指令时，如老年人和儿童。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10872">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a2abf10dd4a8a1d6fd576d8a39a3d284.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85341f39acecd2dfced3750d1c918150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d33e1a00fa5960962c37422748e9d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5f4803a5cea43c0a09a64e58c1142e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RICo-Refined-In-Context-Contribution-for-Automatic-Instruction-Tuning-Data-Selection"><a href="#RICo-Refined-In-Context-Contribution-for-Automatic-Instruction-Tuning-Data-Selection" class="headerlink" title="RICo: Refined In-Context Contribution for Automatic Instruction-Tuning   Data Selection"></a>RICo: Refined In-Context Contribution for Automatic Instruction-Tuning   Data Selection</h2><p><strong>Authors:Yixin Yang, Qingxiu Dong, Linli Yao, Fangwei Zhu, Zhifang Sui</strong></p>
<p>Data selection for instruction tuning is crucial for improving the performance of large language models (LLMs) while reducing training costs. In this paper, we propose Refined Contribution Measurement with In-Context Learning (RICo), a novel gradient-free method that quantifies the fine-grained contribution of individual samples to both task-level and global-level model performance. RICo enables more accurate identification of high-contribution data, leading to better instruction tuning. We further introduce a lightweight selection paradigm trained on RICo scores, enabling scalable data selection with a strictly linear inference complexity. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of RICo. Remarkably, on LLaMA3.1-8B, models trained on 15% of RICo-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by RICo, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones. </p>
<blockquote>
<p>数据选择在指令微调中对于提高大型语言模型（LLM）的性能和降低训练成本至关重要。在本文中，我们提出了基于上下文学习的精细贡献度量（RICo），这是一种新的无需梯度的方法，可以量化单个样本对任务级别和全局级别模型性能的精细贡献。RICo能够更准确地识别高贡献数据，从而实现更好的指令微调。我们进一步引入了一种基于RICo分数的轻量级选择范式，实现了具有严格线性推理复杂度的可扩展数据选择。在三个LLM的12个基准测试和5个配对评估集上的大量实验证明了RICo的有效性。值得注意的是，在LLaMA3.1-8B上，使用RICo选择数据的15%训练的模型在全数据集上表现更优，高出5.42个百分点，并超过了广泛使用的选择方法中的最佳性能，高出2.06个百分点。我们还进一步分析了RICo选择的高贡献样本，这些样本显示了多样化的任务和适当的难度水平，而不仅仅是难度最大的任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Refined Contribution Measurement with In-Context Learning（RICo）的数据选择方法，用于改进大型语言模型（LLM）的性能并降低训练成本。RICo是一种无需梯度的全新方法，能够精确衡量每个样本对任务级别和全局级别模型性能的细微贡献。该方法可以更准确地识别高贡献数据，从而实现更优质的指令微调。此外，本文还引入了一种基于RICo得分的轻量级选择范式，可实现具有严格线性推理复杂度的可扩展数据选择。广泛实验表明，RICo在三个LLM上，跨越12个基准测试和5个配对评估集均表现出良好的效果。特别是，使用RICo选择的数据训练的LLaMA3.1-8B模型在仅使用15%数据的情况下，性能比使用全数据集高出5.42%，并超过了广泛使用的选择方法中的最佳性能2.06%。对RICo选择的高贡献样本的分析显示，这些样本具有多样化的任务和适当的难度级别，并非仅仅是难度最大的样本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RICo是一种新型数据选择方法，用于改进LLM的性能并降低训练成本。</li>
<li>RICo通过精确衡量每个样本的贡献来实现更准确的数据识别。</li>
<li>RICo可以支持更优质的指令微调。</li>
<li>引入了一种基于RICo得分的轻量级选择范式，实现数据选择的可扩展性。</li>
<li>RICo在多个LLM和基准测试上表现出良好的性能提升效果。</li>
<li>使用RICo选择的数据训练的LLaMA模型性能显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0cd8f3458a109e5c4dab98be32846a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-378cf992dac67e4ab54c2a218b8e2482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3af9376a2abb945bebfd53603cd96910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c53a38aac64139f75931c08917b8fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c929de476d7c33c46272947702f9008.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c09a172b44f945ea3cc9daf925f31981.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-21  Synthesis of Communication Policies for Multi-Agent Systems Robust to   Communication Restrictions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cd7c459344316127e56a86ccda73a2dd.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-21  ChartMuseum Testing Visual Reasoning Capabilities of Large   Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23394.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
