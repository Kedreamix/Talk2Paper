<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Optimizing Anytime Reasoning via Budget Relative Policy Optimization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1103a4e20d6934d949a1b818d06d3b19.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-21-æ›´æ–°"><a href="#2025-05-21-æ›´æ–°" class="headerlink" title="2025-05-21 æ›´æ–°"></a>2025-05-21 æ›´æ–°</h1><h2 id="Optimizing-Anytime-Reasoning-via-Budget-Relative-Policy-Optimization"><a href="#Optimizing-Anytime-Reasoning-via-Budget-Relative-Policy-Optimization" class="headerlink" title="Optimizing Anytime Reasoning via Budget Relative Policy Optimization"></a>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</h2><p><strong>Authors:Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin</strong></p>
<p>Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œæå‡æµ‹è¯•æ—¶é—´çš„è®¡ç®—èƒ½åŠ›å¯¹äºå¢å¼ºæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æœ€å¤§åŒ–æ¨ç†è½¨è¿¹æœ«å°¾çš„å¯éªŒè¯å¥–åŠ±ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…é’ˆå¯¹å›ºå®šæ ‡è®°é¢„ç®—ä¸‹çš„æœ€ç»ˆæ€§èƒ½è¿›è¡Œä¼˜åŒ–ï¼Œè¿™é˜»ç¢äº†è®­ç»ƒå’Œéƒ¨ç½²çš„æ•ˆç‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶AnytimeReasonerï¼Œæ—¨åœ¨ä¼˜åŒ–ä»»æ„æ—¶é—´çš„æ¨ç†æ€§èƒ½ï¼Œæ—¨åœ¨æé«˜æ ‡è®°æ•ˆç‡å’Œåœ¨ä¸åŒæ ‡è®°é¢„ç®—çº¦æŸä¸‹çš„æ¨ç†çµæ´»æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹æˆªæ–­ä»¥é€‚åº”ä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·çš„æ ‡è®°é¢„ç®—ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºæ¯æ¬¡æˆªæ–­çš„æ€è€ƒè¿‡ç¨‹æ€»ç»“æœ€ä½³ç­”æ¡ˆä»¥è¿›è¡ŒéªŒè¯ã€‚è¿™ä¸ºæ¨ç†è¿‡ç¨‹å¼•å…¥äº†å¯éªŒè¯çš„å¯†é›†å¥–åŠ±ï¼Œæœ‰åŠ©äºæ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­çš„å¥–åŠ±åˆ†é…ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»¥è§£è€¦çš„æ–¹å¼ä¼˜åŒ–æ€è€ƒå’Œæ€»ç»“ç­–ç•¥ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹å·®å‡å°‘æŠ€æœ¯â€”â€”é¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œä»¥æé«˜åœ¨å¼ºåŒ–æ€è€ƒç­–ç•¥æ—¶å­¦ä¹ è¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡çš„å®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ€è€ƒé¢„ç®—ä¸‹å‡ä¼˜äºGRPOåœ¨å„ç§å…ˆéªŒåˆ†å¸ƒçš„è¡¨ç°ï¼Œæé«˜äº†è®­ç»ƒå’Œæ ‡è®°çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13438v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤„ç†å¤§é‡è®¡ç®—ä»»åŠ¡ï¼Œä¸ºæ­¤å¿…é¡»æ‰©å¤§æµ‹è¯•æ—¶é—´è®¡ç®—è§„æ¨¡ä»¥æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä»…åœ¨æœ€ç»ˆé˜¶æ®µæœ€å¤§åŒ–å¥–åŠ±å¾—åˆ†ï¼Œè€Œè¿™æ— æ³•æ»¡è¶³æŒç»­å˜åŒ–çš„æ ‡è®°éœ€æ±‚å’Œæé«˜æ¨¡å‹çš„çµæ´»æ€§åŠä»¤ç‰Œæ•ˆç‡çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAnytimeReasonerçš„æ–°æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–ä»»æ„æ—¶é—´çš„æ¨ç†æ€§èƒ½ï¼Œæå‡æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œåœ¨ä¸åŒä»¤ç‰Œé¢„ç®—çº¦æŸä¸‹çš„çµæ´»æ€§ã€‚å…¶å®ç°è¿‡ç¨‹åŒ…æ‹¬å¯¹å®Œæ•´æ¨ç†è¿‡ç¨‹çš„æˆªæ–­å¤„ç†ä»¥é€‚åº”ç‰¹å®šé‡‡æ ·ä»¤ç‰Œé¢„ç®—éœ€æ±‚ï¼Œè¦æ±‚æ¨¡å‹å¿…é¡»æ±‡æ€»ä¸åŒæˆªæ–­æƒ…å†µä¸‹çš„æœ€ä¼˜ç­”æ¡ˆä»¥ä¾¿è¿›è¡ŒéªŒè¯ï¼Œå¼•å…¥äº†ä¸€ç§å³æ—¶éªŒè¯æœºåˆ¶å¹¶äº§ç”Ÿäº†å¯†é›†å‹å¥–åŠ±ä¿¡æ¯ä»¥å¢å¼ºå­¦ä¹ çš„å¯é æ€§åŠè°ƒæ•´ä»»åŠ¡çš„åŠ¨æ€æ‰§è¡Œè·¯å¾„ä»¥æå‡æ‰§è¡Œæ•ˆç‡å’Œé¿å…é”™è¿‡è·å–ä¸­é—´é˜¶æ®µæ­£ç¡®çš„å›ç­”è€Œå¯¼è‡´å¤±æ§é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ–¹å·®ç¼©å‡æŠ€æœ¯â€”â€”é¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œä»¥å¢å¼ºç­–ç•¥å­¦ä¹ è¿‡ç¨‹æ—¶çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚åœ¨å®è¯çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å…ˆéªŒåˆ†å¸ƒä¸‹å„æ€è€ƒé¢„ç®—å‡è¡¨ç°å‡ºä¼˜äºGRPOçš„æ•ˆæœï¼Œè¡¨æ˜æ­¤æ–¹æ³•èƒ½å¤Ÿåœ¨æé«˜æ¨ç†æ€§èƒ½å’Œå¢å¼ºè®¡ç®—æ•ˆç‡ä¸¤æ–¹é¢æä¾›ç¨³å¥è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬æå–çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>æµ‹è¯•æ—¶é—´çš„è®¡ç®—æ‰©å±•å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦å…³æ³¨æœ€ç»ˆæ€§èƒ½çš„ä¼˜åŒ–ã€‚</li>
<li>AnytimeReasoneræ¡†æ¶æ—¨åœ¨ä¼˜åŒ–ä»»æ„æ—¶é—´çš„æ¨ç†æ€§èƒ½ï¼Œæé«˜æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡å’Œçµæ´»æ€§ã€‚</li>
<li>AnytimeReasoneré€šè¿‡æˆªæ–­å®Œæ•´çš„æ¨ç†è¿‡ç¨‹ä»¥é€‚åº”ä¸åŒçš„ä»¤ç‰Œé¢„ç®—çº¦æŸï¼Œå¹¶å¼•å…¥å³æ—¶éªŒè¯å’Œå¥–åŠ±ä¿¡æ¯æœºåˆ¶æå‡æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶è¦æ±‚å¯¹æ¨¡å‹åœ¨ä¸åŒæˆªæ–­æ€è€ƒä¸‹çš„æœ€ä¼˜ç­”æ¡ˆè¿›è¡Œæ±‡æ€»ä»¥éªŒè¯è¾“å‡ºå‡†ç¡®æ€§å¹¶æ”¹å–„åŠ¨æ€ä»»åŠ¡è·¯å¾„çš„è°ƒæ•´è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹å·®ç¼©å‡æŠ€æœ¯â€”â€”é¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œä»¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab7f0514cd9aaee07b0e579d019548a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e18da88445987aa06e1873ce9ae3403.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3854d94a83e2188024a13add244d982e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-088a2ec05312ed15c4112a44fdcb572f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-PRM-Enhancing-Multimodal-Mathematical-Reasoning-with-Scalable-Step-Level-Supervision"><a href="#MM-PRM-Enhancing-Multimodal-Mathematical-Reasoning-with-Scalable-Step-Level-Supervision" class="headerlink" title="MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable   Step-Level Supervision"></a>MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable   Step-Level Supervision</h2><p><strong>Authors:Lingxiao Du, Fanqing Meng, Zongkai Liu, Zhixiang Zhou, Ping Luo, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-PRM">https://github.com/ModalMinds/MM-PRM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¸¸å¸¸äº§ç”Ÿé€»è¾‘ä¸ä¸€è‡´æˆ–éƒ¨åˆ†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä¸€ä¸ªå…³é”®çš„å±€é™æ€§åœ¨äºä¸­é—´æ¨ç†æ­¥éª¤ç¼ºä¹ç²¾ç»†çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MM-PRMï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å…¨è‡ªåŠ¨åŒ–å¯æ‰©å±•æ¡†æ¶ä¸­è®­ç»ƒçš„æµç¨‹å¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹MM-Policyï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ ·åŒ–çš„æ•°å­¦æ¨ç†æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†MM-K12æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸€ä¸‡é“å¯éªŒè¯ç­”æ¡ˆçš„å¤šæ¨¡æ€æ•°å­¦é—®é¢˜é›†ï¼Œä½œä¸ºç§å­æ•°æ®ã€‚é€šè¿‡åˆ©ç”¨åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„ç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡70ä¸‡æ­¥çº§åˆ«çš„æ³¨é‡Šï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚æ‰€å¾—çš„PRMç”¨äºåœ¨Best-of-Næ¨ç†è®¾ç½®ä¸­è¯„åˆ†å€™é€‰æ¨ç†è·¯å¾„ï¼Œå¹¶åœ¨å†…éƒ¨é¢†åŸŸï¼ˆMM-K12æµ‹è¯•é›†ï¼‰å’Œå¤–éƒ¨é¢†åŸŸï¼ˆOlympiadBenchã€MathVistaç­‰ï¼‰çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†è½¯æ ‡ç­¾ã€è¾ƒå°å­¦ä¹ ç‡å’Œè·¯å¾„å¤šæ ·æ€§åœ¨ä¼˜åŒ–PRMæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚MM-PRMè¯æ˜è¿‡ç¨‹ç›‘ç£æ˜¯æé«˜å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿé€»è¾‘ç¨³å¥æ€§çš„æœ‰åŠ›å·¥å…·ã€‚æˆ‘ä»¬å·²å°†æ‰€æœ‰ä»£ç å’Œæ•°æ®å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-PRM%E3%80%82">https://github.com/ModalMinds/MM-PRMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†æ–¹é¢å­˜åœ¨çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMM-PRMçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºMM-Policyå’ŒMM-K12ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”Ÿæˆè¶…è¿‡70ä¸‡æ­¥çš„æ³¨é‡Šæ•°æ®ï¼Œå®ç°äº†å¯¹å€™é€‰æ¨ç†è·¯å¾„çš„è¯„åˆ†ï¼Œæ˜¾è‘—æé«˜äº†åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚ç ”ç©¶è¯å®äº†è¿‡ç¨‹ç›‘ç£åœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„é€»è¾‘ç¨³å¥æ€§æ–¹é¢çš„å¼ºå¤§ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸¸äº§ç”Ÿé€»è¾‘ä¸ä¸€è‡´æˆ–éƒ¨åˆ†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>MM-PRMæ¨¡å‹é€šè¿‡æ„å»ºMM-Policyå’ŒMM-K12æ•°æ®é›†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MM-K12æ•°æ®é›†åŒ…å«1ä¸‡ä¸ªå¤šæ¨¡æ€æ•°å­¦é—®é¢˜åŠå…¶å¯éªŒè¯çš„ç­”æ¡ˆï¼Œä½œä¸ºç§å­æ•°æ®ã€‚</li>
<li>åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”Ÿæˆäº†è¶…è¿‡70ä¸‡æ­¥çš„æ³¨é‡Šæ•°æ®ï¼Œå®ç°äº†å…¨è‡ªåŠ¨ã€å¯æ‰©å±•çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚</li>
<li>PRMåœ¨Best-of-Næ¨ç†è®¾ç½®ä¸­å¯¹å€™é€‰æ¨ç†è·¯å¾„è¿›è¡Œè¯„åˆ†ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯å®äº†è¿‡ç¨‹ç›‘ç£åœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„é€»è¾‘ç¨³å¥æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c915a603cf50a0a534b2d84ce3fd7677.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ad800df424f2c31542099f4e1ec67b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learnware-of-Language-Models-Specialized-Small-Language-Models-Can-Do-Big"><a href="#Learnware-of-Language-Models-Specialized-Small-Language-Models-Can-Do-Big" class="headerlink" title="Learnware of Language Models: Specialized Small Language Models Can Do   Big"></a>Learnware of Language Models: Specialized Small Language Models Can Do   Big</h2><p><strong>Authors:Zhi-Hao Tan, Zi-Chen Zhao, Hao-Yu Shi, Xin-Yu Zhang, Peng Tan, Yang Yu, Zhi-Hua Zhou</strong></p>
<p>The learnware paradigm offers a novel approach to machine learning by enabling users to reuse a set of well-trained models for tasks beyond the modelsâ€™ original purposes. It eliminates the need to build models from scratch, instead relying on specifications (representations of a modelâ€™s capabilities) to identify and leverage the most suitable models for new tasks. While learnware has proven effective in many scenarios, its application to language models has remained largely unexplored. At the same time, large language models (LLMs) have demonstrated remarkable universal question-answering abilities, yet they face challenges in specialized scenarios due to data scarcity, privacy concerns, and high computational costs, thus more and more specialized small language models (SLMs) are being trained for specific domains. To address these limitations systematically, the learnware paradigm provides a promising solution by enabling maximum utilization of specialized SLMs, and allowing users to identify and reuse them in a collaborative and privacy-preserving manner.   This paper presents a preliminary attempt to apply the learnware paradigm to language models. We simulated a learnware system comprising approximately 100 learnwares of specialized SLMs with 8B parameters, fine-tuned across finance, healthcare, and mathematics domains. Each learnware contains an SLM and a specification, which enables users to identify the most relevant models without exposing their own data. Experimental results demonstrate promising performance: by selecting one suitable learnware for each task-specific inference, the system outperforms the base SLMs on all benchmarks. Compared to LLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and Llama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses Flan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical domain tasks. </p>
<blockquote>
<p>å­¦ä¹ è½¯ä»¶èŒƒå¼é€šè¿‡ä½¿ç”¨æˆ·èƒ½å¤Ÿé‡å¤ä½¿ç”¨ä¸€ç»„ç»è¿‡è‰¯å¥½è®­ç»ƒçš„æ¨¡å‹æ¥å®Œæˆè¶…å‡ºæ¨¡å‹åŸå§‹ç›®çš„çš„çš„ä»»åŠ¡ï¼Œä¸ºæœºå™¨å­¦ä¹ æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ã€‚å®ƒæ¶ˆé™¤äº†ä»å¤´æ„å»ºæ¨¡å‹çš„éœ€æ±‚ï¼Œè€Œæ˜¯ä¾èµ–äºè§„èŒƒï¼ˆæ¨¡å‹çš„èƒ½åŠ›çš„è¡¨ç¤ºï¼‰æ¥è¯†åˆ«å’Œåˆ©ç”¨æœ€é€‚åˆæ–°ä»»åŠ¡çš„æ¨¡å‹ã€‚è™½ç„¶å­¦ä¹ è½¯ä»¶åœ¨è®¸å¤šåœºæ™¯ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å…¶å¯¹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å´é²œæœ‰æ¢ç´¢ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ˜¾ç¤ºå‡ºæƒŠäººçš„é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºã€éšç§æ‹…å¿§å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ï¼Œå®ƒä»¬åœ¨ç‰¹å®šåœºæ™¯ä¸­çš„è¡¨ç°å—åˆ°é™åˆ¶ã€‚å› æ­¤ï¼Œè¶Šæ¥è¶Šå¤šçš„é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ­£åœ¨æ¥å—è®­ç»ƒã€‚ä¸ºäº†ç³»ç»Ÿåœ°è§£å†³è¿™äº›å±€é™æ€§ï¼Œå­¦ä¹ è½¯ä»¶èŒƒå¼é€šè¿‡æœ€å¤§é™åº¦åœ°åˆ©ç”¨ä¸“é—¨çš„SLMå¹¶å…è®¸ç”¨æˆ·ä»¥åä½œå’Œéšç§ä¿æŠ¤çš„æ–¹å¼è¯†åˆ«å’Œé‡å¤ä½¿ç”¨å®ƒä»¬ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æ˜¯å°è¯•å°†å­¦ä¹ è½¯ä»¶èŒƒå¼åº”ç”¨äºè¯­è¨€æ¨¡å‹çš„åˆæ­¥å°è¯•ã€‚æˆ‘ä»¬æ¨¡æ‹Ÿäº†ä¸€ä¸ªå­¦ä¹ è½¯ä»¶ç³»ç»Ÿï¼Œå…¶ä¸­åŒ…æ‹¬å¤§çº¦100ä¸ªSLMçš„ç»„ä»¶ï¼ˆå…·æœ‰è·¨é‡‘èã€åŒ»ç–—å’Œæ•°å­¦é¢†åŸŸçš„å¾®è°ƒå‚æ•°ï¼‰ï¼Œæ¯ä¸ªå­¦ä¹ è½¯ä»¶ä¸­éƒ½åŒ…å«ä¸€ä¸ªSLMå’Œä¸€ä¸ªè§„èŒƒï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè¯†åˆ«æœ€ç›¸å…³çš„æ¨¡å‹è€Œä¸æš´éœ²è‡ªå·±çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªç‰¹å®šä»»åŠ¡é€‰æ‹©é€‚å½“çš„å­¦ä¹ è½¯ä»¶æ¥è¿›è¡Œæ¨ç†ï¼Œè¯¥ç³»ç»Ÿåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºç¡€SLMçš„æ€§èƒ½ã€‚ä¸LLMç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨é‡‘èé¢†åŸŸä»»åŠ¡ä¸­è‡³å°‘ä¼˜äºQwen 1.5-110Bã€Qwen 2.5-72Bå’ŒLlama 3.1-70B-Instruct 14%ï¼Œå¹¶åœ¨åŒ»ç–—é¢†åŸŸä»»åŠ¡ä¸­è¶…è¶Šäº†åœ¨å¼€æ”¾åŒ»å­¦LLMæ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸ƒçš„Flan-PaLM-540Bã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å­¦ä¹ è½¯ä»¶èŒƒå¼åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„æ–°åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚å­¦ä¹ è½¯ä»¶èŒƒå¼å…è®¸ç”¨æˆ·é‡ç”¨å·²è®­ç»ƒå¥½çš„æ¨¡å‹æ¥å®Œæˆè¶…å‡ºå…¶åŸå§‹ç›®çš„çš„ä»»åŠ¡ã€‚æ–‡ç« å°è¯•å°†å­¦ä¹ è½¯ä»¶èŒƒå¼åº”ç”¨äºè¯­è¨€æ¨¡å‹ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªåŒ…å«çº¦100ä¸ªå­¦ä¹ è½¯ä»¶çš„ä¸“ä¸šå°å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿï¼Œå¹¶åœ¨é‡‘èã€åŒ»ç–—å’Œæ•°å­¦é¢†åŸŸè¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨é€‰æ‹©é€‚åˆæ¯ä¸ªä»»åŠ¡çš„å­¦ä¹ è½¯ä»¶æ—¶ï¼Œå…¶æ€§èƒ½ä¼˜äºåŸºç¡€å°å‹è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ è½¯ä»¶èŒƒå¼å…è®¸ç”¨æˆ·é‡ç”¨å·²è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œé¿å…äº†ä»å¤´å¼€å§‹æ„å»ºæ¨¡å‹çš„ç¹çè¿‡ç¨‹ã€‚</li>
<li>è¯¥èŒƒå¼é€šè¿‡è§„æ ¼ï¼ˆæ¨¡å‹çš„è¡¨ç¤ºï¼‰æ¥è¯†åˆ«å’Œåˆ©ç”¨æœ€é€‚åˆæ–°ä»»åŠ¡çš„æ¨¡å‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶åœ¨é€šç”¨é—®ç­”æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸé¢ä¸´æ•°æ®ç¨€ç¼ºã€éšç§å…³æ³¨å’Œè®¡ç®—æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å­¦ä¹ è½¯ä»¶èŒƒå¼ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æœ€å¤§åŒ–åˆ©ç”¨ä¸“ä¸šå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å¹¶å…è®¸ç”¨æˆ·ä»¥åä½œå’Œéšç§ä¿æŠ¤çš„æ–¹å¼è¯†åˆ«å’Œé‡ç”¨å®ƒä»¬ã€‚</li>
<li>æ¨¡æ‹Ÿç³»ç»ŸåŒ…æ‹¬å¤§çº¦100ä¸ªå­¦ä¹ è½¯ä»¶çš„ä¸“ä¸šå°å‹è¯­è¨€æ¨¡å‹ï¼Œæ¶µç›–é‡‘èã€åŒ»ç–—å’Œæ•°å­¦ç­‰é¢†åŸŸã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨é€‰æ‹©é€‚åˆæ¯ä¸ªä»»åŠ¡çš„å­¦ä¹ è½¯ä»¶æ—¶ï¼Œæ€§èƒ½ä¼˜äºåŸºç¡€å°å‹è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-366e059a335a61a793726536673d75ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9be61d689eb52adbfade6bd104b5c37b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931ceb28429bec55bf1fd30df7461f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2949c55dd9f9705a34d004f0d57a9594.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-714127920524533565d156965e89c4ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Make-Still-Further-Progress-Chain-of-Thoughts-for-Tabular-Data-Leaderboard"><a href="#Make-Still-Further-Progress-Chain-of-Thoughts-for-Tabular-Data-Leaderboard" class="headerlink" title="Make Still Further Progress: Chain of Thoughts for Tabular Data   Leaderboard"></a>Make Still Further Progress: Chain of Thoughts for Tabular Data   Leaderboard</h2><p><strong>Authors:Si-Yang Liu, Qile Zhou, Han-Jia Ye</strong></p>
<p>Tabular data, a fundamental data format in machine learning, is predominantly utilized in competitions and real-world applications. The performance of tabular modelsâ€“such as gradient boosted decision trees and neural networksâ€“can vary significantly across datasets due to differences in feature distributions and task characteristics. Achieving top performance on each dataset often requires specialized expert knowledge. To address this variability, practitioners often aggregate the predictions of multiple models. However, conventional aggregation strategies typically rely on static combination rules and lack instance-level adaptability. In this work, we propose an in-context ensemble framework for tabular prediction that leverages large language models (LLMs) to perform dynamic, instance-specific integration of external model predictions. Without access to raw tabular features or semantic information, our method constructs a context around each test instance using its nearest neighbors and the predictions from a pool of external models. Within this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$), a prompting strategy that guides LLMs through multi-step, interpretable reasoning, making still further progress toward expert-level decision-making. Experimental results show that our method outperforms well-tuned baselines and standard ensemble techniques across a wide range of tabular datasets. </p>
<blockquote>
<p>è¡¨æ ¼æ•°æ®æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„åŸºæœ¬æ•°æ®æ ¼å¼ï¼Œåœ¨ç«èµ›å’Œå®é™…åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç”±äºç‰¹å¾åˆ†å¸ƒå’Œä»»åŠ¡ç‰¹æ€§çš„å·®å¼‚ï¼Œæ¢¯åº¦å¢å¼ºå†³ç­–æ ‘å’Œç¥ç»ç½‘ç»œç­‰è¡¨æ ¼æ¨¡å‹çš„æ€§èƒ½åœ¨ä¸åŒæ•°æ®é›†ä¸Šå¯èƒ½ä¼šæœ‰å¾ˆå¤§å·®å¼‚ã€‚åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½é€šå¸¸éœ€è¦ä¸“ä¸šçš„ä¸“å®¶çŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™ç§å·®å¼‚ï¼Œä»ä¸šè€…é€šå¸¸ä¼šèšåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„èšåˆç­–ç•¥é€šå¸¸ä¾èµ–äºé™æ€çš„ç»„åˆè§„åˆ™ï¼Œç¼ºä¹å®ä¾‹çº§åˆ«çš„é€‚åº”æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè¡¨æ ¼é¢„æµ‹çš„ä¸Šä¸‹æ–‡é›†æˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡ŒåŠ¨æ€ã€é’ˆå¯¹ç‰¹å®šå®ä¾‹çš„å¤–éƒ¨æ¨¡å‹é¢„æµ‹é›†æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è®¿é—®åŸå§‹è¡¨æ ¼ç‰¹å¾æˆ–è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œæ˜¯å›´ç»•æ¯ä¸ªæµ‹è¯•å®ä¾‹æ„å»ºä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œä½¿ç”¨å…¶æœ€è¿‘é‚»å±…å’Œæ¥è‡ªå¤–éƒ¨æ¨¡å‹æ± ä¸­çš„é¢„æµ‹ç»“æœã€‚åœ¨è¿™ä¸ªä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œè¡¨æ ¼æ€ç»´é“¾â€ï¼ˆCoT$^2$ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºç­–ç•¥ï¼Œé€šè¿‡å¤šæ­¥éª¤çš„å¯è§£é‡Šæ¨ç†æ¥å¼•å¯¼LLMï¼Œè¿›ä¸€æ­¥æ¨åŠ¨å‘ä¸“å®¶çº§å†³ç­–åˆ¶å®šè¿ˆè¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¡¨æ ¼æ•°æ®é›†ä¸Šä¼˜äºç»è¿‡è‰¯å¥½è°ƒæ•´çš„åŸºå‡†æ¨¡å‹å’Œæ ‡å‡†çš„é›†æˆæŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13421v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­ä¸»è¦æ¢è®¨äº†è¡¨æ ¼æ•°æ®åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦æ€§åŠå…¶åœ¨ç«èµ›å’Œå®é™…åº”ç”¨ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç”±äºç‰¹å¾åˆ†å¸ƒå’Œä»»åŠ¡ç‰¹æ€§çš„å·®å¼‚ï¼Œæ¢¯åº¦å¢å¼ºå†³ç­–æ ‘å’Œç¥ç»ç½‘ç»œç­‰è¡¨æ ¼æ¨¡å‹çš„æ€§èƒ½åœ¨ä¸åŒæ•°æ®é›†ä¸Šå¯èƒ½ä¼šæœ‰å¾ˆå¤§å·®å¼‚ã€‚ä¸ºäº†åº”å¯¹è¿™ç§å·®å¼‚å¹¶å®ç°æœ€ä½³æ€§èƒ½ï¼Œç ”ç©¶äººå‘˜é€šå¸¸é‡‡ç”¨å¤šç§æ¨¡å‹çš„é¢„æµ‹èšåˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„èšåˆç­–ç•¥é€šå¸¸ä¾èµ–äºé™æ€ç»„åˆè§„åˆ™ï¼Œç¼ºä¹å®ä¾‹çº§åˆ«çš„é€‚åº”æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡é›†æˆæ¡†æ¶ï¼Œç”¨äºåŠ¨æ€ã€ç‰¹å®šå®ä¾‹åœ°æ•´åˆå¤–éƒ¨æ¨¡å‹é¢„æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºæ¯ä¸ªæµ‹è¯•å®ä¾‹çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼ˆä½¿ç”¨å…¶æœ€è¿‘é‚»å±…å’Œå¤–éƒ¨æ¨¡å‹çš„é¢„æµ‹ï¼‰ï¼Œåœ¨ä¸éœ€è¦åŸå§‹è¡¨æ ¼ç‰¹å¾æˆ–è¯­ä¹‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºChain of Tabular Thoughtsï¼ˆCoT$^2$ï¼‰çš„æç¤ºç­–ç•¥ï¼Œé€šè¿‡å¤šæ­¥éª¤ã€å¯è§£é‡Šæ€§çš„æ¨ç†å¼•å¯¼LLMè¿›è¡Œå†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æ ¼æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç²¾å¿ƒè°ƒæ•´çš„åŸºå‡†æ¨¡å‹å’Œæ ‡å‡†çš„é›†æˆæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼æ•°æ®åœ¨æœºå™¨å­¦ä¹ å’Œå®é™…åº”ç”¨ä¸­å¹¿æ³›åº”ç”¨ã€‚</li>
<li>è¡¨æ ¼æ¨¡å‹çš„æ€§èƒ½åœ¨ä¸åŒæ•°æ®é›†ä¸Šå¯èƒ½ä¼šæœ‰æ˜¾è‘—å·®å¼‚ï¼Œéœ€è€ƒè™‘ç‰¹å¾åˆ†å¸ƒå’Œä»»åŠ¡ç‰¹æ€§ã€‚</li>
<li>ä¸ºäº†æé«˜æ€§èƒ½ï¼Œç ”ç©¶äººå‘˜å¸¸é‡‡ç”¨å¤šæ¨¡å‹é¢„æµ‹èšåˆçš„æ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿèšåˆç­–ç•¥ç¼ºä¹å®ä¾‹çº§åˆ«çš„é€‚åº”æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é›†æˆæ¡†æ¶ï¼Œç”¨äºåŠ¨æ€æ•´åˆå¤–éƒ¨æ¨¡å‹é¢„æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ„å»ºæµ‹è¯•å®ä¾‹çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œå¼•å…¥Chain of Tabular Thoughtsï¼ˆCoT$^2$ï¼‰æç¤ºç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2693a9fe345d5ffc5aaa7d8e4a5e0400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0b1ebad1e917b7ca7779ff49ed7c839.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e122b0d730ae7ea0818fff68d54ba38e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FEALLM-Advancing-Facial-Emotion-Analysis-in-Multimodal-Large-Language-Models-with-Emotional-Synergy-and-Reasoning"><a href="#FEALLM-Advancing-Facial-Emotion-Analysis-in-Multimodal-Large-Language-Models-with-Emotional-Synergy-and-Reasoning" class="headerlink" title="FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language   Models with Emotional Synergy and Reasoning"></a>FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language   Models with Emotional Synergy and Reasoning</h2><p><strong>Authors:Zhuozhao Hu, Kaishen Yuan, Xin Liu, Zitong Yu, Yuan Zong, Jingang Shi, Huanjing Yue, Jingyu Yang</strong></p>
<p>Facial Emotion Analysis (FEA) plays a crucial role in visual affective computing, aiming to infer a personâ€™s emotional state based on facial data. Scientifically, facial expressions (FEs) result from the coordinated movement of facial muscles, which can be decomposed into specific action units (AUs) that provide detailed emotional insights. However, traditional methods often struggle with limited interpretability, constrained generalization and reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have shown exceptional performance in various visual tasks, while they still face significant challenges in FEA due to the lack of specialized datasets and their inability to capture the intricate relationships between FEs and AUs. To address these issues, we introduce a novel FEA Instruction Dataset that provides accurate and aligned FE and AU descriptions and establishes causal reasoning relationships between them, followed by constructing a new benchmark, FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to capture more detailed facial information, enhancing its capability in FEA tasks. Our model demonstrates strong performance on FEABench and impressive generalization capability through zero-shot evaluation on various datasets, including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and effectiveness in FEA tasks. The dataset and code will be available at <a target="_blank" rel="noopener" href="https://github.com/953206211/FEALLM">https://github.com/953206211/FEALLM</a>. </p>
<blockquote>
<p>é¢éƒ¨æƒ…æ„Ÿåˆ†æï¼ˆFEAï¼‰åœ¨è§†è§‰æƒ…æ„Ÿè®¡ç®—ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå…¶æ—¨åœ¨åŸºäºé¢éƒ¨æ•°æ®æ¨æ–­ä¸€ä¸ªäººçš„æƒ…æ„ŸçŠ¶æ€ã€‚ç§‘å­¦ä¸Šï¼Œé¢éƒ¨è¡¨æƒ…ï¼ˆFEï¼‰æ˜¯é¢éƒ¨è‚Œè‚‰åè°ƒè¿åŠ¨çš„ç»“æœï¼Œå¯ä»¥åˆ†è§£ä¸ºç‰¹å®šçš„åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ï¼Œä»è€Œæä¾›è¯¦ç»†çš„æƒ…æ„Ÿæ´å¯Ÿã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•å¸¸å¸¸åœ¨è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè€Œåœ¨FEAä¸­ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹ä¸“ä¸šæ•°æ®é›†å’Œæ— æ³•æ•æ‰é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒä¹‹é—´å¤æ‚çš„å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„é¢éƒ¨æƒ…æ„Ÿåˆ†ææŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›å‡†ç¡®å’Œå¯¹é½çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒæè¿°ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å› æœæ¨ç†å…³ç³»ï¼Œç„¶åæ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•FEABenchã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†FEALLMï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„MLLMæ¶æ„ï¼Œæ—¨åœ¨æ•æ‰æ›´è¯¦ç»†çš„é¢éƒ¨ä¿¡æ¯ï¼Œæé«˜å…¶åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨FEABenchä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å„ç§æ•°æ®é›†ï¼ˆåŒ…æ‹¬RAF-DBã€AffectNetã€BP4Då’ŒDISFAï¼‰ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°æ—¶ï¼Œå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/953206211/FEALLM">https://github.com/953206211/FEALLM</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13419v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>é¢éƒ¨è¡¨æƒ…åˆ†æï¼ˆFEAï¼‰åœ¨è§†è§‰æƒ…æ„Ÿè®¡ç®—ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œæ—¨åœ¨åŸºäºé¢éƒ¨æ•°æ®æ¨æ–­äººçš„æƒ…æ„ŸçŠ¶æ€ã€‚é¢éƒ¨è¡¨è¾¾æ˜¯ç”±é¢éƒ¨è‚Œè‚‰åè°ƒè¿åŠ¨äº§ç”Ÿçš„ï¼Œå¯ä»¥åˆ†è§£ä¸ºç‰¹å®šçš„åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ä»¥æä¾›è¯¦ç»†çš„æƒ…æ„Ÿè§è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨è§£é‡Šã€æ¨å¹¿å’Œæ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨FEAæ–¹é¢ä»é¢ä¸´ç¼ºä¹ä¸“ç”¨æ•°æ®é›†å’Œæ— æ³•æ•æ‰é¢éƒ¨è¡¨æƒ…ä¸åŠ¨ä½œå•å…ƒä¹‹é—´å¤æ‚å…³ç³»çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„FEAæŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›äº†å‡†ç¡®å¯¹é½çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒæè¿°ï¼Œå»ºç«‹äº†å®ƒä»¬ä¹‹é—´çš„å› æœå…³ç³»ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†æ–°çš„åŸºå‡†æµ‹è¯•FEABenchï¼Œå¹¶æå‡ºäº†FEALLMï¼Œä¸€ç§æ–°å‹MLLMæ¶æ„ï¼Œèƒ½å¤Ÿæ•æ‰æ›´è¯¦ç»†çš„é¢éƒ¨ä¿¡æ¯ï¼Œå¢å¼ºFEAä»»åŠ¡çš„èƒ½åŠ›ã€‚æ¨¡å‹åœ¨FEABenchä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šé€šè¿‡é›¶æ ·æœ¬è¯„ä¼°å±•ç¤ºäº†å…¶åœ¨FEAä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨è¡¨æƒ…åˆ†æï¼ˆFEAï¼‰æ˜¯è§†è§‰æƒ…æ„Ÿè®¡ç®—ä¸­çš„å…³é”®é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡é¢éƒ¨æ•°æ®æ¨æ–­äººçš„æƒ…æ„ŸçŠ¶æ€ã€‚</li>
<li>é¢éƒ¨è¡¨è¾¾æ˜¯ç”±é¢éƒ¨è‚Œè‚‰çš„åè°ƒè¿åŠ¨äº§ç”Ÿçš„ï¼Œå¯ä»¥åˆ†è§£ä¸ºåŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ä»¥è·å¾—è¯¦ç»†çš„æƒ…æ„Ÿç†è§£ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨FEAæ–¹é¢å­˜åœ¨è§£é‡Šæ€§ã€æ¨å¹¿å’Œæ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨FEAä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹ä¸“ç”¨æ•°æ®é›†å’Œæ•æ‰å¤æ‚å…³ç³»çš„èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>å¼•å…¥æ–°å‹çš„FEAæŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›å‡†ç¡®å¯¹é½çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒæè¿°ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å› æœå…³ç³»ã€‚</li>
<li>æ„å»ºäº†æ–°çš„FEAåŸºå‡†æµ‹è¯•FEABenchä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-927f8f88ba3523bc7da087f24df358bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf72b786d27e40d66f8592056978fe79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717e8a18a3d6ada6d61fed5bbde2bb92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fc32467b9d87e5770b4942ecb1169ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c94279218da334e770e450bd610a88eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MR-Judge-Multimodal-Reasoner-as-a-Judge"><a href="#MR-Judge-Multimodal-Reasoner-as-a-Judge" class="headerlink" title="MR. Judge: Multimodal Reasoner as a Judge"></a>MR. Judge: Multimodal Reasoner as a Judge</h2><p><strong>Authors:Renjie Pi, Felix Bai, Qibin Chen, Simon Wang, Jiulong Shan, Kieran Liu, Meng Cao</strong></p>
<p>The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè¯„ä»·è€…çš„èŒƒå¼ï¼Œåœ¨RLHFå’Œæ¨ç†æ—¶é—´ç¼©æ”¾ä¸­å·²æˆä¸ºä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå¤šæ¨¡æ€æ¨ç†è€…ä½œä¸ºè¯„åˆ¤å‘˜â€ï¼ˆMR Judgeï¼‰è¿™ä¸€èŒƒå¼ï¼Œæ—¨åœ¨èµ‹äºˆé€šç”¨MLLMè¯„åˆ¤å‘˜å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¹¶ä¸ç›´æ¥ä¸ºæ¯ç§å›åº”æ‰“åˆ†ï¼Œè€Œæ˜¯å°†è¯„åˆ¤è¿‡ç¨‹åˆ¶å®šä¸ºå—æ¨ç†å¯å‘çš„å¤šé¡¹é€‰æ‹©é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯„åˆ¤æ¨¡å‹é¦–å…ˆè¿›è¡Œæ¶µç›–å›åº”ä¸åŒæ–¹é¢çš„æ·±æ€ç†Ÿè™‘çš„æ¨ç†ï¼Œå¹¶æœ€ç»ˆä»ä¸­é€‰æ‹©æœ€ä½³çš„å›åº”ã€‚è¿™ç§æ¨ç†è¿‡ç¨‹ä¸ä»…æé«˜äº†è¯„åˆ¤çš„å¯è§£é‡Šæ€§ï¼Œè¿˜å¤§å¤§æé«˜äº†MLLMè¯„åˆ¤å‘˜çš„è¡¨ç°ã€‚ä¸ºäº†åº”å¯¹ç¼ºä¹å¸¦è¯„åˆ†å›åº”çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä»¥ä¸‹ç­–ç•¥æ¥å®ç°è‡ªåŠ¨æ ‡æ³¨ï¼š1ï¼‰åå‘å›åº”å€™é€‰åˆæˆï¼šä»ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†å¼€å§‹ï¼Œæˆ‘ä»¬å°†åŸå§‹å›åº”è§†ä¸ºæœ€ä½³å€™é€‰ï¼Œå¹¶æç¤ºMLLMç”Ÿæˆåˆç†çš„ä½†å­˜åœ¨ç¼ºé™·çš„è´Ÿé¢å€™é€‰ï¼›2ï¼‰åŸºäºæ–‡æœ¬çš„æ¨ç†æå–ï¼šæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªæ•°æ®åˆæˆç®¡é“ï¼Œä»¥ä»åŸºäºæ–‡æœ¬çš„æ¨ç†æ¨¡å‹ä¸­æç‚¼æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨è¯¥æ¨¡å‹ä½¿MLLMè¯„å§”èƒ½å¤Ÿé€šè¿‡çƒ­èº«çš„ç›‘ç£å¾®è°ƒæ¢å¤å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MR Judgeåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„MR Judge-7Båœ¨VL-RewardBenchä¸Šçš„è¡¨ç°ä¼˜äºGPT-4oï¼Œè¶…è¿‡9.9%ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´ç¼©æ”¾æœŸé—´MM-Vetä¸Šçš„æ€§èƒ½æé«˜äº†é«˜è¾¾7.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä¸­ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€šç”¨è¯„ä»·æ³•å®˜æ¨¡å‹â€”â€”å¤šæ¨¡æ€æ¨ç†æ³•å®˜ï¼ˆMR. Judgeï¼‰ã€‚è¯¥æ¨¡å‹å°†è¯„ä»·è¿‡ç¨‹å½¢å¼åŒ–ä¸ºä¸€ä¸ªåŸºäºæ¨ç†çš„å¤šé€‰é—®é¢˜ï¼Œæé«˜äº†è¯„ä¼°çš„å¯é æ€§å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³ç¼ºå°‘å¸¦æœ‰å¾—åˆ†çš„å“åº”é—®é¢˜ï¼Œæå‡ºé€šè¿‡åˆæˆé€†å‘å“åº”å€™é€‰å’ŒåŸºäºæ–‡æœ¬çš„æ¨ç†æå–ç­–ç•¥å®ç°è‡ªåŠ¨æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMR. Judgeåœ¨ä¸åŒä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆæœï¼Œå¦‚MR. Judge-7Båœ¨VL-RewardBenchä¸Šçš„æ€§èƒ½ä¼˜äºGPT-4oè¾¾9.9%ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´å°ºåº¦ä¸Šåœ¨MM-Vetä¸Šçš„æ€§èƒ½æå‡é«˜è¾¾7.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè¯„ä»·æ³•å®˜å·²æˆä¸ºæœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æå‡ºå¤šæ¨¡æ€æ¨ç†æ³•å®˜ï¼ˆMR. Judgeï¼‰æ¨¡å‹ï¼Œå°†è¯„ä»·è¿‡ç¨‹å½¢å¼åŒ–ä¸ºåŸºäºæ¨ç†çš„å¤šé€‰é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆæˆé€†å‘å“åº”å€™é€‰å’ŒåŸºäºæ–‡æœ¬çš„æ¨ç†æå–ç­–ç•¥å®ç°è‡ªåŠ¨æ ‡æ³¨ã€‚</li>
<li>MR. Judgeæ¨¡å‹æé«˜äº†è¯„ä¼°çš„å¯é æ€§å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>MR. Judgeåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨VL-RewardBenchå’ŒMM-Vetä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>MR. Judgeæ¨¡å‹é€šè¿‡ç²¾ç»†è®¾è®¡çš„åˆæˆæ•°æ®ç®¡é“å®ç°å¤æ‚æ¨ç†èƒ½åŠ›çš„æç‚¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1809fb6603c738bee2dc5c105dec5e04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e36a0cfef9d9b2ae0fc9bdfd994aa76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19169a84e543cd10a04e8de471ccc9b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f422a2127e996d925673863d8a00ebad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CompeteSMoE-â€“-Statistically-Guaranteed-Mixture-of-Experts-Training-via-Competition"><a href="#CompeteSMoE-â€“-Statistically-Guaranteed-Mixture-of-Experts-Training-via-Competition" class="headerlink" title="CompeteSMoE â€“ Statistically Guaranteed Mixture of Experts Training via   Competition"></a>CompeteSMoE â€“ Statistically Guaranteed Mixture of Experts Training via   Competition</h2><p><strong>Authors:Nam V. Nguyen, Huy Nguyen, Quang Pham, Van Nguyen, Savitha Ramasamy, Nhat Ho</strong></p>
<p>Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the networkâ€™s depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: <a target="_blank" rel="noopener" href="https://github.com/Fsoft-AIC/CompeteSMoE">https://github.com/Fsoft-AIC/CompeteSMoE</a>. This work is an improved version of the previous study at arXiv:2402.02526 </p>
<blockquote>
<p>ç¨€ç–æ··åˆä¸“å®¶ç½‘ç»œï¼ˆSMoEï¼‰æä¾›äº†ä¸€ç§å¸å¼•äººçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æ‰©å¤§æ¨¡å‹å¤æ‚åº¦ï¼Œè€Œä¸ä»…ä»…æ˜¯å¢åŠ ç½‘ç»œçš„æ·±åº¦æˆ–å®½åº¦ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰æ•ˆçš„SMoEè®­ç»ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¬¡ä¼˜çš„è·¯ç”±è¿‡ç¨‹å¹¶ä¸èƒ½ä½¿æ‰§è¡Œè®¡ç®—çš„ä¸“å®¶ç›´æ¥è´¡çŒ®äºè·¯ç”±è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç«äº‰æœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§å°†ä»¤ç‰Œè·¯ç”±åˆ°å…·æœ‰æœ€é«˜ç¥ç»å“åº”çš„ä¸“å®¶çš„æ–°æœºåˆ¶ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†ç«äº‰æœºåˆ¶çš„æ ·æœ¬æ•ˆç‡é«˜äºä¼ ç»Ÿçš„softmaxè·¯ç”±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†CompeteSMoEï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç®—æ³•ï¼Œé€šè¿‡éƒ¨ç½²è·¯ç”±å™¨æ¥å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œä»è€Œä»¥è¾ƒä½çš„è®­ç»ƒå¼€é”€äº«å—å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸Šçš„å¹¿æ³›å®è¯è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°çš„SMoEç­–ç•¥ç›¸æ¯”ï¼ŒCompeteSMoEå…·æœ‰é«˜æ•ˆæ€§ã€ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Fsoft-AIC/CompeteSMoE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E6%98%AFarXiv:2402.02526%E4%B9%8B%E5%89%8D%E7%A0%94%E7%A9%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E6%94%B9%E8%BF%9B%E7%89%88%E6%9C%AC%E3%80%82">https://github.com/Fsoft-AIC/CompeteSMoEä¸Šè·å¾—ã€‚è¿™é¡¹å·¥ä½œæ˜¯arXiv:2402.02526ä¹‹å‰ç ”ç©¶çš„ä¸€ä¸ªæ”¹è¿›ç‰ˆæœ¬ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13380v1">PDF</a> 52 pages. This work is an improved version of the previous study at   arXiv:2402.02526</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Sparse Mixture of Experts (SMoE)æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ç«äº‰æœºåˆ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç«äº‰æœºåˆ¶é€šè¿‡å°†ä»¤ç‰Œè·¯ç”±åˆ°å“åº”æœ€é«˜çš„ä¸“å®¶æ¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼€å‘äº†ä¸€ç§åä¸ºCompeteSMoEçš„æ–°ç®—æ³•ï¼Œé€šè¿‡éƒ¨ç½²è·¯ç”±å™¨æ¥å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œå®ç°äº†å¼ºå¤§çš„æ€§èƒ½ä¸”è®­ç»ƒå¼€é”€è¾ƒä½ã€‚æ–‡ç« åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸Šçš„å¤§é‡å®è¯è¯„ä¼°è¡¨æ˜ï¼Œç›¸è¾ƒäºæœ€æ–°SMoEç­–ç•¥ï¼ŒCompeteSMoEæ›´æœ‰æ•ˆã€ç¨³å¥ä¸”å¯æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMoEæ¨¡å‹é¢ä¸´ä¸“å®¶è·¯ç”±è¿‡ç¨‹çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­è¿›è¡Œè®¡ç®—çš„ä¸“å®¶å¹¶æœªç›´æ¥å¯¹è·¯ç”±è¿‡ç¨‹åšå‡ºè´¡çŒ®ã€‚</li>
<li>ç«äº‰æœºåˆ¶è¢«æå‡ºä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œè¯¥æœºåˆ¶èƒ½å°†ä»¤ç‰Œè·¯ç”±åˆ°å“åº”æœ€é«˜çš„ä¸“å®¶ã€‚</li>
<li>ç«äº‰æœºåˆ¶çš„ç†è®ºä¼˜åŠ¿åœ¨äºå…¶æ ·æœ¬æ•ˆç‡é«˜äºä¼ ç»Ÿçš„softmaxè·¯ç”±ã€‚</li>
<li>CompeteSMoEç®—æ³•ç®€å•æœ‰æ•ˆï¼Œé€šè¿‡éƒ¨ç½²è·¯ç”±å™¨å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œå®ç°é«˜æ€§èƒ½å’Œä½è®­ç»ƒå¼€é”€ã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºCompeteSMoEåœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°SMoEç­–ç•¥ã€‚</li>
<li>è¯¥ç ”ç©¶æ˜¯å¯¹arXivä¸Šä¹‹å‰ç ”ç©¶çš„ä¸€ç§æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0658908f77581756a2b8f5d0b2ca77f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-069d6d43404dd591aafe6ad3cfd962d0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Thinkless-LLM-Learns-When-to-Think"><a href="#Thinkless-LLM-Learns-When-to-Think" class="headerlink" title="Thinkless: LLM Learns When to Think"></a>Thinkless: LLM Learns When to Think</h2><p><strong>Authors:Gongfan Fang, Xinyin Ma, Xinchao Wang</strong></p>
<p>Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the modelâ€™s ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/VainF/Thinkless">https://github.com/VainF/Thinkless</a> </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹å…·å¤‡æ‰©å±•çš„é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ï¼Œåœ¨éœ€è¦å¤æ‚é€»è¾‘æ¨æ–­çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹æ‰€æœ‰æŸ¥è¯¢è¿›è¡Œç²¾ç»†æ¨ç†é€šå¸¸ä¼šå¯¼è‡´å¤§é‡çš„è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œç‰¹åˆ«æ˜¯å½“è®¸å¤šé—®é¢˜éƒ½æœ‰ç®€å•è§£å†³æ–¹æ¡ˆæ—¶ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå¼€æ”¾æ€§çš„é—®é¢˜ï¼šLLMèƒ½å¦å­¦ä¼šä½•æ—¶æ€è€ƒï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Thinklessï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿèµ‹äºˆLLMæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”é€‰æ‹©çŸ­å½¢å¼å’Œé•¿å½¢å¼æ¨ç†ã€‚Thinklessæ˜¯åœ¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼ä¸‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå¹¶é‡‡ç”¨ä¸¤ä¸ªæ§åˆ¶ä»¤ç‰Œï¼Œ<short>ç”¨äºç®€æ´çš„å›ç­”å’Œ<think>ç”¨äºè¯¦ç»†çš„æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯è§£è€¦ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDeGRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†æ··åˆæ¨ç†çš„å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ§åˆ¶ä»¤ç‰ŒæŸå¤±ï¼Œå®ƒæ§åˆ¶æ¨ç†æ¨¡å¼çš„é€‰æ‹©ï¼›ï¼ˆ2ï¼‰å“åº”æŸå¤±ï¼Œå®ƒæé«˜ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚è¿™ç§è§£è€¦çš„å…¬å¼åŒ–è¡¨è¾¾èƒ½å¤Ÿç²¾ç»†æ§åˆ¶æ¯ä¸ªç›®æ ‡çš„è´¡çŒ®ï¼Œç¨³å®šè®­ç»ƒï¼Œå¹¶æœ‰æ•ˆé˜²æ­¢äº†åŸå§‹GRPOä¸­è§‚å¯Ÿåˆ°çš„å´©æºƒã€‚åœ¨Minerva Algebraã€MATH-500å’ŒGSM8Kç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒThinklessèƒ½å¤Ÿå‡å°‘é•¿é“¾æ€è€ƒçš„ä½¿ç”¨ç‡50%~90%ï¼Œæ˜¾è‘—æé«˜æ¨ç†è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/VainF/Thinkless%E3%80%82">https://github.com/VainF/Thinklessã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Thinklessæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©ç®€æ´å›ç­”å’Œè¯¦ç»†æ¨ç†çš„æ¨¡å¼ï¼ŒåŸºäºä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹èƒ½åŠ›ã€‚Thinklessé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼è®­ç»ƒï¼Œä½¿ç”¨ä¸¤ä¸ªæ§åˆ¶ç¬¦å·ï¼š<short>ç”¨äºç®€æ´å›ç­”ï¼Œ<think>ç”¨äºè¯¦ç»†æ¨ç†ã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯Decoupled Group Relative Policy Optimizationï¼ˆDeGRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†æ··åˆæ¨ç†çš„å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šæ§åˆ¶ç¬¦å·æŸå¤±å’Œå“åº”æŸå¤±ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒThinklessèƒ½å¤Ÿå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿é“¾æ€è€ƒä½¿ç”¨ï¼Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Thinklessæ¡†æ¶è®©LLMèƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©ç®€æ´å›ç­”å’Œè¯¦ç»†æ¨ç†çš„æ¨¡å¼ã€‚</li>
<li>Thinklessé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼è®­ç»ƒï¼Œä½¿ç”¨æ§åˆ¶ç¬¦å·æ¥æ§åˆ¶æ¨ç†æ¨¡å¼ã€‚</li>
<li>Decoupled Group Relative Policy Optimizationï¼ˆDeGRPOï¼‰ç®—æ³•æ˜¯Thinklessçš„æ ¸å¿ƒæ–¹æ³•ï¼Œå®ƒå°†æ··åˆæ¨ç†çš„å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚</li>
<li>æ§åˆ¶ç¬¦å·æŸå¤±è´Ÿè´£é€‰æ‹©æ¨ç†æ¨¡å¼ï¼Œå“åº”æŸå¤±åˆ™æé«˜ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>Thinklessèƒ½å¤Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å°‘LLMçš„é•¿é“¾æ€è€ƒä½¿ç”¨ã€‚</li>
<li>Thinklessæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33aa4bcf4653260e5a0e7235af8c9221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35c050fb0ce9a3ab8e6166d621f4af7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608781e2d2769d48b179ad8b99ab3c63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-604da8dfe31b0c8525b7ab0caf713e98.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Sense-and-Sensitivity-Examining-the-Influence-of-Semantic-Recall-on-Long-Context-Code-Reasoning"><a href="#Sense-and-Sensitivity-Examining-the-Influence-of-Semantic-Recall-on-Long-Context-Code-Reasoning" class="headerlink" title="Sense and Sensitivity: Examining the Influence of Semantic Recall on   Long Context Code Reasoning"></a>Sense and Sensitivity: Examining the Influence of Semantic Recall on   Long Context Code Reasoning</h2><p><strong>Authors:Adam Å torek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</strong></p>
<p>Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information. </p>
<blockquote>
<p>å°½ç®¡ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒæå¤§çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œä½†å®ƒä»¬åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡è¿›è¡Œä»£ç æ¨ç†çš„æœ‰æ•ˆæ€§ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡è°ƒæŸ¥äº†LLMåœ¨å¤§ä»“åº“ä¸­çš„ä»£ç ç‰‡æ®µæ¨ç†èƒ½åŠ›åŠå…¶ä¸å›å¿†èƒ½åŠ›çš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŒºåˆ†äº†è¯æ±‡ä»£ç å›å¿†ï¼ˆé€å­—æ£€ç´¢ï¼‰å’Œè¯­ä¹‰ä»£ç å›å¿†ï¼ˆè®°ä½ä»£ç çš„åŠŸèƒ½ï¼‰ã€‚ä¸ºäº†è¡¡é‡è¯­ä¹‰å›å¿†ï¼Œæˆ‘ä»¬æå‡ºäº†SemTraceï¼Œè¿™æ˜¯ä¸€ç§ä»£ç æ¨ç†æŠ€æœ¯ï¼Œå¯ä»¥è¿½æº¯ç‰¹å®šè¯­å¥å¯¹è¾“å‡ºçš„å½±å“ï¼Œä¸”å…·æœ‰ä¸å¯é¢„æµ‹æ€§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­é‡åŒ–è¯­ä¹‰å›å¿†æ•æ„Ÿåº¦çš„æ–¹æ³•ã€‚å¯¹æœ€æ–°LLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“ä»£ç ç‰‡æ®µæ¥è¿‘è¾“å…¥ä¸Šä¸‹æ–‡çš„ä¸­éƒ¨æ—¶ï¼Œç‰¹åˆ«æ˜¯éœ€è¦ä½¿ç”¨é«˜è¯­ä¹‰å›å¿†çš„æŠ€æœ¯ï¼ˆå¦‚SemTraceï¼‰ï¼Œä»£ç æ¨ç†çš„å‡†ç¡®æ€§ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¯æ±‡å›å¿†çš„ç²’åº¦ä¸åŒï¼Œæ¨¡å‹åœ¨å‡½æ•°æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€è¡Œå›å¿†æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯æ±‡å›å¿†å’Œè¯­ä¹‰å›å¿†ä¹‹é—´å­˜åœ¨æ–­å±‚ï¼Œè¿™è¡¨æ˜äº†ä¸åŒçš„æ½œåœ¨æœºåˆ¶ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•å¯èƒ½è¡¨ç°å‡ºè¾ƒä½çš„è¯­ä¹‰å›å¿†æ•æ„Ÿæ€§ï¼Œå¯èƒ½ä¼šä½ä¼°LLMåœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13353v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç‰‡æ®µæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŠå…¶ä¸ä»£ç å¬å›èƒ½åŠ›çš„å…³è”ã€‚ç ”ç©¶åŒºåˆ†äº†è¯æ±‡ä»£ç å¬å›ï¼ˆé€å­—æ£€ç´¢ï¼‰å’Œè¯­ä¹‰ä»£ç å¬å›ï¼ˆè®°ä½ä»£ç çš„åŠŸèƒ½ï¼‰ã€‚ä¸ºè¡¡é‡è¯­ä¹‰å¬å›èƒ½åŠ›ï¼Œæå‡ºäº†SemTraceä»£ç æ¨ç†æŠ€æœ¯ï¼Œå¹¶ä»‹ç»äº†åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­é‡åŒ–è¯­ä¹‰å¬å›æ•æ„Ÿåº¦çš„æ–¹æ³•ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œéšç€ä»£ç ç‰‡æ®µä½äºè¾“å…¥ä¸Šä¸‹æ–‡ä¸­çš„ä½ç½®é€æ¸å±…ä¸­ï¼Œç‰¹åˆ«æ˜¯éœ€è¦é«˜åº¦è¯­ä¹‰å¬å›çš„æŠ€æœ¯å¦‚SemTraceï¼Œä»£ç æ¨ç†çš„å‡†ç¡®æ€§ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°è¯æ±‡å¬å›èƒ½åŠ›å› ç²’åº¦è€Œå¼‚ï¼Œæ¨¡å‹æ“…é•¿å‡½æ•°æ£€ç´¢ï¼Œä½†åœ¨é€è¡Œå¬å›æ–¹é¢é‡åˆ°å›°éš¾ã€‚é‡è¦çš„æ˜¯ï¼Œè¯æ±‡å¬å›å’Œè¯­ä¹‰å¬å›ä¹‹é—´å­˜åœ¨æ–­å±‚ï¼Œè¯´æ˜å­˜åœ¨ä¸åŒçš„å†…åœ¨æœºåˆ¶ã€‚æœ€åï¼Œç ”ç©¶è¡¨æ˜ç°æœ‰çš„ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•å¯èƒ½è¡¨ç°å‡ºè¾ƒä½çš„è¯­ä¹‰å¬å›æ•æ„Ÿæ€§ï¼Œå¯èƒ½ä¼šä½ä¼°LLMåœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä»£ç ç‰‡æ®µæ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šä¸æ¸…æ¥šã€‚</li>
<li>åŒºåˆ†äº†è¯æ±‡ä»£ç å¬å›å’Œè¯­ä¹‰ä»£ç å¬å›ã€‚</li>
<li>æå‡ºäº†SemTraceæŠ€æœ¯æ¥è¡¡é‡è¯­ä¹‰å¬å›èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨ä»£ç æ¨ç†æ–¹é¢å­˜åœ¨å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜è¯­ä¹‰å¬å›çš„æŠ€æœ¯ä¸Šã€‚</li>
<li>è¯æ±‡å¬å›èƒ½åŠ›å› ç²’åº¦è€Œå¼‚ï¼Œæ¨¡å‹åœ¨å‡½æ•°æ£€ç´¢æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨é€è¡Œå¬å›æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>è¯æ±‡å¬å›å’Œè¯­ä¹‰å¬å›ä¹‹é—´å­˜åœ¨æ–­å±‚ï¼Œè¡¨æ˜å®ƒä»¬æœ‰ä¸åŒçš„å†…åœ¨æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d8350a705151525f12bec85cbdffcb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c51a82f8b477aa0475d656a8c1ac0fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-990fb901fd5afe98db09ada9a957fd2b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Preference-Optimization"><a href="#J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Preference-Optimization" class="headerlink" title="J4R: Learning to Judge with Equivalent Initial State Group Relative   Preference Optimization"></a>J4R: Learning to Judge with Equivalent Initial State Group Relative   Preference Optimization</h2><p><strong>Authors:Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</strong></p>
<p>To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•æ­¥ä¼çš„åŠ å¿«ï¼Œæ¨¡å‹è¾“å‡ºè¯„ä¼°å·²ç»ä»è€—æ—¶çš„äººç±»è¯„ä¼°è½¬å˜ä¸ºè‡ªåŠ¨è¯„ä¼°ã€‚åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æœ¬èº«è¢«ç”¨æ¥è¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹çš„è¾“å‡ºã€‚LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆè¯„ä¼°å™¨ï¼Œæ“…é•¿è¯„ä¼°ç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼Œå¦‚èŠå¤©è´¨é‡ï¼Œä½†åœ¨éœ€è¦æ¨ç†çš„å¤æ‚é¢†åŸŸä¸­è¡¨ç°æŒ£æ‰ï¼Œè¿™äº›é¢†åŸŸçš„æ¨¡å‹å›åº”åŒ…å«æ›´å¤šå®è´¨æ€§çš„æŒ‘æˆ˜å†…å®¹ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰è¯„åˆ¤è€…çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒè¯„åˆ¤è€…ã€‚æˆ‘ä»¬åšå‡ºäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†ç­‰ä»·åˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒè¯„åˆ¤è€…å¯¹æ›´å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­å‡ºç°çš„å®šä½åå·®å…·æœ‰é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ReasoningJudgeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°åœ¨ä»¥å‰çš„å·¥ä½œä¸­æ²¡æœ‰æ¶µç›–çš„å„ç§æ¨ç†ç¯å¢ƒä¸­çš„è¯„åˆ¤è€…ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬ä½¿ç”¨EIS-GRPOè®­ç»ƒäº†J4Rï¼ˆæ¨ç†è¯„åˆ¤è€…ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ª7Bçš„è¯„åˆ¤è€…ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†GPT-4oå’Œä¸‹ä¸€ä¸ªæœ€ä½³å°å‹è¯„åˆ¤è€…6.7%å’Œ9%ï¼Œåœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ä¸æ›´å¤§çš„GRPOè®­ç»ƒè¿‡çš„è¯„åˆ¤è€…ç›¸åŒ¹é…æˆ–æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13346v1">PDF</a> 25 pages, 4 figures, 6 tables. To be updated with links for   code&#x2F;benchmark</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°çš„è½¬å˜ï¼Œä»è€—æ—¶çš„äººåŠ›è¯„ä¼°é€æ¸è½¬å‘è‡ªåŠ¨è¯„ä¼°ã€‚ç°åœ¨ï¼ŒLLMè‡ªèº«è¢«ç”¨äºè¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹çš„è¾“å‡ºã€‚æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè¯„ä»·æ¨¡å‹çš„æ–¹æ¡ˆï¼Œæå‡ºä¸€ç§åä¸ºEIS-GRPOçš„æ–°ç®—æ³•ï¼Œå¢å¼ºäº†è¯„ä»·æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªåä¸ºReasoningJudgeBenchçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è¯„ä»·æ¨¡å‹åœ¨ä¸åŒæ¨ç†åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æœ€ç»ˆï¼Œé€šè¿‡EIS-GRPOè®­ç»ƒçš„J4Ræ¨¡å‹åœ¨ReasoningJudgeBenchä¸Šçš„è¡¨ç°è¶…è¿‡äº†GPT-4oå’Œå…¶ä»–å°å‹è¯„ä»·æ¨¡å‹ï¼Œç”šè‡³ä¸å¤§å‹GRPOè®­ç»ƒçš„è¯„ä»·æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°çš„è½¬å˜ï¼Œç°åœ¨æ›´å¤šåœ°ä¾èµ–äºè‡ªåŠ¨è¯„ä¼°å’ŒLLMè‡ªèº«çš„è¯„ä»·ã€‚</li>
<li>LLM-as-judgeæ¨¡å‹åœ¨è¯„ä¼°ç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼ˆå¦‚èŠå¤©è´¨é‡ï¼‰è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦å¤§é‡æ¨ç†çš„å¤æ‚é¢†åŸŸè¡¨ç°æ¬ ä½³ã€‚</li>
<li>ä¸ºäº†æ”¹è¿›ç°æœ‰è¯„ä»·æ¨¡å‹çš„ä¸è¶³ï¼Œç ”ç©¶è€…é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•EIS-GRPOï¼Œå¢å¼ºäº†è¯„ä»·æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ReasoningJudgeBenchï¼Œç”¨äºè¯„ä¼°è¯„ä»·æ¨¡å‹åœ¨ä¸åŒæ¨ç†åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡EIS-GRPOè®­ç»ƒçš„J4Ræ¨¡å‹åœ¨ReasoningJudgeBenchä¸Šçš„è¡¨ç°è¶…è¿‡äº†GPT-4oå’Œå…¶ä»–å°å‹è¯„ä»·æ¨¡å‹ã€‚</li>
<li>J4Ræ¨¡å‹çš„è¡¨ç°ä¸å¤§å‹GRPOè®­ç»ƒçš„è¯„ä»·æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c206c38f5dddb3041de5540b90c0f44b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93315927c8300339a387f090d719d795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10c1657f1be23f37719d032298207ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99c75154d32ee65f20210787204ec610.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JNLP-at-SemEval-2025-Task-11-Cross-Lingual-Multi-Label-Emotion-Detection-Using-Generative-Models"><a href="#JNLP-at-SemEval-2025-Task-11-Cross-Lingual-Multi-Label-Emotion-Detection-Using-Generative-Models" class="headerlink" title="JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion   Detection Using Generative Models"></a>JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion   Detection Using Generative Models</h2><p><strong>Authors:Jieying Xue, Phuong Minh Nguyen, Minh Le Nguyen, Xin Liu</strong></p>
<p>With the rapid advancement of global digitalization, users from different countries increasingly rely on social media for information exchange. In this context, multilingual multi-label emotion detection has emerged as a critical research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task: (1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity. To tackle multilingual challenges, we leverage pre-trained multilingual models and focus on two architectures: (1) a fine-tuned BERT-based classification model and (2) an instruction-tuned generative LLM. Additionally, we propose two methods for handling multi-label classification: the base method, which maps an input directly to all its corresponding emotion labels, and the pairwise method, which models the relationship between the input text and each emotion category individually. Experimental results demonstrate the strong generalization ability of our approach in multilingual emotion recognition. In Track A, our method achieved Top 4 performance across 10 languages, ranking 1st in Hindi. In Track B, our approach also secured Top 5 performance in 7 languages, highlighting its simplicity and effectiveness\footnote{Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yingjie7/mlingual_multilabel_emo_detection">https://github.com/yingjie7/mlingual_multilabel_emo_detection</a>. </p>
<blockquote>
<p>éšç€å…¨çƒæ•°å­—åŒ–çš„å¿«é€Ÿå‘å±•ï¼Œæ¥è‡ªä¸åŒå›½å®¶çš„ç”¨æˆ·è¶Šæ¥è¶Šä¾èµ–ç¤¾äº¤åª’ä½“è¿›è¡Œä¿¡æ¯äº¤æµã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå¤šè¯­è¨€å¤šæ ‡ç­¾æƒ…æ„Ÿæ£€æµ‹å·²æˆä¸ºä¸€ä¸ªå…³é”®ç ”ç©¶é¢†åŸŸã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³SemEval-2025ä»»åŠ¡11ï¼šæ–‡æœ¬æƒ…æ„Ÿæ£€æµ‹ä¸­çš„é¸¿æ²Ÿé—®é¢˜ã€‚æˆ‘ä»¬çš„è®ºæ–‡é‡ç‚¹å…³æ³¨è¯¥ä»»åŠ¡ä¸­çš„ä¸¤ä¸ªå­é¢†åŸŸï¼šï¼ˆ1ï¼‰èµ›é“Aï¼šå¤šæ ‡ç­¾æƒ…æ„Ÿæ£€æµ‹ï¼›ï¼ˆ2ï¼‰èµ›é“Bï¼šæƒ…æ„Ÿå¼ºåº¦ã€‚ä¸ºäº†åº”å¯¹å¤šè¯­è¨€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸“æ³¨äºä¸¤ç§æ¶æ„ï¼šï¼ˆ1ï¼‰å¾®è°ƒåçš„åŸºäºBERTçš„åˆ†ç±»æ¨¡å‹ï¼›ï¼ˆ2ï¼‰ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¤„ç†å¤šæ ‡ç­¾åˆ†ç±»çš„æ–¹æ³•ï¼šåŸºç¡€æ–¹æ³•ç›´æ¥å°†è¾“å…¥æ˜ å°„åˆ°å…¶å¯¹åº”çš„æ‰€æœ‰æƒ…æ„Ÿæ ‡ç­¾ä¸Šï¼›é…å¯¹æ–¹æ³•åˆ™åˆ†åˆ«å»ºæ¨¡è¾“å…¥æ–‡æœ¬ä¸æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¯­è¨€çš„æƒ…æ„Ÿè¯†åˆ«ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨èµ›é“Aä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨10ç§è¯­è¨€ä¸­å–å¾—äº†å‰4åçš„æˆç»©ï¼Œå¹¶åœ¨å°åœ°è¯­ä¸­æ’åç¬¬1ã€‚åœ¨èµ›é“Bä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿåœ¨7ç§è¯­è¨€ä¸­å–å¾—äº†å‰5åçš„æˆç»©ï¼Œçªæ˜¾äº†å…¶ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ã€æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yingjie7/mlingual_multilabel_emo_detection%E6%89%BE%E5%88%B0%E3%80%91%E3%80%82">https://github.com/yingjie7/mlingual_multilabel_emo_detectionæ‰¾åˆ°ã€‘ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13244v1">PDF</a> Published in The 19th International Workshop on Semantic Evaluation   (SemEval-2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€å…¨çƒæ•°å­—åŒ–è¿›ç¨‹åŠ å¿«ï¼Œä¸åŒå›½å®¶ç”¨æˆ·æ—¥ç›Šä¾èµ–ç¤¾äº¤åª’ä½“è¿›è¡Œä¿¡æ¯äº¤æµï¼Œå¤šè¯­è¨€å¤šæ ‡ç­¾æƒ…æ„Ÿæ£€æµ‹æˆä¸ºé‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ¬ç ”ç©¶å…³æ³¨SemEval-2025 Task 11ï¼šæ–‡æœ¬æƒ…æ„Ÿæ£€æµ‹ä¸­çš„é¸¿æ²Ÿé—®é¢˜ã€‚é’ˆå¯¹è¯¥ä»»åŠ¡çš„ä¸¤ä¸ªå­è¯¾é¢˜ï¼šï¼ˆ1ï¼‰å¤šæ ‡ç­¾æƒ…æ„Ÿæ£€æµ‹å’Œï¼ˆ2ï¼‰æƒ…æ„Ÿå¼ºåº¦è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚ä¸ºåº”å¯¹å¤šè¯­è¨€æŒ‘æˆ˜ï¼Œç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ç§æ¶æ„ï¼Œå³å¾®è°ƒBERTåˆ†ç±»æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åŒæ—¶æå‡ºä¸¤ç§å¤„ç†å¤šæ ‡ç­¾åˆ†ç±»çš„æ–¹æ³•ï¼šåŸºç¡€æ–¹æ³•ç›´æ¥æ˜ å°„è¾“å…¥åˆ°å…¶å¯¹åº”çš„æƒ…æ„Ÿæ ‡ç­¾ï¼Œé…å¯¹æ–¹æ³•åˆ™ç‹¬ç«‹å»ºæ¨¡è¾“å…¥æ–‡æœ¬ä¸æ¯ç§æƒ…æ„Ÿç±»åˆ«ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šè¯­è¨€æƒ…æ„Ÿè¯†åˆ«ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨Aèµ›é“10ç§è¯­è¨€ä¸­ä½åˆ—ç¬¬4ï¼Œå¹¶åœ¨å°åº¦è¯­ä¸­æ’åç¬¬1ï¼›åœ¨Bèµ›é“7ç§è¯­è¨€ä¸­ä½åˆ—ç¬¬5ï¼Œå‡¸æ˜¾å…¶ç®€æ´æ€§å’Œæœ‰æ•ˆæ€§^[æ³¨ï¼šä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/yingjie7/mlingual_multilabel_emo_detection]%E3%80%82">https://github.com/yingjie7/mlingual_multilabel_emo_detection]ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒæ•°å­—åŒ–è¿›ç¨‹ä¸­ï¼Œç¤¾äº¤åª’ä½“æˆä¸ºä¿¡æ¯äº¤æµçš„å…³é”®æ¸ é“ï¼Œå¤šè¯­è¨€å¤šæ ‡ç­¾æƒ…æ„Ÿæ£€æµ‹æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>æœ¬ç ”ç©¶å…³æ³¨SemEval-2025 Task 11ä¸­çš„ä¸¤ä¸ªå­è¯¾é¢˜ï¼šå¤šæ ‡ç­¾æƒ…æ„Ÿæ£€æµ‹å’Œæƒ…æ„Ÿå¼ºåº¦ã€‚</li>
<li>ä¸ºåº”å¯¹å¤šè¯­è¨€æŒ‘æˆ˜ï¼Œç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ç§æ¶æ„ï¼šå¾®è°ƒBERTåˆ†ç±»æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æå‡ºä¸¤ç§å¤„ç†å¤šæ ‡ç­¾åˆ†ç±»çš„æ–¹æ³•ï¼šåŸºç¡€æ–¹æ³•å’Œé…å¯¹æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šè¯­è¨€æƒ…æ„Ÿè¯†åˆ«ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨ç‰¹å®šèµ›é“ä¸­å–å¾—ä¼˜å¼‚æˆç»©ï¼Œå¦‚Aèµ›é“å°åº¦è¯­æ’åç¬¬1ï¼ŒBèµ›é“7ç§è¯­è¨€ä¸­ä½åˆ—ç¬¬5ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1103a4e20d6934d949a1b818d06d3b19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea062cf4fe4c2fdd85ed4c0a8da0cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f78b67c18ff0fd0ed1cb79977430218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4ea68b68328ea8ae91177235864f54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c00e94753e5c0c297de4c7c5441472b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FLASH-Latent-Aware-Semi-Autoregressive-Speculative-Decoding-for-Multimodal-Tasks"><a href="#FLASH-Latent-Aware-Semi-Autoregressive-Speculative-Decoding-for-Multimodal-Tasks" class="headerlink" title="FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for   Multimodal Tasks"></a>FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for   Multimodal Tasks</h2><p><strong>Authors:Zihua Wang, Ruibo Li, Haozhe Du, Joey Tianyi Zhou, Yu Zhang, Xu Yang</strong></p>
<p>Large language and multimodal models (LLMs and LMMs) exhibit strong inference capabilities but are often limited by slow decoding speeds. This challenge is especially acute in LMMs, where visual inputs typically comprise more tokens with lower information density than text â€“ an issue exacerbated by recent trends toward finer-grained visual tokenizations to boost performance. Speculative decoding has been effective in accelerating LLM inference by using a smaller draft model to generate candidate tokens, which are then selectively verified by the target model, improving speed without sacrificing output quality. While this strategy has been extended to LMMs, existing methods largely overlook the unique properties of visual inputs and depend solely on text-based draft models. In this work, we propose \textbf{FLASH} (Fast Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework designed specifically for LMMs, which leverages two key properties of multimodal data to design the draft model. First, to address redundancy in visual tokens, we propose a lightweight latent-aware token compression mechanism. Second, recognizing that visual objects often co-occur within a scene, we employ a semi-autoregressive decoding strategy to generate multiple tokens per forward pass. These innovations accelerate draft decoding while maintaining high acceptance rates, resulting in faster overall inference. Experiments show that FLASH significantly outperforms prior speculative decoding approaches in both unimodal and multimodal settings, achieving up to \textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on visual instruction tuning tasks compared to the original LMM. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLLM å’Œ LMMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¾€å¾€å—åˆ°è§£ç é€Ÿåº¦è¾ƒæ…¢çš„é™åˆ¶ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨ LMM ä¸­å°¤ä¸ºçªå‡ºï¼Œå› ä¸ºè§†è§‰è¾“å…¥é€šå¸¸åŒ…å«æ¯”æ–‡æœ¬æ›´å¤šçš„æ ‡è®°ï¼Œä½†ä¿¡æ¯å¯†åº¦è¾ƒä½â€”â€”è¿™ä¸€é—®é¢˜å› è¿‘æœŸä¸ºæå‡æ€§èƒ½è€Œå‡ºç°çš„æ›´ç²¾ç»†çš„è§†è§‰æ ‡è®°åŒ–è¶‹åŠ¿è€ŒåŠ å‰§ã€‚æŠ•æœºè§£ç é€šè¿‡ä½¿ç”¨è¾ƒå°çš„è‰ç¨¿æ¨¡å‹ç”Ÿæˆå€™é€‰æ ‡è®°æ¥åŠ é€Ÿ LLM æ¨ç†ï¼Œç„¶åç›®æ ‡æ¨¡å‹ä¼šè¿›è¡Œé€‰æ‹©æ€§éªŒè¯ï¼Œä»è€Œåœ¨æé«˜é€Ÿåº¦çš„åŒæ—¶ä¸ç‰ºç‰²è¾“å‡ºè´¨é‡ã€‚è™½ç„¶è¿™ä¸€ç­–ç•¥å·²æ‰©å±•åˆ° LMMï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šå¿½ç•¥äº†è§†è§‰è¾“å…¥çš„ç‹¬ç‰¹å±æ€§ï¼Œä»…ä¾èµ–äºæ–‡æœ¬è‰ç¨¿æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸º FLASHï¼ˆå¿«é€Ÿæ½œåœ¨æ„ŸçŸ¥åŠè‡ªå›å½’å¯å‘å¼æ–¹æ³•ï¼‰çš„æŠ•æœºè§£ç æ¡†æ¶ï¼Œå®ƒæ˜¯ä¸“é—¨é’ˆå¯¹ LMM è®¾è®¡çš„ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®çš„ä¸¤ä¸ªå…³é”®å±æ€§æ¥æ„å»ºè‰ç¨¿æ¨¡å‹ã€‚é¦–å…ˆï¼Œä¸ºäº†è§£å†³è§†è§‰æ ‡è®°çš„å†—ä½™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ½œåœ¨æ„ŸçŸ¥æ ‡è®°å‹ç¼©æœºåˆ¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°è§†è§‰å¯¹è±¡é€šå¸¸ä¼šåœ¨åœºæ™¯ä¸­å…±ç°ï¼Œå› æ­¤é‡‡ç”¨åŠè‡ªå›å½’è§£ç ç­–ç•¥ï¼Œæ¯æ¬¡å‰å‘ä¼ é€’ç”Ÿæˆå¤šä¸ªæ ‡è®°ã€‚è¿™äº›åˆ›æ–°åŠ é€Ÿäº†è‰ç¨¿è§£ç ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ¥å—ç‡ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„æ•´ä½“æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å•æ¨¡æ€è¿˜æ˜¯å¤šæ¨¡æ€ç¯å¢ƒä¸­ï¼ŒFLASH éƒ½æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æŠ•æœºè§£ç æ–¹æ³•ã€‚åœ¨ä¸åŸå§‹ LMM çš„æ¯”è¾ƒä¸­ï¼ŒFLASH åœ¨è§†é¢‘æè¿°å’Œè§†è§‰æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾ 2.68 å€å’Œ 2.55 å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12728v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹ï¼ˆLLMså’ŒLMMsï¼‰çš„æ¨ç†é€Ÿåº¦é™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFLASHçš„æŠ•æœºè§£ç æ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹LMMsçš„ç‰¹æ€§è¿›è¡Œè®¾è®¡ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰æ•°æ®çš„å†—ä½™æ€§å’Œå¯¹è±¡å…±ç°æ€§ï¼Œå®ç°äº†å¿«é€Ÿè§£ç ã€‚å®éªŒè¡¨æ˜ï¼ŒFLASHåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºä¼ ç»ŸæŠ•æœºè§£ç æ–¹æ³•ï¼Œè§†é¢‘æè¿°å’Œè§†è§‰æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡çš„é€Ÿåº¦æå‡æœ€é«˜å¯è¾¾2.68å€å’Œ2.55å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’ŒLMMså…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†è§£ç é€Ÿåº¦è¾ƒæ…¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰è¾“å…¥æ—¶æ›´ä¸ºæ˜æ˜¾ã€‚</li>
<li>ç°æœ‰æŠ•æœºè§£ç æ–¹æ³•ä¸»è¦é¢å‘LLMsï¼Œå¿½ç•¥äº†è§†è§‰è¾“å…¥çš„ç‹¬ç‰¹æ€§ã€‚</li>
<li>FLASHæ¡†æ¶é’ˆå¯¹LMMsè®¾è®¡ï¼Œåˆ©ç”¨è§†è§‰æ•°æ®çš„å†—ä½™æ€§å’Œå¯¹è±¡å…±ç°æ€§æ¥åŠ é€Ÿè§£ç ã€‚</li>
<li>FLASHé‡‡ç”¨è½»é‡çº§çš„æ½œåœ¨æ„ŸçŸ¥ä»¤ç‰Œå‹ç¼©æœºåˆ¶å’ŒåŠè‡ªåŠ¨é€’å½’è§£ç ç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFLASHåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€åœºæ™¯ä¸‹å‡æ˜¾è‘—æé«˜äº†è§£ç é€Ÿåº¦ã€‚</li>
<li>FLASHåœ¨è§†é¢‘æè¿°å’Œè§†è§‰æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ä¸­çš„é€Ÿåº¦æå‡æœ€é«˜å¯è¾¾2.68å€å’Œ2.55å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d854458d913abb813efecb4f2562fdcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc6281967c26711d3e845409d935d024.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e91764fab59dc4f5bd237bd597e63970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f9491fe52a15a8252f32211d376a17e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Shadow-FT-Tuning-Instruct-via-Base"><a href="#Shadow-FT-Tuning-Instruct-via-Base" class="headerlink" title="Shadow-FT: Tuning Instruct via Base"></a>Shadow-FT: Tuning Instruct via Base</h2><p><strong>Authors:Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</strong></p>
<p>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%7D%7BGithub%7D">https://github.com/wutaiqiang/Shadow-FT}{Github}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šé€šè¿‡è¿›ä¸€æ­¥çš„å¾®è°ƒæŒç»­å—ç›Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç›´æ¥è°ƒæ•´INSTRUCTï¼ˆå³æŒ‡ä»¤è°ƒæ•´ï¼‰æ¨¡å‹é€šå¸¸åªä¼šå¸¦æ¥å¾®å°çš„æ”¹è¿›ï¼Œç”šè‡³ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›INSTRUCTå˜ä½“çš„åŸºç¡€é…å¥—BASEæ¨¡å‹åŒ…å«é«˜åº¦ç›¸ä¼¼çš„æƒé‡å€¼ï¼ˆä¾‹å¦‚Llama 3.1  8Bçš„å¹³å‡å€¼å°äº2%ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Shadow-FTæ¡†æ¶ï¼Œåˆ©ç”¨ç›¸åº”çš„BASEæ¨¡å‹æ¥è°ƒæ•´INSTRUCTæ¨¡å‹ã€‚å…³é”®æ€è·¯æ˜¯å¾®è°ƒBASEæ¨¡å‹ï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥ç§»æ¤åˆ°INSTRUCTæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºçš„Shadow-FTæ— éœ€æ·»åŠ é¢å¤–çš„å‚æ•°ï¼Œæ˜“äºå®ç°ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸»æµçš„LLMä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¦‚Qwen 3å’ŒLlama 3ç³»åˆ—ï¼Œå¹¶åœ¨æ¶µç›–ç¼–ç ã€æ¨ç†å’Œæ•°å­¦ä»»åŠ¡çš„19ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShadow-FTæŒç»­ä¼˜äºä¼ ç»Ÿçš„å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆçš„è°ƒæ•´æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒShadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¹¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç›¸ç»“åˆã€‚ç›¸å…³ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT">Github</a>ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12716v1">PDF</a> Under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹é€šè¿‡è¿›ä¸€æ­¥çš„å¾®è°ƒåœ¨å„ç§ä»»åŠ¡ä¸ŠæŒç»­å—ç›Šï¼Œä½†ç›´æ¥å¯¹INSTRUCTæ¨¡å‹è¿›è¡Œå¾®è°ƒå¾€å¾€å¯¼è‡´æ€§èƒ½æå‡æœ‰é™ç”šè‡³å‡ºç°é€€åŒ–ã€‚æœ¬æ–‡æå‡ºäº†Shadow-FTæ¡†æ¶ï¼Œåˆ©ç”¨ç›¸åº”çš„BASEæ¨¡å‹å¯¹INSTRUCTæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚Shadow-FTçš„æ ¸å¿ƒæ€æƒ³æ˜¯å…ˆå¯¹BASEæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥åº”ç”¨åˆ°INSTRUCTæ¨¡å‹ä¸Šã€‚è¯¥æ–¹æ³•æ— éœ€æ·»åŠ é¢å¤–çš„å‚æ•°ï¼Œæ˜“äºå®ç°ï¼Œä¸”èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShadow-FTåœ¨ä¸»æµLLMä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆè°ƒæ•´æ–¹æ³•ã€‚åŒæ—¶ï¼ŒShadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç»“åˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›´æ¥å¾®è°ƒINSTRUCTæ¨¡å‹å¸¸å¸¸å¯¼è‡´æ€§èƒ½æå‡æœ‰é™æˆ–æ€§èƒ½é€€åŒ–ã€‚</li>
<li>æå‡ºçš„Shadow-FTæ¡†æ¶åˆ©ç”¨ç›¸åº”çš„BASEæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>Shadow-FTé€šè¿‡å¯¹BASEæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°åº”ç”¨åˆ°INSTRUCTæ¨¡å‹ä¸Šã€‚</li>
<li>Shadow-FTæ–¹æ³•æ— éœ€é¢å¤–çš„å‚æ•°ï¼Œæ˜“äºå®ç°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒShadow-FTåœ¨ä¸»æµLLMä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>Shadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>Shadow-FTå¯ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç»“åˆä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17fb9242662645175e05b7135628e9df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db1cd1fb6974c657fb8c9fe5a97dbd7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a6a60633b13a1a8a5fe06c4d09ce7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c483fe6239f6a33b6b24849362c7879c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3578d59fe55ea0c10fb3aafa78028437.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Latent-Computation-in-Transformers-with-Latent-Tokens"><a href="#Enhancing-Latent-Computation-in-Transformers-with-Latent-Tokens" class="headerlink" title="Enhancing Latent Computation in Transformers with Latent Tokens"></a>Enhancing Latent Computation in Transformers with Latent Tokens</h2><p><strong>Authors:Yuchang Sun, Yanxi Chen, Yaliang Li, Bolin Ding</strong></p>
<p>Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs. </p>
<blockquote>
<p>é€šè¿‡è¾…åŠ©æ ‡è®°å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºæé«˜æ¨¡å‹æ€§èƒ½çš„ä¸€ç§æœ‰å‰é€”çš„ç­–ç•¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œç§°ä¸ºæ½œåœ¨æ ‡è®°ã€‚è¿™äº›æ˜¯éè‡ªç„¶è¯­è¨€ä¸­ä¸å¯è§£é‡Šçš„è™šæ‹Ÿæ ‡è®°ï¼Œä½†é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼åŸºäºTransformerçš„LLMçš„è‡ªå›å½’è§£ç è¿‡ç¨‹ã€‚æ‰€æå‡ºçš„æ½œåœ¨æ ‡è®°å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°é¢„è®­ç»ƒçš„Transformerä¸­ï¼Œä»¥é«˜æ•ˆå‚æ•°çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´çµæ´»åº”ç”¨ï¼ŒåŒæ—¶ç»™ç°æœ‰æ ‡å‡†TransformeråŸºç¡€è®¾æ–½å¢åŠ æœ€å°çš„å¤æ‚æ€§å¼€é”€ã€‚æˆ‘ä»¬å¯¹æ½œåœ¨æ ‡è®°çš„æ½œåœ¨æœºåˆ¶æå‡ºäº†ä¸€äº›å‡è®¾ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„åˆæˆä»»åŠ¡æ¥éªŒè¯å®ƒä»¬ã€‚æ•°å€¼ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜æ˜¾ä¼˜äºåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ³›åŒ–åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œçªæ˜¾å…¶åœ¨æé«˜LLMé€‚åº”æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ½œè—ä»¤ç‰Œæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚è¯¥ç­–ç•¥é€šè¿‡å¼•å…¥éè‡ªç„¶è¯­è¨€ç†è§£çš„æ½œè—ä»¤ç‰Œï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼åŸºäºTransformerçš„LLMçš„è‡ªå›å½’è§£ç è¿‡ç¨‹ã€‚è¿™äº›ä»¤ç‰Œå¯æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒçš„Transformerä¸­ï¼Œä»¥é«˜æ•ˆçš„æ–¹å¼è®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶çµæ´»åº”ç”¨ï¼ŒåŒæ—¶ä¸ºç°æœ‰çš„æ ‡å‡†Transformeræ¶æ„å¢åŠ æä½çš„å¤æ‚æ€§å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰¹åˆ«æ˜¯éåˆ†å¸ƒæ³›åŒ–åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæé«˜LLMé€‚åº”æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œè—ä»¤ç‰Œæ˜¯ä¸€ç§å¢å¼ºLLMæ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>æ½œè—ä»¤ç‰Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼åŸºäºTransformerçš„LLMçš„è‡ªå›å½’è§£ç è¿‡ç¨‹ã€‚</li>
<li>æ½œè—ä»¤ç‰Œå¯ä»¥æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒçš„Transformerä¸­ï¼Œå¹¶ä»¥å‚æ•°é«˜æ•ˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ½œè—ä»¤ç‰Œåœ¨æ¨ç†æ—¶å…·æœ‰çµæ´»æ€§ï¼Œå¹¶å¢åŠ äº†æä½çš„å¤æ‚æ€§å¼€é”€ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ½œè—ä»¤ç‰Œåœ¨ç‰¹åˆ«æ˜¯éåˆ†å¸ƒæ³›åŒ–åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ½œè—ä»¤ç‰Œæé«˜äº†LLMçš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12a2f38ba075e74979091c88c4fbed1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b83879ed557c6b655a699d65a3ce38ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01701b1030c2f63f41017206d65f7609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f952ffc52515e252f4663859f72954ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44ad86e37e8e6901b5362c7e2042ce7d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Video-GPT-via-Next-Clip-Diffusion"><a href="#Video-GPT-via-Next-Clip-Diffusion" class="headerlink" title="Video-GPT via Next Clip Diffusion"></a>Video-GPT via Next Clip Diffusion</h2><p><strong>Authors:Shaobin Zhuang, Zhipeng Huang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Binxin Yang, Chong Sun, Chen Li, Yali Wang</strong></p>
<p>GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream. The project page is at <a target="_blank" rel="noopener" href="https://video-gpt.github.io/">https://Video-GPT.github.io</a>. </p>
<blockquote>
<p>GPTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œä»…ä»…ä¾é è¯­è¨€åºåˆ—ä¸è¶³ä»¥æè¿°è§†è§‰ä¸–ç•Œä¸­çš„æ—¶ç©ºç»†èŠ‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†é¢‘åºåˆ—æ›´æ“…é•¿æ•æ‰è¿™äº›ç»†èŠ‚ã€‚åŸºäºè¿™ä¸€äº‹å®ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§ç®€æ´çš„è§†é¢‘GPTï¼Œå°†è§†é¢‘è§†ä¸ºè§†è§‰ä¸–ç•Œå»ºæ¨¡çš„æ–°è¯­è¨€ã€‚é€šè¿‡æ¨¡æ‹ŸGPTä¸­çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸‹ä¸€ä¸ªç‰‡æ®µæ‰©æ•£æ¨¡å¼æ¥é¢„è®­ç»ƒè§†é¢‘GPTã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œè¿™ç§ç‹¬ç‰¹çš„æ¨¡å¼å…è®¸è§†é¢‘GPTè§£å†³çŸ­æœŸç”Ÿæˆå’Œé•¿æœŸé¢„æµ‹é—®é¢˜ï¼Œæ ¹æ®å†å²ä¸­çš„å¹²å‡€ç‰‡æ®µè‡ªå›å½’åœ°å¯¹å˜ˆæ‚ç‰‡æ®µè¿›è¡Œé™å™ªã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†é¢‘GPTåœ¨è§†é¢‘é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„æŠ€æœ¯æ€§èƒ½ï¼Œè¿™æ˜¯å®ç°ä¸–ç•Œå»ºæ¨¡çš„å…³é”®å› ç´ ï¼ˆPhysics-IQ Benchmarkï¼šè§†é¢‘GPT 34.97å¯¹Kling 23.64å¯¹Wan 20.89ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å¾ˆå¥½åœ°é€‚åº”è§†é¢‘ç”Ÿæˆå’Œç†è§£æ–¹é¢çš„6ä¸ªä¸»æµä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ä½äº<a target="_blank" rel="noopener" href="https://video-gpt.github.io./">https://Video-GPT.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12489v1">PDF</a> 22 pages, 12 figures, 18 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„Video-GPTæ¨¡å‹ï¼Œå°†è§†é¢‘è§†ä¸ºä¸€ç§æ–°çš„è¯­è¨€æ¥è¿›è¡Œè§†è§‰ä¸–ç•Œå»ºæ¨¡ã€‚é€šè¿‡å€Ÿé‰´GPTä¸­çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼Œå¼•å…¥äº†ä¸‹ä¸€ä¸ªç‰‡æ®µæ‰©æ•£èŒƒå¼æ¥é¢„è®­ç»ƒVideo-GPTã€‚ä¸ä»¥å¾€çš„å·¥ä½œä¸åŒï¼Œè¿™ç§ç‹¬ç‰¹çš„èŒƒå¼ä½¿å¾—Video-GPTèƒ½å¤ŸåŒæ—¶è¿›è¡ŒçŸ­æœŸç”Ÿæˆå’Œé•¿æœŸé¢„æµ‹ã€‚é€šè¿‡è‡ªé€‚åº”å»é™¤å™ªå£°ç‰‡æ®µä¸­çš„å™ªå£°ï¼Œå®ƒè¾¾åˆ°äº†è§†é¢‘é¢„æµ‹æ–¹é¢çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒVideo-GPTåœ¨è§†é¢‘ç”Ÿæˆå’Œç†è§£æ–¹é¢çš„å…­ä¸ªä¸»æµä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶å‡ºè‰²çš„ä¸‹æ¸¸ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-GPTæ¨¡å‹å°†è§†é¢‘è§†ä¸ºä¸€ç§æ–°è¯­è¨€è¿›è¡Œè§†è§‰ä¸–ç•Œå»ºæ¨¡ã€‚</li>
<li>å€Ÿé‰´GPTä¸­çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼Œå¼•å…¥äº†ä¸‹ä¸€ä¸ªç‰‡æ®µæ‰©æ•£èŒƒå¼è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>è¿™ç§æ–°çš„èŒƒå¼ä½¿å¾—Video-GPTèƒ½å¤ŸåŒæ—¶è¿›è¡ŒçŸ­æœŸç”Ÿæˆå’Œé•¿æœŸé¢„æµ‹ã€‚</li>
<li>Video-GPTé€šè¿‡è‡ªé€‚åº”å»é™¤å™ªå£°ç‰‡æ®µä¸­çš„å™ªå£°ï¼Œå®ç°äº†è§†é¢‘é¢„æµ‹çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>Video-GPTåœ¨å¤šä¸ªè§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Video-GPTåœ¨Physics-IQ Benchmarkä¸Šçš„å¾—åˆ†è¿œé«˜äºå…¶ä»–æ¨¡å‹ï¼Œä½“ç°äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5880e8761bc9d693cfd70962ec12448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6159bc24c2e0e6f729f8be17cf4d5d87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522041452fd6400da1ad1c1ec2eba23b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edc96a36afedd6103677e14942e896ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35976ec655681789cdc85f7756300682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d9d368cc7938690b3efbcc92c3eb9a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89ebf858d5221e519d9e99e8f8c4547d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts"><a href="#Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts" class="headerlink" title="Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts"></a>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h2><p><strong>Authors:Qi Feng, Hidetoshi Shimodaira</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹äºç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„ç©ºé—´è®¤çŸ¥ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸ç¼ºä¹å¿…è¦çš„æ¶æ„ç»„ä»¶å’Œç²¾ç»†ç©ºé—´ç†è§£æ‰€éœ€çš„ä¸“é—¨è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æ¨å‡ºäº†ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›çš„æ–°å‹MLLMã€‚ViCA2é‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆSigLIPè¿›è¡Œè¯­ä¹‰åˆ†æï¼Œç»“åˆHieraè¿›è¡Œç©ºé—´ç»“æ„åˆ†æï¼ŒåŒæ—¶é‡‡ç”¨ä»¤ç‰Œæ¯”ç‡æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ViCA-322Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡å¯¹ç©ºé—´å®šä½çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œç”¨äºé’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒæ•´ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹è¾¾åˆ°äº†å¹³å‡å¾—åˆ†56.8åˆ†çš„ä¸šç•Œæœ€ä½³æ°´å¹³ï¼Œæ˜¾è‘—è¶…è¿‡äº†æ›´å¤§çš„å¼€æºæ¨¡å‹ï¼ˆä¾‹å¦‚LLaVA-NeXT-Video-72Bçš„40.9åˆ†ï¼‰å’Œé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼ˆGemini-1.5 Proçš„45.4åˆ†ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¼ºå¤§ç©ºé—´æ™ºèƒ½æ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸”é‡‡ç”¨äº†ç´§å‡‘çš„æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒViCA2ã€å…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12363v1">PDF</a> 26 pages, 19 figures, 4 tables. Code, models, and dataset are   available at our project page: <a target="_blank" rel="noopener" href="https://github.com/nkkbr/ViCA">https://github.com/nkkbr/ViCA</a></p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨ä¸€èˆ¬è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„è§†è§‰ç©ºé—´è®¤çŸ¥ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾è®¾è®¡ç”¨äºå¢å¼ºç©ºé—´æ¨ç†çš„æ–°å‹Multimodalå¤§å‹è¯­è¨€æ¨¡å‹ã€‚ViCA2é‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆSigLIPè¿›è¡Œè¯­ä¹‰åˆ†æå¹¶Hieraè¿›è¡Œç©ºé—´ç»“æ„åˆ†æï¼ŒåŒæ—¶é‡‡ç”¨ä»¤ç‰Œæ¯”ç‡æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†å¤§å‹æ•°æ®é›†ViCA-322Kï¼ŒåŒ…å«è¶…è¿‡32ä¸‡ç»„ç©ºé—´å®šä½é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”¨äºé’ˆå¯¹æ€§æŒ‡å¯¼è°ƒæ•´ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹å¹³å‡å¾—åˆ†è¾¾åˆ°56.8ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–å¼€æºå¤§å‹æ¨¡å‹ï¼ˆå¦‚LLaVA-NeXT-Video-72Bçš„40.9åˆ†ï¼‰å’Œé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼ˆGemini-1.5 Proçš„45.4åˆ†ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¼ºå¤§çš„è§†è§‰ç©ºé—´æ™ºèƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æ¨¡å‹è¾ƒä¸ºç´§å‡‘ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†ViCA2ã€å…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Multimodal Large Language Models (MLLMs)è™½æ“…é•¿ä¸€èˆ¬è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œä½†åœ¨è§†è§‰ç©ºé—´è®¤çŸ¥æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ViCA2æ˜¯ä¸€æ¬¾æ–°å‹MLLMï¼Œæ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰åŒè§†è§‰ç¼–ç å™¨æ¶æ„ã€è¯­ä¹‰åˆ†æã€ç©ºé—´ç»“æ„åˆ†æä»¥åŠé«˜æ•ˆçš„ä»¤ç‰Œæ¯”ç‡æ§åˆ¶æœºåˆ¶ã€‚</li>
<li>å¼€å‘äº†å¤§å‹æ•°æ®é›†ViCA-322Kï¼Œç”¨äºé’ˆå¯¹æ€§æŒ‡å¯¼è°ƒæ•´æ¨¡å‹ã€‚</li>
<li>ViCA2åœ¨VSI-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å¾—åˆ†56.8ï¼Œæ˜¾è‘—è¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ViCA2å®ç°äº†å¼ºå¤§çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ŒåŒæ—¶æ¨¡å‹è¾ƒä¸ºç´§å‡‘ã€‚</li>
<li>ViCA2ã€å…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab70a243d7dd1133839c489e87bd7071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73408c0690e6babe23ec2f59c9dcd46c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b49a1d2f8faf93399797eb2a9195519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94de87aa0b44a40c568d427a06ae798c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TechniqueRAG-Retrieval-Augmented-Generation-for-Adversarial-Technique-Annotation-in-Cyber-Threat-Intelligence-Text"><a href="#TechniqueRAG-Retrieval-Augmented-Generation-for-Adversarial-Technique-Annotation-in-Cyber-Threat-Intelligence-Text" class="headerlink" title="TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique   Annotation in Cyber Threat Intelligence Text"></a>TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique   Annotation in Cyber Threat Intelligence Text</h2><p><strong>Authors:Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, Md Rizwan Parvez</strong></p>
<p>Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights. </p>
<blockquote>
<p>å‡†ç¡®è¯†åˆ«å®‰å…¨æ–‡æœ¬ä¸­çš„å¯¹æŠ—æŠ€æœ¯æ˜¯æœ‰æ•ˆç½‘ç»œé˜²å¾¡çš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´åŸºæœ¬æƒè¡¡ï¼šå®ƒä»¬è¦ä¹ˆä¾èµ–äºå…·æœ‰æœ‰é™é¢†åŸŸç²¾åº¦çš„é€šç”¨æ¨¡å‹ï¼Œè¦ä¹ˆéœ€è¦ä¾èµ–å¤§é‡æ ‡è®°æ•°æ®å’Œç‰¹å®šä»»åŠ¡ä¼˜åŒ–ï¼ˆå¦‚è‡ªå®šä¹‰çš„ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜å’Œå»å™ªï¼‰çš„èµ„æºå¯†é›†å‹ç®¡é“ï¼Œè¿™äº›èµ„æºåœ¨ç‰¹å®šé¢†åŸŸå¾ˆå°‘å¯ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†TechniqueRAGï¼Œè¿™æ˜¯ä¸€ä¸ªé¢†åŸŸç‰¹å®šçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡é›†æˆç°æˆçš„æ£€ç´¢å™¨ã€æŒ‡ä»¤è°ƒä¼˜çš„LLMå’Œå°‘é‡çš„æ–‡æœ¬æŠ€æœ¯é…å¯¹æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»…å¯¹é¢†åŸŸå†…çš„æœ‰é™ç¤ºä¾‹è¿›è¡Œç”Ÿæˆç»„ä»¶çš„å¾®è°ƒæ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä»è€Œé¿å…äº†èµ„æºå¯†é›†å‹çš„æ£€ç´¢è®­ç»ƒéœ€æ±‚ã€‚è™½ç„¶ä¼ ç»Ÿçš„RAGé€šè¿‡è€¦åˆæ£€ç´¢å’Œç”Ÿæˆæ¥ç¼“è§£è™šæ„é—®é¢˜ï¼Œä½†å®ƒå¯¹é€šç”¨æ£€ç´¢å™¨çš„ä¾èµ–å¾€å¾€ä¼šå¼•å…¥å˜ˆæ‚çš„å€™é€‰å¯¹è±¡ï¼Œä»è€Œé™åˆ¶äº†é¢†åŸŸç‰¹å®šçš„ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é›¶æ ·æœ¬LLMé‡æ–°æ’åºæ¥æé«˜æ£€ç´¢è´¨é‡å’Œé¢†åŸŸç‰¹å¼‚æ€§ï¼Œè¿™å°†æ˜ç¡®åœ°å°†æ£€ç´¢åˆ°çš„å€™é€‰å¯¹è±¡ä¸å¯¹æŠ—æŠ€æœ¯å¯¹é½ã€‚åœ¨å¤šä¸ªå®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTechniqueRAGåœ¨ä¸è¿›è¡Œå¤§é‡ç‰¹å®šä»»åŠ¡ä¼˜åŒ–æˆ–æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œè€Œç»¼åˆåˆ†ææä¾›äº†è¿›ä¸€æ­¥çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11988v1">PDF</a> Accepted at ACL (Findings) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å®‰å…¨æ–‡æœ¬ä¸­å‡†ç¡®è¯†åˆ«å¯¹æŠ—æŠ€æœ¯çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„åŸŸç‰¹å®šæ¡†æ¶TechniqueRAGï¼Œé€šè¿‡é›†æˆç°æˆçš„æ£€ç´¢å™¨ã€æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°‘é‡çš„æ–‡æœ¬æŠ€æœ¯å¯¹æ•°æ®å¯¹æ¥å¼¥è¡¥æ•°æ®ç¨€ç¼ºçš„ç¼ºé™·ã€‚è¯¥æ¡†æ¶é€šè¿‡æé«˜æ£€ç´¢è´¨é‡å’Œé¢†åŸŸç‰¹å¼‚æ€§æ¥è§£å†³ä¼ ç»ŸRAGæ–¹æ³•çš„é—®é¢˜ï¼Œå®ç°äº†åœ¨å®‰å…¨æ–‡æœ¬é¢†åŸŸçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®è¯†åˆ«å®‰å…¨æ–‡æœ¬ä¸­çš„å¯¹æŠ—æŠ€æœ¯æ˜¯æœ‰æ•ˆç½‘ç»œé˜²å¾¡çš„å…³é”®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é€šç”¨æ¨¡å‹ä¸é¢†åŸŸç²¾åº¦æœ‰é™çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>TechniqueRAGæ˜¯ä¸€ä¸ªåŸŸç‰¹å®šçš„RAGæ¡†æ¶ï¼Œé›†æˆäº†ç°æˆçš„æ£€ç´¢å™¨ã€æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°‘é‡æ–‡æœ¬æŠ€æœ¯å¯¹æ•°æ®å¯¹ã€‚</li>
<li>TechniqueRAGè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé€šè¿‡åªå¯¹ç”Ÿæˆç»„ä»¶è¿›è¡Œå¾®è°ƒï¼Œè€Œæ— éœ€å¤§é‡é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†å’Œä»»åŠ¡ç‰¹å®šä¼˜åŒ–ã€‚</li>
<li>ä¼ ç»ŸRAGæ–¹æ³•ä¸­çš„æ£€ç´¢å’Œç”Ÿæˆè€¦åˆæœ‰åŠ©äºç¼“è§£è™šæ„é—®é¢˜ï¼Œä½†ä¾èµ–äºé€šç”¨æ£€ç´¢å™¨ï¼Œå¯èƒ½å¯¼è‡´å¼•å…¥å™ªå£°å€™é€‰è€…ï¼Œé™åˆ¶äº†é¢†åŸŸç‰¹å®šçš„ç²¾åº¦ã€‚</li>
<li>TechniqueRAGé€šè¿‡é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°æ’åå¢å¼ºæ£€ç´¢è´¨é‡å’Œé¢†åŸŸç‰¹å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-334934d0876049f5dc08c8ac14038116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64105975f73050a0d216b0acb495578e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0449a68ec7301665b6daf4c9bd9acd7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6006da90d7c9ba6c535904fde0b31051.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedSG-Bench-A-Benchmark-for-Medical-Image-Sequences-Grounding"><a href="#MedSG-Bench-A-Benchmark-for-Medical-Image-Sequences-Grounding" class="headerlink" title="MedSG-Bench: A Benchmark for Medical Image Sequences Grounding"></a>MedSG-Bench: A Benchmark for Medical Image Sequences Grounding</h2><p><strong>Authors:Jingkun Yue, Siqi Zhang, Zinan Jia, Huihuan Xu, Zongbo Han, Xiaohong Liu, Guangyu Wang</strong></p>
<p>Visual grounding is essential for precise perception and reasoning in multimodal large language models (MLLMs), especially in medical imaging domains. While existing medical visual grounding benchmarks primarily focus on single-image scenarios, real-world clinical applications often involve sequential images, where accurate lesion localization across different modalities and temporal tracking of disease progression (e.g., pre- vs. post-treatment comparison) require fine-grained cross-image semantic alignment and context-aware reasoning. To remedy the underrepresentation of image sequences in existing medical visual grounding benchmarks, we propose MedSG-Bench, the first benchmark tailored for Medical Image Sequences Grounding. It comprises eight VQA-style tasks, formulated into two paradigms of the grounding tasks, including 1) Image Difference Grounding, which focuses on detecting change regions across images, and 2) Image Consistency Grounding, which emphasizes detection of consistent or shared semantics across sequential images. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities, and a wide spectrum of anatomical structures and diseases, totaling 9,630 question-answer pairs. We benchmark both general-purpose MLLMs (e.g., Qwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision), observing that even the advanced models exhibit substantial limitations in medical sequential grounding tasks. To advance this field, we construct MedSG-188K, a large-scale instruction-tuning dataset tailored for sequential visual grounding, and further develop MedSeq-Grounder, an MLLM designed to facilitate future research on fine-grained understanding across medical sequential images. The benchmark, dataset, and model are available at <a target="_blank" rel="noopener" href="https://huggingface.co/MedSG-Bench">https://huggingface.co/MedSG-Bench</a> </p>
<blockquote>
<p>è§†è§‰å®šä½åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼Œå¯¹äºç²¾ç¡®æ„ŸçŸ¥å’Œæ¨ç†è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰çš„åŒ»å­¦è§†è§‰å®šä½åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å•å›¾åƒåœºæ™¯ï¼Œä½†ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨é€šå¸¸æ¶‰åŠå›¾åƒåºåˆ—ï¼Œå…¶ä¸­ä¸åŒæ¨¡æ€ä¹‹é—´çš„ç²¾ç¡®ç—…ç¶å®šä½å’Œç–¾ç—…è¿›å±•çš„æ—¶ç©ºè¿½è¸ªï¼ˆä¾‹å¦‚ï¼Œæ²»ç–—å‰åçš„æ¯”è¾ƒï¼‰éœ€è¦ç²¾ç»†çš„è·¨å›¾åƒè¯­ä¹‰å¯¹é½å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰åŒ»å­¦è§†è§‰å®šä½åŸºå‡†æµ‹è¯•ä¸­å›¾åƒåºåˆ—è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedSG-Benchï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºåŒ»å­¦å›¾åƒåºåˆ—å®šä½é‡èº«å®šåˆ¶çš„ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«äº†å…«ä¸ªVQAé£æ ¼çš„ä»»åŠ¡ï¼Œåˆ¶å®šä¸ºä¸¤ç§å®šä½ä»»åŠ¡çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬1ï¼‰å›¾åƒå·®å¼‚å®šä½ï¼Œä¸“æ³¨äºæ£€æµ‹å›¾åƒä¹‹é—´çš„å˜åŒ–åŒºåŸŸï¼›2ï¼‰å›¾åƒä¸€è‡´æ€§å®šä½ï¼Œå¼ºè°ƒæ£€æµ‹åºåˆ—å›¾åƒä¸­ä¸€è‡´æˆ–å…±äº«çš„è¯­ä¹‰ã€‚MedSG-Benchæ¶µç›–äº†76ä¸ªå…¬å…±æ•°æ®é›†ã€10ç§åŒ»å­¦æˆåƒæ¨¡æ€ä»¥åŠå¹¿æ³›çš„è§£å‰–ç»“æ„å’Œç–¾ç—…ï¼Œæ€»å…±9630ä¸ªé—®ç­”å¯¹ã€‚æˆ‘ä»¬å¯¹é€šç”¨MLLMsï¼ˆä¾‹å¦‚Qwen2.5-VLï¼‰å’ŒåŒ»ç–—é¢†åŸŸä¸“ç”¨MLLMsï¼ˆä¾‹å¦‚HuatuoGPT-visionï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å³ä½¿åœ¨åŒ»å­¦åºåˆ—å®šä½ä»»åŠ¡ä¸­ï¼Œå…ˆè¿›æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºç›¸å½“å¤§çš„å±€é™æ€§ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œæˆ‘ä»¬æ„å»ºäº†MedSG-188Kï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºåºåˆ—è§†è§‰å®šä½çš„å¤§å‹æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¹¶è¿›ä¸€æ­¥å‘å±•äº†MedSeq-Grounderï¼Œè¿™æ˜¯ä¸€ä¸ªMLLMï¼Œæ—¨åœ¨ä¿ƒè¿›æœªæ¥å¯¹åŒ»å­¦åºåˆ—å›¾åƒçš„ç²¾ç»†ç†è§£ç ”ç©¶ã€‚è¯¥åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/MedSG-Bench%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/MedSG-Benchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11852v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—æˆåƒé¢†åŸŸä¸­çš„ç²¾ç¡®æ„ŸçŸ¥å’Œæ¨ç†ï¼Œè§†è§‰åŸºç¡€è‡³å…³é‡è¦ã€‚ç°æœ‰åŒ»ç–—è§†è§‰åŸºç¡€åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å•å›¾åƒåœºæ™¯ä¸Šï¼Œè€Œç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠåº”ç”¨é€šå¸¸æ¶‰åŠå›¾åƒåºåˆ—ï¼Œè¦æ±‚ç²¾ç»†çš„è·¨å›¾åƒè¯­ä¹‰å¯¹é½å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMedSG-Benchï¼Œé¦–ä¸ªé’ˆå¯¹åŒ»ç–—å›¾åƒåºåˆ—åŸºç¡€çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…æ‹¬å…«ç§VQAé£æ ¼çš„ä»»åŠ¡ï¼Œåˆ†ä¸ºä¸¤å¤§åŸºç¡€ä»»åŠ¡èŒƒå¼ï¼ŒåŒ…æ‹¬1ï¼‰å›¾åƒå·®å¼‚åŸºç¡€ï¼Œä¸“æ³¨äºæ£€æµ‹å›¾åƒé—´çš„å˜åŒ–åŒºåŸŸï¼›2ï¼‰å›¾åƒä¸€è‡´æ€§åŸºç¡€ï¼Œå¼ºè°ƒæ£€æµ‹åºåˆ—å›¾åƒä¸­ä¸€è‡´æˆ–å…±äº«è¯­ä¹‰ã€‚MedSG-Benchè¦†ç›–76ä¸ªå…¬å…±æ•°æ®é›†ã€10ç§åŒ»å­¦å½±åƒæ¨¡æ€å’Œå¹¿æ³›çš„ç»“æ„ä¸ç–¾ç—…ï¼Œå…±åŒ…å«9630ä¸ªé—®ç­”å¯¹ã€‚æˆ‘ä»¬å¯¹é€šç”¨MLLMsï¼ˆå¦‚Qwen2.5-VLï¼‰å’ŒåŒ»ç–—é¢†åŸŸä¸“ç”¨MLLMsï¼ˆå¦‚HuatuoGPT-visionï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å³ä½¿æ˜¯é«˜çº§æ¨¡å‹åœ¨åŒ»ç–—åºåˆ—åŸºç¡€ä»»åŠ¡ä¸Šä¹Ÿå­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œæˆ‘ä»¬æ„å»ºäº†é’ˆå¯¹åºåˆ—è§†è§‰åŸºç¡€çš„å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MedSG-188Kï¼Œå¹¶å¼€å‘äº†MLLMâ€”â€”MedSeq-Grounderï¼Œä»¥ä¿ƒè¿›å¯¹åŒ»ç–—åºåˆ—å›¾åƒçš„ç²¾ç»†ç†è§£ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰åŸºç¡€åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç²¾ç¡®æ„ŸçŸ¥å’Œæ¨ç†ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—æˆåƒé¢†åŸŸã€‚</li>
<li>ç°æœ‰åŒ»ç–—è§†è§‰åŸºç¡€åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å•å›¾åƒåœºæ™¯ï¼Œä½†å®é™…åº”ç”¨æ¶‰åŠå›¾åƒåºåˆ—ã€‚</li>
<li>MedSG-Benché¦–æ¬¡ä¸ºåŒ»ç–—å›¾åƒåºåˆ—åŸºç¡€æä¾›åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯é«˜çº§MLLMsåœ¨åŒ»ç–—åºåˆ—åŸºç¡€ä»»åŠ¡ä¸Šä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ä¸ºäº†æ”¹è¿›æ¨¡å‹æ€§èƒ½ï¼Œæ„å»ºäº†é’ˆå¯¹åºåˆ—è§†è§‰åŸºç¡€çš„å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MedSG-188Kã€‚</li>
<li>å¼€å‘äº†MedSeq-Grounderè¿™ä¸€MLLMï¼Œä»¥ä¿ƒè¿›å¯¹åŒ»ç–—åºåˆ—å›¾åƒçš„ç²¾ç»†ç†è§£ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df99f3638ead56b1e1cd929e2b247099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ffd97d31a9dfb91511a6c136abbe53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bafc1b02070a60b3b67918fae5d6a710.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ada4dfa1c0cb817a1adbfdfebbe7fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663a71799a93facc5427bc539094574d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe7af8fd9425def2e33349952de04f2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="REI-Bench-Can-Embodied-Agents-Understand-Vague-Human-Instructions-in-Task-Planning"><a href="#REI-Bench-Can-Embodied-Agents-Understand-Vague-Human-Instructions-in-Task-Planning" class="headerlink" title="REI-Bench: Can Embodied Agents Understand Vague Human Instructions in   Task Planning?"></a>REI-Bench: Can Embodied Agents Understand Vague Human Instructions in   Task Planning?</h2><p><strong>Authors:Chenxi Jiang, Chuhao Zhou, Jianfei Yang</strong></p>
<p>Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children. </p>
<blockquote>
<p>æœºå™¨äººä»»åŠ¡è§„åˆ’èƒ½å¤Ÿå°†äººç±»æŒ‡ä»¤åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨åºåˆ—ï¼Œä»è€Œä½¿æœºå™¨äººèƒ½å¤Ÿå®Œæˆä¸€ç³»åˆ—å¤æ‚ä»»åŠ¡ã€‚å°½ç®¡æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è§„åˆ’å™¨å–å¾—äº†æƒŠäººçš„è¡¨ç°ï¼Œä½†å®ƒä»¬å‡è®¾äººç±»æŒ‡ä»¤æ˜¯æ¸…æ™°ç›´æ¥çš„ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„ç”¨æˆ·å¹¶éä¸“å®¶ï¼Œä»–ä»¬å¯¹æœºå™¨äººçš„æŒ‡ä»¤é€šå¸¸åŒ…å«å¤§é‡çš„æ¨¡ç³Šæ€§ã€‚è¯­è¨€å­¦å®¶è®¤ä¸ºï¼Œè¿™ç§æ¨¡ç³Šæ€§å¾€å¾€æºäºæŒ‡ä»£è¡¨è¾¾å¼ï¼ˆREï¼‰ï¼Œå…¶å«ä¹‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¯¹è¯ä¸Šä¸‹æ–‡å’Œç¯å¢ƒã€‚è¿™ç§æ¨¡ç³Šæ€§åœ¨è€å¹´äººå’Œå„¿ç«¥ä¸­æ›´ä¸ºæ™®éï¼Œè€Œæœºå™¨äººåº”è¯¥æ›´å¤šåœ°ä¸ºä»–ä»¬æœåŠ¡ã€‚è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†äººç±»æŒ‡ä»¤ä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ï¼ˆREï¼‰çš„æ¨¡ç³Šæ€§æ˜¯å¦‚ä½•å½±å“åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœºå™¨äººä»»åŠ¡è§„åˆ’çš„ï¼Œä»¥åŠå¦‚ä½•è§£å†³è¿™ä¸€é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå…·æœ‰æ¨¡ç³ŠæŒ‡ä»£è¡¨è¾¾å¼çš„æœºå™¨äººä»»åŠ¡è§„åˆ’åŸºå‡†æµ‹è¯•ï¼ˆREI-Benchï¼‰ï¼Œåœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­æˆ‘ä»¬å‘ç°ï¼ŒæŒ‡ä»£è¡¨è¾¾å¼çš„æ¨¡ç³Šæ€§ä¼šä¸¥é‡é™ä½æœºå™¨äººè§„åˆ’çš„æ€§èƒ½ï¼Œå¯¼è‡´æˆåŠŸç‡ä¸‹é™é«˜è¾¾77.9%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œå¤§å¤šæ•°å¤±è´¥çš„æƒ…å†µæºäºè§„åˆ’ä¸­çš„ç›®æ ‡å¯¹è±¡ç¼ºå¤±ã€‚ä¸ºäº†ç¼“è§£æŒ‡ä»£è¡¨è¾¾å¼çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼šé¢å‘ä»»åŠ¡çš„ä¸Šä¸‹æ–‡è®¤çŸ¥ï¼Œå®ƒä¸ºæœºå™¨äººç”Ÿæˆæ¸…æ™°çš„æŒ‡ä»¤ï¼Œä¸æœ‰æ„è¯†çš„æç¤ºå’Œæ€ç»´é“¾ç›¸æ¯”ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä½¿æœºå™¨äººä»»åŠ¡è§„åˆ’æ›´åŠ å®ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éä¸“ä¸šç”¨æˆ·ï¼ˆå¦‚è€å¹´äººå’Œå„¿ç«¥ï¼‰ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸æœºå™¨äººäº¤äº’ç ”ç©¶ç¤¾åŒºåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10872v2">PDF</a> Under Review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœºå™¨äººä»»åŠ¡è§„åˆ’å°†äººç±»æŒ‡ä»¤åˆ†è§£æˆå¯æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿå®Œæˆä¸€ç³»åˆ—å¤æ‚çš„ä»»åŠ¡ã€‚è™½ç„¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»»åŠ¡è§„åˆ’å™¨å–å¾—äº†ä»¤äººç©ç›®çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å‡è®¾äººç±»æŒ‡ä»¤æ˜¯æ¸…æ™°å’Œç›´æ¥çš„ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„ç”¨æˆ·å¹¶éä¸“å®¶ï¼Œä»–ä»¬å¯¹æœºå™¨äººçš„æŒ‡ä»¤é€šå¸¸åŒ…å«å¤§é‡çš„æ¨¡ç³Šæ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†äººç±»æŒ‡ä»¤ä¸­çš„å‚ç…§è¡¨è¾¾å¼ï¼ˆREsï¼‰çš„æ¨¡ç³Šæ€§å¦‚ä½•å½±å“åŸºäºLLMçš„æœºå™¨äººä»»åŠ¡è§„åˆ’ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•è§£å†³è¿™ä¸€é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå«æœ‰æ¨¡ç³Šå‚ç…§è¡¨è¾¾å¼çš„æœºå™¨äººä»»åŠ¡è§„åˆ’åŸºå‡†æµ‹è¯•ï¼ˆREI-Benchï¼‰ï¼Œå‘ç°å‚ç…§è¡¨è¾¾å¼çš„æ¨¡ç³Šæ€§ä¼šä¸¥é‡é™ä½æœºå™¨äººè§„åˆ’çš„æ€§èƒ½ï¼ŒæˆåŠŸç‡ä¸‹é™é«˜è¾¾77.9%ã€‚æˆ‘ä»¬è¿˜å‘ç°å¤§å¤šæ•°å¤±è´¥çš„æƒ…å†µæ˜¯ç”±äºè§„åˆ’ä¸­çš„ç›®æ ‡ç¼ºå¤±ã€‚ä¸ºäº†ç¼“è§£å‚ç…§è¡¨è¾¾å¼çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼šé¢å‘ä»»åŠ¡çš„ä¸Šä¸‹æ–‡è®¤çŸ¥ï¼Œä¸ºæœºå™¨äººç”Ÿæˆæ¸…æ™°çš„æŒ‡ä»¤ï¼Œå®ç°äº†ä¸æ„è¯†æç¤ºå’Œæ€è€ƒé“¾ç›¸æ¯”çš„å…ˆè¿›æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸ºæœºå™¨äººä¸äººç±»äº’åŠ¨çš„ç ”ç©¶ç¾¤ä½“åšå‡ºäº†è´¡çŒ®ï¼Œä½¿æœºå™¨äººä»»åŠ¡è§„åˆ’æ›´åŠ å®ç”¨ï¼Œç‰¹åˆ«æ˜¯é¢å‘éä¸“ä¸šç”¨æˆ·ï¼Œå¦‚è€å¹´äººå’Œå„¿ç«¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨äººä»»åŠ¡è§„åˆ’èƒ½å°†å¤æ‚çš„äººç±»æŒ‡ä»¤è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œåºåˆ—ã€‚</li>
<li>åŸºäºLLMçš„æœºå™¨äººä»»åŠ¡è§„åˆ’å™¨åœ¨å‡è®¾äººç±»æŒ‡ä»¤æ¸…æ™°ç›´æ¥æ—¶è¡¨ç°æœ€ä½³ã€‚</li>
<li>çœŸå®ä¸–ç•Œçš„ç”¨æˆ·æŒ‡ä»¤å¸¸åŒ…å«æ¨¡ç³Šæ€§ï¼Œä¸»è¦æºäºå‚ç…§è¡¨è¾¾å¼çš„ä½¿ç”¨ã€‚</li>
<li>å‚ç…§è¡¨è¾¾å¼çš„æ¨¡ç³Šæ€§å¯¹æœºå™¨äººä»»åŠ¡è§„åˆ’æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼ŒæˆåŠŸç‡ä¸‹é™å¯é«˜è¾¾77.9%ã€‚</li>
<li>å¤§å¤šæ•°è§„åˆ’å¤±è´¥çš„æƒ…å†µæ˜¯å› ä¸ºç›®æ ‡åœ¨æŒ‡ä»¤ä¸­çš„ç¼ºå¤±ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”é¢å‘ä»»åŠ¡çš„ä¸Šä¸‹æ–‡è®¤çŸ¥ï¼Œä»¥å¤„ç†æ¨¡ç³Šçš„å‚ç…§è¡¨è¾¾å¼ï¼Œä¸ºæœºå™¨äººç”Ÿæˆæ¸…æ™°æŒ‡ä»¤ã€‚</li>
<li>æ­¤æ–¹æ³•å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éä¸“ä¸šç”¨æˆ·çš„æŒ‡ä»¤æ—¶ï¼Œå¦‚è€å¹´äººå’Œå„¿ç«¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2abf10dd4a8a1d6fd576d8a39a3d284.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85341f39acecd2dfced3750d1c918150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d33e1a00fa5960962c37422748e9d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5f4803a5cea43c0a09a64e58c1142e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RICo-Refined-In-Context-Contribution-for-Automatic-Instruction-Tuning-Data-Selection"><a href="#RICo-Refined-In-Context-Contribution-for-Automatic-Instruction-Tuning-Data-Selection" class="headerlink" title="RICo: Refined In-Context Contribution for Automatic Instruction-Tuning   Data Selection"></a>RICo: Refined In-Context Contribution for Automatic Instruction-Tuning   Data Selection</h2><p><strong>Authors:Yixin Yang, Qingxiu Dong, Linli Yao, Fangwei Zhu, Zhifang Sui</strong></p>
<p>Data selection for instruction tuning is crucial for improving the performance of large language models (LLMs) while reducing training costs. In this paper, we propose Refined Contribution Measurement with In-Context Learning (RICo), a novel gradient-free method that quantifies the fine-grained contribution of individual samples to both task-level and global-level model performance. RICo enables more accurate identification of high-contribution data, leading to better instruction tuning. We further introduce a lightweight selection paradigm trained on RICo scores, enabling scalable data selection with a strictly linear inference complexity. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of RICo. Remarkably, on LLaMA3.1-8B, models trained on 15% of RICo-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by RICo, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones. </p>
<blockquote>
<p>æ•°æ®é€‰æ‹©åœ¨æŒ‡ä»¤å¾®è°ƒä¸­å¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½å’Œé™ä½è®­ç»ƒæˆæœ¬è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„ç²¾ç»†è´¡çŒ®åº¦é‡ï¼ˆRICoï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— éœ€æ¢¯åº¦çš„æ–¹æ³•ï¼Œå¯ä»¥é‡åŒ–å•ä¸ªæ ·æœ¬å¯¹ä»»åŠ¡çº§åˆ«å’Œå…¨å±€çº§åˆ«æ¨¡å‹æ€§èƒ½çš„ç²¾ç»†è´¡çŒ®ã€‚RICoèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«é«˜è´¡çŒ®æ•°æ®ï¼Œä»è€Œå®ç°æ›´å¥½çš„æŒ‡ä»¤å¾®è°ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§åŸºäºRICoåˆ†æ•°çš„è½»é‡çº§é€‰æ‹©èŒƒå¼ï¼Œå®ç°äº†å…·æœ‰ä¸¥æ ¼çº¿æ€§æ¨ç†å¤æ‚åº¦çš„å¯æ‰©å±•æ•°æ®é€‰æ‹©ã€‚åœ¨ä¸‰ä¸ªLLMçš„12ä¸ªåŸºå‡†æµ‹è¯•å’Œ5ä¸ªé…å¯¹è¯„ä¼°é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†RICoçš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨LLaMA3.1-8Bä¸Šï¼Œä½¿ç”¨RICoé€‰æ‹©æ•°æ®çš„15%è®­ç»ƒçš„æ¨¡å‹åœ¨å…¨æ•°æ®é›†ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œé«˜å‡º5.42ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶è¶…è¿‡äº†å¹¿æ³›ä½¿ç”¨çš„é€‰æ‹©æ–¹æ³•ä¸­çš„æœ€ä½³æ€§èƒ½ï¼Œé«˜å‡º2.06ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥åˆ†æäº†RICoé€‰æ‹©çš„é«˜è´¡çŒ®æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ˜¾ç¤ºäº†å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œé€‚å½“çš„éš¾åº¦æ°´å¹³ï¼Œè€Œä¸ä»…ä»…æ˜¯éš¾åº¦æœ€å¤§çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRefined Contribution Measurement with In-Context Learningï¼ˆRICoï¼‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚RICoæ˜¯ä¸€ç§æ— éœ€æ¢¯åº¦çš„å…¨æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®è¡¡é‡æ¯ä¸ªæ ·æœ¬å¯¹ä»»åŠ¡çº§åˆ«å’Œå…¨å±€çº§åˆ«æ¨¡å‹æ€§èƒ½çš„ç»†å¾®è´¡çŒ®ã€‚è¯¥æ–¹æ³•å¯ä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«é«˜è´¡çŒ®æ•°æ®ï¼Œä»è€Œå®ç°æ›´ä¼˜è´¨çš„æŒ‡ä»¤å¾®è°ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºRICoå¾—åˆ†çš„è½»é‡çº§é€‰æ‹©èŒƒå¼ï¼Œå¯å®ç°å…·æœ‰ä¸¥æ ¼çº¿æ€§æ¨ç†å¤æ‚åº¦çš„å¯æ‰©å±•æ•°æ®é€‰æ‹©ã€‚å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRICoåœ¨ä¸‰ä¸ªLLMä¸Šï¼Œè·¨è¶Š12ä¸ªåŸºå‡†æµ‹è¯•å’Œ5ä¸ªé…å¯¹è¯„ä¼°é›†å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨RICoé€‰æ‹©çš„æ•°æ®è®­ç»ƒçš„LLaMA3.1-8Bæ¨¡å‹åœ¨ä»…ä½¿ç”¨15%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ¯”ä½¿ç”¨å…¨æ•°æ®é›†é«˜å‡º5.42%ï¼Œå¹¶è¶…è¿‡äº†å¹¿æ³›ä½¿ç”¨çš„é€‰æ‹©æ–¹æ³•ä¸­çš„æœ€ä½³æ€§èƒ½2.06%ã€‚å¯¹RICoé€‰æ‹©çš„é«˜è´¡çŒ®æ ·æœ¬çš„åˆ†ææ˜¾ç¤ºï¼Œè¿™äº›æ ·æœ¬å…·æœ‰å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œé€‚å½“çš„éš¾åº¦çº§åˆ«ï¼Œå¹¶éä»…ä»…æ˜¯éš¾åº¦æœ€å¤§çš„æ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RICoæ˜¯ä¸€ç§æ–°å‹æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›LLMçš„æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>RICoé€šè¿‡ç²¾ç¡®è¡¡é‡æ¯ä¸ªæ ·æœ¬çš„è´¡çŒ®æ¥å®ç°æ›´å‡†ç¡®çš„æ•°æ®è¯†åˆ«ã€‚</li>
<li>RICoå¯ä»¥æ”¯æŒæ›´ä¼˜è´¨çš„æŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºRICoå¾—åˆ†çš„è½»é‡çº§é€‰æ‹©èŒƒå¼ï¼Œå®ç°æ•°æ®é€‰æ‹©çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>RICoåœ¨å¤šä¸ªLLMå’ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡æ•ˆæœã€‚</li>
<li>ä½¿ç”¨RICoé€‰æ‹©çš„æ•°æ®è®­ç»ƒçš„LLaMAæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cd8f3458a109e5c4dab98be32846a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-378cf992dac67e4ab54c2a218b8e2482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3af9376a2abb945bebfd53603cd96910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c53a38aac64139f75931c08917b8fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c929de476d7c33c46272947702f9008.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c09a172b44f945ea3cc9daf925f31981.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Synthesis of Communication Policies for Multi-Agent Systems Robust to   Communication Restrictions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cd7c459344316127e56a86ccda73a2dd.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  ChartMuseum Testing Visual Reasoning Capabilities of Large   Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23394.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
