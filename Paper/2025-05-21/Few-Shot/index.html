<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  GMM-Based Comprehensive Feature Extraction and Relative Distance   Preservation For Few-Shot Cross-Modal Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-50811f3e0aeae9bea6f916af2905c28b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-21-æ›´æ–°"><a href="#2025-05-21-æ›´æ–°" class="headerlink" title="2025-05-21 æ›´æ–°"></a>2025-05-21 æ›´æ–°</h1><h2 id="GMM-Based-Comprehensive-Feature-Extraction-and-Relative-Distance-Preservation-For-Few-Shot-Cross-Modal-Retrieval"><a href="#GMM-Based-Comprehensive-Feature-Extraction-and-Relative-Distance-Preservation-For-Few-Shot-Cross-Modal-Retrieval" class="headerlink" title="GMM-Based Comprehensive Feature Extraction and Relative Distance   Preservation For Few-Shot Cross-Modal Retrieval"></a>GMM-Based Comprehensive Feature Extraction and Relative Distance   Preservation For Few-Shot Cross-Modal Retrieval</h2><p><strong>Authors:Chengsong Sun, Weiping Li, Xiang Li, Yuankun Liu, Lianlei Shan</strong></p>
<p>Few-shot cross-modal retrieval focuses on learning cross-modal representations with limited training samples, enabling the model to handle unseen classes during inference. Unlike traditional cross-modal retrieval tasks, which assume that both training and testing data share the same class distribution, few-shot retrieval involves data with sparse representations across modalities. Existing methods often fail to adequately model the multi-peak distribution of few-shot cross-modal data, resulting in two main biases in the latent semantic space: intra-modal bias, where sparse samples fail to capture intra-class diversity, and inter-modal bias, where misalignments between image and text distributions exacerbate the semantic gap. These biases hinder retrieval accuracy. To address these issues, we propose a novel method, GCRDP, for few-shot cross-modal retrieval. This approach effectively captures the complex multi-peak distribution of data using a Gaussian Mixture Model (GMM) and incorporates a multi-positive sample contrastive learning mechanism for comprehensive feature modeling. Additionally, we introduce a new strategy for cross-modal semantic alignment, which constrains the relative distances between image and text feature distributions, thereby improving the accuracy of cross-modal representations. We validate our approach through extensive experiments on four benchmark datasets, demonstrating superior performance over six state-of-the-art methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬è·¨æ¨¡æ€æ£€ç´¢è‡´åŠ›äºåœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸Šå­¦ä¹ è·¨æ¨¡æ€è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¤„ç†æœªè§è¿‡çš„ç±»åˆ«ã€‚ä¸ä¼ ç»Ÿå‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å…·æœ‰ç›¸åŒç±»åˆ«åˆ†å¸ƒçš„è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸åŒï¼Œå°‘é‡æ ·æœ¬æ£€ç´¢æ¶‰åŠåˆ°è·¨æ¨¡æ€çš„ç¨€ç–è¡¨ç¤ºæ•°æ®ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†å»ºæ¨¡å°‘é‡æ ·æœ¬è·¨æ¨¡æ€æ•°æ®çš„å¤šå³°åˆ†å¸ƒï¼Œå¯¼è‡´æ½œåœ¨è¯­ä¹‰ç©ºé—´ä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦åè§ï¼šæ¨¡æ€å†…åè§ï¼Œå…¶ä¸­ç¨€ç–æ ·æœ¬æ— æ³•æ•æ‰ç±»å†…å¤šæ ·æ€§ï¼›æ¨¡æ€é—´åè§,å…¶ä¸­å›¾åƒå’Œæ–‡æœ¬åˆ†å¸ƒä¹‹é—´çš„é”™ä½åŠ å‰§äº†è¯­ä¹‰é¸¿æ²Ÿã€‚è¿™äº›åè§é˜»ç¢äº†æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå°‘é‡æ ·æœ¬è·¨æ¨¡æ€æ£€ç´¢çš„æ–°æ–¹æ³•GCRDPã€‚è¯¥æ–¹æ³•ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æœ‰æ•ˆåœ°æ•è·äº†æ•°æ®çš„å¤æ‚å¤šå³°åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨äº†å¤šé˜³æ€§æ ·æœ¬å¯¹æ¯”å­¦ä¹ æœºåˆ¶è¿›è¡Œç‰¹å¾çš„ç»¼åˆå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ç­–ç•¥ï¼Œçº¦æŸå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œä»è€Œæé«˜äº†è·¨æ¨¡æ€è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜äºå…­ç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13306v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°æ ·æœ¬çš„è·¨æ¨¡æ€æ£€ç´¢æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨æœ‰é™æ ·æœ¬å­¦ä¹ è·¨æ¨¡æ€è¡¨ç¤ºï¼Œå¯ä»¥å¤„ç†æ¨æ–­æœŸé—´æœªè§è¿‡çš„ç±»åˆ«ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GCRDPæ¥è§£å†³å°æ ·æœ¬çš„è·¨æ¨¡æ€æ£€ç´¢é—®é¢˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æœ‰æ•ˆåœ°æ•æ‰æ•°æ®çš„å¤æ‚å¤šå³°åˆ†å¸ƒï¼Œé‡‡ç”¨å¤šé˜³æ€§æ ·æœ¬å¯¹æ¯”å­¦ä¹ æœºåˆ¶è¿›è¡Œç‰¹å¾å»ºæ¨¡ï¼Œå¹¶å¼•å…¥æ–°çš„è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ç­–ç•¥ï¼Œçº¦æŸå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œæé«˜è·¨æ¨¡æ€è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºå…­ç§æœ€æ–°çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Few-shot cross-modal retrievalå…³æ³¨ä½¿ç”¨æœ‰é™æ ·æœ¬å­¦ä¹ è·¨æ¨¡æ€è¡¨ç¤ºï¼Œä»¥å¤„ç†æœªè§è¿‡çš„ç±»åˆ«ã€‚</li>
<li>ä¼ ç»Ÿè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡å‡å®šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®å…·æœ‰ç›¸åŒçš„ç±»åˆ«åˆ†å¸ƒï¼Œè€Œfew-shotæ£€ç´¢æ¶‰åŠè·¨æ¨¡æ€æ•°æ®çš„ç¨€ç–è¡¨ç¤ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€ä¸èƒ½å……åˆ†å»ºæ¨¡è·¨æ¨¡æ€æ•°æ®çš„å¤šå³°åˆ†å¸ƒï¼Œå¯¼è‡´æ½œä¼è¯­ä¹‰ç©ºé—´ä¸­çš„ä¸¤ç§åè§ï¼šåŒæ¨¡æ€åè§å’Œè·¨æ¨¡æ€åè§ã€‚</li>
<li>æ–‡ç« æå‡ºçš„GCRDPæ–¹æ³•åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ•æ‰æ•°æ®çš„å¤æ‚å¤šå³°åˆ†å¸ƒã€‚</li>
<li>GCRDPé‡‡ç”¨å¤šé˜³æ€§æ ·æœ¬å¯¹æ¯”å­¦ä¹ æœºåˆ¶è¿›è¡Œç‰¹å¾å»ºæ¨¡ï¼Œå¹¶å¼•å…¥æ–°çš„è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜GCRDPåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…­ç§æœ€æ–°çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc03568ea49e6122eb30ac16099e8332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c167936b859688c2a67b5354793255c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-feb42e958c7634fe9cb086413e9c27e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-079079c82908adb084a01ead9f76b424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6fa6ec4e78f72787dc8b76b64913aaa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Local-Details-to-Global-Context-Advancing-Vision-Language-Models-with-Attention-Based-Selection"><a href="#From-Local-Details-to-Global-Context-Advancing-Vision-Language-Models-with-Attention-Based-Selection" class="headerlink" title="From Local Details to Global Context: Advancing Vision-Language Models   with Attention-Based Selection"></a>From Local Details to Global Context: Advancing Vision-Language Models   with Attention-Based Selection</h2><p><strong>Authors:Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang</strong></p>
<p>Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive zero-shot capabilities on downstream tasks. Prior research highlights the crucial role of visual augmentation techniques, like random cropping, in alignment with fine-grained class descriptions generated by large language models (LLMs), significantly enhancing zero-shot performance by incorporating multi-view information. However, the inherent randomness of these augmentations can inevitably introduce background artifacts and cause models to overly focus on local details, compromising global semantic understanding. To address these issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election (\textbf{ABS}) method from local details to global context, which applies attention-guided cropping in both raw images and feature space, supplement global semantic information through strategic feature selection. Additionally, we introduce a soft matching technique to effectively filter LLM descriptions for better alignment. \textbf{ABS} achieves state-of-the-art performance on out-of-distribution generalization and zero-shot classification tasks. Notably, \textbf{ABS} is training-free and even rivals few-shot and test-time adaptation methods. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/BIT-DA/ABS%7D%7B/textcolor%7Bdarkgreen%7D%7Bhttps://github.com/BIT-DA/ABS%7D%7D">https://github.com/BIT-DA/ABS}{\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚æ—©æœŸç ”ç©¶çªå‡ºäº†è§†è§‰å¢å¼ºæŠ€æœ¯ï¼ˆå¦‚éšæœºè£å‰ªï¼‰ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç²¾ç»†ç±»åˆ«æè¿°å¯¹é½çš„å…³é”®ä½œç”¨ï¼Œé€šè¿‡èå…¥å¤šè§†å›¾ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›å¢å¼ºçš„å›ºæœ‰éšæœºæ€§ä¸å¯é¿å…åœ°ä¼šå¼•å…¥èƒŒæ™¯ä¼ªå½±ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦å…³æ³¨å±€éƒ¨ç»†èŠ‚ï¼Œä»è€ŒæŸå®³å…¨å±€è¯­ä¹‰ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å±€éƒ¨ç»†èŠ‚åˆ°å…¨å±€ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›åŸºç¡€é€‰æ‹©ï¼ˆABSï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨åŸå§‹å›¾åƒå’Œç‰¹å¾ç©ºé—´ä¸­åº”ç”¨æ³¨æ„åŠ›å¼•å¯¼è£å‰ªï¼Œé€šè¿‡æˆ˜ç•¥ç‰¹å¾é€‰æ‹©æ¥è¡¥å……å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è½¯åŒ¹é…æŠ€æœ¯ï¼Œä»¥æœ‰æ•ˆåœ°è¿‡æ»¤LLMæè¿°ï¼Œä»¥æ›´å¥½åœ°å¯¹é½ã€‚ABSåœ¨è¶…å‡ºåˆ†å¸ƒæ³›åŒ–å’Œé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒABSæ— éœ€è®­ç»ƒï¼Œç”šè‡³ä¸å°‘æ•°æ ·æœ¬å’Œæµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ç›¸åŒ¹æ•Œã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BIT-DA/ABS">https://github.com/BIT-DA/ABS</a>å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13233v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç ”ç©¶å¼ºè°ƒè§†è§‰å¢å¼ºæŠ€æœ¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç²¾ç»†ç±»åˆ«æè¿°ç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œé€šè¿‡èå…¥å¤šè§†è§’ä¿¡æ¯æ˜¾è‘—æé«˜é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¢å¼ºæŠ€æœ¯çš„éšæœºæ€§å¯èƒ½å¼•å…¥èƒŒæ™¯ä¼ªå½±ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦å…³æ³¨å±€éƒ¨ç»†èŠ‚ï¼Œå½±å“å…¨å±€è¯­ä¹‰ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºæ³¨æ„åŠ›çš„é€‰æ‹©ï¼ˆABSï¼‰æ–¹æ³•ï¼Œä»å±€éƒ¨ç»†èŠ‚è½¬å‘å…¨å±€ä¸Šä¸‹æ–‡ã€‚è¯¥æ–¹æ³•åœ¨åŸå§‹å›¾åƒå’Œç‰¹å¾ç©ºé—´åº”ç”¨æ³¨æ„åŠ›å¼•å¯¼è£å‰ªï¼Œé€šè¿‡ç­–ç•¥æ€§ç‰¹å¾é€‰æ‹©è¡¥å……å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå¼•å…¥è½¯åŒ¹é…æŠ€æœ¯æœ‰æ•ˆè¿‡æ»¤LLMæè¿°ï¼Œå®ç°æ›´å¥½çš„å¯¹é½ã€‚ABSåœ¨è¶…å‡ºåˆ†å¸ƒæ³›åŒ–å’Œé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¸”æ— éœ€è®­ç»ƒï¼Œç”šè‡³å¯ä¸å°‘æ ·æœ¬å’Œæµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ç›¸æŠ—è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
<li>è§†è§‰å¢å¼ºæŠ€æœ¯ä¸LLMç”Ÿæˆçš„ç²¾ç»†ç±»åˆ«æè¿°ç›¸ç»“åˆèƒ½æ˜¾è‘—æé«˜é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>å¢å¼ºæŠ€æœ¯çš„éšæœºæ€§å¯èƒ½å¼•å…¥èƒŒæ™¯ä¼ªå½±å’Œè¿‡åº¦å…³æ³¨å±€éƒ¨ç»†èŠ‚çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„ABSæ–¹æ³•ä»å±€éƒ¨ç»†èŠ‚è½¬å‘å…¨å±€ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åœ¨åŸå§‹å›¾åƒå’Œç‰¹å¾ç©ºé—´è¿›è¡Œè£å‰ªã€‚</li>
<li>ABSé€šè¿‡ç­–ç•¥æ€§ç‰¹å¾é€‰æ‹©è¡¥å……å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ABSé‡‡ç”¨è½¯åŒ¹é…æŠ€æœ¯è¿‡æ»¤LLMæè¿°ï¼Œå®ç°æ›´å¥½çš„æ¨¡å‹å¯¹é½ã€‚</li>
<li>ABSæ–¹æ³•åœ¨è¶…å‡ºåˆ†å¸ƒæ³›åŒ–å’Œé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä¸”æ— éœ€è®­ç»ƒï¼Œå…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a51fb4f521a0c4c8e94aa6f3b78712c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ddbea651d413c90d3f0b2739ec48f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-153bb52ae5ee95e497b63b556bfc0227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-308423443e3d2ceffd1a637da462d646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-706d9c1adcef337caf5ecfcdae458b26.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SynDec-A-Synthesize-then-Decode-Approach-for-Arbitrary-Textual-Style-Transfer-via-Large-Language-Models"><a href="#SynDec-A-Synthesize-then-Decode-Approach-for-Arbitrary-Textual-Style-Transfer-via-Large-Language-Models" class="headerlink" title="SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style   Transfer via Large Language Models"></a>SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style   Transfer via Large Language Models</h2><p><strong>Authors:Han Sun, Zhen Sun, Zongmin Zhang, Linzhao Jia, Wei Shao, Min Zhang</strong></p>
<p>Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æˆä¸ºæ–‡æœ¬é£æ ¼è½¬æ¢çš„ä¸»å¯¼åŠ›é‡ã€‚ç„¶è€Œï¼Œå¯¹äºä»»æ„é£æ ¼è½¬æ¢ï¼ŒLLMé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä¸¥é‡ä¾èµ–æ‰‹åŠ¨æ„å»ºçš„æç¤ºï¼›ï¼ˆ2ï¼‰LLMä¸­å›ºæœ‰çš„åƒµåŒ–é£æ ¼åè§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å…ˆåˆæˆåè§£ç ï¼ˆSynDecï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è‡ªåŠ¨åˆæˆé«˜è´¨é‡æç¤ºï¼Œå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­æ”¾å¤§å…¶ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„å°‘é‡æ ·æœ¬ã€è¿›è¡Œå››ç»´é£æ ¼åˆ†æä»¥åŠé‡æ–°æ’åºå€™é€‰é¡¹æ¥åˆæˆæç¤ºã€‚åœ¨LLMè§£ç é˜¶æ®µï¼Œé€šè¿‡æœ€å¤§åŒ–æœ‰åˆæˆæç¤ºå’Œæ— æç¤ºåœºæ™¯ä¹‹é—´çš„è¾“å‡ºæ¦‚ç‡å¯¹æ¯”ï¼Œä»¥åŠæç¤ºå’Œè´Ÿæ ·æœ¬ä¹‹é—´çš„å¯¹æ¯”ï¼Œæ”¾å¤§äº†TSTæ•ˆåº”ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„äº”ä¸ªä¸Šï¼ŒSynDecä¼˜äºç°æœ‰çš„åŸºäºLLMçš„æœ€å…ˆè¿›æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç°ä»£åˆ°ä¼Šä¸½èç™½æ—¶ä»£çš„è‹±è¯­è½¬æ¢ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾9%ï¼‰ã€‚è¯¦ç»†çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†SynDecçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12821v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMsåœ¨è‡ªç„¶è¯­è¨€é£æ ¼è¿ç§»é¢†åŸŸè¡¨ç°å‡ºæå¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¾èµ–æ‰‹åŠ¨æ„å»ºçš„æç¤ºå’ŒLLMsæœ¬èº«çš„é£æ ¼åè§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Synthesize-then-Decodeï¼ˆSynDecï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨åˆæˆé«˜è´¨é‡æç¤ºå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­æ”¾å¤§å…¶ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é€‰æ‹©ä»£è¡¨æ€§çš„å°‘é‡æ ·æœ¬ã€è¿›è¡Œå››ç»´é£æ ¼åˆ†æä»¥åŠé‡æ–°æ’åºå€™é€‰è€…æ¥åˆæˆæç¤ºã€‚åœ¨LLMè§£ç é˜¶æ®µï¼Œé€šè¿‡æœ€å¤§åŒ–æœ‰åˆæˆæç¤ºä¸æ— æç¤ºåœºæ™¯çš„è¾“å‡ºæ¦‚ç‡å¯¹æ¯”ï¼Œä»¥åŠæç¤ºä¸è´Ÿæ ·æœ¬ä¹‹é—´çš„å¯¹æ¯”ï¼Œæ”¾å¤§äº†TSTæ•ˆåº”ã€‚å®éªŒè¡¨æ˜ï¼ŒSynDecåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„ä¸€æµLLMæ–¹æ³•ï¼ˆä¾‹å¦‚åœ¨ä»ç°ä»£è‹±è¯­åˆ°ä¼Šä¸½èç™½æ—¶ä»£è‹±è¯­çš„è¿ç§»ä¸­å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾9ï¼…ï¼‰ã€‚è¯¦ç»†çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†SynDecçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsåœ¨è‡ªç„¶è¯­è¨€é£æ ¼è¿ç§»æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¾èµ–æ‰‹åŠ¨æç¤ºå’Œå›ºæœ‰é£æ ¼åè§ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„Synthesize-then-Decodeï¼ˆSynDecï¼‰æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨åˆæˆé«˜è´¨é‡æç¤ºå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­æ”¾å¤§å…¶ä½œç”¨ã€‚</li>
<li>SynDecæ–¹æ³•é€šè¿‡é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬ã€è¿›è¡Œå››ç»´é£æ ¼åˆ†æå’Œé‡æ–°æ’åºå€™é€‰è€…æ¥åˆæˆæç¤ºã€‚</li>
<li>åœ¨LLMè§£ç é˜¶æ®µï¼ŒSynDecé€šè¿‡æœ€å¤§åŒ–æœ‰æç¤ºä¸æ— æç¤ºåœºæ™¯çš„è¾“å‡ºæ¦‚ç‡å¯¹æ¯”æ¥æ”¾å¤§æ•ˆæœã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSynDecåœ¨å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚åœ¨ç‰¹å®šé£æ ¼è¿ç§»ä»»åŠ¡ä¸­å‡†ç¡®ç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æ¶ˆèç ”ç©¶éªŒè¯äº†SynDecæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9348dd6a7632138c7e80b76ae9fbff2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4467b68e2d1d148bab3d8b2c47d6caee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5538c3fe9a8ed518e19002f902552c0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ed643084bd8fcfeba0732a60ce89c65.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPKLIP-Aligning-Spike-Video-Streams-with-Natural-Language"><a href="#SPKLIP-Aligning-Spike-Video-Streams-with-Natural-Language" class="headerlink" title="SPKLIP: Aligning Spike Video Streams with Natural Language"></a>SPKLIP: Aligning Spike Video Streams with Natural Language</h2><p><strong>Authors:Yongchang Gao, Meiling Jin, Zhaofei Yu, Tiejun Huang, Guozhang Chen</strong></p>
<p>Spike cameras offer unique sensing capabilities but their sparse, asynchronous output challenges semantic understanding, especially for Spike Video-Language Alignment (Spike-VLA) where models like CLIP underperform due to modality mismatch. We introduce SPKLIP, the first architecture specifically for Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that adaptively models multi-scale temporal dynamics in event streams, and uses spike-text contrastive learning to directly align spike video with language, enabling effective few-shot learning. A full-spiking visual encoder variant, integrating SNN components into our pipeline, demonstrates enhanced energy efficiency. Experiments show state-of-the-art performance on benchmark spike datasets and strong few-shot generalization on a newly contributed real-world dataset. SPKLIPâ€™s energy efficiency highlights its potential for neuromorphic deployment, advancing event-based multimodal research. The source code and dataset are available at [link removed for anonymity]. </p>
<blockquote>
<p>è„‰å†²æ‘„åƒå¤´æä¾›äº†ç‹¬ç‰¹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†å…¶ç¨€ç–ã€å¼‚æ­¥çš„è¾“å‡ºç»™è¯­ä¹‰ç†è§£å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‰å†²è§†é¢‘è¯­è¨€å¯¹é½ï¼ˆSpike-VLAï¼‰æ–¹é¢ï¼ŒCLIPç­‰æ¨¡å‹ç”±äºæ¨¡æ€ä¸åŒ¹é…è€Œè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºSpike-VLAè®¾è®¡çš„ç¬¬ä¸€ä¸ªæ¶æ„SPKLIPã€‚SPKLIPé‡‡ç”¨åˆ†å±‚è„‰å†²ç‰¹å¾æå–å™¨ï¼Œè‡ªé€‚åº”åœ°æ¨¡æ‹Ÿäº‹ä»¶æµä¸­çš„å¤šå°ºåº¦æ—¶é—´åŠ¨æ€ï¼Œå¹¶ä½¿ç”¨è„‰å†²æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›´æ¥å¯¹é½è„‰å†²è§†é¢‘å’Œè¯­è¨€ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚ä¸€ç§å…¨è„‰å†²è§†è§‰ç¼–ç å™¨å˜ä½“ï¼Œå°†SNNç»„ä»¶é›†æˆåˆ°æˆ‘ä»¬çš„æµç¨‹ä¸­ï¼Œå±•ç¤ºäº†å¢å¼ºçš„èƒ½æºæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŸºå‡†è„‰å†²æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨æ–°è´¡çŒ®çš„ç°å®æ•°æ®é›†ä¸Šå…·æœ‰å¾ˆå¼ºçš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚SPKLIPçš„èƒ½æºæ•ˆç‡å‡¸æ˜¾äº†å…¶ç”¨äºç¥ç»å½¢æ€éƒ¨ç½²çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†åŸºäºäº‹ä»¶çš„å¤šåª’ä½“ç ”ç©¶çš„å‘å±•ã€‚æºä»£ç å’Œæ•°æ®é›†å¯åœ¨ï¼ˆä¸ºä¿å¯†è€Œåˆ é™¤çš„é“¾æ¥ï¼‰è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12656v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è„‰å†²è§†é¢‘è¯­è¨€å¯¹é½ï¼ˆSpike-VLAï¼‰çš„æ–°çš„æ¶æ„SPKLIPã€‚SPKLIPé‡‡ç”¨åˆ†å±‚è„‰å†²ç‰¹å¾æå–å™¨ï¼Œè‡ªé€‚åº”åœ°æ¨¡æ‹Ÿäº‹ä»¶æµä¸­çš„å¤šå°ºåº¦æ—¶é—´åŠ¨æ€ï¼Œå¹¶é€šè¿‡è„‰å†²æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›´æ¥å¯¹é½è„‰å†²è§†é¢‘å’Œè¯­è¨€ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚å¼•å…¥å…¨è„‰å†²è§†è§‰ç¼–ç å™¨å˜ä½“ï¼Œå°†è„‰å†²ç¥ç»ç½‘ç»œç»„ä»¶é›†æˆåˆ°æˆ‘ä»¬çš„ç®¡é“ä¸­ï¼Œæé«˜äº†èƒ½æ•ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è„‰å†²æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨æ–°è´¡çŒ®çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPKLIPæ˜¯é’ˆå¯¹è„‰å†²è§†é¢‘è¯­è¨€å¯¹é½ï¼ˆSpike-VLAï¼‰çš„é¦–ä¸ªä¸“é—¨æ¶æ„ã€‚</li>
<li>SPKLIPä½¿ç”¨åˆ†å±‚è„‰å†²ç‰¹å¾æå–å™¨ï¼Œè¯¥æå–å™¨èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ¨¡æ‹Ÿäº‹ä»¶æµä¸­çš„å¤šå°ºåº¦æ—¶é—´åŠ¨æ€ã€‚</li>
<li>é€šè¿‡è„‰å†²æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼ŒSPKLIPå®ç°äº†è„‰å†²è§†é¢‘ä¸è¯­è¨€çš„ç›´æ¥å¯¹é½ã€‚</li>
<li>SPKLIPæ”¯æŒæœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>å…¨è„‰å†²è§†è§‰ç¼–ç å™¨å˜ä½“çš„å¼•å…¥æé«˜äº†èƒ½æ•ˆï¼Œæ˜¾ç¤ºå‡ºåœ¨ç¥ç»å½¢æ€éƒ¨ç½²ä¸­çš„æ½œåŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜SPKLIPåœ¨è„‰å†²æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d3a2d0f27249258b94f86904dd84f0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1b2de04e1396392b6fe71d56e208032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e541bee5d5be2d1f227d87073a38d951.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7ed586859797a3fedcaeeeac70401a7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Heuristics-Generation-for-Solving-Combinatorial-Optimization-Problems-Using-Large-Language-Models"><a href="#Efficient-Heuristics-Generation-for-Solving-Combinatorial-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Efficient Heuristics Generation for Solving Combinatorial Optimization   Problems Using Large Language Models"></a>Efficient Heuristics Generation for Solving Combinatorial Optimization   Problems Using Large Language Models</h2><p><strong>Authors:Xuan Wu, Di Wang, Chunguo Wu, Lijie Wen, Chunyan Miao, Yubin Xiao, You Zhou</strong></p>
<p>Recent studies exploited Large Language Models (LLMs) to autonomously generate heuristics for solving Combinatorial Optimization Problems (COPs), by prompting LLMs to first provide search directions and then derive heuristics accordingly. However, the absence of task-specific knowledge in prompts often leads LLMs to provide unspecific search directions, obstructing the derivation of well-performing heuristics. Moreover, evaluating the derived heuristics remains resource-intensive, especially for those semantically equivalent ones, often requiring omissible resource expenditure. To enable LLMs to provide specific search directions, we propose the Hercules algorithm, which leverages our designed Core Abstraction Prompting (CAP) method to abstract the core components from elite heuristics and incorporate them as prior knowledge in prompts. We theoretically prove the effectiveness of CAP in reducing unspecificity and provide empirical results in this work. To reduce computing resources required for evaluating the derived heuristics, we propose few-shot Performance Prediction Prompting (PPP), a first-of-its-kind method for the Heuristic Generation (HG) task. PPP leverages LLMs to predict the fitness values of newly derived heuristics by analyzing their semantic similarity to previously evaluated ones. We further develop two tailored mechanisms for PPP to enhance predictive accuracy and determine unreliable predictions, respectively. The use of PPP makes Hercules more resource-efficient and we name this variant Hercules-P. Extensive experiments across four HG tasks, five COPs, and eight LLMs demonstrate that Hercules outperforms the state-of-the-art LLM-based HG algorithms, while Hercules-P excels at minimizing required computing resources. In addition, we illustrate the effectiveness of CAP, PPP, and the other proposed mechanisms by conducting relevant ablation studies. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªä¸»ç”Ÿæˆè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPï¼‰çš„å¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡æç¤ºLLMé¦–å…ˆæä¾›æœç´¢æ–¹å‘ï¼Œç„¶åæ®æ­¤æ¨å¯¼å¯å‘å¼ã€‚ç„¶è€Œï¼Œæç¤ºä¸­ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†å¾€å¾€å¯¼è‡´LLMæä¾›éç‰¹å®šçš„æœç´¢æ–¹å‘ï¼Œé˜»ç¢æ¨å¯¼é«˜æ€§èƒ½çš„å¯å‘å¼æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯„ä¼°æ¨å¯¼å‡ºçš„å¯å‘å¼æ–¹æ³•ä»ç„¶éœ€è¦æ¶ˆè€—å¤§é‡èµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¯­ä¹‰ç­‰ä»·çš„æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„èµ„æºæ”¯å‡ºã€‚ä¸ºäº†ä½¿LLMèƒ½å¤Ÿæä¾›ç‰¹å®šçš„æœç´¢æ–¹å‘ï¼Œæˆ‘ä»¬æå‡ºäº†èµ«æ‹‰å…‹æ–¯ç®—æ³•ï¼ˆHerculesï¼‰ï¼Œè¯¥ç®—æ³•åˆ©ç”¨æˆ‘ä»¬è®¾è®¡çš„æ ¸å¿ƒæŠ½è±¡æç¤ºæ³•ï¼ˆCAPï¼‰ï¼Œä»ä¼˜ç§€å¯å‘å¼æ–¹æ³•ä¸­æå–æ ¸å¿ƒç»„ä»¶å¹¶å°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†çº³å…¥æç¤ºä¸­ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†CAPåœ¨å‡å°‘éç‰¹å¼‚æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æœ¬æ–‡ä¸­æä¾›äº†å®è¯ç»“æœã€‚ä¸ºäº†é™ä½è¯„ä¼°æ´¾ç”Ÿå¯å‘å¼æ–¹æ³•æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œæˆ‘ä»¬æå‡ºäº†å¼€åˆ›æ€§çš„æ€§èƒ½é¢„æµ‹æç¤ºæ³•ï¼ˆPPPï¼‰ï¼Œè¿™æ˜¯å¯å‘å¼ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¸€ç§æ–°æ–¹æ³•ã€‚PPPåˆ©ç”¨LLMé€šè¿‡åˆ†æå…¶ä¸å·²è¯„ä¼°å¯å‘å¼æ–¹æ³•çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥é¢„æµ‹æ–°æ´¾ç”Ÿå¯å‘å¼æ–¹æ³•çš„é€‚åº”åº¦å€¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºPPPå¼€å‘äº†ä¸¤ä¸ªå®šåˆ¶æœºåˆ¶ï¼Œåˆ†åˆ«ç”¨äºæé«˜é¢„æµ‹ç²¾åº¦å’Œç¡®å®šä¸å¯é çš„é¢„æµ‹ã€‚PPPçš„ä½¿ç”¨ä½¿èµ«æ‹‰å…‹æ–¯ç®—æ³•æ›´åŠ é«˜æ•ˆï¼Œæˆ‘ä»¬å°†è¿™ç§å˜ä½“å‘½åä¸ºèµ«æ‹‰å…‹æ–¯-Pã€‚åœ¨å››ä¸ªå¯å‘å¼ç”Ÿæˆä»»åŠ¡ã€äº”ä¸ªç»„åˆä¼˜åŒ–é—®é¢˜å’Œå…«ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œèµ«æ‹‰å…‹æ–¯ç®—æ³•ä¼˜äºæœ€æ–°çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„HGç®—æ³•ï¼Œè€Œèµ«æ‹‰å…‹æ–¯-Påˆ™æ“…é•¿æœ€å°åŒ–æ‰€éœ€çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è¿›è¡Œç›¸å…³çš„æ¶ˆèç ”ç©¶ï¼Œè¯´æ˜äº†æ ¸å¿ƒæŠ½è±¡æç¤ºæ³•ï¼ˆCAPï¼‰ã€æ€§èƒ½é¢„æµ‹æç¤ºæ³•ï¼ˆPPPï¼‰ä»¥åŠå…¶ä»–æè®®æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12627v1">PDF</a> Accepted by SIGKDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªä¸»ç”Ÿæˆè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰çš„å¯å‘å¼æ–¹æ³•ã€‚é’ˆå¯¹LLMsåœ¨æä¾›æœç´¢æ–¹å‘æ—¶ç¼ºä¹ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†Heraclesç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡è®¾è®¡æ ¸å¿ƒæŠ½è±¡æç¤ºï¼ˆCAPï¼‰æ–¹æ³•ï¼Œä»ä¼˜ç§€å¯å‘å¼ä¸­æå–æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶å°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†èå…¥æç¤ºä¸­ï¼Œä»è€Œå‡å°‘éç‰¹å¼‚æ€§ã€‚ä¸ºå‡å°‘è¯„ä¼°è¡ç”Ÿå¯å‘å¼æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†æ€§èƒ½é¢„æµ‹æç¤ºï¼ˆPPPï¼‰æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§é¢„æµ‹æ–°å¯å‘å¼çš„é€‚åº”åº¦å€¼ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œè¯æ˜HeraclesåŠå…¶æ”¹è¿›ç‰ˆHeracles-Påœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«ç”¨äºè‡ªä¸»ç”Ÿæˆè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰çš„å¯å‘å¼æ–¹æ³•ã€‚</li>
<li>LLMsåœ¨æä¾›æœç´¢æ–¹å‘æ—¶å› ç¼ºä¹ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†Heraclesç®—æ³•ï¼Œé€šè¿‡æ ¸å¿ƒæŠ½è±¡æç¤ºï¼ˆCAPï¼‰æ–¹æ³•èå…¥ä¼˜ç§€å¯å‘å¼ä¸­çš„æ ¸å¿ƒç»„ä»¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>CAPç†è®ºä¸Šå‡å°‘äº†éç‰¹å¼‚æ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ºå‡å°‘è¯„ä¼°è¡ç”Ÿå¯å‘å¼æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œé¦–æ¬¡æå‡ºäº†æ€§èƒ½é¢„æµ‹æç¤ºï¼ˆPPPï¼‰æ–¹æ³•ã€‚</li>
<li>PPPé€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§é¢„æµ‹æ–°å¯å‘å¼çš„é€‚åº”åº¦å€¼ï¼Œæé«˜äº†Heraclesçš„èµ„æºæ•ˆç‡ï¼Œå½¢æˆäº†Heracles-Pã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-234ba2aba56a27047675531ebd694317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e628b01af774db00e040557d70401c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9eb0425c534aa106ef3f07f461e69a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38de57f477d2564167ebd0868ba3bdb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96e700d5dc2a556bdef1cbf464c29a2e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ProMi-An-Efficient-Prototype-Mixture-Baseline-for-Few-Shot-Segmentation-with-Bounding-Box-Annotations"><a href="#ProMi-An-Efficient-Prototype-Mixture-Baseline-for-Few-Shot-Segmentation-with-Bounding-Box-Annotations" class="headerlink" title="ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation   with Bounding-Box Annotations"></a>ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation   with Bounding-Box Annotations</h2><p><strong>Authors:Florent Chiaroni, Ali Ayub, Ola Ahmad</strong></p>
<p>In robotics applications, few-shot segmentation is crucial because it allows robots to perform complex tasks with minimal training data, facilitating their adaptation to diverse, real-world environments. However, pixel-level annotations of even small amount of images is highly time-consuming and costly. In this paper, we present a novel few-shot binary segmentation method based on bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an efficient prototype-mixture-based method that treats the background class as a mixture of distributions. Our approach is simple, training-free, and effective, accommodating coarse annotations with ease. Compared to existing baselines, ProMi achieves the best results across different datasets with significant gains, demonstrating its effectiveness. Furthermore, we present qualitative experiments tailored to real-world mobile robot tasks, demonstrating the applicability of our approach in such scenarios. Our code: <a target="_blank" rel="noopener" href="https://github.com/ThalesGroup/promi">https://github.com/ThalesGroup/promi</a>. </p>
<blockquote>
<p>åœ¨æœºå™¨äººåº”ç”¨ä¸­ï¼Œå°æ ·åˆ†å‰²ï¼ˆfew-shot segmentationï¼‰è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå…è®¸æœºå™¨äººåœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œä¿ƒè¿›å…¶é€‚åº”å¤šæ ·åŒ–çš„ç°å®ä¸–ç•Œç¯å¢ƒã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯å°‘é‡å›¾åƒçš„åƒç´ çº§æ³¨é‡Šä¹Ÿæ˜¯æå…¶è€—æ—¶å’Œæ˜‚è´µçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œæ¡†æ³¨é‡Šçš„æ–°å‹å°æ ·äºŒè¿›åˆ¶åˆ†å‰²æ–¹æ³•ï¼Œè€Œä¸æ˜¯åƒç´ çº§æ ‡ç­¾ã€‚æˆ‘ä»¬ä»‹ç»äº†ProMiï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŸå‹æ··åˆçš„æœ‰æ•ˆæ–¹æ³•ï¼Œå®ƒå°†èƒŒæ™¯ç±»è§†ä¸ºåˆ†å¸ƒçš„æ··åˆç‰©ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€å•ã€æ— éœ€è®­ç»ƒä¸”æœ‰æ•ˆï¼Œå¯ä»¥è½»æ¾é€‚åº”ç²—ç•¥æ³¨é‡Šã€‚ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼ŒProMiåœ¨ä¸åŒæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼Œå¹¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¶ç›Šï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é’ˆå¯¹ç°å®ä¸–ç•Œä¸­çš„ç§»åŠ¨æœºå™¨äººä»»åŠ¡è¿›è¡Œäº†å®šåˆ¶å®šæ€§å®éªŒï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ­¤ç±»åœºæ™¯ä¸­çš„åº”ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/ThalesGroup/promi%E3%80%82">https://github.com/ThalesGroup/promiã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12547v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œæ¡†æ ‡æ³¨çš„æ–°å‹å°‘æ ·æœ¬äºŒå…ƒåˆ†å‰²æ–¹æ³•ProMiï¼Œç”¨äºæœºå™¨äººåº”ç”¨ä¸­ã€‚è¯¥æ–¹æ³•è§£å†³äº†åƒç´ çº§æ ‡æ³¨è€—æ—¶è€—åŠ›çš„é—®é¢˜ï¼Œé€šè¿‡å°†èƒŒæ™¯ç±»è§†ä¸ºæ··åˆåˆ†å¸ƒæ¥å®ç°è®­ç»ƒå…è´¹çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ï¼Œèƒ½è½»æ¾é€‚åº”ç²—ç•¥æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ŒProMiåœ¨ä¸åŒæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ•ˆæœï¼Œå°¤å…¶åœ¨çœŸå®ä¸–ç•Œç§»åŠ¨æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œæ¡†æ ‡æ³¨çš„å°‘æ ·æœ¬äºŒå…ƒåˆ†å‰²æ–¹æ³•ProMiï¼Œé€‚ç”¨äºæœºå™¨äººåº”ç”¨ã€‚</li>
<li>ProMiè§£å†³äº†åƒç´ çº§æ ‡æ³¨æˆæœ¬é«˜å’Œè€—æ—¶çš„é—®é¢˜ã€‚</li>
<li>ProMié€šè¿‡å°†èƒŒæ™¯ç±»è§†ä¸ºæ··åˆåˆ†å¸ƒæ¥å®ç°å…¶æ–¹æ³•ã€‚</li>
<li>ProMiæ˜¯ç®€å•ä¸”è®­ç»ƒå…è´¹çš„ï¼Œå¹¶èƒ½è½»æ¾é€‚åº”ç²—ç•¥æ ‡æ³¨ã€‚</li>
<li>ProMiç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºçœŸå®ä¸–ç•Œçš„ç§»åŠ¨æœºå™¨äººä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1d933e34dccd2247e2dcd7b0a38fe2fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e68050d20781f4b34edb66766540a9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24e9266d1b433311e4965d265e818b05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e93678929b41b72e815a01921dd723c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcf2f52ac494d794b42b286cd591df1a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Relation-Extraction-or-Pattern-Matching-Unravelling-the-Generalisation-Limits-of-Language-Models-for-Biographical-RE"><a href="#Relation-Extraction-or-Pattern-Matching-Unravelling-the-Generalisation-Limits-of-Language-Models-for-Biographical-RE" class="headerlink" title="Relation Extraction or Pattern Matching? Unravelling the Generalisation   Limits of Language Models for Biographical RE"></a>Relation Extraction or Pattern Matching? Unravelling the Generalisation   Limits of Language Models for Biographical RE</h2><p><strong>Authors:Varvara Arzt, Allan Hanbury, Michael Wiegand, GÃ¡bor Recski, Terra Blevins</strong></p>
<p>Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with unseen data, even within similar domains. Notably, higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts. Our results also show that data quality, rather than lexical similarity, is key to robust transfer, and the choice of optimal adaptation strategy depends on the quality of data available: while fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data. However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results. Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability. </p>
<blockquote>
<p>å…³ç³»æŠ½å–ï¼ˆREï¼‰æ¨¡å‹çš„ä¸€èˆ¬åŒ–èƒ½åŠ›åˆ†æå¯¹äºè¯„ä¼°æ¨¡å‹æ˜¯å­¦ä¼šç¨³å¥çš„å…³ç³»æ¨¡å¼ï¼Œè¿˜æ˜¯ä¾èµ–äºå¶ç„¶çš„å…³è”è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„è·¨æ•°æ®é›†å®éªŒå‘ç°ï¼ŒREæ¨¡å‹åœ¨å¤„ç†æœªè§è¿‡çš„æ•°æ®æ—¶é¢ä¸´å›°éš¾ï¼Œå³ä½¿åœ¨ç±»ä¼¼çš„é¢†åŸŸå†…ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¾ƒé«˜çš„æ•°æ®é›†å†…éƒ¨æ€§èƒ½å¹¶ä¸è¡¨æ˜æ›´å¥½çš„å¯è½¬ç§»æ€§ï¼Œåè€Œé€šå¸¸è¡¨æ˜å¯¹ç‰¹å®šæ•°æ®é›†çš„è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œæ•°æ®è´¨é‡è€Œéè¯æ±‡ç›¸ä¼¼æ€§æ‰æ˜¯å®ç°ç¨³å¥è½¬ç§»çš„å…³é”®ï¼Œè€Œæœ€ä½³é€‚åº”ç­–ç•¥çš„é€‰æ‹©å–å†³äºå¯ç”¨çš„æ•°æ®è´¨é‡ï¼šè™½ç„¶åœ¨é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¾®è°ƒï¼ˆfine-tuningï¼‰æä¾›äº†æœ€ä½³è·¨æ•°æ®é›†æ€§èƒ½ï¼Œä½†åœ¨å™ªå£°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»¥å°‘æ•°æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ›´åŠ æœ‰æ•ˆã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé›¶å°„å‡»ï¼ˆzero-shotï¼‰åŸºçº¿æœ‰æ—¶è¶…è¿‡äº†æ‰€æœ‰è·¨æ•°æ®é›†çš„ç»“æœã€‚REåŸºå‡†æµ‹è¯•ä¸­çš„ç»“æ„æ€§é—®é¢˜ï¼ˆå¦‚æ¯ä¸ªæ ·æœ¬çš„å•å…³ç³»çº¦æŸå’Œéæ ‡å‡†åŒ–çš„è´Ÿç±»å®šä¹‰ï¼‰è¿›ä¸€æ­¥é˜»ç¢äº†æ¨¡å‹çš„å¯è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12533v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå…³ç³»æŠ½å–æ¨¡å‹ï¼ˆREï¼‰çš„æ³›åŒ–èƒ½åŠ›å¯¹äºè¯„ä¼°æ¨¡å‹æ˜¯å¦å­¦ä¹ ç¨³å¥çš„å…³ç³»æ¨¡å¼è‡³å…³é‡è¦ã€‚ç ”ç©¶å‘ç°REæ¨¡å‹åœ¨å¤„ç†æœªè§æ•°æ®æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸”é«˜æ•°æ®é›†å†…éƒ¨æ€§èƒ½å¹¶ä¸ä¸€å®šæ„å‘³ç€è‰¯å¥½çš„è¿ç§»èƒ½åŠ›ã€‚æ•°æ®è´¨é‡å¯¹ç¨³å¥è¿ç§»è‡³å…³é‡è¦ï¼Œæœ€ä½³é€‚åº”ç­–ç•¥çš„é€‰æ‹©å–å†³äºå¯ç”¨æ•°æ®çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œç»“æ„é—®é¢˜å¦‚REåŸºå‡†æµ‹è¯•ä¸­çš„å•ä¸€å…³ç³»æ ·æœ¬é™åˆ¶å’Œéæ ‡å‡†åŒ–è´Ÿé¢ç±»åˆ«å®šä¹‰è¿›ä¸€æ­¥é˜»ç¢æ¨¡å‹çš„å¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å…³ç³»æŠ½å–æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
<li>é«˜æ•°æ®é›†å†…éƒ¨æ€§èƒ½å¹¶ä¸ä¿è¯è‰¯å¥½çš„è·¨æ•°æ®é›†è¿ç§»èƒ½åŠ›ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡ã€‚</li>
<li>æ•°æ®è´¨é‡å¯¹æ¨¡å‹ç¨³å¥è¿ç§»è‡³å…³é‡è¦ï¼Œä¼˜è´¨æ•°æ®æ›´é€‚åˆç²¾ç»†è°ƒæ•´æ¨¡å‹ã€‚</li>
<li>å™ªå£°æ•°æ®æ›´é€‚åˆé‡‡ç”¨å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç­–ç•¥ã€‚</li>
<li>é›¶æ ·æœ¬åŸºçº¿åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œè¶…å‡ºæ‰€æœ‰è·¨æ•°æ®é›†ç»“æœã€‚</li>
<li>REåŸºå‡†æµ‹è¯•å­˜åœ¨ç»“æ„é—®é¢˜ï¼Œå¦‚å•ä¸€å…³ç³»æ ·æœ¬é™åˆ¶å’Œéæ ‡å‡†åŒ–è´Ÿé¢ç±»åˆ«å®šä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7478d668d869a4023ec7ffca4b80201b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec419c1660ce435823b1a617c34a1ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ed23d918b3d4defc54d6133e32db8d6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Spectral-Spatial-Self-Supervised-Learning-for-Few-Shot-Hyperspectral-Image-Classification"><a href="#Spectral-Spatial-Self-Supervised-Learning-for-Few-Shot-Hyperspectral-Image-Classification" class="headerlink" title="Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral   Image Classification"></a>Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral   Image Classification</h2><p><strong>Authors:Wenchen Chen, Yanmei Zhang, Zhongwei Xiao, Jianping Chu, Xingbo Wang</strong></p>
<p>Few-shot classification of hyperspectral images (HSI) faces the challenge of scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning (FSL) offer promising avenues to address this issue. However, existing methods often struggle to adapt to the spatial geometric diversity of HSIs and lack sufficient spectral prior knowledge. To tackle these challenges, we propose a method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification (S4L-FSC), aimed at improving the performance of few-shot HSI classification. Specifically, we first leverage heterogeneous datasets to pretrain a spatial feature extractor using a designed Rotation-Mirror Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach enables the model to learn the spatial geometric diversity of HSIs using rotation and mirroring labels as supervisory signals, while acquiring transferable spatial meta-knowledge through few-shot learning. Subsequently, homogeneous datasets are utilized to pretrain a spectral feature extractor via a combination of FSL and Masked Reconstruction Self-Supervised Learning (MR-SSL). The model learns to reconstruct original spectral information from randomly masked spectral vectors, inferring spectral dependencies. In parallel, FSL guides the model to extract pixel-level discriminative features, thereby embedding rich spectral priors into the model. This spectral-spatial pretraining method, along with the integration of knowledge from heterogeneous and homogeneous sources, significantly enhances model performance. Extensive experiments on four HSI datasets demonstrate the effectiveness and superiority of the proposed S4L-FSC approach for few-shot HSI classification. </p>
<blockquote>
<p>é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„few-shotåˆ†ç±»é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯æ ‡æ³¨æ ·æœ¬ç¨€ç¼ºã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œfew-shotå­¦ä¹ ï¼ˆFSLï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”é«˜å…‰è°±å›¾åƒçš„ç©ºé—´å‡ ä½•å¤šæ ·æ€§ï¼Œå¹¶ä¸”ç¼ºä¹è¶³å¤Ÿçš„è°±å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é«˜å…‰è°±å›¾åƒfew-shotåˆ†ç±»çš„æ–¹æ³•ï¼Œåä¸ºè°±ç©ºé—´è‡ªç›‘ç£å­¦ä¹ ï¼ˆSpectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classificationï¼Œç®€ç§°S4L-FSCï¼‰ï¼Œæ—¨åœ¨æé«˜few-shot HSIåˆ†ç±»çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å¼‚è´¨æ•°æ®é›†é¢„è®­ç»ƒä¸€ä¸ªç©ºé—´ç‰¹å¾æå–å™¨ï¼Œé‡‡ç”¨è®¾è®¡çš„æ—‹è½¬é•œåƒè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆRM-SSLï¼‰ä¸few-shotå­¦ä¹ ç›¸ç»“åˆã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ—‹è½¬å’Œé•œåƒæ ‡ç­¾ä½œä¸ºç›‘ç£ä¿¡å·æ¥å­¦ä¹ é«˜å…‰è°±å›¾åƒçš„ç©ºé—´å‡ ä½•å¤šæ ·æ€§ï¼ŒåŒæ—¶é€šè¿‡few-shotå­¦ä¹ è·å¾—å¯è½¬ç§»çš„ç©ºé—´å…ƒçŸ¥è¯†ã€‚ç„¶åï¼Œä½¿ç”¨åŒè´¨æ•°æ®é›†é¢„è®­ç»ƒä¸€ä¸ªè°±ç‰¹å¾æå–å™¨ï¼Œç»“åˆfew-shotå­¦ä¹ å’Œæ©ç é‡å»ºè‡ªç›‘ç£å­¦ä¹ ï¼ˆMR-SSLï¼‰ã€‚æ¨¡å‹å­¦ä¹ ä»éšæœºæ©ç çš„è°±å‘é‡ä¸­é‡å»ºåŸå§‹è°±ä¿¡æ¯ï¼Œæ¨æ–­è°±ä¾èµ–æ€§ã€‚åŒæ—¶ï¼Œfew-shotå­¦ä¹ æŒ‡å¯¼æ¨¡å‹æå–åƒç´ çº§åˆ«çš„åˆ¤åˆ«ç‰¹å¾ï¼Œä»è€Œå°†ä¸°å¯Œçš„è°±å…ˆéªŒåµŒå…¥æ¨¡å‹ä¸­ã€‚è¿™ç§è°±ç©ºé—´é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥åŠæ¥è‡ªå¼‚è´¨å’ŒåŒè´¨æºçš„çŸ¥è¯†çš„æ•´åˆï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å››ä¸ªHSIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„S4L-FSCæ–¹æ³•åœ¨few-shot HSIåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12482v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/Wenchen-Chen/S4L-FSC">https://github.com/Wenchen-Chen/S4L-FSC</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„å°‘é‡æ ·æœ¬åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡ç»“åˆè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ–¹æ³•æå‡ºçš„ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆâ€”â€”å…‰è°±ç©ºé—´è‡ªç›‘ç£å­¦ä¹ ç”¨äºå°‘é‡é«˜å…‰è°±å›¾åƒåˆ†ç±»ï¼ˆS4L-FSCï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å¼‚è´¨æ•°æ®é›†è¿›è¡Œç©ºé—´ç‰¹å¾æå–å™¨çš„é¢„è®­ç»ƒï¼Œå¹¶ç»“åˆæ—‹è½¬é•œåƒè‡ªç›‘ç£å­¦ä¹ å’Œå°‘é‡å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¼šé«˜å…‰è°±å›¾åƒçš„ç©ºé—´å‡ ä½•å¤šæ ·æ€§ã€‚æ¥ç€ä½¿ç”¨åŒè´¨æ•°æ®é›†è¿›è¡Œå…‰è°±ç‰¹å¾æå–å™¨çš„é¢„è®­ç»ƒï¼Œç»“åˆå°‘é‡å­¦ä¹ å’Œæ©è†œé‡å»ºè‡ªç›‘ç£å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¹ é‡å»ºåŸå§‹å…‰è°±ä¿¡æ¯å¹¶æå–åƒç´ çº§åˆ¤åˆ«ç‰¹å¾ã€‚é€šè¿‡æ•´åˆæ¥è‡ªå¼‚è´¨å’ŒåŒè´¨æ¥æºçš„çŸ¥è¯†ï¼ŒS4L-FSCæ–¹æ³•èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œåœ¨å››ä¸ªHSIæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„å°‘é‡æ ·æœ¬åˆ†ç±»æŒ‘æˆ˜ï¼Œè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ˜¯æœ‰æ•ˆçš„è§£å†³æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”HSIsçš„ç©ºé—´å‡ ä½•å¤šæ ·æ€§ï¼Œç¼ºä¹è¶³å¤Ÿçš„è°±å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æå‡ºçš„S4L-FSCæ–¹æ³•é€šè¿‡ç»“åˆç©ºé—´ç‰¹å¾æå–å’Œè°±ç‰¹å¾æå–ï¼Œæ—¨åœ¨æé«˜å°‘æ ·æœ¬HSIåˆ†ç±»çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¼‚è´¨æ•°æ®é›†è¿›è¡Œç©ºé—´ç‰¹å¾æå–å™¨çš„é¢„è®­ç»ƒï¼Œç»“åˆæ—‹è½¬é•œåƒè‡ªç›‘ç£å­¦ä¹ å’Œå°‘é‡å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¼šç©ºé—´å‡ ä½•å¤šæ ·æ€§ã€‚</li>
<li>åˆ©ç”¨åŒè´¨æ•°æ®é›†è¿›è¡Œå…‰è°±ç‰¹å¾æå–å™¨çš„é¢„è®­ç»ƒï¼Œç»“åˆå°‘é‡å­¦ä¹ å’Œæ©è†œé‡å»ºè‡ªç›‘ç£å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¹ é‡å»ºåŸå§‹å…‰è°±ä¿¡æ¯å’Œæå–åƒç´ çº§åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>S4L-FSCæ–¹æ³•æ•´åˆæ¥è‡ªå¼‚è´¨å’ŒåŒè´¨æ¥æºçš„çŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f89306cd7c1708bd063ea02c3f82086.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50dc36d58490ee96ef50d8b9eefca537.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a812ad0ee8b74ab88eda9bfde4124347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d72c72203ce2cc00f50ea0d6f1ef791c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6ca3e91f85fc69c788f0c69b04c64fe.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CLIP-aware-Domain-Adaptive-Super-Resolution"><a href="#CLIP-aware-Domain-Adaptive-Super-Resolution" class="headerlink" title="CLIP-aware Domain-Adaptive Super-Resolution"></a>CLIP-aware Domain-Adaptive Super-Resolution</h2><p><strong>Authors:Zhengyang Lu, Qian Xia, Weifan Wang, Feng Wang</strong></p>
<p>This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a novel framework that addresses the critical challenge of domain generalization in single image super-resolution. By leveraging the semantic capabilities of CLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented performance across diverse domains and extreme scaling factors. The proposed method integrates CLIP-guided feature alignment mechanism with a meta-learning inspired few-shot adaptation strategy, enabling efficient knowledge transfer and rapid adaptation to target domains. A custom domain-adaptive module processes CLIP features alongside super-resolution features through a multi-stage transformation process, including CLIP feature processing, spatial feature generation, and feature fusion. This intricate process ensures effective incorporation of semantic information into the super-resolution pipeline. Additionally, CDASR employs a multi-component loss function that combines pixel-wise reconstruction, perceptual similarity, and semantic consistency. Extensive experiments on benchmark datasets demonstrate CDASRâ€™s superiority, particularly in challenging scenarios. On the Urban100 dataset at $\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over existing methods, with even larger improvements of up to 0.30dB observed at $\times$16 scaling. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†CLIPæ„ŸçŸ¥åŸŸè‡ªé€‚åº”è¶…åˆ†è¾¨ç‡ï¼ˆCDASRï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è§£å†³äº†å•å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸä¸­çš„åŸŸæ³›åŒ–è¿™ä¸€å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰çš„è¯­ä¹‰èƒ½åŠ›ï¼ŒCDASRåœ¨ä¸åŒåŸŸå’Œæç«¯ç¼©æ”¾å› å­ä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•å°†CLIPå¼•å¯¼çš„ç‰¹å¾å¯¹é½æœºåˆ¶ä¸å—å…ƒå­¦ä¹ å¯å‘çš„å°‘æ ·æœ¬è‡ªé€‚åº”ç­–ç•¥ç›¸ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆçš„çŸ¥è¯†è½¬ç§»å’Œå¿«é€Ÿé€‚åº”ç›®æ ‡åŸŸã€‚ä¸€ä¸ªå®šåˆ¶çš„åŸŸè‡ªé€‚åº”æ¨¡å—é€šè¿‡å¤šé˜¶æ®µè½¬æ¢è¿‡ç¨‹å¤„ç†CLIPç‰¹å¾å’Œè¶…åˆ†è¾¨ç‡ç‰¹å¾ï¼ŒåŒ…æ‹¬CLIPç‰¹å¾å¤„ç†ã€ç©ºé—´ç‰¹å¾ç”Ÿæˆå’Œç‰¹å¾èåˆã€‚è¿™ä¸€ç²¾ç»†çš„è¿‡ç¨‹ç¡®ä¿äº†è¯­ä¹‰ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥è¶…åˆ†è¾¨ç‡ç®¡é“ã€‚æ­¤å¤–ï¼ŒCDASRé‡‡ç”¨å¤šç»„ä»¶æŸå¤±å‡½æ•°ï¼Œç»“åˆäº†åƒç´ çº§çš„é‡å»ºã€æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†CDASRçš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚åœ¨Urban100æ•°æ®é›†ä¸Šï¼Œä»¥Ã—8çš„ç¼©æ”¾æ¯”ä¾‹ï¼ŒCDASRè¾ƒç°æœ‰æ–¹æ³•å®ç°äº†0.15dBçš„PSNRå¢ç›Šï¼Œè€Œåœ¨Ã—16çš„ç¼©æ”¾æ¯”ä¾‹ä¸‹ï¼Œç”šè‡³è§‚å¯Ÿåˆ°äº†é«˜è¾¾0.30dBçš„æ›´å¤§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12391v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥å·¥ä½œæå‡ºäº†CLIPæ„ŸçŸ¥åŸŸè‡ªé€‚åº”è¶…åˆ†è¾¨ç‡ï¼ˆCDASRï¼‰æ¡†æ¶ï¼Œè§£å†³äº†å•å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„åŸŸæ³›åŒ–é—®é¢˜ã€‚ç»“åˆCLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰çš„è¯­ä¹‰èƒ½åŠ›ï¼ŒCDASRåœ¨ä¸åŒåŸŸå’Œæç«¯ç¼©æ”¾å› å­ä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡CLIPå¼•å¯¼çš„ç‰¹å¾å¯¹é½æœºåˆ¶å’Œå…ƒå­¦ä¹ é©±åŠ¨çš„å°æ ·æœ¬è‡ªé€‚åº”ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„çŸ¥è¯†è½¬ç§»å’Œå¿«é€Ÿé€‚åº”ç›®æ ‡åŸŸçš„èƒ½åŠ›ã€‚å¤šé˜¶æ®µè½¬æ¢è¿‡ç¨‹åŒ…æ‹¬CLIPç‰¹å¾å¤„ç†ã€ç©ºé—´ç‰¹å¾ç”Ÿæˆå’Œç‰¹å¾èåˆï¼Œç¡®ä¿è¯­ä¹‰ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥è¶…åˆ†è¾¨ç‡ç®¡é“ä¸­ã€‚æ­¤å¤–ï¼ŒCDASRé‡‡ç”¨å¤šç»„ä»¶æŸå¤±å‡½æ•°ï¼Œç»“åˆåƒç´ çº§é‡å»ºã€æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCDASRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨Urban100æ•°æ®é›†ä¸Šæ”¾å¤§å…«å€æ—¶ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPSNRå¢ç›Šæé«˜äº†é«˜è¾¾0.15dBã€‚åœ¨æ”¾å¤§å€æ•°æ›´é«˜æ—¶ï¼Œå¦‚æ”¾å¤§åå…­å€æ—¶ï¼Œå…¶æ€§èƒ½æå‡æ›´å¤§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ„ŸçŸ¥åŸŸè‡ªé€‚åº”è¶…åˆ†è¾¨ç‡ï¼ˆCDASRï¼‰æ¡†æ¶è§£å†³äº†å•å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„åŸŸæ³›åŒ–æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨CLIPçš„è¯­ä¹‰èƒ½åŠ›å®ç°è·¨ä¸åŒåŸŸçš„å‡ºè‰²æ€§èƒ½ã€‚</li>
<li>é€šè¿‡CLIPå¼•å¯¼çš„ç‰¹å¾å¯¹é½å’Œå…ƒå­¦ä¹ é©±åŠ¨çš„å°æ ·æœ¬è‡ªé€‚åº”ç­–ç•¥å®ç°å¿«é€Ÿé€‚åº”ç›®æ ‡åŸŸã€‚</li>
<li>å¤šé˜¶æ®µè½¬æ¢è¿‡ç¨‹ç¡®ä¿è¯­ä¹‰ä¿¡æ¯èå…¥è¶…åˆ†è¾¨ç‡å¤„ç†ä¸­ã€‚</li>
<li>é‡‡ç”¨å¤šç»„ä»¶æŸå¤±å‡½æ•°ï¼Œå¹³è¡¡åƒç´ çº§é‡å»ºã€æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜CDASRåœ¨æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6daf57c6bec3a05d0570b1311607ca0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36271ef32fb6d4b003fe66b7ae4e3674.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51daf11bce87b879096e90d5d8446d05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0624afd23ee64c1e55e7e31b06941a4b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLMSR-XLLM25-An-Empirical-Study-of-LLM-for-Structural-Reasoning"><a href="#LLMSR-XLLM25-An-Empirical-Study-of-LLM-for-Structural-Reasoning" class="headerlink" title="LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning"></a>LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning</h2><p><strong>Authors:Xinye Li, Mingqi Wan, Dianbo Sui</strong></p>
<p>We present Team asdfo123â€™s submission to the LLMSR@XLLM25 shared task, which evaluates large language models on producing fine-grained, controllable, and interpretable reasoning processes. Systems must extract all problem conditions, decompose a chain of thought into statement-evidence pairs, and verify the logical validity of each pair. Leveraging only the off-the-shelf Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that first enumerates all conditions and then guides the model to label, cite, and adjudicate every reasoning step. A lightweight post-processor based on regular expressions normalises spans and enforces the official JSON schema. Without fine-tuning, external retrieval, or ensembling, our method ranks 5th overall, achieving macro F1 scores on par with substantially more complex and resource-consuming pipelines. We conclude by analysing the strengths and limitations of our approach and outlining directions for future research in structural reasoning with LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/asdfo123/LLMSR-asdfo123">https://github.com/asdfo123/LLMSR-asdfo123</a>. </p>
<blockquote>
<p>æˆ‘ä»¬åœ¨æ­¤ä»‹ç»Team asdfo123åœ¨LLMSR@XLLM25å…±äº«ä»»åŠ¡ä¸­çš„æäº¤å†…å®¹ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç²¾ç»†ã€å¯æ§ã€å¯è§£é‡Šæ¨ç†è¿‡ç¨‹æ–¹é¢çš„èƒ½åŠ›ã€‚ç³»ç»Ÿå¿…é¡»æå–æ‰€æœ‰é—®é¢˜æ¡ä»¶ï¼Œå°†æ€ç»´é“¾æ¡åˆ†è§£æˆé™ˆè¿°-è¯æ®å¯¹ï¼Œå¹¶éªŒè¯æ¯å¯¹é€»è¾‘çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä»…åˆ©ç”¨ç°æˆçš„Meta-Llama-3-8B-Instructï¼Œè®¾è®¡äº†ä¸€ä¸ªç®€æ´çš„å°‘é‡å¤šè½®æç¤ºï¼Œé¦–å…ˆåˆ—ä¸¾æ‰€æœ‰æ¡ä»¶ï¼Œç„¶åå¼•å¯¼æ¨¡å‹å¯¹æ¯ä¸€æ­¥è¿›è¡Œæ ‡æ³¨ã€å¼•ç”¨å’Œè£å†³ã€‚åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„è½»é‡çº§åå¤„ç†å™¨ç”¨äºæ ‡å‡†åŒ–è·¨åº¦å¹¶å¼ºåˆ¶æ‰§è¡Œå®˜æ–¹JSONæ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€å¾®è°ƒã€å¤–éƒ¨æ£€ç´¢æˆ–é›†æˆï¼Œæ€»ä½“æ’åç¬¬äº”ï¼Œå®è§‚F1åˆ†æ•°ä¸æ›´å¤æ‚å’Œèµ„æºå¯†é›†å‹çš„ç®¡é“ç›¸å½“ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥åœ¨ç»“æ„æ¨ç†ä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/asdfo123/LLMSR-asdfo123%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/asdfo123/LLMSR-asdfo123æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12328v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¢é˜Ÿasdfo123åœ¨LLMSR@XLLM25å…±äº«ä»»åŠ¡ä¸­çš„æäº¤å†…å®¹ï¼Œå±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç²¾ç»†ç²’åº¦ã€å¯æ§ä¸”å¯è§£é‡Šæ¨ç†è¿‡ç¨‹æ–¹é¢çš„è¯„ä¼°ã€‚å›¢é˜Ÿåˆ©ç”¨ç°æˆçš„Meta-Llama-3-8B-Instructæ¨¡å‹ï¼Œé€šè¿‡ç®€æ´çš„å°‘é‡å¤šè½®æç¤ºï¼Œåˆ—ä¸¾æ‰€æœ‰æ¡ä»¶ï¼Œå¹¶æŒ‡å¯¼æ¨¡å‹å¯¹æ¯ä¸€æ­¥æ¨ç†è¿›è¡Œæ ‡æ³¨ã€å¼•ç”¨å’Œè£å†³ã€‚åŸºäºæ­£åˆ™è¡¨è¾¾å¼è½»é‡çº§åå¤„ç†å™¨å¯¹è·¨åº¦è¿›è¡Œæ ‡å‡†åŒ–å¹¶å¼ºåˆ¶æ‰§è¡Œå®˜æ–¹JSONæ¨¡å¼ã€‚æ— éœ€å¾®è°ƒã€å¤–éƒ¨æ£€ç´¢æˆ–é›†æˆæŠ€æœ¯ï¼Œè¯¥æ–¹æ³•æ€»ä½“æ’åç¬¬äº”ï¼Œå®è§‚F1åˆ†æ•°ä¸æ›´å¤æ‚å’Œèµ„æºå¯†é›†å‹çš„ç®¡é“ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¢é˜Ÿasdfo123åœ¨LLMSR@XLLM25ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä»–ä»¬åˆ©ç”¨Meta-Llama-3-8B-Instructæ¨¡å‹ï¼Œé€šè¿‡å°‘é‡å¤šè½®æç¤ºè¿›è¡Œæ¨ç†ã€‚</li>
<li>ç³»ç»Ÿå¿…é¡»æå–é—®é¢˜æ¡ä»¶ï¼Œåˆ†è§£æ€ç»´é“¾æˆè¯­å¥è¯æ®å¯¹ï¼Œå¹¶éªŒè¯æ¯å¯¹çš„é€»è¾‘æœ‰æ•ˆæ€§ã€‚</li>
<li>å›¢é˜Ÿä½¿ç”¨åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„è½»é‡çº§åå¤„ç†å™¨æ¥æ ‡å‡†åŒ–è¾“å‡ºå¹¶éµå®ˆå®˜æ–¹JSONæ¨¡å¼ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¾®è°ƒã€å¤–éƒ¨æ£€ç´¢æˆ–é›†æˆæŠ€æœ¯ï¼Œæ€»ä½“æ’åç¬¬äº”ã€‚</li>
<li>å®è§‚F1åˆ†æ•°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸æ›´å¤æ‚çš„ç®¡é“ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb520296f36d126fbcd6c45dd82d1340.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4c3937745e16ccc6b8eea2a7acce8e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-730d5087fa9a955e192c54b4d89d56c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1be938e17b7a5b3ea3827ee61831459d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bridging-Generative-and-Discriminative-Learning-Few-Shot-Relation-Extraction-via-Two-Stage-Knowledge-Guided-Pre-training"><a href="#Bridging-Generative-and-Discriminative-Learning-Few-Shot-Relation-Extraction-via-Two-Stage-Knowledge-Guided-Pre-training" class="headerlink" title="Bridging Generative and Discriminative Learning: Few-Shot Relation   Extraction via Two-Stage Knowledge-Guided Pre-training"></a>Bridging Generative and Discriminative Learning: Few-Shot Relation   Extraction via Two-Stage Knowledge-Guided Pre-training</h2><p><strong>Authors:Quanjiang Guo, Jinchuan Zhang, Sijie Wang, Ling Tian, Zhao Kang, Bin Yan, Weidong Xiao</strong></p>
<p>Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE through in-context learning (ICL), their general-purpose training objectives often result in suboptimal performance for task-specific relation extraction. To overcome these challenges, we propose TKRE (Two-Stage Knowledge-Guided Pre-training for Relation Extraction), a novel framework that synergistically integrates LLMs with traditional relation extraction models, bridging generative and discriminative learning paradigms. TKRE introduces two key innovations: (1) leveraging LLMs to generate explanation-driven knowledge and schema-constrained synthetic data, addressing the issue of data scarcity; and (2) a two-stage pre-training strategy combining Masked Span Language Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational reasoning and generalization. Together, these components enable TKRE to effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in FSRE and underscoring its potential for broader application in low-resource scenarios. \footnote{The code and data are released on <a target="_blank" rel="noopener" href="https://github.com/UESTC-GQJ/TKRE">https://github.com/UESTC-GQJ/TKRE</a>. </p>
<blockquote>
<p>é¢å‘å°æ ·æœ¬å…³ç³»æŠ½å–ï¼ˆFSREï¼‰ä¾ç„¶æ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ï¼ŒåŸå› åœ¨äºæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºä»¥åŠç°æœ‰æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨FSREä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬çš„ä¸€èˆ¬æ€§è®­ç»ƒç›®æ ‡å¾€å¾€å¯¼è‡´é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å…³ç³»æŠ½å–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TKREï¼ˆå…³ç³»æŠ½å–çš„ä¸¤é˜¶æ®µçŸ¥è¯†å¼•å¯¼é¢„è®­ç»ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ•´åˆLLMå’Œä¼ ç»Ÿå…³ç³»æŠ½å–æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œèåˆäº†ç”Ÿæˆå¼å’Œåˆ¤åˆ«å¼å­¦ä¹ èŒƒå¼ã€‚TKREå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰åˆ©ç”¨LLMç”Ÿæˆè§£é‡Šé©±åŠ¨çš„çŸ¥è¯†å’Œæ¨¡å¼çº¦æŸçš„åˆæˆæ•°æ®ï¼Œè§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼›ï¼ˆ2ï¼‰ç»“åˆæ©ç è·¨åº¦è¯­è¨€å»ºæ¨¡ï¼ˆMSLMï¼‰å’Œè·¨åº¦çº§åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆSCLï¼‰çš„ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºå…³ç³»æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œä½¿TKREèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹FSREä»»åŠ¡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¯æ˜äº†TKREçš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†FSREçš„æœ€æ–° state-of-the-art æ€§èƒ½ï¼Œå¹¶çªæ˜¾äº†å…¶åœ¨ä½èµ„æºåœºæ™¯ä¸­çš„æ›´å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12236v1">PDF</a> 13 pages, 6 figures, Appear on IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºTKREçš„æ–°çš„å…³ç³»æå–æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿå…³ç³»æå–æ¨¡å‹ï¼Œå…‹æœäº†æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™çš„æŒ‘æˆ˜ã€‚TKREå¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè§£é‡Šé©±åŠ¨çš„çŸ¥è¯†å’Œæ¨¡å¼çº¦æŸçš„åˆæˆæ•°æ®ï¼Œä»¥åŠç»“åˆé®ç½©è·¨åº¦è¯­è¨€å»ºæ¨¡å’Œè·¨åº¦çº§åˆ«å¯¹æ¯”å­¦ä¹ çš„ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ã€‚è¿™äº›ç»„ä»¶å…±åŒä½¿TKREèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å°‘é‡å°„å‡»å…³ç³»æå–ä»»åŠ¡ï¼Œå¹¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TKREæ˜¯ä¸€ä¸ªæ–°çš„å…³ç³»æå–æ¡†æ¶ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿå…³ç³»æå–æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>LLMsåœ¨TKREä¸­ç”¨äºç”Ÿæˆè§£é‡Šé©±åŠ¨çš„çŸ¥è¯†å’Œæ¨¡å¼çº¦æŸçš„åˆæˆæ•°æ®ï¼Œä»¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>TKREé‡‡ç”¨ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é®ç½©è·¨åº¦è¯­è¨€å»ºæ¨¡å’Œè·¨åº¦çº§åˆ«å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>è¿™äº›ç­–ç•¥å¢å¼ºäº†TKREçš„å…³ç³»æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>TKREåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨å°‘é‡å°„å‡»å…³ç³»æå–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>TKREçš„æ½œåœ¨åº”ç”¨ä¸ä»…é™äºå…³ç³»æå–ï¼Œè¿˜å¯åº”ç”¨äºä½èµ„æºåœºæ™¯ä¸­çš„æ›´å¹¿æ³›ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25c077124b47bc2f7a35486a353b52cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b24325af0803ea307f3e5ee225aa113d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3108c40fa0e3982e0ad052716e5515a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28491e8426028a8a88104ed1e5b9a38c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2747e574d0791a7ec7e0a52509511085.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d2f895750c0a4eae8a7d34a92038b48.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning"><a href="#Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning" class="headerlink" title="Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning"></a>Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning</h2><p><strong>Authors:Shaobo Wang, Ziming Wang, Xiangqi Jin, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang</strong></p>
<p>Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the modelâ€™s predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup. </p>
<blockquote>
<p>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ˜¯æœ‰æ•ˆéƒ¨ç½²å®ƒä»¬çš„å…³é”®ã€‚éšç€æ•°æ®é›†è§„æ¨¡çš„æ‰©å¤§ï¼Œé«˜æ•ˆé€‰æ‹©æœ€ä½³å­é›†è¿›è¡Œè®­ç»ƒå¯¹äºå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç›®æ ‡æ•°æ®é›†å¯¹è¯„åˆ†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¸ä»…è€—æ—¶è€Œä¸”èµ„æºå¯†é›†ï¼Œæˆ–è€…ä¾èµ–äºæ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„å¯å‘å¼æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Data Whispererï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å°‘é‡ä¸Šä¸‹æ–‡å†…å­¦ä¹ æ¥å¯¹è¦è¿›è¡Œå¾®è°ƒæ¨¡å‹çš„ã€‚æˆ‘ä»¬åœ¨åŸå§‹å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œæ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒData Whispererä»…ä½¿ç”¨10%çš„æ•°æ®ä¾¿åœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šå®ç°äº†ç›¸è¾ƒäºGSM8Kæ•°æ®é›†çš„æ›´ä½³æ€§èƒ½ï¼Œå¹¶ä¸”ç›¸å¯¹äºç°æœ‰æ–¹æ³•å®ç°äº†3.1ä¸ªç‚¹çš„æå‡å’Œ7.4å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12212v1">PDF</a> Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯¹äºå…¶æœ‰æ•ˆéƒ¨ç½²è‡³å…³é‡è¦ã€‚éšç€æ•°æ®é›†è§„æ¨¡çš„æ‰©å¤§ï¼Œä¸ºäº†å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ï¼Œé€‰æ‹©æœ€ä½³çš„å­é›†è¿›è¡Œè®­ç»ƒå˜å¾—å°¤ä¸ºé‡è¦ã€‚é’ˆå¯¹ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Data Whispereræ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œåˆ©ç”¨å°‘é‡æ•°æ®ç»“åˆå¾…å¾®è°ƒæ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡åœ¨ä¸åŒçš„ä»»åŠ¡å’Œæ¨¡å‹ä¸Šè¿›è¡Œçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒData Whispereråœ¨ä»…ä½¿ç”¨å°‘é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºGSM8Kæ•°æ®é›†å®ç°äº†æ›´ä½³æ€§èƒ½æå‡ã€‚å°¤å…¶æ˜¯åœ¨å¯¹Llama-3-8B-Instructæ¨¡å‹çš„è¯„ä¼°ä¸­ï¼Œä½¿ç”¨ä»…10%çš„æ•°æ®å®ç°äº†æ˜¾è‘—è¶…è¶Šï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å®ç°äº†3.1ç‚¹çš„æ€§èƒ½æå‡å’Œé«˜è¾¾7.4å€çš„æé€Ÿã€‚è¿™ä¸€æ–°æ–¹æ³•æ—¢å¿«é€Ÿåˆæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„å¾®è°ƒè‡³å…³é‡è¦ï¼Œä½†æ•°æ®é›†è§„æ¨¡çš„æ‰©å¤§å¸¦æ¥äº†å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•éœ€è¦é•¿æ—¶é—´ç²¾ç»†è°ƒæ•´è¯„åˆ†æ¨¡å‹æˆ–ä¾èµ–ä¸èƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„å¯å‘å¼æ–¹æ³•ã€‚</li>
<li>Data Whispereræ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¾…å¾®è°ƒæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œå°‘é‡æ•°æ®è®­ç»ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-696da886d7f18fa9fb88c584ffd5c5cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b45dc95a439d1d199d01bbda0ae96514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d50cfb0cae1e151984528fcd02d7c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ec03c60ec22094a18bcbeca4944c4a5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Do-different-prompting-methods-yield-a-common-task-representation-in-language-models"><a href="#Do-different-prompting-methods-yield-a-common-task-representation-in-language-models" class="headerlink" title="Do different prompting methods yield a common task representation in   language models?"></a>Do different prompting methods yield a common task representation in   language models?</h2><p><strong>Authors:Guy Davidson, Todd M. Gureckis, Brenden M. Lake, Adina Williams</strong></p>
<p>Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through function vectors, recently proposed as a mechanism to extract few-shot ICL task representations. We generalize function vectors to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task presentations do not induce a common task representation but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining textual instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms. </p>
<blockquote>
<p>æ¼”ç¤ºå’ŒæŒ‡ä»¤æ˜¯ä¿ƒä½¿è¯­è¨€æ¨¡å‹æ‰§è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡çš„ä¸»è¦ä¸¤ç§æ–¹æ³•ã€‚é€šè¿‡ä¸åŒçš„æ–¹å¼å‘ˆç°ç›¸åŒçš„ä»»åŠ¡æ˜¯å¦ä¼šå¯¼è‡´ä»»åŠ¡è¡¨ç¤ºçš„ç›¸ä¼¼æ€§ï¼Ÿå¯¹ä»»åŠ¡è¡¨ç¤ºæœºåˆ¶æœ‰æ›´å¥½çš„ç†è§£å¯ä»¥æä¾›è§£é‡Šæ€§æ´å¯Ÿï¼Œå¹¶å¯èƒ½æœ‰åŠ©äºå¼•å¯¼æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡åŠŸèƒ½å‘é‡è¿›è¡Œç ”ç©¶ï¼Œè¯¥åŠŸèƒ½å‘é‡æœ€è¿‘è¢«æå‡ºä½œä¸ºæå–å°‘é‡ICLä»»åŠ¡è¡¨ç¤ºçš„æœºåˆ¶ã€‚æˆ‘ä»¬å°†åŠŸèƒ½å‘é‡æ¨å¹¿åˆ°æ›¿ä»£ä»»åŠ¡å±•ç¤ºï¼Œä¾§é‡äºç®€çŸ­çš„æ–‡æœ¬æŒ‡ä»¤æç¤ºï¼Œå¹¶æˆåŠŸæå–å‡ºæŒ‡ä»¤åŠŸèƒ½å‘é‡ï¼Œæé«˜äº†é›¶å°„å‡»ä»»åŠ¡å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºæ¼”ç¤ºå’ŒæŒ‡ä»¤çš„åŠŸèƒ½å‘é‡åˆ©ç”¨ä¸åŒçš„æ¨¡å‹ç»„ä»¶ï¼Œå¹¶æä¾›äº†ä¸€äº›æ§åˆ¶æ–¹æ³•æ¥åˆ†ç¦»å®ƒä»¬å¯¹ä»»åŠ¡æ€§èƒ½çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸åŒçš„ä»»åŠ¡å±•ç¤ºå¹¶æ²¡æœ‰å½¢æˆé€šç”¨çš„ä»»åŠ¡è¡¨ç¤ºï¼Œè€Œæ˜¯å¼•å‘äº†éƒ¨åˆ†é‡å çš„ä¸åŒæœºåˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç»“åˆæ–‡æœ¬æŒ‡ä»¤å’Œä»»åŠ¡æ¼”ç¤ºçš„å®è·µæä¾›äº†åŸåˆ™æ”¯æŒï¼Œæš—ç¤ºäº†åœ¨å„ç§å‘ˆç°å½¢å¼ä¸­æ™®éç›‘æµ‹ä»»åŠ¡æ¨æ–­çš„æŒ‘æˆ˜æ€§ï¼Œå¹¶é¼“åŠ±è¿›ä¸€æ­¥è€ƒå¯ŸLLMçš„ä»»åŠ¡æ¨æ–­æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12075v1">PDF</a> 9 pages, 4 figures; under review</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†é€šè¿‡ä¸åŒæ–¹å¼å‘ˆç°ç›¸åŒä»»åŠ¡æ—¶ï¼Œè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡è¡¨ç°ã€‚é€šè¿‡åŠŸèƒ½å‘é‡è¿™ä¸€æœºåˆ¶ï¼Œå¯¹åŸºäºæ¼”ç¤ºå’ŒæŒ‡ä»¤çš„ä»»åŠ¡è¡¨ç¤ºè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶æˆåŠŸæå–äº†æŒ‡ä»¤åŠŸèƒ½å‘é‡ï¼Œæé«˜äº†é›¶æ ·æœ¬ä»»åŠ¡å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºæ¼”ç¤ºå’ŒæŒ‡ä»¤çš„åŠŸèƒ½å‘é‡åˆ©ç”¨ä¸åŒçš„æ¨¡å‹ç»„ä»¶ï¼Œå¯¹ä»»åŠ¡æ€§èƒ½åšå‡ºäº†ä¸åŒçš„è´¡çŒ®ã€‚ç»“æœæš—ç¤ºä¸åŒçš„ä»»åŠ¡å‘ˆç°æ–¹å¼ä¸ä¼šå½¢æˆå…±åŒçš„ä»»åŠ¡è¡¨ç¤ºï¼Œè€Œæ˜¯æ¿€å‘éƒ¨åˆ†é‡å çš„æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¼”ç¤ºå’ŒæŒ‡ä»¤æ˜¯ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œç”¨äºå¼•å¯¼è¯­è¨€æ¨¡å‹æ‰§è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ï¼ˆICLï¼‰ã€‚</li>
<li>é€šè¿‡åŠŸèƒ½å‘é‡æœºåˆ¶ï¼Œç ”ç©¶äº†ä¸åŒä»»åŠ¡å‘ˆç°æ–¹å¼å¯¹è¯­è¨€æ¨¡å‹ä»»åŠ¡è¡¨ç°çš„å½±å“ã€‚</li>
<li>æˆåŠŸæå–äº†æŒ‡ä»¤åŠŸèƒ½å‘é‡ï¼Œæé«˜äº†é›¶æ ·æœ¬ä»»åŠ¡å‡†ç¡®æ€§ã€‚</li>
<li>æ¼”ç¤ºå’ŒæŒ‡ä»¤åŸºäºçš„åŠŸèƒ½å‘é‡åˆ©ç”¨ä¸åŒçš„æ¨¡å‹ç»„ä»¶ã€‚</li>
<li>ä¸åŒçš„ä»»åŠ¡å‘ˆç°æ–¹å¼ä¸ä¼šå½¢æˆå…±åŒçš„ä»»åŠ¡è¡¨ç¤ºï¼Œè€Œæ˜¯æ¿€å‘éƒ¨åˆ†é‡å çš„æœºåˆ¶ã€‚</li>
<li>ç»“åˆæ–‡æœ¬æŒ‡ä»¤å’Œä»»åŠ¡æ¼”ç¤ºçš„å®è·µå¾—åˆ°äº†æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95a9e7143713810c78052929949c4d9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4379d232fdded1fa4ac1ab6d5a05d300.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18046737cb7dfa4f8cea85ff3ed197b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb56b643a3e3f4e708136c42728b6463.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Cross-Model-Transfer-of-Task-Vectors-via-Few-Shot-Orthogonal-Alignment"><a href="#Cross-Model-Transfer-of-Task-Vectors-via-Few-Shot-Orthogonal-Alignment" class="headerlink" title="Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment"></a>Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment</h2><p><strong>Authors:Kazuhiko Kawamoto, Atsuhiro Endo, Hiroshi Kera</strong></p>
<p>Task arithmetic enables efficient model editing by representing task-specific changes as vectors in parameter space. Task arithmetic typically assumes that the source and target models are initialized from the same pre-trained parameters. This assumption limits its applicability in cross-model transfer settings, where models are independently pre-trained on different datasets. To address this challenge, we propose a method based on few-shot orthogonal alignment, which aligns task vectors to the parameter space of a differently pre-trained target model. These transformations preserve key properties of task vectors, such as norm and rank, and are learned using only a small number of labeled examples. We evaluate the method using two Vision Transformers pre-trained on YFCC100M and LAION400M, and test on eight classification datasets. Experimental results show that our method improves transfer accuracy over direct task vector application and achieves performance comparable to few-shot fine-tuning, while maintaining the modularity and reusability of task vectors. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/kawakera-lab/CrossModelTransfer">https://github.com/kawakera-lab/CrossModelTransfer</a>. </p>
<blockquote>
<p>ä»»åŠ¡ç®—æœ¯é€šè¿‡è¡¨ç¤ºç‰¹å®šä»»åŠ¡çš„æ”¹å˜ä¸ºå‚æ•°ç©ºé—´ä¸­çš„å‘é‡æ¥å®ç°é«˜æ•ˆçš„æ¨¡å‹ç¼–è¾‘ã€‚ä»»åŠ¡ç®—æœ¯é€šå¸¸å‡è®¾æºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹éƒ½æ˜¯ä»ç›¸åŒçš„é¢„è®­ç»ƒå‚æ•°è¿›è¡Œåˆå§‹åŒ–çš„ã€‚è¿™ä¸€å‡è®¾é™åˆ¶äº†å…¶åœ¨è·¨æ¨¡å‹è¿ç§»è®¾ç½®ä¸­çš„åº”ç”¨ï¼Œå…¶ä¸­æ¨¡å‹åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œç‹¬ç«‹é¢„è®­ç»ƒã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡æ­£äº¤å¯¹é½çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä»»åŠ¡å‘é‡å¯¹é½åˆ°ä¸åŒé¢„è®­ç»ƒçš„ç›®æ ‡æ¨¡å‹çš„å‚æ•°ç©ºé—´ã€‚è¿™äº›è½¬æ¢ä¿ç•™äº†ä»»åŠ¡å‘é‡çš„å…³é”®å±æ€§ï¼Œä¾‹å¦‚èŒƒæ•°å’Œç­‰çº§ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåœ¨YFCC100Må’ŒLAION400Mä¸Šé¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨å…«ä¸ªåˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†ç›´æ¥åº”ç”¨ä»»åŠ¡å‘é‡çš„è¿ç§»ç²¾åº¦ï¼Œå¹¶å®ç°äº†ä¸å°‘é‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä»»åŠ¡å‘é‡çš„æ¨¡å—åŒ–å’Œå¯é‡ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kawakera-lab/CrossModelTransfer">https://github.com/kawakera-lab/CrossModelTransfer</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12021v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>ä»»åŠ¡ç®—æœ¯é€šè¿‡å‚æ•°ç©ºé—´ä¸­çš„å‘é‡è¡¨ç¤ºç‰¹å®šä»»åŠ¡çš„å˜åŒ–ï¼Œä»è€Œå®ç°æ¨¡å‹çš„å¿«é€Ÿç¼–è¾‘ã€‚å½“æºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒå‚æ•°åˆå§‹åŒ–æ—¶ï¼Œä»»åŠ¡ç®—æœ¯æ•ˆæœå¾ˆå¥½ã€‚ç„¶è€Œï¼Œåœ¨è·¨æ¨¡å‹è¿ç§»è®¾ç½®ä¸­ï¼Œæ¨¡å‹åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œç‹¬ç«‹é¢„è®­ç»ƒï¼Œè¿™ä¸€å‡è®¾é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°‘æ ·æœ¬æ­£äº¤å¯¹é½çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä»»åŠ¡å‘é‡å¯¹é½åˆ°ä¸åŒé¢„è®­ç»ƒç›®æ ‡æ¨¡å‹çš„å‚æ•°ç©ºé—´ã€‚è¿™äº›è½¬æ¢ä¿ç•™äº†ä»»åŠ¡å‘é‡çš„å…³é”®å±æ€§ï¼Œå¦‚èŒƒæ•°å’Œç§©ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨æ¨¡å‹è¿ç§»ä¸­æé«˜äº†ä»»åŠ¡å‘é‡çš„åº”ç”¨æ•ˆæœï¼Œå®ç°äº†ä¸å°‘æ ·æœ¬å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä»»åŠ¡å‘é‡çš„æ¨¡å—åŒ–å’Œå¯é‡ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»åŠ¡ç®—æœ¯é€šè¿‡å‚æ•°ç©ºé—´ä¸­çš„å‘é‡è¡¨ç¤ºç‰¹å®šä»»åŠ¡çš„å˜åŒ–ï¼Œç”¨äºé«˜æ•ˆæ¨¡å‹ç¼–è¾‘ã€‚</li>
<li>æºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„é¢„è®­ç»ƒå‚æ•°åˆå§‹åŒ–å½±å“ä»»åŠ¡ç®—æœ¯çš„åº”ç”¨èŒƒå›´ã€‚</li>
<li>åœ¨è·¨æ¨¡å‹è¿ç§»è®¾ç½®ä¸­ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥åº”ç”¨ä»»åŠ¡ç®—æœ¯ï¼Œå› ä¸ºæ¨¡å‹åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œç‹¬ç«‹é¢„è®­ç»ƒã€‚</li>
<li>æå‡ºäº†åŸºäºå°‘æ ·æœ¬æ­£äº¤å¯¹é½çš„æ–¹æ³•ï¼Œå°†ä»»åŠ¡å‘é‡å¯¹é½åˆ°ä¸åŒé¢„è®­ç»ƒç›®æ ‡æ¨¡å‹çš„å‚æ•°ç©ºé—´ã€‚</li>
<li>è½¬æ¢ä¿ç•™äº†ä»»åŠ¡å‘é‡çš„å…³é”®å±æ€§ï¼Œå¦‚èŒƒæ•°å’Œç§©ã€‚</li>
<li>ä½¿ç”¨å°‘é‡æ ‡è®°æ ·æœ¬è¿›è¡Œå­¦ä¹ çš„æ•ˆæœè‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b008590378666cd051360af10d514672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d6966dcb5824103b105eccd0ec52288.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2241d58c28334a0f4e20998f858af593.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AdaptMol-Adaptive-Fusion-from-Sequence-String-to-Topological-Structure-for-Few-shot-Drug-Discovery"><a href="#AdaptMol-Adaptive-Fusion-from-Sequence-String-to-Topological-Structure-for-Few-shot-Drug-Discovery" class="headerlink" title="AdaptMol: Adaptive Fusion from Sequence String to Topological Structure   for Few-shot Drug Discovery"></a>AdaptMol: Adaptive Fusion from Sequence String to Topological Structure   for Few-shot Drug Discovery</h2><p><strong>Authors:Yifan Dai, Xuanbai Ren, Tengfei Ma, Qipeng Yan, Yiping Liu, Yuansheng Liu, Xiangxiang Zeng</strong></p>
<p>Accurate molecular property prediction (MPP) is a critical step in modern drug development. However, the scarcity of experimental validation data poses a significant challenge to AI-driven research paradigms. Under few-shot learning scenarios, the quality of molecular representations directly dictates the theoretical upper limit of model performance. We present AdaptMol, a prototypical network integrating Adaptive multimodal fusion for Molecular representation. This framework employs a dual-level attention mechanism to dynamically integrate global and local molecular features derived from two modalities: SMILES sequences and molecular graphs. (1) At the local level, structural features such as atomic interactions and substructures are extracted from molecular graphs, emphasizing fine-grained topological information; (2) At the global level, the SMILES sequence provides a holistic representation of the molecule. To validate the necessity of multimodal adaptive fusion, we propose an interpretable approach based on identifying molecular active substructures to demonstrate that multimodal adaptive fusion can efficiently represent molecules. Extensive experiments on three commonly used benchmarks under 5-shot and 10-shot settings demonstrate that AdaptMol achieves state-of-the-art performance in most cases. The rationale-extracted method guides the fusion of two modalities and highlights the importance of both modalities. </p>
<blockquote>
<p>ç²¾ç¡®åˆ†å­å±æ€§é¢„æµ‹ï¼ˆMPPï¼‰æ˜¯ç°ä»£è¯ç‰©å¼€å‘ä¸­çš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œå®éªŒéªŒè¯æ•°æ®çš„ç¨€ç¼ºæ€§å¯¹AIé©±åŠ¨çš„ç ”ç©¶æ¨¡å¼æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨å°‘é‡å­¦ä¹ åœºæ™¯ä¸‹ï¼Œåˆ†å­è¡¨ç¤ºçš„è´¨é‡ç›´æ¥å†³å®šäº†æ¨¡å‹æ€§èƒ½çš„ç†è®ºä¸Šé™ã€‚æˆ‘ä»¬æå‡ºäº†AdaptMolï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆè‡ªé€‚åº”å¤šæ¨¡å¼èåˆçš„åŸå‹ç½‘ç»œè¿›è¡Œåˆ†å­è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒé‡å±‚æ¬¡çš„æ³¨æ„æœºåˆ¶ï¼ŒåŠ¨æ€èåˆå…¨å±€å’Œå±€éƒ¨åˆ†å­ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ¥è‡ªä¸¤ç§æ¨¡å¼ï¼šSMILESåºåˆ—å’Œåˆ†å­å›¾å½¢ã€‚(1)åœ¨å±€éƒ¨å±‚é¢ï¼Œä»åˆ†å­å›¾ä¸­æå–ç»“æ„ç‰¹å¾ï¼Œå¦‚åŸå­ç›¸äº’ä½œç”¨å’Œå­ç»“æ„ï¼Œå¼ºè°ƒç²¾ç»†çš„æ‹“æ‰‘ä¿¡æ¯ï¼›(2)åœ¨å…¨å±€å±‚é¢ï¼ŒSMILESåºåˆ—ä¸ºåˆ†å­æä¾›äº†æ•´ä½“è¡¨ç¤ºã€‚ä¸ºäº†éªŒè¯å¤šæ¨¡å¼è‡ªé€‚åº”èåˆçš„å¿…è¦æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯†åˆ«åˆ†å­æ´»æ€§å­ç»“æ„çš„å¯è§£é‡Šæ–¹æ³•ï¼Œè¯æ˜å¤šæ¨¡å¼è‡ªé€‚åº”èåˆå¯ä»¥æœ‰æ•ˆåœ°ä»£è¡¨åˆ†å­ã€‚åœ¨5æ¬¡å’Œ10æ¬¡è®¾ç½®ä¸‹ï¼Œå¯¹ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAdaptMolåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚æ‰€æå–çš„ç†æ€§æ–¹æ³•æŒ‡å¯¼äº†ä¸¤ç§æ¨¡å¼çš„èåˆï¼Œå¹¶å¼ºè°ƒäº†ä¸¤ç§æ¨¡å¼çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11878v1">PDF</a> 15 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>é€‚åº”åˆ†å­è¡¨ç¤ºçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°ä»£è¯ç‰©å¼€å‘ä¸­å‡†ç¡®åˆ†å­å±æ€§é¢„æµ‹ï¼ˆMPPï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥åŸå‹ç½‘ç»œå¹¶ç»“åˆè‡ªé€‚åº”å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œæå‡ºä¸€ç§åä¸ºAdaptMolçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒçº§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€èåˆå…¨å±€å’Œå±€éƒ¨åˆ†å­ç‰¹å¾ï¼Œç‰¹å¾æ¥æºäºSMILESåºåˆ—å’Œåˆ†å­å›¾ä¸¤ç§æ¨¡å¼ã€‚å±€éƒ¨å±‚é¢å¼ºè°ƒç²¾ç»†çš„æ‹“æ‰‘ä¿¡æ¯ï¼Œå¦‚åŸå­ç›¸äº’ä½œç”¨å’Œå­ç»“æ„ï¼›å…¨å±€å±‚é¢åˆ™æä¾›åˆ†å­çš„æ•´ä½“è¡¨ç¤ºã€‚é€šè¿‡å¯è§£é‡Šçš„è¯†åˆ«åˆ†å­æ´»æ€§å­ç»“æ„çš„æ–¹æ³•éªŒè¯äº†å¤šæ¨¡æ€è‡ªé€‚åº”èåˆçš„å¿…è¦æ€§ã€‚åœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdaptMolåœ¨5-shotå’Œ10-shotè®¾ç½®ä¸‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Accurateåˆ†å­å±æ€§é¢„æµ‹ï¼ˆMPPï¼‰æ˜¯ç°ä»£è¯ç‰©å¼€å‘çš„å…³é”®æ­¥éª¤ï¼Œç¼ºä¹å®éªŒéªŒè¯æ•°æ®å¯¹AIç ”ç©¶é€ æˆæŒ‘æˆ˜ã€‚</li>
<li>AdaptMolæ¡†æ¶ç»“åˆäº†è‡ªé€‚åº”å¤šæ¨¡æ€èåˆæŠ€æœ¯å’ŒåŸå‹ç½‘ç»œã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒçº§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€èåˆå…¨å±€å’Œå±€éƒ¨åˆ†å­ç‰¹å¾ã€‚</li>
<li>å±€éƒ¨å±‚é¢å…³æ³¨ç²¾ç»†çš„æ‹“æ‰‘ä¿¡æ¯ï¼Œå¦‚åŸå­ç›¸äº’ä½œç”¨å’Œå­ç»“æ„ï¼›å…¨å±€å±‚é¢æä¾›åˆ†å­æ•´ä½“è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡è¯†åˆ«åˆ†å­æ´»æ€§å­ç»“æ„çš„æ–¹æ³•éªŒè¯äº†å¤šæ¨¡æ€è‡ªé€‚åº”èåˆçš„å¿…è¦æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdaptMolåœ¨few-shotè®¾ç½®ä¸‹å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9da502bf3387f4dc5382a928369bf780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52e93cb768d1ebe688792e8d6faa483c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50811f3e0aeae9bea6f916af2905c28b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generalizable-Vision-Language-Few-Shot-Adaptation-with-Predictive-Prompts-and-Negative-Learning"><a href="#Generalizable-Vision-Language-Few-Shot-Adaptation-with-Predictive-Prompts-and-Negative-Learning" class="headerlink" title="Generalizable Vision-Language Few-Shot Adaptation with Predictive   Prompts and Negative Learning"></a>Generalizable Vision-Language Few-Shot Adaptation with Predictive   Prompts and Negative Learning</h2><p><strong>Authors:Sriram Mandalika</strong></p>
<p>Few-shot adaptation remains a core challenge for vision-language models (VLMs), especially under limited supervision and noisy support samples. We propose PromptFuseNL, a unified framework that enhances few-shot generalization by combining predictive prompt tuning with dual-branch positive and negative learning. The method refines class prototypes through task-conditioned residuals, multi-stage cross-modal coordination, and semantic hard negative mining. To address label noise, we introduce an unsupervised instance reweighting strategy that downweights unreliable support examples without requiring additional labels or structural changes. PromptFuseNL fuses visual and textual cues through lightweight modules for efficient and discriminative prediction. Evaluated across 15 benchmarks, it consistently surpasses existing prompt- and adapter-based methods in all shot settings while remaining highly efficient, achieving up to 300x faster training and 1000x lower FLOPs compared to full prompt tuning, achieving a new state-of-the-art for robust and scalable few-shot vision-language adaptation. </p>
<blockquote>
<p>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰é™çš„ç›‘ç£å’Œå™ªå£°æ ·æœ¬çš„æ”¯æŒä¸‹ï¼Œå°æ ·æœ¬é€‚åº”ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†PromptFuseNLè¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒé€šè¿‡é¢„æµ‹æç¤ºè°ƒæ•´ä¸åŒåˆ†æ”¯æ­£è´Ÿå­¦ä¹ ç›¸ç»“åˆï¼Œæé«˜äº†å°æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»»åŠ¡æ¡ä»¶æ®‹å·®ã€å¤šé˜¶æ®µè·¨æ¨¡æ€åè°ƒå’Œè¯­ä¹‰ç¡¬è´ŸæŒ–æ˜æ¥ä¼˜åŒ–ç±»åŸå‹ã€‚ä¸ºäº†è§£å†³æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— ç›‘ç£çš„å®ä¾‹é‡åŠ æƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–æ ‡ç­¾æˆ–ç»“æ„æ›´æ”¹çš„æƒ…å†µä¸‹é™ä½ä¸å¯é çš„æ”¯æŒæ ·æœ¬çš„æƒé‡ã€‚PromptFuseNLé€šè¿‡è½»é‡çº§æ¨¡å—èåˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œä»¥å®ç°é«˜æ•ˆå’Œé‰´åˆ«æ€§é¢„æµ‹ã€‚åœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå®ƒåœ¨æ‰€æœ‰å°„å‡»è®¾ç½®ä¸­éƒ½è¶…è¿‡äº†ç°æœ‰çš„åŸºäºæç¤ºå’Œé€‚é…å™¨çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆç‡ï¼Œä¸å…¨æç¤ºè°ƒæ•´ç›¸æ¯”ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†300å€ï¼ŒFLOPsé™ä½äº†1000å€ï¼Œä¸ºå®ç°ç¨³å¥ä¸”å¯æ‰©å±•çš„å°æ ·æœ¬è§†è§‰è¯­è¨€é€‚åº”è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11758v1">PDF</a> </p>
<p><strong>Summary</strong><br>å°‘æ•°æ ·æœ¬é€‚åº”ä»æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™ç›‘ç£ä¸å™ªå£°æ ·æœ¬çš„æ”¯æŒä¸‹ã€‚æˆ‘ä»¬æå‡ºPromptFuseNLç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹æç¤ºè°ƒæ•´ä¸æ­£è´Ÿæ ·æœ¬å­¦ä¹ çš„åŒåˆ†æ”¯ç»“åˆï¼Œæé«˜å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤æ–¹æ³•é€šè¿‡ä»»åŠ¡æ¡ä»¶æ®‹å·®ã€å¤šé˜¶æ®µè·¨æ¨¡æ€åè°ƒå’Œè¯­ä¹‰ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æ¥ä¼˜åŒ–ç±»åŸå‹ã€‚ä¸ºè§£å†³æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥æ— ç›‘ç£å®ä¾‹é‡åŠ æƒç­–ç•¥ï¼Œåœ¨ä¸éœ€é¢å¤–æ ‡ç­¾æˆ–ç»“æ„æ›´æ”¹çš„æƒ…å†µä¸‹é™ä½ä¸å¯é æ”¯æŒæ ·æœ¬çš„æƒé‡ã€‚PromptFuseNLé€šè¿‡è½»é‡çº§æ¨¡å—èåˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå®ç°é«˜æ•ˆå’Œé‰´åˆ«æ€§é¢„æµ‹ã€‚åœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰çš„æç¤ºå’Œé€‚é…å™¨æ–¹æ³•ï¼Œå®ƒåœ¨æ‰€æœ‰å°„å‡»è®¾ç½®ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”é«˜åº¦é«˜æ•ˆï¼Œä¸å®Œæ•´æç¤ºè°ƒæ•´ç›¸æ¯”å®ç°äº†é«˜è¾¾300å€çš„æ›´å¿«è®­ç»ƒå’Œé«˜è¾¾æ›´ä½çš„FLOPsæ•°é‡ï¼Œæˆä¸ºæ–°çš„å°‘æ ·æœ¬è§†è§‰è¯­è¨€é€‚åº”æŠ€æœ¯çš„æ°å‡ºä»£è¡¨ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PromptFuseNLæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸å™ªå£°æ ·æœ¬ä¸‹çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡é¢„æµ‹æç¤ºè°ƒæ•´ä¸æ­£è´Ÿæ ·æœ¬å­¦ä¹ çš„ç»“åˆæ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä»»åŠ¡æ¡ä»¶æ®‹å·®ã€å¤šé˜¶æ®µè·¨æ¨¡æ€åè°ƒå’Œè¯­ä¹‰ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æ¥ä¼˜åŒ–ç±»åŸå‹ã€‚</li>
<li>å¼•å…¥æ— ç›‘ç£å®ä¾‹é‡åŠ æƒç­–ç•¥æ¥è§£å†³æ ‡ç­¾å™ªå£°é—®é¢˜ã€‚</li>
<li>PromptFuseNLæ¡†æ¶å®ç°äº†é«˜æ•ˆå’Œé‰´åˆ«æ€§çš„é¢„æµ‹ï¼Œé€šè¿‡è½»é‡çº§æ¨¡å—èåˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf91ad710823de3a5959df9050cd08e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-862b7906a9f4fc24189ec9991453b26f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LoFT-LoRA-fused-Training-Dataset-Generation-with-Few-shot-Guidance"><a href="#LoFT-LoRA-fused-Training-Dataset-Generation-with-Few-shot-Guidance" class="headerlink" title="LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance"></a>LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance</h2><p><strong>Authors:Jae Myung Kim, Stephan Alaniz, Cordelia Schmid, Zeynep Akata</strong></p>
<p>Despite recent advances in text-to-image generation, using synthetically generated data seldom brings a significant boost in performance for supervised learning. Oftentimes, synthetic datasets do not faithfully recreate the data distribution of real data, i.e., they lack the fidelity or diversity needed for effective downstream model training. While previous work has employed few-shot guidance to address this issue, existing methods still fail to capture and generate features unique to specific real images. In this paper, we introduce a novel dataset generation framework named LoFT, LoRA-Fused Training-data Generation with Few-shot Guidance. Our method fine-tunes LoRA weights on individual real images and fuses them at inference time, producing synthetic images that combine the features of real images for improved diversity and fidelity of generated data. We evaluate the synthetic data produced by LoFT on 10 datasets, using 8 to 64 real images per class as guidance and scaling up to 1000 images per class. Our experiments show that training on LoFT-generated data consistently outperforms other synthetic dataset methods, significantly increasing accuracy as the dataset size increases. Additionally, our analysis demonstrates that LoFT generates datasets with high fidelity and sufficient diversity, which contribute to the performance improvement. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/LoFT">https://github.com/ExplainableML/LoFT</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä½¿ç”¨åˆæˆæ•°æ®å¾ˆå°‘èƒ½æ˜¾è‘—æé«˜ç›‘ç£å­¦ä¹ çš„æ€§èƒ½ã€‚é€šå¸¸ï¼Œåˆæˆæ•°æ®é›†æ— æ³•çœŸå®åœ°é‡ç°çœŸå®æ•°æ®çš„æ•°æ®åˆ†å¸ƒï¼Œå³å®ƒä»¬ç¼ºä¹æœ‰æ•ˆä¸‹æ¸¸æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„ä¿çœŸåº¦æˆ–å¤šæ ·æ€§ã€‚è™½ç„¶ä¹‹å‰çš„å·¥ä½œå·²ç»é‡‡ç”¨å°æ ·æœ¬æŒ‡å¯¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶æ— æ³•æ•è·å¹¶ç”Ÿæˆç‰¹å®šçœŸå®å›¾åƒæ‰€ç‹¬æœ‰çš„ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹æ•°æ®é›†ç”Ÿæˆæ¡†æ¶ï¼Œåä¸ºLoFTï¼ˆèåˆå°æ ·æœ¬æŒ‡å¯¼çš„LoRAè®­ç»ƒæ•°æ®ç”Ÿæˆï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¾®è°ƒæ¯ä¸ªçœŸå®å›¾åƒçš„LoRAæƒé‡å¹¶åœ¨æ¨ç†æ—¶é—´è¿›è¡Œèåˆï¼Œç”Ÿæˆåˆæˆå›¾åƒï¼Œè¿™äº›å›¾åƒç»“åˆäº†çœŸå®å›¾åƒçš„ç‰¹å¾ï¼Œæé«˜äº†ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬åœ¨10ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†LoFTç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œä½¿ç”¨æ¯ç±»8åˆ°64å¼ çœŸå®å›¾åƒä½œä¸ºæŒ‡å¯¼ï¼Œå¹¶æ‰©å±•åˆ°æ¯ç±»1000å¼ å›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨LoFTç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå§‹ç»ˆä¼˜äºå…¶ä»–åˆæˆæ•°æ®é›†æ–¹æ³•ï¼Œéšç€æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLoFTç”Ÿæˆçš„æ•°æ®é›†å…·æœ‰é«˜ä¿çœŸå’Œè¶³å¤Ÿçš„å¤šæ ·æ€§ï¼Œè¿™æœ‰åŠ©äºæ€§èƒ½æ”¹è¿›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/LoFT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ExplainableML/LoFTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11703v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLoFTçš„æ–°å‹æ•°æ®é›†ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å°‘æ ·æœ¬æŒ‡å¯¼ï¼Œé€šè¿‡å¾®è°ƒLoRAæƒé‡å¹¶èåˆçœŸå®å›¾åƒï¼Œç”Ÿæˆåˆæˆå›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨LoFTç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒä¼˜äºå…¶ä»–åˆæˆæ•°æ®é›†æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ•°æ®é›†è§„æ¨¡å¢å¤§æ—¶ï¼Œå‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoFTæ˜¯ä¸€ç§æ–°å‹æ•°æ®é›†ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨å°‘æ ·æœ¬æŒ‡å¯¼ç­–ç•¥ã€‚</li>
<li>LoFTé€šè¿‡å¾®è°ƒLoRAæƒé‡å¹¶èåˆçœŸå®å›¾åƒæ¥ç”Ÿæˆåˆæˆå›¾åƒã€‚</li>
<li>LoFTç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–åˆæˆæ•°æ®é›†æ–¹æ³•ã€‚</li>
<li>éšç€æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ï¼Œä½¿ç”¨LoFTç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„å‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚</li>
<li>LoFTç”Ÿæˆçš„æ•°æ®é›†å…·æœ‰é«˜ä¿çœŸåº¦å’Œè¶³å¤Ÿçš„å¤šæ ·æ€§ã€‚</li>
<li>LoFTæ¡†æ¶çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9500e7b0b49345ddcf9990c18956b122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a9466ce6cf65f389bbc36304125bf83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-813c1a353292fa46cb66d2cb16dd8503.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec2263de6c50141726c77e59a484cd24.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AdaptCLIP-Adapting-CLIP-for-Universal-Visual-Anomaly-Detection"><a href="#AdaptCLIP-Adapting-CLIP-for-Universal-Visual-Anomaly-Detection" class="headerlink" title="AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection"></a>AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection</h2><p><strong>Authors:Bin-Bin Gao, Yue Zhou, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang</strong></p>
<p>Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-&#x2F;few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at <a target="_blank" rel="noopener" href="https://github.com/gaobb/AdaptCLIP">https://github.com/gaobb/AdaptCLIP</a>. </p>
<blockquote>
<p>é€šç”¨è§†è§‰å¼‚å¸¸æ£€æµ‹æ—¨åœ¨ä»æ–°é¢–æˆ–æœªè§è¿‡çš„è§†è§‰é¢†åŸŸè¯†åˆ«å¼‚å¸¸å€¼ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒï¼Œè¿™åœ¨å¼€æ”¾åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä»…ä½¿ç”¨é›¶å¼ æˆ–å°‘æ•°å‡ å¼ æ­£å¸¸å›¾åƒè¿›è¡Œè®­ç»ƒçš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨è®¾è®¡æç¤ºæ¨¡æ¿ã€å¤æ‚çš„ä»¤ç‰Œäº¤äº’æˆ–éœ€è¦é¢å¤–çš„å¾®è°ƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´çµæ´»æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºä¸¤ä¸ªå…³é”®è§è§£çš„AdaptCLIPã€‚é¦–å…ˆï¼Œåº”äº¤æ›¿å­¦ä¹ è‡ªé€‚åº”çš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œè€Œä¸æ˜¯è”åˆå­¦ä¹ ã€‚å…¶æ¬¡ï¼ŒæŸ¥è¯¢å’Œæ­£å¸¸å›¾åƒæç¤ºä¹‹é—´çš„æ¯”è¾ƒå­¦ä¹ åº”èå…¥ä¸Šä¸‹æ–‡å’Œå¯¹é½çš„æ®‹å·®ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä»…ä¾èµ–æ®‹å·®ç‰¹å¾ã€‚AdaptCLIPå°†CLIPæ¨¡å‹ä½œä¸ºåŸºç¡€æœåŠ¡ï¼Œä»…åœ¨è¾“å…¥æˆ–è¾“å‡ºç«¯æ·»åŠ ä¸‰ä¸ªç®€å•çš„é€‚é…å™¨ï¼šè§†è§‰é€‚é…å™¨ã€æ–‡æœ¬é€‚é…å™¨å’Œæç¤ºæŸ¥è¯¢é€‚é…å™¨ã€‚AdaptCLIPæ”¯æŒè·¨é¢†åŸŸçš„é›¶&#x2F;å°‘é•œå¤´æ³›åŒ–ï¼Œä¸€æ—¦åœ¨åŸºç¡€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ƒåœ¨ç›®æ ‡é¢†åŸŸä¸Šé‡‡ç”¨æ— è®­ç»ƒæ–¹å¼ã€‚AdaptCLIPåœ¨æ¥è‡ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„12ä¸ªå¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç«äº‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/gaobb/AdaptCLIP%E4%B8%8A%E6%8F%90%E4%BE%9BAdaptCLIP%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/gaobb/AdaptCLIPä¸Šæä¾›AdaptCLIPçš„ä»£ç å’Œæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09926v2">PDF</a> 27 pages, 15 figures, 22 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPæ¨¡å‹çš„AdaptCLIPæ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œç»“åˆä¸Šä¸‹æ–‡å’Œä½™å¼¦ç‰¹å¾çš„å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°äº†æ— éœ€é¢å¤–ç²¾ç»†è°ƒæ•´çš„è·¨åŸŸé›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ã€‚æ­¤æ–¹æ³•åªéœ€åœ¨è¾“å…¥æˆ–è¾“å‡ºç«¯æ·»åŠ å°‘é‡é€‚é…å™¨å³å¯æ”¯æŒç›®æ ‡åŸŸçš„æ¨å¹¿ã€‚AdaptCLIPåœ¨å¤šä¸ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaptCLIPæ—¨åœ¨å®ç°è·¨æœªçŸ¥è§†è§‰é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹ï¼Œæ— éœ€é¢å¤–ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>åŸºäºCLIPæ¨¡å‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œä»…ä½¿ç”¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ­£å¸¸å›¾åƒã€‚</li>
<li>AdaptCLIPé‡‡ç”¨äº¤æ›¿å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„æ–¹å¼ï¼Œå¢å¼ºæ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œç»“åˆä¸Šä¸‹æ–‡å’Œä½™å¼¦ç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>ä»…é€šè¿‡æ·»åŠ ä¸‰ä¸ªç®€å•é€‚é…å™¨ï¼ˆè§†è§‰é€‚é…å™¨ã€æ–‡æœ¬é€‚é…å™¨å’Œæç¤ºæŸ¥è¯¢é€‚é…å™¨ï¼‰æ¥å¢å¼ºCLIPæ¨¡å‹çš„åŠŸèƒ½ã€‚</li>
<li>AdaptCLIPåœ¨å¤šä¸ªå¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40b4a9ae6e3ed27e72c227cb185968ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1fd26f5b505fd52bcda2cf860d2e8ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a145f33eba3ef29ee50146e66f001838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ebc40820010eafa9b61124bc505c52d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ae81f72e141180ed2bb3b903f5a660a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Option-ID-Based-Elimination-For-Multiple-Choice-Questions"><a href="#Option-ID-Based-Elimination-For-Multiple-Choice-Questions" class="headerlink" title="Option-ID Based Elimination For Multiple Choice Questions"></a>Option-ID Based Elimination For Multiple Choice Questions</h2><p><strong>Authors:Zhenhao Zhu, Bulou Liu, Qingyao Ai, Yiqun Liu</strong></p>
<p>Multiple choice questions (MCQs) are a popular and important task for evaluating large language models (LLMs). Based on common strategies people use when answering MCQs, the process of elimination (PoE) has been proposed as an effective problem-solving method. Existing PoE methods typically either have LLMs directly identify incorrect options or score options and replace lower-scoring ones with [MASK]. However, both methods suffer from inapplicability or suboptimal performance. To address these issues, this paper proposes a novel option-ID based PoE ($\text{PoE}<em>{\text{ID}}$). $\text{PoE}</em>{\text{ID}}$ critically incorporates a debiasing technique to counteract LLMs token bias, enhancing robustness over naive ID-based elimination. It features two strategies: $\text{PoE}<em>{\text{ID}}^{\text{log}}$, which eliminates options whose IDs have log probabilities below the average threshold, and $\text{PoE}</em>{\text{ID}}^{\text{seq}}$, which iteratively removes the option with the lowest ID probability. We conduct extensive experiments with 6 different LLMs on 4 diverse datasets. The results demonstrate that $\text{PoE}<em>{\text{ID}}$, especially $\text{PoE}</em>{\text{ID}}^{\text{log}}$, significantly improves zero-shot and few-shot MCQs performance, particularly in datasets with more options. Our analyses demonstrate that $\text{PoE}_{\text{ID}}^{\text{log}}$ enhances the LLMsâ€™ confidence in selecting the correct option, and the option elimination strategy outperforms methods relying on [MASK] replacement. We further investigate the limitations of LLMs in directly identifying incorrect options, which stem from their inherent deficiencies. </p>
<blockquote>
<p>å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æµè¡Œä¸”é‡è¦çš„ä»»åŠ¡ã€‚åŸºäºäººä»¬å›ç­”é€‰æ‹©é¢˜æ—¶å¸¸ç”¨çš„ç­–ç•¥ï¼Œæå‡ºäº†åŸºäºæ’é™¤æ³•ï¼ˆPoEï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è§£å†³é—®é¢˜çš„æ–¹æ³•ã€‚ç°æœ‰çš„PoEæ–¹æ³•é€šå¸¸è¦ä¹ˆè®©LLMç›´æ¥è¯†åˆ«é”™è¯¯é€‰é¡¹ï¼Œè¦ä¹ˆå¯¹é€‰é¡¹è¿›è¡Œè¯„åˆ†å¹¶å°†å¾—åˆ†è¾ƒä½çš„é€‰é¡¹æ›¿æ¢ä¸º[MASK]ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨ä¸é€‚ç”¨æˆ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€‰é¡¹IDçš„PoEï¼ˆPoE IDï¼‰ã€‚PoE IDå·§å¦™åœ°èå…¥äº†å»åæŠ€æœ¯æ¥å¯¹æŠ—LLMä»¤ç‰Œåå·®ï¼Œç›¸å¯¹äºç®€å•çš„åŸºäºIDçš„æ¶ˆé™¤æ–¹æ³•å¢å¼ºäº†ç¨³å¥æ€§ã€‚å®ƒåŒ…å«ä¸¤ç§ç­–ç•¥ï¼šPoE ID logï¼Œå®ƒæ¶ˆé™¤é‚£äº›IDå¯¹æ•°æ¦‚ç‡ä½äºå¹³å‡é˜ˆå€¼çš„é€‰é¡¹ï¼›PoE ID seqï¼Œå®ƒè¿­ä»£åœ°ç§»é™¤å…·æœ‰æœ€ä½IDæ¦‚ç‡çš„é€‰é¡¹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹å…­ä¸ªä¸åŒçš„LLMè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯PoE ID logï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é€‰æ‹©é¢˜ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰æ›´å¤šé€‰é¡¹çš„æ•°æ®é›†ä¸Šã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒPoE ID logå¢å¼ºäº†LLMåœ¨é€‰æ‹©æ­£ç¡®é€‰é¡¹æ—¶çš„ä¿¡å¿ƒï¼Œå¹¶ä¸”åŸºäºé€‰é¡¹æ¶ˆé™¤çš„ç­–ç•¥ä¼˜äºä¾èµ–äº[MASK]æ›¿æ¢çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†LLMåœ¨ç›´æ¥è¯†åˆ«é”™è¯¯é€‰é¡¹æ–¹é¢çš„å±€é™æ€§ï¼Œè¿™äº›å±€é™æ€§æºäºå…¶å›ºæœ‰çš„ç¼ºé™·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15175v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººä»¬åœ¨å›ç­”é€‰æ‹©é¢˜æ—¶çš„å¸¸ç”¨ç­–ç•¥ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è§£é¢˜æ–¹æ³•â€”â€”æ’é™¤æ³•ï¼ˆPoEï¼‰ã€‚ç°æœ‰çš„PoEæ–¹æ³•è¦ä¹ˆç›´æ¥è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯†åˆ«é”™è¯¯é€‰é¡¹ï¼Œè¦ä¹ˆå¯¹é€‰é¡¹è¿›è¡Œè¯„åˆ†å¹¶ç”¨[MASK]æ›¿æ¢ä½åˆ†é€‰é¡¹ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨ä¸é€‚ç”¨æˆ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€‰é¡¹IDçš„PoEï¼ˆ$\text{PoE}<em>{\text{ID}}$ï¼‰ã€‚$\text{PoE}</em>{\text{ID}}$å·§å¦™åœ°èå…¥äº†ä¸€ç§å»åæŠ€æœ¯ï¼Œä»¥æŠµæ¶ˆLLMçš„ä»¤ç‰Œåç½®ï¼Œæé«˜äº†å¯¹åŸºäºå•çº¯IDæ¶ˆé™¤çš„ç¨³å¥æ€§ã€‚å®ƒåŒ…å«ä¸¤ç§ç­–ç•¥ï¼š$\text{PoE}<em>{\text{ID}}^{\text{log}}$ï¼Œæ¶ˆé™¤IDå¯¹æ•°æ¦‚ç‡ä½äºå¹³å‡é˜ˆå€¼çš„é€‰é¡¹ï¼›ä»¥åŠ$\text{PoE}</em>{\text{ID}}^{\text{seq}}$ï¼Œè¿­ä»£ç§»é™¤IDæ¦‚ç‡æœ€ä½çš„é€‰æ‹©é¡¹ã€‚å®éªŒè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é€‰é¡¹è¾ƒå¤šçš„æ•°æ®é›†ä¸Šï¼Œ$\text{PoE}<em>{\text{ID}}$ï¼Œå°¤å…¶æ˜¯$\text{PoE}</em>{\text{ID}}^{\text{log}}$ï¼Œèƒ½æ˜¾è‘—æé«˜é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é€‰æ‹©é¢˜çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ’é™¤æ³•ï¼ˆPoEï¼‰è¢«æå‡ºä½œä¸ºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€‰æ‹©é¢˜ä»»åŠ¡ä¸­çš„é—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰PoEæ–¹æ³•å­˜åœ¨ä¸é€‚ç”¨æˆ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºé€‰é¡¹IDçš„PoEæ–¹æ³•ï¼ˆ$\text{PoE}<em>{\text{ID}}$ï¼‰ï¼ŒåŒ…å«ä¸¤ç§ç­–ç•¥ï¼š$\text{PoE}</em>{\text{ID}}^{\text{log}}$å’Œ$\text{PoE}_{\text{ID}}^{\text{seq}}$ã€‚</li>
<li>$\text{PoE}_{\text{ID}}$é€šè¿‡èå…¥å»åæŠ€æœ¯ï¼Œæé«˜äº†LLMåœ¨é€‰æ‹©é¢˜ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªLLMå’Œå¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œ$\text{PoE}<em>{\text{ID}}$ç‰¹åˆ«æ˜¯$\text{PoE}</em>{\text{ID}}^{\text{log}}$è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>$\text{PoE}_{\text{ID}}^{\text{log}}$å¢å¼ºäº†LLMé€‰æ‹©æ­£ç¡®é€‰é¡¹çš„ä¿¡å¿ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd55977516b77c2e730d0049d0703d1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae7e1f6456d19d47b8b2192395998fc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7e35c51eb8d1b53bc30c9c909a9ca4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cade0256534cb7612dff64d72f61c18.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VICON-Vision-In-Context-Operator-Networks-for-Multi-Physics-Fluid-Dynamics-Prediction"><a href="#VICON-Vision-In-Context-Operator-Networks-for-Multi-Physics-Fluid-Dynamics-Prediction" class="headerlink" title="VICON: Vision In-Context Operator Networks for Multi-Physics Fluid   Dynamics Prediction"></a>VICON: Vision In-Context Operator Networks for Multi-Physics Fluid   Dynamics Prediction</h2><p><strong>Authors:Yadi Cao, Yuxuan Liu, Liu Yang, Rose Yu, Hayden Schaeffer, Stanley Osher</strong></p>
<p>In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose Vision In-Context Operator Networks (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICONâ€™s adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9% compared to DPOT and 44.7% compared to MPP, while requiring only 72.5% and 34.8% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in imperfect measurement systems where sampling frequencies may differ or frames might be dropped - common challenges in real-world settings - without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41% relative performance degradation compared to 71.37%-74.49% degradation in baseline methods, demonstrating its versatility for deploying in realistic applications. Our scripts for processing datasets and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Eydcao/VICON">https://github.com/Eydcao/VICON</a>. </p>
<blockquote>
<p>In-Context Operator Networksï¼ˆICONsï¼‰å·²è¯æ˜å…·å¤‡åœ¨å°‘æ•°æƒ…å¢ƒä¸‹å­¦ä¹ å¤šç§åå¾®åˆ†æ–¹ç¨‹æ“ä½œç¬¦çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰ICONå°†æ¯ä¸ªç©ºé—´ç‚¹è§†ä¸ºå•ä¸ªæ ‡è®°ï¼Œåœ¨å¤„ç†é«˜ç»´ç©ºé—´ä¸­çš„å¯†é›†æ•°æ®æ—¶ï¼Œè®¡ç®—æ•ˆç‡å—åˆ°ä¸¥é‡é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºVision In-Context Operator Networksï¼ˆVICONï¼‰ï¼Œå®ƒç»“åˆäº†è§†è§‰è½¬æ¢å™¨æ¶æ„ï¼Œé€šè¿‡å—æ“ä½œé«˜æ•ˆå¤„ç†äºŒç»´æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™ICONå¯¹å¤šç‰©ç†ç³»ç»Ÿå’Œä¸åŒæ—¶é—´æ­¥é•¿çš„é€‚åº”æ€§ã€‚åœ¨ä¸‰ä¸ªæµä½“åŠ¨åŠ›å­¦åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°æ˜¾ç¤ºï¼ŒVICONæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯DPOTå’ŒMPPï¼Œä¸DPOTç›¸æ¯”å‡å°‘äº†å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®çš„ç™¾åˆ†æ¯”è¾¾åˆ°37.9%ï¼Œä¸MPPç›¸æ¯”å‡å°‘äº†å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®çš„ç™¾åˆ†æ¯”è¾¾åˆ°44.7%ï¼ŒåŒæ—¶ä»…éœ€è¦å…¶å„è‡ªæ¨ç†æ—¶é—´çš„ç™¾åˆ†æ¯”è¾¾åˆ°åˆ†åˆ«ä¸º72.5%å’Œ34.8%ã€‚VICONè‡ªç„¶æ”¯æŒçµæ´»çš„æ—¶é—´æ­¥é•¿æ»šåŠ¨ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æµ‹é‡ç³»ç»Ÿä¸å®Œç¾çš„æƒ…å†µä¸‹å³æ—¶éƒ¨ç½²ï¼Œå…¶ä¸­é‡‡æ ·é¢‘ç‡å¯èƒ½æœ‰æ‰€ä¸åŒæˆ–å¸§å¯èƒ½ä¼šä¸¢å¤±â€”â€”ç°å®åœºæ™¯ä¸­å¸¸è§çš„é—®é¢˜â€”â€”æ— éœ€è¿›è¡Œå†æ¬¡è®­ç»ƒæˆ–æ’å€¼å¤„ç†ã€‚åœ¨è¿™äº›å®é™…åœºæ™¯ä¸­ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”VICONå±•ç°å‡ºæ˜¾è‘—çš„ç¨³å¥æ€§ï¼Œç›¸å¯¹æ€§èƒ½ä¸‹é™ä»…ä¸º24.41%ï¼Œè€ŒåŸºçº¿æ–¹æ³•æ€§èƒ½ä¸‹é™ä¸ºä»‹äº71.37%-74.49%ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®åº”ç”¨ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¤„ç†è„šæœ¬å’Œä»£ç å…¬å¼€å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Eydcao/VICON%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Eydcao/VICONè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16063v3">PDF</a> update 1 more baseline + 1 more experiment setup (performance for   temporal measurements with dropped frames); updated to Nueral IPS format.   Refined writing and presentations</p>
<p><strong>Summary</strong></p>
<p>VICONæ˜¯ä¸€ç§åŸºäºVision Transformeræ¶æ„çš„In-Context Operator Networksï¼ˆICONsï¼‰çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†äºŒç»´æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™ICONsåœ¨å¤šç‰©ç†ç³»ç»Ÿå’Œä¸åŒæ—¶é—´æ­¥é•¿çš„é€‚åº”æ€§ã€‚åœ¨ä¸‰ä¸ªæµä½“åŠ¨åŠ›å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVICONæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºå‡†çº¿ï¼Œå‡å°‘äº†å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®ï¼Œå¹¶æä¾›äº†çµæ´»çš„æ»šåŠ¨ç­–ç•¥ä»¥åº”å¯¹ä¸åŒçš„æ—¶é—´æ­¥é•¿è·¨åº¦ã€‚VICONåœ¨å¤„ç†ç°å®ä¸–ç•Œä¸­çš„ä¸å®Œç¾æµ‹é‡ç³»ç»Ÿæ—¶å±•ç°å‡ºå‡ºè‰²çš„ç¨³å¥æ€§ï¼Œä¸”åœ¨æ•°æ®é›†å¤„ç†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒä»¥ä¾›ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VICONæ•´åˆäº†Vision Transformeræ¶æ„ï¼Œæ—¨åœ¨æé«˜å¤„ç†äºŒç»´æ•°æ®æ—¶çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>VICONä¿ç•™äº†ICONså¯¹å¤šç‰©ç†ç³»ç»Ÿå’Œä¸åŒæ—¶é—´æ­¥é•¿çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæµä½“åŠ¨åŠ›å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVICONæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºå‡†çº¿ã€‚</li>
<li>VICONå‡å°‘äº†å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>VICONæ”¯æŒçµæ´»çš„æ»šåŠ¨ç­–ç•¥ï¼Œé€‚åº”ä¸åŒçš„æ—¶é—´æ­¥é•¿è·¨åº¦ï¼Œé€‚ç”¨äºä¸å®Œç¾æµ‹é‡ç³»ç»Ÿã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œçš„æŒ‘æˆ˜ä¸­ï¼ŒVICONå±•ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9545288bde404a798f18b697d3224466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec102855865092460d2f3315e6ffb8ca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0c0a12e0cc68f8d5a98251283187ccc4.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Unified Cross-modal Translation of Score Images, Symbolic Music, and   Performance Audio
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3ec637713004f9dcd03966fd2af0c98.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Unified Cross-modal Translation of Score Images, Symbolic Music, and   Performance Audio
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
