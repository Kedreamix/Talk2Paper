<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-21  Unified Cross-modal Translation of Score Images, Symbolic Music, and   Performance Audio">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0c0a12e0cc68f8d5a98251283187ccc4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-21-更新"><a href="#2025-05-21-更新" class="headerlink" title="2025-05-21 更新"></a>2025-05-21 更新</h1><h2 id="Unified-Cross-modal-Translation-of-Score-Images-Symbolic-Music-and-Performance-Audio"><a href="#Unified-Cross-modal-Translation-of-Score-Images-Symbolic-Music-and-Performance-Audio" class="headerlink" title="Unified Cross-modal Translation of Score Images, Symbolic Music, and   Performance Audio"></a>Unified Cross-modal Translation of Score Images, Symbolic Music, and   Performance Audio</h2><p><strong>Authors:Jongmin Jung, Dongmin Kim, Sihun Lee, Seola Cho, Hyungjoon Soh, Irmak Bukey, Chris Donahue, Dasaem Jeong</strong></p>
<p>Music exists in various modalities, such as score images, symbolic scores, MIDI, and audio. Translations between each modality are established as core tasks of music information retrieval, such as automatic music transcription (audio-to-MIDI) and optical music recognition (score image to symbolic score). However, most past work on multimodal translation trains specialized models on individual translation tasks. In this paper, we propose a unified approach, where we train a general-purpose model on many translation tasks simultaneously. Two key factors make this unified approach viable: a new large-scale dataset and the tokenization of each modality. Firstly, we propose a new dataset that consists of more than 1,300 hours of paired audio-score image data collected from YouTube videos, which is an order of magnitude larger than any existing music modal translation datasets. Secondly, our unified tokenization framework discretizes score images, audio, MIDI, and MusicXML into a sequence of tokens, enabling a single encoder-decoder Transformer to tackle multiple cross-modal translation as one coherent sequence-to-sequence task. Experimental results confirm that our unified multitask model improves upon single-task baselines in several key areas, notably reducing the symbol error rate for optical music recognition from 24.58% to a state-of-the-art 13.67%, while similarly substantial improvements are observed across the other translation tasks. Notably, our approach achieves the first successful score-image-conditioned audio generation, marking a significant breakthrough in cross-modal music generation. </p>
<blockquote>
<p>音乐存在于多种模态中，如乐谱图像、符号乐谱、MIDI和音乐音频。各种模态之间的翻译被确立为音乐信息检索的核心任务，如自动音乐转录（音频到MIDI）和乐谱识别（乐谱图像到符号乐谱）。然而，过去大多数关于多模态翻译的工作都是在单个翻译任务上训练特定模型。在本文中，我们提出了一种通用方法，即同时训练多个翻译任务的通用模型。两个关键因素使这种统一的方法可行：一是新的大规模数据集和每种模态的标记化。首先，我们提出了一个新的数据集，该数据集包含从YouTube视频收集的超过1300小时的配对音频乐谱图像数据，其规模是现有音乐模态翻译数据集的十倍。其次，我们的统一标记化框架将乐谱图像、音频、MIDI和MusicXML离散成一系列标记，使得单个编码器-解码器转换器能够作为一个连贯的序列到序列任务来处理多个跨模态翻译。实验结果证实，我们的多任务统一模型在几个关键领域优于单任务基线模型，特别是将乐谱识别的符号错误率从24.58%降低到最先进的13.67%，同时在其他翻译任务上也观察到了类似的显著改进。值得注意的是，我们的方法实现了首次乐谱图像条件下的音频生成，标志着跨模态音乐生成方面的一个重大突破。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12863v1">PDF</a> Submitted to IEEE Transactions on Audio, Speech and Language   Processing (TASLPRO)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种统一的多模态音乐翻译方法，该方法使用一个通用模型同时处理多种翻译任务，包括音频转MIDI和乐谱图像转乐谱符号等。该研究创新点在于：一是构建了一个大规模的多模态音乐翻译数据集，包含超过1300小时的音频和乐谱图像配对数据；二是提出了一个统一的符号化框架，将乐谱图像、音频、MIDI和音乐XML等格式离散化为一系列符号序列，使得一个单一的编码解码器就能够处理多个跨模态翻译任务。实验结果显示，该统一多任务模型在多个关键领域优于单任务基准模型，如光学音乐识别的符号错误率从24.58%降低到最新的13.67%。此外，该研究还首次实现了基于乐谱图像条件的音频生成，标志着跨模态音乐生成领域的一个重大突破。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一个统一的多模态音乐翻译方法，涵盖多种音乐模态之间的翻译任务。</li>
<li>构建了一个大规模的音乐模态翻译数据集，包含音频和乐谱图像配对数据。</li>
<li>提出了一个统一的符号化框架，将不同音乐模态转换为符号序列，为统一模型处理多模态翻译任务提供了可能。</li>
<li>统一多任务模型在跨模态音乐翻译任务上表现出优异性能，显著降低了光学音乐识别的符号错误率。</li>
<li>研究实现了首次基于乐谱图像条件的音频生成，标志着跨模态音乐生成的重要进展。</li>
<li>该研究为音乐信息检索中的多模态翻译任务提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1a223183fb176ab81ab89175c9726687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5cadc55808ebc7fccd5cec3fe54c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dccfac73f319fc103a37e8dcb72563a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3ec637713004f9dcd03966fd2af0c98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef52a0d56c85acf174ea222360d85b3e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RoboFAC-A-Comprehensive-Framework-for-Robotic-Failure-Analysis-and-Correction"><a href="#RoboFAC-A-Comprehensive-Framework-for-Robotic-Failure-Analysis-and-Correction" class="headerlink" title="RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and   Correction"></a>RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and   Correction</h2><p><strong>Authors:Weifeng Lu, Minghao Ye, Zewei Ye, Ruihan Tao, Shuo Yang, Bo Zhao</strong></p>
<p>Vision-Language-Action (VLA) models have recently advanced robotic manipulation by translating natural-language instructions and image information into sequential control actions. However, these models often underperform in open-world scenarios, as they are predominantly trained on successful expert demonstrations and exhibit a limited capacity for failure recovery. In this work, we present a Robotic Failure Analysis and Correction (RoboFAC) framework to address this issue. Firstly, we construct RoboFAC dataset comprising 9,440 erroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks and 53 scenes in both simulation and real-world environments. Leveraging our dataset, we develop RoboFAC model, which is capable of Task Understanding, Failure Analysis and Failure Correction. Experimental results demonstrate that the RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark. Furthermore, we integrate the RoboFAC model into a real-world VLA control pipeline as an external supervision providing correction instructions, yielding a 29.1% relative improvement on average on four real-world tasks. The results show that our RoboFAC framework effectively handles robotic failures and assists the VLA model in recovering from failures. </p>
<blockquote>
<p>视觉语言动作（VLA）模型最近通过将自然语言指令和图像信息翻译成一系列控制动作，推动了机器人操作技术的发展。然而，这些模型在开放世界场景中往往表现不佳，因为它们主要基于成功的专家演示，并且在故障恢复方面能力有限。针对这一问题，我们在工作中提出了机器人故障分析与纠正（RoboFAC）框架。首先，我们构建了RoboFAC数据集，其中包含9440条错误的操作轨迹和78623对问答，涉及仿真和真实环境中的16个任务和53个场景。利用我们的数据集，我们开发了RoboFAC模型，具备任务理解、故障分析和故障纠正能力。实验结果表明，RoboFAC模型在我们的评估基准测试上的表现优于GPT-4o，达到34.1%。此外，我们将RoboFAC模型集成到现实世界中的VLA控制管道中，作为外部监督提供纠正指令，在四个真实任务上平均提高了29.1%的相对表现。结果表明，我们的RoboFAC框架有效地处理了机器人故障，并帮助VLA模型从故障中恢复。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12224v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究提出了Robotic Failure Analysis and Correction（RoboFAC）框架，以解决Vision-Language-Action（VLA）模型在开放世界场景下性能不足的问题。该框架构建了包含错误操作轨迹和问答对的RoboFAC数据集，并开发了具备任务理解、故障分析和故障纠正能力的RoboFAC模型。实验结果表明，RoboFAC模型在评估基准测试上优于GPT-4o达34.1%，并且集成到真实世界的VLA控制管道中后，平均相对改进率为29.1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLA模型通过将自然语言指令和图像信息翻译为连续的控制动作，推动了机器人操作的发展。</li>
<li>VLA模型在开放世界场景中的性能有待提高，尤其对于错误恢复的能力有限。</li>
<li>研究人员构建了RoboFAC数据集，包含错误操作轨迹和跨不同任务和场景的问答对。</li>
<li>开发出的RoboFAC模型具备任务理解、故障分析和纠正功能。</li>
<li>实验显示，RoboFAC模型在评估基准测试上的性能优于GPT-4o达34.1%。</li>
<li>在真实世界的VLA控制管道中集成RoboFAC模型后，实现了平均相对改进率29.1%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eea799b80640e0aa70556c743e3ed253.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d721beeab6efb78459f0508c77139380.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c0a12e0cc68f8d5a98251283187ccc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28a9190085b1bad16da446f77f187593.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20e377315da7bbbc9c0cd5ad31d1a81.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluation-and-optimization-of-deep-learning-models-for-enhanced-detection-of-brain-cancer-using-transmission-optical-microscopy-of-thin-brain-tissue-samples"><a href="#Evaluation-and-optimization-of-deep-learning-models-for-enhanced-detection-of-brain-cancer-using-transmission-optical-microscopy-of-thin-brain-tissue-samples" class="headerlink" title="Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples"></a>Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples</h2><p><strong>Authors:Mohnish Sao, Mousa Alrubayan, Prabhakar Pradhan</strong></p>
<p>Optical transmission spectroscopy is one method to understand brain tissue structural properties from brain tissue biopsy samples, yet manual interpretation is resource intensive and prone to inter observer variability. Deep convolutional neural networks (CNNs) offer automated feature learning directly from raw brightfield images. Here, we evaluate ResNet50 and DenseNet121 on a curated dataset of 2,931 bright-field transmission optical microscopy images of thin brain tissue, split into 1,996 for training, 437 for validation, and 498 for testing. Our two stage transfer learning protocol involves initial training of a classifier head on frozen pretrained feature extractors, followed by fine tuning of deeper convolutional blocks with extensive data augmentation (rotations, flips, intensity jitter) and early stopping. DenseNet121 achieves 88.35 percent test accuracy, 0.9614 precision, 0.8667 recall, and 0.9116 F1 score the best performance compared to ResNet50 (82.12 percent, 0.9035, 0.8142, 0.8563). Detailed analysis of confusion matrices, training and validation curves, and classwise prediction distributions illustrates robust convergence and minimal bias. These findings demonstrate the superior generalization of dense connectivity on limited medical datasets and outline future directions for multi-class tumor grading and clinical translation. </p>
<blockquote>
<p>光学传输光谱法是一种通过脑组织活检样本了解脑组织结构性质的方法，但人工解读需要耗费大量资源，并且容易因观察者之间差异而导致结果差异。深度卷积神经网络（CNN）可以直接从原始明场图像中实现自动化特征学习。在此，我们对包含有用于训练的1996张图像、用于验证的437张图像以及用于测试的498张图像的精选数据集进行了ResNet50和DenseNet121评估。我们的两阶段迁移学习协议涉及首先在冻结的预训练特征提取器上训练分类器头部，然后通过丰富的数据增强（旋转、翻转、强度抖动）进行更深的卷积块的微调并使用早期停止策略。DenseNet121在测试集上取得了最佳性能，测试准确度为88.35%，精确度、召回率和F1分数分别为0.9614、0.8667和0.9116。与ResNet50（测试准确度为82.12%，精确度、召回率和F1分数分别为0.9035、0.8142和0.8563）相比具有优势。通过对混淆矩阵、训练和验证曲线以及类别预测分布进行详细分析，显示出稳健的收敛性和较小的偏差。这些结果表明密集连接在有限的医学数据集上具有优越的一般化能力，并概述了未来多类肿瘤分级和临床转化的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11735v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong>：利用光学传输光谱法了解脑组织活检样本的结构特性，但手动解读需要大量资源且易出现观察者间差异。本研究采用深度卷积神经网络（CNN）直接从原始明场图像中学习特征，对ResNet50和DenseNet121进行评估。在精选的包含2931张薄脑组织明场透射光学显微镜图像的数据集上，应用两阶段迁移学习协议，实现分类器头部的初步训练以及对冻结的预训练特征提取器的精细调整。通过数据增强（旋转、翻转、强度抖动）和早期停止策略，DenseNet121取得了最佳性能，测试准确度为88.35%，精确度、召回率和F1分数分别为0.9614、0.8667和0.9116。而ResNet50的性能稍逊，准确率为82.12%。研究结果表明DenseNet具有优越的泛化能力，未来有望应用于多类肿瘤分级和临床转化研究。</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>本研究使用深度卷积神经网络（CNN）从薄脑组织明场透射光学显微镜图像中自动学习特征。</li>
<li>通过两阶段迁移学习协议，DenseNet121在数据集上取得了最佳性能。</li>
<li>DenseNet121的测试准确度达到88.35%，并且具有较高的精确度、召回率和F1分数。</li>
<li>研究表明DenseNet具有优越的泛化能力，尤其在有限的医学数据集上。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11735">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-736b31df4f8ccfb8023b1def31398409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f41c0c47b5fff1a002e3bf704b07e69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-945dc21300ba24d5b54a51a28e85f3b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2a48cd6105529db4575d10a05a59f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1d6409e5eb1888eee6f00e5afd70392.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PrePrompt-Predictive-prompting-for-class-incremental-learning"><a href="#PrePrompt-Predictive-prompting-for-class-incremental-learning" class="headerlink" title="PrePrompt: Predictive prompting for class incremental learning"></a>PrePrompt: Predictive prompting for class incremental learning</h2><p><strong>Authors:Libo Huang, Zhulin An, Chuanguang Yang, Boyu Diao, Fei Wang, Yan Zeng, Zhifeng Hao, Yongjun Xu</strong></p>
<p>Class Incremental Learning (CIL) based on pre-trained models offers a promising direction for open-world continual learning. Existing methods typically rely on correlation-based strategies, where an image’s classification feature is used as a query to retrieve the most related key prompts and select the corresponding value prompts for training. However, these approaches face an inherent limitation: fitting the entire feature space of all tasks with only a few trainable prompts is fundamentally challenging. We propose Predictive Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based limitations by leveraging pre-trained models’ natural classification ability to predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a two-stage prediction framework: task-specific prompt prediction followed by label prediction. While theoretically appealing, this framework risks bias toward recent classes due to missing historical data for older classifier calibration. PrePrompt then mitigates this by incorporating feature translation, dynamically balancing stability and plasticity. Experiments across multiple benchmarks demonstrate PrePrompt’s superiority over state-of-the-art prompt-based CIL methods. Code available at \href{github.com&#x2F;libo-huang&#x2F;preprompt}{github.com&#x2F;libo-huang&#x2F;preprompt}. </p>
<blockquote>
<p>基于预训练模型的类增量学习（CIL）为开放世界持续学习提供了有前景的方向。现有方法通常依赖于基于关联的策略，其中使用图像的分类特征作为查询来检索最相关的关键提示，并选择相应的值提示进行训练。然而，这些方法面临一个固有的局限性：用少数可训练提示来适应所有任务的整体特征空间具有根本挑战性。我们提出了预测提示（PrePrompt），这是一种新型CIL框架，它通过利用预训练模型的天然分类能力来预测特定任务的提示，从而避免了基于关联的限制。具体来说，PrePrompt将CIL分解为一个两阶段预测框架：特定任务的提示预测，然后是标签预测。虽然这在理论上很有吸引力，但由于缺少旧分类器的历史数据进行校准，这个框架可能偏向最近的类别。PrePrompt通过结合特征翻译来减轻这一问题，动态平衡稳定性和可塑性。在多个基准测试上的实验表明，PrePrompt优于最先进的基于提示的CIL方法。代码可在<a href="github.com/libo-huang/preprompt">github.com&#x2F;libo-huang&#x2F;preprompt</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08586v2">PDF</a> 16 pages, 29 figures, conference</p>
<p><strong>Summary</strong></p>
<p>基于预训练模型的类增量学习（CIL）为开放世界持续学习提供了有前景的方向。现有方法通常依赖于基于关联的策略，使用图像的识别特征作为查询来检索最相关的关键提示，并选择对应的值提示进行训练。然而，这些方法面临内在局限：用少量可训练提示来拟合所有任务的整个特征空间具有根本挑战性。我们提出预测提示（PrePrompt）这一新型CIL框架，通过利用预训练模型的天然分类能力来预测任务特定提示，从而规避了基于关联的限制。PrePrompt将CIL分解为两个阶段：任务特定提示预测和标签预测。尽管理论上具有吸引力，但由于缺少对旧分类器的校准历史数据，此框架可能偏向最近的类别。PrePrompt通过引入特征翻译来平衡稳定性和可塑性，从而缓解这一问题。在多个基准测试上的实验表明，PrePrompt优于最新的提示式CIL方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>类增量学习(CIL)是开放世界持续学习的一个有前途的方向。</li>
<li>现有方法主要依赖基于关联的策略，使用图像分类特征作为查询来检索提示。</li>
<li>基于预训练模型的PrePrompt框架通过预测任务特定提示来规避关联限制。</li>
<li>PrePrompt将CIL分解为任务特定提示预测和标签预测两个阶段。</li>
<li>PrePrompt可能偏向最近的类别，因为缺少旧分类器的校准历史数据。</li>
<li>特征翻译被引入到PrePrompt中以平衡稳定性和可塑性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08586">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-facf7fa48b38a06aa66f71fd02a3bf1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b5978e59d577d95ac59b8206c7d5a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-071d91910c0f8c3c56f90fa0b44b737e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Origin-Identification-for-Text-Guided-Image-to-Image-Diffusion-Models"><a href="#Origin-Identification-for-Text-Guided-Image-to-Image-Diffusion-Models" class="headerlink" title="Origin Identification for Text-Guided Image-to-Image Diffusion Models"></a>Origin Identification for Text-Guided Image-to-Image Diffusion Models</h2><p><strong>Authors:Wenhao Wang, Yifan Sun, Zongxin Yang, Zhentao Tan, Zhengdong Hu, Yi Yang</strong></p>
<p>Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused for spreading misinformation, infringing on copyrights, and evading content tracing. This motivates us to introduce the task of origin IDentification for text-guided Image-to-image Diffusion models (ID$^2$), aiming to retrieve the original image of a given translated query. A straightforward solution to ID$^2$ involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due to visual discrepancy across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID$^2$ task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset, OriPID, contains abundant Origins and guided Prompts, which can be used to train and test potential IDentification models across various diffusion models. In the method section, we first prove the existence of a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder (VAE) embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can be generalized across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods ($+31.6%$ mAP), even those with generalization designs. The project is available at <a target="_blank" rel="noopener" href="https://id2icml.github.io/">https://id2icml.github.io</a>. </p>
<blockquote>
<p>文本引导的图像到图像扩散模型在根据文本提示进行图像转换方面表现出色，可以进行精确和创造性的视觉修改。然而，这种强大的技术可能会被误用，用于传播错误信息、侵犯版权和规避内容追踪。这促使我们引入针对文本引导的图像到图像扩散模型的起源识别任务（ID$^2$），旨在检索给定翻译查询的原始图像。一种解决ID$^2$的直观方法是通过训练专门的深度嵌入模型来提取和比较查询图像和参考图像的特征。然而，由于不同扩散模型产生的各代图像之间的视觉差异，这种基于相似度的方法在用一个模型训练而在另一个模型测试时会失败，从而限制了其在现实世界应用中的有效性。为了解决所提出的ID$^2$任务的这一挑战，我们提供了第一个数据集和一种理论上有保证的方法，两者都强调通用性。我们精心制作的数据集OriPID包含丰富的起源和引导提示，可用于训练和测试各种扩散模型的潜在识别模型。在方法部分，我们首先证明存在一个线性变换可以最小化预训练变分自编码器（VAE）生成的样本嵌入与其原始样本之间的距离。随后，演示了这种简单的线性变换可以在不同的扩散模型之间进行泛化。实验结果表明，所提出的方法达到了令人满意的泛化性能，显著超越了基于相似度的方法（提高31.6%的mAP），甚至是那些具有泛化设计的方法。该项目在<a target="_blank" rel="noopener" href="https://id2icml.github.io可用./">https://id2icml.github.io可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02376v2">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong>：文本引导的图像到图像扩散模型能够根据文本提示进行精确而富有创意的图像修改。但该技术可能会被误用，如传播误导信息、侵犯版权和规避内容追踪。为此，提出了针对文本引导的图像到图像扩散模型的起源识别任务（ID$^2$），旨在检索给定翻译查询的原始图像。我们为此任务贡献第一个数据集和一种理论上有保证的方法，都强调通用性。数据集包含丰富的起源和引导提示，可用于训练和测试跨各种扩散模型的潜在识别模型。方法部分证明存在一个线性变换可以最小化生成样本的预训练变分自编码器嵌入与其起源之间的距离，这种简单的线性变换可以在不同的扩散模型中进行泛化。实验结果验证了该方法的良好泛化性能，显著优于基于相似度的方法（提高31.6%的mAP）。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>文本引导的图像到图像扩散模型可以根据文本提示进行精确和创意的图像修改。</li>
<li>这种技术可能被误用，导致传播误导信息、侵犯版权和规避内容追踪等问题。</li>
<li>针对此问题，提出了起源识别任务（ID$^2$），旨在检索给定翻译查询的原始图像。</li>
<li>贡献了一个针对ID$^2$任务的数据集（OriPID），包含用于训练和测试的起源和引导提示。</li>
<li>提出了一种理论上有保证的方法，通过线性变换来最小化生成样本与其起源之间的距离，并在不同的扩散模型中进行泛化。</li>
<li>实验结果表明该方法具有良好的泛化性能，显著优于基于相似度的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02376">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d8de49df7a1812a7b7969949c4e739e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d46e2f2c3483dfcfba22c6eb6f9b28ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cab6ee8cc28c13a3e3261d7b12d8c9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953a8b3e2f7ad28187df1b032ed5b565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f009329749a2fb68399d6a6e2105ddbf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-modal-MRI-Translation-via-Evidential-Regression-and-Distribution-Calibration"><a href="#Multi-modal-MRI-Translation-via-Evidential-Regression-and-Distribution-Calibration" class="headerlink" title="Multi-modal MRI Translation via Evidential Regression and Distribution   Calibration"></a>Multi-modal MRI Translation via Evidential Regression and Distribution   Calibration</h2><p><strong>Authors:Jiyao Liu, Shangqi Gao, Yuxin Li, Lihao Liu, Xin Gao, Zhaohu Xing, Junzhi Ning, Yanzhou Su, Xiao-Yong Zhang, Junjun He, Ningsheng Xu, Xiahai Zhuang</strong></p>
<p>Multi-modal Magnetic Resonance Imaging (MRI) translation leverages information from source MRI sequences to generate target modalities, enabling comprehensive diagnosis while overcoming the limitations of acquiring all sequences. While existing deep-learning-based multi-modal MRI translation methods have shown promising potential, they still face two key challenges: 1) lack of reliable uncertainty quantification for synthesized images, and 2) limited robustness when deployed across different medical centers. To address these challenges, we propose a novel framework that reformulates multi-modal MRI translation as a multi-modal evidential regression problem with distribution calibration. Our approach incorporates two key components: 1) an evidential regression module that estimates uncertainties from different source modalities and an explicit distribution mixture strategy for transparent multi-modal fusion, and 2) a distribution calibration mechanism that adapts to source-target mapping shifts to ensure consistent performance across different medical centers. Extensive experiments on three datasets from the BraTS2023 challenge demonstrate that our framework achieves superior performance and robustness across domains. </p>
<blockquote>
<p>多模态磁共振成像（MRI）翻译技术利用源MRI序列的信息来生成目标模态，从而实现全面诊断，同时克服了获取所有序列的局限性。虽然现有的基于深度学习的多模态MRI翻译方法已显示出有前途的潜力，但它们仍面临两个关键挑战：一是合成图像缺乏可靠的不确定性量化，二是在不同医疗中心部署时的稳健性有限。为了解决这些挑战，我们提出了一种新的框架，它将多模态MRI翻译重新表述为一个具有分布校准的多模态证据回归问题。我们的方法包含两个关键组成部分：一是证据回归模块，用于估计来自不同源模态的不确定性，并给出一个透明的多模态融合显式分布混合策略；二是分布校准机制，该机制能够适应源目标映射变化，以确保在不同医疗中心之间的性能一致性。在BraTS2023挑战的三个数据集上的广泛实验表明，我们的框架在跨域性能方面表现出卓越的性能和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07372v2">PDF</a> Early accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>多模态磁共振成像（MRI）转换利用源MRI序列的信息生成目标模态，实现全面诊断，同时克服获取所有序列的限制。针对现有深度学习多模态MRI转换方法面临的挑战，包括合成图像的不确定性量化不足和不同医疗中心部署的鲁棒性有限，我们提出了一种新的框架。该框架将多模态MRI转换重新构建为多模态证据回归问题，并引入分布校准机制。通过证据回归模块估计不同源模态的不确定性，并采用透明多模态融合策略进行分布校准。实验表明，该框架在BraTS2023挑战赛的三个数据集上表现出卓越的性能和跨域鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态MRI转换利用源MRI序列信息生成目标模态，促进全面诊断。</li>
<li>现有深度学习多模态MRI转换方法面临两个主要挑战：缺乏合成图像的不确定性量化以及跨不同医疗中心的鲁棒性有限。</li>
<li>新框架将多模态MRI转换重新构建为多模态证据回归问题，以处理源MRI序列的不确定性。</li>
<li>新框架引入证据回归模块，从多种源模态估计不确定性，并采用透明的多模态融合策略。</li>
<li>分布校准机制能够适应源-目标映射变化，确保在不同医疗中心之间的一致性能。</li>
<li>在BraTS2023挑战赛的数据集上进行的大量实验表明，新框架在性能和鲁棒性方面表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07372">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-82e23d5d6d2149263417f04e91c3b4a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96123a9c6798b777cb76890db439723d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-614ca8371fabf46ed2c2cdee9ead7e35.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a51de098d53d08fbc73756f3b7da7ab.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-05-21  HiERO understanding the hierarchy of human behavior enhances reasoning   on egocentric videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-50811f3e0aeae9bea6f916af2905c28b.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-21  GMM-Based Comprehensive Feature Extraction and Relative Distance   Preservation For Few-Shot Cross-Modal Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
