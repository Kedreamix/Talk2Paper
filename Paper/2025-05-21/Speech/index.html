<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Granary Speech Recognition and Translation Dataset in 25 European   Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-87fe7b6258e531bf70c29a1a81942aeb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-21-æ›´æ–°"><a href="#2025-05-21-æ›´æ–°" class="headerlink" title="2025-05-21 æ›´æ–°"></a>2025-05-21 æ›´æ–°</h1><h2 id="Granary-Speech-Recognition-and-Translation-Dataset-in-25-European-Languages"><a href="#Granary-Speech-Recognition-and-Translation-Dataset-in-25-European-Languages" class="headerlink" title="Granary: Speech Recognition and Translation Dataset in 25 European   Languages"></a>Granary: Speech Recognition and Translation Dataset in 25 European   Languages</h2><p><strong>Authors:Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg</strong></p>
<p>Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at <a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary">https://hf.co/datasets/nvidia/Granary</a> </p>
<blockquote>
<p>å¤šä»»åŠ¡å’Œå¤šè¯­è¨€æ–¹æ³•å¯¹äºå¤§å‹æ¨¡å‹æœ‰ç›Šï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºï¼Œé’ˆå¯¹ä½èµ„æºè¯­è¨€çš„è¯­éŸ³å¤„ç†ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Granaryï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–25ç§æ¬§æ´²è¯­è¨€çš„è¯­éŸ³æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œç”¨äºè¯†åˆ«å’Œç¿»è¯‘ã€‚è¿™æ˜¯ç”¨äºè½¬å½•å’Œç¿»è¯‘çš„æ­¤ç±»è§„æ¨¡çš„é¦–æ¬¡å¼€æºå°è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨ä¼ªæ ‡ç­¾ç®¡é“æé«˜æ•°æ®è´¨é‡ï¼ŒåŒ…æ‹¬åˆ†æ®µã€ä¸¤é˜¶æ®µæ¨æ–­ã€å¹»è§‰è¿‡æ»¤å’Œæ ‡ç‚¹æ¢å¤ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨EuroLLMä»ä¼ªæ ‡ç­¾è½¬å½•ä¸­ç”Ÿæˆç¿»è¯‘å¯¹ï¼Œéšåè¿›è¡Œæ•°æ®å¤„ç†ç®¡é“ã€‚æˆ‘ä»¬çš„è®¾è®¡æ³¨é‡æ•ˆç‡ï¼Œèƒ½åœ¨æ•°å°æ—¶å†…å¤„ç†å¤§é‡æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡åœ¨é’ˆå¯¹é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„å…ˆå‰æ•´ç†çš„æ•°æ®é›†ä¸Šæ¯”è¾ƒæ€§èƒ½æ¥è¯„ä¼°ç»è¿‡å¤„ç†çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§çº¦å‡å°‘50%çš„æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†ç±»ä¼¼çš„æ€§èƒ½ã€‚æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://hf.co/datasets/nvidia/Granaryä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13404v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¤šè¯­ç§è¯­éŸ³æ•°æ®é›†Granaryå‘å¸ƒï¼ŒåŒ…å«25ç§æ¬§æ´²è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ•°æ®ã€‚é‡‡ç”¨ä¼ªæ ‡ç­¾æµæ°´çº¿æé«˜æ•°æ®è´¨é‡ï¼Œå¹¶ä½¿ç”¨EuroLLMç”Ÿæˆç¿»è¯‘é…å¯¹ã€‚è®­ç»ƒåœ¨æ­¤æ•°æ®ä¸Šçš„æ¨¡å‹åœ¨é«˜ä½èµ„æºè¯­è¨€ä¸Šçš„æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œä»…ä½¿ç”¨çº¦50%çš„æ•°æ®ä¾¿è¾¾åˆ°äº†ç›¸ä¼¼çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Granaryæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­ç§è¯­éŸ³æ•°æ®é›†ï¼Œæ¶µç›–25ç§æ¬§æ´²è¯­è¨€ï¼Œæ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€è¯­éŸ³å¤„ç†çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¼ªæ ‡ç­¾æµæ°´çº¿æé«˜æ•°æ®è´¨é‡ï¼ŒåŒ…æ‹¬åˆ†æ®µã€ä¸¤é˜¶æ®µæ¨æ–­ã€å¹»è§‰è¿‡æ»¤å’Œæ ‡ç‚¹æ¢å¤ã€‚</li>
<li>ä½¿ç”¨EuroLLMä»ä¼ªæ ‡ç­¾è½¬å½•ç”Ÿæˆç¿»è¯‘é…å¯¹ã€‚</li>
<li>è®¾è®¡äº†é«˜æ•ˆçš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œå¯åœ¨æ•°å°æ—¶å†…å¤„ç†å¤§é‡æ•°æ®ã€‚</li>
<li>åœ¨å·²æ•´ç†çš„é«˜ã€ä½èµ„æºè¯­è¨€æ•°æ®é›†ä¸Šè¯„ä¼°è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å‡å°‘çº¦50%æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†ç›¸ä¼¼çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f17d33fb20fc0b6dbbeea053cd826431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1351ee0dd08e8d9946525486538385c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a31acfa0ff8b0f5423755fcea7040ad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f55deb30f8415f6c3755638f6ccd8ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4101ea0665a3dd64b2bf784bb87a5bb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-744a25304eeda920133272d6511fc538.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Time-Frequency-Based-Attention-Cache-Memory-Model-for-Real-Time-Speech-Separation"><a href="#Time-Frequency-Based-Attention-Cache-Memory-Model-for-Real-Time-Speech-Separation" class="headerlink" title="Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech   Separation"></a>Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech   Separation</h2><p><strong>Authors:Guo Chen, Kai Li, Runxuan Yang, Xiaolin Hu</strong></p>
<p>Existing causal speech separation models often underperform compared to non-causal models due to difficulties in retaining historical information. To address this, we propose the Time-Frequency Attention Cache Memory (TFACM) model, which effectively captures spatio-temporal relationships through an attention mechanism and cache memory (CM) for historical information storage. In TFACM, an LSTM layer captures frequency-relative positions, while causal modeling is applied to the time dimension using local and global representations. The CM module stores past information, and the causal attention refinement (CAR) module further enhances time-based feature representations for finer granularity. Experimental results showed that TFACM achieveed comparable performance to the SOTA TF-GridNet-Causal model, with significantly lower complexity and fewer trainable parameters. For more details, visit the project page: <a target="_blank" rel="noopener" href="https://cslikai.cn/TFACM/">https://cslikai.cn/TFACM/</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å› æœè¯­éŸ³åˆ†ç¦»æ¨¡å‹ç”±äºéš¾ä»¥ä¿ç•™å†å²ä¿¡æ¯ï¼Œé€šå¸¸ä¸éå› æœæ¨¡å‹ç›¸æ¯”è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´-é¢‘ç‡æ³¨æ„åŠ›ç¼“å­˜å†…å­˜ï¼ˆTFACMï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å’Œç¼“å­˜å†…å­˜ï¼ˆCMï¼‰æœ‰æ•ˆæ•æ‰æ—¶ç©ºå…³ç³»ï¼Œç”¨äºå†å²ä¿¡æ¯çš„å­˜å‚¨ã€‚åœ¨TFACMä¸­ï¼ŒLSTMå±‚æ•è·é¢‘ç‡ç›¸å¯¹ä½ç½®ï¼Œè€Œå› æœå»ºæ¨¡åˆ™åº”ç”¨äºæ—¶é—´ç»´åº¦ï¼Œä½¿ç”¨å±€éƒ¨å’Œå…¨å±€è¡¨ç¤ºã€‚CMæ¨¡å—å­˜å‚¨è¿‡å»çš„ä¿¡æ¯ï¼Œå› æœæ³¨æ„åŠ›ç»†åŒ–ï¼ˆCARï¼‰æ¨¡å—è¿›ä¸€æ­¥å¢å¼ºäº†åŸºäºæ—¶é—´çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„ç²’åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTFACMæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯TF-GridNet-Causalæ¨¡å‹çš„æ°´å¹³ï¼ŒåŒæ—¶å¤æ‚åº¦æ›´ä½ï¼Œå¯è®­ç»ƒå‚æ•°æ›´å°‘ã€‚å¦‚éœ€æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cslikai.cn/TFACM/%E3%80%82">https://cslikai.cn/TFACM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13094v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ç°æœ‰å› æœè¯­éŸ³åˆ†ç¦»æ¨¡å‹å› éš¾ä»¥ä¿ç•™å†å²ä¿¡æ¯è€Œæ€§èƒ½ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºTFACMæ¨¡å‹ï¼Œå®ƒé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å’Œç¼“å­˜å­˜å‚¨å™¨æ•æ‰æ—¶ç©ºå…³ç³»æ¥æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚TFACMä½¿ç”¨LSTMå±‚æ•æ‰é¢‘ç‡ç›¸å¯¹ä½ç½®ï¼Œå¹¶åœ¨æ—¶é—´ç»´åº¦ä¸Šåº”ç”¨å› æœå»ºæ¨¡ã€‚ç¼“å­˜å­˜å‚¨å™¨å­˜å‚¨è¿‡å»ä¿¡æ¯ï¼Œå› æœæ³¨æ„åŠ›ç»†åŒ–æ¨¡å—è¿›ä¸€æ­¥æé«˜äº†æ—¶é—´ç‰¹å¾çš„ç²¾ç»†åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTFACMæ¨¡å‹æ€§èƒ½ä¸SOTA TF-GridNet-Causalæ¨¡å‹ç›¸å½“ï¼Œä½†å¤æ‚åº¦æ›´ä½ï¼Œå¯è®­ç»ƒå‚æ•°æ›´å°‘ã€‚æ›´å¤šè¯¦æƒ…è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cslikai.cn/TFACM/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç°æœ‰å› æœè¯­éŸ³åˆ†ç¦»æ¨¡å‹ç”±äºéš¾ä»¥ä¿ç•™å†å²ä¿¡æ¯è€Œæ€§èƒ½å—é™ã€‚</li>
<li>TFACMæ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å’Œç¼“å­˜å­˜å‚¨å™¨æ•æ‰æ—¶ç©ºå…³ç³»æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LSTMå±‚ç”¨äºæ•æ‰é¢‘ç‡ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚</li>
<li>æ—¶é—´ç»´åº¦ä¸Šåº”ç”¨äº†å› æœå»ºæ¨¡ï¼Œé‡‡ç”¨å±€éƒ¨å’Œå…¨å±€è¡¨ç¤ºã€‚</li>
<li>ç¼“å­˜å­˜å‚¨å™¨æ¨¡å—ç”¨äºå­˜å‚¨è¿‡å»ä¿¡æ¯ã€‚</li>
<li>å› æœæ³¨æ„åŠ›ç»†åŒ–æ¨¡å—æé«˜äº†æ—¶é—´ç‰¹å¾çš„ç²¾ç»†åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b7f1886d39e7cbe12f79668f292689c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2edf2e7eb27e6ecec64353c78e9a23ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d4bf0c1ae49da49d583a7923caf831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba2efad559d3740bf38f05f853cc6081.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Cross-modal-Knowledge-Transfer-Learning-as-Graph-Matching-Based-on-Optimal-Transport-for-ASR"><a href="#Cross-modal-Knowledge-Transfer-Learning-as-Graph-Matching-Based-on-Optimal-Transport-for-ASR" class="headerlink" title="Cross-modal Knowledge Transfer Learning as Graph Matching Based on   Optimal Transport for ASR"></a>Cross-modal Knowledge Transfer Learning as Graph Matching Based on   Optimal Transport for ASR</h2><p><strong>Authors:Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai</strong></p>
<p>Transferring linguistic knowledge from a pretrained language model (PLM) to acoustic feature learning has proven effective in enhancing end-to-end automatic speech recognition (E2E-ASR). However, aligning representations between linguistic and acoustic modalities remains a challenge due to inherent modality gaps. Optimal transport (OT) has shown promise in mitigating these gaps by minimizing the Wasserstein distance (WD) between linguistic and acoustic feature distributions. However, previous OT-based methods overlook structural relationships, treating feature vectors as unordered sets. To address this, we propose Graph Matching Optimal Transport (GM-OT), which models linguistic and acoustic sequences as structured graphs. Nodes represent feature embeddings, while edges capture temporal and sequential relationships. GM-OT minimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD) (between edges), leading to a fused Gromov-Wasserstein distance (FGWD) formulation. This enables structured alignment and more efficient knowledge transfer compared to existing OT-based approaches. Theoretical analysis further shows that prior OT-based methods in linguistic knowledge transfer can be viewed as a special case within our GM-OT framework. We evaluate GM-OT on Mandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge transfer. Experimental results demonstrate significant performance gains over state-of-the-art models, validating the effectiveness of our approach. </p>
<blockquote>
<p>å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ä¸­çš„è¯­è¨€çŸ¥è¯†è½¬ç§»åˆ°å£°å­¦ç‰¹å¾å­¦ä¹ ï¼Œå·²è¢«è¯æ˜å¯ä»¥å¢å¼ºç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰çš„æ•ˆæœã€‚ç„¶è€Œï¼Œç”±äºå›ºæœ‰çš„æ¨¡æ€å·®è·ï¼Œè¯­è¨€æ¨¡æ€å’Œå£°éŸ³æ¨¡æ€ä¹‹é—´çš„è¡¨ç¤ºå¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰é€šè¿‡æœ€å°åŒ–è¯­è¨€ç‰¹å¾å’Œå£°éŸ³ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„Wassersteinè·ç¦»ï¼ˆWDï¼‰æ¥å¼¥è¡¥è¿™äº›å·®è·ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„åŸºäºOTçš„æ–¹æ³•å¿½è§†äº†ç»“æ„å…³ç³»ï¼Œå°†ç‰¹å¾å‘é‡è§†ä¸ºæ— åºé›†åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å›¾åŒ¹é…æœ€ä¼˜ä¼ è¾“ï¼ˆGM-OTï¼‰ï¼Œå®ƒå°†è¯­è¨€ç‰¹å¾å’Œå£°éŸ³åºåˆ—å»ºæ¨¡ä¸ºç»“æ„å›¾ã€‚èŠ‚ç‚¹ä»£è¡¨ç‰¹å¾åµŒå…¥ï¼Œè€Œè¾¹ç¼˜æ•æ‰æ—¶åºå’Œé¡ºåºå…³ç³»ã€‚GM-OTåŒæ—¶æœ€å°åŒ–èŠ‚ç‚¹ä¹‹é—´çš„WDå’Œè¾¹ç¼˜ä¹‹é—´çš„Gromov-Wassersteinè·ç¦»ï¼ˆGWDï¼‰ï¼Œä»è€Œå½¢æˆäº†èåˆGromov-Wassersteinè·ç¦»ï¼ˆFGWDï¼‰çš„å…¬å¼ã€‚è¿™ä½¿å¾—ç»“æ„å¯¹é½å’ŒçŸ¥è¯†è½¬ç§»ä¸ç°æœ‰çš„åŸºäºOTçš„æ–¹æ³•ç›¸æ¯”æ›´åŠ é«˜æ•ˆã€‚ç†è®ºåˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå…ˆå‰çš„è¯­è¨€çŸ¥è¯†è½¬ç§»ä¸­çš„åŸºäºOTçš„æ–¹æ³•å¯ä»¥è¢«è§†ä¸ºæˆ‘ä»¬GM-OTæ¡†æ¶å†…çš„ç‰¹æ®Šæƒ…å†µã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨åŸºäºCTCçš„E2E-ASRç³»ç»Ÿå¯¹æ™®é€šè¯ASRä¸Šè¯„ä¼°GM-OTè¿›è¡ŒçŸ¥è¯†è½¬ç§»çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13079v1">PDF</a> To appear in Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„è¯­è¨€çŸ¥è¯†è½¬ç§»åˆ°å£°å­¦ç‰¹å¾å­¦ä¹ ä»¥æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰æ€§èƒ½çš„æ–¹æ³•ã€‚é’ˆå¯¹è¯­è¨€ä¸å£°å­¦æ¨¡æ€é—´å­˜åœ¨çš„è¡¨ç¤ºå¯¹é½é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å›¾åŒ¹é…æœ€ä¼˜ä¼ è¾“ï¼ˆGM-OTï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†è¯­è¨€ä¸å£°å­¦åºåˆ—å»ºæ¨¡ä¸ºç»“æ„å›¾ï¼Œé€šè¿‡æœ€å°åŒ–èŠ‚ç‚¹é—´çš„Wassersteinè·ç¦»ï¼ˆWDï¼‰å’Œè¾¹ç¼˜é—´çš„Gromov-Wassersteinè·ç¦»ï¼ˆGWDï¼‰ï¼Œå®ç°ç»“æ„åŒ–å¯¹é½å’Œæ›´é«˜æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGM-OTåœ¨æ±‰è¯­è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„è¯­è¨€çŸ¥è¯†è½¬ç§»å¯¹æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰æ€§èƒ½æœ‰æ•ˆã€‚</li>
<li>å¯¹é½è¯­è¨€ä¸å£°å­¦æ¨¡æ€çš„è¡¨ç¤ºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºä¸¤ç§æ¨¡æ€ä¹‹é—´å­˜åœ¨å›ºæœ‰çš„å·®è·ã€‚</li>
<li>ç°æœ‰åŸºäºæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰çš„æ–¹æ³•å¿½è§†äº†ç»“æ„å…³ç³»ï¼Œå°†ç‰¹å¾å‘é‡è§†ä¸ºæ— åºé›†åˆã€‚</li>
<li>å›¾åŒ¹é…æœ€ä¼˜ä¼ è¾“ï¼ˆGM-OTï¼‰æ–¹æ³•è¢«æå‡ºï¼Œå°†è¯­è¨€ä¸å£°å­¦åºåˆ—å»ºæ¨¡ä¸ºç»“æ„å›¾ï¼Œå®ç°ç»“æ„åŒ–å¯¹é½ã€‚</li>
<li>GM-OTé€šè¿‡æœ€å°åŒ–èŠ‚ç‚¹é—´çš„Wassersteinè·ç¦»ï¼ˆWDï¼‰å’Œè¾¹ç¼˜é—´çš„Gromov-Wassersteinè·ç¦»ï¼ˆGWDï¼‰ï¼Œå®ç°æ›´é«˜æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>ç†è®ºåˆ†ææ˜¾ç¤ºï¼Œå…ˆå‰çš„åŸºäºOTçš„è¯­è¨€çŸ¥è¯†è½¬ç§»æ–¹æ³•å¯ä»¥ä½œä¸ºGM-OTæ¡†æ¶çš„ç‰¹ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd1c3b6226a1c75d3be037dfe9300d1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdaadfa7406fd7aceac493c67435eeb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f428c4e79d0eadb7b848dfd4dd9b06f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e615f8dff05403f45b0e3245401aca09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb550af9fb8d21f686f0ead43fc166ef.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Suicide-Risk-Assessment-Using-Multimodal-Speech-Features-A-Study-on-the-SW1-Challenge-Dataset"><a href="#Suicide-Risk-Assessment-Using-Multimodal-Speech-Features-A-Study-on-the-SW1-Challenge-Dataset" class="headerlink" title="Suicide Risk Assessment Using Multimodal Speech Features: A Study on the   SW1 Challenge Dataset"></a>Suicide Risk Assessment Using Multimodal Speech Features: A Study on the   SW1 Challenge Dataset</h2><p><strong>Authors:Ambre Marie, Ilias Maoudj, Guillaume Dardenne, GwenolÃ© Quellec</strong></p>
<p>The 1st SpeechWellness Challenge conveys the need for speech-based suicide risk assessment in adolescents. This study investigates a multimodal approach for this challenge, integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM. Additionally, handcrafted acoustic features â€“ including MFCCs, spectral contrast, and pitch-related statistics â€“ were incorporated. We explored three fusion strategies: early concatenation, modality-specific processing, and weighted attention with mixup regularization. Results show that weighted attention provided the best generalization, achieving 69% accuracy on the development set, though a performance gap between development and test sets highlights generalization challenges. Our findings, strictly tied to the MINI-KID framework, emphasize the importance of refining embedding representations and fusion mechanisms to enhance classification reliability. </p>
<blockquote>
<p>ç¬¬ä¸€å±ŠSpeechWellnessæŒ‘æˆ˜èµ›å‡¸æ˜¾äº†é’å°‘å¹´è¯­éŸ³è‡ªæ€é£é™©è¯„ä¼°çš„å¿…è¦æ€§ã€‚æœ¬ç ”ç©¶é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œé‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•ï¼Œå°†è‡ªåŠ¨è½¬å½•ä¸WhisperXã€ä¸­æ–‡RoBERTaçš„è¯­è¨€åµŒå…¥å’ŒWavLMçš„éŸ³é¢‘åµŒå…¥ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œè¿˜ç»“åˆäº†æ‰‹å·¥åˆ¶ä½œçš„å£°å­¦ç‰¹å¾ï¼ŒåŒ…æ‹¬MFCCsã€è°±å¯¹æ¯”å’ŒéŸ³é«˜ç›¸å…³ç»Ÿè®¡ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§èåˆç­–ç•¥ï¼šæ—©æœŸè¿æ¥ã€æ¨¡æ€ç‰¹å®šå¤„ç†å’Œå¸¦æ··åˆæ­£åˆ™åŒ–çš„åŠ æƒæ³¨æ„åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒåŠ æƒæ³¨æ„åŠ›æä¾›äº†æœ€ä½³æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¼€å‘é›†ä¸Šè¾¾åˆ°69%çš„å‡†ç¡®ç‡ï¼Œä½†å¼€å‘é›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ€§èƒ½å·®è·çªå‡ºäº†æ³›åŒ–æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å‘ç°ä¸¥æ ¼ä¸MINI-KIDæ¡†æ¶ç›¸å…³ï¼Œå¼ºè°ƒæ”¹è¿›åµŒå…¥è¡¨ç¤ºå’Œèåˆæœºåˆ¶çš„é‡è¦æ€§ï¼Œä»¥æé«˜åˆ†ç±»å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13069v1">PDF</a> Submitted to the SpeechWellness Challenge at Interspeech 2025; 5   pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³å¥åº·æŒ‘æˆ˜å‡¸æ˜¾é’å°‘å¹´è‡ªæ€é£é™©è¯„ä¼°çš„é‡è¦æ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•´åˆè‡ªåŠ¨è½¬å½•æŠ€æœ¯ä¸å¤šæ¨¡æ€æ–¹æ³•ï¼Œè¿ç”¨WhisperXå®ç°è¯­éŸ³è¯†åˆ«å’Œå¤šæ¨¡æ€è¯„ä¼°ã€‚ç ”ç©¶é‡‡ç”¨äº†æ¥è‡ªä¸­æ–‡RoBERTaçš„è¯­è¨€åµŒå…¥ä¸æ¥è‡ªWavLMçš„éŸ³é¢‘åµŒå…¥ï¼Œå¹¶ç»“åˆæ‰‹å·¥åˆ¶ä½œçš„å£°éŸ³ç‰¹å¾å¦‚MFCCsã€è°±å¯¹æ¯”ä»¥åŠéŸ³é«˜ç»Ÿè®¡ã€‚å®éªŒæ¢è®¨äº†æ—©æœŸæ‹¼æ¥ã€æ¨¡æ€ç‰¹å®šå¤„ç†å’ŒåŠ æƒæ³¨æ„åŠ›ä¸mixupæ­£åˆ™åŒ–ä¸‰ç§èåˆç­–ç•¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŠ æƒæ³¨æ„åŠ›ç­–ç•¥åœ¨å¼€å‘é›†ä¸Šå–å¾—äº†æœ€ä½³æ³›åŒ–æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†69%ï¼Œä½†åœ¨å¼€å‘é›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ€§èƒ½å·®è·ä¹Ÿåæ˜ äº†æ³›åŒ–çš„æŒ‘æˆ˜ã€‚åŸºäºMINI-KIDæ¡†æ¶çš„ç»“æœå¼ºè°ƒäº†ç²¾ç‚¼åµŒå…¥è¡¨è¾¾å’Œèåˆæœºåˆ¶å¯¹æå‡åˆ†ç±»å¯é æ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç¬¬ä¸€é¡¹â€œè¯­éŸ³å¥åº·æŒ‘æˆ˜â€ä¼ è¾¾äº†è¯„ä¼°é’å°‘å¹´è‡ªæ€é£é™©çš„éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€æ–¹æ³•åº”å¯¹æŒ‘æˆ˜ï¼Œç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸å¤šç§ç‰¹å¾å¤„ç†æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨WhisperXè¿›è¡Œè¯­éŸ³è¯†åˆ«å’Œä¸­æ–‡RoBERTaæä¾›çš„è¯­è¨€åµŒå…¥ä»¥åŠWavLMæä¾›çš„éŸ³é¢‘åµŒå…¥æŠ€æœ¯ã€‚</li>
<li>æ‰‹å·¥åˆ¶ä½œçš„å£°å­¦ç‰¹å¾å¦‚MFCCsã€è°±å¯¹æ¯”å’ŒéŸ³é«˜ç»Ÿè®¡è¢«çº³å…¥ç ”ç©¶ä¸­ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ä¸‰ç§èåˆç­–ç•¥ï¼Œå…¶ä¸­åŠ æƒæ³¨æ„åŠ›ç­–ç•¥è¡¨ç°æœ€ä½³ï¼Œå¼€å‘é›†ä¸Šå‡†ç¡®ç‡è¾¾åˆ°äº†69%ã€‚</li>
<li>å­˜åœ¨æ³›åŒ–æŒ‘æˆ˜ï¼Œè¡¨ç°åœ¨å¼€å‘é›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ€§èƒ½å·®è·ä¸Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0652b94bf16e6b8ff8e17798d91ad259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a931e7f12ef3ff5d7afec8f3dab1c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64d66c15d99c5d01db5d4d5f5bbec12c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DualCodec-A-Low-Frame-Rate-Semantically-Enhanced-Neural-Audio-Codec-for-Speech-Generation"><a href="#DualCodec-A-Low-Frame-Rate-Semantically-Enhanced-Neural-Audio-Codec-for-Speech-Generation" class="headerlink" title="DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec   for Speech Generation"></a>DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec   for Speech Generation</h2><p><strong>Authors:Jiaqi Li, Xiaolong Lin, Zhekai Li, Shixi Huang, Yuancheng Wang, Chaoren Wang, Zhenpeng Zhan, Zhizheng Wu</strong></p>
<p>Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos and codes are available at: <a target="_blank" rel="noopener" href="https://dualcodec.github.io/">https://dualcodec.github.io</a> </p>
<blockquote>
<p>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨æ˜¯è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸ºåŸºç¡€è¯­éŸ³ç”Ÿæˆçš„åŸºçŸ³æ„å»ºå—ã€‚é€šå¸¸ï¼Œå¸§ç‡å’ŒéŸ³é¢‘è´¨é‡ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ä½å¸§ç‡ã€è¯­ä¹‰å¢å¼ºçš„ç¼–è§£ç å™¨æ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•å°†è¯­ä¹‰ä¸°å¯Œçš„è‡ªç›‘ç£ï¼ˆSSLï¼‰è¡¨ç¤ºå½¢å¼è’¸é¦åˆ°ç¬¬ä¸€å±‚ç¼–è§£ç å™¨ä»¤ç‰Œä¸­ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†DualCodecï¼Œä¸€ç§åŒæµç¼–ç æ–¹æ³•ï¼Œå®ƒåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¼–è§£ç å™¨æ¡†æ¶å†…é›†æˆäº†SSLå’Œæ³¢å½¢è¡¨ç¤ºå½¢å¼ã€‚åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼ŒDualCodecå¢å¼ºäº†ç¬¬ä¸€å±‚ç¼–è§£ç å™¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿ç¼–è§£ç å™¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä½å¸§ç‡ä¸‹ä¿æŒé«˜éŸ³é¢‘è´¨é‡ã€‚è¯·æ³¨æ„ï¼Œä½å¸§ç‡çš„ç¼–è§£ç å™¨æé«˜äº†è¯­éŸ³ç”Ÿæˆçš„æ•ˆç‡ã€‚åœ¨éŸ³é¢‘ç¼–è§£ç å™¨å’Œè¯­éŸ³ç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¯å®äº†ä¸æœ€æ–°ç¼–è§£ç å™¨ç³»ç»Ÿï¼ˆå¦‚Mimi Codecã€SpeechTokenizerã€DACå’ŒEncodecï¼‰ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„DualCodecçš„æœ‰æ•ˆæ€§ã€‚æ¼”ç¤ºå’Œä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://dualcodec.github.io/">https://dualcodec.github.io</a> ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13000v1">PDF</a> Accepted to Interspeech 2025. Github:   <a target="_blank" rel="noopener" href="https://github.com/jiaqili3/dualcodec">https://github.com/jiaqili3/dualcodec</a></p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç è§£ç å™¨ä¸ºåŸºäºè¯­è¨€æ¨¡å‹çš„è¯­éŸ³ç”Ÿæˆæä¾›äº†åŸºç¡€æ„å»ºå—ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä½å¸§ç‡ã€è¯­ä¹‰å¢å¼ºçš„ç¼–ç è§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è¯­ä¹‰ä¸°å¯Œçš„è‡ªç›‘ç£è¡¨ç¤ºèå…¥ç¬¬ä¸€å±‚ç¼–ç è§£ç å™¨æ ‡è®°ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰çš„ä¸€æµç¼–ç è§£ç å™¨ç³»ç»Ÿï¼Œå¦‚Mimi Codecã€SpeechTokenizerã€DACå’ŒEncodecç­‰ï¼Œæœ¬æ–‡æå‡ºçš„DualCodecåœ¨ä½å¸§ç‡ä¸‹èƒ½ç»´æŒè¾ƒé«˜çš„éŸ³é¢‘è´¨é‡å¹¶æå‡è¯­éŸ³ç”Ÿæˆçš„æ•ˆç‡ã€‚æ›´å¤šæ¼”ç¤ºå’Œä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://dualcodec.github.io/">https://dualcodec.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç è§£ç å™¨ä¸ºè¯­éŸ³ç”Ÿæˆæä¾›äº†åŸºç¡€æ¡†æ¶ã€‚</li>
<li>å½“å‰æŠ€æœ¯ä¸­å¸§ç‡å’ŒéŸ³é¢‘è´¨é‡å­˜åœ¨æƒè¡¡é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†DualCodecæ¨¡å‹ï¼Œé›†æˆäº†è‡ªç›‘ç£è¡¨ç¤ºå’Œæ³¢å½¢è¡¨ç¤ºã€‚</li>
<li>DualCodecåœ¨ä½å¸§ç‡ä¸‹å¢å¼ºç¬¬ä¸€å±‚ç¼–ç è§£ç å™¨çš„è¯­ä¹‰ä¿¡æ¯å¹¶ç»´æŒè¾ƒé«˜éŸ³é¢‘è´¨é‡ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›ç¼–ç è§£ç å™¨ç³»ç»Ÿç›¸æ¯”ï¼ŒDualCodecèƒ½æœ‰æ•ˆæå‡è¯­éŸ³ç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
<li>DualCodecæ¼”ç¤ºå’Œä»£ç å¯åœ¨çº¿è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4bed3ffaaa0c2125cc3cf955c0aa96c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8a7ac705f99fe47ed1df872cd751cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7375e7181842c0664144e1d4988c43c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b1b23d66190915d82bb044ec46c21ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ceabbcf5eab70194cc86a7168c0ad3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78522799940b56dbfad3f60c215e3abb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-467613e1101d79a7cedf5d2b8c535128.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Calm-Whisper-Reduce-Whisper-Hallucination-On-Non-Speech-By-Calming-Crazy-Heads-Down"><a href="#Calm-Whisper-Reduce-Whisper-Hallucination-On-Non-Speech-By-Calming-Crazy-Heads-Down" class="headerlink" title="Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming   Crazy Heads Down"></a>Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming   Crazy Heads Down</h2><p><strong>Authors:Yingzhi Wang, Anas Alhmoud, Saad Alsahly, Muhammad Alqurishi, Mirco Ravanelli</strong></p>
<p>OpenAIâ€™s Whisper has achieved significant success in Automatic Speech Recognition. However, it has consistently been found to exhibit hallucination issues, particularly in non-speech segments, which limits its broader application in complex industrial settings.   In this paper, we introduce a novel method to reduce Whisperâ€™s hallucination on non-speech segments without using any pre- or post-possessing techniques. Specifically, we benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder to the hallucination problem by performing a head-wise mask. Our findings reveal that only 3 of the 20 heads account for over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune these three crazy heads using a collection of non-speech data. The results show that our best fine-tuned model, namely Calm-Whisper, achieves over 80% reduction in non-speech hallucination with only less than 0.1% WER degradation on LibriSpeech test-clean and test-other. </p>
<blockquote>
<p>OpenAIçš„Whisperåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œäººä»¬å‘ç°å®ƒå­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éè¯­éŸ³ç‰‡æ®µä¸­ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸ä½¿ç”¨ä»»ä½•é¢„å¤„ç†æˆ–åå¤„ç†æŠ€æœ¯çš„æƒ…å†›ä¸‹å‡å°‘Whisperåœ¨éè¯­éŸ³ç‰‡æ®µä¸Šçš„å¹»è§‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ‰§è¡Œå¤´å‘æ©ç ï¼Œå¯¹Whisper-large-v3è§£ç å™¨ä¸­æ¯ä¸ªè‡ªæ³¨æ„åŠ›å¤´çš„å¹»è§‰é—®é¢˜è´¡çŒ®è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä»…æœ‰3ä¸ªå¤´åœ¨UrbanSoundæ•°æ®é›†ä¸Šçš„å¹»è§‰è´¡çŒ®è¶…è¿‡75%ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨éè¯­éŸ³æ•°æ®çš„é›†åˆå¯¹è¿™ä¸‰ä¸ªç–¯ç‹‚çš„å¤´éƒ¨è¿›è¡Œå¾®è°ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æœ€ä½³è°ƒæ•´åçš„æ¨¡å‹â€”â€”Calm-Whisperï¼Œåœ¨éè¯­éŸ³å¹»è§‰æ–¹é¢å®ç°äº†è¶…è¿‡80%çš„å‡å°‘ï¼ŒåŒæ—¶åœ¨LibriSpeechæµ‹è¯•æ¸…æ´å’Œæµ‹è¯•å…¶ä»–æ–¹é¢çš„è¯è¯­é”™è¯¯ç‡é™ä½ä¸åˆ°0.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12969v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>OpenAIçš„Whisperåœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨éè¯­éŸ³ç‰‡æ®µä¸­å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•é¢„å¤„ç†æˆ–åå¤„ç†æŠ€å·§ï¼Œå³å¯å‡å°‘Whisperåœ¨éè¯­éŸ³ç‰‡æ®µä¸Šçš„å¹»è§‰ã€‚é€šè¿‡å¯¹é¢å¤´è¿›è¡Œé€ä¸€åˆ†æï¼Œæˆ‘ä»¬å‘ç°åœ¨UrbanSoundæ•°æ®é›†ä¸Šï¼Œä»…æœ‰ä¸‰ä¸ªå¤´éƒ¨å¯¼è‡´è¶…è¿‡75%çš„å¹»è§‰ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´è¿™ä¸‰ä¸ªå¼‚å¸¸å¤´ï¼Œå¹¶åˆ©ç”¨éè¯­éŸ³æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°çš„æœ€ä½³æ¨¡å‹Calm-Whisperåœ¨éè¯­éŸ³å¹»è§‰ä¸Šå‡å°‘äº†è¶…è¿‡80%ï¼ŒåŒæ—¶LibriSpeechæµ‹è¯•é›†çš„è¯é”™è¯¯ç‡ä»…å¢åŠ ä¸åˆ°0.1%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OpenAIçš„Whisperåœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨éè¯­éŸ³ç‰‡æ®µä¸­å­˜åœ¨å¹»è§‰é—®é¢˜ã€‚</li>
<li>åœ¨å¤„ç†éè¯­éŸ³ç‰‡æ®µæ—¶ï¼Œä»…å°‘æ•°å¤´éƒ¨è´¡çŒ®äº†å¤§éƒ¨åˆ†çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´å¯¼è‡´å¹»è§‰çš„ä¸»è¦å¤´éƒ¨ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å¹»è§‰ç°è±¡ã€‚</li>
<li>ä½¿ç”¨éè¯­éŸ³æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶å‡å°‘éè¯­éŸ³å¹»è§‰ã€‚</li>
<li>Calm-Whisperæ¨¡å‹åœ¨éè¯­éŸ³å¹»è§‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡å°‘äº†è¶…è¿‡80%çš„å¹»è§‰ã€‚</li>
<li>å°½ç®¡åœ¨éè¯­éŸ³ç‰‡æ®µä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†Calm-Whisperåœ¨LibriSpeechæµ‹è¯•é›†çš„è¯é”™è¯¯ç‡å¢åŠ å¾ˆå°‘ï¼ˆä¸åˆ°0.1%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eae96fcd43726256a47126c493505e15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b413d9bbee5f298abe2de78fd690d5b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd18d587b0fc7c7a2eecfbab1d8474c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f458170d336ba78363cd5fa61b6eb963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff21168cb58cfd6d1754fee71c91703a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c61db5dafb1b6c1d0bde540a64650c40.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OZSpeech-One-step-Zero-shot-Speech-Synthesis-with-Learned-Prior-Conditioned-Flow-Matching"><a href="#OZSpeech-One-step-Zero-shot-Speech-Synthesis-with-Learned-Prior-Conditioned-Flow-Matching" class="headerlink" title="OZSpeech: One-step Zero-shot Speech Synthesis with   Learned-Prior-Conditioned Flow Matching"></a>OZSpeech: One-step Zero-shot Speech Synthesis with   Learned-Prior-Conditioned Flow Matching</h2><p><strong>Authors:Hieu-Nghia Huynh-Nguyen, Ngoc Son Nguyen, Huynh Nguyen Dang, Thieu Vo, Truong-Son Hy, Van Nguyen</strong></p>
<p>Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and neural network architectures. Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework. However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training. To address these challenges, we introduce OZSpeech, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps. Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS systemâ€™s ability to precisely clone the prompt speech. Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation. Audio samples are available at our demo page <a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/">https://ozspeech.github.io/OZSpeech_Web/</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿåœ¨è¿‘å¹´æ¥å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè¿™ä¸€è¿›å±•å¾—ç›Šäºæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œæ¶æ„çš„æ”¹è¿›ã€‚å°†è¾“å‡ºè¯­éŸ³è§†ä¸ºæ•°æ®åˆ†å¸ƒï¼Œä¹‹å‰çš„æ–¹æ³•ç»å¸¸åœ¨æµåŒ¹é…æ¡†æ¶ä¸­ä½¿ç”¨ä¼ ç»Ÿçš„è¯­éŸ³è¡¨ç¤ºæ–¹æ³•ï¼Œä¾‹å¦‚æ³¢å½¢æˆ–é¢‘è°±å›¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å…·æœ‰å±€é™æ€§ï¼ŒåŒ…æ‹¬å¿½ç•¥äº†å„ç§è¯­éŸ³å±æ€§ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥çš„é¢å¤–çº¦æŸå¯¼è‡´çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OZSpeechï¼Œè¿™æ˜¯ç¬¬ä¸€ç§æ¢ç´¢æœ€ä¼˜ä¼ è¾“æ¡ä»¶æµåŒ¹é…çš„TTSæ–¹æ³•ï¼Œé‡‡ç”¨ä¸€æ­¥é‡‡æ ·å’Œå­¦åˆ°çš„å…ˆéªŒä½œä¸ºæ¡ä»¶ï¼Œæœ‰æ•ˆåœ°å¿½ç•¥äº†å…ˆå‰çš„çŠ¶æ€å¹¶å‡å°‘äº†é‡‡æ ·æ­¥éª¤çš„æ•°é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿è¡Œåœ¨ä»¤ç‰Œæ ¼å¼çš„è¯­éŸ³è§£çº ç¼ ã€åˆ†è§£çš„ç»„ä»¶ä¸Šï¼Œèƒ½å¤Ÿå¯¹æ¯ä¸ªè¯­éŸ³å±æ€§è¿›è¡Œç²¾ç¡®å»ºæ¨¡ï¼Œè¿™å¢å¼ºäº†TTSç³»ç»Ÿç²¾ç¡®å…‹éš†æç¤ºè¯­éŸ³çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€è¯­è°ƒç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒæ–¹é¢å‡å®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æœ‰å‰é€”çš„æ€§èƒ½ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢<a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ozspeech.github.io/OZSpeech_Web/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12800v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¾—ç›Šäºæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œæ¶æ„çš„æ”¹è¿›ã€‚æœ¬æ–‡æå‡ºOZSpeechæ–¹æ³•ï¼Œé¦–æ¬¡åœ¨TTSé¢†åŸŸä¸­æ¢ç´¢æœ€ä¼˜ä¼ è¾“æ¡ä»¶æµåŒ¹é…ï¼Œé‡‡ç”¨ä¸€æ­¥é‡‡æ ·å’Œå­¦ä¹ çš„å…ˆéªŒæ¡ä»¶ï¼Œæœ‰æ•ˆå¿½ç•¥å…ˆå‰çŠ¶æ€ï¼Œå‡å°‘é‡‡æ ·æ­¥éª¤æ•°é‡ã€‚è¯¥æ–¹æ³•åœ¨è¯­éŸ³çš„åˆ†è§£ç»„ä»¶ä¸Šè¿›è¡Œæ“ä½œï¼Œèƒ½å‡†ç¡®å»ºæ¨¡å„ç§è¯­éŸ³ç‰¹å¾ï¼Œæé«˜äº†TTSç³»ç»Ÿåœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€è¯­è°ƒç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒæ–¹é¢çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TTSç³»ç»Ÿè¿‘å¹´æ¥æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œå¾—ç›Šäºæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„å‘å±•ã€‚</li>
<li>ä¹‹å‰çš„TTSæ–¹æ³•å¸¸ä½¿ç”¨ä¼ ç»Ÿè¯­éŸ³è¡¨ç¤ºï¼Œå¦‚æ³¢å½¢æˆ–é¢‘è°±å›¾ï¼Œåœ¨æµåŒ¹é…æ¡†æ¶å†…å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>OZSpeechæ˜¯é¦–ä¸ªåœ¨TTSä¸­æ¢ç´¢æœ€ä¼˜ä¼ è¾“æ¡ä»¶æµåŒ¹é…çš„æ–¹æ³•ã€‚</li>
<li>OZSpeeché‡‡ç”¨ä¸€æ­¥é‡‡æ ·å’Œå­¦ä¹ çš„å…ˆéªŒæ¡ä»¶ï¼Œå¿½ç•¥å…ˆå‰çŠ¶æ€ï¼Œå‡å°‘é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>OZSpeechæ–¹æ³•åœ¨è¯­éŸ³åˆ†è§£ç»„ä»¶ä¸Šæ“ä½œï¼Œèƒ½å‡†ç¡®å»ºæ¨¡å„ç§è¯­éŸ³ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOZSpeechåœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€è¯­è°ƒç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒç­‰æ–¹é¢æ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/">https://ozspeech.github.io/OZSpeech_Web/</a>ä½“éªŒéŸ³é¢‘æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c243f932226d92c40468490fa20dd6de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7b9151fe44927a89689f6c4b1c4048a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea6aa40f2e6467556f52b55cf84de6c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RoVo-Robust-Voice-Protection-Against-Unauthorized-Speech-Synthesis-with-Embedding-Level-Perturbations"><a href="#RoVo-Robust-Voice-Protection-Against-Unauthorized-Speech-Synthesis-with-Embedding-Level-Perturbations" class="headerlink" title="RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations"></a>RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations</h2><p><strong>Authors:Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, Daeseon Choi</strong></p>
<p>With the advancement of AI-based speech synthesis technologies such as Deep Voice, there is an increasing risk of voice spoofing attacks, including voice phishing and fake news, through unauthorized use of othersâ€™ voices. Existing defenses that inject adversarial perturbations directly into audio signals have limited effectiveness, as these perturbations can easily be neutralized by speech enhancement methods. To overcome this limitation, we propose RoVo (Robust Voice), a novel proactive defense technique that injects adversarial perturbations into high-dimensional embedding vectors of audio signals, reconstructing them into protected speech. This approach effectively defends against speech synthesis attacks and also provides strong resistance to speech enhancement models, which represent a secondary attack threat.   In extensive experiments, RoVo increased the Defense Success Rate (DSR) by over 70% compared to unprotected speech, across four state-of-the-art speech synthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial speaker-verification API, effectively neutralizing speech synthesis attack. Moreover, RoVoâ€™s perturbations remained robust even under strong speech enhancement conditions, outperforming traditional methods. A user study confirmed that RoVo preserves both naturalness and usability of protected speech, highlighting its effectiveness in complex and evolving threat scenarios. </p>
<blockquote>
<p>éšç€åŸºäºæ·±åº¦å£°éŸ³ç­‰AIè¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œé€šè¿‡æœªç»æˆæƒçš„ä»–äººå£°éŸ³è¿›è¡Œè¯­éŸ³æ¬ºéª—æ”»å‡»ï¼ˆåŒ…æ‹¬è¯­éŸ³é’“é±¼å’Œè™šå‡æ–°é—»ï¼‰çš„é£é™©æ—¥ç›Šå¢åŠ ã€‚ç›´æ¥åœ¨éŸ³é¢‘ä¿¡å·ä¸­æ³¨å…¥å¯¹æŠ—æ€§å¹²æ‰°çš„ç°æœ‰é˜²å¾¡æªæ–½æ•ˆæœæœ‰é™ï¼Œå› ä¸ºè¿™äº›å¹²æ‰°å¾ˆå®¹æ˜“è¢«è¯­éŸ³å¢å¼ºæ–¹æ³•æ‰€ä¸­å’Œã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RoVoï¼ˆRobust Voiceï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸»åŠ¨é˜²å¾¡æŠ€æœ¯ï¼Œå°†å¯¹æŠ—æ€§å¹²æ‰°æ³¨å…¥éŸ³é¢‘ä¿¡å·çš„é«˜ç»´åµŒå…¥å‘é‡ä¸­ï¼Œç„¶åé‡å»ºä¸ºå—ä¿æŠ¤çš„è¯­éŸ³ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé˜²å¾¡è¯­éŸ³åˆæˆæ”»å‡»ï¼Œå¹¶å¯¹ä»£è¡¨æ¬¡è¦æ”»å‡»å¨èƒçš„è¯­éŸ³å¢å¼ºæ¨¡å‹æä¾›äº†å¼ºå¤§çš„æŠµæŠ—åŠ›ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œä¸æœªå—ä¿æŠ¤çš„è¯­éŸ³ç›¸æ¯”ï¼ŒRoVoå°†é˜²å¾¡æˆåŠŸç‡ï¼ˆDSRï¼‰æé«˜äº†70%ä»¥ä¸Šï¼Œæ¶µç›–äº†å››ç§æœ€å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å•†ç”¨è¯­éŸ³è¯†åˆ«APIä¸Šï¼ŒRoVoçš„DSRè¾¾åˆ°äº†99.5%ï¼Œæœ‰æ•ˆåœ°ä¸­å’Œäº†è¯­éŸ³åˆæˆæ”»å‡»ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å¼ºçƒˆçš„è¯­éŸ³å¢å¼ºæ¡ä»¶ä¸‹ï¼ŒRoVoçš„å¹²æ‰°ä»ç„¶ç¨³å¥ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¯å®ï¼ŒRoVoä¿ç•™äº†å—ä¿æŠ¤è¯­éŸ³çš„è‡ªç„¶æ€§å’Œå¯ç”¨æ€§ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚å’Œä¸æ–­å‘å±•çš„å¨èƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºAIçš„è¯­éŸ³åˆæˆæŠ€æœ¯å¦‚Deep Voiceçš„å¿«é€Ÿå‘å±•å¸¦æ¥çš„è¯­éŸ³æ¬ºéª—æ”»å‡»é£é™©ï¼ŒåŒ…æ‹¬è¯­éŸ³é’“é±¼å’Œå‡æ–°é—»ã€‚ç°æœ‰çš„ç›´æ¥åœ¨éŸ³é¢‘ä¿¡å·ä¸­æ³¨å…¥å¯¹æŠ—æ€§æ‰°åŠ¨çš„é˜²å¾¡æ–¹æ³•æ•ˆæœæœ‰é™ï¼Œå®¹æ˜“è¢«è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸­å’Œã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºRoVoï¼ˆRobust Voiceï¼‰è¿™ä¸€æ–°å‹ä¸»åŠ¨é˜²å¾¡æŠ€æœ¯ï¼Œå°†å¯¹æŠ—æ€§æ‰°åŠ¨æ³¨å…¥éŸ³é¢‘ä¿¡å·çš„é«˜ç»´åµŒå…¥å‘é‡ä¸­ï¼Œé‡æ„ä¸ºä¿æŠ¤è¯­éŸ³ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆæŠµå¾¡è¯­éŸ³åˆæˆæ”»å‡»ï¼Œå¹¶å¯¹è¯­éŸ³å¢å¼ºæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æŠµæŠ—åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRoVoç›¸è¾ƒäºæœªä¿æŠ¤çš„è¯­éŸ³ï¼Œåœ¨å››ç§å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹ä¸Šçš„é˜²å¾¡æˆåŠŸç‡ï¼ˆDSRï¼‰æé«˜äº†70%ä»¥ä¸Šã€‚ç‰¹åˆ«æ˜¯åœ¨å•†ä¸šè¯­éŸ³è¯†åˆ«APIä¸Šï¼ŒRoVoçš„DSRè¾¾åˆ°99.5%ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRoVoæ—¢ä¿æŒäº†ä¿æŠ¤è¯­éŸ³çš„è‡ªç„¶æ€§ï¼Œåˆä¿è¯äº†å¯ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIè¯­éŸ³åˆæˆæŠ€æœ¯å¦‚Deep Voiceçš„è¿›æ­¥å¢åŠ äº†è¯­éŸ³æ¬ºéª—æ”»å‡»çš„é£é™©ã€‚</li>
<li>ç›®å‰ç›´æ¥çš„éŸ³é¢‘ä¿¡å·å¯¹æŠ—æ€§æ‰°åŠ¨é˜²å¾¡æ–¹æ³•å®¹æ˜“è¢«è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸­å’Œã€‚</li>
<li>RoVoæ˜¯ä¸€ç§æ–°å‹ä¸»åŠ¨é˜²å¾¡æŠ€æœ¯ï¼Œå°†æ‰°åŠ¨æ³¨å…¥éŸ³é¢‘ä¿¡å·çš„é«˜ç»´åµŒå…¥å‘é‡ä¸­ã€‚</li>
<li>RoVoæœ‰æ•ˆæŠµå¾¡è¯­éŸ³åˆæˆæ”»å‡»å’Œè¯­éŸ³å¢å¼ºæ¨¡å‹çš„å¨èƒã€‚</li>
<li>RoVoåœ¨å¤šç§å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹ä¸Šçš„é˜²å¾¡æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>å•†ä¸šè¯­éŸ³è¯†åˆ«APIä¸Šï¼ŒRoVoçš„é˜²å¾¡æˆåŠŸç‡è¾¾åˆ°99.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa34982b0b1a7c43396f3023c6c2763f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319386b67e3804d8bf39be30815b9ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3920983b2afe1e96e515cd26c6a28565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab7d27b07152d54b88a6eb423ba039d0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis"><a href="#Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis" class="headerlink" title="Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis"></a>Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis</h2><p><strong>Authors:Dong Yang, Yiyi Cai, Yuki Saito, Lixu Wang, Hiroshi Saruwatari</strong></p>
<p>We propose a shallow flow matching (SFM) mechanism to enhance flow matching (FM)-based text-to-speech (TTS) models within a coarse-to-fine generation paradigm. SFM constructs intermediate states along the FM paths using coarse output representations. During training, we introduce an orthogonal projection method to adaptively determine the temporal position of these states, and apply a principled construction strategy based on a single-segment piecewise flow. The SFM inference starts from the intermediate state rather than pure noise and focuses computation on the latter stages of the FM paths. We integrate SFM into multiple TTS models with a lightweight SFM head. Experiments show that SFM consistently improves the naturalness of synthesized speech in both objective and subjective evaluations, while significantly reducing inference when using adaptive-step ODE solvers. Demo and codes are available at <a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/">https://ydqmkkx.github.io/SFMDemo/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æµ…æµåŒ¹é…ï¼ˆSFMï¼‰æœºåˆ¶ï¼Œä»¥åœ¨ç²—åˆ°ç»†ç”ŸæˆèŒƒå¼å†…å¢å¼ºåŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚SFMåˆ©ç”¨ç²—è¾“å‡ºè¡¨ç¤ºæ„å»ºFMè·¯å¾„ä¸­çš„ä¸­é—´çŠ¶æ€ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ­£äº¤æŠ•å½±æ–¹æ³•æ¥è‡ªé€‚åº”åœ°ç¡®å®šè¿™äº›çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶åŸºäºå•æ®µåˆ†æ®µæµåº”ç”¨äº†ä¸€ç§æœ‰åŸåˆ™çš„æ„å»ºç­–ç•¥ã€‚SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œè€Œä¸æ˜¯ä»çº¯å™ªå£°å¼€å§‹ï¼Œå¹¶å°†è®¡ç®—é‡ç‚¹æ”¾åœ¨FMè·¯å¾„çš„åæœŸé˜¶æ®µã€‚æˆ‘ä»¬å°†SFMé›†æˆåˆ°å¤šä¸ªTTSæ¨¡å‹ä¸­ï¼Œä½¿ç”¨è½»é‡çº§çš„SFMå¤´ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å®¢è§‚è¿˜æ˜¯ä¸»è§‚è¯„ä¼°ä¸­ï¼ŒSFMéƒ½èƒ½æŒç»­æé«˜åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼ŒåŒæ—¶åœ¨é‡‡ç”¨è‡ªé€‚åº”æ­¥é•¿ODEæ±‚è§£å™¨æ—¶ï¼Œèƒ½æ˜¾è‘—é™ä½æ¨ç†æ—¶é—´ã€‚æ¼”ç¤ºå’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E8%8E%B7%E5%8F%96%E3%80%82">https://ydqmkkx.github.io/SFMDemo/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ…æµåŒ¹é…ï¼ˆSFMï¼‰æœºåˆ¶ï¼Œç”¨äºåœ¨ç²—åˆ°ç»†ç”ŸæˆèŒƒå¼ä¸­å¢å¼ºåŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚SFMé€šè¿‡åœ¨FMè·¯å¾„ä¸Šæ„å»ºä¸­é—´çŠ¶æ€ï¼Œåˆ©ç”¨ç²—è¾“å‡ºè¡¨ç¤ºã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥æ­£äº¤æŠ•å½±æ–¹æ³•è‡ªé€‚åº”ç¡®å®šè¿™äº›çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶åŸºäºå•æ®µåˆ†æ®µæµåº”ç”¨æœ‰åŸåˆ™çš„æ„å»ºç­–ç•¥ã€‚SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œè€Œéçº¯å™ªå£°ï¼Œå¹¶å°†è®¡ç®—é‡ç‚¹æ”¾åœ¨FMè·¯å¾„çš„åæœŸé˜¶æ®µã€‚å®éªŒè¡¨æ˜ï¼ŒSFMåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡æé«˜äº†åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œåœ¨ä½¿ç”¨è‡ªé€‚åº”æ­¥é•¿ODEæ±‚è§£å™¨æ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æµ…æµåŒ¹é…ï¼ˆSFMï¼‰æœºåˆ¶ï¼Œç”¨äºå¢å¼ºåŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>SFMåœ¨FMè·¯å¾„ä¸Šæ„å»ºä¸­é—´çŠ¶æ€ï¼Œåˆ©ç”¨ç²—è¾“å‡ºè¡¨ç¤ºã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ­£äº¤æŠ•å½±æ–¹æ³•è‡ªé€‚åº”ç¡®å®šä¸­é—´çŠ¶æ€çš„æ—¶é—´ä½ç½®ã€‚</li>
<li>é‡‡ç”¨å•æ®µåˆ†æ®µæµçš„ç­–ç•¥è¿›è¡Œæœ‰åŸåˆ™çš„æ„å»ºã€‚</li>
<li>SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œæé«˜è¯­éŸ³åˆæˆçš„è‡ªç„¶åº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSFMåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡æœ‰æ•ˆï¼Œä¸”èƒ½æ˜¾è‘—å‡å°‘æ¨ç†æ—¶é—´ã€‚</li>
<li>æä¾›äº†æ¼”ç¤ºå’Œä»£ç é“¾æ¥ï¼Œæ–¹ä¾¿ç”¨æˆ·äº†è§£å’Œå®ç°è¯¥æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05b333ababfe421ec07abae91b89b520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93ccf19c0b69d0575db4e284364761f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5e6db09a073f6c66aee05d9747bc3f7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BenSParX-A-Robust-Explainable-Machine-Learning-Framework-for-Parkinsonâ€™s-Disease-Detection-from-Bengali-Conversational-Speech"><a href="#BenSParX-A-Robust-Explainable-Machine-Learning-Framework-for-Parkinsonâ€™s-Disease-Detection-from-Bengali-Conversational-Speech" class="headerlink" title="BenSParX: A Robust Explainable Machine Learning Framework for   Parkinsonâ€™s Disease Detection from Bengali Conversational Speech"></a>BenSParX: A Robust Explainable Machine Learning Framework for   Parkinsonâ€™s Disease Detection from Bengali Conversational Speech</h2><p><strong>Authors:Riad Hossain, Muhammad Ashad Kabir, Arat Ibne Golam Mowla, Animesh Chandra Roy, Ranjit Kumar Ghosh</strong></p>
<p>Parkinsonâ€™s disease (PD) poses a growing global health challenge, with Bangladesh experiencing a notable rise in PD-related mortality. Early detection of PD remains particularly challenging in resource-constrained settings, where voice-based analysis has emerged as a promising non-invasive and cost-effective alternative. However, existing studies predominantly focus on English or other major languages; notably, no voice dataset for PD exists for Bengali - posing a significant barrier to culturally inclusive and accessible healthcare solutions. Moreover, most prior studies employed only a narrow set of acoustic features, with limited or no hyperparameter tuning and feature selection strategies, and little attention to model explainability. This restricts the development of a robust and generalizable machine learning model. To address this gap, we present BenSparX, the first Bengali conversational speech dataset for PD detection, along with a robust and explainable machine learning framework tailored for early diagnosis. The proposed framework incorporates diverse acoustic feature categories, systematic feature selection methods, and state-of-the-art machine learning algorithms with extensive hyperparameter optimization. Furthermore, to enhance interpretability and trust in model predictions, the framework incorporates SHAP (SHapley Additive exPlanations) analysis to quantify the contribution of individual acoustic features toward PD detection. Our framework achieves state-of-the-art performance, yielding an accuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further externally validated our approach by applying the framework to existing PD datasets in other languages, where it consistently outperforms state-of-the-art approaches. To facilitate further research and reproducibility, the dataset has been made publicly available at <a target="_blank" rel="noopener" href="https://github.com/Riad071/BenSParX">https://github.com/Riad071/BenSParX</a>. </p>
<blockquote>
<p>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ˜¯ä¸€ä¸ªæ—¥ç›Šä¸¥é‡çš„å…¨çƒå¥åº·æŒ‘æˆ˜ï¼Œå­ŸåŠ æ‹‰å›½çš„PDç›¸å…³æ­»äº¡ç‡ä¹Ÿå‡ºç°äº†æ˜¾è‘—ä¸Šå‡ã€‚åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ï¼ŒPDçš„æ—©æœŸæ£€æµ‹ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ï¼ŒåŸºäºå£°éŸ³çš„åˆ†æå·²å‡ºç°ä¸ºå‰æ™¯å¹¿é˜”çš„å¾®åˆ›ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æˆ–å…¶ä»–ä¸»è¦è¯­è¨€ä¸Šï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°šæœªå­˜åœ¨ç”¨äºPDæ£€æµ‹çš„å­ŸåŠ æ‹‰è¯­è¯­éŸ³æ•°æ®é›†ï¼Œè¿™å¯¹åŒ…å«æ–‡åŒ–å’Œå¯è®¿é—®æ€§çš„åŒ»ç–—ä¿å¥è§£å†³æ–¹æ¡ˆé€ æˆäº†é‡å¤§éšœç¢ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ—©æœŸç ”ç©¶ä»…ä½¿ç”¨æœ‰é™çš„å£°å­¦ç‰¹å¾ï¼Œæœ‰é™çš„æˆ–æ²¡æœ‰è¶…å‚æ•°è°ƒæ•´å’Œç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œå¹¶ä¸”å¾ˆå°‘å…³æ³¨æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¿™é™åˆ¶äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BenSparXï¼Œè¿™æ˜¯ç”¨äºPDæ£€æµ‹çš„ç¬¬ä¸€ä¸ªå­ŸåŠ æ‹‰è¯­å¯¹è¯è¯­éŸ³æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªé’ˆå¯¹æ—©æœŸè¯Šæ–­é‡èº«å®šåˆ¶çš„ç¨³å¥ä¸”å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚æ‰€æå‡ºçš„æ¡†æ¶ç»“åˆäº†å¤šç§å£°å­¦ç‰¹å¾ç±»åˆ«ã€ç³»ç»Ÿçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ä»¥åŠå…ˆè¿›çš„æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œå¹¿æ³›çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ¨¡å‹é¢„æµ‹çš„è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†SHAPï¼ˆSHapley Additive exPlanationsï¼‰åˆ†æï¼Œä»¥é‡åŒ–å•ä¸ªå£°å­¦ç‰¹å¾å¯¹PDæ£€æµ‹çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå‡†ç¡®ç‡ä¸º95.77%ï¼ŒF1åˆ†æ•°ä¸º95.57%ï¼ŒAUC-ROCä¸º0.982ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å°†è¯¥æ¡†æ¶åº”ç”¨äºå…¶ä»–è¯­è¨€çš„ç°æœ‰PDæ•°æ®é›†æ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å®ƒéƒ½ä¼˜äºä¸šç•Œé¢†å…ˆçš„æ–¹æ³•ã€‚ä¸ºäº†ä¾¿äºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¯é‡å¤æ€§ï¼Œè¯¥æ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Riad071/BenSParX%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Riad071/BenSParXä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12192v1">PDF</a> 46 pages, 16 figures</p>
<p><strong>æ‘˜è¦</strong><br>    å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰å…¨çƒæŒ‘æˆ˜æ—¥ç›Šä¸¥å³»ï¼Œå­ŸåŠ æ‹‰å›½PDç›¸å…³æ­»äº¡ç‡æ˜¾è‘—ä¸Šå‡ã€‚åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ï¼Œæ—©æœŸå‘ç°PDå°¤ä¸ºå›°éš¾ï¼ŒåŸºäºå£°éŸ³çš„åˆ†ææˆä¸ºäº†ä¸€ç§æœ‰å‰æ™¯çš„éä¾µå…¥æ€§å’Œç»æµæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä½†ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æˆ–å…¶ä»–ä¸»è¦è¯­è¨€ä¸Šï¼Œå­ŸåŠ æ‹‰è¯­çš„PDè¯­éŸ³æ•°æ®é›†å°šä¸å­˜åœ¨ï¼Œè¿™æˆä¸ºå®ç°åŒ…å®¹æ€§å’Œå¯è®¿é—®æ€§åŒ»ç–—ä¿å¥è§£å†³æ–¹æ¡ˆçš„é‡å¤§éšœç¢ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶ä»…ä½¿ç”¨æœ‰é™çš„å£°å­¦ç‰¹å¾ï¼Œç¼ºä¹è¶…å‚æ•°è°ƒæ•´å’Œç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œä¸”å¯¹æ¨¡å‹è§£é‡Šæ€§çš„å…³æ³¨å¾ˆå°‘ï¼Œè¿™é™åˆ¶äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BenSparXï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºPDæ£€æµ‹çš„å­ŸåŠ æ‹‰è¯­ä¼šè¯è¯­éŸ³æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªç”¨äºæ—©æœŸè¯Šæ–­çš„ç¨³å¥ä¸”å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶çº³å…¥å„ç±»å£°å­¦ç‰¹å¾ã€ç³»ç»Ÿçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•å’Œå¸¦æœ‰å¹¿æ³›è¶…å‚æ•°ä¼˜åŒ–çš„å…ˆè¿›æœºå™¨å­¦ä¹ ç®—æ³•ã€‚æ­¤å¤–ï¼Œä¸ºæé«˜æ¨¡å‹é¢„æµ‹çš„è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ï¼Œæ¡†æ¶ç»“åˆäº†SHAPï¼ˆSHapley Additive exPlanationsï¼‰åˆ†æï¼Œä»¥é‡åŒ–ä¸ªäººå£°å­¦ç‰¹å¾å¯¹PDæ£€æµ‹çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†å…ˆè¿›çš„è¡¨ç°æ°´å¹³ï¼Œå‡†ç¡®ç‡95.77%ï¼ŒF1åˆ†æ•°95.57%ï¼ŒAUC-ROCä¸º0.982ã€‚é€šè¿‡å°†è¯¥æ¡†æ¶åº”ç”¨äºå…¶ä»–è¯­è¨€çš„ç°æœ‰PDæ•°æ®é›†è¿›è¡Œå¤–éƒ¨éªŒè¯ï¼Œè¯å®å…¶å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä¸ºä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¯é‡å¤æ€§ï¼Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Riad071/BenSParx%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Riad071/BenSParxä¸Šå…¬å¼€æä¾›ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰åœ¨å…¨çƒèŒƒå›´å†…æ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œå­ŸåŠ æ‹‰å›½çš„æ­»äº¡ç‡æ˜¾è‘—ä¸Šå‡ã€‚</li>
<li>åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­æ—©æœŸå‘ç°PDå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå£°éŸ³åˆ†ææˆä¸ºäº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç›®å‰ç¼ºä¹é’ˆå¯¹å­ŸåŠ æ‹‰è¯­çš„PDè¯­éŸ³æ•°æ®é›†ï¼Œé™åˆ¶äº†åŒ…å®¹æ€§å’Œå¯è®¿é—®æ€§åŒ»ç–—ä¿å¥è§£å†³æ–¹æ¡ˆçš„å®ç°ã€‚</li>
<li>æå‡ºçš„BenSparXæ•°æ®é›†å’Œæœºå™¨å­¦ä¹ æ¡†æ¶ç»“åˆäº†å¤šç§å£°å­¦ç‰¹å¾ã€ç³»ç»Ÿç‰¹å¾é€‰æ‹©æ–¹æ³•å’Œå…ˆè¿›çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ¡†æ¶åˆ©ç”¨SHAPåˆ†ææé«˜æ¨¡å‹é¢„æµ‹çš„è§£é‡Šæ€§ã€‚</li>
<li>æ¡†æ¶è¾¾åˆ°äº†å…ˆè¿›çš„è¡¨ç°æ°´å¹³ï¼Œå¹¶åœ¨å¤–éƒ¨éªŒè¯ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7c4fecdec4e47f61080415d247c35b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a2f03924730ccdb52845b31abcb4d2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SepPrune-Structured-Pruning-for-Efficient-Deep-Speech-Separation"><a href="#SepPrune-Structured-Pruning-for-Efficient-Deep-Speech-Separation" class="headerlink" title="SepPrune: Structured Pruning for Efficient Deep Speech Separation"></a>SepPrune: Structured Pruning for Efficient Deep Speech Separation</h2><p><strong>Authors:Yuqi Li, Kai Li, Xin Yin, Zhifei Yang, Junhao Dong, Zeyu Dong, Chuanguang Yang, Yingli Tian, Yao Lu</strong></p>
<p>Although deep learning has substantially advanced speech separation in recent years, most existing studies continue to prioritize separation quality while overlooking computational efficiency, an essential factor for low-latency speech processing in real-time applications. In this paper, we propose SepPrune, the first structured pruning framework specifically designed to compress deep speech separation models and reduce their computational cost. SepPrune begins by analyzing the computational structure of a given model to identify layers with the highest computational burden. It then introduces a differentiable masking strategy to enable gradient-driven channel selection. Based on the learned masks, SepPrune prunes redundant channels and fine-tunes the remaining parameters to recover performance. Extensive experiments demonstrate that this learnable pruning paradigm yields substantial advantages for channel pruning in speech separation models, outperforming existing methods. Notably, a model pruned with SepPrune can recover 85% of the performance of a pre-trained model (trained over hundreds of epochs) with only one epoch of fine-tuning, and achieves convergence 36$\times$ faster than training from scratch. Code is available at <a target="_blank" rel="noopener" href="https://github.com/itsnotacie/SepPrune">https://github.com/itsnotacie/SepPrune</a>. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ åœ¨è¿‘å¹´æ¥å·²ç»æå¤§åœ°æ¨åŠ¨äº†è¯­éŸ³åˆ†ç¦»æŠ€æœ¯çš„å‘å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä»ç„¶ä¼˜å…ˆé‡è§†åˆ†ç¦»è´¨é‡ï¼Œè€Œå¿½è§†äº†è®¡ç®—æ•ˆç‡è¿™ä¸€å®æ—¶åº”ç”¨ä¸­ä½å»¶è¿Ÿè¯­éŸ³å¤„ç†çš„å…³é”®å› ç´ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SepPruneï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºå‹ç¼©æ·±åº¦è¯­éŸ³åˆ†ç¦»æ¨¡å‹è€Œè®¾è®¡çš„ç»“æ„åŒ–å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨é™ä½å…¶è®¡ç®—æˆæœ¬ã€‚SepPruneé¦–å…ˆåˆ†æç»™å®šæ¨¡å‹çš„è®¡ç®—ç»“æ„ï¼Œä»¥è¯†åˆ«è®¡ç®—è´Ÿæ‹…æœ€é«˜çš„å±‚ã€‚ç„¶åï¼Œå®ƒå¼•å…¥äº†ä¸€ç§å¯å¾®åˆ†çš„æ©ç ç­–ç•¥ï¼Œä»¥å®ç°æ¢¯åº¦é©±åŠ¨çš„é€šé“é€‰æ‹©ã€‚åŸºäºå­¦ä¹ çš„æ©ç ï¼ŒSepPruneå‰ªå»å†—ä½™é€šé“å¹¶å¯¹å‰©ä½™å‚æ•°è¿›è¡Œå¾®è°ƒä»¥æ¢å¤æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¿™ç§å¯å­¦ä¹ çš„å‰ªæèŒƒå¼åœ¨è¯­éŸ³åˆ†ç¦»æ¨¡å‹çš„é€šé“å‰ªææ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨SepPruneä¿®å‰ªçš„æ¨¡å‹åªéœ€ä¸€ä¸ªå‘¨æœŸçš„å¾®è°ƒå°±å¯ä»¥æ¢å¤é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç»è¿‡æ•°ç™¾ä¸ªå‘¨æœŸçš„è®­ç»ƒï¼‰çš„85%æ€§èƒ½ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ¯”ä»å¤´å¼€å§‹è®­ç»ƒå¿«36å€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/itsnotacie/SepPrune%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/itsnotacie/SepPruneæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12079v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºSepPruneï¼Œä¸€ä¸ªä¸“ä¸ºæ·±åº¦è¯­éŸ³åˆ†ç¦»æ¨¡å‹è®¾è®¡çš„ç»“æ„åŒ–å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®ƒé€šè¿‡åˆ†ææ¨¡å‹çš„è®¡ç®—ç»“æ„æ¥è¯†åˆ«è®¡ç®—è´Ÿæ‹…æœ€é«˜çš„å±‚ï¼Œå¼•å…¥å¯å¾®æ©ç ç­–ç•¥è¿›è¡Œæ¢¯åº¦é©±åŠ¨çš„é€šé“é€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼ŒSepPruneçš„å‰ªæç­–ç•¥åœ¨è¯­éŸ³åˆ†ç¦»æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¿«é€Ÿæ¢å¤æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æé«˜æ”¶æ•›é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SepPruneæ˜¯é¦–ä¸ªä¸“ä¸ºæ·±åº¦è¯­éŸ³åˆ†ç¦»æ¨¡å‹è®¾è®¡çš„ç»“æ„åŒ–å‰ªææ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨æé«˜è¯­éŸ³åˆ†ç¦»æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>SepPruneé€šè¿‡åˆ†ææ¨¡å‹çš„è®¡ç®—ç»“æ„æ¥è¯†åˆ«è´Ÿæ‹…æœ€é«˜çš„å±‚ã€‚</li>
<li>ä½¿ç”¨å¯å¾®æ©ç ç­–ç•¥è¿›è¡Œæ¢¯åº¦é©±åŠ¨çš„é€šé“é€‰æ‹©ã€‚</li>
<li>SepPruneçš„å‰ªæç­–ç•¥åœ¨è¯­éŸ³åˆ†ç¦»æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¿«é€Ÿæ¢å¤æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç›¸è¾ƒäºé¢„è®­ç»ƒæ¨¡å‹ï¼Œç»è¿‡SepPruneçš„æ¨¡å‹åªéœ€ä¸€ä¸ªepochçš„å¾®è°ƒå³å¯æ¢å¤85%çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5ae0a8554adc45fb8aa8df19cfbe80c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-995878b293c92f1635cce7e7a5b864c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87fe7b6258e531bf70c29a1a81942aeb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhanced-Multimodal-Hate-Video-Detection-via-Channel-wise-and-Modality-wise-Fusion"><a href="#Enhanced-Multimodal-Hate-Video-Detection-via-Channel-wise-and-Modality-wise-Fusion" class="headerlink" title="Enhanced Multimodal Hate Video Detection via Channel-wise and   Modality-wise Fusion"></a>Enhanced Multimodal Hate Video Detection via Channel-wise and   Modality-wise Fusion</h2><p><strong>Authors:Yinghui Zhang, Tailin Chen, Yuchen Zhang, Zeyu Fu</strong></p>
<p>The rapid rise of video content on platforms such as TikTok and YouTube has transformed information dissemination, but it has also facilitated the spread of harmful content, particularly hate videos. Despite significant efforts to combat hate speech, detecting these videos remains challenging due to their often implicit nature. Current detection methods primarily rely on unimodal approaches, which inadequately capture the complementary features across different modalities. While multimodal techniques offer a broader perspective, many fail to effectively integrate temporal dynamics and modality-wise interactions essential for identifying nuanced hate content. In this paper, we present CMFusion, an enhanced multimodal hate video detection model utilizing a novel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts features from text, audio, and video modalities using pre-trained models and then incorporates a temporal cross-attention mechanism to capture dependencies between video and audio streams. The learned features are then processed by channel-wise and modality-wise fusion modules to obtain informative representations of videos. Our extensive experiments on a real-world dataset demonstrate that CMFusion significantly outperforms five widely used baselines in terms of accuracy, precision, recall, and F1 score. Comprehensive ablation studies and parameter analyses further validate our design choices, highlighting the modelâ€™s effectiveness in detecting hate videos. The source codes will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/EvelynZ10/cmfusion">https://github.com/EvelynZ10/cmfusion</a>. </p>
<blockquote>
<p>éšç€TikTokå’ŒYouTubeç­‰å¹³å°è§†é¢‘å†…å®¹çš„è¿…é€Ÿå´›èµ·ï¼Œä¿¡æ¯ä¼ æ’­çš„æ–¹å¼å·²ç»å‘ç”Ÿäº†å˜é©ï¼Œä½†åŒæ—¶ä¹Ÿä¿ƒè¿›äº†æœ‰å®³å†…å®¹çš„æ‰©æ•£ï¼Œç‰¹åˆ«æ˜¯ä»‡æ¨è§†é¢‘ã€‚å°½ç®¡åœ¨æ‰“å‡»ä»‡æ¨è¨€è®ºæ–¹é¢ä»˜å‡ºäº†å·¨å¤§åŠªåŠ›ï¼Œä½†ç”±äºä»‡æ¨è§†é¢‘çš„éšæ™¦æ€§ï¼Œæ£€æµ‹è¿™äº›è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å½“å‰çš„æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºå•æ¨¡æ€æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ— æ³•å……åˆ†æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„äº’è¡¥ç‰¹å¾ã€‚è™½ç„¶å¤šæ¨¡æ€æŠ€æœ¯æä¾›äº†æ›´å¹¿æ³›çš„è§†è§’ï¼Œä½†è®¸å¤šæŠ€æœ¯åœ¨æ•´åˆè¯†åˆ«ä»‡æ¨å†…å®¹æ‰€å¿…éœ€çš„æ—¶é—´åŠ¨æ€å’Œæ¨¡æ€é—´äº¤äº’æ–¹é¢æ•ˆæœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CMFusionï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨æ–°å‹é€šé“çº§å’Œæ¨¡æ€çº§èåˆæœºåˆ¶çš„é«˜çº§å¤šæ¨¡æ€ä»‡æ¨è§†é¢‘æ£€æµ‹æ¨¡å‹ã€‚CMFusioné¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä»æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€ä¸­æå–ç‰¹å¾ï¼Œç„¶åé‡‡ç”¨æ—¶é—´äº¤å‰æ³¨æ„æœºåˆ¶æ¥æ•æ‰è§†é¢‘å’ŒéŸ³é¢‘æµä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œé€šè¿‡é€šé“çº§å’Œæ¨¡æ€çº§èåˆæ¨¡å—å¤„ç†æ‰€å­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œä»¥è·å–è§†é¢‘çš„ä¿¡æ¯åŒ–è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºçº¿ç›¸æ¯”ï¼ŒCMFusionåœ¨å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œå‚æ•°åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©ï¼Œçªæ˜¾äº†è¯¥æ¨¡å‹åœ¨æ£€æµ‹ä»‡æ¨è§†é¢‘æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/EvelynZ10/cmfusion%E4%B8%8A%E5%BC%80%E6%9C%89%E3%80%82">https://github.com/EvelynZ10/cmfusionä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12051v1">PDF</a> ICDMW 2024, Github: <a target="_blank" rel="noopener" href="https://github.com/EvelynZ10/cmfusion">https://github.com/EvelynZ10/cmfusion</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨è§†é¢‘å¹³å°ä¸Šä»‡æ¨è§†é¢‘çš„ä¼ æ’­é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šåª’ä½“ä»‡æ¨è§†é¢‘æ£€æµ‹æ¨¡å‹CMFusionã€‚è¯¥æ¨¡å‹é€šè¿‡èåˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œç»“åˆæ—¶é—´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆæ•æ‰è§†é¢‘å’ŒéŸ³é¢‘æµä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCMFusionåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰æ–¹é¢å‡ä¼˜äºäº”ç§å¹¿æ³›ä½¿ç”¨çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å†…å®¹åœ¨TikTokå’ŒYouTubeç­‰å¹³å°ä¸Šè¿…é€Ÿå´›èµ·ï¼Œæ”¹å˜äº†ä¿¡æ¯ä¼ æ’­æ–¹å¼ï¼ŒåŒæ—¶ä¹Ÿä¿ƒè¿›äº†æœ‰å®³å†…å®¹çš„æ‰©æ•£ï¼Œç‰¹åˆ«æ˜¯ä»‡æ¨è§†é¢‘ã€‚</li>
<li>å½“å‰ä»‡æ¨è§†é¢‘æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºå•æ¨¡æ€æ–¹æ³•ï¼Œæ— æ³•å……åˆ†æ•æ‰ä¸åŒæ¨¡æ€çš„äº’è¡¥ç‰¹å¾ã€‚</li>
<li>å¤šæ¨¡æ€æŠ€æœ¯è™½ç„¶æä¾›äº†æ›´å¹¿æ³›çš„è§†è§’ï¼Œä½†è®¸å¤šæ–¹æ³•æœªèƒ½æœ‰æ•ˆåœ°ç»“åˆæ—¶é—´åŠ¨æ€å’Œæ¨¡æ€é—´äº¤äº’ï¼Œè¿™å¯¹äºè¯†åˆ«å¾®å¦™çš„ä»‡æ¨å†…å®¹è‡³å…³é‡è¦ã€‚</li>
<li>CMFusionæ˜¯ä¸€ç§å¢å¼ºçš„å¤šåª’ä½“ä»‡æ¨è§†é¢‘æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡æ¸ é“å’Œæ¨¡æ€èåˆæœºåˆ¶ï¼Œæå–æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ç‰¹å¾ï¼Œå¹¶æ•æ‰è§†é¢‘å’ŒéŸ³é¢‘æµä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚</li>
<li>CMFusionæ¨¡å‹åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºäº”ç§å¹¿æ³›ä½¿ç”¨çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶å’Œå‚æ•°åˆ†æéªŒè¯äº†CMFusionæ¨¡å‹è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf7608f63e781c7bde95dbfafaca05b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b168cbfa8a26e956215ea0843f11f295.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1f3df9a33144da0913fcb6ad72f4c36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96e3e2f0e2b08d2475b6f0f42b4edfb3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning"><a href="#Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning" class="headerlink" title="Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning"></a>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning</h2><p><strong>Authors:Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty</strong></p>
<p>Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. </p>
<blockquote>
<p>å¯¹æŠ—æ€§è¨€è®ºå·²è¢«è¯æ˜æ˜¯ç½‘ä¸Šæ‰“å‡»ä»‡æ¨è¨€è®ºçš„æœ‰åŠ›å·¥å…·ã€‚ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»…æ ¹æ®ç‰¹å®šæ„å›¾ï¼ˆå•ä¸€å±æ€§ï¼‰ç”Ÿæˆå¯¹æŠ—æ€§è¨€è®ºã€‚ç„¶è€Œï¼Œè€ƒè™‘å¤šä¸ªå±æ€§åŒæ—¶çš„å…¨é¢æ–¹æ³•å¯èƒ½ä¼šäº§ç”Ÿæ›´ç»†è‡´å’Œæœ‰æ•ˆçš„å›åº”ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†HiPPrOï¼Œå³å¸¦æœ‰åå¥½ä¼˜åŒ–çš„å±‚æ¬¡å‰ç¼€å­¦ä¹ ï¼ˆHierarchical Prefix learning with Preference Optimizationï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚å®ƒåˆ©ç”¨ç‰¹å®šå±æ€§å‰ç¼€åµŒå…¥ç©ºé—´åœ¨ç”Ÿæˆå¯¹æŠ—æ€§è¨€è®ºè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§è¿›è¡Œå±‚æ¬¡ä¼˜åŒ–ã€‚ä¹‹åï¼Œæˆ‘ä»¬ç»“åˆäº†å‚è€ƒå’Œæ— å¥–åŠ±åå¥½ä¼˜åŒ–ï¼Œä»¥ç”Ÿæˆæ›´å…·å»ºè®¾æ€§çš„å¯¹æŠ—æ€§è¨€è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰13973ä¸ªå¯¹æŠ—æ€§è¨€è®ºå®ä¾‹è¿›è¡Œäº†æƒ…æ„Ÿæ ‡ç­¾æ ‡æ³¨ï¼Œç”±äº”ä½æ ‡æ³¨è€…å®Œæˆã€‚HiPPrOåˆ©ç”¨å±‚æ¬¡å‰ç¼€ä¼˜åŒ–æœ‰æ•ˆåœ°æ•´åˆäº†è¿™äº›åŒé‡å±æ€§ã€‚ä¸€é¡¹å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHiPPrOåœ¨æ„å›¾ä¸€è‡´æ€§æ–¹é¢å–å¾—äº†çº¦38%çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨Rouge-1ã€Rouge-2å’ŒRouge-Læ–¹é¢åˆ†åˆ«å–å¾—äº†çº¦3%ã€çº¦2%ã€çº¦3%çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºå‡ ä¸ªåŸºå‡†æ¨¡å‹ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œçªå‡ºäº†ç”Ÿæˆå¯¹æŠ—æ€§è¨€è®ºçš„å¢å¼ºå…³è”æ€§å’Œæ°å½“æ€§ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å¤šå±æ€§æ¡ä»¶åœ¨æå‡å¯¹æŠ—æ€§è¨€è®ºç”Ÿæˆç³»ç»Ÿæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11958v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹æŠ—ç½‘ç»œä»‡æ¨è¨€è®ºçš„æœ‰æ•ˆå·¥å…·â€”â€”åè¯­ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨äºåŸºäºç‰¹å®šæ„å›¾çš„åè¯­ç”Ÿæˆï¼Œè€Œæœ¬æ–‡æå‡ºä¸€ç§ç»¼åˆè€ƒè™‘å¤šç§å±æ€§çš„å…¨é¢æ–¹æ³•ï¼Œä»¥äº§ç”Ÿæ›´ç²¾ç»†å’Œæœ‰æ•ˆçš„å›åº”ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†HiPPrOæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å±æ€§ç‰¹å®šçš„å‰ç¼€åµŒå…¥ç©ºé—´è¿›è¡Œåˆ†å±‚ä¼˜åŒ–ï¼Œå¹¶åœ¨åè¯­ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæ”¹è¿›ã€‚å®éªŒè¯æ˜ï¼ŒHiPPrOæ–¹æ³•åœ¨æ„å›¾ç¬¦åˆæ€§å’Œè¯„ä»·æŒ‡æ•°æ–¹é¢ç›¸æ¯”åŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºå¤šå±æ€§æ¡ä»¶åœ¨æå‡åè¯­ç”Ÿæˆç³»ç»Ÿæ•ˆèƒ½æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åè¯­æ˜¯ç½‘ç»œä¸Šå¯¹æŠ—ä»‡æ¨è¨€è®ºçš„æœ‰åŠ›å·¥å…·ã€‚</li>
<li>ä»¥å¾€ç ”ç©¶ä¸»è¦å…³æ³¨åŸºäºå•ä¸€æ„å›¾çš„åè¯­ç”Ÿæˆï¼Œè€Œå…¨é¢è€ƒè™‘å¤šç§å±æ€§çš„æ–¹æ³•èƒ½ç”Ÿæˆæ›´ç²¾ç»†å’Œæœ‰æ•ˆçš„å›åº”ã€‚</li>
<li>å¼•å…¥HiPPrOæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å‰ç¼€å­¦ä¹ å¹¶ç»“åˆåå¥½ä¼˜åŒ–è¿›è¡Œåè¯­ç”Ÿæˆã€‚</li>
<li>HiPPrOåœ¨æ„å›¾ç¬¦åˆæ€§å’Œè¯„ä»·æŒ‡æ•°æ–¹é¢è¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯æ˜äº†HiPPrOæ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨ç”Ÿæˆåè¯­çš„ç›¸å…³æ€§ã€‚</li>
<li>æƒ…æ„Ÿæ ‡ç­¾åœ¨åè¯­ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½œç”¨è¢«é‡è§†ï¼Œå¹¶è¿›è¡Œäº†ç›¸åº”çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa9803cfe3746e50b53c81cd5e944eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63b5065b1d72b5e10bcd4d5b80a5a3c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1620bb008ba79d3faab82f5af4606c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3d78f214ce8f0f97b0e2c4ddd7e29e8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ASR-FAIRBENCH-Measuring-and-Benchmarking-Equity-Across-Speech-Recognition-Systems"><a href="#ASR-FAIRBENCH-Measuring-and-Benchmarking-Equity-Across-Speech-Recognition-Systems" class="headerlink" title="ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech   Recognition Systems"></a>ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech   Recognition Systems</h2><p><strong>Authors:Anand Rai, Satyam Rahangdale, Utkarsh Anand, Animesh Mukherjee</strong></p>
<p>Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday applications, yet significant disparities in performance across diverse demographic groups persist. In this work, we introduce the ASR-FAIRBENCH leaderboard which is designed to assess both the accuracy and equity of ASR models in real-time. Leveraging the Metaâ€™s Fair-Speech dataset, which captures diverse demographic characteristics, we employ a mixed-effects Poisson regression model to derive an overall fairness score. This score is integrated with traditional metrics like Word Error Rate (WER) to compute the Fairness Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our approach reveals significant performance disparities in SOTA ASR models across demographic groups and offers a benchmark to drive the development of more inclusive ASR technologies. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨æ—¥å¸¸åº”ç”¨ä¸­å·²ç»æ— å¤„ä¸åœ¨ï¼Œä½†åœ¨ä¸åŒäººå£ç¾¤ä½“ä¸­çš„æ€§èƒ½ä»å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ASR-FAIRBENCHæ’è¡Œæ¦œï¼Œæ—¨åœ¨å®æ—¶è¯„ä¼°ASRæ¨¡å‹çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨Metaçš„Fair-Speechæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ•æ‰äº†å¤šæ ·åŒ–çš„äººå£ç‰¹å¾ï¼Œé‡‡ç”¨æ··åˆæ•ˆåº”æ³Šæ¾å›å½’æ¨¡å‹å¾—å‡ºæ•´ä½“å…¬å¹³åˆ†æ•°ã€‚è¯¥åˆ†æ•°ä¸è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç­‰ä¼ ç»ŸæŒ‡æ ‡ç›¸ç»“åˆï¼Œè®¡ç®—å‡ºå…¬å¹³è°ƒæ•´åçš„ASRåˆ†æ•°ï¼ˆFAASï¼‰ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ­ç¤ºäº†æœ€å…ˆè¿›ASRæ¨¡å‹åœ¨ä¸åŒäººå£ç¾¤ä½“ä¸­çš„æ€§èƒ½å·®å¼‚ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·åŒ…å®¹æ€§çš„ASRæŠ€æœ¯æä¾›äº†åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11572v1">PDF</a> Paper accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong>ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨æ—¥å¸¸åº”ç”¨ä¸­å·²æ™®åŠï¼Œä½†ä¸åŒäººç¾¤æ€§èƒ½å·®è·æ˜¾è‘—ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ASR-FAIRBENCHæ’è¡Œæ¦œï¼Œæ—¨åœ¨å®æ—¶è¯„ä¼°ASRæ¨¡å‹çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚åˆ©ç”¨Metaçš„Fair-Speechæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ•æ‰äº†å¤šç§äººå£ç‰¹å¾ï¼Œæˆ‘ä»¬é‡‡ç”¨æ··åˆæ•ˆåº”Poissonå›å½’æ¨¡å‹å¾—å‡ºæ•´ä½“å…¬å¹³å¾—åˆ†ã€‚è¯¥å¾—åˆ†ä¸è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç­‰ä¼ ç»ŸæŒ‡æ ‡ç›¸ç»“åˆï¼Œè®¡ç®—å‡ºå…¬å¹³è°ƒæ•´åçš„ASRå¾—åˆ†ï¼ˆFAASï¼‰ï¼Œä¸ºå…¨é¢è¯„ä¼°æ¡†æ¶æä¾›ä¾æ®ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†é¡¶å°–ASRæ¨¡å‹åœ¨ä¸åŒäººç¾¤ä¸­çš„æ€§èƒ½å·®è·ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·åŒ…å®¹æ€§çš„ASRæŠ€æœ¯æä¾›äº†åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ASRç³»ç»Ÿåœ¨æ™®åŠåº”ç”¨ä¸­ä»å­˜åœ¨ä¸åŒäººç¾¤çš„æ€§èƒ½å·®å¼‚é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ASR-FAIRBENCHæ’è¡Œæ¦œä»¥è¯„ä¼°ASRæ¨¡å‹çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚</li>
<li>åˆ©ç”¨Fair-Speechæ•°æ®é›†æ¥æ•æ‰å¤šç§äººå£ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨æ··åˆæ•ˆåº”Poissonå›å½’æ¨¡å‹è¯„ä¼°ASRæ¨¡å‹çš„å…¬å¹³æ€§ã€‚</li>
<li>ç»“åˆä¼ ç»ŸæŒ‡æ ‡å¦‚è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå…¬å¹³å¾—åˆ†ï¼Œæå‡ºå…¬å¹³è°ƒæ•´åçš„ASRå¾—åˆ†ï¼ˆFAASï¼‰ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†ç°æœ‰é¡¶å°–ASRæ¨¡å‹åœ¨ä¸åŒäººç¾¤ä¸­çš„æ€§èƒ½å·®è·ã€‚</li>
<li>ä¸ºå¼€å‘æ›´åŒ…å®¹çš„ASRæŠ€æœ¯æä¾›äº†åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5dab00e8ec467899483f6e4c16ee8688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f267586351e49e23d99626c51a77a8f7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ArrayDPS-Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior"><a href="#ArrayDPS-Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior" class="headerlink" title="ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior"></a>ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior</h2><p><strong>Authors:Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury</strong></p>
<p>Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a>. </p>
<blockquote>
<p>ç›²è¯­éŸ³åˆ†ç¦»ï¼ˆBSSï¼‰æ—¨åœ¨ä»éº¦å…‹é£é˜µåˆ—è®°å½•çš„éŸ³é¢‘æ··åˆä¸­åˆ†ç¦»å‡ºå¤šä¸ªè¯­éŸ³æºã€‚è¿™ä¸ªé—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç›²é€†é—®é¢˜ï¼Œå³éº¦å…‹é£é˜µåˆ—çš„å‡ ä½•å½¢çŠ¶ã€æˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰å’Œè¯­éŸ³æºéƒ½æ˜¯æœªçŸ¥çš„ã€‚æˆ‘ä»¬æå‡ºArrayDPSä»¥æ— ç›‘ç£ã€é˜µåˆ—æ— å…³å’Œç”Ÿæˆçš„æ–¹å¼è§£å†³BSSé—®é¢˜ã€‚æ ¸å¿ƒç†å¿µå»ºç«‹åœ¨æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰çš„åŸºç¡€ä¸Šï¼Œä½†ä¸åŒäºDPSä¸­å¯èƒ½æ€§æ˜¯å¯è¿½è¸ªçš„ï¼ŒArrayDPSå¿…é¡»é€šè¿‡åˆ¶å®šä¸€ä¸ªå•ç‹¬çš„ä¼˜åŒ–é—®é¢˜æ¥è¿‘ä¼¼å¯èƒ½æ€§ã€‚ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆè¿‘ä¼¼äºæˆ¿é—´å£°å­¦ä»¥åŠéº¦å…‹é£ä¹‹é—´çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ã€‚è¿™äº›è¿‘ä¼¼å€¼ï¼Œè¿åŒæ‰©æ•£å…ˆéªŒï¼Œåœ¨ArrayDPSé‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£ï¼Œå¹¶æœ€ç»ˆäº§ç”Ÿåˆ†ç¦»çš„è¯­éŸ³æºã€‚æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªç®€å•çš„å•è¯´è¯äººè¯­éŸ³æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œä»¥åŠéº¦å…‹é£è®°å½•çš„æ··åˆå£°éŸ³ï¼›æ— éœ€éº¦å…‹é£é˜µåˆ—ä¿¡æ¯ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒArrayDPSåœ¨SDRæ–¹é¢ä¼˜äºæ‰€æœ‰åŸºçº¿æ— ç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶ä¸æœ‰ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚éŸ³é¢‘æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/%E3%80%82">https://arraydps.github.io/ArrayDPSDemo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05657v2">PDF</a> Paper Accepted at ICML2025 Demo:   <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/ArrayDPS/ArrayDPS">https://github.com/ArrayDPS/ArrayDPS</a></p>
<p><strong>Summary</strong><br>ç›²è¯­éŸ³åˆ†ç¦»ï¼ˆBSSï¼‰æ˜¯ä»éº¦å…‹é£é˜µåˆ—å½•åˆ¶çš„éŸ³é¢‘æ··åˆä¸­åˆ†ç¦»å¤šä¸ªè¯­éŸ³æºçš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºArrayDPSä»¥æ— ç›‘ç£ã€é˜µåˆ—æ— å…³å’Œç”Ÿæˆçš„æ–¹å¼è§£å†³BSSé—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³åŸºäºæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œä½†ä¸åŒäºDPSä¸­çš„ä¼¼ç„¶æ€§å¯ä»¥è®¡ç®—ï¼ŒArrayDPSå¿…é¡»é€šè¿‡æ„å»ºå•ç‹¬çš„ä¼˜åŒ–é—®é¢˜æ¥è¿‘ä¼¼ä¼¼ç„¶æ€§ã€‚è¯¥ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆè¿‘ä¼¼äºæˆ¿é—´å£°å­¦ä»¥åŠéº¦å…‹é£ä¹‹é—´çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ã€‚è¿™äº›è¿‘ä¼¼å€¼ä¸æ‰©æ•£å…ˆéªŒå€¼ä¸€èµ·ï¼Œåœ¨ArrayDPSé‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£ï¼Œæœ€ç»ˆäº§ç”Ÿåˆ†ç¦»çš„è¯­éŸ³æºã€‚æˆ‘ä»¬ä»…éœ€è¦ä¸€ä¸ªç®€å•çš„å•è¯´è¯äººè¯­éŸ³æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒå€¼ï¼Œä»¥åŠéº¦å…‹é£å½•åˆ¶çš„æ··åˆå£°éŸ³ï¼Œæ— éœ€çŸ¥é“éº¦å…‹é£é˜µåˆ—çš„ä¿¡æ¯ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒArrayDPSåœ¨æ— äººç›‘ç£çš„æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶åœ¨SDRæ–¹é¢ä¸æœ‰ç›‘ç£çš„æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›²è¯­éŸ³åˆ†ç¦»ï¼ˆBSSï¼‰æ˜¯ä»éº¦å…‹é£é˜µåˆ—å½•åˆ¶çš„éŸ³é¢‘ä¸­åˆ†ç¦»å¤šä¸ªè¯­éŸ³æºçš„é—®é¢˜ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ArrayDPSä»¥æ— ç›‘ç£ã€é˜µåˆ—æ— å…³å’Œç”Ÿæˆçš„æ–¹å¼è§£å†³BSSé—®é¢˜ã€‚</li>
<li>ArrayDPSçš„æ ¸å¿ƒæ€æƒ³åŸºäºæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œä½†éœ€è¦è¿‘ä¼¼ä¼¼ç„¶æ€§ã€‚</li>
<li>è¯¥ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆè¿‘ä¼¼äºæˆ¿é—´å£°å­¦åŠéº¦å…‹é£é—´çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ã€‚</li>
<li>ArrayDPSç»“åˆäº†æ‰©æ•£å…ˆéªŒä¸ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œè¿­ä»£é‡‡æ ·ï¼Œæœ€ç»ˆäº§ç”Ÿåˆ†ç¦»çš„è¯­éŸ³æºã€‚</li>
<li>ä»…éœ€ç®€å•çš„å•è¯´è¯äººè¯­éŸ³æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œä»¥åŠæ··åˆå½•éŸ³ï¼Œæ— éœ€çŸ¥é“éº¦å…‹é£é˜µåˆ—çš„å…·ä½“ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27c858c5b17f850bb254236cf2c28660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b35aefbe735cc768ec22f24f852e03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9d3c70b32a33862eba0211bdeacd57d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57da60f5e871e72ab306d4133bd89a44.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PAHA-Parts-Aware-Audio-Driven-Human-Animation-with-Diffusion-Model"><a href="#PAHA-Parts-Aware-Audio-Driven-Human-Animation-with-Diffusion-Model" class="headerlink" title="PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model"></a>PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model</h2><p><strong>Authors:S. Z. Zhou, Y. B. Wang, J. F. Wu, T. Hu, J. N. Zhang, Z. J. Li, Y. Liu</strong></p>
<p>Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„äººå½¢åŠ¨ç”»æŠ€æœ¯å¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’é¢†åŸŸï¼Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°è¿›ä¸€æ­¥æ¨åŠ¨äº†å…¶å‘å±•ã€‚å½“å‰ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºå¤šé˜¶æ®µç”Ÿæˆå’Œä¸­é—´è¡¨ç¤ºï¼Œå¯¼è‡´æ¨ç†æ—¶é—´é•¿ï¼Œç‰¹å®šå‰æ™¯åŒºåŸŸç”Ÿæˆè´¨é‡å’ŒéŸ³ç”»åŒæ­¥é—®é¢˜ã€‚è¿™äº›ç¼ºç‚¹ä¸»è¦æ˜¯ç”±äºç¼ºä¹å±€éƒ¨ç²¾ç»†ç›‘ç£æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PAHAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç«¯åˆ°ç«¯éŸ³é¢‘é©±åŠ¨äººä½“ä¸ŠåŠéƒ¨åˆ†åŠ¨ç”»æ¡†æ¶ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§å…³é”®æ–¹æ³•ï¼šé›¶ä»¶æ„ŸçŸ¥é‡åŠ æƒï¼ˆPARï¼‰å’Œé›¶ä»¶ä¸€è‡´æ€§å¢å¼ºï¼ˆPCEï¼‰ã€‚PARæ ¹æ®å§¿æ€ç½®ä¿¡åº¦åˆ†æ•°åŠ¨æ€è°ƒæ•´åŒºåŸŸè®­ç»ƒæŸå¤±æƒé‡ï¼Œæœ‰æ•ˆæé«˜è§†è§‰è´¨é‡ã€‚PCEæ„å»ºå¹¶è®­ç»ƒåŸºäºæ‰©æ•£çš„åŒºåŸŸéŸ³è§†é¢‘åˆ†ç±»å™¨ï¼Œä»¥æé«˜è¿åŠ¨å’Œè¯­éŸ³éŸ³é¢‘çš„ä¸€è‡´æ€§ã€‚ä¹‹åï¼Œæˆ‘ä»¬ä¸ºå‰è¿°åˆ†ç±»å™¨è®¾è®¡äº†ä¸¤ç§æ–°é¢–æ¨ç†æŒ‡å¯¼æ–¹æ³•ï¼Œå³é¡ºåºæŒ‡å¯¼ï¼ˆSGï¼‰å’Œå·®å¼‚æŒ‡å¯¼ï¼ˆDGï¼‰ï¼Œä»¥åˆ†åˆ«å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†CNASï¼Œé¦–ä¸ªå…¬å¼€çš„ä¸­æ–‡æ–°é—»ä¸»æ’­è¯­éŸ³æ•°æ®é›†ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’ŒéªŒè¯ã€‚å¤§é‡çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒPAHAåœ¨éŸ³é¢‘è¿åŠ¨å¯¹é½å’Œè§†é¢‘ç›¸å…³è¯„ä¼°æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’ŒCNASæ•°æ®é›†å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03603v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘é©±åŠ¨çš„äººä½“åŠ¨ç”»æŠ€æœ¯åœ¨äººæœºäº¤äº’ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä»¥åŠæ‰©æ•£æ¨¡å‹çš„å‡ºç°å¯¹å…¶å‘å±•çš„è¿›ä¸€æ­¥æ¨åŠ¨ã€‚é’ˆå¯¹å½“å‰æ–¹æ³•å­˜åœ¨çš„é•¿æœŸæ¨ç†æ—¶é—´ã€ç‰¹å®šå‰æ™¯åŒºåŸŸç”Ÿæˆè´¨é‡å’ŒéŸ³é¢‘è¿åŠ¨ä¸€è‡´æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„éŸ³é¢‘é©±åŠ¨ä¸Šèº«äººä½“åŠ¨ç”»æ¡†æ¶PAHAï¼Œå¹¶å¼•å…¥äº†Parts-Aware Re-weightingï¼ˆPARï¼‰å’ŒParts Consistency Enhancementï¼ˆPCEï¼‰ä¸¤ç§å…³é”®æ–¹æ³•ã€‚é€šè¿‡æ„å»ºå’Œè®­ç»ƒåŸºäºæ‰©æ•£çš„åŒºåŸŸéŸ³é¢‘è§†è§‰åˆ†ç±»å™¨ï¼Œæé«˜äº†è¿åŠ¨ä¸€è‡´æ€§åŠä¸è¯­éŸ³çš„åè°ƒæ€§ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸¤ç§æ–°å‹æ¨ç†æŒ‡å¯¼æ–¹æ³•ï¼ŒSequential Guidanceï¼ˆSGï¼‰å’ŒDifferential Guidanceï¼ˆDGï¼‰ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œå»ºç«‹äº†é¦–ä¸ªä¸­æ–‡æ–°é—»ä¸»æ’­è¯­éŸ³æ•°æ®é›†CNASï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’ŒéªŒè¯ã€‚å®éªŒå’Œç”¨æˆ·ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPAHAåœ¨éŸ³é¢‘è¿åŠ¨å¯¹é½å’Œè§†é¢‘ç›¸å…³è¯„ä¼°æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨çš„äººä½“åŠ¨ç”»æŠ€æœ¯åœ¨äººæœºäº¤äº’ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°è¿›ä¸€æ­¥æ¨åŠ¨äº†å…¶å‘å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨é•¿æœŸæ¨ç†æ—¶é—´ã€ç”Ÿæˆè´¨é‡åŠéŸ³é¢‘è¿åŠ¨ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>PAHAæ¡†æ¶é€šè¿‡å¼•å…¥PARå’ŒPCEæ–¹æ³•ï¼Œæé«˜äº†è§†è§‰è´¨é‡åŠè¿åŠ¨ä¸éŸ³é¢‘çš„ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†SGå’ŒDGä¸¤ç§æ¨ç†æŒ‡å¯¼æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>å»ºç«‹äº†é¦–ä¸ªä¸­æ–‡æ–°é—»ä¸»æ’­è¯­éŸ³æ•°æ®é›†CNASï¼Œç”¨äºç ”ç©¶å’ŒéªŒè¯ã€‚</li>
<li>å®éªŒå’Œç”¨æˆ·ç ”ç©¶ç»“æœæ˜¾ç¤ºPAHAæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-101623ac900caccb8976ae60e2c09597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8f966dc8284a9b15a27770d20fbfb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89d3a4c95fa931d1bd6af93bf478182e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7db20fb7d8db03a4ec414596b385621.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="USpeech-Ultrasound-Enhanced-Speech-with-Minimal-Human-Effort-via-Cross-Modal-Synthesis"><a href="#USpeech-Ultrasound-Enhanced-Speech-with-Minimal-Human-Effort-via-Cross-Modal-Synthesis" class="headerlink" title="USpeech: Ultrasound-Enhanced Speech with Minimal Human Effort via   Cross-Modal Synthesis"></a>USpeech: Ultrasound-Enhanced Speech with Minimal Human Effort via   Cross-Modal Synthesis</h2><p><strong>Authors:Luca Jiang-Tao Yu, Running Zhao, Sijie Ji, Edith C. H. Ngai, Chenshu Wu</strong></p>
<p>Speech enhancement is crucial for ubiquitous human-computer interaction. Recently, ultrasound-based acoustic sensing has emerged as an attractive choice for speech enhancement because of its superior ubiquity and performance. However, due to inevitable interference from unexpected and unintended sources during audio-ultrasound data acquisition, existing solutions rely heavily on human effort for data collection and processing. This leads to significant data scarcity that limits the full potential of ultrasound-based speech enhancement. To address this, we propose USpeech, a cross-modal ultrasound synthesis framework for speech enhancement with minimal human effort. At its core is a two-stage framework that establishes the correspondence between visual and ultrasonic modalities by leveraging audio as a bridge. This approach overcomes challenges from the lack of paired video-ultrasound datasets and the inherent heterogeneity between video and ultrasound data. Our framework incorporates contrastive video-audio pre-training to project modalities into a shared semantic space and employs an audio-ultrasound encoder-decoder for ultrasound synthesis. We then present a speech enhancement network that enhances speech in the time-frequency domain and recovers the clean speech waveform via a neural vocoder. Comprehensive experiments show USpeech achieves remarkable performance using synthetic ultrasound data comparable to physical data, outperforming state-of-the-art ultrasound-based speech enhancement baselines. USpeech is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/aiot-lab/USpeech/">https://github.com/aiot-lab/USpeech/</a>. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºå¯¹äºæ— å¤„ä¸åœ¨çš„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼ŒåŸºäºè¶…å£°çš„å£°å­¦æ„ŸçŸ¥å› å…¶æ™®éçš„é€‚ç”¨æ€§å’Œå“è¶Šçš„æ€§èƒ½è€Œæˆä¸ºè¯­éŸ³å¢å¼ºçš„ä¸€ä¸ªå¸å¼•äººçš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œåœ¨éŸ³é¢‘è¶…å£°æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­ï¼Œä¸å¯é¿å…åœ°ä¼šå­˜åœ¨æ¥è‡ªæ„å¤–å’Œéé¢„æœŸæºçš„å¹²æ‰°ï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆä¸¥é‡ä¾èµ–äºäººå·¥çš„æ•°æ®æ”¶é›†å’Œå¤„ç†ã€‚è¿™å¯¼è‡´äº†æ•°æ®ä¸¥é‡åŒ®ä¹ï¼Œé™åˆ¶äº†åŸºäºè¶…å£°çš„è¯­éŸ³å¢å¼ºçš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†USpeechï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€è¶…å£°åˆæˆæ¡†æ¶ï¼Œç”¨äºè¯­éŸ³å¢å¼ºï¼Œåªéœ€æœ€å°çš„äººå·¥å¹²é¢„ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡éŸ³é¢‘ä½œä¸ºæ¡¥æ¢ï¼Œå»ºç«‹è§†è§‰å’Œè¶…å£°æ¨¡æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ç¼ºä¹é…å¯¹è§†é¢‘è¶…å£°æ•°æ®é›†å’Œè§†é¢‘ä¸è¶…å£°æ•°æ®ä¹‹é—´å›ºæœ‰çš„å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†å¯¹æ¯”è§†é¢‘éŸ³é¢‘é¢„è®­ç»ƒï¼Œå°†æ¨¡æ€æŠ•å½±åˆ°å…±äº«è¯­ä¹‰ç©ºé—´ï¼Œå¹¶é‡‡ç”¨äº†éŸ³é¢‘è¶…å£°ç¼–ç å™¨-è§£ç å™¨è¿›è¡Œè¶…å£°åˆæˆã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­éŸ³å¢å¼ºç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨æ—¶é¢‘åŸŸå¢å¼ºè¯­éŸ³ï¼Œå¹¶é€šè¿‡ç¥ç»vocoderæ¢å¤å¹²å‡€çš„è¯­éŸ³æ³¢å½¢ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒUSpeechä½¿ç”¨åˆæˆè¶…å£°æ•°æ®å®ç°äº†ä¸ç‰©ç†æ•°æ®ç›¸å½“çš„å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºäºè¶…å£°çš„è¯­éŸ³å¢å¼ºåŸºçº¿ã€‚USpeechå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/aiot-lab/USpeech/%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/aiot-lab/USpeech/å¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22076v2">PDF</a> Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable   and Ubiquitous Technologies (ACM IMWUT&#x2F;UbiComp 2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†USpeechï¼Œä¸€ç§åˆ©ç”¨è·¨æ¨¡æ€è¶…å£°åˆæˆæŠ€æœ¯çš„è¯­éŸ³å¢å¼ºæ¡†æ¶ï¼Œå‡ ä¹æ— éœ€äººå·¥å¹²é¢„ã€‚è¯¥æ¡†æ¶é€šè¿‡å»ºç«‹è§†é¢‘ä¸è¶…å£°æ³¢ä¹‹é—´çš„å¯¹åº”å…³ç³»æ¥è§£å†³ç¼ºä¹é…å¥—çš„è§†é¢‘-è¶…å£°æ³¢æ•°æ®é›†çš„é—®é¢˜ï¼Œå€ŸåŠ©éŸ³é¢‘ä½œä¸ºåª’ä»‹æ¥è§£å†³è¿™ç§å†…åœ¨çš„å·®å¼‚ã€‚æ¡†æ¶ä¸­åŒ…å«å¯¹æ¯”è§†é¢‘-éŸ³é¢‘é¢„è®­ç»ƒä»¥åŠä¸€ä¸ªç”¨äºåˆæˆè¶…å£°æ³¢çš„éŸ³é¢‘-è¶…å£°æ³¢ç¼–ç å™¨è§£ç å™¨ã€‚åŒæ—¶ï¼Œå¼•å…¥è¯­éŸ³å¢å¼ºç½‘ç»œåœ¨æ—¶é¢‘åŸŸå†…å¢å¼ºè¯­éŸ³ï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œvocoderæ¢å¤æ¸…æ´è¯­éŸ³æ³¢å½¢ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨åˆæˆè¶…å£°æ³¢æ•°æ®çš„USpeechæ€§èƒ½å“è¶Šï¼Œä¸ç‰©ç†æ•°æ®ç›¸æ¯”è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºäºè¶…å£°æ³¢çš„è¯­éŸ³å¢å¼ºåŸºçº¿æŠ€æœ¯ã€‚USpeechå·²å¼€æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºå¯¹äºæ™®éçš„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>è¶…å£°æ³¢åœ¨è¯­éŸ³å¢å¼ºä¸­å±•ç°å‡ºè‰²çš„æ™®åŠæ€§å’Œæ€§èƒ½ï¼Œæˆä¸ºæ–°å…´çš„é€‰æ‹©ã€‚</li>
<li>ç”±äºéŸ³é¢‘å’Œè¶…å£°æ³¢æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­çš„å¹²æ‰°å’Œè¯¯é…å¯¹ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆéœ€è¦å¤§é‡äººå·¥æ•°æ®æ”¶é›†å’Œå¤„ç†ï¼Œå¯¼è‡´æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨æ¨¡æ€è¶…å£°åˆæˆæ¡†æ¶USpeechï¼Œæ—¨åœ¨ä»¥æœ€å°çš„æ‰‹å·¥åŠªåŠ›è¿›è¡Œè¯­éŸ³å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-580a157b24f361a8ff708207045e2aa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e871f49ee8aed0c9524999d3cc671021.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2e8272ada953be7648fbecc7bfe7dc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc486bcf3ddd8dc695c7acf15a56a01c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="USEF-TSE-Universal-Speaker-Embedding-Free-Target-Speaker-Extraction"><a href="#USEF-TSE-Universal-Speaker-Embedding-Free-Target-Speaker-Extraction" class="headerlink" title="USEF-TSE: Universal Speaker Embedding Free Target Speaker Extraction"></a>USEF-TSE: Universal Speaker Embedding Free Target Speaker Extraction</h2><p><strong>Authors:Bang Zeng, Ming Li</strong></p>
<p>Target speaker extraction aims to separate the voice of a specific speaker from mixed speech. Traditionally, this process has relied on extracting a speaker embedding from a reference speech, in which a speaker recognition model is required. However, identifying an appropriate speaker recognition model can be challenging, and using the target speaker embedding as reference information may not be optimal for target speaker extraction tasks. This paper introduces a Universal Speaker Embedding-Free Target Speaker Extraction (USEF-TSE) framework that operates without relying on speaker embeddings. USEF-TSE utilizes a multi-head cross-attention mechanism as a frame-level target speaker feature extractor. This innovative approach allows mainstream speaker extraction solutions to bypass the dependency on speaker recognition models and better leverage the information available in the enrollment speech, including speaker characteristics and contextual details. Additionally, USEF-TSE can seamlessly integrate with other time-domain or time-frequency domain speech separation models to achieve effective speaker extraction. Experimental results show that our proposed method achieves state-of-the-art (SOTA) performance in terms of Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) on the WSJ0-2mix, WHAM!, and WHAMR! datasets, which are standard benchmarks for monaural anechoic, noisy and noisy-reverberant two-speaker speech separation and speaker extraction. The results on the LibriMix and the blind test set of the ICASSP 2023 DNS Challenge demonstrate that the model performs well on more diverse and out-of-domain data. For access to the source code, please visit: <a target="_blank" rel="noopener" href="https://github.com/ZBang/USEF-TSE">https://github.com/ZBang/USEF-TSE</a>. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»å‡ºç‰¹å®šè¯´è¯äººçš„å£°éŸ³ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ä¾èµ–äºä»å‚è€ƒè¯­éŸ³ä¸­æå–è¯´è¯äººåµŒå…¥ï¼Œè¿™éœ€è¦è¯´è¯äººè¯†åˆ«æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¯†åˆ«åˆé€‚çš„è¯´è¯äººè¯†åˆ«æ¨¡å‹å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”ä½¿ç”¨ç›®æ ‡è¯´è¯äººåµŒå…¥ä½œä¸ºå‚è€ƒä¿¡æ¯å¯èƒ½ä¸æ˜¯é’ˆå¯¹ç›®æ ‡è¯´è¯äººæå–ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— é€šç”¨è¯´è¯äººåµŒå…¥ç›®æ ‡è¯´è¯äººæå–ï¼ˆUSEF-TSEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€ä¾èµ–è¯´è¯äººåµŒå…¥å³å¯è¿è¡Œã€‚USEF-TSEåˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„æœºåˆ¶ä½œä¸ºå¸§çº§ç›®æ ‡è¯´è¯äººç‰¹å¾æå–å™¨ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•å…è®¸ä¸»æµè¯´è¯äººæå–è§£å†³æ–¹æ¡ˆç»•è¿‡å¯¹è¯´è¯äººè¯†åˆ«æ¨¡å‹çš„ä¾èµ–ï¼Œå¹¶æ›´å¥½åœ°åˆ©ç”¨æŠ¥åè¯­éŸ³ä¸­çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯´è¯äººç‰¹å¾å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚æ­¤å¤–ï¼ŒUSEF-TSEå¯ä»¥æ— ç¼é›†æˆåˆ°å…¶ä»–æ—¶åŸŸæˆ–æ—¶é¢‘åŸŸè¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è¯´è¯äººæå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨WSJ0-2mixã€WHAMï¼å’ŒWHAMRï¼æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚è¿™äº›æ•°æ®é›†æ˜¯å•æ ¸æ¶ˆå£°ã€å™ªå£°å’Œå™ªå£°æ··å“åŒå£°é“è¯­éŸ³åˆ†ç¦»å’Œè¯´è¯äººæå–çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ã€‚åœ¨LibriMixå’ŒICASSP 2023 DNSæŒ‘æˆ˜çš„ç›²æµ‹è¯•é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ›´å¤šæ ·åŒ–å’Œé¢†åŸŸå¤–çš„æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚å¦‚éœ€è®¿é—®æºä»£ç ï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZBang/USEF-TSE%E3%80%82">https://github.com/ZBang/USEF-TSEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02615v2">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing (TASLP)</p>
<p><strong>æ‘˜è¦</strong><br>ç›®æ ‡è¯´è¯äººæå–æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»ç‰¹å®šè¯´è¯äººçš„å£°éŸ³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— é€šç”¨è¯´è¯äººåµŒå…¥ç›®æ ‡è¯´è¯äººæå–ï¼ˆUSEF-TSEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€ä¾èµ–è¯´è¯äººåµŒå…¥å³å¯è¿ä½œã€‚USEF-TSEä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„æœºåˆ¶ä½œä¸ºå¸§çº§ç›®æ ‡è¯´è¯äººç‰¹å¾æå–å™¨ï¼Œä½¿ä¸»æµè¯´è¯äººæå–è§£å†³æ–¹æ¡ˆç»•è¿‡å¯¹è¯´è¯äººè¯†åˆ«æ¨¡å‹çš„ä¾èµ–ï¼Œæ›´å¥½åœ°åˆ©ç”¨æŠ¥åè¯­éŸ³ä¸­çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯´è¯äººç‰¹å¾å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚æ­¤å¤–ï¼ŒUSEF-TSEå¯ä»¥æ— ç¼é›†æˆåˆ°å…¶ä»–æ—¶åŸŸæˆ–æ—¶é¢‘åŸŸè¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è¯´è¯äººæå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨WSJ0-2mixã€WHAM!å’ŒWHAMR!æ•°æ®é›†ä¸Šçš„å°ºåº¦ä¸å˜ä¿¡å·å¤±çœŸæ¯”ï¼ˆSI-SDRï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚è¯¥æ¨¡å‹åœ¨æ›´å¤šæ ·åŒ–å’ŒéåŸŸæ•°æ®çš„LibriMixå’ŒICASSP 2023 DNS Challengeçš„ç›²æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›®æ ‡è¯´è¯äººæå–æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»ç‰¹å®šè¯´è¯äººçš„å£°éŸ³ã€‚</li>
<li>ä¼ ç»Ÿçš„è¯´è¯äººæå–æ–¹æ³•ä¾èµ–äºä»å‚è€ƒè¯­éŸ³ä¸­æå–è¯´è¯äººåµŒå…¥ï¼Œè¿™éœ€è¦ä½¿ç”¨è¯´è¯äººè¯†åˆ«æ¨¡å‹ï¼Œä½†è¯†åˆ«åˆé€‚çš„æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>USEF-TSEæ¡†æ¶æå‡ºäº†ä¸€ç§ä¸ä¾èµ–é€šç”¨è¯´è¯äººåµŒå…¥çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„æœºåˆ¶è¿›è¡Œç›®æ ‡è¯´è¯äººç‰¹å¾æå–ã€‚</li>
<li>è¯¥æ–¹æ³•ç»•è¿‡äº†å¯¹è¯´è¯äººè¯†åˆ«æ¨¡å‹çš„ä¾èµ–ï¼Œå¹¶æ›´å¥½åœ°åˆ©ç”¨äº†æŠ¥åè¯­éŸ³ä¸­çš„ä¿¡æ¯ã€‚</li>
<li>USEF-TSEæ¡†æ¶å¯ä»¥ä¸å…¶ä»–è¯­éŸ³åˆ†ç¦»æ¨¡å‹æ— ç¼é›†æˆï¼Œå®ç°æœ‰æ•ˆçš„è¯´è¯äººæå–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUSEF-TSEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šæ ·åŒ–å’ŒéåŸŸæ•°æ®ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œæºä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f9c8a9c7c3dd9687047309eeb600afb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1fb77093b0c33ce9b82323f50a59e76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bc5942f67cba55e25a622e43f5c80cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6eddbed0e27b0dea332db968d17308.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47cf946a696cdd0fe49676bccf321c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd2c409b622a88073bd1eb90b41e47c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a263f60f0937bcab0fdd9413a099b08.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="An-interpretable-speech-foundation-model-for-depression-detection-by-revealing-prediction-relevant-acoustic-features-from-long-speech"><a href="#An-interpretable-speech-foundation-model-for-depression-detection-by-revealing-prediction-relevant-acoustic-features-from-long-speech" class="headerlink" title="An interpretable speech foundation model for depression detection by   revealing prediction-relevant acoustic features from long speech"></a>An interpretable speech foundation model for depression detection by   revealing prediction-relevant acoustic features from long speech</h2><p><strong>Authors:Qingkun Deng, Saturnino Luz, Sofia de la Fuente Garcia</strong></p>
<p>Speech-based depression detection tools could aid early screening. Here, we propose an interpretable speech foundation model approach to enhance the clinical applicability of such tools. We introduce a speech-level Audio Spectrogram Transformer (AST) to detect depression using long-duration speech instead of short segments, along with a novel interpretation method that reveals prediction-relevant acoustic features for clinician interpretation. Our experiments show the proposed model outperforms a segment-level AST, highlighting the impact of segment-level labelling noise and the advantage of leveraging longer speech duration for more reliable depression detection. Through interpretation, we observe our model identifies reduced loudness and F0 as relevant depression signals, aligning with documented clinical findings. This interpretability supports a responsible AI approach for speech-based depression detection, rendering such tools more clinically applicable. </p>
<blockquote>
<p>åŸºäºè¯­éŸ³çš„æŠ‘éƒç—‡æ£€æµ‹å·¥å…·æœ‰åŠ©äºæ—©æœŸç­›æŸ¥ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„è¯­éŸ³åŸºç¡€æ¨¡å‹æ–¹æ³•ï¼Œä»¥æé«˜æ­¤ç±»å·¥å…·çš„ä¸´åºŠé€‚ç”¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­éŸ³çº§åˆ«çš„éŸ³é¢‘é¢‘è°±å›¾è½¬æ¢å™¨ï¼ˆASTï¼‰æ¥æ£€æµ‹æŠ‘éƒç—‡ï¼Œä½¿ç”¨é•¿æ—¶è¯­éŸ³è€Œä¸æ˜¯çŸ­ç‰‡æ®µï¼Œä»¥åŠä¸€ç§æ–°çš„è§£é‡Šæ–¹æ³•ï¼Œæ­ç¤ºä¸é¢„æµ‹ç›¸å…³çš„å£°å­¦ç‰¹å¾ï¼Œä¾›ä¸´åºŠåŒ»ç”Ÿè§£é‡Šã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°ä¼˜äºç‰‡æ®µçº§åˆ«çš„ASTï¼Œçªå‡ºäº†ç‰‡æ®µçº§åˆ«æ ‡ç­¾å™ªå£°çš„å½±å“ï¼Œä»¥åŠåˆ©ç”¨æ›´é•¿çš„è¯­éŸ³æŒç»­æ—¶é—´è¿›è¡Œæ›´å¯é çš„æŠ‘éƒç—‡æ£€æµ‹çš„ä¼˜åŠ¿ã€‚é€šè¿‡è§£é‡Šï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ¨¡å‹è¯†åˆ«å‡ºé™ä½çš„å“åº¦å’ŒF0ä¸æŠ‘éƒç—‡ç›¸å…³ï¼Œè¿™ä¸ä¸´åºŠå‘ç°ç›¸å»åˆã€‚è¿™ç§å¯è§£é‡Šæ€§æ”¯æŒäº†åŸºäºè¯­éŸ³çš„æŠ‘éƒç—‡æ£€æµ‹çš„è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œä½¿è¿™ç±»å·¥å…·æ›´é€‚ç”¨äºä¸´åºŠå®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03138v3">PDF</a> 5 pages, 3 figures. arXiv admin note: substantial text overlap with   arXiv:2309.13476</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯­éŸ³åŸºç¡€çš„æŠ‘éƒç—‡æ£€æµ‹å·¥å…·å¯è¾…åŠ©æ—©æœŸç­›æŸ¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„è¯­éŸ³åŸºç¡€æ¨¡å‹æ–¹æ³•ï¼Œä»¥æé«˜æ­¤ç±»å·¥å…·çš„ä¸´åºŠé€‚ç”¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºè¯­éŸ³å±‚é¢çš„éŸ³é¢‘é¢‘è°±å›¾è½¬æ¢å™¨ï¼ˆASTï¼‰æ¥æ£€æµ‹æŠ‘éƒç—‡ï¼Œä½¿ç”¨é•¿æ—¶è¯­éŸ³è€ŒéçŸ­ç‰‡æ®µè¿›è¡Œæ£€æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è§£é‡Šæ–¹æ³•ï¼Œæ­ç¤ºä¸é¢„æµ‹ç›¸å…³çš„å£°å­¦ç‰¹å¾ï¼Œä»¥ä¾›ä¸´åºŠåŒ»ç”Ÿè§£è¯»ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹ä¼˜äºåŸºäºç‰‡æ®µçº§åˆ«çš„ASTæ¨¡å‹ï¼Œå‡¸æ˜¾äº†ç‰‡æ®µçº§åˆ«æ ‡ç­¾å™ªå£°çš„å½±å“ä»¥åŠåˆ©ç”¨è¾ƒé•¿è¯­éŸ³æ—¶é•¿è¿›è¡Œæ›´å¯é çš„æŠ‘éƒç—‡æ£€æµ‹çš„ä¼˜åŠ¿ã€‚é€šè¿‡è§£è¯»ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹è¯†åˆ«å‡ºçš„ä½æ²‰éŸ³é‡å’ŒF0ä¸æŠ‘éƒç—‡æœ‰å…³çš„å£°éŸ³ä¿¡å·æ˜¯ä¸€è‡´çš„ï¼Œè¿™ä¸ä¸´åºŠå‘ç°ç›¸å»åˆã€‚è¿™ç§å¯è§£é‡Šæ€§æ”¯æŒäº†é¢å‘è¯­éŸ³çš„æŠ‘éƒç—‡æ£€æµ‹è´£ä»»AIæ–¹æ³•ï¼Œä½¿æ­¤ç±»å·¥å…·æ›´å…·ä¸´åºŠé€‚ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³åŸºç¡€çš„æŠ‘éƒç—‡æ£€æµ‹å·¥å…·å…·æœ‰æ—©æœŸç­›æŸ¥çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„è¯­éŸ³åŸºç¡€æ¨¡å‹æ–¹æ³•ï¼Œä»¥å¢å¼ºä¸´åºŠé€‚ç”¨æ€§ã€‚</li>
<li>å¼•å…¥äº†åŸºäºè¯­éŸ³å±‚é¢çš„Audio Spectrogram Transformerï¼ˆASTï¼‰æ¥æ£€æµ‹æŠ‘éƒç—‡ã€‚</li>
<li>æå‡ºçš„æ–°è§£é‡Šæ–¹æ³•æœ‰åŠ©äºæ­ç¤ºé¢„æµ‹ç›¸å…³çš„å£°å­¦ç‰¹å¾ï¼Œæ”¯æŒåŒ»ç”Ÿè§£è¯»ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹æ€§èƒ½ä¼˜äºåŸºäºç‰‡æ®µçº§åˆ«çš„ASTæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è¯†åˆ«å‡ºçš„å£°éŸ³ç‰¹å¾ä¸ä¸´åºŠå‘ç°ç›¸ç¬¦ï¼Œå¦‚é™ä½çš„éŸ³é‡å’ŒF0å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d03c7bc2d547c7ad269759f6b88f838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd9ceb054f3b2b5cdd7eaedf49e34cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9776b1595712db6241ecce474b57b76d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01efc1e060d207a3fe142647bedf88a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48391da3030faf343647c61893d130c1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2de1590f7cdc3ac735530a08d72b030a.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Higher fidelity perceptual image and video compression with a latent   conditioned residual denoising diffusion model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8aa3c044634d4c71c24020aa158d44cb.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  DC-Seg Disentangled Contrastive Learning for Brain Tumor Segmentation   with Missing Modalities
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
