<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  ChartMuseum Testing Visual Reasoning Capabilities of Large   Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cd7c459344316127e56a86ccda73a2dd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-21-æ›´æ–°"><a href="#2025-05-21-æ›´æ–°" class="headerlink" title="2025-05-21 æ›´æ–°"></a>2025-05-21 æ›´æ–°</h1><h2 id="ChartMuseum-Testing-Visual-Reasoning-Capabilities-of-Large-Vision-Language-Models"><a href="#ChartMuseum-Testing-Visual-Reasoning-Capabilities-of-Large-Vision-Language-Models" class="headerlink" title="ChartMuseum: Testing Visual Reasoning Capabilities of Large   Vision-Language Models"></a>ChartMuseum: Testing Visual Reasoning Capabilities of Large   Vision-Language Models</h2><p><strong>Authors:Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett</strong></p>
<p>Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks â€“ where frontier models perform similarly and near saturation â€“ our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs. </p>
<blockquote>
<p>å›¾è¡¨ç†è§£å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦æ•´åˆå¤æ‚çš„æ–‡æœ¬å’Œè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„LVLMsåœ¨è¿™äº›æŠ€èƒ½ä¹‹é—´è¡¨ç°å‡ºæ˜æ˜¾çš„å¤±è¡¡ï¼Œåœ¨æ–‡æœ¬ä¸­éš¾ä»¥æ‰§è¡Œçš„è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°ä¸è¶³ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªåªèƒ½é€šè¿‡è§†è§‰æ¨ç†è§£å†³çš„åˆæˆæ•°æ®é›†è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œå¹¶å‘ç°éšç€è§†è§‰å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œäººç±»æ€§èƒ½ä¿æŒç¨³å¥ã€‚éšåï¼Œæˆ‘ä»¬ä»‹ç»äº†ChartMuseumï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å›¾è¡¨é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1162ä¸ªä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ï¼Œæ¶µç›–å¤šç§æ¨ç†ç±»å‹ï¼Œè¿™äº›é—®é¢˜æ˜¯ä»æ¥è‡ª184ä¸ªæ¥æºçš„çœŸå®ä¸–ç•Œå›¾è¡¨ä¸­ç²¾å¿ƒæŒ‘é€‰çš„ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬æ¨ç†ã€‚ä¸ä¹‹å‰å›¾è¡¨ç†è§£åŸºå‡†æµ‹è¯•ä¸­çš„æ¨¡å‹è¡¨ç°ç›¸ä¼¼ä¸”æ¥è¿‘é¥±å’Œçš„æƒ…å†µä¸åŒï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ¨¡å‹ä¸äººç±»æ€§èƒ½ä¹‹é—´çš„å·¨å¤§å·®è·ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°åŒºåˆ†äº†æ¨¡å‹çš„èƒ½åŠ›ï¼šè™½ç„¶äººç±»è¾¾åˆ°93%çš„å‡†ç¡®ç‡ï¼Œä½†è¡¨ç°æœ€ä½³çš„æ¨¡å‹Gemini-2.5-Proä»…è¾¾åˆ°63.0%ï¼Œé¢†å…ˆçš„å¼€æºLVLM Qwen2.5-VL-72B-Instructä»…è¾¾åˆ°38.5%ã€‚æ­¤å¤–ï¼Œåœ¨ä¸»è¦éœ€è¦è§†è§‰æ¨ç†çš„é—®é¢˜ä¸Šï¼Œæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½æ¯”ä¾§é‡äºæ–‡æœ¬æ¨ç†çš„é—®é¢˜ä¸‹é™äº†35%-55%ã€‚æœ€åï¼Œæˆ‘ä»¬çš„å®šæ€§é”™è¯¯åˆ†ææ­ç¤ºäº†å¯¹å½“å‰LVLMså…·æœ‰æŒ‘æˆ˜æ€§çš„ç‰¹å®šç±»åˆ«çš„è§†è§‰æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾è¡¨ç†è§£ä¸Šæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼ŒLVLMsåœ¨è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹æ—¥ç›Šå¤æ‚çš„è§†è§‰å†…å®¹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„å›¾è¡¨é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•ChartMuseumï¼ŒåŒ…å«1,162ä¸ªçœŸå®ä¸–ç•Œå›¾è¡¨çš„ä¸“å®¶æ ‡æ³¨é—®é¢˜ã€‚ç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•ï¼ŒChartMuseumæ›´èƒ½å‡¸æ˜¾æ¨¡å‹ä¸äººç±»çš„æ€§èƒ½å·®è·ï¼Œæœ‰æ•ˆåŒºåˆ†æ¨¡å‹èƒ½åŠ›ã€‚å°½ç®¡äººç±»å‡†ç¡®ç‡é«˜è¾¾93%ï¼Œä½†æœ€ä½³æ¨¡å‹ä»…è¾¾åˆ°63%ï¼Œå¼€æºLVLMæ¨¡å‹Qwen2.5-VL-72B-Instructä»…è¾¾åˆ°38.5%ã€‚åœ¨ä¸»è¦ä¾èµ–è§†è§‰æ¨ç†çš„é—®é¢˜ä¸Šï¼Œæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½è¾ƒæ–‡æœ¬æ¨ç†ä¸»å¯¼çš„é—®é¢˜ä¸‹é™äº†35%-55%ã€‚æ­¤å¤–ï¼Œå®šæ€§é”™è¯¯åˆ†ææ­ç¤ºäº†å½“å‰LVLMsåœ¨ç‰¹å®šç±»åˆ«è§†è§‰æ¨ç†ä¸Šçš„éš¾ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾è¡¨ç†è§£æ–¹é¢é¢ä¸´è§†è§‰æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹åœ¨å¤æ‚è§†è§‰å†…å®¹ä¸‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œäººç±»æ€§èƒ½ä¿æŒç¨³å®šã€‚</li>
<li>ChartMuseumåŸºå‡†æµ‹è¯•åŒ…å«å¤šç§æ¨ç†ç±»å‹çš„çœŸå®ä¸–ç•Œå›¾è¡¨é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹ä¸äººç±»çš„æ€§èƒ½å·®è·æ˜¾è‘—ï¼Œæœ€ä½³æ¨¡å‹æ€§èƒ½ä»è¿œä½äºäººç±»ã€‚</li>
<li>åœ¨è§†è§‰æ¨ç†ä¸»å¯¼çš„é—®é¢˜ä¸Šï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™å¹…åº¦è¾ƒå¤§ã€‚</li>
<li>å½“å‰LVLMsåœ¨ç‰¹å®šç±»åˆ«çš„è§†è§‰æ¨ç†ä¸Šä»å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d92279f7c525d3e958992b8a260d1404.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee5b13b8e02d9da24825c0519df6e6e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ce4d44a4c549534739078b068243f49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be52a0cdd07390c477328772aaa3f9e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca11e21ce0b852addd2a264d167ff5d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86a80b75028a40d157f57dfca68eb997.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Optimizing-Anytime-Reasoning-via-Budget-Relative-Policy-Optimization"><a href="#Optimizing-Anytime-Reasoning-via-Budget-Relative-Policy-Optimization" class="headerlink" title="Optimizing Anytime Reasoning via Budget Relative Policy Optimization"></a>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</h2><p><strong>Authors:Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin</strong></p>
<p>Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency. </p>
<blockquote>
<p>æ‰©å±•æµ‹è¯•æ—¶é—´çš„è®¡ç®—å¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æœ€å¤§åŒ–æ¨ç†è½¨è¿¹ç»“æŸæ—¶è·å¾—çš„éªŒè¯å¥–åŠ±ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…åœ¨å›ºå®šä¸”è¾ƒå¤§çš„ä»¤ç‰Œé¢„ç®—ä¸‹ä¼˜åŒ–æœ€ç»ˆæ€§èƒ½ï¼Œè¿™é˜»ç¢äº†è®­ç»ƒå’Œéƒ¨ç½²çš„æ•ˆç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€œAnytimeReasonerâ€ï¼Œä»¥ä¼˜åŒ–ä»»ä½•æ—¶é—´çš„æ¨ç†æ€§èƒ½ï¼Œæ—¨åœ¨æé«˜ä»¤ç‰Œæ•ˆç‡å’Œåœ¨ä¸åŒä»¤ç‰Œé¢„ç®—çº¦æŸä¸‹çš„æ¨ç†çµæ´»æ€§ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å°†å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹æˆªæ–­ä»¥é€‚åº”ä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·çš„ä»¤ç‰Œé¢„ç®—ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºæ¯æ¬¡æˆªæ–­çš„æ€è€ƒè¿‡ç¨‹æ€»ç»“æœ€ä½³ç­”æ¡ˆä»¥è¿›è¡ŒéªŒè¯ã€‚è¿™ä¸ºæ¨ç†è¿‡ç¨‹å¼•å…¥äº†å¯éªŒè¯çš„å¯†é›†å¥–åŠ±ï¼Œæœ‰åŠ©äºRLä¼˜åŒ–ä¸­æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»¥è§£è€¦çš„æ–¹å¼ä¼˜åŒ–æ€è€ƒå’Œæ€»ç»“ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹å·®é™ä½æŠ€æœ¯ï¼Œå³é¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œä»¥æé«˜åœ¨å¼ºåŒ–æ€è€ƒç­–ç•¥æ—¶å­¦ä¹ è¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡çš„å®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ€è€ƒé¢„ç®—ä¸‹å‡ä¼˜äºGRPOï¼Œæé«˜äº†è®­ç»ƒå’Œä»¤ç‰Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13438v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶é—´çš„è®¡ç®—èƒ½åŠ›æ¥å¢å¼ºã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦é’ˆå¯¹å›ºå®šçš„ä»¤ç‰Œé¢„ç®—ä¼˜åŒ–æœ€ç»ˆæ€§èƒ½ï¼Œé™åˆ¶äº†è®­ç»ƒå’Œéƒ¨ç½²çš„æ•ˆç‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶AnytimeReasonerï¼Œæ—¨åœ¨ä¼˜åŒ–ä»»æ„æ—¶é—´ç‚¹çš„æ¨ç†æ€§èƒ½ï¼Œæé«˜ä»¤ç‰Œæ•ˆç‡å’Œåº”å¯¹ä¸åŒä»¤ç‰Œé¢„ç®—çº¦æŸçš„çµæ´»æ€§ã€‚é€šè¿‡æˆªæ–­å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹ä»¥é€‚åº”æ¥è‡ªå…ˆéªŒåˆ†å¸ƒçš„é‡‡æ ·ä»¤ç‰Œé¢„ç®—ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºæ¯ä¸ªæˆªæ–­çš„æ€è€ƒè¿‡ç¨‹æ€»ç»“æœ€ä½³ç­”æ¡ˆä»¥ä¾›éªŒè¯ã€‚è¿™å¼•å…¥äº†å¯éªŒè¯çš„å¯†é›†å¥–åŠ±åˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¿ƒè¿›äº†æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­çš„ä¿¡ç”¨åˆ†é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹å·®å‡å°‘æŠ€æœ¯â€”â€”é¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œä»¥æé«˜å¼ºåŒ–æ€è€ƒç­–ç•¥æ—¶å­¦ä¹ å’Œè¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚åœ¨æ•°ç†æ¨ç†ä»»åŠ¡ä¸­çš„ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ€è€ƒé¢„ç®—ä¸‹å‡ä¼˜äºGRPOï¼Œæé«˜äº†è®­ç»ƒå’Œä»¤ç‰Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´çš„è®¡ç®—èƒ½åŠ›å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å­˜åœ¨å›ºå®šä»¤ç‰Œé¢„ç®—é™åˆ¶ï¼Œå½±å“è®­ç»ƒå’Œéƒ¨ç½²æ•ˆç‡ã€‚</li>
<li>AnytimeReasoneræ¡†æ¶æ—¨åœ¨ä¼˜åŒ–ä»»æ„æ—¶é—´ç‚¹çš„æ¨ç†æ€§èƒ½å’Œæé«˜ä»¤ç‰Œæ•ˆç‡ã€‚</li>
<li>é€šè¿‡æˆªæ–­æ€è€ƒè¿‡ç¨‹å¹¶é€‚åº”é‡‡æ ·ä»¤ç‰Œé¢„ç®—ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºæ¯ä¸ªæˆªæ–­çš„æ€è€ƒè¿‡ç¨‹æ€»ç»“æœ€ä½³ç­”æ¡ˆã€‚</li>
<li>å¼•å…¥å¯éªŒè¯çš„å¯†é›†å¥–åŠ±åˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>å¼•å…¥é¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰æŠ€æœ¯ï¼Œæé«˜å­¦ä¹ å’Œè¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab7f0514cd9aaee07b0e579d019548a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e18da88445987aa06e1873ce9ae3403.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3854d94a83e2188024a13add244d982e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088a2ec05312ed15c4112a44fdcb572f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MM-PRM-Enhancing-Multimodal-Mathematical-Reasoning-with-Scalable-Step-Level-Supervision"><a href="#MM-PRM-Enhancing-Multimodal-Mathematical-Reasoning-with-Scalable-Step-Level-Supervision" class="headerlink" title="MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable   Step-Level Supervision"></a>MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable   Step-Level Supervision</h2><p><strong>Authors:Lingxiao Du, Fanqing Meng, Zongkai Liu, Zhixiang Zhou, Ping Luo, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-PRM">https://github.com/ModalMinds/MM-PRM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†åœ¨å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç»å¸¸äº§ç”Ÿé€»è¾‘ä¸ä¸€è‡´æˆ–éƒ¨åˆ†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä¸€ä¸ªå…³é”®é™åˆ¶åœ¨äºä¸­é—´æ¨ç†æ­¥éª¤ç¼ºä¹ç²¾ç»†çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MM-PRMï¼Œè¿™æ˜¯ä¸€ç§åœ¨å…¨è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„æ¡†æ¶å†…è®­ç»ƒçš„æµç¨‹å¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†MM-Policyï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤šæ ·åŒ–çš„æ•°å­¦æ¨ç†æ•°æ®ä¸Šè®­ç»ƒçš„å¼ºå¤§å¤šæ¨¡æ€æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†MM-K12ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1ä¸‡é“å¯éªŒè¯ç­”æ¡ˆçš„å¤šæ¨¡æ€æ•°å­¦é—®é¢˜æ•°æ®é›†ï¼Œä½œä¸ºç§å­æ•°æ®ã€‚åˆ©ç”¨åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„ç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡70ä¸‡æ­¥çš„æ³¨é‡Šï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚æ‰€å¾—çš„PRMç”¨äºåœ¨Best-of-Næ¨ç†è®¾ç½®ä¸­è¯„åˆ†å€™é€‰æ¨ç†è·¯å¾„ï¼Œå¹¶åœ¨åŸŸå†…ï¼ˆMM-K12æµ‹è¯•é›†ï¼‰å’ŒåŸŸå¤–ï¼ˆOlympiadBenchã€MathVistaç­‰ï¼‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†è½¯æ ‡ç­¾ã€è¾ƒå°å­¦ä¹ ç‡å’Œè·¯å¾„å¤šæ ·æ€§åœ¨ä¼˜åŒ–PRMæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚MM-PRMè¯æ˜ï¼Œæµç¨‹ç›‘ç£æ˜¯æé«˜å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿé€»è¾‘ç¨³å¥æ€§çš„æœ‰åŠ›å·¥å…·ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-PRM">https://github.com/ModalMinds/MM-PRM</a>ä¸Šå‘å¸ƒäº†æ‰€æœ‰ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13427v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMM-PRMçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªå¼ºå¤§çš„å¤šåª’ä½“æ¨¡å‹MM-Policyå’Œåˆ¶ä½œæ•°æ®é›†MM-K12ä½œä¸ºç§å­æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”Ÿæˆè¶…è¿‡70ä¸‡æ­¥éª¤çº§åˆ«çš„æ³¨é‡Šï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥æ¨¡å‹åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶éªŒè¯äº†è½¯æ ‡ç­¾ã€å°å­¦ä¹ ç‡å’Œè·¯å¾„å¤šæ ·æ€§åœ¨ä¼˜åŒ–PRMæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¯æ˜äº†è¿‡ç¨‹ç›‘ç£åœ¨å¢å¼ºå¤šåª’ä½“æ¨ç†ç³»ç»Ÿçš„é€»è¾‘ç¨³å¥æ€§æ–¹é¢çš„å¼ºå¤§ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä¸Šä»æœ‰å›°éš¾ã€‚</li>
<li>ç¼ºä¹ä¸­é—´æ¨ç†æ­¥éª¤çš„ç²¾ç»†ç›‘ç£æ˜¯MLLMsé¢ä¸´çš„å…³é”®é™åˆ¶ã€‚</li>
<li>æå‡ºMM-PRMè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å…¨è‡ªåŠ¨å’Œå¯æ‰©å±•çš„æ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ„å»ºMM-Policyå¼ºå¤šåª’ä½“æ¨¡å‹å’ŒMM-K12æ•°æ®é›†ï¼Œä½œä¸ºç§å­æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”Ÿæˆå¤§é‡æ­¥éª¤çº§åˆ«çš„æ³¨é‡Šï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>MM-PRMæ¨¡å‹åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c915a603cf50a0a534b2d84ce3fd7677.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ad800df424f2c31542099f4e1ec67b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="G1-Bootstrapping-Perception-and-Reasoning-Abilities-of-Vision-Language-Model-via-Reinforcement-Learning"><a href="#G1-Bootstrapping-Perception-and-Reasoning-Abilities-of-Vision-Language-Model-via-Reinforcement-Learning" class="headerlink" title="G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language   Model via Reinforcement Learning"></a>G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language   Model via Reinforcement Learning</h2><p><strong>Authors:Liang Chen, Hongcheng Gao, Tianyu Liu, Zhiqi Huang, Flood Sung, Xinyu Zhou, Yuxin Wu, Baobao Chang</strong></p>
<p>Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This &#96;&#96;knowing-doingâ€™â€™ gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at <a target="_blank" rel="noopener" href="https://github.com/chenllliang/G1">https://github.com/chenllliang/G1</a> to foster future research in advancing VLMs as capable interactive agents. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è®¸å¤šç›´æ¥çš„å¤šæ¨¡å¼ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å°†è¿™ä¸€èƒ½åŠ›è½¬åŒ–ä¸ºæ¸¸æˆç­‰äº¤äº’æ€§ä¸°å¯Œã€è§†è§‰ä¸°å¯Œçš„ç¯å¢ƒä¸­çš„æœ‰æ•ˆå†³ç­–æ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚è¿™ç§â€œçŸ¥è€Œä¸èƒ½è¡Œâ€çš„å·®è·æå¤§åœ°é™åˆ¶äº†å®ƒä»¬ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„æ½œåŠ›ï¼Œå› ä¸ºé¢†å…ˆçš„VLMså¾€å¾€åœ¨ç®€å•çš„æ¸¸æˆä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLM-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªå®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒï¼Œå…¶ä¸­åŒ…å«å…·æœ‰ç»Ÿä¸€æ¥å£å’Œå¯è°ƒã€ç»„åˆéš¾åº¦çš„å„ç§è§†è§‰æ¸¸æˆï¼Œä¸“é—¨ä¸ºå¯æ‰©å±•çš„å¤šæ¸¸æˆå¹¶è¡Œè®­ç»ƒè€Œè®¾è®¡ã€‚åˆ©ç”¨VLM-Gymï¼Œæˆ‘ä»¬ä½¿ç”¨çº¯RLé©±åŠ¨çš„è‡ªæˆ‘è¿›åŒ–è®­ç»ƒG0æ¨¡å‹ï¼Œå±•ç¤ºäº†æ–°å…´çš„æ„ŸçŸ¥å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¼“è§£ç”±æ¸¸æˆå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†G1æ¨¡å‹ã€‚G1åœ¨RLå¾®è°ƒä¹‹å‰åŠ å…¥äº†å¢å¼ºæ„ŸçŸ¥çš„å†·å¯åŠ¨é˜¶æ®µã€‚æˆ‘ä»¬çš„G1æ¨¡å‹åœ¨æ‰€æœ‰æ¸¸æˆä¸­éƒ½è¶…è¶Šäº†å…¶æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚Claude-3.7-Sonnet-Thinkingã€‚ç³»ç»Ÿåˆ†ææ­ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„å‘ç°ï¼šæ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’å¼•å¯¼ã€ç›¸äº’ä¿ƒè¿›ã€‚VLM-Gymå’ŒRLè®­ç»ƒçš„æºä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenllliang/G1%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E6%9C%AA%E6%9D%A5%E5%9C%A8%E5%B0%86VLMs%E5%8F%91%E5%B1%95%E4%B8%BA%E6%9C%89%E8%83%BD%E5%8A%9B%E4%BA%A4%E4%BA%92%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E6%96%B9%E9%9D%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E5%B7%A5%E4%BD%9C%E3%80%82">https://github.com/chenllliang/G1å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨å°†VLMså‘å±•ä¸ºæœ‰èƒ½åŠ›äº¤äº’çš„æ™ºèƒ½ä½“æ–¹é¢çš„ç ”ç©¶å·¥ä½œã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13426v1">PDF</a> 21 pages, 14 figures, code released at   <a target="_blank" rel="noopener" href="https://github.com/chenllliang/G1">https://github.com/chenllliang/G1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨äº¤äº’å¼ä¸°å¯Œè§†è§‰ç¯å¢ƒä¸­å†³ç­–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºVLM-Gymçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒã€‚é€šè¿‡åœ¨è¯¥ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œå®ç°äº†å¯¹VLMsçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„äº’ç›¸ä¿ƒè¿›æå‡ã€‚ä½œè€…è®­ç»ƒäº†G0æ¨¡å‹å¹¶å¼€å‘äº†G1æ¨¡å‹ä»¥åº”å¯¹ä¸åŒæ¸¸æˆçš„æŒ‘æˆ˜ï¼Œå…¶ä¸­G1æ¨¡å‹åœ¨æ„ŸçŸ¥èƒ½åŠ›ä¸Šè¿›è¡Œäº†å¢å¼ºå¹¶å®ç°äº†å¯¹è€å¸ˆçš„è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨ç›´æ¥çš„å¤šæ¨¡å¼ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰ä¸°å¯Œç¯å¢ƒä¸‹çš„å†³ç­–åˆ¶å®šä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚å­˜åœ¨æ‰€è°“çš„â€œçŸ¥é“åšä¸åˆ°â€çš„å·®è·ã€‚</li>
<li>VLM-Gymæ˜¯ä¸€ä¸ªä¸“ä¸ºè§£å†³æ­¤é—®é¢˜è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œæä¾›å¤šæ ·åŒ–çš„è§†è§‰æ¸¸æˆï¼Œå¹¶å…·å¤‡ç»Ÿä¸€çš„æ¥å£å’Œå¯è°ƒæ•´çš„ç»„åˆéš¾åº¦ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡çš„å¤šæ¸¸æˆå¹¶è¡Œè®­ç»ƒã€‚</li>
<li>G0æ¨¡å‹é€šè¿‡çº¯RLé©±åŠ¨çš„è‡ªæˆ‘è¿›åŒ–è¿›è¡Œè®­ç»ƒï¼Œå±•ç°å‡ºæ–°å…´çš„æ„ŸçŸ¥å’Œæ¨ç†æ¨¡å¼ã€‚</li>
<li>G1æ¨¡å‹åœ¨RLå¾®è°ƒå‰å¢åŠ äº†æ„ŸçŸ¥å¢å¼ºçš„å†·å¯åŠ¨é˜¶æ®µï¼Œæ€§èƒ½è¶…è¶Šäº†è€å¸ˆå’Œå…¶ä»–é¢†å…ˆæ¨¡å‹å¦‚Claude-3.7-Sonnet-Thinkingã€‚</li>
<li>ç³»ç»Ÿåˆ†æå‘ç°æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’æ¨åŠ¨æå‡ã€‚</li>
<li>ä½œè€…å…¬å¼€äº†åŒ…æ‹¬VLM-Gymå’ŒRLè®­ç»ƒçš„æºä»£ç ï¼Œä»¥æ¨åŠ¨æœªæ¥ç ”ç©¶åœ¨å°†VLMså‘å±•ä¸ºå…·æœ‰äº¤äº’èƒ½åŠ›çš„æ™ºèƒ½ä½“æ–¹é¢çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fb6ff395100bd61358c86776a7c2adcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9298c9f90402e4e8bb3669e497d91efc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7c459344316127e56a86ccda73a2dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6186d2428c2f1cc1ec6a7b45b32c9eb2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FEALLM-Advancing-Facial-Emotion-Analysis-in-Multimodal-Large-Language-Models-with-Emotional-Synergy-and-Reasoning"><a href="#FEALLM-Advancing-Facial-Emotion-Analysis-in-Multimodal-Large-Language-Models-with-Emotional-Synergy-and-Reasoning" class="headerlink" title="FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language   Models with Emotional Synergy and Reasoning"></a>FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language   Models with Emotional Synergy and Reasoning</h2><p><strong>Authors:Zhuozhao Hu, Kaishen Yuan, Xin Liu, Zitong Yu, Yuan Zong, Jingang Shi, Huanjing Yue, Jingyu Yang</strong></p>
<p>Facial Emotion Analysis (FEA) plays a crucial role in visual affective computing, aiming to infer a personâ€™s emotional state based on facial data. Scientifically, facial expressions (FEs) result from the coordinated movement of facial muscles, which can be decomposed into specific action units (AUs) that provide detailed emotional insights. However, traditional methods often struggle with limited interpretability, constrained generalization and reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have shown exceptional performance in various visual tasks, while they still face significant challenges in FEA due to the lack of specialized datasets and their inability to capture the intricate relationships between FEs and AUs. To address these issues, we introduce a novel FEA Instruction Dataset that provides accurate and aligned FE and AU descriptions and establishes causal reasoning relationships between them, followed by constructing a new benchmark, FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to capture more detailed facial information, enhancing its capability in FEA tasks. Our model demonstrates strong performance on FEABench and impressive generalization capability through zero-shot evaluation on various datasets, including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and effectiveness in FEA tasks. The dataset and code will be available at <a target="_blank" rel="noopener" href="https://github.com/953206211/FEALLM">https://github.com/953206211/FEALLM</a>. </p>
<blockquote>
<p>é¢éƒ¨è¡¨æƒ…åˆ†æï¼ˆFEAï¼‰åœ¨è§†è§‰æƒ…æ„Ÿè®¡ç®—ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå…¶ç›®çš„åœ¨äºæ ¹æ®é¢éƒ¨æ•°æ®æ¨æ–­ä¸€ä¸ªäººçš„æƒ…æ„ŸçŠ¶æ€ã€‚ä»ç§‘å­¦çš„è§’åº¦æ¥çœ‹ï¼Œé¢éƒ¨è¡¨æƒ…æ˜¯é¢éƒ¨è‚Œè‚‰åè°ƒè¿åŠ¨çš„ç»“æœï¼Œå¯ä»¥åˆ†è§£ä¸ºç‰¹å®šçš„åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ï¼Œä»è€Œæä¾›è¯¦ç»†çš„æƒ…æ„Ÿæ´å¯Ÿã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•åœ¨è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨FEAä¸­ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹ä¸“ä¸šæ•°æ®é›†å’Œæ— æ³•æ•æ‰é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒä¹‹é—´å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„é¢éƒ¨è¡¨æƒ…åˆ†ææŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›å‡†ç¡®å’Œå¯¹é½çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒæè¿°ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å› æœå…³ç³»ã€‚éšåï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å¹³å°FEABenchã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†FEALLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨æ•æ‰æ›´è¯¦ç»†çš„é¢éƒ¨ä¿¡æ¯ï¼Œæé«˜å…¶åœ¨é¢éƒ¨è¡¨æƒ…åˆ†æä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨FEABenchä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å„ç§æ•°æ®é›†ä¸Šé€šè¿‡é›¶æ ·æœ¬è¯„ä¼°å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬RAF-DBã€AffectNetã€BP4Då’ŒDISFAï¼Œè¯æ˜äº†å…¶åœ¨é¢éƒ¨è¡¨æƒ…åˆ†æä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/953206211/FEALLM">https://github.com/953206211/FEALLM</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13419v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>é¢éƒ¨æƒ…æ„Ÿåˆ†æï¼ˆFEAï¼‰åœ¨è§†è§‰æƒ…æ„Ÿè®¡ç®—ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œé€šè¿‡é¢éƒ¨æ•°æ®æ¨æ–­äººçš„æƒ…æ„ŸçŠ¶æ€ã€‚ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œæ¨ç†èƒ½åŠ›æœ‰é™çš„æŒ‘æˆ˜ã€‚è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨FEAä¸­ä»é¢ä¸´ç‰¹å®šæ•°æ®é›†ç¼ºä¹å’Œéš¾ä»¥æ•æ‰é¢éƒ¨è¡¨æƒ…ï¼ˆFEsï¼‰ä¸åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ä¹‹é—´å¤æ‚å…³ç³»çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†æ–°å‹FEAæŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›å‡†ç¡®å¯¹é½çš„FEå’ŒAUæè¿°ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å› æœå…³ç³»ã€‚æ­¤å¤–ï¼Œä»–ä»¬æ„å»ºäº†æ–°åŸºå‡†æµ‹è¯•FEABenchï¼Œå¹¶æå‡ºäº†FEALLMï¼Œä¸€ç§æ—¨åœ¨æ•æ‰æ›´å¤šç»†èŠ‚é¢éƒ¨ä¿¡æ¯çš„æ–°å‹MLLMæ¶æ„ã€‚è¯¥æ¨¡å‹åœ¨FEABenchä¸Šè¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†ä¸Šå±•ç¤ºäº†å…¶é›¶æ ·æœ¬è¯„ä¼°çš„å‡ºè‰²æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨FEAä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨æƒ…æ„Ÿåˆ†æï¼ˆFEAï¼‰åœ¨è§†è§‰æƒ…æ„Ÿè®¡ç®—ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œæ—¨åœ¨åŸºäºé¢éƒ¨æ•°æ®æ¨æ–­äººçš„æƒ…æ„ŸçŠ¶æ€ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†ææ–¹é¢å­˜åœ¨è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œæ¨ç†èƒ½åŠ›çš„å±€é™ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†æï¼ˆFEAï¼‰ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹ä¸“ç”¨æ•°æ®é›†å’Œæ•æ‰å¤æ‚å…³ç³»çš„èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>ä¸ºæ”¹å–„FEAï¼Œå¼•å…¥äº†æ–°å‹çš„FEAæŒ‡ä»¤æ•°æ®é›†ï¼Œæä¾›å‡†ç¡®çš„é¢éƒ¨è¡¨æƒ…ï¼ˆFEsï¼‰å’ŒåŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰æè¿°ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å› æœå…³ç³»ã€‚</li>
<li>å»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•FEABenchã€‚</li>
<li>æå‡ºäº†FEALLMï¼Œä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨æé«˜åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-927f8f88ba3523bc7da087f24df358bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf72b786d27e40d66f8592056978fe79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-717e8a18a3d6ada6d61fed5bbde2bb92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc32467b9d87e5770b4942ecb1169ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c94279218da334e770e450bd610a88eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AdaptThink-Reasoning-Models-Can-Learn-When-to-Think"><a href="#AdaptThink-Reasoning-Models-Can-Learn-When-to-Think" class="headerlink" title="AdaptThink: Reasoning Models Can Learn When to Think"></a>AdaptThink: Reasoning Models Can Learn When to Think</h2><p><strong>Authors:Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, Juanzi Li</strong></p>
<p>Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/THU-KEG/AdaptThink">https://github.com/THU-KEG/AdaptThink</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹é€šè¿‡é‡‡ç”¨äººç±»ç±»ä¼¼çš„æ·±åº¦æ€è€ƒï¼Œåœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå†—é•¿çš„æ€è€ƒè¿‡ç¨‹æå¤§åœ°å¢åŠ äº†æ¨ç†å¼€é”€ï¼Œä½¿æ•ˆç‡æˆä¸ºå…³é”®ç“¶é¢ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œå¯¹äºç›¸å¯¹ç®€å•çš„ä»»åŠ¡ï¼ŒNoThinkingï¼ˆæç¤ºæ¨ç†æ¨¡å‹è·³è¿‡æ€è€ƒå¹¶ç›´æ¥ç”Ÿæˆæœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼‰åœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†AdaptThinkï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºæ•™æˆæ¨ç†æ¨¡å‹æ ¹æ®é—®é¢˜éš¾åº¦è‡ªé€‚åº”é€‰æ‹©æœ€ä½³æ€è€ƒæ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼ŒAdaptThinkå…·æœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰çº¦æŸä¼˜åŒ–ç›®æ ‡ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ä¿æŒæ•´ä½“æ€§èƒ½çš„åŒæ—¶é€‰æ‹©NoThinkingï¼›ï¼ˆ2ï¼‰é‡è¦æ€§é‡‡æ ·ç­–ç•¥ï¼Œåœ¨ç­–ç•¥å†…è®­ç»ƒä¸­å¹³è¡¡æ€è€ƒå’ŒNoThinkingæ ·æœ¬ï¼Œä»è€Œå®ç°å†·å¯åŠ¨å¹¶å…è®¸æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ¢ç´¢å’Œåˆ©ç”¨ä¸¤ç§æ€è€ƒæ¨¡å¼ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒAdaptThinkæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ï¼Œå¹¶è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¸‰ä¸ªæ•°å­¦æ•°æ®é›†ä¸Šï¼ŒAdaptThinkå°†DeepSeek-R1-Distill-Qwen-1.5Bçš„å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†53%ï¼ŒåŒæ—¶æé«˜äº†å…¶å‡†ç¡®æ€§2.4%ï¼Œè¿™çªæ˜¾äº†è‡ªé€‚åº”é€‰æ‹©æ€è€ƒæ¨¡å¼åœ¨ä¼˜åŒ–æ¨ç†è´¨é‡å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/AdaptThink%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/THU-KEG/AdaptThinkä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13417v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåœ¨ç®€å•çš„ä»»åŠ¡ä¸Šé‡‡ç”¨NoThinkingç­–ç•¥ï¼Œä½¿æ¨ç†æ¨¡å‹è·³è¿‡æ€è€ƒè¿‡ç¨‹ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä»¥æé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºAdaptThinkçš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿æ¨ç†æ¨¡å‹èƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„éš¾æ˜“ç¨‹åº¦è‡ªé€‚åº”é€‰æ‹©æœ€ä½³æ€è€ƒæ¨¡å¼ã€‚AdaptThinkåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€æ˜¯é€šè¿‡çº¦æŸä¼˜åŒ–ç›®æ ‡é¼“åŠ±æ¨¡å‹åœ¨ç»´æŒæ€»ä½“æ€§èƒ½çš„åŒæ—¶é€‰æ‹©NoThinkingï¼›äºŒæ˜¯é‡‡ç”¨é‡è¦æ€§é‡‡æ ·ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡æ€è€ƒå’ŒNoThinkingæ ·æœ¬ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢å’Œåˆ©ç”¨ä¸¤ç§æ€è€ƒæ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒAdaptThinkåœ¨å‡å°‘æ¨ç†æˆæœ¬çš„åŒæ—¶æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‰ä¸ªæ•°å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoThinkingç­–ç•¥åœ¨ç®€å•ä»»åŠ¡ä¸Šèƒ½æé«˜æ¨ç†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œé€šè¿‡è·³è¿‡æ€è€ƒè¿‡ç¨‹ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚</li>
<li>AdaptThinkæ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ ¹æ®é—®é¢˜çš„éš¾æ˜“ç¨‹åº¦ä½¿æ¨ç†æ¨¡å‹è‡ªé€‚åº”é€‰æ‹©æœ€ä½³æ€è€ƒæ¨¡å¼ã€‚</li>
<li>AdaptThinkåŒ…å«çº¦æŸä¼˜åŒ–ç›®æ ‡å’Œé‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>çº¦æŸä¼˜åŒ–ç›®æ ‡é¼“åŠ±æ¨¡å‹åœ¨ç»´æŒæ€»ä½“æ€§èƒ½çš„åŒæ—¶é€‰æ‹©NoThinkingã€‚</li>
<li>é‡è¦æ€§é‡‡æ ·ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡æ€è€ƒå’ŒNoThinkingæ ·æœ¬ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢å’Œåˆ©ç”¨ä¸¤ç§æ€è€ƒæ¨¡å¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAdaptThinkåœ¨å‡å°‘æ¨ç†æˆæœ¬çš„åŒæ—¶æé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e0ea32d414cc42129b2e6e8c752d5325.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cf6f5bc757b1a6828b924b97411eddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-936719d0a9ad706ca02c4b4ed3451af4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0af905a3b45c8fd5d6d3aa6b53ebb746.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MR-Judge-Multimodal-Reasoner-as-a-Judge"><a href="#MR-Judge-Multimodal-Reasoner-as-a-Judge" class="headerlink" title="MR. Judge: Multimodal Reasoner as a Judge"></a>MR. Judge: Multimodal Reasoner as a Judge</h2><p><strong>Authors:Renjie Pi, Felix Bai, Qibin Chen, Simon Wang, Jiulong Shan, Kieran Liu, Meng Cao</strong></p>
<p>The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè¯„ä¼°æ³•å®˜çš„èŒƒå¼å·²ç»ä½œä¸ºRLHFå’Œæ¨ç†æ—¶é—´ç¼©æ”¾ä¸­çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå¤šæ¨¡æ€æ¨ç†æ³•å®˜ï¼ˆMR. Judgeï¼‰â€è¿™ä¸€èŒƒå¼ï¼Œæ—¨åœ¨èµ‹äºˆé€šç”¨MLLMæ³•å®˜å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥ä¸ºæ¯ä¸ªå›ç­”åˆ†é…åˆ†æ•°çš„æ–¹å¼ä¸åŒï¼Œæˆ‘ä»¬å°†åˆ¤æ–­è¿‡ç¨‹åˆ¶å®šä¸ºå—æ¨ç†å¯å‘çš„å¤šé¡¹é€‰æ‹©é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæ³•å®˜æ¨¡å‹é¦–å…ˆè¿›è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†ï¼Œæ¶µç›–å›ç­”çš„ä¸åŒæ–¹é¢ï¼Œå¹¶æœ€ç»ˆä»ä¸­é€‰æ‹©æœ€ä½³å›ç­”ã€‚è¿™ç§æ¨ç†è¿‡ç¨‹ä¸ä»…æé«˜äº†åˆ¤æ–­çš„å¯è§£é‡Šæ€§ï¼Œè¿˜å¤§å¤§æé«˜äº†MLLMæ³•å®˜çš„æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹ç¼ºä¹å¸¦åˆ†æ•°å›ç­”çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä»¥ä¸‹ç­–ç•¥æ¥å®ç°è‡ªåŠ¨æ ‡æ³¨ï¼š1ï¼‰åå‘å“åº”å€™é€‰åˆæˆï¼šä»ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†å¼€å§‹ï¼Œæˆ‘ä»¬å°†åŸå§‹å“åº”è§†ä¸ºæœ€ä½³å€™é€‰ï¼Œå¹¶æç¤ºMLLMç”Ÿæˆåˆç†çš„è´Ÿé¢å€™é€‰å¯¹è±¡ï¼›2ï¼‰åŸºäºæ–‡æœ¬çš„ç†ç”±æå–ï¼šæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªæ•°æ®åˆæˆç®¡é“ï¼Œç”¨äºä»åŸºäºæ–‡æœ¬çš„æ¨ç†æ¨¡å‹ä¸­æç‚¼æ¨ç†èƒ½åŠ›ï¼Œè¯¥èƒ½åŠ›è¢«ç”¨æ¥é€šè¿‡é¢„çƒ­ç›‘ç£å¾®è°ƒä½¿MLLMæ³•å®˜é‡æ–°è·å¾—å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MR. Judgeåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡æœ‰æ•ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„MR. Judge-7Båœ¨VL-RewardBenchä¸Šçš„è¡¨ç°è¶…è¿‡äº†GPT-4oçš„9.9%ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´ç¼©æ”¾æœŸé—´MM-Vetä¸Šçš„æ€§èƒ½æé«˜äº†é«˜è¾¾7.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè¯„ä»·è€…çš„èŒƒå¼åœ¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œæ¨ç†æ—¶é—´ç¼©æ”¾ä¸­çš„æœ‰æ•ˆæ€§ã€‚æå‡ºä¸€ç§åä¸ºâ€œå¤šæ¨¡æ€åˆ¤æ–­è€…â€ï¼ˆMR. Judgeï¼‰çš„èŒƒå¼ï¼Œé€šè¿‡å°†å…¶è½¬åŒ–ä¸ºæ¨ç†å¯å‘å¼çš„å¤šé€‰é¢˜é—®é¢˜ï¼Œèµ‹äºˆé€šç”¨MLLMsè¯„ä»·è€…å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†æé«˜è¯„ä¼°çš„é€æ˜åº¦å¹¶å¢å¼ºMLLMè¯„ä»·è€…çš„æ€§èƒ½ï¼Œå°†åˆ¤æ–­è¿‡ç¨‹è½¬åŒ–ä¸ºæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºè§£å†³ç¼ºä¹å¸¦åˆ†æ•°å“åº”çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªåŠ¨æ ‡æ³¨ç­–ç•¥ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†MR. Judgeçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè¯„ä»·è€…å·²æˆä¸ºæœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œå¤šæ¨¡æ€åˆ¤æ–­è€…â€ï¼ˆMR. Judgeï¼‰çš„èŒƒå¼ï¼Œå°†åˆ¤æ–­è¿‡ç¨‹è½¬åŒ–ä¸ºæ¨ç†å¯å‘å¼çš„å¤šé€‰é¢˜é—®é¢˜ï¼Œä»¥æé«˜é€šç”¨MLLMsè¯„ä»·è€…çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å°†åˆ¤æ–­è¿‡ç¨‹è½¬åŒ–ä¸ºæ·±æ€ç†Ÿè™‘çš„æ¨ç†ï¼Œä¸ä»…æé«˜äº†è¯„ä»·çš„é€æ˜åº¦ï¼Œè¿˜å¢å¼ºäº†MLLMè¯„ä»·è€…çš„æ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹ç¼ºä¹å¸¦åˆ†æ•°å“åº”çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªåŠ¨æ ‡æ³¨ç­–ç•¥ï¼ŒåŒ…æ‹¬åå‘å“åº”å€™é€‰åˆæˆå’ŒåŸºäºæ–‡æœ¬çš„ç†ç”±æå–æ–¹æ³•ã€‚</li>
<li>MR. Judge-7Båœ¨VL-RewardBenchä¸Šçš„è¡¨ç°ä¼˜äºGPT-4oè¾¾9.9%ã€‚</li>
<li>åœ¨æ¨ç†æ—¶é—´ç¼©æ”¾æ—¶ï¼ŒMR. Judgeèƒ½æé«˜MM-Vetçš„æ€§èƒ½ï¼Œæé«˜å¹…åº¦é«˜è¾¾7.7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1809fb6603c738bee2dc5c105dec5e04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e36a0cfef9d9b2ae0fc9bdfd994aa76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19169a84e543cd10a04e8de471ccc9b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f422a2127e996d925673863d8a00ebad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Advancing-Generalization-Across-a-Variety-of-Abstract-Visual-Reasoning-Tasks"><a href="#Advancing-Generalization-Across-a-Variety-of-Abstract-Visual-Reasoning-Tasks" class="headerlink" title="Advancing Generalization Across a Variety of Abstract Visual Reasoning   Tasks"></a>Advancing Generalization Across a Variety of Abstract Visual Reasoning   Tasks</h2><p><strong>Authors:MikoÅ‚aj MaÅ‚kiÅ„ski, Jacek MaÅ„dziuk</strong></p>
<p>The abstract visual reasoning (AVR) domain presents a diverse suite of analogy-based tasks devoted to studying model generalization. Recent years have brought dynamic progress in the field, particularly in i.i.d. scenarios, in which models are trained and evaluated on the same data distributions. Nevertheless, o.o.d. setups that assess model generalization to new test distributions remain challenging even for the most recent models. To advance generalization in AVR tasks, we present the Pathways of Normalized Group Convolution model (PoNG), a novel neural architecture that features group convolution, normalization, and a parallel design. We consider a wide set of AVR benchmarks, including Ravenâ€™s Progressive Matrices and visual analogy problems with both synthetic and real-world images. The experiments demonstrate strong generalization capabilities of the proposed model, which in several settings outperforms the existing literature methods. </p>
<blockquote>
<p>æŠ½è±¡è§†è§‰æ¨ç†ï¼ˆAVRï¼‰é¢†åŸŸæä¾›äº†ä¸€ç³»åˆ—åŸºäºç±»æ¯”çš„ä»»åŠ¡ï¼Œä¸“é—¨ç”¨äºç ”ç©¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿‘å¹´æ¥ï¼Œè¯¥é¢†åŸŸå–å¾—äº†åŠ¨æ€è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¯ä»¥åœ¨åŒä¸€æ•°æ®åˆ†å¸ƒä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œå¯¹äºæœ€æ–°æ¨¡å‹è€Œè¨€ï¼Œè¯„ä¼°å…¶åœ¨æ–°æµ‹è¯•åˆ†å¸ƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ç§æ³›åŒ–èƒ½åŠ›çš„æµ‹è¯•æ¶‰åŠä¸åŒåˆ†å¸ƒï¼ˆo.o.d.ï¼‰ã€‚ä¸ºäº†æ¨è¿›AVRä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†è·¯å¾„å½’ä¸€åŒ–ç»„å·ç§¯æ¨¡å‹ï¼ˆPoNGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå…·æœ‰ç»„å·ç§¯ã€å½’ä¸€åŒ–å’Œå¹¶è¡Œè®¾è®¡çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸€ç³»åˆ—AVRåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬Ravençš„è¿›æ­¥çŸ©é˜µå’ŒåŒ…å«åˆæˆå›¾åƒå’ŒçœŸå®ä¸–ç•Œå›¾åƒçš„è§†è§‰ç±»æ¯”é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºç°æœ‰æ–‡çŒ®ä¸­çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13391v1">PDF</a> Accepted to the 34th International Joint Conference on Artificial   Intelligence (IJCAI 2025)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸåœ¨æŠ½è±¡è§†è§‰æ¨ç†ï¼ˆAVRï¼‰é¢†åŸŸå‡ºç°äº†åŠ¨æ€è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œå¯¹äºæ¨¡å‹æ¨å¹¿åˆ°æ–°çš„æµ‹è¯•åˆ†å¸ƒï¼ˆo.o.d.ï¼‰è®¾ç½®ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºæ¨è¿›AVRä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œæå‡ºäº†Pathways of Normalized Group Convolutionæ¨¡å‹ï¼ˆPoNGï¼‰ï¼Œå®ƒé‡‡ç”¨åˆ†ç»„å·ç§¯ã€å½’ä¸€åŒ–å’Œå¹¶è¡Œè®¾è®¡çš„æ–°ç¥ç»ç½‘ç»œæ¶æ„ã€‚ç»è¿‡åœ¨å¹¿æ³›çš„AVRåŸºå‡†æµ‹è¯•é›†ï¼ˆåŒ…æ‹¬Ravençš„æ¸è¿›çŸ©é˜µå’Œè§†è§‰ç±»æ¯”é—®é¢˜ï¼ŒåŒ…æ‹¬åˆæˆå›¾åƒå’ŒçœŸå®ä¸–ç•Œå›¾åƒï¼‰ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºç°æœ‰æ–‡çŒ®æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVRé¢†åŸŸè¿‘å¹´æ¥åœ¨æ¨¡å‹æ³›åŒ–æ–¹é¢å–å¾—åŠ¨æ€è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨i.i.d.åœºæ™¯ã€‚</li>
<li>æ¨¡å‹åœ¨o.o.d.è®¾ç½®ä¸‹æ³›åŒ–è‡³æ–°æµ‹è¯•åˆ†å¸ƒä»å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„â€”â€”Pathways of Normalized Group Convolutionæ¨¡å‹ï¼ˆPoNGï¼‰ï¼Œèåˆäº†åˆ†ç»„å·ç§¯ã€å½’ä¸€åŒ–å’Œå¹¶è¡Œè®¾è®¡ã€‚</li>
<li>PoNGæ¨¡å‹åœ¨å¤šç§AVRåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜PoNGæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒPoNGæ¨¡å‹çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–‡çŒ®æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22fa090a5cac4695bc0aa6af43345e1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c59e6cea012bf2e447151e6b75b70c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8daf5bd441ef1453c2688e588e9144bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d986f05e92240ad2d5e070c0599dae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbbc5361ac2f217d870ac5e6de50efcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54b45cf6b6f758d83f15d8c174332bcc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Thinkless-LLM-Learns-When-to-Think"><a href="#Thinkless-LLM-Learns-When-to-Think" class="headerlink" title="Thinkless: LLM Learns When to Think"></a>Thinkless: LLM Learns When to Think</h2><p><strong>Authors:Gongfan Fang, Xinyin Ma, Xinchao Wang</strong></p>
<p>Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the modelâ€™s ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/VainF/Thinkless">https://github.com/VainF/Thinkless</a> </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæ‰©å±•çš„è¿é”æ¨ç†ï¼Œåœ¨éœ€è¦å¤æ‚é€»è¾‘æ¨æ–­çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹æ‰€æœ‰æŸ¥è¯¢éƒ½è¿›è¡Œç²¾ç»†æ¨ç†å¾€å¾€ä¼šå¯¼è‡´å¤§é‡çš„è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œç‰¹åˆ«æ˜¯å½“è®¸å¤šé—®é¢˜éƒ½æœ‰ç®€å•ç›´æ¥çš„è§£å†³æ–¹æ¡ˆæ—¶ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå¼€æ”¾æ€§çš„é—®é¢˜ï¼šLLMèƒ½å¦å­¦ä¼šä½•æ—¶æ€è€ƒï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Thinklessï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿèµ‹äºˆLLMæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”é€‰æ‹©çŸ­å½¢å¼å’Œé•¿å½¢å¼æ¨ç†çš„èƒ½åŠ›ã€‚Thinklessæ˜¯åœ¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼ä¸‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå¹¶é‡‡ç”¨äº†ä¸¤ä¸ªæ§åˆ¶ä»¤ç‰Œï¼š<short>ç”¨äºç®€æ´å›åº”ï¼Œ<think>ç”¨äºè¯¦ç»†æ¨ç†ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯è§£è€¦ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDeGRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†æ··åˆæ¨ç†çš„å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ§åˆ¶ä»¤ç‰ŒæŸå¤±ï¼Œå®ƒæ§åˆ¶æ¨ç†æ¨¡å¼çš„é€‰æ‹©ï¼›ï¼ˆ2ï¼‰å“åº”æŸå¤±ï¼Œå®ƒæé«˜ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚è¿™ç§è§£è€¦å…¬å¼ä½¿æ¯ä¸ªç›®æ ‡çš„è´¡çŒ®å®ç°ç²¾ç»†æ§åˆ¶ï¼Œç¨³å®šè®­ç»ƒï¼Œå¹¶æœ‰æ•ˆé˜²æ­¢äº†åŸå§‹GRPOä¸­è§‚å¯Ÿåˆ°çš„å´©æºƒç°è±¡ã€‚ä»å®è¯ä¸Šçœ‹ï¼Œåœ¨Minerva Algebraã€MATH-500å’ŒGSM8Kç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒThinklessèƒ½å¤Ÿå‡å°‘é•¿é“¾æ€è€ƒçš„ä½¿ç”¨ç‡è¾¾åˆ°50%-90%ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VainF/Thinkless%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/VainF/Thinklessæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13379v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šèƒ½è¿›è¡Œé“¾å¼æ€ç»´æ¨ç†çš„è¯­è¨€æ¨¡å‹å·²åœ¨å¤æ‚é€»è¾‘æ¨æ–­ä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹æ‰€æœ‰æŸ¥è¯¢è¿›è¡Œç²¾ç»†æ¨ç†ä¼šå¯¼è‡´å¤§é‡è®¡ç®—èµ„æºæµªè´¹ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¸å¤šé—®é¢˜å­˜åœ¨ç®€å•è§£å†³æ–¹æ¡ˆæ—¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†Thinklessæ¡†æ¶ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½è‡ªé€‚åº”é€‰æ‹©ç®€æ´æˆ–é•¿ç¯‡æ¨ç†æ¨¡å¼ï¼Œè¿™å–å†³äºä»»åŠ¡å¤æ‚æ€§å’Œæ¨¡å‹èƒ½åŠ›ã€‚Thinklessé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼è®­ç»ƒï¼Œä½¿ç”¨ä¸¤ä¸ªæ§åˆ¶ç¬¦å·<short>å’Œ<think>æ¥åˆ†åˆ«æ§åˆ¶ç®€çŸ­å›ç­”å’Œè¯¦ç»†æ¨ç†ã€‚å…¶æ ¸å¿ƒç®—æ³•ä¸ºè§£è€¦ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDeGRPOï¼‰ï¼Œå°†æ··åˆæ¨ç†çš„å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šæ§åˆ¶ç¬¦å·æŸå¤±å’Œå“åº”æŸå¤±ã€‚åœ¨Minerva Algebraã€MATH-500å’ŒGSM8Kç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šï¼ŒThinklesså°†é•¿ç¯‡æ¨ç†çš„ä½¿ç”¨é‡å‡å°‘äº†50%-90%ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é€»è¾‘æ¨æ–­ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¨é‡ç²¾ç»†æ¨ç†å¼•å‘è®¡ç®—èµ„æºæµªè´¹é—®é¢˜ã€‚</li>
<li>Thinklessæ¡†æ¶ä½¿è¯­è¨€æ¨¡å‹èƒ½è‡ªé€‚åº”é€‰æ‹©ç®€æ´æˆ–é•¿ç¯‡æ¨ç†æ¨¡å¼ã€‚</li>
<li>Thinklessé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼è®­ç»ƒï¼Œä½¿ç”¨<short>å’Œ<think>ä¸¤ä¸ªæ§åˆ¶ç¬¦å·ã€‚</li>
<li>è§£è€¦ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDeGRPOï¼‰ç®—æ³•å°†æ··åˆæ¨ç†çš„å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºæ§åˆ¶ç¬¦å·æŸå¤±å’Œå“åº”æŸå¤±ä¸¤éƒ¨åˆ†ã€‚</li>
<li>Thinklessæ¡†æ¶èƒ½æ˜¾è‘—å‡å°‘é•¿ç¯‡æ¨ç†çš„ä½¿ç”¨é‡ï¼Œæé«˜è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33aa4bcf4653260e5a0e7235af8c9221.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35c050fb0ce9a3ab8e6166d621f4af7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608781e2d2769d48b179ad8b99ab3c63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-604da8dfe31b0c8525b7ab0caf713e98.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Sense-and-Sensitivity-Examining-the-Influence-of-Semantic-Recall-on-Long-Context-Code-Reasoning"><a href="#Sense-and-Sensitivity-Examining-the-Influence-of-Semantic-Recall-on-Long-Context-Code-Reasoning" class="headerlink" title="Sense and Sensitivity: Examining the Influence of Semantic Recall on   Long Context Code Reasoning"></a>Sense and Sensitivity: Examining the Influence of Semantic Recall on   Long Context Code Reasoning</h2><p><strong>Authors:Adam Å torek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</strong></p>
<p>Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information. </p>
<blockquote>
<p>å°½ç®¡ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒæå¤§çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œä½†å®ƒä»¬åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡è¿›è¡Œä»£ç æ¨ç†çš„æœ‰æ•ˆæ€§ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡è°ƒæŸ¥äº†LLMåœ¨å¤§ä»“åº“ä¸­çš„ä»£ç ç‰‡æ®µæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠå…¶ä¸å›å¿†èƒ½åŠ›çš„å…³è”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹è¯æ±‡ä»£ç å›å¿†ï¼ˆé€å­—æ£€ç´¢ï¼‰å’Œè¯­ä¹‰ä»£ç å›å¿†ï¼ˆè®°ä½ä»£ç çš„åŠŸèƒ½ï¼‰è¿›è¡Œäº†åŒºåˆ†ã€‚ä¸ºäº†è¡¡é‡è¯­ä¹‰å›å¿†ï¼Œæˆ‘ä»¬æå‡ºäº†SemTraceè¿™ä¸€ä»£ç æ¨ç†æŠ€æœ¯ï¼Œå¯ä»¥å½’å› å’Œé¢„æµ‹ç‰¹å®šè¯­å¥å¯¹è¾“å‡ºçš„å½±å“ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é‡åŒ–ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¯­ä¹‰å›å¿†æ•æ„Ÿåº¦çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯¹æœ€æ–°LLMçš„è¯„ä¼°è¡¨æ˜ï¼Œéšç€ä»£ç ç‰‡æ®µæ¥è¿‘è¾“å…¥ä¸Šä¸‹æ–‡çš„ä¸­é—´éƒ¨åˆ†ï¼Œä»£ç æ¨ç†çš„å‡†ç¡®æ€§å¤§å¹…ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜åº¦è¯­ä¹‰å›å¿†çš„æŠ€æœ¯å¦‚SemTraceä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¯æ±‡å›å¿†çš„ç²’åº¦æœ‰æ‰€ä¸åŒï¼Œæ¨¡å‹åœ¨å‡½æ•°æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€è¡Œå›å¿†æ–¹é¢é‡åˆ°å›°éš¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯æ±‡å›å¿†å’Œè¯­ä¹‰å›å¿†ä¹‹é—´å­˜åœ¨æ–­å±‚ï¼Œè¡¨æ˜å­˜åœ¨ä¸åŒçš„åŸºç¡€æœºåˆ¶ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•å¯èƒ½è¡¨ç°å‡ºè¾ƒä½çš„è¯­ä¹‰å›å¿†æ•æ„Ÿæ€§ï¼Œå¯èƒ½ä¼šä½ä¼°LLMåœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13353v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç æ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šå¾…æ˜ç¡®ã€‚æœ¬æ–‡æ¢è®¨äº†LLMåœ¨å¤„ç†å¤§å‹ä»£ç åº“ä¸­çš„ä»£ç ç‰‡æ®µæ—¶çš„æ¨ç†èƒ½åŠ›åŠå…¶ä¸å¬å›èƒ½åŠ›çš„å…³è”ã€‚æ–‡ç« åŒºåˆ†äº†è¯æ±‡ä»£ç å¬å›ï¼ˆé€å­—æ£€ç´¢ï¼‰å’Œè¯­ä¹‰ä»£ç å¬å›ï¼ˆè®°ä½ä»£ç çš„åŠŸèƒ½ï¼‰ã€‚ä¸ºäº†è¡¡é‡è¯­ä¹‰å¬å›ï¼Œæˆ‘ä»¬æå‡ºäº†SemTraceè¿™ä¸€ä»£ç æ¨ç†æŠ€æœ¯ï¼Œå¯ä»¥è¿½æº¯ç‰¹å®šè¯­å¥å¯¹è¾“å‡ºçš„å½±å“ã€‚é€šè¿‡å¯¹ç°æœ‰åŸºå‡†æ¨¡å‹çš„è¯­ä¹‰å¬å›æ•æ„Ÿæ€§è¿›è¡Œé‡åŒ–è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å…ˆè¿›LLMçš„ä»£ç æ¨ç†å‡†ç¡®æ€§éšç€ä»£ç ç‰‡æ®µåœ¨è¾“å…¥ä¸Šä¸‹æ–‡ä¸­çš„ä½ç½®è€Œæ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜è¯­ä¹‰å¬å›çš„æŠ€æœ¯å¦‚SemTraceä¸­ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°è¯æ±‡å¬å›åœ¨ç²’åº¦ä¸Šæœ‰æ‰€ä¸åŒï¼Œæ¨¡å‹æ“…é•¿å‡½æ•°æ£€ç´¢ï¼Œä½†åœ¨é€è¡Œå¬å›æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æœ€åï¼Œç ”ç©¶å‘ç°è¯æ±‡å’Œè¯­ä¹‰å¬å›ä¹‹é—´å­˜åœ¨æ–­å±‚ï¼Œæç¤ºä¸¤è€…èƒŒåæœ‰ä¸åŒçš„æœºåˆ¶ã€‚æœ¬æ–‡çš„å‘ç°è¡¨æ˜ï¼Œå½“å‰ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•çš„è¯­ä¹‰å¬å›æ•æ„Ÿæ€§å¯èƒ½è¾ƒä½ï¼Œå¯èƒ½ä¼šä½ä¼°LLMåœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä»£ç æ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šå¾…æ˜ç¡®ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†LLMå¤„ç†å¤§å‹ä»£ç åº“ä¸­çš„ä»£ç ç‰‡æ®µæ—¶çš„æ¨ç†èƒ½åŠ›ï¼ŒåŠå…¶ä¸å¬å›èƒ½åŠ›çš„å…³ç³»ã€‚</li>
<li>åŒºåˆ†äº†è¯æ±‡ä»£ç å¬å›ï¼ˆé€å­—æ£€ç´¢ï¼‰å’Œè¯­ä¹‰ä»£ç å¬å›ï¼ˆç†è§£ä»£ç åŠŸèƒ½ï¼‰ã€‚</li>
<li>æå‡ºSemTraceè¿™ä¸€ä»£ç æ¨ç†æŠ€æœ¯æ¥è¡¡é‡è¯­ä¹‰å¬å›ã€‚</li>
<li>å…ˆè¿›LLMåœ¨ä»£ç æ¨ç†æ–¹é¢å­˜åœ¨å‡†ç¡®æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜è¯­ä¹‰å¬å›çš„æŠ€æœ¯ä¸­ã€‚</li>
<li>LLMåœ¨è¯æ±‡å¬å›çš„ç²’åº¦ä¸Šè¡¨ç°ä¸åŒï¼Œæ“…é•¿å‡½æ•°æ£€ç´¢ä½†é€è¡Œå¬å›å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d8350a705151525f12bec85cbdffcb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c51a82f8b477aa0475d656a8c1ac0fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-990fb901fd5afe98db09ada9a957fd2b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Preference-Optimization"><a href="#J4R-Learning-to-Judge-with-Equivalent-Initial-State-Group-Relative-Preference-Optimization" class="headerlink" title="J4R: Learning to Judge with Equivalent Initial State Group Relative   Preference Optimization"></a>J4R: Learning to Judge with Equivalent Initial State Group Relative   Preference Optimization</h2><p><strong>Authors:Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</strong></p>
<p>To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•æ­¥ä¼çš„åŠ å¿«ï¼Œæ¨¡å‹è¾“å‡ºè¯„ä¼°å·²ä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ã€‚åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­ï¼ŒLLMæœ¬èº«è¢«èµ‹äºˆè¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹è¾“å‡ºçš„ä»»åŠ¡ã€‚LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆå¼è¯„ä¼°å™¨ï¼Œæ“…é•¿è¯„ä¼°ç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼Œå¦‚èŠå¤©è´¨é‡ï¼Œä½†åœ¨éœ€è¦å¤§é‡æ¨ç†çš„é¢†åŸŸå´è¡¨ç°æŒ£æ‰ï¼Œå› ä¸ºè¿™äº›é¢†åŸŸçš„æ¨¡å‹å›åº”åŒ…å«æ›´å¤šå®è´¨æ€§å’Œå…·æœ‰æŒ‘æˆ˜çš„å†…å®¹ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰è¯„åˆ¤è€…çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒè¯„åˆ¤è€…ã€‚æˆ‘ä»¬åšå‡ºäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒè¯„åˆ¤è€…ï¼Œä»¥åº”å¯¹åœ¨æ›´å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­å‡ºç°çš„å®šä½åè§ã€‚ ï¼ˆ2ï¼‰æˆ‘ä»¬æ¨å‡ºäº†ReasoningJudgeBenchï¼Œä¸€ä¸ªå¯ä»¥åœ¨å¤šæ ·åŒ–æ¨ç†ç¯å¢ƒä¸­è¯„ä¼°è¯„åˆ¤è€…çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå¼¥è¡¥äº†å…ˆå‰å·¥ä½œæœªæ¶µç›–çš„é¢†åŸŸã€‚ ï¼ˆ3ï¼‰æˆ‘ä»¬è®­ç»ƒäº†ç”¨äºæ¨ç†çš„è¯„åˆ¤è€…ï¼ˆJ4Rï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨EIS-GRPOè®­ç»ƒçš„7Bè¯„åˆ¤è€…ï¼Œå…¶æ€§èƒ½ä¼˜äºGPT-4oå’Œä¸‹ä¸€ä¸ªæœ€ä½³å°å‹è¯„åˆ¤è€…6.7ï¼…å’Œ9ï¼…ï¼Œåœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ä¸æ›´å¤§çš„GRPOè®­ç»ƒè¯„åˆ¤è€…ç›¸åŒ¹é…ç”šè‡³æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13346v1">PDF</a> 25 pages, 4 figures, 6 tables. To be updated with links for   code&#x2F;benchmark</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°çš„è½¬å˜ï¼Œä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒLLMä½œä¸ºè¯„ä»·è€…çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰è¯„ä»·è€…åœ¨å¤æ‚é¢†åŸŸä¸­çš„çŸ­æ¿é—®é¢˜ã€‚ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šæå‡ºç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œå¢å¼ºè¯„ä»·è€…å¯¹ä½ç½®åå·®çš„ç¨³å¥æ€§ï¼›æ¨å‡ºReasoningJudgeBenchåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„æ¨ç†åœºæ™¯ï¼›è®­ç»ƒå‡ºJudge for Reasoningï¼ˆJ4Rï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥EIS-GRPOç®—æ³•è®­ç»ƒï¼Œæ€§èƒ½è¶…è¿‡GPT-4oå’Œå…¶ä»–å°å‹è¯„ä»·è€…æ¨¡å‹ï¼ŒåŒæ—¶åœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šè¡¨ç°å‡ºåŒ¹é…æˆ–è¶…è¶Šæ›´å¤§è§„æ¨¡GRPOè®­ç»ƒè¯„ä»·è€…çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†æ¨¡å‹è¾“å‡ºè¯„ä¼°ä»äººåŠ›è¯„ä¼°å‘è‡ªåŠ¨è¯„ä¼°çš„è½¬å˜ã€‚</li>
<li>LLMè‡ªèº«è¢«ç”¨æ¥è¯„ä¼°å’Œæ‰¹åˆ¤å…¶ä»–æ¨¡å‹è¾“å‡ºï¼Œå…¶ä¸­LLM-as-judgeæ¨¡å‹åœ¨ç®€å•é¢†åŸŸå¦‚èŠå¤©è´¨é‡è¯„ä»·ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦å¤§é‡æ¨ç†çš„é¢†åŸŸè¡¨ç°è¾ƒå·®ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«ç”¨äºæ”¹è¿›ç°æœ‰è¯„ä»·è€…çš„ä¸è¶³ï¼Œé€šè¿‡è®­ç»ƒå¢å¼ºè¯„ä»·è€…çš„ç¨³å¥æ€§ã€‚</li>
<li>æå‡ºäº†Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO)ç®—æ³•ï¼Œå¸®åŠ©è¯„ä»·è€…åº”å¯¹å¤æ‚ç¯å¢ƒä¸‹çš„ä½ç½®åå·®é—®é¢˜ã€‚</li>
<li>æ¨å‡ºäº†ReasoningJudgeBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°åœ¨å¤šæ ·åŒ–æ¨ç†åœºæ™¯ä¸­çš„è¯„ä»·è€…æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒäº†Judge for Reasoning (J4R) æ¨¡å‹ï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–å°å‹è¯„ä»·è€…æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c206c38f5dddb3041de5540b90c0f44b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93315927c8300339a387f090d719d795.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10c1657f1be23f37719d032298207ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99c75154d32ee65f20210787204ec610.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unlabeled-Data-or-Pre-trained-Model-Rethinking-Semi-Supervised-Learning-and-Pretrain-Finetuning"><a href="#Unlabeled-Data-or-Pre-trained-Model-Rethinking-Semi-Supervised-Learning-and-Pretrain-Finetuning" class="headerlink" title="Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning   and Pretrain-Finetuning"></a>Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning   and Pretrain-Finetuning</h2><p><strong>Authors:Song-Lin Li, Rui Zhu, Yu-Feng Li, Lan-Zhe Guo</strong></p>
<p>Semi-supervised learning (SSL) alleviates the cost of data labeling process by exploiting unlabeled data, and has achieved promising results on various tasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm has garnered significant attention in recent years, and exploiting pre-trained models could also reduce the requirement of labeled data in downstream tasks. Therefore, a question naturally occurs: \emph{When the labeled data is scarce in the target tasks, should we exploit unlabeled data or pre-trained models?} To answer this question, we select pre-trained Vision-Language Models (VLMs) as representative pretrain-finetuning instances and propose \textit{Few-shot SSL} â€“ a framework that enables fair comparison between these two paradigms by controlling the amount of labeled data used. Extensive experiments across various settings demonstrate that pre-trained VLMs generally outperform SSL methods in nearly all cases, except when the data has low resolution or lacks clear semantic structure. Therefore, we encourage future SSL research to compare with pre-trained models and explore deeper integration, such as using pre-trained knowledge to enhance pseudo-labeling. To support future research, we release our unified reproduction and evaluation framework. Codes are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566">https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566</a> </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é€šè¿‡åˆ©ç”¨æœªæ ‡è®°æ•°æ®å‡è½»äº†æ•°æ®æ ‡è®°è¿‡ç¨‹çš„æˆæœ¬ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ç­‰å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚åŒæ—¶ï¼Œè¿‘å¹´æ¥é¢„è®­ç»ƒå¾®è°ƒèŒƒå¼ä¹Ÿå¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¹Ÿå¯ä»¥å‡å°‘ä¸‹æ¸¸ä»»åŠ¡ä¸­å¯¹æ ‡è®°æ•°æ®çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œä¸€ä¸ªè‡ªç„¶è€Œç„¶çš„é—®é¢˜æ˜¯ï¼šå½“ç›®æ ‡ä»»åŠ¡çš„æ ‡è®°æ•°æ®ç¨€ç¼ºæ—¶ï¼Œæˆ‘ä»¬åº”è¯¥åˆ©ç”¨æœªæ ‡è®°æ•°æ®è¿˜æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€‰æ‹©é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºä»£è¡¨æ€§çš„é¢„è®­ç»ƒå¾®è°ƒå®ä¾‹ï¼Œå¹¶æå‡ºäº†â€œFew-shot SSLâ€æ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶ä½¿ç”¨çš„æ ‡è®°æ•°æ®é‡ï¼Œä½¿è¿™ä¸¤ç§èŒƒå¼ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚åœ¨å¤šç§è®¾ç½®çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œé™¤æ•°æ®åˆ†è¾¨ç‡ä½æˆ–ç¼ºä¹æ˜ç¡®è¯­ä¹‰ç»“æ„çš„æƒ…å†µå¤–ï¼Œé¢„è®­ç»ƒçš„VLMé€šå¸¸åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½ä¼˜äºSSLæ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±æœªæ¥çš„SSLç ”ç©¶ä¸é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæ¢ç´¢æ›´æ·±å…¥çš„é›†æˆï¼Œä¾‹å¦‚åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æ¥æé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚ä¸ºäº†æ”¯æŒæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ç»Ÿä¸€çš„å¤åˆ¶å’Œè¯„ä¼°æ¡†æ¶ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566">https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13317v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å¯ä»¥åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®é™ä½æ•°æ®æ ‡æ³¨æˆæœ¬å¹¶åœ¨å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸Šå–å¾—æœ‰å‰æ™¯çš„ç»“æœçš„åŒæ—¶ï¼Œé¢„è®­ç»ƒå¾®è°ƒï¼ˆPretrain-Finetuningï¼‰èŒƒå¼è¿‘å¹´æ¥ä¹Ÿå¤‡å—å…³æ³¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é™ä½ä¸‹æ¸¸ä»»åŠ¡å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚å½“ç›®æ ‡ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºæ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Ÿä¸ºå›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»¥é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸ºä¾‹å­ï¼Œæå‡ºäº†â€œå°æ ·æœ¬SSLâ€æ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶ä½¿ç”¨çš„æ ‡æ³¨æ•°æ®é‡ï¼Œå…¬å¹³åœ°æ¯”è¾ƒè¿™ä¸¤ç§èŒƒå¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé™¤äº†æ•°æ®åˆ†è¾¨ç‡ä½æˆ–è¯­ä¹‰ç»“æ„ä¸æ˜ç¡®çš„æƒ…å†µå¤–ï¼Œé¢„è®­ç»ƒçš„VLMsåœ¨å‡ ä¹æ‰€æœ‰æƒ…å†µä¸‹éƒ½ä¼˜äºSSLæ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±æœªæ¥çš„SSLç ”ç©¶è¦ä¸é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæ¢ç´¢æ›´æ·±å…¥çš„é›†æˆï¼Œå¦‚åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†å¢å¼ºä¼ªæ ‡ç­¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰èƒ½å¤Ÿåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®é™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
<li>é¢„è®­ç»ƒå¾®è°ƒï¼ˆPretrain-Finetuningï¼‰èŒƒå¼å¤‡å—å…³æ³¨ï¼Œå¯é™ä½ä¸‹æ¸¸ä»»åŠ¡å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>åœ¨ç›®æ ‡ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºæ—¶ï¼Œåº”æ¢è®¨å¦‚ä½•å¹³è¡¡åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§è®¾ç½®ä¸‹çš„å®éªŒè¡¨ç°æ™®éä¼˜äºSSLæ–¹æ³•ï¼Œé™¤éæ•°æ®åˆ†è¾¨ç‡ä½æˆ–è¯­ä¹‰ç»“æ„ä¸æ˜ç¡®ã€‚</li>
<li>æœªæ¥çš„SSLç ”ç©¶éœ€è¦æ›´å¤šåœ°ä¸é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¯”è¾ƒå’Œé›†æˆã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†å¢å¼ºä¼ªæ ‡ç­¾æ˜¯ä¸€ç§æ½œåœ¨çš„æ·±å…¥é›†æˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b4be752361475c84af977735786803bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0de7c6aca41e2bb5d45c5466bbf86d4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a0b009fbc59179619f5bf13e0d0220.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5be6b3568948e3da01f70d05a957412.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b78307b96048f8d297d107b0dcbed03.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Seek-in-the-Dark-Reasoning-via-Test-Time-Instance-Level-Policy-Gradient-in-Latent-Space"><a href="#Seek-in-the-Dark-Reasoning-via-Test-Time-Instance-Level-Policy-Gradient-in-Latent-Space" class="headerlink" title="Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient   in Latent Space"></a>Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient   in Latent Space</h2><p><strong>Authors:Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, Zilong Zheng</strong></p>
<p>Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the modelâ€™s latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs. </p>
<blockquote>
<p>äººç±»çš„æ¨ç†èƒ½åŠ›ä½œä¸ºäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®ç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿‡ç¨‹ä¸­ä»ç„¶æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æ¨¡å‹æ€§èƒ½åœ¨è®­ç»ƒè§„æ¨¡æ•ˆåº”ä¸‹æœ‰æ‰€æå‡ï¼Œä½†ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒç®—æ³•æ–¹é¢ï¼Œå¦‚ç¾éš¾æ€§é—å¿˜å’Œæ–°å‹è®­ç»ƒæ•°æ®çš„æœ‰é™å¯ç”¨æ€§ã€‚ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œæµ‹è¯•æ—¶ç¼©æ”¾é€šè¿‡åœ¨æµ‹è¯•æ—¶å¢åŠ è®¡ç®—é‡è€Œä¸æ›´æ–°å‚æ•°æ¥æé«˜æ¨ç†æ€§èƒ½ã€‚ä¸åŒäºè¯¥èŒƒå¼ä¸­ä»¥å‰å…³æ³¨äºç¬¦å·ç©ºé—´çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ›´æœ‰æ•ˆçš„æ¨ç†ï¼Œå¹¶æ›´å¥½åœ°éµå¾ªæµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹ã€‚æˆ‘ä»¬ä»‹ç»äº†LatentSeekï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ¨¡å‹æ½œåœ¨ç©ºé—´å†…çš„æµ‹è¯•æ—¶å®ä¾‹çº§é€‚åº”ï¼ˆTTIAï¼‰æ¥æé«˜LLMæ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒLatentSeekåˆ©ç”¨ç­–ç•¥æ¢¯åº¦æ¥è¿­ä»£æ›´æ–°æ½œåœ¨è¡¨ç¤ºï¼Œç”±è‡ªæˆ‘ç”Ÿæˆçš„å¥–åŠ±ä¿¡å·å¼•å¯¼ã€‚LatentSeekåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬GSM8Kã€MATH-50 æ‰©å±•åˆ°ä¸­æ–‡ç•¥ï¼ˆå¾…è¡¥å……ï¼‰-å¾…å®¡é˜…è¯¥éƒ¨åˆ†æ˜¯å¦å¯ä»¥åˆ å»æˆ–å‡å°‘è‡³è¶³å¤Ÿç²¾ç®€çš„è¡¨è¿°ç­‰æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒLatentSeekæŒç»­è¶…è¶Šå¼ºå¤§çš„åŸºçº¿ï¼Œå¦‚æ€ç»´é“¾æç¤ºå’Œå¾®è°ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLatentSeekéå¸¸é«˜æ•ˆï¼Œé€šå¸¸åœ¨è§£å†³å¹³å‡å¤æ‚åº¦é—®é¢˜çš„å‡ ä¸ªè¿­ä»£å†…æ”¶æ•›ï¼Œå¹¶ä¸”ä»é¢å¤–çš„è¿­ä»£ä¸­å—ç›Šï¼Œä»è€Œçªå‡ºäº†æ½œåœ¨ç©ºé—´ä¸­æµ‹è¯•æ—¶ç¼©æ”¾æ³•çš„æ½œåŠ›ã€‚è¿™äº›å‘ç°ä½¿LatentSeekæˆä¸ºæé«˜LLMæ¨ç†èƒ½åŠ›çš„è½»ä¾¿ã€å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿½æ±‚äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿‡ç¨‹ä¸­é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚ä¸ºäº†æå‡LLMçš„æ¨ç†æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•â€”â€”LatentSeekæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æµ‹è¯•æ—¶é—´å®ä¾‹çº§é€‚åº”ï¼ˆTTIAï¼‰åœ¨æ¨¡å‹çš„æ½œåœ¨ç©ºé—´å†…å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentSeekåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬GSM8Kã€MATH-500å’ŒAIME2024ç­‰å¤šä¸ªæ¶æ„çš„LLMã€‚å…¶ä¼˜åŠ¿åœ¨äºé«˜æ•ˆä¸”å¯ç¼©æ”¾ï¼Œæœ‰æœ›æˆä¸ºæå‡LLMæ¨ç†èƒ½åŠ›çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿½æ±‚äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿‡ç¨‹ä¸­ä»é¢ä¸´æ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>å°½ç®¡æ¨¡å‹æ€§èƒ½å·²æœ‰æ‰€æå‡ï¼Œä½†ä»å­˜åœ¨è®­ç»ƒç®—æ³•çš„æŒ‘æˆ˜ï¼Œå¦‚ç¾éš¾æ€§é—å¿˜å’Œæ–°å‹è®­ç»ƒæ•°æ®çš„æœ‰é™æ€§ã€‚</li>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾é€šè¿‡å¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—è€Œä¸æ›´æ–°å‚æ•°æ¥æé«˜æ¨ç†æ€§èƒ½ã€‚</li>
<li>LatentSeekæ¡†æ¶åˆ©ç”¨æ½œåœ¨ç©ºé—´å†…çš„æµ‹è¯•æ—¶é—´å®ä¾‹çº§é€‚åº”ï¼ˆTTIAï¼‰å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LatentSeekä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ¥è¿­ä»£æ›´æ–°æ½œåœ¨è¡¨ç¤ºï¼Œç”±è‡ªæˆ‘ç”Ÿæˆçš„å¥–åŠ±ä¿¡å·å¼•å¯¼ã€‚</li>
<li>LatentSeekåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚GSM8Kã€MATH-500å’ŒAIME2024ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4aa34f1faaa5a92c0b2fbac05abd1945.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5ba93b56dac8ff32d6b907c3ec97652.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c9b9a8208488d4f67c5d13da20f595.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c84b54e0b5a387711a6da711f3e97d46.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RBF-Quantifying-and-Optimizing-Reasoning-Boundaries-across-Measurable-and-Unmeasurable-Capabilities-for-Chain-of-Thought-Reasoning"><a href="#RBF-Quantifying-and-Optimizing-Reasoning-Boundaries-across-Measurable-and-Unmeasurable-Capabilities-for-Chain-of-Thought-Reasoning" class="headerlink" title="RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable   and Unmeasurable Capabilities for Chain-of-Thought Reasoning"></a>RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable   and Unmeasurable Capabilities for Chain-of-Thought Reasoning</h2><p><strong>Authors:Qiguang Chen, Libo Qin, Jinhao Liu, Yue Liao, Jiaqi Wang, Jingxuan Zhou, Wanxiang Che</strong></p>
<p>Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/LightChen233/reasoning-boundary">https://github.com/LightChen233/reasoning-boundary</a>. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚ä»»åŠ¡æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æˆæ•ˆï¼Œè¿™å¼•å‘äº†å¯¹å…¶åº•å±‚æœºåˆ¶çš„ç ”ç©¶çƒ­æ½®ã€‚ç„¶è€Œï¼Œå¯¹äºçœŸå®ä¸–ç•Œçš„åº”ç”¨æ¥è¯´ï¼Œè¿˜å­˜åœ¨ä¸¤ä¸ªä¸»è¦çš„æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹å®šé‡æŒ‡æ ‡å’Œå¯æ“ä½œçš„æŒ‡å—æ¥è¯„ä¼°å’Œä¼˜åŒ–å¯è¡¡é‡çš„CoTèƒ½åŠ›è¾¹ç•Œï¼›äºŒæ˜¯ç¼ºä¹è¯„ä¼°ä¸å¯æµ‹é‡çš„CoTèƒ½åŠ›è¾¹ç•Œçš„æ–¹æ³•ï¼Œå¦‚å¤šæ¨¡æ€æ„ŸçŸ¥ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†Reasoning Boundary Framework++ï¼ˆRBF++ï¼‰ã€‚é’ˆå¯¹ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†æ¨ç†è¾¹ç•Œï¼ˆRBï¼‰å®šä¹‰ä¸ºCoTæ€§èƒ½çš„æœ€å¤§æé™ï¼Œå¹¶æå‡ºäº†ä¸€ç§RBç»„åˆå®šå¾‹ï¼Œèƒ½å¤Ÿåœ¨å„ç§CoTä»»åŠ¡ä¸­è¿›è¡Œå®šé‡åˆ†æå¹¶æä¾›å¯æ“ä½œçš„æŒ‡å¯¼ã€‚å¯¹äºç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ’å®šå‡è®¾ï¼Œç”¨åœºæ™¯ç‰¹å®šçš„å¸¸æ•°æ¥æ›¿ä»£ä¸å¯æµ‹é‡çš„RBsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¨ç†è¾¹ç•Œåˆ’åˆ†æœºåˆ¶ï¼Œå°†ä¸å¯æµ‹é‡çš„RBsåˆ†ä¸ºä¸¤ä¸ªå­è¾¹ç•Œï¼Œä¾¿äºé‡åŒ–å’Œä¼˜åŒ–ä¸å¯æµ‹é‡çš„é¢†åŸŸçŸ¥è¯†å’Œå¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹38ä¸ªæ¨¡å‹è¿›è¡Œäº†æ¶‰åŠ13é¡¹ä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨è·¨æ¨¡æ€è®¾ç½®ä¸­çš„å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†10ç§CoTç­–ç•¥ï¼Œä»ä¸¤ä¸ªäº’è¡¥çš„è§’åº¦ä¸ºä¼˜åŒ–å’Œè¡°é€€æä¾›äº†è§è§£ï¼Œå¹¶æ‰©å±•äº†ç”¨äºæµ‹é‡LLMæ¨ç†ä¸­RBçš„è¯„ä»·åŸºå‡†ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¨åŠ¨å¯¹LLMsä¸­çš„RBå’Œä¼˜åŒ–ç­–ç•¥çš„ç†è§£ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/LightChen233/reasoning-boundary%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/LightChen233/reasoning-boundaryè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13307v1">PDF</a> Manuscript</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½å¢å¼ºã€‚é’ˆå¯¹ç°å®åº”ç”¨ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜â€”â€”ç¼ºä¹å®šé‡æŒ‡æ ‡å’Œå¯æ“ä½œçš„æŒ‡å—æ¥è¯„ä¼°å’Œä¼˜åŒ–å¯æµ‹é‡çš„CoTèƒ½åŠ›è¾¹ç•Œï¼Œä»¥åŠæ— æ³•è¯„ä¼°ä¸å¯æµ‹é‡çš„CoTèƒ½åŠ›è¾¹ç•Œï¼ˆå¦‚å¤šæ¨¡æ€æ„ŸçŸ¥ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†Reasoning Boundary Framework++ï¼ˆRBF++ï¼‰ã€‚é€šè¿‡å®šä¹‰æ¨ç†è¾¹ç•Œï¼ˆRBï¼‰å’Œç»„åˆå®šå¾‹æ¥è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸ºå„ç§CoTä»»åŠ¡æä¾›å®šé‡åˆ†æå’Œå¯æ“ä½œæŒ‡å¯¼ã€‚å¯¹äºç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œé€šè¿‡æ’å®šå‡è®¾æ›¿æ¢ä¸å¯æµ‹é‡çš„RBsï¼Œå¹¶æå‡ºæ¨ç†è¾¹ç•Œåˆ’åˆ†æœºåˆ¶ï¼Œå°†ä¸å¯æµ‹é‡çš„RBsåˆ†ä¸ºä¸¤ä¸ªå­è¾¹ç•Œï¼Œä»è€Œé‡åŒ–å¹¶ä¼˜åŒ–ä¸å¯æµ‹é‡çš„é¢†åŸŸçŸ¥è¯†å’Œå¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨è·¨æ¨¡æ€è®¾ç½®ä¸­çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Thought (CoT) æ¨ç†å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹è¯„ä¼°å’Œä¼˜åŒ–CoTèƒ½åŠ›è¾¹ç•Œçš„å®šé‡æŒ‡æ ‡å’Œå¯æ“ä½œæ–¹æ³•ï¼Œä»¥åŠæ— æ³•è¯„ä¼°ä¸å¯æµ‹é‡çš„CoTèƒ½åŠ›è¾¹ç•Œï¼ˆå¦‚å¤šæ¨¡æ€æ„ŸçŸ¥ï¼‰ã€‚</li>
<li>å¼•å…¥Reasoning Boundary Framework++ï¼ˆRBF++ï¼‰æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>å®šä¹‰æ¨ç†è¾¹ç•Œï¼ˆRBï¼‰ä½œä¸ºCoTæ€§èƒ½çš„æœ€å¤§é™åˆ¶ï¼Œå¹¶æå‡ºé’ˆå¯¹å„ç§CoTä»»åŠ¡çš„ç»„åˆå®šå¾‹ã€‚</li>
<li>é€šè¿‡æ’å®šå‡è®¾å¤„ç†ä¸å¯æµ‹é‡çš„RBsï¼Œå¹¶åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­å¼•å…¥æ¨ç†è¾¹ç•Œåˆ’åˆ†æœºåˆ¶ã€‚</li>
<li>å¹¿æ³›å®éªŒéªŒè¯äº†æ¡†æ¶åœ¨è·¨æ¨¡æ€è®¾ç½®ä¸­çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f28167c3df4913bf07519bd58bf08cf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-962bf0dd6351106d08cb5012592bab12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e829cfb5106baa66a1748fad131a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-115f03eee48896e0a91489a53980b5f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8da9d37835cf5cf635510fa7558b3096.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccdcf69cf4f4ccb9ff9567a5a9a96656.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Iâ€™ll-believe-it-when-I-see-it-Images-increase-misinformation-sharing-in-Vision-Language-Models"><a href="#Iâ€™ll-believe-it-when-I-see-it-Images-increase-misinformation-sharing-in-Vision-Language-Models" class="headerlink" title="Iâ€™ll believe it when I see it: Images increase misinformation sharing in   Vision-Language Models"></a>Iâ€™ll believe it when I see it: Images increase misinformation sharing in   Vision-Language Models</h2><p><strong>Authors:Alice Plebe, Timothy Douglas, Diana Riazi, R. Maria del Rio-Chanona</strong></p>
<p>Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its effect on vision-language models (VLMs) remains unclear. We present the first study examining how images influence VLMsâ€™ propensity to reshare news content, whether this effect varies across model families, and how persona conditioning and content attributes modulate this behavior. To support this analysis, we introduce two methodological contributions: a jailbreaking-inspired prompting strategy that elicits resharing decisions from VLMs while simulating users with antisocial traits and political alignments; and a multimodal dataset of fact-checked political news from PolitiFact, paired with corresponding images and ground-truth veracity labels. Experiments across model families reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Persona conditioning further modulates this effect: Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the tested models, only Claude-3-Haiku demonstrates robustness to visual misinformation. These findings highlight emerging risks in multimodal model behavior and motivate the development of tailored evaluation frameworks and mitigation strategies for personalized AI systems. Code and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/3lis/misinfo_vlm">https://github.com/3lis/misinfo_vlm</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–°é—»æ¨èç³»ç»Ÿä¸­çš„é›†æˆæ—¥ç›Šå¢å¤šï¼Œå¼•å‘äº†äººä»¬å¯¹å®ƒä»¬ä¼ æ’­é”™è¯¯ä¿¡æ¯ä½œç”¨çš„æ‹…å¿§ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œè§†è§‰å†…å®¹å¯ä»¥æå‡äººç±»å¯¹äºä¿¡æ¯çš„å¯ä¿¡åº¦å’Œå…±äº«æ€§ï¼Œä½†å…¶å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å½±å“ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬é¦–æ¬¡ç ”ç©¶äº†å›¾åƒå¦‚ä½•å½±å“VLMsé‡æ–°åˆ†äº«æ–°é—»å†…å®¹çš„å€¾å‘ï¼Œæ¢è®¨äº†è¿™ç§å½±å“åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­çš„å·®å¼‚ï¼Œä»¥åŠäººæ ¼è®¾å®šå’Œå†…å®¹å±æ€§å¦‚ä½•è°ƒèŠ‚è¿™ç§è¡Œä¸ºã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•è®ºä¸Šçš„è´¡çŒ®ï¼šä¸€ç§å—è¶Šç‹±å¯å‘è€Œè®¾è®¡çš„æç¤ºç­–ç•¥ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿå…·æœ‰åç¤¾ä¼šç‰¹è´¨å’Œæ”¿æ²»å€¾å‘çš„ç”¨æˆ·æ¥æ¿€å‘VLMsçš„é‡æ–°åˆ†äº«å†³ç­–ï¼›å¦ä¸€ä¸ªæ˜¯æ¥è‡ªPolitiFactçš„äº‹å®æ ¸æŸ¥æ”¿æ²»æ–°é—»å¤šåª’ä½“æ•°æ®é›†ï¼Œé…ä»¥ç›¸åº”çš„å›¾åƒå’ŒçœŸå®å‡†ç¡®æ€§æ ‡ç­¾ã€‚è·¨æ¨¡å‹å®¶æ—çš„è¯•éªŒè¡¨æ˜ï¼Œå›¾ç‰‡çš„å­˜åœ¨ä½¿çœŸå®æ–°é—»å’Œè™šå‡æ–°é—»çš„é‡æ–°åˆ†äº«ç‡åˆ†åˆ«æé«˜äº†4.8%å’Œ15.0%ã€‚äººæ ¼è®¾å®šè¿›ä¸€æ­¥è°ƒèŠ‚äº†è¿™ä¸€å½±å“ï¼šé»‘æš—ä¸‰é‡ç‰¹è´¨æ”¾å¤§äº†è™šå‡æ–°é—»çš„é‡æ–°åˆ†äº«ï¼Œè€Œå…±å’Œå…šå€¾å‘çš„æ¡£æ¡ˆåˆ™é™ä½äº†çœŸå®æ€§æ•æ„Ÿæ€§ã€‚åœ¨æµ‹è¯•çš„æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œåªæœ‰Claude-3-Haikuå¯¹è§†è§‰é”™è¯¯ä¿¡æ¯è¡¨ç°å‡ºç¨³å¥æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¤šæ¨¡æ€æ¨¡å‹è¡Œä¸ºçš„æ–°å…´é£é™©ï¼Œå¹¶æ¨åŠ¨äº†ä¸ºä¸ªæ€§åŒ–AIç³»ç»Ÿé‡èº«å®šåˆ¶è¯„ä¼°æ¡†æ¶å’Œç¼“è§£ç­–ç•¥çš„å‘å±•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/3lis/misinfo_vlm%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/3lis/misinfo_vlmä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–°é—»æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¼•å‘äººä»¬å¯¹ä¼ æ’­è¯¯å¯¼ä¿¡æ¯çš„æ‹…å¿§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†å›¾åƒå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é‡ä¼ æ–°é—»å†…å®¹å€¾å‘æ€§çš„å½±å“ï¼Œä»¥åŠä¸åŒæ¨¡å‹å®¶æ—ä¹‹é—´çš„å·®å¼‚å’Œä¸ªäººç‰¹è´¨å’Œå†…å®¹å±æ€§å¦‚ä½•å½±å“è¿™ç§è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œå›¾åƒçš„å­˜åœ¨ä½¿çœŸå®æ–°é—»çš„å†åˆ†äº«ç‡æé«˜äº†4.8%ï¼Œè€Œè¯¯å¯¼æ€§æ–°é—»çš„å†åˆ†äº«ç‡æé«˜äº†15%ã€‚æ­¤å¤–ï¼Œä¸ªäººç‰¹è´¨å¦‚é»‘æš—ä¸‰è§’ç‰¹å¾ä¼šåŠ å‰§è¯¯å¯¼æ€§æ–°é—»çš„å†åˆ†äº«ï¼Œè€Œå…±å’Œå…šå€¾å‘çš„è´¦æˆ·å¯¹çœŸå®æ€§ä¿¡æ¯è¾ƒä¸ºæ•æ„Ÿã€‚åœ¨æµ‹è¯•çš„æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œåªæœ‰Claude-3-Haikuå¯¹è§†è§‰è¯¯å¯¼ä¿¡æ¯è¡¨ç°å‡ºç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡å¼æ¨¡å‹è¡Œä¸ºçš„æ–°å…´é£é™©ï¼Œå¹¶å‘¼åä¸ºä¸ªæ€§åŒ–AIç³»ç»Ÿåˆ¶å®šé’ˆå¯¹æ€§çš„è¯„ä¼°æ¡†æ¶å’Œç¼“è§£ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–°é—»æ¨èç³»ç»Ÿä¸­çš„é›†æˆå¼•å‘å…³äºä¼ æ’­è¯¯å¯¼ä¿¡æ¯çš„æ‹…å¿§ã€‚</li>
<li>å›¾åƒå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é‡ä¼ æ–°é—»å†…å®¹çš„å½±å“å°šä¸æ¸…æ¥šã€‚</li>
<li>å›¾åƒå¯ä»¥æ˜¾è‘—æé«˜çœŸå®å’Œè¯¯å¯¼æ€§æ–°é—»çš„å†åˆ†äº«ç‡ã€‚</li>
<li>ä¸ªäººç‰¹è´¨ï¼ˆå¦‚é»‘æš—ä¸‰è§’ç‰¹å¾ï¼‰å¯èƒ½åŠ å‰§è¯¯å¯¼æ€§æ–°é—»çš„åˆ†äº«ã€‚</li>
<li>ä¸åŒæ¨¡å‹å®¶æ—åœ¨é‡ä¼ æ–°é—»å†…å®¹æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚</li>
<li>Claude-3-Haikuæ¨¡å‹å¯¹è§†è§‰è¯¯å¯¼ä¿¡æ¯å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4dcc7154d9db231cf03aa0450fab72f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b04afdf3553c2979cf41ac48e8a4af1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6975c286c3bdef9470c2ef6a82c3ab3b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CSC-SQL-Corrective-Self-Consistency-in-Text-to-SQL-via-Reinforcement-Learning"><a href="#CSC-SQL-Corrective-Self-Consistency-in-Text-to-SQL-via-Reinforcement-Learning" class="headerlink" title="CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement   Learning"></a>CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement   Learning</h2><p><strong>Authors:Lei Sheng, Shuai-Shuai Xu</strong></p>
<p>Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution accuracy, while the 7B model achieves 69.19%. The code will be open sourced at <a target="_blank" rel="noopener" href="https://github.com/CycloneBoy/csc_sql">https://github.com/CycloneBoy/csc_sql</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†å…³äºå…³ç³»æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSQLæŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæµ‹è¯•æ—¶çš„ç¼©æ”¾æŠ€æœ¯ï¼Œå¦‚è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSelf-Consistencyï¼‰å’Œè‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Correctionï¼‰ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬æ¥æé«˜SQLç”Ÿæˆå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¹Ÿæœ‰æ˜æ˜¾çš„å±€é™æ€§ï¼šè‡ªæˆ‘ä¸€è‡´æ€§å¯èƒ½ä¼šé€‰æ‹©æ¬¡ä¼˜è¾“å‡ºï¼Œå°½ç®¡å¾—åˆ°äº†å¤šæ•°æŠ•ç¥¨ï¼Œè€Œè‡ªæˆ‘ä¿®æ­£é€šå¸¸åªè§£å†³è¯­æ³•é”™è¯¯ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†CSC-SQLï¼Œä¸€ç§å°†è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘ä¿®æ­£ç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚CSC-SQLé€‰æ‹©å¹¶è¡Œé‡‡æ ·ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸¤ä¸ªè¾“å‡ºï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ä¿®è®¢æ¨¡å‹ä¸­è¿›è¡Œæ ¡æ­£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹SQLç”Ÿæˆå’Œä¿®è®¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¯å®äº†CSC-SQLçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚åœ¨BIRDå¼€å‘é›†ä¸Šï¼Œæˆ‘ä»¬çš„3Bæ¨¡å‹æ‰§è¡Œå‡†ç¡®æ€§è¾¾åˆ°65.28%ï¼Œè€Œ7Bæ¨¡å‹è¾¾åˆ°69.19%ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/CycloneBoy/csc_sql%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/CycloneBoy/csc_sqlå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13271v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSQLæŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯ï¼Œå¦‚è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSelf-Consistencyï¼‰å’Œè‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Correctionï¼‰ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å·¥ä½œé‡æ¥æé«˜SQLç”Ÿæˆå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æœ‰å±€é™æ€§ï¼šè‡ªæˆ‘ä¸€è‡´æ€§å¯èƒ½ä¼šé€‰æ‹©æ¬¡ä¼˜è¾“å‡ºï¼Œè€Œè‡ªæˆ‘ä¿®æ­£ä¸»è¦è§£å†³è¯­æ³•é”™è¯¯ã€‚ä¸ºäº†ç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•CSC-SQLï¼Œå®ƒç»“åˆäº†è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘ä¿®æ­£ã€‚CSC-SQLä»å¹¶è¡Œé‡‡æ ·ä¸­é€‰æ‹©å‡ºç°æœ€é¢‘ç¹çš„ä¸¤ä¸ªè¾“å‡ºï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ä¸€ä¸ªä¿®æ­£åˆå¹¶æ¨¡å‹ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹SQLç”Ÿæˆå’Œä¿®æ­£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜CSC-SQLçš„æœ‰æ•ˆæ€§åŠé€šç”¨æ€§ã€‚åœ¨BIRDå¼€å‘é›†ä¸Šï¼Œæˆ‘ä»¬çš„3Bæ¨¡å‹æ‰§è¡Œå‡†ç¡®æ€§è¾¾åˆ°65.28%ï¼Œè€Œ7Bæ¨¡å‹è¾¾åˆ°69.19%ã€‚ä»£ç å°†åœ¨CycloneBoy&#x2F;csc_sqlä¸Šå¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºSQLæŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯å¦‚è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSelf-Consistencyï¼‰å’Œè‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Correctionï¼‰èƒ½æé«˜SQLç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚<br>3 å•ç‹¬ä½¿ç”¨æ—¶è¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æå‡ºä¸€ç§é›†æˆä¸¤ç§æ–¹æ³•çš„æ–°ç­–ç•¥æ¥æé«˜æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†CSC-SQLæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ </li>
<li>CSC-SQLæ–¹æ³•é€‰æ‹©ä¸¤ä¸ªæœ€å¸¸è§çš„è¾“å‡ºå¹¶å¯¹å…¶è¿›è¡Œåˆå¹¶å’Œä¿®æ­£ä»¥æé«˜å‡†ç¡®æ€§ã€‚ </li>
<li>é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜è¾“å‡ºè´¨é‡ã€‚ </li>
<li>å®éªŒç»“æœè¡¨æ˜CSC-SQLçš„æœ‰æ•ˆæ€§åŠé€šç”¨æ€§ï¼Œåœ¨BIRDå¼€å‘é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e87389c124afab1d04038cfde70acd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d52982d288c2faacd9741fa9e9293297.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6758804ff880645646c8e510f9096893.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a702e535bcc2c20b10a98a638927eff4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Effective-and-Transparent-RAG-Adaptive-Reward-Reinforcement-Learning-for-Decision-Traceability"><a href="#Effective-and-Transparent-RAG-Adaptive-Reward-Reinforcement-Learning-for-Decision-Traceability" class="headerlink" title="Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning   for Decision Traceability"></a>Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning   for Decision Traceability</h2><p><strong>Authors:Jingyi Ren, Yekun Xu, Xiaolong Wang, Weitao Li, Weizhi Ma, Yang Liu</strong></p>
<p>Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generatorâ€™s (LLMâ€™s) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡RAGåœ¨ä¸åŒé¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†ä»å­˜åœ¨ä¸€äº›æœªè§£å†³çš„æŒ‘æˆ˜ï¼š1ï¼‰æœ‰æ•ˆæ€§ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¼€å‘æ›´å¼ºå¤§çš„RAGæ£€ç´¢å™¨ä¸Šï¼Œä½†å¦‚ä½•å¢å¼ºç”Ÿæˆå™¨ï¼ˆLLMï¼‰åˆ©ç”¨æ£€ç´¢ä¿¡æ¯è¿›è¡Œæ¨ç†å’Œç”Ÿæˆçš„èƒ½åŠ›ï¼Ÿ2ï¼‰é€æ˜åº¦ã€‚å¤§å¤šæ•°RAGæ–¹æ³•å¿½ç•¥äº†å“ªäº›æ£€ç´¢å†…å®¹å®é™…ä¸Šå¯¹æ¨ç†è¿‡ç¨‹æœ‰æ‰€è´¡çŒ®ï¼Œå¯¼è‡´ç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯è§æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ARENAï¼ˆè‡ªé€‚åº”å¥–åŠ±è¯æ®å¯¼èˆªä»£ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„é€æ˜RAGç”Ÿæˆå™¨æ¡†æ¶ï¼Œå¹¶æå‡ºäº†æˆ‘ä»¬çš„å¥–åŠ±ã€‚åŸºäºç»“æ„åŒ–ç”Ÿæˆå’Œè‡ªé€‚åº”å¥–åŠ±è®¡ç®—ï¼Œæˆ‘ä»¬çš„åŸºäºRLçš„è®­ç»ƒä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å…³é”®è¯æ®ï¼Œè¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œå¹¶ç”Ÿæˆå…·æœ‰å¯è§£é‡Šå†³ç­–è½¨è¿¹çš„ç­”æ¡ˆã€‚åº”ç”¨äºQwen2.5-7B-Instructå’ŒLlama3.1-8B-Instructï¼Œä¸å„ç§RAGåŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰å¤šè·³é—®ç­”æ•°æ®é›†ä¸Šå®ç°äº†10-30%çš„æ”¹è¿›ï¼Œå¯ä¸æœ€å…ˆè¿›çš„å•†ä¸šå¼€å‘LLMï¼ˆä¾‹å¦‚OpenAI-o1ã€DeepSeek-R1ï¼‰ç›¸åª²ç¾ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒARENAå…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§ï¼Œå¯ä»¥é€‚åº”æ–°çš„æ•°æ®é›†è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13258v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>RAGæŠ€æœ¯åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä»é¢ä¸´æœ‰æ•ˆæ€§å’Œé€æ˜åº¦æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºARENAæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å…³é”®è¯æ®ï¼Œè¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œå¹¶ç”Ÿæˆå…·æœ‰å¯è§£é‡Šå†³ç­–è½¨è¿¹çš„ç­”æ¡ˆã€‚åœ¨å¤šä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šï¼Œä¸å•†ä¸šé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒARENAå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RAGæŠ€æœ¯åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†ä»éœ€è§£å†³æœ‰æ•ˆæ€§å’Œé€æ˜åº¦é—®é¢˜ã€‚</li>
<li>ARENAæ¡†æ¶è¢«æå‡ºï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ARENAèƒ½å¤Ÿè¯†åˆ«å…³é”®è¯æ®ï¼Œè¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œå¹¶ç”Ÿæˆå…·æœ‰å¯è§£é‡Šå†³ç­–è½¨è¿¹çš„ç­”æ¡ˆã€‚</li>
<li>ä¸å¤šç§RAGåŸºå‡†æµ‹è¯•å’Œå¤šè·³é—®ç­”æ•°æ®é›†ç›¸æ¯”ï¼ŒARENAå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ARENAçš„æ€§èƒ½ä¸å•†ä¸šé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAI-o1ï¼ŒDeepSeek-R1ï¼‰ç›¸å½“ã€‚</li>
<li>ARENAå…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§ï¼Œå¯ä»¥é€‚åº”æ–°æ•°æ®é›†è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37dee88b59482cf88ca646d7d3f774a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f181a702b0584cbade512e6a0946d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-837420bce99cc2a873e93244e6612504.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237f79ab68d460250679c72a30b794b3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Adversarial-Testing-in-LLMs-Insights-into-Decision-Making-Vulnerabilities"><a href="#Adversarial-Testing-in-LLMs-Insights-into-Decision-Making-Vulnerabilities" class="headerlink" title="Adversarial Testing in LLMs: Insights into Decision-Making   Vulnerabilities"></a>Adversarial Testing in LLMs: Insights into Decision-Making   Vulnerabilities</h2><p><strong>Authors:Lili Zhang, Haomiaomiao Wang, Long Cheng, Libao Deng, Tomas Ward</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment. While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments. This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions. Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility. We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation. Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment. Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šèå…¥ç°å®ä¸–ç•Œçš„å†³ç­–ç³»ç»Ÿï¼Œç†è§£å…¶è¡Œä¸ºæ¼æ´å¯¹äºäººå·¥æ™ºèƒ½å®‰å…¨å’Œå¯¹é½æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨æ¨ç†å‡†ç¡®æ€§æˆ–äº‹å®æ­£ç¡®æ€§ï¼Œå´å¾€å¾€å¿½è§†LLMæ˜¯å¦èƒ½å¤Ÿå¯¹å¯¹æŠ—æ€§æ“ä½œä¿æŒç¨³å¥ï¼Œæˆ–åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¯¹æŠ—æ€§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹ç³»ç»Ÿåœ°å‹åŠ›æµ‹è¯•LLMçš„å†³ç­–è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†è®¤çŸ¥å¿ƒç†å­¦å’Œåšå¼ˆè®ºçš„æ–¹æ³•è®ºï¼Œæ¢è®¨æ¨¡å‹å¦‚ä½•å“åº”ä¸¤ç§å…¸å‹ä»»åŠ¡ï¼šåŒè‡‚åŒªå¾’ä»»åŠ¡å’Œå¤šè½®ä¿¡ä»»ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡æ•æ‰åˆ°äº†æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„æƒè¡¡ã€ç¤¾ä¼šåˆä½œå’Œæˆ˜ç•¥çµæ´»æ€§çš„å…³é”®æ–¹é¢ã€‚æˆ‘ä»¬å°†è¯¥æ¡†æ¶åº”ç”¨äºå‡ ç§æœ€å…ˆè¿›çš„LLMï¼ŒåŒ…æ‹¬GPT-3.5ã€GPT-4ã€åŒå­åº§-æ°æ–‡çŸ³æœ¨ã€æ·±åº¦å¯»æ±‚vé“ç§‘æ³›æ±‚ä¹åœŸå¯»è‡³ç»ˆå‘ˆä¼šç§˜ç‰ˆæœ¬å…±ç½‘äºŒå‰ä¸‰ä»£æ¼”ç¤ºæ“ä½œæ–¹æ³•ä¸æœ«åˆ¶æ£€ç´¢åŸŸç•¥é’šæ”¹è¿›å­¦è¨€ç®€è‰å‘å±•è¿¹è¡¨æ˜æœ¯æ¼”ç¤ºæ³•æŠ¤æœåŠ¡æ‚¦èªä½“ç³»çŠ¶æ€ç­‰é¢†åŸŸå…¸å‹çš„æœ‰æ³½å…ˆä¸¤å»ºæ¨¡ç»†åŠ¡å°†æ‚‰åº”ç°ä»£å†œå®¤è¿æˆ¿ä¸­çš„è¿­ä»£ä¸‹è´§æˆæœç‡ç‹¬ç‰¹å‘å¸ƒåŸåªå¼ºæ€€æˆ·è¿˜é›†ç®¡ç†å¤šå›å¼€å‘å¤šé¡µå…±è§ˆæ“ä½œè§„èŒƒä¹‹è¯­ç³»çš„ç­–ç•¥çµæ´»æ€§ä»¥åŠæ“çºµè¯±å¯¼ä¸‹å¯¹ç­–ç•¥çš„é€‚åº”æ€§æ–¹é¢å­˜åœ¨çš„ç‰¹å®šè„†å¼±æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸åŒæ¨¡å‹ä¹‹é—´çš„è¡Œä¸ºæ¨¡å¼å·®å¼‚ï¼Œå¹¶å¼ºè°ƒäº†é€‚åº”æ€§å’Œå…¬å¹³æ€§è®¤çŸ¥å¯¹äºå¯ä¿¡äººå·¥æ™ºèƒ½éƒ¨ç½²çš„é‡è¦æ€§ã€‚è¿™é¡¹å·¥ä½œå¹¶éæä¾›æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§è¯Šæ–­LLMä»£ç†å†³ç­–å¼±ç‚¹çš„æ–¹æ³•ï¼Œä¸ºå¯¹é½å’Œå®‰å…¨ç ”ç©¶æä¾›å¯æ“ä½œæ€§çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13195v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®å†³ç­–ç³»ç»Ÿä¸­çš„é›†æˆåº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¯¹å…¶è¡Œä¸ºè„†å¼±æ€§çš„ç†è§£å¯¹AIå®‰å…¨å’Œå¯¹é½è‡³å…³é‡è¦ã€‚ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨æ¨ç†å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§ï¼Œå´å¿½è§†äº†LLMså¯¹å¯¹æŠ—æ€§æ“çºµçš„ç¨³å¥æ€§å’ŒåŠ¨æ€ç¯å¢ƒä¸­ç­–ç•¥é€‚åº”èƒ½åŠ›ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªå¯¹æŠ—æ€§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°å¯¹LLMsçš„å†³ç­–è¿‡ç¨‹è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´è®¤çŸ¥å¿ƒç†å­¦å’Œåšå¼ˆè®ºçš„æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªå…¸å‹ä»»åŠ¡ï¼šåŒè‡‚åŒªå¾’ä»»åŠ¡å’Œå¤šè½®ä¿¡ä»»ä»»åŠ¡ï¼Œæ¢ç©¶æ¨¡å‹å¦‚ä½•åº”å¯¹æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„æƒè¡¡ã€ç¤¾ä¼šåˆä½œå’Œç­–ç•¥çµæ´»æ€§ã€‚åº”ç”¨äºå¤šä¸ªå…ˆè¿›LLMsçš„ç»“æœæ­ç¤ºäº†æ¨¡å‹ç‰¹å®šçš„æ˜“å—æ“çºµæ€§å’Œç­–ç•¥é€‚åº”çš„åƒµåŒ–æ€§ã€‚æœ¬æ–‡å¼ºè°ƒé€‚åº”æ€§å’Œå…¬å¹³æ€§è¯†åˆ«å¯¹äºAIå¯é éƒ¨ç½²çš„é‡è¦æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯Šæ–­LLMæ™ºèƒ½ä½“å†³ç­–ä¸­çš„å¼±ç‚¹ï¼Œä¸ºå¯¹é½å’Œå®‰å…¨ç ”ç©¶æä¾›å¯æ“ä½œæ€§çš„è§è§£ï¼Œè€Œéæä¾›æ€§èƒ½åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®å†³ç­–ç³»ç»Ÿä¸­çš„åº”ç”¨å¸¦æ¥äº†å¯¹å…¶è¡Œä¸ºè„†å¼±æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨æ¨ç†å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œé€‚åº”åŠ¨æ€ç¯å¢ƒçš„èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥çš„å¯¹æŠ—æ€§è¯„ä¼°æ¡†æ¶æ—¨åœ¨ç³»ç»Ÿæµ‹è¯•LLMsçš„å†³ç­–è¿‡ç¨‹ï¼Œå€Ÿé‰´äº†è®¤çŸ¥å¿ƒç†å­¦å’Œåšå¼ˆè®ºçš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªå…¸å‹ä»»åŠ¡ï¼šåŒè‡‚åŒªå¾’ä»»åŠ¡å’Œå¤šè½®ä¿¡ä»»ä»»åŠ¡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè€ƒå¯Ÿæ¨¡å‹çš„æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡ã€ç¤¾ä¼šåˆä½œå’Œç­–ç•¥çµæ´»æ€§ã€‚</li>
<li>å¯¹å¤šä¸ªå…ˆè¿›çš„LLMsåº”ç”¨è¯¥æ¡†æ¶åï¼Œå‘ç°æ¨¡å‹åœ¨åº”å¯¹æ“çºµå’Œç­–ç•¥é€‚åº”æ–¹é¢å­˜åœ¨ç‰¹å®šå¼±ç‚¹ã€‚</li>
<li>é€‚åº”æ€§å’Œå…¬å¹³æ€§è¯†åˆ«å¯¹äºAIçš„å¯é éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c8a6fb7fdbd26e97f6531a5c5c61e89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4232e7936412f2eec3ccef48c137f9b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d04ca6f21fa4b9ad5e8f0b58df5cd15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bf640cbd9b99b40a4cb785f236ec7f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b52ef5271041dcf2f4fe1466a39bafd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5596ff1cbeafe0d18a698e3db261c39.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Auditing-Meta-Cognitive-Hallucinations-in-Reasoning-Large-Language-Models"><a href="#Auditing-Meta-Cognitive-Hallucinations-in-Reasoning-Large-Language-Models" class="headerlink" title="Auditing Meta-Cognitive Hallucinations in Reasoning Large Language   Models"></a>Auditing Meta-Cognitive Hallucinations in Reasoning Large Language   Models</h2><p><strong>Authors:Haolang Lu, Yilian Liu, Jingxin Xu, Guoshun Nan, Yuanlong Yu, Zhican Chen, Kun Wang</strong></p>
<p>The development of Reasoning Large Language Models (RLLMs) has significantly improved multi-step reasoning capabilities, but it has also made hallucination problems more frequent and harder to eliminate. While existing approaches mitigate hallucinations through external knowledge integration, model parameter analysis, or self-verification, they often fail to capture how hallucinations emerge and evolve across the reasoning chain. In this work, we study the causality of hallucinations under constrained knowledge domains by auditing the Chain-of-Thought (CoT) trajectory and assessing the modelâ€™s cognitive confidence in potentially erroneous or biased claims. Our analysis reveals that in long-CoT settings, RLLMs can iteratively reinforce biases and errors through flawed reflective reasoning, eventually leading to hallucinated reasoning paths. Surprisingly, even direct interventions at the origin of hallucinations often fail to reverse their effects, as reasoning chains exhibit â€˜chain disloyaltyâ€™ â€“ a resistance to correction and a tendency to preserve flawed logic. Furthermore, we show that existing hallucination detection methods are less reliable and interpretable than previously assumed in complex reasoning scenarios. Unlike methods such as circuit tracing that require access to model internals, our black-box auditing approach supports interpretable long-chain hallucination attribution, offering better generalizability and practical utility. Code and data are available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/repo_for_meta_hallucination">https://anonymous.4open.science/r/repo_for_meta_hallucination</a> </p>
<blockquote>
<p>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰çš„å‘å±•æ˜¾è‘—æé«˜äº†å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼Œä½†ä¹Ÿä½¿è™šæ„é—®é¢˜æ›´åŠ é¢‘ç¹ä¸”éš¾ä»¥æ¶ˆé™¤ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šè¿‡å¤–éƒ¨çŸ¥è¯†æ•´åˆã€æ¨¡å‹å‚æ•°åˆ†ææˆ–è‡ªæˆ‘éªŒè¯æ¥ç¼“è§£è™šæ„ç°è±¡ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æ•æ‰è™šæ„å¦‚ä½•åœ¨æ¨ç†é“¾ä¸­æ¶Œç°å’Œæ¼”å˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®¡æ ¸æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹å¹¶è¯„ä¼°æ¨¡å‹å¯¹å¯èƒ½é”™è¯¯æˆ–åè§ä¸»å¼ çš„è®¤çŸ¥ä¿¡å¿ƒï¼Œæ¥ç ”ç©¶å—æ§çŸ¥è¯†åŸŸä¸‹è™šæ„çš„å› æœå…³ç³»ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨é•¿æœŸçš„CoTè®¾ç½®ä¸­ï¼ŒRLLMså¯ä»¥é€šè¿‡é”™è¯¯çš„åæ€æ¨ç†è¿­ä»£åœ°å¼ºåŒ–åè§å’Œé”™è¯¯ï¼Œæœ€ç»ˆå¯¼è‡´è™šæ„çš„æ¨ç†è·¯å¾„ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿åœ¨è™šæ„çš„åŸå§‹æ¥æºå¤„è¿›è¡Œç›´æ¥å¹²é¢„ä¹Ÿå¸¸å¸¸æ— æ³•é€†è½¬å…¶å½±å“ï¼Œå› ä¸ºæ¨ç†é“¾è¡¨ç°å‡ºâ€œé“¾ä¸å¿ â€â€”â€”ä¸€ç§å¯¹ä¿®æ­£çš„æŠµæŠ—åŠ›å’Œä¿æŒé”™è¯¯é€»è¾‘çš„å€¾å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œç°æœ‰çš„è™šæ„æ£€æµ‹æ–¹æ³•åœ¨å¤æ‚çš„æ¨ç†åœºæ™¯ä¸­å¹¶ä¸åƒä»¥å‰å‡è®¾çš„é‚£ä¹ˆå¯é å’Œå¯è§£é‡Šã€‚ä¸åŒäºéœ€è¦è®¿é—®æ¨¡å‹å†…éƒ¨çš„ç”µè·¯è·Ÿè¸ªç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„é»‘ç›’å®¡è®¡æ–¹æ³•æ”¯æŒå¯è§£é‡Šçš„é•¿é“¾è™šæ„å½’å› ï¼Œæä¾›æ›´å¥½çš„é€šç”¨æ€§å’Œå®ç”¨æ•ˆç”¨ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/repo_for_meta_hallucination%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/repo_for_meta_hallucinationè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13143v1">PDF</a> 33 pages (including references and appendix),11 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å‘å±•æ‰€å¸¦æ¥çš„å¹»è§‰é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯èƒ½é€šè¿‡é”™è¯¯çš„åæ€æ¨ç†æ¥å¼ºåŒ–åè§å’Œé”™è¯¯ï¼Œæœ€ç»ˆå¯¼è‡´å¹»è§‰æ¨ç†è·¯å¾„çš„äº§ç”Ÿã€‚è€Œä¸”ï¼Œå³ä½¿åœ¨å¹»è§‰çš„æºå¤´è¿›è¡Œå¹²é¢„ï¼Œä¹Ÿå¾€å¾€æ— æ³•æ¶ˆé™¤å…¶å½±å“ï¼Œå› ä¸ºæ¨ç†é“¾å­˜åœ¨â€œé“¾ä¸å¿ â€ç°è±¡ï¼Œå³å¯¹æŠ—çº æ­£å¹¶ä¿æŒé”™è¯¯é€»è¾‘çš„å€¾å‘ã€‚åŒæ—¶ï¼Œç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸‹å¯é æ€§è¾ƒä½ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§é»‘ç›’å®¡è®¡æ–¹æ³•ï¼Œå¯ä»¥å¯¹é•¿é“¾å¹»è§‰è¿›è¡Œå¯è§£é‡Šçš„å½’å› ï¼Œå…·æœ‰æ›´å¥½çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œä½†å¹»è§‰é—®é¢˜ä¹Ÿæ›´ä¸ºé¢‘ç¹å’Œéš¾ä»¥æ¶ˆé™¤ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å¤–éƒ¨çŸ¥è¯†æ•´åˆã€æ¨¡å‹å‚æ•°åˆ†ææˆ–è‡ªæˆ‘éªŒè¯æ¥å‡è½»å¹»è§‰ï¼Œä½†å¾€å¾€æ— æ³•æ•æ‰å¹»è§‰åœ¨æ¨ç†é“¾ä¸­çš„äº§ç”Ÿå’Œæ¼”å˜ã€‚</li>
<li>åœ¨é•¿æ¨ç†é“¾ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½é€šè¿‡é”™è¯¯çš„åæ€æ¨ç†æ¥å¼ºåŒ–åè§å’Œé”™è¯¯ï¼Œå¯¼è‡´å¹»è§‰æ¨ç†è·¯å¾„ã€‚</li>
<li>æ¨ç†é“¾å­˜åœ¨â€œé“¾ä¸å¿ â€ç°è±¡ï¼Œå³ä½¿ç›´æ¥å¹²é¢„å¹»è§‰æºå¤´ä¹Ÿéš¾ä»¥æ¶ˆé™¤å…¶å½±å“ã€‚</li>
<li>ç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸‹å¯é æ€§è¾ƒä½ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§é»‘ç›’å®¡è®¡æ–¹æ³•ï¼Œå¯ä»¥å¯¹é•¿é“¾å¹»è§‰è¿›è¡Œå¯è§£é‡Šçš„å½’å› ï¼Œå…·æœ‰æ›´å¥½çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2918ea6ea32d9f9a110b9cce50aeb0c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b77dca08f56e5eb5a4ea5afebc45f45f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9f42223f6745af9aa2d182be913d27e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bd02131b5ec18b6acc34e40f67bd40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f69b5e18300a7b44b1acdc37563fd6f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1103a4e20d6934d949a1b818d06d3b19.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Optimizing Anytime Reasoning via Budget Relative Policy Optimization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c9338689d9410e43b2c8ad7d852cd948.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  Towards Robust and Controllable Text-to-Motion via Masked Autoregressive   Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
