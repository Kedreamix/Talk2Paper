<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Exploring Superior Function Calls via Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-de452fb78ddc8091a4e3ca8421b239de.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-10-æ›´æ–°"><a href="#2025-08-10-æ›´æ–°" class="headerlink" title="2025-08-10 æ›´æ–°"></a>2025-08-10 æ›´æ–°</h1><h2 id="Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="Exploring Superior Function Calls via Reinforcement Learning"></a>Exploring Superior Function Calls via Reinforcement Learning</h2><p><strong>Authors:Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</strong></p>
<p>Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02% overall accuracy, outperforming standard GRPO by up to 6% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community. </p>
<blockquote>
<p>å‡½æ•°è°ƒç”¨èƒ½åŠ›å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œå½“å‰çš„è®­ç»ƒæ–¹æ³•æ— æ³•åˆ¶å®šç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè‚¤æµ…çš„æ¨¡å¼åŒ¹é…ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥åº”å¯¹ç»“æ„å‡½æ•°è°ƒç”¨ä¸­çš„å¤æ‚åŠ¨ä½œç©ºé—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡é‡èº«å®šåˆ¶çš„æˆ˜ç•¥ç†µåŸºæ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ï¼Œæ€ç»´ç”Ÿæˆä¸­ç»“æ„æ¨ç†çš„ç¼ºä¹ï¼Œä»¥åŠå‚æ•°æå–çš„éªŒè¯ä¸è¶³ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡æµç¨‹é€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ï¼Œç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º86.02%ï¼Œåœ¨å¤æ‚çš„å¤šå‡½æ•°åœºæ™¯ä¸­æ¯”æ ‡å‡†GRPOé«˜å‡º6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ˜¾ç¤ºå‡ºç‰¹åˆ«å¤§çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå‡½æ•°è°ƒç”¨çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦ç¤¾åŒºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05118v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ç¼ºå¤±é—®é¢˜ï¼Œç°æœ‰è®­ç»ƒç­–ç•¥æ— æ³•æœ‰æ•ˆæ„å»ºç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºç­–ç•¥ç›¸å¯¹åˆ†ç»„çš„ç†µæ¢ç´¢å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼Œä¸“é—¨è§£å†³å‡½æ•°è°ƒç”¨ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€é“¾å¼æ€ç»´ç”Ÿæˆä¸­çš„ç»“æ„åŒ–æ¨ç†ç¼ºå¤±ä»¥åŠå‚æ•°æå–éªŒè¯ä¸è¶³ã€‚é€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“ï¼Œç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬é€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ã€‚åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º86.02%ï¼Œåœ¨å¤æ‚çš„å¤šåŠŸèƒ½åœºæ™¯ä¸Šç›¸æ¯”æ ‡å‡†GRPOçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾6%ã€‚ç‰¹åˆ«åœ°ï¼Œè¯¥æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œè¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸­æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦å­¦æœ¯ç•Œã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è®­ç»ƒç­–ç•¥æ— æ³•ä¸ºå‡½æ•°è°ƒç”¨ä»»åŠ¡å‘å±•å‡ºç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•é¢ä¸´å¤æ‚çš„åŠ¨ä½œç©ºé—´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç­–ç•¥ç›¸å¯¹åˆ†ç»„çš„ç†µæ¢ç´¢å¢å¼ºç­–ç•¥ä¼˜åŒ–æ¥è§£å†³å‡½æ•°è°ƒç”¨ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†å‡½æ•°è°ƒç”¨çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šæ¢ç´¢ä¸è¶³ã€ç»“æ„åŒ–æ¨ç†ç¼ºå¤±ä»¥åŠå‚æ•°æå–éªŒè¯ä¸è¶³ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“ç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤æ‚çš„å¤šåŠŸèƒ½åœºæ™¯ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0325cf663bc2ee3e304bd86b481f48d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d94d61b38a07599bc20a0e10b4dfbc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b28f51069eb3fb09395131ccbd154141.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d73c27cbc0a454ff3b0b9fd13a851629.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models"><a href="#SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models" class="headerlink" title="SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models"></a>SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</h2><p><strong>Authors:Dai Do, Manh Nguyen, Svetha Venkatesh, Hung Le</strong></p>
<p>Large language models (LLMs) have shown strong reasoning capabilities when fine-tuned with reinforcement learning (RL). However, such methods require extensive data and compute, making them impractical for smaller models. Current approaches to curriculum learning or data selection are largely heuristic-driven or demand extensive computational resources, limiting their scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced learning framework that enables efficient learning based on the capability of the model being trained through optimizing which data to use and when. First, we apply \emph{cluster-based data reduction} to partition training data by semantics and difficulty, extracting a compact yet diverse subset that reduces redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms, optimized to allocate training samples based on model current performance. Experiments across multiple reasoning benchmarks show that SPaRFT achieves comparable or better accuracy than state-of-the-art baselines while using up to (100\times) fewer samples. Ablation studies and analyses further highlight the importance of both data clustering and adaptive selection. Our results demonstrate that carefully curated, performance-driven training curricula can unlock strong reasoning abilities in LLMs with minimal resources. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¾®è°ƒæ—¶æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä½¿å¾—å®ƒä»¬å¯¹äºè¾ƒå°çš„æ¨¡å‹æ¥è¯´ä¸åˆ‡å®é™…ã€‚å½“å‰çš„æ•™å­¦æ–¹æ³•æˆ–æ•°æ®é€‰æ‹©ä¸»è¦åŸºäºå¯å‘å¼æ–¹æ³•ï¼Œæˆ–è€…éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä»è€Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†SPaRFTï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–ä½¿ç”¨å“ªäº›æ•°æ®ä»¥åŠä½•æ—¶ä½¿ç”¨æ•°æ®ï¼Œå®ç°åŸºäºæ¨¡å‹èƒ½åŠ›çš„æœ‰æ•ˆå­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºèšç±»çš„æ•°æ®ç¼©å‡æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰å’Œéš¾åº¦å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ†åŒºï¼Œæå–å‡ºç´§å‡‘ä¸”å¤šæ ·åŒ–çš„å­é›†ï¼Œä»¥å‡å°‘å†—ä½™ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ•°æ®é›†ç¾¤è§†ä¸ºå¤šä¸ªæ­¦è£…å¸¦ï¼Œé‡‡ç”¨å¤šè‡‚åŒªå¾’ç®—æ³•ä¼˜åŒ–è®­ç»ƒæ ·æœ¬çš„åˆ†é…ï¼Œæ ¹æ®æ¨¡å‹å½“å‰æ€§èƒ½è¿›è¡Œå†³ç­–ã€‚è·¨å¤šä¸ªæ¨ç†åŸºå‡†çš„å®éªŒè¡¨æ˜ï¼ŒSPaRFTçš„å‡†ç¡®ç‡ä¸æœ€å…ˆè¿›çš„åŸºå‡†ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡å‡å°‘äº†é«˜è¾¾(100å€)ã€‚æ¶ˆèç ”ç©¶å’Œåˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†æ•°æ®èšç±»å’Œè‡ªé€‚åº”é€‰æ‹©çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒç­–åˆ’çš„ã€ä»¥æ€§èƒ½ä¸ºå¯¼å‘çš„è®­ç»ƒè¯¾ç¨‹å¯ä»¥è§£é”å°å‹è¯­è¨€æ¨¡å‹çš„æœ€å¼ºæ¨ç†èƒ½åŠ›åªéœ€æå°‘èµ„æºå³å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05015v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¾®è°ƒåå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ­¤æ–¹æ³•éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¯¹äºå°å‹æ¨¡å‹æ¥è¯´å¹¶ä¸å®ç”¨ã€‚å½“å‰çš„æ•™å­¦æ³•æˆ–æ•°æ®é€‰æ‹©æ–¹æ³•ä¸»è¦ä¾èµ–äºå¯å‘å¼æˆ–å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSPaRFTçš„è‡ªæˆ‘èŠ‚å¥å­¦ä¹ æ¡†æ¶ï¼ŒåŸºäºæ¨¡å‹çš„èƒ½åŠ›ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­æ•°æ®å’Œæ—¶é—´çš„åˆ†é…ï¼Œå®ç°é«˜æ•ˆå­¦ä¹ ã€‚é€šè¿‡èšç±»å‡å°‘æ•°æ®å†—ä½™ï¼Œå°†è®­ç»ƒæ•°æ®æŒ‰è¯­ä¹‰å’Œéš¾åº¦è¿›è¡Œåˆ†åŒºï¼Œæå–å‡ºç´§å‡‘ä¸”å¤šæ ·åŒ–çš„å­é›†ã€‚ç„¶åï¼Œåˆ©ç”¨å¤šè‡‚è€è™æœºç®—æ³•ä¼˜åŒ–æ•°æ®é›†ç¾¤çš„åˆ†é…ï¼Œæ ¹æ®æ¨¡å‹å½“å‰æ€§èƒ½æ¥åˆ†é…è®­ç»ƒæ ·æœ¬ã€‚å®éªŒè¯æ˜ï¼ŒSPaRFTåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡æœ€å¤šå‡å°‘äº†(100)å€ã€‚åˆ†æå’Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†æ•°æ®èšç±»å’Œè‡ªé€‚åº”é€‰æ‹©çš„é‡è¦æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒç­–åˆ’çš„æ€§èƒ½é©±åŠ¨è®­ç»ƒè¯¾ç¨‹å¯ä»¥åœ¨æœ‰é™èµ„æºä¸‹è§£é”LLMsçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†éœ€å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li>
<li>å½“å‰æ•™å­¦æ³•æˆ–æ•°æ®é€‰æ‹©æ–¹æ³•å—é™äºå¯å‘å¼æˆ–è®¡ç®—èµ„æºï¼Œç¼ºä¹å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>SPaRFTæ¡†æ¶ç»“åˆè‡ªæˆ‘èŠ‚å¥å­¦ä¹ å’Œä¼˜åŒ–ç®—æ³•ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒæ•°æ®ä½¿ç”¨ã€‚</li>
<li>é€šè¿‡èšç±»å‡å°‘æ•°æ®å†—ä½™ï¼ŒæŒ‰è¯­ä¹‰å’Œéš¾åº¦åˆ†åŒºè®­ç»ƒæ•°æ®ã€‚</li>
<li>åˆ©ç”¨å¤šè‡‚è€è™æœºç®—æ³•ä¼˜åŒ–æ•°æ®é›†ç¾¤åˆ†é…ï¼Œæ ¹æ®æ¨¡å‹æ€§èƒ½åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>å®éªŒè¯æ˜SPaRFTåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ ·æœ¬ä½¿ç”¨é‡å¤§å¹…å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3bb3511153175deb68b131295ec59cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd542886032745f4534d25d1b75abc9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Agnostics-Learning-to-Code-in-Any-Programming-Language-via-Reinforcement-with-a-Universal-Learning-Environment"><a href="#Agnostics-Learning-to-Code-in-Any-Programming-Language-via-Reinforcement-with-a-Universal-Learning-Environment" class="headerlink" title="Agnostics: Learning to Code in Any Programming Language via   Reinforcement with a Universal Learning Environment"></a>Agnostics: Learning to Code in Any Programming Language via   Reinforcement with a Universal Learning Environment</h2><p><strong>Authors:Aleksander Boruch-Gruszecki, Yangtian Zi, Zixuan Wu, Tejas Oberoi, Carolyn Jane Anderson, Joydeep Biswas, Arjun Guha</strong></p>
<p>Large language models (LLMs) already excel at writing code in high-resource languages such as Python and JavaScript, yet stumble on low-resource languages that remain essential to science and engineering. Besides the obvious shortage of pre-training data, post-training itself is a bottleneck: every new language seems to require new datasets, test harnesses, and reinforcement-learning (RL) infrastructure.   We introduce Agnostics, a language-agnostic post-training pipeline that eliminates this per-language engineering. The key idea is to judge code solely by its externally observable behavior, so a single verifier can test solutions written in any language. Concretely, we (i) use an LLM to rewrite existing unit-test datasets into an I&#x2F;O format, (ii) supply a short configuration that tells the verifier how to compile and run a target language, and (iii) apply reinforcement learning with verifiable rewards (RLVR) in a robust code execution environment.   Applied to five low-resource languagesâ€“Lua, Julia, R, OCaml, and Fortranâ€“Agnostics (1) improves Qwen-3 4B to performance that rivals other 16B-70B open-weight models; (2) scales cleanly to larger and diverse model families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for ${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on MultiPL-E and a new multi-language version LiveCodeBench that we introduce.   We will release the language-agnostic training datasets (Ag-MBPP-X, Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use configurations, making RL post-training in any programming language as simple as editing a short YAML file. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»åœ¨é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚Pythonå’ŒJavaScriptï¼‰çš„ç¼–ç¨‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¯¹ç§‘å­¦å’Œå·¥ç¨‹è‡³å…³é‡è¦çš„ä½èµ„æºè¯­è¨€æ–¹é¢å´é­é‡å›°å¢ƒã€‚é™¤äº†æ˜æ˜¾çš„é¢„è®­ç»ƒæ•°æ®çŸ­ç¼ºå¤–ï¼Œåç»­çš„è®­ç»ƒæœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªç“¶é¢ˆï¼šæ¯ç§æ–°è¯­è¨€ä¼¼ä¹éƒ½éœ€è¦æ–°çš„æ•°æ®é›†ã€æµ‹è¯•ç¨‹åºå’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åŸºç¡€è®¾æ–½ã€‚æˆ‘ä»¬å¼•å…¥äº†Agnosticsï¼Œè¿™æ˜¯ä¸€ç§è¯­è¨€æ— å…³çš„åè®­ç»ƒç®¡é“ï¼Œæ¶ˆé™¤äº†æ¯ç§è¯­è¨€çš„å·¥ç¨‹ã€‚åŸºæœ¬æ€æƒ³æ˜¯æ ¹æ®ä»£ç çš„å¤–åœ¨å¯è§‚å¯Ÿè¡Œä¸ºæ¥åˆ¤æ–­ä»£ç ï¼Œå› æ­¤å•ä¸ªéªŒè¯å™¨å¯ä»¥æµ‹è¯•ä»»ä½•è¯­è¨€ç¼–å†™çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ï¼ˆiï¼‰ä½¿ç”¨LLMå°†ç°æœ‰çš„å•å…ƒæµ‹è¯•æ•°æ®é›†é‡å†™æˆI&#x2F;Oæ ¼å¼ï¼Œï¼ˆiiï¼‰æä¾›ä¸€ä¸ªç®€çŸ­é…ç½®ï¼Œå‘Šè¯‰éªŒè¯å™¨å¦‚ä½•ç¼–è¯‘å’Œè¿è¡Œç›®æ ‡è¯­è¨€ï¼Œï¼ˆiiiï¼‰åœ¨ä¸€ä¸ªç¨³å®šçš„ä»£ç æ‰§è¡Œç¯å¢ƒä¸­åº”ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚åº”ç”¨äºäº”ç§ä½èµ„æºè¯­è¨€â€”â€”Luaã€Juliaã€Rã€OCamlå’ŒFortranâ€”â€”Agnosticsï¼ˆ1ï¼‰å°†Qwen-3 4Bçš„æ€§èƒ½æé«˜è‡³ä¸å…¶ä»–16B-70Bå…¬å¼€æƒé‡æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼›ï¼ˆ2ï¼‰èƒ½å¤Ÿå¹²å‡€åœ°æ‰©å±•åˆ°æ›´å¤§å’Œå¤šæ ·åŒ–çš„æ¨¡å‹å®¶æ—ï¼ˆQwen-3 8Bã€DeepSeek Coder 6.7B Instructã€Phi 4 Miniï¼‰ï¼›ï¼ˆ3ï¼‰å¯¹äºâ‰¤16Bå‚æ•°æ¨¡å‹ï¼Œåœ¨æˆ‘ä»¬å¼•å…¥çš„MultiPL-Eå’Œæ–°çš„å¤šè¯­è¨€ç‰ˆæœ¬LiveCodeBenchä¸Šï¼Œè®¾ç½®äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„pass@1ç»“æœã€‚æˆ‘ä»¬å°†å‘å¸ƒè¯­è¨€æ— å…³çš„è®­ç»ƒæ•°æ®é›†ï¼ˆAg-MBPP-Xã€Ag-Codeforces-Xã€Ag-LiveCodeBench-Xï¼‰ã€è®­ç»ƒä»£ç å’Œå³ç”¨çš„é…ç½®ï¼Œä½¿ä»»ä½•ç¼–ç¨‹è¯­è¨€çš„å¼ºåŒ–åè®­ç»ƒåªéœ€ç¼–è¾‘ä¸€ä¸ªç®€çŸ­çš„YAMLæ–‡ä»¶å³å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04865v1">PDF</a> 18 pages, 19 figures. For artifacts, see <a target="_blank" rel="noopener" href="https://agnostics.abgru.me/">https://agnostics.abgru.me</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†åƒPythonå’ŒJavaScriptè¿™æ ·çš„é«˜èµ„æºè¯­è¨€ç¼–ç¨‹æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç§‘å­¦å·¥ç¨‹å¿…éœ€çš„ä½èµ„æºè¯­è¨€æ—¶é‡åˆ°å›°éš¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Agnosticsï¼Œä¸€ç§è¯­è¨€æ— å…³çš„åè®­ç»ƒç®¡é“ï¼Œå®ƒæ¶ˆé™¤äº†æ¯ç§è¯­è¨€æ‰€éœ€çš„å·¥ç¨‹ã€‚Agnosticsçš„å…³é”®æ€æƒ³æ˜¯é€šè¿‡åˆ¤æ–­ä»£ç çš„å¤–åœ¨è¡Œä¸ºæ¥è¯„ä¼°ä»£ç ï¼Œä»è€Œä½¿ç”¨å•ä¸ªéªŒè¯å™¨æµ‹è¯•ä»»ä½•è¯­è¨€ç¼–å†™çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡é‡å†™ç°æœ‰å•å…ƒæµ‹è¯•æ•°æ®é›†ä¸ºI&#x2F;Oæ ¼å¼ã€æä¾›å‘Šè¯‰éªŒè¯å™¨å¦‚ä½•ç¼–è¯‘å’Œè¿è¡Œç›®æ ‡è¯­è¨€çš„ç®€çŸ­é…ç½®ï¼Œä»¥åŠåº”ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨ç¨³å¥çš„ä»£ç æ‰§è¡Œç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¯¹äº”ç§ä½èµ„æºè¯­è¨€â€”â€”Luaã€Juliaã€Rã€OCamlå’ŒFortrançš„åº”ç”¨ã€‚Agnosticsä¸ä»…æé«˜äº†æ€§èƒ½ï¼Œè¿˜è½»æ¾æ‰©å±•åˆ°äº†æ›´å¤§çš„æ¨¡å‹å®¶æ—ï¼Œå¹¶ä¸ºæˆ‘ä»¬å¼•å…¥çš„æ–°å¤šè¯­è¨€ç‰ˆæœ¬LiveCodeBenchå’ŒMultiPL-Eè®¾ç½®äº†æ–°çš„æœ€å…ˆè¿›çš„pass@1ç»“æœã€‚æˆ‘ä»¬å°†å‘å¸ƒè¯­è¨€æ— å…³çš„è®­ç»ƒæ•°æ®é›†ã€è®­ç»ƒä»£ç å’Œå³ç”¨å‹é…ç½®ï¼Œä½¿å¾—åœ¨ä»»ä½•ç¼–ç¨‹è¯­è¨€ä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒåªéœ€ç¼–è¾‘ä¸€ä¸ªç®€çŸ­çš„YAMLæ–‡ä»¶å³å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä½èµ„æºè¯­è¨€ç¼–ç¨‹æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹é¢„è®­ç»ƒæ•°æ®å’Œåè®­ç»ƒç“¶é¢ˆã€‚</li>
<li>Agnosticsæ˜¯ä¸€ç§è¯­è¨€æ— å…³çš„åè®­ç»ƒç®¡é“ï¼Œé€šè¿‡åˆ¤æ–­ä»£ç çš„å¤–åœ¨è¡Œä¸ºæ¥è¯„ä¼°ä»£ç ï¼Œæ¶ˆé™¤æ¯ç§è¯­è¨€æ‰€éœ€çš„å·¥ç¨‹ã€‚</li>
<li>Agnosticså®ç°äº†å¯¹äº”ç§ä½èµ„æºè¯­è¨€çš„åº”ç”¨ï¼Œå¹¶æé«˜äº†æ€§èƒ½ã€‚</li>
<li>Agnosticsèƒ½å¤Ÿè½»æ¾æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹å®¶æ—ã€‚</li>
<li>Agnosticsä¸ºæˆ‘ä»¬å¼•å…¥çš„æ–°å¤šè¯­è¨€ç‰ˆæœ¬LiveCodeBenchå’ŒMultiPL-Eè®¾ç½®äº†æ–°çš„æœ€å…ˆè¿›çš„pass@1ç»“æœã€‚</li>
<li>å‘å¸ƒäº†è¯­è¨€æ— å…³çš„è®­ç»ƒæ•°æ®é›†ã€è®­ç»ƒä»£ç å’Œå³ç”¨å‹é…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-613706381b3f6b9c8e6dda1528eba326.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18b8c8f3022492ce84a9606137571b16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cca7554d08ebb881065ae5c0e9082710.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-416811f34c4969386fc568a2502f5cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ac4bbac68335132c36fb6e7bf308e63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e86acbee81d128467fa08fbffd9014d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VER-Bench-Evaluating-MLLMs-on-Reasoning-with-Fine-Grained-Visual-Evidence"><a href="#VER-Bench-Evaluating-MLLMs-on-Reasoning-with-Fine-Grained-Visual-Evidence" class="headerlink" title="VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual   Evidence"></a>VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual   Evidence</h2><p><strong>Authors:Chenhui Qiang, Zhaoyang Wei, Xumeng Han Zipeng Wang, Siyao Li, Xiangyuan Lan, Jianbin Jiao, Zhenjun Han</strong></p>
<p>With the rapid development of MLLMs, evaluating their visual capabilities has become increasingly crucial. Current benchmarks primarily fall into two main types: basic perception benchmarks, which focus on local details but lack deep reasoning (e.g., â€œwhat is in the image?â€), and mainstream reasoning benchmarks, which concentrate on prominent image elements but may fail to assess subtle clues requiring intricate analysis. However, profound visual understanding and complex reasoning depend more on interpreting subtle, inconspicuous local details than on perceiving salient, macro-level objects. These details, though occupying minimal image area, often contain richer, more critical information for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel framework to evaluate MLLMsâ€™ ability to: 1) identify fine-grained visual clues, often occupying on average just 0.25% of the image area; 2) integrate these clues with world knowledge for complex reasoning. Comprising 374 carefully designed questions across Geospatial, Temporal, Situational, Intent, System State, and Symbolic reasoning, each question in VER-Bench is accompanied by structured evidence: visual clues and question-related reasoning derived from them. VER-Bench reveals current modelsâ€™ limitations in extracting subtle visual evidence and constructing evidence-based arguments, highlighting the need to enhance modelsâ€™s capabilities in fine-grained visual evidence extraction, integration, and reasoning for genuine visual understanding and human-like analysis. Dataset and additional materials are available <a target="_blank" rel="noopener" href="https://github.com/verbta/ACMMM-25-Materials">https://github.com/verbta/ACMMM-25-Materials</a>. </p>
<blockquote>
<p>éšç€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°å®ƒä»¬çš„è§†è§‰èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç›®å‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¸¤ç§ç±»å‹ï¼šåŸºæœ¬æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼Œä¾§é‡äºå±€éƒ¨ç»†èŠ‚ï¼Œä½†ç¼ºä¹æ·±åº¦æ¨ç†ï¼ˆä¾‹å¦‚ï¼Œâ€œå›¾åƒä¸­æœ‰ä»€ä¹ˆï¼Ÿâ€ï¼‰ï¼›ä»¥åŠä¸»æµçš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œä¾§é‡äºå›¾åƒä¸­çš„çªå‡ºå…ƒç´ ï¼Œä½†å¯èƒ½æ— æ³•è¯„ä¼°éœ€è¦ç»†è‡´åˆ†æçš„ç»†å¾®çº¿ç´¢ã€‚ç„¶è€Œï¼Œæ·±åˆ»çš„è§†è§‰ç†è§£å’Œå¤æ‚çš„æ¨ç†æ›´å¤šåœ°ä¾èµ–äºè§£é‡Šç»†å¾®çš„ã€ä¸å¼•äººæ³¨ç›®çš„å±€éƒ¨ç»†èŠ‚ï¼Œè€Œä¸æ˜¯æ„ŸçŸ¥æ˜¾è‘—ã€å®è§‚çº§åˆ«çš„å¯¹è±¡ã€‚è¿™äº›ç»†èŠ‚è™½ç„¶å æ®çš„å›¾åƒé¢ç§¯å¾ˆå°ï¼Œä½†å¾€å¾€åŒ…å«æ›´ä¸°å¯Œã€æ›´å…³é”®çš„ä¿¡æ¯ï¼Œæœ‰åˆ©äºè¿›è¡Œç¨³å¥çš„åˆ†æã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†VER-Benchè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼š1ï¼‰è¯†åˆ«å¹³å‡åªå å›¾åƒé¢ç§¯0.25%çš„ç»†å¾®è§†è§‰çº¿ç´¢ï¼›2ï¼‰å°†è¿™äº›çº¿ç´¢ä¸ä¸–ç•ŒçŸ¥è¯†ç›¸ç»“åˆï¼Œè¿›è¡Œå¤æ‚æ¨ç†ã€‚VER-BenchåŒ…å«374ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ¶µç›–åœ°ç†ç©ºé—´ã€æ—¶é—´ã€æƒ…å¢ƒã€æ„å›¾ã€ç³»ç»ŸçŠ¶æ€å’Œç¬¦å·æ¨ç†ç­‰æ–¹é¢ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ä¼´æœ‰ç»“æ„åŒ–è¯æ®ï¼šè§†è§‰çº¿ç´¢å’Œæºäºè¿™äº›çº¿ç´¢çš„é—®é¢˜ç›¸å…³æ¨ç†ã€‚VER-Benchæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æå–ç»†å¾®è§†è§‰è¯æ®å’Œæ„å»ºåŸºäºè¯æ®è®ºè¯æ–¹é¢çš„å±€é™æ€§ï¼Œå¼ºè°ƒéœ€è¦å¢å¼ºæ¨¡å‹åœ¨ç»†å¾®è§†è§‰è¯æ®æå–ã€æ•´åˆå’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œä»¥å®ç°çœŸæ­£çš„è§†è§‰ç†è§£å’Œäººç±»æ ·æœ¬çš„åˆ†æã€‚æ•°æ®é›†å’Œå…¶ä»–ææ–™å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/verbta/ACMMM-25-Materials%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/verbta/ACMMM-25-Materialsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04852v1">PDF</a> Accept by ACMM2025 Dataset track</p>
<p><strong>Summary</strong></p>
<p>éšç€MLLMsçš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°å…¶è§†è§‰èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚å½“å‰è¯„ä¼°æ ‡å‡†ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šä¾§é‡å±€éƒ¨ç»†èŠ‚çš„æ„ŸçŸ¥è¯„ä¼°å’Œä¾§é‡æ˜¾è‘—å›¾åƒå…ƒç´ æ¨ç†çš„ä¸»æµè¯„ä¼°æ ‡å‡†ã€‚ç„¶è€Œï¼Œæ·±åˆ»çš„è§†è§‰ç†è§£å’Œå¤æ‚æ¨ç†æ›´ä¾èµ–äºå¯¹å¾®å¦™ã€ä¸å¼•äººæ³¨ç›®çš„å±€éƒ¨ç»†èŠ‚çš„è§£é‡Šã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†VER-Benchæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°MLLMsè¯†åˆ«ä»…å å›¾åƒé¢ç§¯å¹³å‡0.25%çš„ç²¾ç»†è§†è§‰çº¿ç´¢çš„èƒ½åŠ›ï¼Œä»¥åŠå°†è¿™äº›çº¿ç´¢ä¸ä¸–ç•ŒçŸ¥è¯†ç›¸ç»“åˆè¿›è¡Œå¤æ‚æ¨ç†çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åœ°ç†ç©ºé—´ã€æ—¶é—´ã€æƒ…å¢ƒã€æ„å›¾ã€ç³»ç»ŸçŠ¶æ€å’Œç¬¦å·æ¨ç†ç­‰374ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é™„æœ‰ç»“æ„åŒ–è¯æ®ã€‚è¿™æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æå–ç»†å¾®è§†è§‰è¯æ®å’Œæ„å»ºåŸºäºè¯æ®çš„è®ºè¯æ–¹é¢çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¢å¼ºæ¨¡å‹åœ¨ç²¾ç»†è§†è§‰è¯æ®æå–ã€æ•´åˆå’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsçš„è§†è§‰èƒ½åŠ›è¯„ä¼°å˜å¾—é‡è¦ï¼Œéœ€è¦æ–°çš„è¯„ä¼°æ¡†æ¶æ¥å¼¥è¡¥ç°æœ‰åŸºå‡†æµ‹è¯•çš„ä¸è¶³ã€‚</li>
<li>VER-Benchæ¡†æ¶æ—¨åœ¨è¯„ä¼°MLLMså¯¹ç²¾ç»†è§†è§‰çº¿ç´¢çš„è¯†åˆ«èƒ½åŠ›ï¼Œè¿™äº›çº¿ç´¢ä»…å å›¾åƒé¢ç§¯çš„0.25%ã€‚</li>
<li>VER-Benchå¼ºè°ƒå°†è§†è§‰çº¿ç´¢ä¸ä¸–ç•ŒçŸ¥è¯†ç»“åˆè¿›è¡Œå¤æ‚æ¨ç†çš„é‡è¦æ€§ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æå–ç»†å¾®è§†è§‰è¯æ®å’Œæ„å»ºåŸºäºè¯æ®çš„è®ºè¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>VER-BenchåŒ…å«å¤šç§ç±»å‹çš„é—®é¢˜ï¼Œæ¶µç›–åœ°ç†ç©ºé—´ã€æ—¶é—´ã€æƒ…å¢ƒç­‰ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¯ä¸ªé—®é¢˜åœ¨VER-Benchä¸­éƒ½é…æœ‰ç»“æ„åŒ–è¯æ®ï¼ŒåŒ…æ‹¬è§†è§‰çº¿ç´¢å’Œä¸é—®é¢˜ç›¸å…³çš„æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5ab61769d08d0c6585996e423624a71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-574d1f3a3475dff18ad8d3b2d037c4f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be6c54b0f2b021bb6adbbc0a998e4e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3141f3793ddcb1612630865b4d54bbe2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a36f3d558d94e0613f7461d355bb2d9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-Reasoning-Abilities-Under-Non-Ideal-Conditions-After-RL-Fine-Tuning"><a href="#Large-Language-Models-Reasoning-Abilities-Under-Non-Ideal-Conditions-After-RL-Fine-Tuning" class="headerlink" title="Large Language Models Reasoning Abilities Under Non-Ideal Conditions   After RL-Fine-Tuning"></a>Large Language Models Reasoning Abilities Under Non-Ideal Conditions   After RL-Fine-Tuning</h2><p><strong>Authors:Chang Tian, Matthew B. Blaschko, Mingzhe Xing, Xiuxing Li, Yinliang Yue, Marie-Francine Moens</strong></p>
<p>Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs. We formally define and evaluate these challenging scenarios. We fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM) using RL with a representative policy-gradient algorithm and then test their performance on eight public datasets. Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities. Although we propose a scenario-specific remediation method, our results suggest current methods leave these reasoning deficits largely unresolved. This work highlights that the reasoning abilities of large models are often overstated and underscores the importance of evaluating models under non-ideal scenarios. The code and data will be released at XXXX. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®æŠ€æœ¯ï¼Œç­–ç•¥æ¢¯åº¦ç®—æ³•åœ¨è®­ç»ƒåçš„é˜¶æ®µå æ®ä¸»å¯¼åœ°ä½ï¼Œå› å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºå‡†æµ‹è¯•éƒ½åœ¨ç†æƒ³åŒ–çš„ç¯å¢ƒä¸‹è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¿½ç•¥äº†åœ¨ç°å®ã€éç†æƒ³åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸‰ä¸ªå…·æœ‰å®é™…ç›¸å…³æ€§çš„ä»£è¡¨æ€§éç†æƒ³åœºæ™¯ï¼šæ‘˜è¦æ¨ç†ã€ç²¾ç»†å™ªå£°æŠ‘åˆ¶å’Œä¸Šä¸‹æ–‡è¿‡æ»¤ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘ï¼Œè¯¥æ–¹å‘ä»¥è„‘ç§‘å­¦çš„ç ”ç©¶ç»“æœä¸ºæŒ‡å¯¼ï¼Œè¡¨æ˜äººç±»æ¨ç†åœ¨å­˜åœ¨ç¼ºé™·çš„è¾“å…¥ä¸‹ä»ç„¶å¯é ã€‚æˆ‘ä»¬å¯¹è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯è¿›è¡Œäº†æ­£å¼çš„å®šä¹‰å’Œè¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œä¸€ä¸ªä»£è¡¨æ€§çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•å¾®è°ƒäº†ä¸‰ä¸ªLLMå’Œä¸€ä¸ªæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆLVLMï¼‰ï¼Œç„¶ååœ¨å…«ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šæµ‹è¯•å®ƒä»¬çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ”¹è¿›äº†ç†æƒ³ç¯å¢ƒä¸‹çš„åŸºçº¿æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ‰€æœ‰ä¸‰ä¸ªéç†æƒ³åœºæ™¯ä¸­æ€§èƒ½å‡æ˜¾è‘—ä¸‹é™ï¼Œæš´éœ²äº†é«˜çº§æ¨ç†èƒ½åŠ›çš„å…³é”®å±€é™æ€§ã€‚å°½ç®¡æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ç‰¹å®šåœºæ™¯çš„è¡¥æ•‘æ–¹æ³•ï¼Œä½†æˆ‘ä»¬çš„ç»“æœä»è¡¨æ˜å½“å‰çš„æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ— æ³•è§£å†³è¿™äº›æ¨ç†ç¼ºé™·ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¾€å¾€è¢«é«˜ä¼°äº†ï¼Œå¹¶å¼ºè°ƒäº†åœ¨éç†æƒ³ç¯å¢ƒä¸‹è¯„ä¼°æ¨¡å‹çš„é‡è¦æ€§ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨XXXXå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04848v1">PDF</a> large language models, large vision-language model, reasoning,   non-ideal conditions, reinforcement learning</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«ç”¨æ¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ç­–ç•¥æ¢¯åº¦ç®—æ³•å› å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§è€Œåœ¨åè®­ç»ƒé˜¶æ®µå æ®ä¸»å¯¼åœ°ä½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºå‡†æµ‹è¯•éƒ½åœ¨ç†æƒ³åŒ–çš„ç¯å¢ƒä¸‹è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¿½ç•¥äº†åœ¨éç†æƒ³åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æœ¬æ–‡ç¡®å®šäº†ä¸‰ä¸ªå…·æœ‰å®é™…æ„ä¹‰çš„ä»£è¡¨æ€§éç†æƒ³åœºæ™¯ï¼šæ‘˜è¦æ¨ç†ã€ç²¾ç»†å™ªå£°æŠ‘åˆ¶å’Œä¸Šä¸‹æ–‡è¿‡æ»¤ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘ï¼Œè¯¥æ–¹å‘ç”±è„‘ç§‘å­¦å‘ç°å¼•å¯¼ï¼Œå³äººç±»æ¨ç†å¯ä»¥åœ¨ä¸å®Œç¾çš„è¾“å…¥ä¸‹ä»ç„¶å¯é ã€‚æˆ‘ä»¬å¯¹ä¸‰ä¸ªLLMã€ä¸€ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è¿›è¡Œäº†RLç­–ç•¥æ¢¯åº¦ç®—æ³•çš„å¾®è°ƒï¼Œå¹¶åœ¨å…«ä¸ªå…¬å…±æ•°æ®é›†ä¸Šæµ‹è¯•äº†å®ƒä»¬çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶RLå¾®è°ƒæé«˜äº†ç†æƒ³ç¯å¢ƒä¸‹çš„åŸºçº¿æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ‰€æœ‰ä¸‰ç§éç†æƒ³åœºæ™¯ä¸­ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæš´éœ²å‡ºé«˜çº§æ¨ç†èƒ½åŠ›çš„å…³é”®å±€é™æ€§ã€‚å°½ç®¡æˆ‘ä»¬æå‡ºäº†é’ˆå¯¹ç‰¹å®šåœºæ™¯çš„è¡¥æ•‘æ–¹æ³•ï¼Œä½†ç»“æœä»è¡¨æ˜å½“å‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£å†³è¿™äº›æ¨ç†ç¼ºé™·ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ç»å¸¸è¢«è¿‡åˆ†å¤¸å¤§ï¼Œå¹¶å¼ºè°ƒäº†åœ¨éç†æƒ³ç¯å¢ƒä¸‹è¯„ä¼°æ¨¡å‹çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºæé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ç­–ç•¥æ¢¯åº¦ç®—æ³•è¡¨ç°çªå‡ºã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°ç†æƒ³ç¯å¢ƒä¸‹çš„è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå¿½ç•¥äº†éç†æƒ³åœºæ™¯ã€‚</li>
<li>è®ºæ–‡ç¡®å®šäº†ä¸‰ä¸ªå…·æœ‰å®é™…æ„ä¹‰çš„éç†æƒ³åœºæ™¯ï¼šæ‘˜è¦æ¨ç†ã€ç²¾ç»†å™ªå£°æŠ‘åˆ¶å’Œä¸Šä¸‹æ–‡è¿‡æ»¤ã€‚</li>
<li>è„‘ç§‘å­¦å‘ç°è¡¨æ˜äººç±»æ¨ç†å¯ä»¥åœ¨ä¸å®Œç¾çš„è¾“å…¥ä¸‹ä»ç„¶å¯é ï¼Œè®ºæ–‡åŸºäºæ­¤å¼•å…¥æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨éç†æƒ³åœºæ™¯ä¸‹ï¼Œç»è¿‡RLè°ƒæ•™çš„æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæš´éœ²å‡ºå…¶æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚</li>
<li>å°½ç®¡å°è¯•æå‡ºé’ˆå¯¹ç‰¹å®šåœºæ™¯çš„è¡¥æ•‘æ–¹æ³•ï¼Œä½†å½“å‰æ¨¡å‹åœ¨éç†æƒ³åœºæ™¯ä¸‹çš„æ¨ç†ç¼ºé™·ä»æœªå¾—åˆ°æœ‰æ•ˆè§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1de229e60e92e05cb530716234aa1977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19ab94663bd314fce2bdc80043c7362d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de452fb78ddc8091a4e3ca8421b239de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e92bbe8f9217dd282a9164c8daccb5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-892b1448ad5e8aa94f7129eb85fb439d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Uncertainty-aware-Predict-Then-Optimize-Framework-for-Equitable-Post-Disaster-Power-Restoration"><a href="#Uncertainty-aware-Predict-Then-Optimize-Framework-for-Equitable-Post-Disaster-Power-Restoration" class="headerlink" title="Uncertainty-aware Predict-Then-Optimize Framework for Equitable   Post-Disaster Power Restoration"></a>Uncertainty-aware Predict-Then-Optimize Framework for Equitable   Post-Disaster Power Restoration</h2><p><strong>Authors:Lin Jiang, Dahai Yu, Rongchao Xu, Tian Tang, Guang Wang</strong></p>
<p>The increasing frequency of extreme weather events, such as hurricanes, highlights the urgent need for efficient and equitable power system restoration. Many electricity providers make restoration decisions primarily based on the volume of power restoration requests from each region. However, our data-driven analysis reveals significant disparities in request submission volume, as disadvantaged communities tend to submit fewer restoration requests. This disparity makes the current restoration solution inequitable, leaving these communities vulnerable to extended power outages. To address this, we aim to propose an equity-aware power restoration strategy that balances both restoration efficiency and equity across communities. However, achieving this goal is challenging for two reasons: the difficulty of predicting repair durations under dataset heteroscedasticity, and the tendency of reinforcement learning agents to favor low-uncertainty actions, which potentially undermine equity. To overcome these challenges, we design a predict-then-optimize framework called EPOPR with two key components: (1) Equity-Conformalized Quantile Regression for uncertainty-aware repair duration prediction, and (2) Spatial-Temporal Attentional RL that adapts to varying uncertainty levels across regions for equitable decision-making. Experimental results show that our EPOPR effectively reduces the average power outage duration by 3.60% and decreases inequity between different communities by 14.19% compared to state-of-the-art baselines. </p>
<blockquote>
<p>æç«¯å¤©æ°”äº‹ä»¶ï¼Œå¦‚é¾™å·é£çš„é¢‘å‘ï¼Œå‡¸æ˜¾äº†é«˜æ•ˆå…¬å¹³çš„ç”µåŠ›ç³»ç»Ÿæ¢å¤çš„ç´§è¿«éœ€æ±‚ã€‚è®¸å¤šç”µåŠ›ä¾›åº”å•†ä¸»è¦åŸºäºå„åœ°åŒºçš„ç”µåŠ›æ¢å¤è¯·æ±‚æ•°é‡æ¥åšå‡ºæ¢å¤å†³ç­–ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ•°æ®é©±åŠ¨åˆ†ææ˜¾ç¤ºï¼Œè¯·æ±‚æäº¤çš„æ•°é‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå› ä¸ºè´«å›°ç¤¾åŒºå¾€å¾€æäº¤è¾ƒå°‘çš„æ¢å¤è¯·æ±‚ã€‚è¿™ç§å·®å¼‚ä½¿å¾—å½“å‰çš„æ¢å¤è§£å†³æ–¹æ¡ˆä¸å…¬å¹³ï¼Œä½¿è¿™äº›ç¤¾åŒºå®¹æ˜“å—åˆ°é•¿æœŸåœç”µçš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ—¨åœ¨æå‡ºä¸€ç§å…¬å¹³æ„è¯†å¼ºçš„ç”µåŠ›æ¢å¤ç­–ç•¥ï¼Œåœ¨ç¤¾åŒºä¹‹é—´å¹³è¡¡æ¢å¤æ•ˆç‡å’Œå…¬å¹³ã€‚ç„¶è€Œï¼Œå®ç°è¿™ä¸€ç›®æ ‡é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šæ•°æ®é›†å¼‚æ–¹å·®æ€§ä¸‹é¢„æµ‹ä¿®å¤æŒç»­æ—¶é—´çš„éš¾åº¦ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ ä»£ç†äººå€¾å‘äºé€‰æ‹©ä½ä¸ç¡®å®šæ€§è¡ŒåŠ¨çš„è¶‹åŠ¿ï¼Œè¿™å¯èƒ½ä¼šç ´åå…¬å¹³ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåä¸ºEPOPRçš„é¢„æµ‹ä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒæœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰å…¬å¹³åˆè§„åˆ†ä½æ•°å›å½’ï¼Œç”¨äºä¸ç¡®å®šæ€§æ„è¯†ä¿®å¤æŒç»­æ—¶é—´é¢„æµ‹ï¼›ï¼ˆ2ï¼‰æ—¶ç©ºæ³¨æ„åŠ›å¼ºåŒ–å­¦ä¹ ï¼Œé€‚åº”å„åœ°åŒºä¸åŒä¸ç¡®å®šæ€§æ°´å¹³ï¼Œä»¥å…¬å¹³å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„EPOPRå¹³å‡åœç”µæŒç»­æ—¶é—´å‡å°‘äº†3.60%ï¼Œä¸åŒç¤¾åŒºä¹‹é—´çš„ä¸å…¬å¹³ç°è±¡å‡å°‘äº†14.19%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04780v1">PDF</a> 9 pages,12 figures</p>
<p><strong>Summary</strong>ï¼šæç«¯å¤©æ°”äº‹ä»¶é¢‘å‘ï¼Œå¦‚é¾™å·é£ï¼Œå‡¸æ˜¾äº†é«˜æ•ˆå…¬å¹³åœ°æ¢å¤ç”µåŠ›ç³»ç»Ÿçš„ç´§è¿«éœ€æ±‚ã€‚å½“å‰ç”µåŠ›æ¢å¤å†³ç­–ä¸»è¦åŸºäºå„åœ°åŒºçš„ç”µåŠ›æ¢å¤è¯·æ±‚æ•°é‡ï¼Œä½†æ•°æ®åˆ†ææ˜¾ç¤ºåŠ£åŠ¿ç¤¾åŒºæäº¤çš„æ¢å¤è¯·æ±‚è¾ƒå°‘ï¼Œå¯¼è‡´å½“å‰è§£å†³æ–¹æ¡ˆä¸å…¬å¹³ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§å…¼é¡¾æ¢å¤æ•ˆç‡å’Œå…¬å¹³çš„ç”µåŠ›æ¢å¤ç­–ç•¥ï¼Œé¢ä¸´æ•°æ®é›†å¼‚æ–¹å·®æ€§å¯¼è‡´çš„ç»´ä¿®æ—¶é—´é¢„æµ‹éš¾åº¦å’Œå¼ºåŒ–å­¦ä¹ ä»£ç†å€¾å‘äºé€‰æ‹©ä½ä¸ç¡®å®šæ€§è¡ŒåŠ¨ä»¥ç ´åå…¬å¹³æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹ä¼˜åŒ–æ¡†æ¶EPOPRï¼ŒåŒ…æ‹¬ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç»´ä¿®æ—¶é—´é¢„æµ‹å’Œé€‚åº”ä¸åŒåœ°åŒºä¸ç¡®å®šæ€§æ°´å¹³çš„å…¬å¹³å†³ç­–åˆ¶å®šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEPOPRä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œå¹³å‡åœç”µæ—¶é—´å‡å°‘äº†3.60%ï¼Œä¸åŒç¤¾åŒºä¹‹é—´çš„ä¸å…¬å¹³æ€§å‡å°‘äº†14.19%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æç«¯å¤©æ°”äº‹ä»¶å‡¸æ˜¾äº†ç”µåŠ›æ¢å¤ç³»ç»Ÿéœ€è¦é«˜æ•ˆå…¬å¹³çš„æ¢å¤ç­–ç•¥ã€‚</li>
<li>å½“å‰ç”µåŠ›æ¢å¤å†³ç­–ä¸»è¦åŸºäºè¯·æ±‚æ•°é‡ï¼Œä½†åŠ£åŠ¿ç¤¾åŒºæäº¤çš„æ¢å¤è¯·æ±‚è¾ƒå°‘ï¼Œå¯¼è‡´è§£å†³æ–¹æ¡ˆä¸å…¬å¹³ã€‚</li>
<li>æå‡ºä¸€ç§ç”µåŠ›æ¢å¤ç­–ç•¥ï¼Œæ—¨åœ¨å¹³è¡¡æ¢å¤æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚</li>
<li>å®ç°è¯¥ç­–ç•¥é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ•°æ®é›†å¼‚æ–¹å·®æ€§å¯¼è‡´çš„ç»´ä¿®æ—¶é—´é¢„æµ‹éš¾åº¦å’Œå¼ºåŒ–å­¦ä¹ ä»£ç†çš„å†³ç­–å€¾å‘ã€‚</li>
<li>è®¾è®¡äº†é¢„æµ‹ä¼˜åŒ–æ¡†æ¶EPOPRæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ç»´ä¿®æ—¶é—´é¢„æµ‹å’Œé€‚åº”ä¸åŒåœ°åŒºä¸ç¡®å®šæ€§æ°´å¹³çš„å†³ç­–åˆ¶å®šã€‚</li>
<li>EPOPRä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œå¹³å‡åœç”µæ—¶é—´å‡å°‘äº†3.6%ï¼Œç¤¾åŒºé—´çš„ä¸å…¬å¹³æ€§å‡å°‘äº†14.19%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-67eb89f0591e38b1b04ea3fe67c9c734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c1ce7a4645dbd74d552b2936ab6e6d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c42c547dcc72b130a253708923f32d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c092f152ac23d122c35a7b0a8a27841a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec6452ae1e9ef2c8a5f39ee68ea05b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5214cf83ad8c0ae5a758d15aa6766ed0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-afe9d56898e80320a0d968e766efb68a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AttriLens-Mol-Attribute-Guided-Reinforcement-Learning-for-Molecular-Property-Prediction-with-Large-Language-Models"><a href="#AttriLens-Mol-Attribute-Guided-Reinforcement-Learning-for-Molecular-Property-Prediction-with-Large-Language-Models" class="headerlink" title="AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular   Property Prediction with Large Language Models"></a>AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular   Property Prediction with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang</strong></p>
<p>Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended &#96;&#96;thinkingâ€™â€™ process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the modelâ€™s reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the modelâ€™s inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in <a target="_blank" rel="noopener" href="https://github.com/szu-tera/AttriLens-Mol">https://github.com/szu-tera/AttriLens-Mol</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¾…åŠ©åˆ†å­å±æ€§é¢„æµ‹ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºäººå·¥åˆ¶ä½œçš„æç¤ºå’Œæ€ç»´é“¾æ¨¡æ¿ã€‚è™½ç„¶æœ€è¿‘çš„å…ˆè¿›çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼Œå¦‚DeepSeek-R1ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ›´é•¿çš„â€œæ€è€ƒâ€è¿‡ç¨‹ï¼Œä½†å®ƒä»¬çš„æ¨ç†å¯èƒ½è¿‡äºå†—é•¿ä¸”ç¼ºä¹ç›¸å…³æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†AttriLens-Molï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†å­å±æ€§é¢„æµ‹çš„ã€ä»¥å±æ€§ä¸ºå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚AttriLens-Molé€šè¿‡ä»¥ä¸‹æ–¹å¼å¼•å¯¼æ¨¡å‹çš„æ¨ç†ï¼šï¼ˆ1ï¼‰æ ¼å¼å¥–åŠ±é¼“åŠ±åŸºäºå±æ€§çš„ç»“æ„åŒ–è¾“å‡ºï¼›ï¼ˆ2ï¼‰è®¡æ•°å¥–åŠ±é¿å…åˆ—ä¸¾ä¸ç›¸å…³çš„å±æ€§ï¼›ï¼ˆ3ï¼‰åˆç†æ€§å¥–åŠ±ä½¿ç”¨é«˜çº§LLMå’ŒRDKitéªŒè¯ç”Ÿæˆå±æ€§çš„ç›¸å…³æ€§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿéšå¼åœ°æ¿€å‘æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ç›¸å…³åˆ†å­å±æ€§çš„å†…åœ¨çŸ¥è¯†ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¿›è¡Œåˆ†å­å±æ€§é¢„æµ‹ã€‚å¯¹å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„AttriLens-Molæ–¹æ³•å¯¹7Bè§„æ¨¡çš„R1-Distilled-Qwen2.5å’ŒR1-Distilled-LLaMA3.1æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œåœ¨4000ä¸ªæ ·æœ¬ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œè·å¾—äº†ä¸ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼ˆMol-Instructionsã€ChemDFMç­‰ï¼‰å’Œé«˜çº§æ¨¡å‹ï¼ˆGPT-3.5ã€GPT-4oã€DeepSeek-V3ã€DeepSeek-R1ç­‰ï¼‰ç›¸å½“æˆ–æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºç›®æ ‡å±æ€§æå–çš„å±æ€§ï¼Œå½“ç”¨ä½œå¯è§£é‡Šå†³ç­–æ ‘æ¨¡å‹çš„ç‰¹å¾æ—¶ï¼Œä¸é€šè¿‡æç¤ºLLMäº§ç”Ÿçš„å±æ€§ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚è¿™è¡¨æ˜AttriLens-Molæœ‰æ•ˆåœ°å¼•å‘äº†æ›´ç›¸å…³å’Œé¢„æµ‹æ€§çš„åˆ†å­å±æ€§ï¼Œæé«˜äº†å±æ€§é¢„æµ‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/szu-tera/AttriLens-Mol">https://github.com/szu-tera/AttriLens-Mol</a>å‘å¸ƒäº†ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04748v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­å±æ€§é¢„æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»éœ€äººå·¥æç¤ºå’Œæ€ç»´é“¾æ¨¡æ¿ã€‚æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ›´é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œä½†å­˜åœ¨æ¨ç†å†—é•¿å’Œç¼ºä¹ç›¸å…³æ€§çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºAttriLens-Molï¼Œä¸€ä¸ªç”¨äºåˆ†å­å±æ€§é¢„æµ‹çš„ã€ä»¥å±æ€§å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡æ ¼å¼å¥–åŠ±ã€è®¡æ•°å¥–åŠ±å’Œåˆç†æ€§å¥–åŠ±æ¥å¼•å¯¼æ¨¡å‹çš„æ¨ç†ï¼Œé¿å…äº†æšä¸¾ä¸ç›¸å…³å±æ€§ï¼Œå¹¶éªŒè¯äº†ç”Ÿæˆå±æ€§çš„ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨AttriLens-Molæ–¹æ³•è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä¸ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼ˆå¦‚Mol-Instructionsã€ChemDFMç­‰ï¼‰å’Œå…ˆè¿›æ¨¡å‹ï¼ˆå¦‚GPT-3.5ã€GPT-4oã€DeepSeek-V3ã€DeepSeek-R1ç­‰ï¼‰ç›¸æ¯”ï¼Œç»“æœç›¸å½“æˆ–æ›´å¥½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨AttriLens-Molæå–çš„ç›®æ ‡å±æ€§ä½œä¸ºç‰¹å¾ï¼Œç”¨äºå¯è§£é‡Šæ€§å†³ç­–æ ‘æ¨¡å‹æ—¶ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¯´æ˜AttriLens-Molèƒ½æ›´æœ‰æ•ˆåœ°å¼•å‡ºç›¸å…³ä¸”é¢„æµ‹æ€§çš„åˆ†å­å±æ€§ï¼Œæé«˜é¢„æµ‹å±æ€§çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­å±æ€§é¢„æµ‹ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†ä»éœ€æ”¹è¿›æ¨ç†è¿‡ç¨‹çš„å†—ä½™å’Œç›¸å…³æ€§é—®é¢˜ã€‚</li>
<li>AttriLens-Molæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œä½¿ç”¨å±æ€§å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œåˆ†å­å±æ€§é¢„æµ‹ã€‚</li>
<li>AttriLens-Molé€šè¿‡æ ¼å¼å¥–åŠ±ã€è®¡æ•°å¥–åŠ±å’Œåˆç†æ€§å¥–åŠ±æ¥å¼•å¯¼æ¨¡å‹æ¨ç†ã€‚</li>
<li>AttriLens-Molèƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­å±æ€§é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹å’Œæ–¹æ³•çš„æ¯”è¾ƒæ˜¾ç¤ºï¼ŒAttriLens-Molè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>AttriLens-Molæå–çš„å±æ€§ä½œä¸ºç‰¹å¾æ—¶ï¼Œå¯æ˜¾è‘—æé«˜å†³ç­–æ ‘æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>AttriLens-Molæœ‰æ•ˆåœ°å¼•å‡ºç›¸å…³ä¸”é¢„æµ‹æ€§çš„åˆ†å­å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe30ff0147ae33b00511a3d8c1eb845b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6160f081944d6bef4d07882e6e3c712d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2182ae26b415a9939c70037e18ffba56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10d6d8f493e4cd6de721923b5f516b87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07d92e93a87b9dc6a4a3ac47b1d8ad7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c06b1b80fab6e2f55d2dcbbce0d84ce.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GM-PRM-A-Generative-Multimodal-Process-Reward-Model-for-Multimodal-Mathematical-Reasoning"><a href="#GM-PRM-A-Generative-Multimodal-Process-Reward-Model-for-Multimodal-Mathematical-Reasoning" class="headerlink" title="GM-PRM: A Generative Multimodal Process Reward Model for Multimodal   Mathematical Reasoning"></a>GM-PRM: A Generative Multimodal Process Reward Model for Multimodal   Mathematical Reasoning</h2><p><strong>Authors:Jianghangfan Zhang, Yibo Yan, Kening Zheng, Xin Zou, Song Dai, Xuming Hu</strong></p>
<p>Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities but often struggle with complex, multi-step mathematical reasoning, where minor errors in visual perception or logical deduction can lead to complete failure. While Process Reward Models (PRMs) offer step-by-step supervision, existing multimodal PRMs are limited to being binary verifiers that can identify but not correct errors, offering little explanatory power. To address these deficiencies, we introduce the Generative Multimodal Process Reward Model (GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an active reasoning collaborator. Instead of a simple scalar score, GM-PRM provides a fine-grained, interpretable analysis of each reasoning step, evaluating its step intent, visual alignment, and logical soundness. More critically, GM-PRM is trained to generate a corrected version of the first erroneous step it identifies. This unique corrective capability enables our new test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework actively enhances solution quality by using the PRMâ€™s generated correction to guide the policy model toward a more promising reasoning trajectory, thereby improving the diversity and correctness of the solution pool. We demonstrate that GM-PRM achieves state-of-the-art results on multiple multimodal math benchmarks, significantly boosting policy model performance with remarkable data efficiency, requiring only a 20K-sample training dataset. Our code will be released upon acceptance. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¡¨ç°å‡ºäº†ä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚çš„å¤šæ­¥éª¤æ•°å­¦æ¨ç†æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œè§†è§‰æ„ŸçŸ¥æˆ–é€»è¾‘æ¨æ–­ä¸­çš„å°é”™è¯¯éƒ½å¯èƒ½å¯¼è‡´å®Œå…¨å¤±è´¥ã€‚è™½ç„¶è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æä¾›äº†é€æ­¥ç›‘ç£ï¼Œä½†ç°æœ‰çš„å¤šæ¨¡æ€PRMä»…é™äºä½œä¸ºäºŒè¿›åˆ¶éªŒè¯å™¨ï¼Œåªèƒ½è¯†åˆ«é”™è¯¯è€Œæ— æ³•çº æ­£é”™è¯¯ï¼Œè§£é‡ŠåŠ›å¾ˆå°ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”Ÿæˆå¼å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆGM-PRMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†PRMä»è¢«åŠ¨åˆ¤æ–­è€…è½¬å˜ä¸ºç§¯ææ¨ç†åˆä½œè€…çš„æ–°å‹èŒƒå¼ã€‚GM-PRMä¸å†æä¾›ç®€å•çš„æ ‡é‡åˆ†æ•°ï¼Œè€Œæ˜¯å¯¹æ¯ä¸€æ­¥æ¨ç†è¿›è¡Œç²¾ç»†çš„ã€å¯è§£é‡Šçš„åˆ†æï¼Œè¯„ä¼°å…¶æ­¥éª¤æ„å›¾ã€è§†è§‰å¯¹é½å’Œé€»è¾‘åˆç†æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒGM-PRMç»è¿‡è®­ç»ƒï¼Œå¯ä»¥ç”Ÿæˆå…¶è¯†åˆ«çš„ç¬¬ä¸€ä¸ªé”™è¯¯æ­¥éª¤çš„ä¿®æ­£ç‰ˆæœ¬ã€‚è¿™ç§ç‹¬ç‰¹çš„çº æ­£èƒ½åŠ›ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡‡ç”¨æ–°çš„æµ‹è¯•æ—¶é—´æ¨ç†ç­–ç•¥ï¼Œå³ç²¾ç»†åŒ–æœ€ä½³Nç­–ç•¥ï¼ˆRefined-BoNï¼‰ã€‚è¯¥æ¡†æ¶ç§¯ææé«˜äº†è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼Œåˆ©ç”¨PRMç”Ÿæˆçš„ä¿®æ­£æ¥å¼•å¯¼æ”¿ç­–æ¨¡å‹èµ°å‘æ›´æœ‰å‰é€”çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæé«˜äº†è§£å†³æ–¹æ¡ˆæ± çš„å¤šæ ·æ€§å’Œæ­£ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¯æ˜äº†GM-PRMå®ç°äº†æœ€æ–°ç»“æœï¼Œæ˜¾è‘—æé«˜äº†æ”¿ç­–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸”æ•°æ®æ•ˆç‡æƒŠäººï¼Œåªéœ€è¦ä¸€ä¸ª2ä¸‡æ ·æœ¬çš„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨é€šè¿‡åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04088v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚å¤šæ­¥éª¤æ•°å­¦æ¨ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä»¥åŠç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¼å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆGM-PRMï¼‰ï¼Œè¯¥æ¨¡å‹å°†PRMä»è¢«åŠ¨åˆ¤æ–­è½¬å˜ä¸ºç§¯ææ¨ç†åˆä½œè€…ã€‚GM-PRMä¸ä»…èƒ½æä¾›æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ç²¾ç»†å¯è§£é‡Šæ€§åˆ†æï¼Œè¿˜èƒ½å¯¹ç¬¬ä¸€æ­¥ä¸­çš„é”™è¯¯è¿›è¡Œä¿®æ­£ï¼Œå¹¶é€šè¿‡æ”¹è¿›çš„æœ€ä½³Nç­–ç•¥æé«˜ç­–ç•¥æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æˆæœï¼Œè¡¨ç°å‡ºæƒŠäººçš„æ•°æ®æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsåœ¨å¤„ç†å¤æ‚å¤šæ­¥éª¤æ•°å­¦æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¾®å°çš„è§†è§‰æ„ŸçŸ¥æˆ–é€»è¾‘æ¨å¯¼é”™è¯¯å¯èƒ½å¯¼è‡´å®Œå…¨å¤±è´¥ã€‚</li>
<li>ç°æœ‰PRMä»…é™äºä½œä¸ºäºŒè¿›åˆ¶éªŒè¯å™¨ï¼Œåªèƒ½è¯†åˆ«é”™è¯¯è€Œæ— æ³•çº æ­£ï¼Œç¼ºä¹è§£é‡ŠåŠ›ã€‚</li>
<li>GM-PRMæ˜¯ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œå°†PRMä»è¢«åŠ¨åˆ¤æ–­è½¬å˜ä¸ºç§¯ææ¨ç†åˆä½œè€…ï¼Œæä¾›ç²¾ç»†çš„å¯è§£é‡Šæ€§åˆ†æå¹¶çº æ­£é”™è¯¯æ¨ç†æ­¥éª¤ã€‚</li>
<li>GM-PRMé€šè¿‡æ”¹è¿›çš„æœ€ä½³Nç­–ç•¥ï¼ˆRefined-BoNï¼‰æé«˜ç­–ç•¥æ¨¡å‹çš„æ€§èƒ½ï¼Œæå‡è§£å†³æ–¹æ¡ˆçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>GM-PRMåœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æˆæœï¼Œæ˜¾ç¤ºå‡ºè‰²çš„æ•°æ®æ•ˆç‡ï¼Œä»…éœ€20Kæ ·æœ¬è®­ç»ƒé›†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-389c3ca98abc689c838d5822ae644c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8da2cf6df9630684005dde0be3ea6c08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe361fa67d4f03c4bedc9839673e4566.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70e90567d5e6d5690d939d9f76b2c33c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Sotopia-RL-Reward-Design-for-Social-Intelligence"><a href="#Sotopia-RL-Reward-Design-for-Social-Intelligence" class="headerlink" title="Sotopia-RL: Reward Design for Social Intelligence"></a>Sotopia-RL: Reward Design for Social Intelligence</h2><p><strong>Authors:Haofei Yu, Zhengyang Qi, Yining Zhao, Kolby Nottingham, Keyang Xuan, Bodhisattwa Prasad Majumder, Hao Zhu, Paul Pu Liang, Jiaxuan You</strong></p>
<p>Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/sotopia-lab/sotopia-rl">https://github.com/sotopia-lab/sotopia-rl</a>. </p>
<blockquote>
<p>ç¤¾ä¼šæ™ºèƒ½å·²ç»æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å‚ä¸ç°å®ä¸–ç•Œçš„ç¤¾ä¼šä»»åŠ¡ï¼Œä¾‹å¦‚ä½å®¿ã€åŠè¯´ã€åä½œå’Œè°ˆåˆ¤ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯è®­ç»ƒç¤¾ä¼šæ™ºèƒ½ä»£ç†çš„è‡ªç„¶é€‰æ‹©ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹é€šè¿‡ç¤¾ä¼šäº’åŠ¨ç›´æ¥å­¦ä¹ å¤æ‚ç­–ç•¥ã€‚ç„¶è€Œï¼Œç¤¾ä¼šäº’åŠ¨å…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾ï¼Œä¸ºRLè®­ç»ƒè®¾ç½®äº†éšœç¢ï¼šï¼ˆ1ï¼‰éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œå³è¨€è®ºå…·æœ‰é—´æ¥å’Œå»¶è¿Ÿçš„å½±å“ï¼Œä½¿ä¿¡ç”¨åˆ†é…å˜å¾—å¤æ‚ï¼›ï¼ˆ2ï¼‰å¤šç»´æ€§ï¼Œå…¶ä¸­å»ºç«‹èæ´½å…³ç³»æˆ–å¯»æ±‚çŸ¥è¯†ç­‰è¡Œä¸ºé—´æ¥ä¿ƒè¿›ç›®æ ‡å®ç°ã€‚è¿™äº›ç‰¹å¾ä½¿å¾—åŸºäºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰çš„RLä¸å•ç»´æƒ…èŠ‚çº§å¥–åŠ±æ•ˆç‡ä½ä¸‹ä¸”ä¸ç¨³å®šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Sotopia-RLï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒå°†ç²—ç•¥çš„æƒ…èŠ‚çº§åé¦ˆç»†åŒ–ä¸ºè¯è¯­çº§çš„å¤šç»´å¥–åŠ±ã€‚è¯è¯­çº§ä¿¡ç”¨åˆ†é…é€šè¿‡å°†ç»“æœå½’äºå•ä¸ªè¨€è®ºæ¥å‡è½»éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œè€Œå¤šç»´å¥–åŠ±æ•æ‰ç¤¾ä¼šäº’åŠ¨çš„å…¨éƒ¨ä¸°å¯Œæ€§å¹¶å‡å°‘å¥–åŠ±ç ´è§£ã€‚åœ¨Sotopiaï¼ˆä¸€ä¸ªå¼€æ”¾çš„ç¤¾ä¼šå­¦ä¹ ç¯å¢ƒï¼‰çš„å®éªŒä¸­ï¼ŒSotopia-RLå®ç°äº†æœ€å…ˆè¿›çš„ç¤¾äº¤ç›®æ ‡å®Œæˆåˆ†æ•°ï¼ˆSotopia-hardä¸Šä¸º7.17ï¼ŒSotopia-fullä¸Šä¸º8.31ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¯å®äº†è¯è¯­çº§ä¿¡ç”¨åˆ†é…å’Œå¤šç»´å¥–åŠ±è®¾è®¡å¯¹äºRLè®­ç»ƒçš„å¿…è¦æ€§ã€‚æˆ‘ä»¬çš„å®ç°å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/sotopia-lab/sotopia-rl%E3%80%82">https://github.com/sotopia-lab/sotopia-rlã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆ</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03905v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong>ï¼š<br>ç¤¾ä¼šæ™ºèƒ½å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´å·²ç»æˆä¸ºä¸€é¡¹è‡³å…³é‡è¦çš„èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€‚åˆè®­ç»ƒå…·æœ‰ç¤¾ä¼šæ™ºèƒ½çš„ä»£ç†ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹é€šè¿‡ç›´æ¥çš„ç¤¾ä¼šäº’åŠ¨å­¦ä¹ å¤æ‚ç­–ç•¥ã€‚ç„¶è€Œï¼Œç¤¾ä¼šäº’åŠ¨å…·æœ‰éƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’Œå¤šç»´æ€§ï¼Œè¿™æ„æˆäº†å¯¹RLè®­ç»ƒçš„éšœç¢ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Sotopia-RLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç²—ç•¥çš„ç‰‡æ®µçº§åé¦ˆç»†åŒ–ä¸ºè¨€è¯­çº§çš„å¤šç»´å¥–åŠ±ã€‚è¨€è¯­çº§çš„ä¿¡ç”¨åˆ†é…é€šè¿‡ä¸ªäººè¨€è¯­å½’å±ç»“æœæ¥ç¼“è§£éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œè€Œå¤šç»´å¥–åŠ±æ•æ‰ç¤¾ä¼šäº’åŠ¨çš„å…¨éƒ¨ä¸°å¯Œæ€§å¹¶å‡å°‘å¥–åŠ±ç ´è§£ã€‚åœ¨Sotopiaå¼€æ”¾ç¤¾äº¤å­¦ä¹ ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒSotopia-RLå®ç°äº†å…ˆè¿›çš„ç¤¾ä¼šç›®æ ‡å®Œæˆåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¤¾ä¼šæ™ºèƒ½å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½¿å…¶èƒ½æœ‰æ•ˆå®Œæˆç°å®ç¤¾äº¤ä»»åŠ¡ï¼Œå¦‚åè°ƒã€åŠè¯´ã€åˆä½œå’Œè°ˆåˆ¤ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€‚åˆè®­ç»ƒç¤¾ä¼šæ™ºèƒ½ä»£ç†ï¼Œèƒ½ç›´æ¥é€šè¿‡ç¤¾ä¼šäº’åŠ¨å­¦ä¹ å¤æ‚ç­–ç•¥ã€‚</li>
<li>ç¤¾ä¼šäº’åŠ¨å…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾ï¼šéƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’Œå¤šç»´æ€§ï¼Œä¸ºRLè®­ç»ƒè®¾ç½®éšœç¢ã€‚</li>
<li>Sotopia-RLæ¡†æ¶é€šè¿‡ç»†åŒ–ç²—ç•¥çš„ç‰‡æ®µçº§åé¦ˆä¸ºè¨€è¯­çº§çš„å¤šç»´å¥–åŠ±æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>è¨€è¯­çº§çš„ä¿¡ç”¨åˆ†é…é€šè¿‡ä¸ªäººè¨€è¯­å½’å±ç»“æœç¼“è§£éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€‚</li>
<li>å¤šç»´å¥–åŠ±æ•æ‰ç¤¾ä¼šäº’åŠ¨çš„å…¨éƒ¨ä¸°å¯Œæ€§ï¼Œå‡å°‘å¥–åŠ±ç ´è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36cba909bee506edd2cb59d884400d58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3cf026a6fa6ed42d3fe09ea7d146e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5292a9dac1897ccbbf695127f4029ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4ffc8f6be46555db410a075e813cde.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MoKA-Mixture-of-Kronecker-Adapters"><a href="#MoKA-Mixture-of-Kronecker-Adapters" class="headerlink" title="MoKA: Mixture of Kronecker Adapters"></a>MoKA: Mixture of Kronecker Adapters</h2><p><strong>Authors:Mohammadreza Sadeghi, Mahsa Ghazvini Nejad, MirHamed Jafarzadeh Asl, Yu Gu, Yuanhao Yu, Masoud Asgharian, Vahid Partovi Nia</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—å¼€é”€è‡³å…³é‡è¦ã€‚ä½é˜¶å®¶æ—é€‚é…å™¨é€šå¸¸ç”¨äºæœ‰æ•ˆåœ°æ§åˆ¶å‚æ•°å¤§å°ï¼ŒåŒæ—¶ä¿æŒLLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºé˜¶æ•°é™åˆ¶ï¼Œå®ƒä»¬çš„è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œå¾€å¾€åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†æ··åˆå…‹ç½—å†…å…‹é€‚é…å™¨ï¼ˆMoKAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°ä¸€ä»£å…‹ç½—å†…å…‹é€‚é…å™¨ï¼Œé€šè¿‡æ¨¡æ‹Ÿæƒé‡æ›´æ–°ä½œä¸ºå…‹ç½—å†…å…‹äº§å“çš„æ··åˆæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºçš„é€‚é…å™¨åˆ©ç”¨é—¨æ§æœºåˆ¶æ¥è¡¡é‡æ¯ä¸ªå…‹ç½—å†…å…‹å› å­çš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ›´å…·è¡¨ç°åŠ›çš„é€‚é…ã€‚æ­¤å¤–ï¼ŒMoKAå®ç°äº†é˜¶æ•°çµæ´»æ€§ï¼Œåœ¨å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ã€‚ä¸ºäº†ç¡®ä¿ç¡¬ä»¶æ•ˆç‡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ ‡å‡†çŸ©é˜µè¿ç®—é‡æ–°åˆ¶å®šäº†å…‹ç½—å†…å…‹è®¡ç®—ï¼Œå¯ä»¥æ— ç¼éƒ¨ç½²åœ¨GPUä¼˜åŒ–çš„ç¡¬ä»¶ä¸Šã€‚æˆ‘ä»¬åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä½¿ç”¨LLaMA2-7Bå’ŒLLaMA3-8Bæ¨¡å‹çš„ä½æ¯”ç‰¹é‡åŒ–ç‰ˆæœ¬ã€‚MoKAä¸ä»…ä¼˜äºPEFTåŸºçº¿ï¼Œè¿˜å°†è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘äº†é«˜è¾¾27å€ï¼Œåœ¨å®ç°æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€å…ˆè¿›çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03527v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—å¼€é”€è‡³å…³é‡è¦ã€‚ä½ç§©å®¶æ—é€‚é…å™¨å¸¸ç”¨äºæœ‰æ•ˆåœ°æ§åˆ¶å‚æ•°å¤§å°ï¼ŒåŒæ—¶ä¿æŒLLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç§©çš„é™åˆ¶ï¼Œå®ƒä»¬åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠ›å¾€å¾€å—é™ã€‚æˆ‘ä»¬æå‡ºäº†æ··åˆå…‹ç½—å†…å…‹é€‚é…å™¨ï¼ˆMoKAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°ä¸€ä»£å…‹ç½—å†…å…‹é€‚é…å™¨ï¼Œé€šè¿‡å°†æƒé‡æ›´æ–°å»ºæ¨¡ä¸ºå…‹ç½—å†…å…‹äº§å“çš„æ··åˆæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚æ‰€æå‡ºçš„é€‚é…å™¨åˆ©ç”¨é—¨æ§æœºåˆ¶æ¥è¡¡é‡æ¯ä¸ªå…‹ç½—å†…å…‹å› å­çš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ›´ä¸°å¯Œçš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼ŒMoKAå®ç°äº†ç§©çµæ´»æ€§ï¼Œåœ¨å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ã€‚ä¸ºç¡®ä¿ç¡¬ä»¶æ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çŸ©é˜µæ“ä½œé‡æ–°åˆ¶å®šå…‹ç½—å†…å…‹è®¡ç®—ï¼Œå¯åœ¨GPUä¼˜åŒ–ç¡¬ä»¶ä¸Šè¿›è¡Œæ— ç¼éƒ¨ç½²ã€‚æˆ‘ä»¬åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå¯¹LLaMAçš„ä½æ¯”ç‰¹é‡åŒ–ç‰ˆæœ¬è¿›è¡Œäº†å¤§é‡å®éªŒã€‚MoKAä¸ä»…ä¼˜äºPEFTåŸºçº¿ï¼Œè¿˜å°†è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘è‡³åŸæ¥çš„27å€ï¼Œå®ç°äº†æ€§èƒ½ä¸å‚æ•°æ•ˆç‡ä¹‹é—´çš„æœ€æ–°æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾ˆé‡è¦ï¼Œèƒ½å‡å°‘è®¡ç®—å¼€é”€ã€‚</li>
<li>ä½ç§©å®¶æ—é€‚é…å™¨èƒ½æœ‰æ•ˆæ§åˆ¶å‚æ•°å¤§å°å¹¶ç»´æŒLLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™ã€‚</li>
<li>Mixture of Kronecker Adapters (MoKA) é€šè¿‡å»ºæ¨¡æƒé‡æ›´æ–°ä¸ºå…‹ç½—å†…å…‹äº§å“çš„æ··åˆæ¥è§£å†³ä½ç§©é€‚é…å™¨çš„é™åˆ¶ã€‚</li>
<li>MoKAåˆ©ç”¨é—¨æ§æœºåˆ¶è¡¡é‡å…‹ç½—å†…å…‹å› å­çš„é‡è¦æ€§ï¼Œå®ç°æ›´ä¸°å¯Œçš„é€‚åº”æ€§ã€‚</li>
<li>MoKAåœ¨å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ï¼Œå®ç°äº†ç§©çµæ´»æ€§ã€‚</li>
<li>MoKAé€šè¿‡æ ‡å‡†çŸ©é˜µæ“ä½œè¿›è¡Œå…‹ç½—å†…å…‹è®¡ç®—æ”¹é©ï¼Œç¡®ä¿ç¡¬ä»¶æ•ˆç‡å¹¶åœ¨GPUä¸Šæ— ç¼éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4443f751a71a452718be124dbfef1d9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-630e237ca6f6c10aa0615ef71d011781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d55f986e80ffc090350d0401166bf4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd277e69c722ab5f5e72e17a26b1caa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22ac4a556347ba46b204e5e5c6249821.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning"><a href="#Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning" class="headerlink" title="Training Long-Context, Multi-Turn Software Engineering Agents with   Reinforcement Learning"></a>Training Long-Context, Multi-Turn Software Engineering Agents with   Reinforcement Learning</h2><p><strong>Authors:Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, Sergei Skvortsov, Boris Yangel</strong></p>
<p>Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation.   To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agentâ€™s success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models. </p>
<blockquote>
<p>å…³äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•è½®é—®é¢˜ä¸Šï¼Œå¦‚æ•°å­¦æ¨ç†æˆ–å•æ¬¡ä»£ç ç”Ÿæˆã€‚è™½ç„¶è¿™äº›é—®é¢˜å¯ä»¥è¢«è§†ä¸ºä»¤ç‰Œçº§çš„å¤šè½®MDPï¼Œä½†è¿™ç§è§‚ç‚¹å¯¹åº”äºå¤šè½®äº¤äº’çš„é€€åŒ–æƒ…å†µï¼Œå³ç¯å¢ƒä¸æä¾›åé¦ˆã€‚è¿™ä¸è®¸å¤šç°å®ä¸–ç•Œé¢†åŸŸå½¢æˆå¯¹æ¯”ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ï¼Œå®ƒè¦æ±‚ä¸å…·æœ‰çŠ¶æ€çš„ç¯å¢ƒè¿›è¡Œä¸°å¯Œçš„å¤šè½®äº¤äº’ï¼Œå¹¶æ ¹æ®æ¯ä¸ªåŠ¨ä½œäº§ç”Ÿéå¾®ä¸è¶³é“çš„è§‚å¯Ÿç»“æœã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å°†å¼ºåŒ–å­¦ä¹ æˆåŠŸåº”ç”¨äºè¿™ä¸€é€šç”¨ä½“ç³»ã€‚ä½¿ç”¨ç»è¿‡ä¿®æ”¹çš„è§£è€¦ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ï¼Œæˆ‘ä»¬åŸºäºQwen2.5-72B-Instructè®­ç»ƒäº†ä¸€ä¸ªæ™ºèƒ½ä½“æ¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ™ºèƒ½ä½“åœ¨SWE-bench VerifiedåŸºå‡†æµ‹è¯•ä¸Šçš„æˆåŠŸç‡ï¼Œä»æ‹’ç»å¾®è°ƒåŸºå‡†çš„20%æé«˜åˆ°39%ï¼Œå¹¶ä¸”æ²¡æœ‰ä¾èµ–ä»»ä½•æ•™å¸ˆæ¨¡å‹ã€‚åœ¨SWE-rebenchä¸Šï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“ä½¿ç”¨ç›¸åŒçš„æ¶æ„ä¸é¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼ˆå¦‚DeepSeek-V3-0324å’ŒQwen3-235B-A22Bï¼‰ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ï¼Œä¸ºåŸºäºå¼€æºæ¨¡å‹æ„å»ºèƒ½å¤Ÿè§£å†³å¤æ‚ç°å®ä¸–ç•Œé—®é¢˜çš„æ›´å¼ºå¤§è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†å¯è¡Œçš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ä¸»è¦é›†ä¸­åœ¨å•å›åˆé—®é¢˜ä¸Šï¼Œå¦‚æ•°å­¦æ¨ç†æˆ–å•æ¬¡ä»£ç ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„é—®é¢˜ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ï¼Œéœ€è¦ä¸°å¯Œçš„å¤šå›åˆäº¤äº’ã€‚æœ¬ç ”ç©¶æˆåŠŸå°†RLåº”ç”¨äºè¿™ä¸€é¢†åŸŸï¼Œé‡‡ç”¨æ”¹è¿›çš„å»è€¦åˆä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ï¼ŒåŸºäºQwen2.5-72B-Instructè®­ç»ƒçš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡æ™ºèƒ½ä½“æˆåŠŸç‡æé«˜æ˜¾è‘—ã€‚æ­¤ç ”ç©¶å±•ç°äº†è§£å†³å¤æ‚ç°å®ä¸–ç•Œé—®é¢˜çš„å¯è¡Œè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä¸»è¦é›†ä¸­åœ¨å•å›åˆé—®é¢˜ä¸Šã€‚</li>
<li>ç°å®ä¸–ç•Œé—®é¢˜å¦‚è½¯ä»¶å·¥ç¨‹éœ€è¦ä¸°å¯Œçš„å¤šå›åˆäº¤äº’ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ”¹è¿›çš„å»è€¦åˆä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ç®—æ³•ã€‚</li>
<li>åŸºäºQwen2.5-72B-Instructè®­ç»ƒæ™ºèƒ½ä½“è§£å†³è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚</li>
<li>æ™ºèƒ½ä½“åœ¨SWE-bench Verifiedä¸Šçš„æˆåŠŸç‡ä»åŸºçº¿æå‡åˆ°39%ã€‚</li>
<li>ä¸ä¾èµ–æ•™å¸ˆæ¨¡å‹å®ç°äº†æå‡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-628a59a5f9593b95bdcdd66e144fd2f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c97da8446c806bb4fc879baf566ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddfb163b7290ff0304b665d616842307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3879b4f9ba25a8a6d19066f513e37b9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair"><a href="#On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair" class="headerlink" title="On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair"></a>On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair</h2><p><strong>Authors:Dong wang, Junji Yu, Honglin Shu, Michael Fu, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen</strong></p>
<p>Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C&#x2F;C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitations. Although some work has begun to explore LLMsâ€™ repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows Go consistently achieves the highest effectiveness across all model types, while C&#x2F;C++ performs the worst. Based on findings, we discuss the promise of LLM on multilingual vulnerability repair and the reasons behind LLMâ€™s failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs for multilingual vulnerability repair. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å„ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ–¹æ³•å·²è¢«æå‡ºï¼Œç”¨äºè‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…é™äºç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æä¾›äº†ä¸è¯­è¨€æ— å…³çš„èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œæ˜¾ç¤ºå‡ºå…‹æœå¤šè¯­è¨€æ¼æ´é™åˆ¶çš„æ½œåŠ›ã€‚å°½ç®¡ä¸€äº›å·¥ä½œå·²ç»å¼€å§‹æ¢ç´¢LLMçš„ä¿®å¤æ€§èƒ½ï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè°ƒæŸ¥äº†è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•å’Œæœ€å‰æ²¿çš„LLMåœ¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡æŒ‡ä»¤å¾®è°ƒä¸”ä½¿ç”¨å°‘æ•°æç¤ºè¿›è¡Œå¼•å¯¼è®­ç»ƒçš„GPT-4oï¼Œåœ¨ä¸é¢†å…ˆçš„æ–¹æ³•VulMasterç«äº‰ä¸­è¡¨ç°å¼ºåŠ²ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æ›´æœ‰å¯èƒ½ä¿®å¤æœ€å±é™©çš„æ¼æ´ã€‚ç»è¿‡æŒ‡ä»¤è°ƒæ ¡çš„GPT-4oåœ¨ä¹‹å‰æœªè§è¯­è¨€çš„æ¼æ´ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼ŒGoåœ¨æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­å§‹ç»ˆå®ç°æœ€é«˜æ•ˆæœï¼Œè€ŒC&#x2F;C++è¡¨ç°æœ€å·®ã€‚åŸºäºç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬è®¨è®ºäº†LLMåœ¨å¤šè¯­è¨€æ¼æ´ä¿®å¤æ–¹é¢çš„å‰æ™¯ä»¥åŠLLMå¤±è´¥æ¡ˆä¾‹èƒŒåçš„åŸå› ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡è·¨å¤šç§è¯­è¨€ç ”ç©¶ä¿®å¤æ–¹æ³•å’ŒLLMï¼Œçªæ˜¾äº†é‡‡ç”¨LLMè¿›è¡Œå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„å¹¿é˜”å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03470v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ·±åº¦å­¦ä¹ å·²åº”ç”¨äºè‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´ï¼Œä½†ä»…é™äºç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æä¾›äº†è·¨è¯­è¨€çš„ä¿®å¤èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ½œåœ¨ä¼˜åŠ¿ã€‚ç ”ç©¶å¯¹æ¯”äº†å¤šç§è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•å’Œå…ˆè¿›çš„LLMæ¨¡å‹åœ¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒæŒ‡ä»¤å¾®è°ƒåçš„GPT-4oåœ¨å°‘æ•°æç¤ºä¸‹è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶ä¸”LLMæ–¹æ³•æ›´æ“…é•¿ä¿®å¤ç‹¬ç‰¹å’Œå±é™©çš„æ¼æ´ã€‚åˆ†æå‘ç°Goè·å¾—æœ€é«˜çš„ä¿®å¤æ•ˆæœï¼Œè€ŒC&#x2F;C++è¡¨ç°æœ€å·®ã€‚æœ¬æ–‡è®¨è®ºäº†LLMåœ¨å¤šè¯­è¨€æ¼æ´ä¿®å¤ä¸­çš„æ½œåŠ›åŠå…¶å¤±è´¥åŸå› ï¼Œå¹¶é¦–æ¬¡åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹å¯¹æ¯”äº†ä¿®å¤æ–¹æ³•å’ŒLLMæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ å·²ç”¨äºè‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´ï¼Œä½†ä»…é™äºç‰¹å®šç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡è·¨è¯­è¨€ä¿®å¤èƒ½åŠ›ï¼Œå¯¹å…‹æœå¤šè¯­è¨€æ¼æ´é™åˆ¶å±•ç°æ½œåŠ›ã€‚</li>
<li>GPT-4oåœ¨æŒ‡ä»¤å¾®è°ƒåè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¯ä¸é¢†å…ˆçš„VulMasteræ–¹æ³•ç›¸æŠ—è¡¡ã€‚</li>
<li>LLMæ–¹æ³•æ›´æ“…é•¿ä¿®å¤ç‹¬ç‰¹å’Œå±é™©çš„æ¼æ´ã€‚</li>
<li>åœ¨æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­ï¼ŒGoè·å¾—æœ€é«˜çš„ä¿®å¤æ•ˆæœï¼Œè€ŒC&#x2F;C++è¡¨ç°æœ€å·®ã€‚</li>
<li>LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ¼æ´ä¿®å¤å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0645a16447deda0f74b821db44e52506.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4100e5b2b0e7f651825c8d1450308a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f21042fafdcf88c85716f9cf596b5a57.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Neurosymbolic-AI-Approaches-to-Interpretable-Logical-Reasoning"><a href="#A-Comparative-Study-of-Neurosymbolic-AI-Approaches-to-Interpretable-Logical-Reasoning" class="headerlink" title="A Comparative Study of Neurosymbolic AI Approaches to Interpretable   Logical Reasoning"></a>A Comparative Study of Neurosymbolic AI Approaches to Interpretable   Logical Reasoning</h2><p><strong>Authors:Michael K. Chen</strong></p>
<p>General logical reasoning, defined as the ability to reason deductively on domain-agnostic tasks, continues to be a challenge for large language models (LLMs). Current LLMs fail to reason deterministically and are not interpretable. As such, there has been a recent surge in interest in neurosymbolic AI, which attempts to incorporate logic into neural networks. We first identify two main neurosymbolic approaches to improving logical reasoning: (i) the integrative approach comprising models where symbolic reasoning is contained within the neural network, and (ii) the hybrid approach comprising models where a symbolic solver, separate from the neural network, performs symbolic reasoning. Both contain AI systems with promising results on domain-specific logical reasoning benchmarks. However, their performance on domain-agnostic benchmarks is understudied. To the best of our knowledge, there has not been a comparison of the contrasting approaches that answers the following question: Which approach is more promising for developing general logical reasoning? To analyze their potential, the following best-in-class domain-agnostic models are introduced: Logic Neural Network (LNN), which uses the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the hybrid approach. Using both models as case studies and representatives of each approach, our analysis demonstrates that the hybrid approach is more promising for developing general logical reasoning because (i) its reasoning chain is more interpretable, and (ii) it retains the capabilities and advantages of existing LLMs. To support future works using the hybrid approach, we propose a generalizable framework based on LLM-SS that is modular by design, model-agnostic, domain-agnostic, and requires little to no human input. </p>
<blockquote>
<p>é€šç”¨é€»è¾‘æ¨ç†ï¼Œè¢«å®šä¹‰ä¸ºèƒ½åœ¨é¢†åŸŸæ— å…³çš„ä»»åŠ¡ä¸Šè¿›è¡Œæ¼”ç»æ¨ç†çš„èƒ½åŠ›ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ— æ³•ç¡®å®šæ€§åœ°è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸”ä¸å¯è§£é‡Šã€‚å› æ­¤ï¼Œç¥ç»ç¬¦å·äººå·¥æ™ºèƒ½è¿‘æœŸå¼•èµ·äº†äººä»¬çš„æå¤§å…´è¶£ï¼Œå®ƒè¯•å›¾å°†é€»è¾‘èå…¥ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬é¦–å…ˆç¡®å®šäº†ä¸¤ç§ä¸»è¦çš„ç¥ç»ç¬¦å·æ–¹æ³•æ¥æ”¹è¿›é€»è¾‘æ¨ç†ï¼šï¼ˆiï¼‰åŒ…å«æ¨¡å‹åœ¨å†…çš„æ•´åˆæ–¹æ³•ï¼Œå…¶ä¸­ç¬¦å·æ¨ç†åŒ…å«åœ¨ç¥ç»ç½‘ç»œä¸­ï¼›ï¼ˆiiï¼‰æ··åˆæ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªä¸ç¥ç»ç½‘ç»œåˆ†å¼€çš„ç¬¦å·æ±‚è§£å™¨æ¥è¿›è¡Œç¬¦å·æ¨ç†ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½åŒ…å«åœ¨æœ‰å‰é€”çš„AIç³»ç»Ÿä¸­ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šé¢†åŸŸçš„é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é¢†åŸŸæ— å…³åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿˜æ²¡æœ‰å¯¹æ¯”è¿™ä¸¤ç§ä¸åŒæ–¹æ³•çš„æ¯”è¾ƒï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼šå“ªç§æ–¹æ³•å¯¹äºå¼€å‘é€šç”¨é€»è¾‘æ¨ç†æ›´æœ‰å‰é€”ï¼Ÿä¸ºäº†åˆ†æå®ƒä»¬çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä»¥ä¸‹æœ€ä½³é¢†åŸŸæ— å…³æ¨¡å‹ï¼šé‡‡ç”¨æ•´åˆæ–¹æ³•çš„é€»è¾‘ç¥ç»ç½‘ç»œï¼ˆLNNï¼‰å’Œé‡‡ç”¨æ··åˆæ–¹æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¬¦å·æ±‚è§£å™¨ï¼ˆLLM-SSï¼‰ã€‚é€šè¿‡ä»¥è¿™ä¸¤ä¸ªæ¨¡å‹ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶å¹¶ä»£è¡¨å„è‡ªçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ··åˆæ–¹æ³•å¯¹äºå¼€å‘é€šç”¨é€»è¾‘æ¨ç†æ›´æœ‰å‰é€”ï¼Œå› ä¸ºï¼ˆiï¼‰å…¶æ¨ç†é“¾æ›´å¯è§£é‡Šï¼Œï¼ˆiiï¼‰å®ƒä¿ç•™äº†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å’Œä¼˜åŠ¿ã€‚ä¸ºäº†æ”¯æŒä½¿ç”¨æ··åˆæ–¹æ³•çš„æœªæ¥å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºLLM-SSçš„å¯æ³›åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æŒ‰è®¾è®¡æ˜¯æ¨¡å—åŒ–çš„ã€æ¨¡å‹æ— å…³ã€é¢†åŸŸæ— å…³ï¼Œå¹¶ä¸”å‡ ä¹ä¸éœ€è¦äººå·¥è¾“å…¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03366v1">PDF</a> Accepted to NeSy 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é€»è¾‘æ¨ç†æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä»‹ç»äº†ä¸¤ç§æ”¹å–„é€»è¾‘æ¨ç†çš„ç¥ç»ç¬¦å·æ–¹æ³•ï¼šé›†æˆæ–¹æ³•å’Œæ··åˆæ–¹æ³•ã€‚é€šè¿‡å¯¹æœ€ä½³é¢†åŸŸæ— å…³æ¨¡å‹Logic Neural Networkå’ŒLLM-Symbolic Solverçš„åˆ†æï¼Œå‘ç°æ··åˆæ–¹æ³•æ›´æœ‰å¯èƒ½ç”¨äºå‘å±•é€šç”¨é€»è¾‘æ¨ç†ï¼Œå› å…¶æ¨ç†é“¾æ›´å¯è§£é‡Šï¼Œä¸”ä¿ç•™äº†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å’Œä¼˜åŠ¿ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºLLM-SSçš„å¯æ¨¡å—åŒ–ã€æ¨¡å‹æ— å…³ã€é¢†åŸŸæ— å…³ä¸”éœ€è¦å¾ˆå°‘äººä¸ºè¾“å…¥çš„é€šç”¨æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹ç¡®å®šæ€§çš„æ¨ç†å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>ç¥ç»ç¬¦å·AIçš„é›†æˆæ–¹æ³•å’Œæ··åˆæ–¹æ³•è¢«ç”¨æ¥æ”¹å–„é€»è¾‘æ¨ç†ã€‚</li>
<li>Logic Neural Networkä»£è¡¨é›†æˆæ–¹æ³•ï¼Œè€ŒLLM-Symbolic Solverä»£è¡¨æ··åˆæ–¹æ³•ã€‚</li>
<li>æ··åˆæ–¹æ³•åœ¨å¼€å‘é€šç”¨é€»è¾‘æ¨ç†æ–¹é¢æ›´æœ‰å‰æ™¯ï¼Œå› ä¸ºå®ƒçš„æ¨ç†é“¾æ›´å¯è§£é‡Šï¼Œå¹¶ä¿ç•™äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>æ··åˆæ–¹æ³•æ¡†æ¶åº”æ¨¡å—åŒ–ã€æ¨¡å‹æ— å…³ã€é¢†åŸŸæ— å…³ï¼Œå¹¶å°½é‡å‡å°‘äººä¸ºè¾“å…¥ã€‚</li>
<li>æ–‡æœ¬å‘¼åå¯¹è¿™ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”ç ”ç©¶ï¼Œä»¥æ˜ç¡®å“ªç§æ–¹æ³•æ›´æœ‰æ½œåŠ›ç”¨äºé€šç”¨é€»è¾‘æ¨ç†çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03366">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a76d7fb176fa5239eea400165c35d233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db67c712f72e9673c5c198b579a0d353.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1584584e228d42f591cea7a6ecc3ddfe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48bb46093a737a010983ca607d60e7f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff57bb6c944fdf5e0d49434203515cef.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Trustworthy-Multimodal-Moderation-via-Policy-Aligned-Reasoning-and-Hierarchical-Labeling"><a href="#Towards-Trustworthy-Multimodal-Moderation-via-Policy-Aligned-Reasoning-and-Hierarchical-Labeling" class="headerlink" title="Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning   and Hierarchical Labeling"></a>Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning   and Hierarchical Labeling</h2><p><strong>Authors:Anqi Li, Wenwei Jin, Jintao Tong, Pengda Qin, Weijia Li, Guo Lu</strong></p>
<p>Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term â€œHierarchicalâ€ reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/lianqi1008/Hi-Guard">https://github.com/lianqi1008/Hi-Guard</a>. </p>
<blockquote>
<p>ç¤¾äº¤å¹³å°å·²ç»å½»åº•æ”¹å˜äº†ä¿¡æ¯å…±äº«çš„æ–¹å¼ï¼Œä½†ä¹ŸåŠ é€Ÿäº†æœ‰å®³å’Œè¿åæ”¿ç­–å†…å®¹çš„ä¼ æ’­ã€‚ä¸ºäº†ç¡®ä¿å¤§è§„æ¨¡çš„å®‰å…¨æ€§å’Œåˆè§„æ€§ï¼Œç›‘ç®¡ç³»ç»Ÿå¿…é¡»è¶…è¶Šæ•ˆç‡ï¼Œæä¾›å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºå˜ˆæ‚çš„ã€æ ‡ç­¾é©±åŠ¨çš„å­¦ä¹ ï¼Œç¼ºä¹ä¸ç›‘ç®¡è§„åˆ™çš„åŒ¹é…ï¼Œå¹¶äº§ç”Ÿé˜»ç¢äººå·¥å®¡æŸ¥çš„ä¸é€æ˜å†³ç­–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚å®ˆå«ï¼ˆHi-Guardï¼‰ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå¼•å…¥æ–°æ”¿ç­–å†³ç­–èŒƒå¼çš„å¤šæ¨¡å¼ç›‘ç®¡æ¡†æ¶ã€‚æœ¯è¯­â€œåˆ†å±‚â€åæ˜ äº†æˆ‘ä»¬ç³»ç»Ÿè®¾è®¡ä¸­çš„ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆ1ï¼‰åˆ†å±‚ç›‘ç®¡ç®¡é“ï¼Œå…¶ä¸­è½»é‡çº§äºŒå…ƒæ¨¡å‹é¦–å…ˆè¿‡æ»¤å®‰å…¨å†…å®¹ï¼Œæ›´å¼ºæ¨¡å‹å¤„ç†ç²¾ç»†é£é™©åˆ†ç±»ï¼›ï¼ˆ2ï¼‰ç¬¬äºŒé˜¶æ®µçš„åˆ†å±‚åˆ†ç±»æ³•ï¼Œæ¨¡å‹åœ¨å±‚æ¬¡åˆ†ç±»æ³•ä¸Šè¿›è¡Œè·¯å¾„åˆ†ç±»ï¼Œä»ç²—ç•¥åˆ°ç²¾ç»†çº§åˆ«ã€‚ä¸ºäº†ç¡®ä¿ä¸ä¸æ–­å‘å±•çš„ç›‘ç®¡æ”¿ç­–ç›¸åŒ¹é…ï¼ŒHi-Guardç›´æ¥å°†è§„åˆ™å®šä¹‰çº³å…¥æ¨¡å‹æç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºç»“æ„åŒ–é¢„æµ‹å’Œæ¨ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šçº§è½¯è¾¹ç•Œå¥–åŠ±ï¼Œå¹¶é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œå¯¹è¯­ä¹‰ç›¸é‚»çš„é”™è¯¯åˆ†ç±»è¿›è¡Œæƒ©ç½šï¼Œæé«˜è§£é‡Šè´¨é‡ã€‚å¤§é‡å®éªŒå’Œç°å®ä¸–ç•Œéƒ¨ç½²è¡¨æ˜ï¼ŒHi-Guardåœ¨åˆ†ç±»ç²¾åº¦ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºå¯æ‰©å±•ã€é€æ˜å’Œå¯é çš„å†…å®¹å®‰å…¨ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/lianqi1008/Hi-Guard%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lianqi1008/Hi-Guardä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03296v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“å¹³å°åœ¨ä¿¡æ¯åˆ†äº«æ–¹é¢å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†åŒæ—¶ä¹ŸåŠ é€Ÿäº†æœ‰å®³å’Œè¿è§„å†…å®¹çš„ä¼ æ’­ã€‚ä¸ºç¡®ä¿å¤§è§„æ¨¡å†…å®¹çš„å®‰å…¨æ€§å’Œåˆè§„æ€§ï¼Œå®¡æ ¸ç³»ç»Ÿå¿…é¡»è¶…è¶Šæ•ˆç‡ï¼Œæä¾›å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚å½“å‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºå˜ˆæ‚çš„æ ‡ç­¾é©±åŠ¨å­¦ä¹ ï¼Œç¼ºä¹ä¸å®¡æ ¸è§„åˆ™çš„åŒ¹é…åº¦ï¼Œå¹¶äº§ç”Ÿé˜»ç¢äººå·¥å®¡æŸ¥çš„ä¸é€æ˜å†³ç­–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå±‚æ¬¡å®ˆå«ï¼ˆHi-Guardï¼‰è¿™ä¸€å¤šæ¨¡å¼å®¡æ ¸æ¡†æ¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸ç­–ç•¥åŒ¹é…çš„å†³ç­–èŒƒå¼ã€‚â€œå±‚æ¬¡åŒ–â€åæ˜ äº†æˆ‘ä»¬ç³»ç»Ÿè®¾è®¡ä¸­çš„ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šä¸€æ˜¯å±‚æ¬¡åŒ–çš„å®¡æ ¸ç®¡é“ï¼Œé¦–å…ˆæ˜¯è½»ä¾¿çš„äºŒå…ƒæ¨¡å‹è¿‡æ»¤å®‰å…¨å†…å®¹ï¼Œç„¶åæ˜¯æ›´å¼ºçš„æ¨¡å‹è¿›è¡Œç²¾ç»†é£é™©åˆ†ç±»ï¼›äºŒæ˜¯åœ¨ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å±‚æ¬¡åŒ–çš„åˆ†ç±»æ³•ï¼Œæ¨¡å‹åœ¨ä»æœ€ç²—ç³™åˆ°æœ€ç²¾ç»†çš„å±‚æ¬¡ç»“æ„ä¸Šè¿›è¡Œè·¯å¾„åˆ†ç±»ã€‚ä¸ºç¡®ä¿ä¸ä¸æ–­å‘å±•çš„å®¡æ ¸ç­–ç•¥ç›¸åŒ¹é…ï¼ŒHi-Guardç›´æ¥å°†è§„åˆ™å®šä¹‰çº³å…¥æ¨¡å‹æç¤ºä¸­ã€‚é€šè¿‡å¼•å…¥å¤šå±‚æ¬¡è½¯è¾¹ç¼˜å¥–åŠ±ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒæŠ‘åˆ¶è¯­ä¹‰ç›¸é‚»çš„è¯¯åˆ†ç±»ï¼Œæé«˜è§£é‡Šè´¨é‡ã€‚å¤§é‡å®éªŒå’Œç°å®ä¸–ç•Œéƒ¨ç½²è¡¨æ˜ï¼ŒHi-Guardåœ¨åˆ†ç±»ç²¾åº¦ã€æ¨å¹¿å’Œå¯è§£é‡Šæ€§æ–¹é¢å–å¾—äº†å“è¶Šçš„æˆç»©ï¼Œä¸ºå¯æ‰©å±•ã€é€æ˜å’Œå¯é çš„å†…å®¹å®‰å…¨ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¹³å°åœ¨ä¿¡æ¯åˆ†äº«ä¸­å¸¦æ¥å˜é©ï¼Œä½†è¿è§„å†…å®¹ä¼ æ’­é—®é¢˜åŠ å‰§ã€‚</li>
<li>å½“å‰çš„å®¡æ ¸ç³»ç»Ÿéœ€è¦è¶…è¶Šæ•ˆç‡ï¼Œè¿½æ±‚å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–æ ‡ç­¾é©±åŠ¨å­¦ä¹ ï¼Œç¼ºä¹ä¸å®¡æ ¸è§„åˆ™çš„åŒ¹é…ï¼Œå†³ç­–ä¸é€æ˜ã€‚</li>
<li>å¼•å…¥Hi-Guardå¤šå±‚æ¬¡å®ˆå«æ¡†æ¶ï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–å®¡æ ¸ç®¡é“å’Œåˆ†ç±»æ³•ã€‚</li>
<li>Hi-Guardç›´æ¥çº³å…¥å®¡æ ¸ç­–ç•¥è§„åˆ™ï¼Œæé«˜æ¨¡å‹ä¸è§„åˆ™åŒ¹é…åº¦ã€‚</li>
<li>é€šè¿‡å¤šå±‚æ¬¡è½¯è¾¹ç¼˜å¥–åŠ±å’ŒGRPOä¼˜åŒ–ï¼Œæé«˜åˆ†ç±»ç²¾åº¦å’Œè§£é‡Šè´¨é‡ã€‚</li>
<li>å®éªŒå’Œç°å®ä¸–ç•Œéƒ¨ç½²æ˜¾ç¤ºHi-Guardåœ¨åˆ†ç±»ã€æ¨å¹¿å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7449461b0c3c7aab4d94b43a0082b421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ced4c100626efc566c6757d6fe92ad04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a696268807ed2c5d863f6e35632cd7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4085ee5a15b4ff340eb39d2e134083e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c2340cbbd7633f7cef7c35f976d52d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-580bcbddb375a6d1603899c3f4b7eb5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afed65eff1f56e2c9a11ce57f6ef8902.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VRPO-Rethinking-Value-Modeling-for-Robust-RL-Training-under-Noisy-Supervision"><a href="#VRPO-Rethinking-Value-Modeling-for-Robust-RL-Training-under-Noisy-Supervision" class="headerlink" title="VRPO: Rethinking Value Modeling for Robust RL Training under Noisy   Supervision"></a>VRPO: Rethinking Value Modeling for Robust RL Training under Noisy   Supervision</h2><p><strong>Authors:Dingwei Zhu, Shihan Dou, Zhiheng Xi, Senjie Jin, Guoqiang Zhang, Jiazheng Zhang, Junjie Ye, Mingxu Chai, Enyu Zhou, Ming Zhang, Caishuang Huang, Yunke Zhang, Yuran Wang, Tao Gui</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or imperfect reward supervision in real-world settings, which undermines policy stability and generalization. Such noise may cause models to lose attention on key words during advantage estimation. While prior work focuses on reward denoising or filtering poor data, it often overlooks the critical role of the value model in policy optimization. In this work, we show that a strong value model is essential for mitigating noise by absorbing unstable signals and enabling more reliable advantage estimation. We propose VRPO, a value-centric framework for robust PPO training under noisy supervision. VRPO combines two core designs: (1) an auxiliary loss guided by entropy and perplexity from a frozen language model, and (2) a variational information bottleneck. These mechanisms enhance the value modelâ€™s ability to filter out noise and capture key words from the context during advantage estimation, transforming it from a passive predictor into an active regulator of noise. Experiments on math reasoning, science QA, and multi-turn dialogue, under both rule-based and model-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO baselines. Our findings underscore the often-overlooked importance of the value model in RLHF and offer a principled and practical approach to robust policy optimization in noisy real-world environments. </p>
<blockquote>
<p>åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­å¸¸å¸¸é¢ä¸´å¥–åŠ±ç›‘ç£å­˜åœ¨å™ªå£°æˆ–ä¸å®Œç¾çš„é—®é¢˜ï¼Œè¿™ç ´åäº†ç­–ç•¥çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™æ ·çš„å™ªå£°å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹åœ¨ä¼°è®¡ä¼˜åŠ¿æ—¶å¿½ç•¥å…³é”®è¯ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶é›†ä¸­åœ¨å¥–åŠ±å»å™ªæˆ–è¿‡æ»¤ä¸è‰¯æ•°æ®ä¸Šï¼Œä½†å®ƒç»å¸¸å¿½ç•¥ä»·å€¼æ¨¡å‹åœ¨ç­–ç•¥ä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ä¸€ä¸ªå¼ºå¤§çš„ä»·å€¼æ¨¡å‹å¯¹äºç¼“è§£å™ªå£°è‡³å…³é‡è¦ï¼Œé€šè¿‡å¸æ”¶ä¸ç¨³å®šä¿¡å·å¹¶å¯æ›´åŠ å¯é çš„ä¼˜ç‚¹ä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºäº†VRPOï¼Œè¿™æ˜¯ä¸€ä¸ªä»·å€¼å¯¼å‘çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å™ªå£°ç›‘ç£ä¸‹è¿›è¡Œç¨³å¥çš„PPOè®­ç»ƒã€‚VRPOç»“åˆäº†ä¸¤ç§æ ¸å¿ƒè®¾è®¡ï¼šï¼ˆ1ï¼‰ç”±å†»ç»“è¯­è¨€æ¨¡å‹çš„ç†µå’Œå›°æƒ‘åº¦å¼•å¯¼çš„è¾…åŠ©æŸå¤±ï¼›ï¼ˆ2ï¼‰å˜åˆ†ä¿¡æ¯ç“¶é¢ˆã€‚è¿™äº›æœºåˆ¶å¢å¼ºäº†ä»·å€¼æ¨¡å‹åœ¨ä¼°è®¡ä¼˜åŠ¿æ—¶ä»ä¸Šä¸‹æ–‡ä¸­è¿‡æ»¤å™ªå£°å¹¶æ•è·å…³é”®è¯çš„èƒ½åŠ›ï¼Œå°†å…¶ä»è¢«åŠ¨çš„é¢„æµ‹å™¨è½¬å˜ä¸ºå¯¹å™ªå£°çš„ä¸»åŠ¨è°ƒèŠ‚å™¨ã€‚åœ¨æ•°å­¦æ¨ç†ã€ç§‘å­¦é—®ç­”å’Œå¤šè½®å¯¹è¯æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼Œåœ¨åŸºäºè§„åˆ™å’Œæ¨¡å‹äº§ç”Ÿçš„å™ªå£°å¥–åŠ±ä¸‹ï¼ŒVRPOå§‹ç»ˆä¼˜äºPPOå’ŒGRPOåŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ç»å¸¸è¢«å¿½è§†çš„ä»·å€¼æ¨¡å‹åœ¨RLHFä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå……æ»¡å™ªå£°çš„ç°å®ç¯å¢ƒä¸­çš„ç¨³å¥ç­–ç•¥ä¼˜åŒ–æä¾›äº†ç†è®ºå’Œå®è·µç›¸ç»“åˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03058v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å®é™…åº”ç”¨ä¸­å¸¸å—åˆ°å™ªå£°æˆ–ä¸å®Œç¾çš„å¥–åŠ±ç›‘ç£å½±å“ï¼Œå¯¼è‡´ç­–ç•¥ç¨³å®šæ€§å’Œæ³›åŒ–æ€§ä¸‹é™ã€‚æœ¬æ–‡æå‡ºVRPOï¼Œä¸€ä¸ªä»¥ä»·å€¼æ¨¡å‹ä¸ºä¸­å¿ƒã€åœ¨å™ªå£°ç›‘ç£ä¸‹è¿›è¡Œç¨³å¥PPOè®­ç»ƒçš„æ¡†æ¶ã€‚VRPOç»“åˆäº†ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šä¸€æ˜¯é€šè¿‡å†»ç»“è¯­è¨€æ¨¡å‹å¼•å¯¼çš„ç†µå’Œå›°æƒ‘åº¦è¾…åŠ©æŸå¤±ï¼ŒäºŒæ˜¯å˜åˆ†ä¿¡æ¯ç“¶é¢ˆã€‚è¿™äº›æœºåˆ¶æå‡äº†ä»·å€¼æ¨¡å‹åœ¨ä¼˜åŠ¿è¯„ä¼°ä¸­è¿‡æ»¤å™ªå£°å’Œæ•æ‰å…³é”®è¯çš„èƒ½åŠ›ï¼Œå°†å…¶ä»è¢«åŠ¨é¢„æµ‹è½¬å˜ä¸ºç§¯æè°ƒæ§å™ªå£°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è§„åˆ™æ¨¡å‹å’ŒåŸºäºæ¨¡å‹çš„å™ªå£°å¥–åŠ±ä¸‹ï¼ŒVRPOåœ¨æ•°å­¦æ¨ç†ã€ç§‘å­¦é—®ç­”å’Œå¤šè½®å¯¹è¯ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚äºPPOå’ŒGRPOåŸºçº¿æ–¹æ³•ã€‚å¼ºè°ƒäº†ä»·å€¼æ¨¡å‹åœ¨RLHFä¸­çš„é‡è¦æ€§å¹¶æä¾›äº†é¢å‘å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´å™ªå£°æˆ–ä¸å®Œç¾å¥–åŠ±ç›‘ç£çš„æŒ‘æˆ˜ã€‚</li>
<li>å™ªå£°å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨ä¼˜åŠ¿ä¼°è®¡æ—¶å¿½ç•¥å…³é”®è¯ã€‚</li>
<li>ç°æœ‰å·¥ä½œå¤šå…³æ³¨å¥–åŠ±å»å™ªæˆ–è¿‡æ»¤ä¸è‰¯æ•°æ®ï¼Œå´å¿½è§†äº†ä»·å€¼æ¨¡å‹åœ¨ç­–ç•¥ä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ã€‚</li>
<li>VRPOæ¡†æ¶ç»“åˆè¾…åŠ©æŸå¤±å’Œå˜åˆ†ä¿¡æ¯ç“¶é¢ˆæ¥æå‡ä»·å€¼æ¨¡å‹åœ¨ä¼˜åŠ¿è¯„ä¼°ä¸­çš„æ€§èƒ½ã€‚</li>
<li>VRPOæ¡†æ¶èƒ½å°†ä»·å€¼æ¨¡å‹ä»è¢«åŠ¨é¢„æµ‹è½¬å˜ä¸ºç§¯æè°ƒæ§å™ªå£°çš„è§’è‰²ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVRPOåœ¨æ•°å­¦æ¨ç†ã€ç§‘å­¦é—®ç­”å’Œå¤šè½®å¯¹è¯ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€‚åº”äºå™ªå£°ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19cac0ea4cf166459e6633aa6a833c47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e531f98119dd8574993c5a2457ff2c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5aa3ad6e84f43b0976593c2b0197566.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2b56ddef7a4b81816dc1c0a8b04485d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3dfaac9f2322b0b59dd2ca47b069b68c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76c2401bc73df7bb2e45908d7ae4cd13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e946e76a3ebfa57158eb2383d8c13361.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLMâ€™s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸€æŒ‘æˆ˜æ ¹æœ¬æºäºæ·±å±‚çš„ç»“æ„ä¾èµ–æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é…å¤‡LLMä»¥æ˜ç¡®ã€å¯é‡å¤ä½¿ç”¨çš„æ•°å­¦ç»“æ„çš„ä¸¤é˜¶æ®µå› æœæ¡†æ¶ï¼Œåä¸º<strong>CAMA</strong>ï¼ˆå› æœæ•°å­¦å®¶ï¼‰ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAé¦–å…ˆé€šè¿‡ç»“åˆLLMå…ˆéªŒçŸ¥è¯†å’Œåº”ç”¨äºé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¯­æ–™åº“çš„å› æœå‘ç°ç®—æ³•ï¼Œæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯è§£å†³æ–¹æ¡ˆç­–ç•¥çš„é«˜çº§è¡¨ç¤ºã€‚ç”Ÿæˆçš„MCGç¼–ç äº†å¿…è¦çš„çŸ¥è¯†ç‚¹å’Œå®ƒä»¬çš„å› æœä¾èµ–å…³ç³»ã€‚ä¸ºäº†æ›´å¥½åœ°ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ï¼ŒCAMAè¿›ä¸€æ­¥é€šè¿‡æ¥è‡ªé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹æ‰€é€‰å­é›†çš„è¿­ä»£åé¦ˆæ¥ä¼˜åŒ–MCGã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹äºæ–°é—®é¢˜ï¼ŒCAMAä¼šæ ¹æ®é—®é¢˜çš„å†…å®¹å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ï¼Œä»MCGä¸­åŠ¨æ€æå–ä¸ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚è¿™ä¸ªå­å›¾ç¼–ç äº†æœ€ç›¸å…³çš„çŸ¥è¯†ç‚¹å’Œå®ƒä»¬çš„å› æœä¾èµ–å…³ç³»ï¼Œç„¶åæ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAæ˜¾è‘—æé«˜äº†LLMè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»“æ„åŒ–æŒ‡å¯¼å§‹ç»ˆä¼˜äºéç»“æ„åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”çº³å…¥ä¸å¯¹ç§°å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”å¸¦æ¥æ›´å¤§å¹…åº¦çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆCAMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜çº§è§£å†³æ–¹æ¡ˆç­–ç•¥è¡¨ç¤ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒCAMAæ ¹æ®é—®é¢˜å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ä»MCGä¸­æå–ç›¸å…³å­å›¾ï¼Œå¹¶æŒ‡å¯¼LLMçš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸”ç»“æ„åŒ–çš„æŒ‡å¯¼æ–¹å¼ä¼˜äºéç»“æ„åŒ–æ–¹å¼ï¼Œè€ƒè™‘ä¸å¯¹ç§°å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”æ•ˆæœæ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å› æœæ•°å­¦å®¶ï¼ˆCAMAï¼‰æ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒ…å«å­¦ä¹ å’Œæ¨ç†ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œç»“åˆLLMå…ˆéªŒçŸ¥è¯†å’Œå› æœå‘ç°ç®—æ³•ï¼Œå¯¹é—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¿›è¡Œè¡¨ç¤ºã€‚</li>
<li>MCGç¼–ç äº†é‡è¦çš„çŸ¥è¯†ç‚¹åŠå…¶å› æœå…³ç³»ã€‚</li>
<li>CAMAé€šè¿‡è¿­ä»£åé¦ˆä¼˜åŒ–MCGï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼ŒCAMAä»MCGä¸­æå–ä¸ä»»åŠ¡ç›¸å…³çš„å­å›¾ï¼Œå¹¶æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58da7f78ab2d364eb3339b882ecd3c4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb36c79e159593473a6f104fb2645231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef83bcc7ab4751853333c5463f3b0db0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agentsâ€™ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æœ€è¿‘è¡¨ç°å‡ºåœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢çš„ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™æ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥éª¤äº¤äº’å®ç°çš„ã€‚è™½ç„¶è¿™äº›ä»£ç†æœ‰æ½œåŠ›å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†å®ƒä»¬çš„è§£å†³é—®é¢˜è¿‡ç¨‹ï¼Œå³ä»£ç†å®Œæˆä»»åŠ¡çš„äº¤äº’è½¨è¿¹ï¼Œä»ç„¶è¢«ä½ä¼°ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥å¼•å¯¼ä»£ç†æœç€æ­£ç¡®çš„æ–¹å‘è§£å†³é—®é¢˜ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†ä¸åŒè½¨è¿¹ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶ä¸”ç¼ºä¹æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œè¿™å¯¼è‡´äº†å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–ä»–ä»¬çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå…³é”®æ“ä½œæ¥é‡æ–°è®¿é—®å’Œæ”¹è¿›å…ˆå‰çš„è½¨è¿¹ï¼šä¿®è®¢ã€é‡ç»„å’Œç²¾ç‚¼ã€‚è¿™ç§è¿›åŒ–æœºåˆ¶å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒé€šè¿‡æ™ºèƒ½åœ°æ¢ç´¢å—å…ˆå‰è½¨è¿¹æŒ‡å¯¼çš„å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œè¶…è¶Šäº†å±€éƒ¨æœ€ä¼˜ï¼›ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨è·¨è½¨è¿¹çš„çµæ„Ÿæ¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡è½»æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒSE-Agentå®ç°äº†æŒç»­çš„è‡ªæˆ‘è¿›åŒ–ï¼Œé€æ­¥æé«˜äº†æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬åœ¨SWE-bench Verifiedä¸Šè¯„ä¼°äº†SE-Agentï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„GitHubé—®é¢˜ã€‚åœ¨äº”ä¸ªå¼ºå¤§çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆSE-Agentå¸¦æ¥äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨SWE-bench Verifiedä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å¼€æºä»£ç†ä¸­çš„æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥éª¤äº¤äº’å±•ç°å‡ºå¼ºå¤§çš„å¤æ‚ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚å°½ç®¡LLMåœ¨å¤„ç†å¤æ‚ä»»åŠ¡ä¸Šå…·æœ‰æ½œåŠ›ï¼Œä½†å…¶è§£å†³ä»»åŠ¡çš„è¿‡ç¨‹å³äº¤äº’è½¨è¿¹å°šæœªå¾—åˆ°å……åˆ†çš„æŒ–æ˜ã€‚é’ˆå¯¹è¿™ä¸€ç°çŠ¶ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè®©æ™ºèƒ½ä½“é€šè¿‡è‡ªæˆ‘è¿›åŒ–ä¼˜åŒ–å…¶æ¨ç†è¿‡ç¨‹çš„æ¡†æ¶ã€‚SE-Agenté€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç²¾ç‚¼ä¸‰ä¸ªå…³é”®æ“ä½œæ¥é‡æ–°è¯„ä¼°å’Œå¢å¼ºå…ˆå‰çš„è½¨è¿¹ï¼Œä»è€Œå®ç°æ™ºèƒ½ä½“çš„è¿ç»­è‡ªæˆ‘è¿›åŒ–ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚åœ¨SWE-bench Verifiedä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Agentç›¸è¾ƒäºå…¶ä»–å¼€æºæ™ºèƒ½ä½“å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸å¯¹æ”¹è¿›å¹…åº¦é«˜è¾¾55%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹å…·å¤‡å¼ºå¤§çš„å¤æ‚ä»»åŠ¡å¤„ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤šæ­¥éª¤ç¯å¢ƒäº¤äº’ä¸­å±•ç°çªå‡ºä¼˜åŠ¿ã€‚</li>
<li>æ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹å³äº¤äº’è½¨è¿¹å¯¹äºè§£å†³ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†å½“å‰å¯¹å…¶çš„æŒ–æ˜ä»æ˜¾ä¸è¶³ã€‚</li>
<li>SE-Agentæ¡†æ¶èƒ½å¤Ÿè®©æ™ºèƒ½ä½“é€šè¿‡è‡ªæˆ‘è¿›åŒ–ä¼˜åŒ–å…¶æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¿®è®¢ã€é‡ç»„å’Œç²¾ç‚¼å…ˆå‰çš„è½¨è¿¹ã€‚</li>
<li>SE-Agentèƒ½å¤Ÿæ‰©å¤§æœç´¢ç©ºé—´å¹¶æ¢ç´¢å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œä»è€Œæé«˜äº†æ™ºèƒ½ä½“çš„æ€§èƒ½å¹¶é™ä½äº†å› æ¨ç†è·¯å¾„ä¸å½“è€Œå¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚</li>
<li>SE-Agentåœ¨SWE-bench Verifiedä¸Šçš„å®éªŒè¡¨ç°å“è¶Šï¼Œç›¸è¾ƒäºå…¶ä»–å¼€æºæ™ºèƒ½ä½“æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>SE-Agentæ¡†æ¶å…¬å¼€å¯ç”¨ï¼Œä¸ºæ™ºèƒ½ä½“çš„è¿›åŒ–ç ”ç©¶æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54b48f09ac4403bd42b74224c37da297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models"><a href="#The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models" class="headerlink" title="The SMeL Test: A simple benchmark for media literacy in language models"></a>The SMeL Test: A simple benchmark for media literacy in language models</h2><p><strong>Authors:Gustaf Ahdritz, Anat Kleiman</strong></p>
<p>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently succeeds; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it. </p>
<blockquote>
<p>äº’è”ç½‘å……æ–¥ç€å¤§é‡æ— å½’å±ã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯é çš„å†…å®¹ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸æ‰¿æ‹…è‡ªä¸»ç½‘é¡µæµè§ˆçš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬æ˜¯å¦æŒæ¡äº†äººç±»ç ”ç©¶è€…ç”¨æ¥æµè§ˆè¿™ç§å˜ˆæ‚ç¯å¢ƒçš„ç®€å•å¯å‘å¼æŠ€æœ¯å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹ä¸»åŠ¨è¿‡æ»¤ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å„ç§å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´å‹LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹å§‹ç»ˆè¡¨ç°è‰¯å¥½ï¼›è™½ç„¶æ¨ç†ä¸é«˜åˆ†ç›¸å…³ï¼Œä½†å³ä½¿æ˜¯æˆ‘ä»¬çš„æµ‹è¯•ä¸­è¡¨ç°æœ€å¥½çš„APIæ¨¡å‹ä¹Ÿæœ‰é«˜è¾¾70%çš„å¹»è§‰æƒ…å†µã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”å°å‹æ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½è¿›ä¸€æ­¥æ­ç¤ºè¿™ç§é‡è¦çš„å¹»è§‰ç°è±¡ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„å¯¹æŠ—å¹»è§‰çš„æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02074v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šäº’è”ç½‘å……æ–¥ç€æœªç»è¯å®ã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯é çš„å†…å®¹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å¸¸ç”¨äºè‡ªåŠ¨æµè§ˆç½‘é¡µï¼Œä½†å®ƒä»¬æ˜¯å¦æŒæ¡äº†äººç±»ç ”ç©¶äººå‘˜ç”¨äºå¯¼èˆªæ­¤å¤æ‚ç¯å¢ƒçš„ç®€å•å¯å‘å¼æ–¹æ³•å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ€ä½é™åº¦çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šæƒ…å†µä¸‹ä¸»åŠ¨è¿‡æ»¤ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¸¸ç”¨çš„æŒ‡ä»¤è°ƒä¼˜LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰æ¨¡å‹å§‹ç»ˆè¡¨ç°è‰¯å¥½ï¼›è™½ç„¶æ¨ç†ä¸é«˜åˆ†æœ‰å…³ï¼Œä½†å³ä½¿æˆ‘ä»¬æµ‹è¯•çš„æœ€ä½³APIæ¨¡å‹ä¹Ÿæœ‰é«˜è¾¾70%çš„å¹»æƒ³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šä¼˜äºè¾ƒå°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ›´å¤šåœ°æ­ç¤ºè¿™ç§å¹»è§‰çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„å¯¹æŠ—æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äº’è”ç½‘å­˜åœ¨å¤§é‡ä¸å¯é å†…å®¹ï¼Œå¦‚æœªç»è¯å®ã€æ•…æ„è¯¯å¯¼çš„ä¿¡æ¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨æµè§ˆç½‘é¡µæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå³å¦‚ä½•è¯†åˆ«å’Œå¤„ç†è¿™äº›ä¸å¯é ä¿¡æ¯ã€‚</li>
<li>åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeL Testï¼‰æ˜¯ä¸€ç§æ–°æå‡ºçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨è¿‡æ»¤ä¸å¯é ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰çš„æŒ‡ä»¤è°ƒä¼˜LLMsï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œåœ¨SMeLæµ‹è¯•ä¸­è¡¨ç°ä¸ä¸€ï¼Œæ²¡æœ‰æ¨¡å‹èƒ½å¤Ÿå§‹ç»ˆå¦‚ä¸€åœ°æˆåŠŸã€‚</li>
<li>æ¨ç†èƒ½åŠ›ä¸åœ¨SMeLæµ‹è¯•ä¸­è·å¾—é«˜åˆ†æœ‰å…³ï¼Œä½†æœ€ä½³æ¨¡å‹ä»å­˜åœ¨é«˜è¾¾70%çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>æ›´å¤§ã€æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ä¸ä¸€å®šåœ¨SMeLæµ‹è¯•ä¸­è¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3f6ad270387a73fe6256698b4a7090b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Stochastic-Encodings-for-Active-Feature-Acquisition"><a href="#Stochastic-Encodings-for-Active-Feature-Acquisition" class="headerlink" title="Stochastic Encodings for Active Feature Acquisition"></a>Stochastic Encodings for Active Feature Acquisition</h2><p><strong>Authors:Alexander Norcliffe, Changhee Lee, Fergus Imrie, Mihaela van der Schaar, Pietro Lio</strong></p>
<p>Active Feature Acquisition is an instance-wise, sequential decision making problem. The aim is to dynamically select which feature to measure based on current observations, independently for each test instance. Common approaches either use Reinforcement Learning, which experiences training difficulties, or greedily maximize the conditional mutual information of the label and unobserved features, which makes myopic acquisitions. To address these shortcomings, we introduce a latent variable model, trained in a supervised manner. Acquisitions are made by reasoning about the features across many possible unobserved realizations in a stochastic latent space. Extensive evaluation on a large range of synthetic and real datasets demonstrates that our approach reliably outperforms a diverse set of baselines. </p>
<blockquote>
<p>ä¸»åŠ¨ç‰¹å¾è·å–æ˜¯ä¸€ä¸ªé’ˆå¯¹æ¯ä¸ªå®ä¾‹çš„åºè´¯å†³ç­–é—®é¢˜ã€‚å…¶ç›®æ ‡æ˜¯æ ¹æ®å½“å‰è§‚å¯Ÿï¼Œç‹¬ç«‹åœ°ä¸ºæ¯ä¸ªæµ‹è¯•å®ä¾‹åŠ¨æ€é€‰æ‹©è¦è¿›è¡Œæµ‹é‡çš„ç‰¹å¾ã€‚å¸¸è§çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒå›°éš¾ï¼Œè¦ä¹ˆè´ªå©ªåœ°æœ€å¤§åŒ–æ ‡ç­¾å’Œæœªè§‚æµ‹ç‰¹å¾çš„æ¡ä»¶äº’ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´ç›®å…‰çŸ­æµ…çš„è·å–ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€šè¿‡ç›‘ç£æ–¹å¼è®­ç»ƒçš„æ½œåœ¨å˜é‡æ¨¡å‹ã€‚é€šè¿‡åœ¨ä¸€ä¸ªéšæœºæ½œåœ¨ç©ºé—´ä¸­æ¨ç†è®¸å¤šå¯èƒ½çš„æœªè§‚æµ‹å®ç°æ‰€å¯¹åº”çš„ç‰¹å¾æ¥è¿›è¡Œç‰¹å¾è·å–ã€‚åœ¨å¤šç§åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯é åœ°ä¼˜äºå„ç§åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01957v3">PDF</a> 31 pages, 15 figures, 17 tables, published at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸»åŠ¨ç‰¹å¾è·å–ï¼ˆActive Feature Acquisitionï¼‰æ˜¯ä¸€ç§é’ˆå¯¹æ¯ä¸ªæµ‹è¯•å®ä¾‹è¿›è¡ŒåŠ¨æ€é€‰æ‹©ç‰¹å¾çš„å®ä¾‹çº§å†³ç­–é—®é¢˜ã€‚æ–‡ç« æå‡ºçš„æ–¹æ³•åŸºäºç›‘ç£å­¦ä¹ è®­ç»ƒä¸€ä¸ªæ½œåœ¨å˜é‡æ¨¡å‹ï¼Œé€šè¿‡å¯¹éšæœºæ½œåœ¨ç©ºé—´ä¸­å¯èƒ½çš„æœªè§‚å¯Ÿç‰¹å¾è¿›è¡Œæ¨ç†æ¥é€‰æ‹©ç‰¹å¾ï¼Œä»è€Œå…‹æœäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒå›°éš¾å’ŒåŸºäºæ¡ä»¶äº’ä¿¡æ¯çš„è´ªå©ªæ–¹æ³•é€ æˆçš„çŸ­è§†é‡‡é›†é—®é¢˜ã€‚ç»è¿‡å¹¿æ³›çš„æ•°æ®é›†éªŒè¯ï¼Œæœ¬æ–‡æ–¹æ³•ç›¸è¾ƒäºä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•å…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»åŠ¨ç‰¹å¾è·å–æ˜¯ä¸€ç§é’ˆå¯¹æ¯ä¸ªæµ‹è¯•å®ä¾‹åŠ¨æ€é€‰æ‹©ç‰¹å¾çš„å®ä¾‹çº§å†³ç­–é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¸¸ç”¨äºç‰¹å¾é‡‡é›†ï¼Œä½†å­˜åœ¨è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚</li>
<li>åŸºäºæ¡ä»¶äº’ä¿¡æ¯çš„è´ªå©ªæ–¹æ³•ä¼šå¯¼è‡´çŸ­è§†çš„é‡‡é›†å†³ç­–ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºç›‘ç£å­¦ä¹ çš„æ½œåœ¨å˜é‡æ¨¡å‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡åœ¨éšæœºæ½œåœ¨ç©ºé—´ä¸­æ¨ç†å¯èƒ½çš„æœªè§‚å¯Ÿç‰¹å¾æ¥åšå‡ºé‡‡é›†å†³ç­–ã€‚</li>
<li>è¯¥æ–¹æ³•ç»è¿‡åœ¨å¤šç§åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºä¼˜äºå¤šç§åŸºå‡†æµ‹è¯•çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96718ec1d6414bbc073b17840f171dbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f6500545ace2bd366d22b31ad66e9b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Word-Overuse-and-Alignment-in-Large-Language-Models-The-Influence-of-Learning-from-Human-Feedback"><a href="#Word-Overuse-and-Alignment-in-Large-Language-Models-The-Influence-of-Learning-from-Human-Feedback" class="headerlink" title="Word Overuse and Alignment in Large Language Models: The Influence of   Learning from Human Feedback"></a>Word Overuse and Alignment in Large Language Models: The Influence of   Learning from Human Feedback</h2><p><strong>Authors:Tom S. Juzek, Zina B. Ward</strong></p>
<p>Large Language Models (LLMs) are known to overuse certain terms like â€œdelveâ€ and â€œintricate.â€ The exact reasons for these lexical choices, however, have been unclear. Using Metaâ€™s Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations â€“ namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‡åº¦ä½¿ç”¨æŸäº›è¯æ±‡ï¼Œå¦‚â€œæ·±å…¥ç ”ç©¶â€å’Œâ€œå¤æ‚â€ã€‚ç„¶è€Œï¼Œå¯¹äºè¿™äº›è¯æ±‡é€‰æ‹©çš„ç¡®åˆ‡åŸå› å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶åˆ©ç”¨Metaçš„Llamaæ¨¡å‹ï¼Œæ¢è®¨å­¦ä¹ äººç±»åé¦ˆï¼ˆLHFï¼‰çš„è´¡çŒ®ï¼Œæˆ‘ä»¬å°†å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆå’Œç›´æ¥åå¥½ä¼˜åŒ–çº³å…¥å…¶ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ£€æµ‹æ½œåœ¨ç”±LHFå¼•èµ·çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯æ±‡åå¥½çš„ç®€å•ç¨‹åºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡æ¨¡æ‹ŸLHFç¨‹åºå¹¶è¯æ˜å‚ä¸è€…ç³»ç»Ÿæ€§åœ°åå¥½åŒ…å«æŸäº›è¯æ±‡çš„æ–‡æœ¬å˜ä½“ï¼Œæ›´æœ‰è¯´æœåŠ›åœ°å°†LHFä¸è¯æ±‡è¿‡åº¦ä½¿ç”¨è”ç³»èµ·æ¥ã€‚è¿™ç§è¯æ±‡è¿‡åº¦ä½¿ç”¨å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§ä¸å¯¹é½ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†ä¸åŒäººç¾¤â€”â€”å³LHFå·¥ä½œè€…ä¸LLMç”¨æˆ·ä¹‹é—´çš„è¯æ±‡æœŸæœ›å¯èƒ½å­˜åœ¨åˆ†æ­§ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºä¸æ–­å‘å±•çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ç ”ç©¶åšå‡ºäº†è´¡çŒ®ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å’Œç¨‹åºé€æ˜åº¦åœ¨å¯¹é½ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01930v1">PDF</a> Accepted for publication in the Proceedings of the 5th Workshop on   Bias and Fairness in AI (BIAS 2025) at ECML PKDD</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨è¿‡åº¦ä½¿ç”¨æŸäº›æœ¯è¯­çš„ç°è±¡ï¼Œä¾‹å¦‚â€œæ·±å…¥ç ”ç©¶â€å’Œâ€œå¤æ‚çš„â€ã€‚æœ¬ç ”ç©¶åˆ©ç”¨Metaçš„Llamaæ¨¡å‹ï¼Œæ¢è®¨äº†ä»äººç±»åé¦ˆä¸­å­¦ä¹ ï¼ˆLHFï¼‰å¯¹æ­¤ç°è±¡çš„å½±å“ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ£€æµ‹LLMæ½œåœ¨LHFè¯±å¯¼è¯æ±‡åå¥½çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ¨¡æ‹ŸLHFç¨‹åºå®éªŒè¯æ˜å‚ä¸è€…å¯¹åŒ…å«æŸäº›è¯æ±‡çš„æ–‡æœ¬ç‰ˆæœ¬æœ‰ç³»ç»Ÿæ€§åå¥½ã€‚è¿™ç§è¯æ±‡è¿‡åº¦ä½¿ç”¨å¯ä»¥è¢«è§†ä¸ºä¸€ç§ä¸å¯¹é½ï¼Œä½†æœ¬ç ”ç©¶å¼ºè°ƒäº†ä¸åŒäººç¾¤â€”â€”å³LHFå·¥ä½œè€…ä¸LLMç”¨æˆ·ä¹‹é—´çš„è¯æ±‡æœŸæœ›æ½œåœ¨åˆ†æ­§ã€‚æœ¬ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§ç ”ç©¶åšå‡ºè´¡çŒ®ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å’Œç¨‹åºé€æ˜åœ¨è°ƒæ•´ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨è¿‡åº¦ä½¿ç”¨æŸäº›æœ¯è¯­çš„ç°è±¡ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨Metaçš„Llamaæ¨¡å‹æ¢è®¨äº†ä»äººç±»åé¦ˆä¸­å­¦ä¹ ï¼ˆLHFï¼‰å¯¹LLMè¯æ±‡é€‰æ‹©çš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ£€æµ‹LLMæ½œåœ¨LHFè¯±å¯¼è¯æ±‡åå¥½çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å®éªŒæ¨¡æ‹ŸLHFç¨‹åºï¼Œè¯æ˜å‚ä¸è€…å¯¹åŒ…å«æŸäº›ç‰¹å®šè¯æ±‡çš„æ–‡æœ¬ç‰ˆæœ¬æœ‰ç³»ç»Ÿæ€§åå¥½ã€‚</li>
<li>è¯æ±‡è¿‡åº¦ä½¿ç”¨å¯è§†ä¸ºä¸€ç§ä¸å¯¹é½ç°è±¡ã€‚</li>
<li>ä¸åŒäººç¾¤ï¼ˆå¦‚LHFå·¥ä½œè€…ä¸LLMç”¨æˆ·ï¼‰ä¹‹é—´çš„è¯æ±‡æœŸæœ›å­˜åœ¨æ½œåœ¨åˆ†æ­§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3db1fb6034326343cea2d5ad8abcbd31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cec54a4bfdab45b4c42424155aa58710.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43c793489a4c5a2b113824c05279fddb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9acde00b43fc0b1fe920d46825989a98.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Scaling LLM Planning NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-318d27d9be819bf0860cf58759e12635.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  MOGO Residual Quantized Hierarchical Causal Transformer for   High-Quality and Real-Time 3D Human Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
