<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Scaling LLM Planning NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9acde00b43fc0b1fe920d46825989a98.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-10-æ›´æ–°"><a href="#2025-08-10-æ›´æ–°" class="headerlink" title="2025-08-10 æ›´æ–°"></a>2025-08-10 æ›´æ–°</h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Effective agent performance relies on the ability to compose tools and agents into effective workflows. However, progress in Large Language Model (LLM) planning and reasoning is limited by the scarcity of scalable, reliable evaluation data. This study addresses this limitation by identifying a suitable workflow domain for LLM application. I introduce NL2Flow, a fully automated system for parametrically generating planning problems, which are expressed in natural language, a structured intermediate representation, and formal PDDL, and rigorously evaluating the quality of generated plans. NL2Flow generates a dataset of 2296 low-difficulty problems in automated workflow generation and evaluates multiple open-sourced, instruct-tuned LLMs without task-specific optimization or architectural modifications. Results reveal that the highest performing model achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible plans. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. To investigate the potential of LLMs as natural language-to-JSON translators for workflow definition, and to facilitate integration with downstream symbolic computation tools and a symbolic planner, I evaluated the LLMâ€™s translation performance on natural language workflow descriptions. I observed that translating natural language into a JSON representation of a workflow problem yielded a lower success rate than generating a plan directly, suggesting that unnecessary decomposition of the reasoning task may degrade performance and highlighting the benefit of models capable of reasoning directly from natural language to action. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>æœ‰æ•ˆçš„ä»£ç†æ€§èƒ½å–å†³äºå°†å·¥å…·å’Œä»£ç†ç»„åˆæˆæœ‰æ•ˆå·¥ä½œæµç¨‹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘å¯æ‰©å±•ã€å¯é çš„è¯„ä¼°æ•°æ®ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„åˆ’å’Œæ¨ç†è¿›å±•å—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶é€šè¿‡ç¡®å®šé€‚åˆLLMåº”ç”¨çš„æµç¨‹é¢†åŸŸæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚ä½œè€…ä»‹ç»äº†NL2Flowï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„ç³»ç»Ÿï¼Œå¯ä»¥å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä»¥è‡ªç„¶è¯­è¨€ã€ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºå’Œæ­£å¼PDDLæ¥è¡¨è¾¾ï¼Œå¹¶å¯¹ç”Ÿæˆçš„è®¡åˆ’è´¨é‡è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ã€‚NL2Flowç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«2296ä¸ªä½éš¾åº¦é—®é¢˜çš„æ•°æ®é›†ï¼Œç”¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆï¼Œå¹¶è¯„ä¼°äº†å¤šä¸ªå¼€æºã€ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–æˆ–æ¶æ„ä¿®æ”¹ã€‚ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢è¾¾åˆ°äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä½³è®¡åˆ’æ–¹é¢è¾¾åˆ°äº†69%ï¼Œä»…é™äºå…·æœ‰å¯è¡Œè®¡åˆ’çš„é—®é¢˜ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹å¾å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚ä¸ºäº†ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªç„¶è¯­è¨€åˆ°JSONçš„å·¥ä½œæµå®šä¹‰ç¿»è¯‘å™¨çš„æ½œåŠ›ï¼Œå¹¶ä¿ƒè¿›å…¶ä¸ä¸‹æ¸¸ç¬¦å·è®¡ç®—å·¥å…·å’Œç¬¦å·è§„åˆ’å™¨çš„é›†æˆï¼Œä½œè€…è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å·¥ä½œæµæè¿°ä¸Šçš„ç¿»è¯‘æ€§èƒ½ã€‚ä½œè€…è§‚å¯Ÿåˆ°ï¼Œå°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆå·¥ä½œæµé—®é¢˜çš„JSONè¡¨ç¤ºå½¢å¼çš„æˆåŠŸç‡ä½äºç›´æ¥ç”Ÿæˆè®¡åˆ’çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜ä¸å¿…è¦çš„åˆ†è§£æ¨ç†ä»»åŠ¡å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œå¹¶çªå‡ºæ˜¾ç¤ºç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹çš„å¥½å¤„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ï¼Œç†è§£è¿™äº›ç³»ç»Ÿå†…ä¸æ–­å˜åŒ–çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå°†è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v3">PDF</a> 26 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„åº”ç”¨ï¼Œé€šè¿‡è¯†åˆ«é€‚åˆLLMåº”ç”¨çš„æµç¨‹é¢†åŸŸï¼Œè§£å†³äº†è¯„ä»·æ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚å¼•å…¥NL2Flowç³»ç»Ÿï¼Œå¯å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œä»¥è‡ªç„¶è¯­è¨€ã€ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºå’Œæ­£å¼PDDLè¡¨è¾¾ï¼Œå¹¶å¯¹ç”Ÿæˆè®¡åˆ’çš„è´¨é‡è¿›è¡Œä¸¥è°¨è¯„ä¼°ã€‚ç ”ç©¶ç”Ÿæˆäº†2296ä¸ªä½éš¾åº¦è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹é—®é¢˜æ•°æ®é›†ï¼Œå¹¶å¯¹å¤šä¸ªå¼€æºã€æŒ‡ä»¤è°ƒä¼˜çš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡ä¼˜åŒ–æˆ–æ¶æ„ä¿®æ”¹ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ€§èƒ½æœ€å¥½çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢æˆåŠŸç‡ä¸º86%ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢æˆåŠŸç‡ä¸º69%ï¼Œä»…é™äºæœ‰å¯è¡Œè®¡åˆ’çš„é—®é¢˜ã€‚å›å½’åˆ†ææ˜¾ç¤ºï¼Œé—®é¢˜ç‰¹å¾å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚è¯„ä¼°äº†LLMä½œä¸ºè‡ªç„¶è¯­è¨€åˆ°JSONç¿»è¯‘å™¨çš„æ½œåŠ›ï¼Œç”¨äºå·¥ä½œæµç¨‹å®šä¹‰ï¼Œå¹¶ä¸ä¸‹æ¸¸ç¬¦å·è®¡ç®—å·¥å…·å’Œç¬¦å·è§„åˆ’å™¨è¿›è¡Œé›†æˆã€‚å‘ç°å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆå·¥ä½œæµç¨‹é—®é¢˜çš„JSONè¡¨ç¤ºå½¢å¼æˆåŠŸç‡è¾ƒä½ï¼Œç›´æ¥ç”Ÿæˆè®¡åˆ’çš„æˆåŠŸç‡æ›´é«˜ã€‚è¿™è¡¨æ˜å°†æ¨ç†ä»»åŠ¡è¿‡åº¦åˆ†è§£å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹çš„ä¼˜åŠ¿ã€‚éšç€LLMæ¨ç†åœ¨è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ä¸Šçš„æ‰©å±•ï¼Œç†è§£è¿™äº›ç³»ç»Ÿä¸­ä¸æ–­å˜åŒ–çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå°†è‡³å…³é‡è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶é€šè¿‡NL2Flowç³»ç»Ÿè§£å†³äº†LLMåœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„è¯„ä»·æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œè¯¥ç³»ç»Ÿå¯å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ã€‚</li>
<li>è¯„ä¼°äº†å¤šä¸ªLLMsåœ¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ï¼Œå‘ç°æ€§èƒ½æœ€å¥½çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢æˆåŠŸç‡ä¸º86%ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢ä¸º69%ã€‚</li>
<li>å›å½’åˆ†ææ˜¾ç¤ºé—®é¢˜ç‰¹å¾ã€æ¨¡å‹å’Œæç¤ºè®¾è®¡å…±åŒå½±å“è®¡åˆ’ç”Ÿæˆã€‚</li>
<li>è¯„ä¼°äº†LLMä½œä¸ºè‡ªç„¶è¯­è¨€åˆ°JSONç¿»è¯‘å™¨çš„æ½œåŠ›ï¼Œå‘ç°ç›´æ¥è¿›è¡Œè®¡åˆ’ç”Ÿæˆæ¯”ç¿»è¯‘è¡¨ç¤ºæ³•æ›´æˆåŠŸã€‚</li>
<li>è¿‡åº¦åˆ†è§£æ¨ç†ä»»åŠ¡å¯èƒ½ä¼šé™ä½LLMçš„æ€§èƒ½ã€‚</li>
<li>éšç€LLMæ¨ç†åœ¨æ›´å¤æ‚é—®é¢˜ä¸Šçš„æ‰©å±•ï¼Œç†è§£å…¶ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå˜å¾—è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db1a26ecfa40678fa6c892a859b25ea4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad8fbb06b9226d44d5933b6f55c6e886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437e6002cf95e423276f26901f987d9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ae47048764fa0ab3f30f98365be80fa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Factuality-for-Dialogue-Response-Generation-via-Graph-Based-Knowledge-Augmentation"><a href="#Improving-Factuality-for-Dialogue-Response-Generation-via-Graph-Based-Knowledge-Augmentation" class="headerlink" title="Improving Factuality for Dialogue Response Generation via Graph-Based   Knowledge Augmentation"></a>Improving Factuality for Dialogue Response Generation via Graph-Based   Knowledge Augmentation</h2><p><strong>Authors:Xiangyan Chen, Yujian Gan, Yimeng Gu, Matthew Purver</strong></p>
<p>Large Language Models (LLMs) succeed in many natural language processing tasks. However, their tendency to hallucinate - generate plausible but inconsistent or factually incorrect text - can cause significant problems in certain tasks, including response generation in dialogue. To mitigate this issue, we propose two novel graph knowledge-augmented frameworks, Dialogue Response Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue Response Generation (GA-DRG), which combine reasoning-guided dialogue reformulation, dialogue sense knowledge selection, and graph-enhanced response generation to improve the factuality of dialogue responses. To evaluate the factuality of generated responses, we propose a dialogue fact score that addresses the limitations of existing fact-score methods in dialogue settings, providing a more reliable assessment of factual consistency. We evaluate our methods using different baselines on the OpendialKG and HybriDialogue datasets. Our methods noticeably improve factuality compared to other graph knowledge-augmentation baselines, including the state-of-the-art G-retriever, achieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in terms of dialogue fact score. The code will be released on GitHub. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬å€¾å‘äºäº§ç”Ÿåˆç†ä½†è‡ªç›¸çŸ›ç›¾æˆ–äº‹å®é”™è¯¯çš„æ–‡æœ¬ï¼Œè¿™åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ˆåŒ…æ‹¬å¯¹è¯ç”Ÿæˆå“åº”ï¼‰å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡é—®é¢˜ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°å‹çš„å›¾çŸ¥è¯†å¢å¼ºæ¡†æ¶ï¼Œå³åŸºäºæ–‡æœ¬å›¾çš„å¯¹è¯å“åº”ç”Ÿæˆï¼ˆTG-DRGï¼‰å’Œå›¾æ„ŸçŸ¥å¯¹è¯å“åº”ç”Ÿæˆï¼ˆGA-DRGï¼‰ã€‚è¿™ä¸¤ä¸ªæ¡†æ¶ç»“åˆäº†æ¨ç†å¼•å¯¼çš„å¯¹è¯é‡æ„ã€å¯¹è¯å¸¸è¯†çŸ¥è¯†é€‰æ‹©å’Œå›¾å¢å¼ºå“åº”ç”Ÿæˆï¼Œä»¥æé«˜å¯¹è¯å“åº”çš„äº‹å®æ€§ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆå“åº”çš„äº‹å®æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹è¯äº‹å®å¾—åˆ†ï¼Œè§£å†³äº†ç°æœ‰äº‹å®å¾—åˆ†æ–¹æ³•åœ¨å¯¹è¯ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œä¸ºäº‹å®ä¸€è‡´æ€§æä¾›äº†æ›´å¯é çš„è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨OpendialKGå’ŒHybriDialogueæ•°æ®é›†ä¸Šä½¿ç”¨äº†ä¸åŒçš„åŸºçº¿æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨äº‹å®æ€§ä¸Šæ˜¾è‘—æ”¹è¿›äº†å…¶ä»–å›¾å½¢çŸ¥è¯†å¢å¼ºåŸºçº¿ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„Gæ£€ç´¢å™¨ï¼Œåœ¨OpendialKGä¸Šæé«˜äº†3.47%ï¼Œåœ¨HybriDialogueä¸Šæé«˜äº†3.12%ï¼Œå¯¹è¯äº‹å®å¾—åˆ†æœ‰æ‰€æé«˜ã€‚ä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12496v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ä¸­å­˜åœ¨è™šæ„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸¤ç§æ–°å‹çš„å›¾çŸ¥è¯†å¢å¼ºæ¡†æ¶ï¼Œå³æ–‡æœ¬åŒ–å›¾å¯¹è¯å“åº”ç”Ÿæˆï¼ˆTG-DRGï¼‰å’Œå›¾æ„ŸçŸ¥å¯¹è¯å“åº”ç”Ÿæˆï¼ˆGA-DRGï¼‰ï¼Œç»“åˆæ¨ç†å¼•å¯¼å¯¹è¯é‡æ„ã€å¯¹è¯æƒ…å¢ƒçŸ¥è¯†é€‰æ‹©å’Œå›¾å¢å¼ºå“åº”ç”Ÿæˆï¼Œä»¥æé«˜å¯¹è¯å“åº”çš„äº‹å®æ€§ã€‚ä¸ºè¯„ä¼°ç”Ÿæˆå“åº”çš„äº‹å®æ€§ï¼Œæœ¬æ–‡æå‡ºå¯¹è¯äº‹å®è¯„åˆ†æ–¹æ³•ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨å¯¹è¯ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œæä¾›æ›´å¯é çš„è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨OpendialKGå’ŒHybriDialogueæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº‹å®æ€§ï¼Œç›¸è¾ƒäºå…¶ä»–å›¾çŸ¥è¯†å¢å¼ºåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„G-retrieverï¼Œåœ¨å¯¹è¯äº‹å®è¯„åˆ†ä¸Šåˆ†åˆ«æé«˜äº†3.47%å’Œ3.12%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¯¹è¯ç”Ÿæˆä¸­å­˜åœ¨è™šæ„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸¤ç§æ–°å‹çš„å›¾çŸ¥è¯†å¢å¼ºæ¡†æ¶TG-DRGå’ŒGA-DRGï¼Œä»¥æé«˜å¯¹è¯å“åº”çš„äº‹å®æ€§ã€‚</li>
<li>ç»“åˆæ¨ç†å¼•å¯¼å¯¹è¯é‡æ„ã€å¯¹è¯æƒ…å¢ƒçŸ¥è¯†é€‰æ‹©å’Œå›¾å¢å¼ºå“åº”ç”Ÿæˆã€‚</li>
<li>æå‡ºå¯¹è¯äº‹å®è¯„åˆ†æ–¹æ³•ï¼Œä»¥æ›´å¯é åœ°è¯„ä¼°ç”Ÿæˆå“åº”çš„äº‹å®æ€§ã€‚</li>
<li>æ‰€ææ–¹æ³•åœ¨OpendialKGå’ŒHybriDialogueæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº‹å®æ€§ã€‚</li>
<li>ä¸å…¶ä»–å›¾çŸ¥è¯†å¢å¼ºåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€ææ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œåœ¨å¯¹è¯äº‹å®è¯„åˆ†ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52594d55c544adc7dfc068b89cabeeb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b8353941804bbdeb65a8dd719d3bac4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777e4f461fca04278e61d7a6e3007579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1feb62991787b3be7c9f403567c2c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08b436994d0fc028a5be471e3aa471f7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models"><a href="#Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models" class="headerlink" title="Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models"></a>Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models</h2><p><strong>Authors:Michael Li, Nishant Subramani</strong></p>
<p>Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand todayâ€™s language models, we investigate how 25 models - from classical architectures (BERT, DeBERTa, GPT-2) to modern large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) - represent lexical identity and inflectional morphology across six typologically diverse languages. Using linear and nonlinear classifiers trained on hidden activations, we predict word lemmas and inflectional features layer by layer. We find that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout. Additional experiments probe the nature of these encodings: attention and residual analyses examine where within layers information can be recovered, steering vector experiments test what information can be functionally manipulated, and intrinsic dimensionality analyses explore how the representational structure evolves across layers. Remarkably, these encoding patterns emerge across all models we test, despite differences in architecture, size, and training regime (pretrained and instruction-tuned variants). This suggests that, even with substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties are important for next token prediction and are learned early during pretraining. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing">https://github.com/ml5885/model_internal_sleuthing</a> </p>
<blockquote>
<p>ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ä¸»è¦ç”±å¤§å‹åŸºäºtransformerçš„è¯­è¨€æ¨¡å‹ä¸»å¯¼ï¼Œç„¶è€Œæˆ‘ä»¬å¯¹å®ƒä»¬å¦‚ä½•ç¼–ç è¯­è¨€ä¿¡æ¯çš„ç†è§£è¿˜ä»…é™äºæ—©æœŸæ¨¡å‹ï¼Œå¦‚BERTå’ŒGPT-2ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å½“ä»Šçš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»ç»å…¸æ¶æ„ï¼ˆBERTã€DeBERTaã€GPT-2ï¼‰åˆ°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPythiaã€OLMo-2ã€Gemma-2ã€Qwen2.5ã€Llama-3.1ï¼‰çš„25ç§æ¨¡å‹æ˜¯å¦‚ä½•åœ¨å…­ç§è¯­è¨€ç±»å‹ä¸°å¯Œçš„è¯­è¨€ä¸­è¡¨ç¤ºè¯æ±‡èº«ä»½å’Œå±ˆæŠ˜å½¢æ€ã€‚æˆ‘ä»¬ä½¿ç”¨åœ¨éšè—æ¿€æ´»ä¸Šè®­ç»ƒçš„çº¿æ€§å’Œéçº¿æ€§åˆ†ç±»å™¨é€å±‚é¢„æµ‹è¯å¹²å’Œå±ˆæŠ˜ç‰¹å¾ã€‚æˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨æ—©æœŸå±‚æ¬¡ä¸­ä»¥çº¿æ€§æ–¹å¼é›†ä¸­è¯æ±‡ä¿¡æ¯ï¼Œè€Œåœ¨åæœŸå±‚æ¬¡ä¸­åˆ™è¶Šæ¥è¶Šä»¥éçº¿æ€§æ–¹å¼é›†ä¸­ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒå±ˆæŠ˜ä¿¡æ¯åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å‡åŒ€å¯è®¿é—®å¹¶ä¸”å¯çº¿æ€§åˆ†ç¦»ã€‚é¢å¤–çš„å®éªŒæ¢è®¨äº†è¿™äº›ç¼–ç çš„æœ¬è´¨ï¼šæ³¨æ„åŠ›åˆ†æå’Œæ®‹å·®åˆ†ææ£€æŸ¥å¯ä»¥åœ¨å“ªäº›å±‚æ¬¡å†…æ¢å¤ä¿¡æ¯ï¼Œè½¬å‘å‘é‡å®éªŒæµ‹è¯•å¯ä»¥æ“ä½œå“ªäº›ä¿¡æ¯ï¼Œä»¥åŠå†…åœ¨ç»´åº¦åˆ†ææ¢ç´¢å±‚æ¬¡é—´è¡¨ç¤ºç»“æ„å¦‚ä½•æ¼”å˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬åœ¨æµ‹è¯•æ‰€æœ‰æ¨¡å‹æ—¶éƒ½å‘ç°äº†è¿™äº›ç¼–ç æ¨¡å¼ï¼Œè¿™äº›æ¨¡å‹åœ¨æ¶æ„ã€è§„æ¨¡å’Œè®­ç»ƒåˆ¶åº¦ï¼ˆé¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´å˜ä½“ï¼‰æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚è¿™è¡¨æ˜ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†transformeræ¨¡å‹ä»¥ç›¸ä¼¼çš„æ–¹å¼ç»„ç»‡è¯­è¨€ä¿¡æ¯ï¼Œè¿™è¡¨æ˜è¿™äº›å±æ€§å¯¹äºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å¾ˆé‡è¦ï¼Œå¹¶ä¸”åœ¨é¢„è®­ç»ƒæœŸé—´æ—©æœŸå°±å·²ç»å­¦ä¹ åˆ°äº†ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ml5885/model_internal_sleuthingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02132v3">PDF</a> INTERPLAY Workshop COLM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¤§å‹åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹å¦‚ä½•ç¼–ç è¯­è¨€ä¿¡æ¯ã€‚é€šè¿‡å¯¹åŒ…æ‹¬ç»å…¸æ¶æ„ï¼ˆå¦‚BERTå’ŒGPT-2ï¼‰å’Œç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†…çš„å…±25ç§æ¨¡å‹çš„æ·±å…¥ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨æ—©æœŸå±‚çº§ä¸­é‡‡ç”¨çº¿æ€§æ–¹å¼é›†ä¸­è¯æ±‡ä¿¡æ¯ï¼Œå¹¶åœ¨åç»­å±‚çº§ä¸­é€æ¸é‡‡ç”¨éçº¿æ€§æ–¹å¼ã€‚åŒæ—¶ï¼Œè¿™äº›æ¨¡å‹ä¿æŒå°†å±ˆæŠ˜ä¿¡æ¯å‡åŒ€åˆ†å¸ƒå¹¶å§‹ç»ˆä¿æŒçº¿æ€§å¯åˆ†ã€‚ç ”ç©¶è¿˜é€šè¿‡ä¸€ç³»åˆ—å®éªŒæ¢ç©¶äº†è¿™äº›ç¼–ç çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›ä¸æ®‹å·®åˆ†æã€å¼•å¯¼å‘é‡å®éªŒå’Œå†…åœ¨ç»´åº¦åˆ†æã€‚å°½ç®¡æ¨¡å‹åœ¨æ¶æ„ã€è§„æ¨¡å’Œè®­ç»ƒæœºåˆ¶ä¸Šå­˜åœ¨å·®å¼‚ï¼Œä½†å®ƒä»¬å±•ç°å‡ºç›¸ä¼¼çš„ç¼–ç æ¨¡å¼ã€‚è¿™è¡¨æ˜å³ä¾¿æ˜¯åœ¨LLMæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›æ­¥çš„èƒŒæ™¯ä¸‹ï¼ŒTransformeræ¨¡å‹çš„ç»„ç»‡è¯­è¨€ä¿¡æ¯çš„æ–¹å¼ä»ç„¶æ˜¯ç¨³å®šçš„ã€‚å¯¹äºä¸‹ä¸€æ­¥çš„è¯é¢„æµ‹è€Œè¨€ï¼Œè¿™äº›ç‰¹æ€§å°¤ä¸ºé‡è¦ï¼Œå¹¶ä¸”ä¸»è¦æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µæ—©æœŸå°±å½¢æˆçš„ã€‚ç›¸å…³çš„ä»£ç å¯ä»¥åœ¨ç›¸å…³ç½‘ç«™å…¬å¼€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ä¸»å¯¼ç°ä»£NLPé¢†åŸŸçš„ç ”ç©¶ã€‚å®ƒä»¬é‡‡ç”¨åˆ†å±‚çš„ä¿¡æ¯å¤„ç†æ–¹å¼å¤„ç†è¯æ±‡ä¿¡æ¯å’Œå±ˆæŠ˜ä¿¡æ¯ã€‚è¯æ±‡ä¿¡æ¯åœ¨å‰æœŸå±‚çº§ä¸­ä»¥çº¿æ€§æ–¹å¼ä¸ºä¸»ï¼ŒåæœŸå±‚çº§åˆ™é€æ¸é‡‡ç”¨éçº¿æ€§æ–¹å¼å¤„ç†ã€‚å±ˆæŠ˜ä¿¡æ¯åˆ™åœ¨æ•´ä¸ªæ¨¡å‹ä¸­ä¿æŒå‡åŒ€åˆ†å¸ƒå’Œçº¿æ€§å¯åˆ†æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96cd69d1b9a2218567a77df9dcacbb79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2742a35216d98c65b210c44c504fb414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db330404ca1d94a62043ea30b6d52c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb73a9d3683ad83e7a4738eca31b768b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6bca6895f3fa6aebf4f45de6def30d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tell-Me-Who-Your-Students-Are-GPT-Can-Generate-Valid-Multiple-Choice-Questions-When-Studentsâ€™-Mis-Understanding-Is-Hinted"><a href="#Tell-Me-Who-Your-Students-Are-GPT-Can-Generate-Valid-Multiple-Choice-Questions-When-Studentsâ€™-Mis-Understanding-Is-Hinted" class="headerlink" title="Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice   Questions When Studentsâ€™ (Mis)Understanding Is Hinted"></a>Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice   Questions When Studentsâ€™ (Mis)Understanding Is Hinted</h2><p><strong>Authors:Machi Shimmei, Masaki Uto, Yuichiroh Matsubayashi, Kentaro Inui, Aditi Mallavarapu, Noboru Matsuda</strong></p>
<p>The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT. </p>
<blockquote>
<p>æœ¬ç ”ç©¶çš„ä¸»è¦ç›®æ ‡æ˜¯å¼€å‘å¹¶è¯„ä¼°ä¸€ç§åˆ›æ–°æ€§çš„æç¤ºæŠ€æœ¯â€”â€”AnaQuestï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ã€‚åœ¨AnaQuestä¸­ï¼Œé€‰æ‹©é¡¹æ˜¯å…³äºå¤æ‚æ¦‚å¿µçš„å¥å­çº§æ–­è¨€ã€‚è¯¥æŠ€æœ¯ç»“åˆäº†å½¢æˆæ€§è¯„ä¼°å’Œç»ˆç»“æ€§è¯„ä¼°ã€‚åœ¨å½¢æˆæ€§é˜¶æ®µï¼Œå­¦ç”Ÿä»¥è‡ªç”±æ–‡æœ¬çš„å½¢å¼å›ç­”å…³äºç›®æ ‡æ¦‚å¿µçš„é—®é¢˜ã€‚åœ¨ç»ˆç»“æ€§è¯„ä¼°ä¸­ï¼ŒAnaQueståˆ†æè¿™äº›å›ç­”ä»¥ç”Ÿæˆæ­£ç¡®å’Œé”™è¯¯çš„æ–­è¨€ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆçš„MCQçš„æœ‰æ•ˆæ€§ï¼Œåº”ç”¨é¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰æ¯”è¾ƒäº†AnaQuestç”Ÿæˆçš„å¤šé¡¹é€‰æ‹©é¢˜ã€åŸºçº¿ChatGPTæç¤ºå’Œäººç±»åˆ¶ä½œçš„é¢˜ç›®ä¹‹é—´çš„é¡¹ç›®ç‰¹å¾ã€‚å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸“å®¶å¯¼å¸ˆè®¤ä¸ºAIæ¨¡å‹ç”Ÿæˆçš„MCQä¸äººç±»å¯¼å¸ˆåˆ›å»ºçš„é¢˜ç›®åŒæ ·æœ‰æ•ˆã€‚ç„¶è€Œï¼ŒåŸºäºIRTçš„åˆ†ææ˜¾ç¤ºï¼ŒAnaQuestç”Ÿæˆçš„é—®é¢˜â€”â€”ç‰¹åˆ«æ˜¯é‚£äº›å¸¦æœ‰é”™è¯¯æ–­è¨€ï¼ˆfoilï¼‰çš„é—®é¢˜â€”â€”åœ¨éš¾åº¦å’ŒåŒºåˆ†åº¦æ–¹é¢æ›´æ¥è¿‘äºäººç±»åˆ¶ä½œçš„é¢˜ç›®ï¼Œè€Œä¸æ˜¯ChatGPTç”Ÿæˆçš„é¢˜ç›®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05815v2">PDF</a> This is a pre-print version of a paper to appear in AIED2025. The   camera-ready version is available at   <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-99264-3_16">https://link.springer.com/chapter/10.1007/978-3-031-99264-3_16</a></p>
<p><strong>Summary</strong></p>
<p>AnaQuestæ˜¯ä¸€é¡¹æ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šé€‰é¢˜ï¼ˆMCQsï¼‰çš„åˆ›æ–°æç¤ºæŠ€æœ¯ã€‚å®ƒç»“åˆäº†å½¢æˆæ€§è¯„ä¼°å’Œç»ˆç»“æ€§è¯„ä¼°ï¼Œé€šè¿‡å¯¹å­¦ç”Ÿå…³äºç›®æ ‡æ¦‚å¿µçš„è‡ªç”±æ–‡æœ¬å›ç­”è¿›è¡Œåˆ†æï¼Œç”Ÿæˆæ­£ç¡®å’Œé”™è¯¯çš„æ–­è¨€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„MCQsä¸ä¸“å®¶æ•™å¸ˆç”Ÿæˆçš„é¢˜ç›®å…·æœ‰ç›¸åŒçš„æ•ˆåº¦ï¼Œä½†AnaQuestç”Ÿæˆçš„é—®é¢˜åœ¨éš¾åº¦å’ŒåŒºåˆ†åº¦ä¸Šæ›´æ¥è¿‘äººç±»åˆ¶ä½œçš„é¢˜ç›®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnaQuestæ˜¯ä¸€ç§ç”¨äºç”Ÿæˆå¤šé€‰é¢˜çš„åˆ›æ–°æç¤ºæŠ€æœ¯ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æŠ€æœ¯æ•´åˆäº†å½¢æˆæ€§å’Œç»ˆç»“æ€§è¯„ä¼°æ¥ç”Ÿæˆæ­£ç¡®å’Œé”™è¯¯çš„æ–­è¨€ä½œä¸ºé€‰æ‹©é¢˜é€‰é¡¹ã€‚</li>
<li>é€šè¿‡Item Response Theoryï¼ˆIRTï¼‰å¯¹AnaQuestç”Ÿæˆçš„é¢˜ç›®ä¸ChatGPTå’Œäººå·¥ç”Ÿæˆçš„é¢˜ç›®è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚</li>
<li>ä¸“å®¶æ•™å¸ˆè®¤ä¸ºAIæ¨¡å‹ç”Ÿæˆçš„MCQsä¸äººå·¥åˆ¶ä½œçš„é¢˜ç›®å…·æœ‰ç›¸åŒçš„æ•ˆåº¦ã€‚</li>
<li>AnaQuestç”Ÿæˆçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åŒ…å«é”™è¯¯æ–­è¨€çš„é€‰é¡¹ï¼Œåœ¨éš¾åº¦å’ŒåŒºåˆ†åº¦ä¸Šæ›´æ¥è¿‘äººç±»åˆ¶ä½œçš„é¢˜ç›®ã€‚</li>
<li>AnaQuestæŠ€æœ¯æœ‰åŠ©äºæé«˜é¢˜ç›®ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ï¼Œæœ‰åŠ©äºæ•™è‚²è¯„ä¼°å’Œå­¦ç”Ÿçš„å­¦ä¹ å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-796127144b71c2dde79b5d1ee4cdc67e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b78ae34db3d2b5dab6b10719dc61c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8da321da25cc845e4bb45e4abb6999ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53b52141e6f246a3f96284c333732c78.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Modality-Barrier-Universal-Embedding-Learning-with-Multimodal-LLMs"><a href="#Breaking-the-Modality-Barrier-Universal-Embedding-Learning-with-Multimodal-LLMs" class="headerlink" title="Breaking the Modality Barrier: Universal Embedding Learning with   Multimodal LLMs"></a>Breaking the Modality Barrier: Universal Embedding Learning with   Multimodal LLMs</h2><p><strong>Authors:Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, Jiankang Deng</strong></p>
<p>The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM&#39;s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¡†æ¶å·²æˆä¸ºå¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢å’Œèšç±»ä¸­ã€‚ç„¶è€Œï¼Œå®ƒçš„æœ‰æ•ˆæ€§å—åˆ°ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼šï¼ˆ1ï¼‰æ–‡æœ¬ä»¤ç‰Œæˆªæ–­ï¼Œï¼ˆ2ï¼‰å­¤ç«‹çš„å›¾åƒæ–‡æœ¬ç¼–ç ï¼Œä»¥åŠï¼ˆ3ï¼‰ç”±äºè¯è¢‹è¡Œä¸ºå¯¼è‡´çš„æˆåˆ†ç¼ºé™·ã€‚è™½ç„¶æœ€è¿‘çš„å¤šåª’ä½“è¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨é€šç”¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å­¦ä¹ å¯è½¬ç§»çš„å¤šæ¨¡æ€è¡¨ç¤ºæ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniMEï¼ˆé€šç”¨å¤šæ¨¡æ€åµŒå…¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨MLLMå­¦ä¹ ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„åˆ¤åˆ«è¡¨ç¤ºçš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡å¼ºå¤§çš„åŸºäºLLMçš„æ•™å¸ˆæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ¤åˆ«çŸ¥è¯†è’¸é¦ï¼Œä»¥å¢å¼ºMLLMçš„è¯­è¨€ç»„ä»¶çš„åµŒå…¥èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¡¬è´Ÿå¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥è¿›ä¸€æ­¥æ¨åŠ¨åˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆç¼“è§£å‡é˜´æ€§æ±¡æŸ“ï¼Œç„¶ååœ¨æ¯ä¸ªæ‰¹æ¬¡å†…ä¸ºæ¯ä¸ªå®ä¾‹é‡‡æ ·å¤šä¸ªç¡¬è´Ÿæ ·æœ¬ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†åˆ¤åˆ«åŠ›ï¼Œè€Œä¸”æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨MMEBåŸºå‡†æµ‹è¯•å’Œå¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬æ£€ç´¢ä»¥åŠç»„åˆæ£€ç´¢ã€‚ç»“æœè¡¨æ˜ï¼ŒUniMEåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡å®ç°äº†æ€§èƒ½æ”¹è¿›ï¼Œå±•ç°å‡ºå“è¶Šçš„åˆ¤åˆ«å’Œç»„åˆèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17432v3">PDF</a> 13 pages, 8 figures, Accepted by ACM MM2025, Project page:   <a target="_blank" rel="noopener" href="https://garygutc.github.io/UniME">https://garygutc.github.io/UniME</a></p>
<p><strong>Summary</strong></p>
<p>CLIPæ¡†æ¶åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ–¹é¢ï¼Œå°¤å…¶åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢å’Œèšç±»ä¸­ï¼Œå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ä½†å…¶å­˜åœ¨æ–‡æœ¬ä»¤ç‰Œæˆªæ–­ã€å­¤ç«‹çš„å›¾åƒæ–‡æœ¬ç¼–ç å’Œå› è¯è¢‹è¡Œä¸ºå¯¼è‡´çš„æˆåˆ†ç¼ºé™·ç­‰ä¸‰ä¸ªå…³é”®å±€é™æ€§ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åœ¨å­¦ä¹ å¯è½¬ç§»çš„å¤šæ¨¡æ€è¡¨ç¤ºæ–¹é¢çš„æ½œåŠ›ä»è¢«æ¢ç´¢ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºUniMEï¼ˆé€šç”¨å¤šæ¨¡æ€åµŒå…¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨MLLMså­¦ä¹ åˆ¤åˆ«è¡¨ç¤ºç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡å¼ºå¤§çš„LLMæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ¤åˆ«çŸ¥è¯†è’¸é¦ï¼Œä»¥å¢å¼ºMLLMè¯­è¨€ç»„ä»¶çš„åµŒå…¥èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µï¼Œå¼•å…¥å¢å¼ºæŒ‡ä»¤è°ƒæ•´ç¡¬è´Ÿæ ·æœ¬è¿›ä¸€æ­¥ä¼˜åŒ–åˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å‡å°‘å‡é˜´æ€§æ±¡æŸ“å¹¶å¯¹æ¯ä¸ªå®ä¾‹åœ¨æ‰¹æ¬¡å†…é‡‡æ ·å¤šä¸ªç¡¬è´Ÿæ ·æœ¬ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†åˆ¤åˆ«åŠ›ï¼Œè¿˜æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚åœ¨MMEBåŸºå‡†æµ‹è¯•å’Œå¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniMEåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½æ”¹è¿›ï¼Œè¡¨ç°å‡ºå“è¶Šçš„åˆ¤åˆ«å’Œç»„åˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¡†æ¶åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨æ–‡æœ¬ä»¤ç‰Œæˆªæ–­ã€å›¾åƒæ–‡æœ¬ç¼–ç å­¤ç«‹å’Œæˆåˆ†ç¼ºé™·ç­‰å±€é™æ€§ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å­¦ä¹ å¯è½¬ç§»çš„å¤šæ¨¡æ€è¡¨ç¤ºæ–¹é¢æ½œåŠ›æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>UniMEæ˜¯ä¸€ä¸ªåˆ©ç”¨MLLMsçš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„åˆ¤åˆ«è¡¨ç¤ºã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¼ºå¤§çš„LLMæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ¤åˆ«çŸ¥è¯†è’¸é¦ä»¥å¢å¼ºåµŒå…¥èƒ½åŠ›ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¼•å…¥å¢å¼ºæŒ‡ä»¤è°ƒæ•´ç¡¬è´Ÿæ ·æœ¬ï¼Œæé«˜åˆ¤åˆ«åŠ›å¹¶å¢å¼ºæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>UniMEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½æ”¹è¿›ï¼Œè¡¨ç°å‡ºå“è¶Šçš„åˆ¤åˆ«å’Œç»„åˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc8ef9ecd92cd1195e100ec30b2c7fdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d95b642890279ec57cc11c85802fdcdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8716d55020311824a2426daecc785a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f5ff6f281eb4279c4068960ddbd21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b69f51a25e9b500301043305c5456397.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ArXivBench-When-You-Should-Avoid-Using-ChatGPT-for-Academic-Writing"><a href="#ArXivBench-When-You-Should-Avoid-Using-ChatGPT-for-Academic-Writing" class="headerlink" title="ArXivBench: When You Should Avoid Using ChatGPT for Academic Writing"></a>ArXivBench: When You Should Avoid Using ChatGPT for Academic Writing</h2><p><strong>Authors:Ning Li, Jingran Zhang, Justin Cui</strong></p>
<p>Large language models (LLMs) demonstrate strong capabilities in reasoning and question answering, yet their tendency to generate factually incorrect content remains a critical challenge. This study evaluates proprietary and open-source LLMs on generating relevant research papers with accurate arXiv links. Our evaluation reveals critical academic risks: LLMs frequently generate incorrect arXiv links or references to non-existent papers, fundamentally undermining their ability to properly attribute research contributions to the actual authors. We introduce arXivBench, a benchmark specifically designed to assess LLM performance across eight major subject categories on arXiv and five subfields within computer science, one of the most popular categories among them. Our findings show concerning accuracy variations across subjects, with Claude-3.5-Sonnet exhibiting a substantial advantage in generating both relevant and accurate responses. Notably, most LLMs perform significantly better in Artificial Intelligence than other subfields. This benchmark provides a standardized tool for evaluating LLM reliability in scientific contexts, promoting more dependable academic use in research environments. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/liningresearch/arXivBench">https://github.com/liningresearch/arXivBench</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/arXivBenchLLM/arXivBench">https://huggingface.co/datasets/arXivBenchLLM/arXivBench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’Œé—®ç­”æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ç”Ÿæˆäº‹å®é”™è¯¯å†…å®¹çš„å€¾å‘ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸“æœ‰å’Œå¼€æºçš„LLMåœ¨ç”Ÿæˆå¸¦æœ‰å‡†ç¡®arXivé“¾æ¥çš„ç›¸å…³ç ”ç©¶è®ºæ–‡æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†é‡è¦çš„å­¦æœ¯é£é™©ï¼šLLMç»å¸¸ç”Ÿæˆé”™è¯¯çš„arXivé“¾æ¥æˆ–å¼•ç”¨ä¸å­˜åœ¨çš„è®ºæ–‡ï¼Œä»æ ¹æœ¬ä¸Šå‰Šå¼±äº†å®ƒä»¬æ­£ç¡®å½’å±ç ”ç©¶è´¡çŒ®ç»™å®é™…ä½œè€…çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†arXivBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMåœ¨arXivå…«å¤§ä¸»é¢˜ç±»åˆ«å’Œè®¡ç®—æœºç§‘å­¦äº”å¤§å­é¢†åŸŸæ€§èƒ½çš„å·¥å…·ï¼Œè¿™æ˜¯å®ƒä»¬ä¸­æœ€å—æ¬¢è¿çš„ä¸€ä¸ªç±»åˆ«ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°åœ¨ä¸åŒä¸»é¢˜ä¹‹é—´å‡ºç°äº†ä»¤äººæ‹…å¿§çš„å‡†ç¡®æ€§å˜åŒ–ï¼Œå…¶ä¸­Claude-3.5-Sonnetåœ¨ç”Ÿæˆæ—¢ç›¸å…³åˆå‡†ç¡®å“åº”æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§å¤šæ•°LLMåœ¨äººå·¥æ™ºèƒ½æ–¹é¢çš„è¡¨ç°è¦å¥½äºå…¶ä»–å­é¢†åŸŸã€‚è¿™ä¸ªåŸºå‡†å·¥å…·ä¸ºè¯„ä¼°LLMåœ¨ç§‘å­¦ç¯å¢ƒä¸­çš„å¯é æ€§æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ï¼Œä¿ƒè¿›äº†åœ¨å­¦æœ¯ç¯å¢ƒä¸­æ›´å¯é çš„ç”¨é€”ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/liningresearch/arXivBench%E5%92%8Chttps://huggingface.co/datasets/arXivBenchLLM/arXivBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liningresearch/arXivBenchå’Œhttps://huggingface.co/datasets/arXivBenchLLM/arXivBenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10496v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’Œé—®ç­”æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”Ÿæˆäº‹å®é”™è¯¯å†…å®¹çš„é—®é¢˜ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¯¹ä¸“æœ‰å’Œå¼€æºLLMç”Ÿæˆå…·æœ‰å‡†ç¡®arXivé“¾æ¥çš„ç›¸å…³ç ”ç©¶è®ºæ–‡çš„èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºå‡ºé‡è¦çš„å­¦æœ¯é£é™©ï¼šLLMç»å¸¸ç”Ÿæˆé”™è¯¯çš„arXivé“¾æ¥æˆ–å¼•ç”¨ä¸å­˜åœ¨çš„è®ºæ–‡ï¼Œä»æ ¹æœ¬ä¸Šç ´åäº†å…¶æ­£ç¡®å½’å±ç ”ç©¶è´¡çŒ®çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼•å…¥äº†arXivBenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMåœ¨arXivå…«å¤§ä¸»é¢˜ç±»åˆ«å’Œè®¡ç®—æœºç§‘å­¦äº”ä¸ªå­é¢†åŸŸçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå„ä¸»é¢˜ä¹‹é—´çš„å‡†ç¡®ç‡å­˜åœ¨å·®å¼‚ï¼Œå…¶ä¸­Claude-3.5-Sonnetåœ¨ç”Ÿæˆç›¸å…³å’Œå‡†ç¡®å“åº”æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§å¤šæ•°LLMåœ¨äººå·¥æ™ºèƒ½æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–å­é¢†åŸŸã€‚æ­¤åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°LLMåœ¨ç§‘å­¦ç¯å¢ƒä¸­çš„å¯é æ€§æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ï¼Œä¿ƒè¿›äº†ç ”ç©¶ç¯å¢ƒä¸­æ›´å¯é çš„å­¦æœ¯ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿæˆå¸¦æœ‰å‡†ç¡®arXivé“¾æ¥çš„ç ”ç©¶è®ºæ–‡æ—¶å­˜åœ¨äº‹å®é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>LLMæœ‰æ—¶ä¼šç”Ÿæˆé”™è¯¯çš„arXivé“¾æ¥æˆ–å¼•ç”¨ä¸å­˜åœ¨çš„è®ºæ–‡ï¼Œè¿™å½±å“äº†å¯¹ç ”ç©¶è´¡çŒ®çš„æ­£ç¡®å½’å±ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·arXivBenchï¼Œç”¨äºè¯„ä¼°LLMåœ¨å¤šä¸ªå­¦æœ¯é¢†åŸŸï¼ˆåŒ…æ‹¬è®¡ç®—æœºç§‘å­¦çš„å­é¢†åŸŸï¼‰çš„è¡¨ç°ã€‚</li>
<li>åœ¨ä¸åŒçš„ä¸»é¢˜ç±»åˆ«ä¸­ï¼ŒLLMçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œå…¶ä¸­Claude-3.5-Sonnetåœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>LLMåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„è¡¨ç°é€šå¸¸ä¼˜äºå…¶ä»–å­é¢†åŸŸã€‚</li>
<li>arXivBenchæä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å·¥å…·æ¥è¯„ä¼°LLMåœ¨ç§‘å­¦ç¯å¢ƒä¸­çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a441f7e04cd3954db8e47b9c4112e5c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81bab22a40fe8b5dd2e23c7d267ce4b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-876cc5585637479a30d851190fef08a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4fda89dd95e5b67423fc1c05c1b964.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aab5e979382e06953f30ceaddbb6fcf4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R2Vul-Learning-to-Reason-about-Software-Vulnerabilities-with-Reinforcement-Learning-and-Structured-Reasoning-Distillation"><a href="#R2Vul-Learning-to-Reason-about-Software-Vulnerabilities-with-Reinforcement-Learning-and-Structured-Reasoning-Distillation" class="headerlink" title="R2Vul: Learning to Reason about Software Vulnerabilities with   Reinforcement Learning and Structured Reasoning Distillation"></a>R2Vul: Learning to Reason about Software Vulnerabilities with   Reinforcement Learning and Structured Reasoning Distillation</h2><p><strong>Authors:Martin Weyssow, Chengran Yang, Junkai Chen, Ratnadira Widyasari, Ting Zhang, Huihui Huang, Huu Hung Nguyen, Yan Naing Tun, Tan Bui, Yikun Li, Ang Han Wei, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, David Lo</strong></p>
<p>Large language models (LLMs) have shown promising performance in software vulnerability detection, yet their reasoning capabilities remain unreliable. We propose R2Vul, a method that combines reinforcement learning from AI feedback (RLAIF) and structured reasoning distillation to teach small code LLMs to detect vulnerabilities while generating security-aware explanations. Unlike prior chain-of-thought and instruction tuning approaches, R2Vul rewards well-founded over deceptively plausible vulnerability explanations through RLAIF, which results in more precise detection and high-quality reasoning generation. To support RLAIF, we construct the first multilingual preference dataset for vulnerability detection, comprising 18,000 high-quality samples in C#, JavaScript, Java, Python, and C. We evaluate R2Vul across five programming languages and against four static analysis tools, eight state-of-the-art LLM-based baselines, and various fine-tuning approaches. Our results demonstrate that a 1.5B R2Vul model exceeds the performance of its 32B teacher model and leading commercial LLMs such as Claude-4-Opus. Furthermore, we introduce a lightweight calibration step that reduces false positive rates under varying imbalanced data distributions. Finally, through qualitative analysis, we show that both LLM and human evaluators consistently rank R2Vul modelâ€™s reasoning higher than other reasoning-based baselines. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶æ¼æ´æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ä»ç„¶ä¸å¯é ã€‚æˆ‘ä»¬æå‡ºäº†R2Vulæ–¹æ³•ï¼Œå®ƒç»“åˆäº†äººå·¥æ™ºèƒ½åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰å’Œç»“æ„åŒ–æ¨ç†è’¸é¦æŠ€æœ¯ï¼Œç”¨äºæ•™æˆå°å‹ä»£ç LLMæ£€æµ‹æ¼æ´ï¼ŒåŒæ—¶ç”Ÿæˆå®‰å…¨æ„è¯†çš„è§£é‡Šã€‚ä¸å…ˆå‰çš„æ€ç»´é“¾å’ŒæŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸åŒï¼ŒR2Vulé€šè¿‡RLAIFå¥–åŠ±åŸºäºäº‹å®çš„æ¼æ´è§£é‡Šï¼Œè¿™äº›è§£é‡Šé¿å…äº†ä¼¼æ˜¯è€Œéçš„æ¬ºéª—æ€§ï¼Œä»è€Œå®ç°äº†æ›´ç²¾ç¡®çš„æ£€æµ‹å’Œé«˜è´¨é‡çš„æ¨ç†ç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒRLAIFï¼Œæˆ‘ä»¬æ„å»ºäº†ç”¨äºæ¼æ´æ£€æµ‹çš„ç¬¬ä¸€å¥—å¤šè¯­è¨€åå¥½æ•°æ®é›†ï¼ŒåŒ…å«C#ã€JavaScriptã€Javaã€Pythonå’ŒCä¸­çš„18,000ä¸ªé«˜è´¨é‡æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹äº”ç§ç¼–ç¨‹è¯­è¨€è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸å››ç§é™æ€åˆ†æå·¥å…·ã€å…«ä¸ªæœ€æ–°çš„LLMåŸºå‡†æ¨¡å‹å’Œå„ç§å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸€ä¸ªè§„æ¨¡ä¸º1.5Bçš„R2Vulæ¨¡å‹è¶…è¿‡äº†å…¶è§„æ¨¡ä¸º32Bçš„æ•™å¸ˆæ¨¡å‹å’Œé¢†å…ˆçš„å•†ä¸šLLMï¼ˆå¦‚Claude-4-Opusï¼‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„æ ¡å‡†æ­¥éª¤ï¼Œè¯¥æ­¥éª¤å¯ä»¥é™ä½åœ¨ä¸åŒä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒä¸‹çš„è¯¯æŠ¥ç‡ã€‚æœ€åï¼Œé€šè¿‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯æ˜LLMå’Œäººç±»è¯„ä¼°è€…éƒ½ä¸€è‡´åœ°è®¤ä¸ºR2Vulæ¨¡å‹çš„æ¨ç†èƒ½åŠ›é«˜äºå…¶ä»–åŸºäºæ¨ç†çš„åŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04699v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›å°šå¾…æé«˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†R2Vulæ–¹æ³•ï¼Œå®ƒç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œç»“æ„åŒ–æ¨ç†è’¸é¦æŠ€æœ¯ï¼Œç”¨äºæ•™æˆå°å‹ä»£ç LLMæ£€æµ‹æ¼æ´å¹¶ç”Ÿæˆå®‰å…¨æ„è¯†çš„è§£é‡Šã€‚R2Vulé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰å¥–åŠ±æœ‰æ ¹æ®çš„ã€ä¸æ¬ºéª—æ€§çš„æ¼æ´è§£é‡Šï¼Œä»è€Œæé«˜æ¼æ´æ£€æµ‹çš„ç²¾ç¡®æ€§å’Œé«˜è´¨é‡æ¨ç†ç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒRLAIFï¼Œç ”ç©¶è€…æ„å»ºäº†é¦–ä¸ªç”¨äºæ¼æ´æ£€æµ‹çš„å¤šè¯­è¨€åå¥½æ•°æ®é›†ï¼ŒåŒ…å«C#ã€JavaScriptã€Javaã€Pythonå’ŒCçš„18,000é«˜è´¨é‡æ ·æœ¬ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒR2Vulæ¨¡å‹æ€§èƒ½è¶…è¶Šå¤§å‹æ¨¡å‹åŠå•†ä¸šLLMï¼Œå¹¶å¼•å…¥è½»é‡çº§æ ¡å‡†æ­¥éª¤ä»¥é™ä½ä¸åŒä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒä¸‹çš„è¯¯æŠ¥ç‡ã€‚å®šæ€§åˆ†ææ˜¾ç¤ºï¼ŒLLMå’Œäººç±»è¯„ä¼°è€…å‡è®¤ä¸ºR2Vulæ¨¡å‹çš„æ¨ç†èƒ½åŠ›é«˜äºå…¶ä»–åŸºäºæ¨ç†çš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ¨ç†èƒ½åŠ›éœ€æå‡ã€‚</li>
<li>R2Vulæ–¹æ³•ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œç»“æ„åŒ–æ¨ç†è’¸é¦æŠ€æœ¯ï¼Œç”¨äºæ•™æˆLLMæ£€æµ‹æ¼æ´å¹¶ç”Ÿæˆå®‰å…¨æ„è¯†çš„è§£é‡Šã€‚</li>
<li>RLAIFé€šè¿‡å¥–åŠ±æœ‰æ ¹æ®çš„ã€ä¸æ¬ºéª—æ€§çš„æ¼æ´è§£é‡Šï¼Œæé«˜æ¼æ´æ£€æµ‹çš„ç²¾ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚</li>
<li>ç ”ç©¶è€…æ„å»ºäº†é¦–ä¸ªç”¨äºæ¼æ´æ£€æµ‹çš„å¤šè¯­è¨€åå¥½æ•°æ®é›†ã€‚</li>
<li>R2Vulæ¨¡å‹æ€§èƒ½è¶…è¶Šå¤§å‹æ¨¡å‹åŠå•†ä¸šLLMã€‚</li>
<li>å¼•å…¥è½»é‡çº§æ ¡å‡†æ­¥éª¤ä»¥é™ä½è¯¯æŠ¥ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2962813dc677a286a0f1ddd6c685ab7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c182b1ab637ac0e1b5995fb3710cbcdf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b85547f1b52c2d5b07c34fbe7eda4939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e656244b8300eb0f9ec21c57d41a7ba6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SciReplicate-Bench-Benchmarking-LLMs-in-Agent-driven-Algorithmic-Reproduction-from-Research-Papers"><a href="#SciReplicate-Bench-Benchmarking-LLMs-in-Agent-driven-Algorithmic-Reproduction-from-Research-Papers" class="headerlink" title="SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic   Reproduction from Research Papers"></a>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic   Reproduction from Research Papers</h2><p><strong>Authors:Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He</strong></p>
<p>This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency&#x2F;API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmarkâ€™s difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at <a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench">https://github.com/xyzCS/SciReplicate-Bench</a> and project homepage at <a target="_blank" rel="noopener" href="https://xyzcs.github.io/scireplicate.github.io/">https://xyzcs.github.io/scireplicate.github.io/</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ¹æ®æœ€æ–°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®ºæ–‡ä¸­çš„ç®—æ³•æè¿°ç”Ÿæˆä»£ç çš„èƒ½åŠ›ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦ä¸¤ä¸ªå…³é”®æŠ€èƒ½ï¼šï¼ˆ1ï¼‰ç®—æ³•ç†è§£ï¼šä»è®ºæ–‡å’Œå­¦æœ¯æ–‡çŒ®ä¸­ç»¼åˆä¿¡æ¯ï¼Œä»¥ç†è§£å®ç°é€»è¾‘ï¼›ï¼ˆ2ï¼‰ç¼–ç ä¸“ä¸šçŸ¥è¯†ï¼šè¯†åˆ«ä¾èµ–å…³ç³»å¹¶æ­£ç¡®å®ç°å¿…è¦çš„APIã€‚ä¸ºäº†è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SciReplicate-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ¥è‡ª2024å¹´å‘è¡¨çš„36ç¯‡NLPè®ºæ–‡çš„100ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰è¯¦ç»†çš„æ³¨é‡Šå’Œå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ã€‚åŸºäºSciReplicate-Benchï¼Œæˆ‘ä»¬æå‡ºäº†Sci-Reproducerï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªä»£ç†ç»„æˆçš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç†è§£æ–‡çŒ®ä¸­ç®—æ³•æ¦‚å¿µçš„Paper Agentå’Œä¸€ä¸ªä»å­˜å‚¨åº“ä¸­æ£€ç´¢ä¾èµ–å…³ç³»å¹¶å®ç°è§£å†³æ–¹æ¡ˆçš„Code Agentã€‚ä¸ºäº†è¯„ä¼°ç®—æ³•ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨ç†å›¾å‡†ç¡®æ€§ï¼Œå®ƒé‡åŒ–äº†ä»ä»£ç æ³¨é‡Šå’Œç»“æ„æ´¾ç”Ÿçš„ç”Ÿæˆæ¨ç†å›¾ä¸å‚è€ƒæ¨ç†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è¯„ä¼°å®ç°è´¨é‡ï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰§è¡Œå‡†ç¡®æ€§ã€CodeBLEUä»¥åŠå­˜å‚¨åº“ä¾èµ–&#x2F;APIå¬å›æŒ‡æ ‡ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å„ç§å¼ºå¤§çš„éæ¨ç†å’Œæ¨ç†LLMä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚è¡¨ç°æœ€ä½³çš„LLMä½¿ç”¨ModelName~ä»…è¾¾åˆ°39%çš„æ‰§è¡Œå‡†ç¡®æ€§ï¼Œè¿™çªå‡ºäº†è¯¥åŸºå‡†æµ‹è¯•çš„å›°éš¾ç¨‹åº¦ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†ç¼ºå¤±æˆ–ä¸ä¸€è‡´çš„ç®—æ³•æè¿°æ˜¯æˆåŠŸå¤åˆ¶çš„ä¸»è¦éšœç¢ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%BB%E9%A1%B5https://xyzcs.github.io/scireplicate.github.io/%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/xyzCS/SciReplicate-Benchå’Œé¡¹ç›®ä¸»é¡µhttps://xyzcs.github.io/scireplicate.github.io/ä¸Šæä¾›äº†æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00255v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ¹æ®æœ€æ–°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®ºæ–‡ä¸­çš„ç®—æ³•æè¿°ç”Ÿæˆä»£ç çš„èƒ½åŠ›ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦ä¸¤ä¸ªå…³é”®æŠ€èƒ½ï¼š1ï¼‰ç®—æ³•ç†è§£ï¼šä»è®ºæ–‡å’Œå­¦æœ¯æ–‡çŒ®ä¸­ç»¼åˆä¿¡æ¯ä»¥ç†è§£å®ç°é€»è¾‘ï¼›2ï¼‰ç¼–ç ä¸“ä¸šçŸ¥è¯†ï¼šè¯†åˆ«ä¾èµ–å…³ç³»å¹¶æ­£ç¡®å®ç°å¿…è¦çš„APIã€‚ä¸ºäº†è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SciReplicate-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±36ç¯‡äºXXXXå¹´å‘è¡¨çš„NLPè®ºæ–‡ä¸­çš„XXXXä¸ªä»»åŠ¡ç»„æˆçš„åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰è¯¦ç»†çš„æ³¨é‡Šå’Œå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ã€‚åŸºäºSciReplicate-Benchï¼Œæˆ‘ä»¬æå‡ºäº†Sci-Reproducerï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è®ºæ–‡ä»£ç†å’Œä»£ç ä»£ç†ç»„æˆçš„åŒä»£ç†æ¡†æ¶ï¼Œè®ºæ–‡ä»£ç†è´Ÿè´£ä»æ–‡çŒ®ä¸­è§£é‡Šç®—æ³•æ¦‚å¿µï¼Œè€Œä»£ç ä»£ç†åˆ™ä»å­˜å‚¨åº“ä¸­æ£€ç´¢ä¾èµ–å…³ç³»å¹¶å®ç°è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è¯„ä¼°ç®—æ³•ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨ç†å›¾å‡†ç¡®æ€§ï¼Œè¯¥æŒ‡æ ‡é‡åŒ–äº†ä»ä»£ç æ³¨é‡Šå’Œç»“æ„æ´¾ç”Ÿçš„ç”Ÿæˆæ¨ç†å›¾å’Œå‚è€ƒæ¨ç†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è¯„ä¼°å®ç°è´¨é‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ‰§è¡Œå‡†ç¡®æ€§ã€CodeBLEUä»¥åŠå­˜å‚¨åº“ä¾èµ–&#x2F;APIå¬å›ç‡æŒ‡æ ‡ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å¯¹å„ç§å¼ºå¤§çš„éæ¨ç†å’Œæ¨ç†LLMä½œä¸ºåŸºç¡€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚è¡¨ç°æœ€ä½³çš„LLMæ¨¡å‹åç§°~ä»…è¾¾åˆ°XXXX%çš„æ‰§è¡Œå‡†ç¡®æ€§ï¼Œå‡¸æ˜¾äº†æœ¬åŸºå‡†æµ‹è¯•çš„å›°éš¾åº¦ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†ç¼ºå¤±æˆ–ä¸ä¸€è‡´çš„ç®—æ³•æè¿°æ˜¯æˆåŠŸå¤åˆ¶çš„ä¸»è¦éšœç¢ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench%E4%B8%8A%E8%8E%B7%E5%8F%AF%E4%BB%A5%E9%A1%B5%E9%A1%B5%E9%9D%A2%E4%B8%BAhttps://xyzcs.github.io/scireplicate.github.io/">https://github.com/xyzCS/SciReplicate-Benchä¸Šè·å–ï¼Œé¡¹ç›®ä¸»é¡µä¸ºhttps://xyzcs.github.io/scireplicate.github.io/ã€‚</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ¹æ®NLPè®ºæ–‡ä¸­çš„ç®—æ³•æè¿°ç”Ÿæˆä»£ç çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†SciReplicate-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ªæœ€æ–°NLPè®ºæ–‡çš„ä»»åŠ¡ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°LLMåœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å¼•å…¥Sci-Reproduceræ¡†æ¶ï¼ŒåŒ…å«è®ºæ–‡ä»£ç†å’Œä»£ç ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£ç®—æ³•ç†è§£å’Œä»£ç å®ç°ã€‚</li>
<li>é‡‡ç”¨å¤šç§æŒ‡æ ‡è¯„ä¼°ç®—æ³•ç†è§£å’Œä»£ç å®ç°è´¨é‡ï¼ŒåŒ…æ‹¬æ¨ç†å›¾å‡†ç¡®æ€§ã€æ‰§è¡Œå‡†ç¡®æ€§ã€CodeBLEUä»¥åŠå­˜å‚¨åº“ä¾èµ–&#x2F;APIå¬å›ç‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€ä¼˜ç§€çš„LLMæ¨¡å‹ï¼Œåœ¨æ‰§è¡Œå‡†ç¡®æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™çªæ˜¾äº†æ­¤ä»»åŠ¡çš„å¤æ‚æ€§ã€‚</li>
<li>åˆ†æå’ŒæŒ‡å‡ºç®—æ³•æè¿°çš„ç¼ºå¤±æˆ–ä¸ä¸€è‡´æ˜¯å¤åˆ¶è¿‡ç¨‹ä¸­çš„ä¸»è¦éš¾é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-418037b50dd90d2816dc080497441c3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e410978de51867cd150f7d3627453c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a572f9fb0cc02fccde0bd660398cac5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348e4e7f2e6f8fb76e5d7e5b10d42240.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning"><a href="#Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning" class="headerlink" title="Teaching LLMs How to Learn with Contextual Fine-Tuning"></a>Teaching LLMs How to Learn with Contextual Fine-Tuning</h2><p><strong>Authors:Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan</strong></p>
<p>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When humanâ€™s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, â€œcan prompting help us teach LLMs how to learnâ€. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the modelâ€™s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains. </p>
<blockquote>
<p>æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–æä¾›é¢„æœŸçš„æ¨¡å‹æ“ä½œä¸Šä¸‹æ–‡ï¼Œæ˜¯å¼•å¯¼æ­¤ç±»æ¨¡å‹çš„è¾“å‡ºä»¥æ»¡è¶³å…¶åœ¨è®­ç»ƒåçš„äººç±»éœ€æ±‚çš„æœ‰æ•ˆæ–¹å¼ã€‚ä½†åœ¨å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œé€šå¸¸éœ€è¦å¾®è°ƒLLMï¼Œä»¥æ”¹å–„å…¶å†…å­˜ä¸­çš„çŸ¥è¯†ç±»å‹æˆ–åœ¨æ–°é¢†åŸŸè¿›è¡Œå¼€æ”¾å¼æ¨ç†çš„èƒ½åŠ›ã€‚å½“äººç±»å­¦ä¹ æ–°æ¦‚å¿µæ—¶ï¼Œæˆ‘ä»¬å¸¸å¸¸é€šè¿‡å°†æ­£åœ¨ç ”ç©¶çš„æ–°ææ–™ä¸å·²ç»å­¦è¿‡çš„æ¦‚å¿µè”ç³»èµ·æ¥æ¥å­¦ä¹ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šâ€œæç¤ºèƒ½å¦å¸®åŠ©æˆ‘ä»¬æ•™ä¼šLLMå¦‚ä½•å­¦ä¹ â€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§æŒ‡ä»¤å¾®è°ƒçš„æ–°æ³›åŒ–ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡å¾®è°ƒï¼Œä»¥å¾®è°ƒLLMã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æŒ‡ä»¤æç¤ºï¼Œæ¨¡ä»¿äººç±»åœ¨å­¦ä¹ å’Œè§£å†³é—®é¢˜ä¸­çš„è®¤çŸ¥ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„è§£é‡Šå’Œç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜ï¼Œè¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›æé«˜äº†LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›ï¼Œæ— è®ºæ˜¯åœ¨åŒ»ç–—è¿˜æ˜¯é‡‘èé¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09032v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œé€šè¿‡æç¤ºç‰¹å®šæ¨¡å‹æ“ä½œæ¨¡å¼å¯ä»¥æœ‰æ•ˆå¼•å¯¼æ¨¡å‹è¾“å‡ºä»¥æ»¡è¶³äººç±»éœ€æ±‚ã€‚åœ¨å¿«é€Ÿå˜åŒ–çš„é¢†åŸŸä¸­ï¼Œéœ€è¦å¾®è°ƒLLMä»¥æé«˜å…¶è®°å¿†çŸ¥è¯†æˆ–åœ¨æ–°é¢†åŸŸè¿›è¡Œå¼€æ”¾å¼æ¨ç†çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡å¾®è°ƒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ¨¡æ‹Ÿäººç±»å­¦ä¹ å’Œè§£å†³é—®é¢˜çš„è®¤çŸ¥ç­–ç•¥çš„æŒ‡ä»¤æç¤ºæ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçš„ç†è§£å’Œè§£é‡Šèƒ½åŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡å®è¯è¯æ˜ï¼Œè¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•èƒ½æé«˜LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¾®è°ƒèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—å’Œé‡‘èé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡æç¤ºå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä»¥æ»¡è¶³äººç±»éœ€æ±‚ã€‚</li>
<li>åœ¨å¿«é€Ÿå˜åŒ–çš„é¢†åŸŸä¸­éœ€è¦å¾®è°ƒLLMã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡å¾®è°ƒã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒåˆ©ç”¨æŒ‡ä»¤æç¤ºæ¨¡æ‹Ÿäººç±»å­¦ä¹ å’Œè§£å†³é—®é¢˜çš„è®¤çŸ¥ç­–ç•¥ã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçš„ç†è§£å’Œè§£é‡Šèƒ½åŠ›ã€‚</li>
<li>å®è¯è¯æ˜ä¸Šä¸‹æ–‡å¾®è°ƒèƒ½æé«˜LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¾®è°ƒèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38e2747557f574c3120eac5e9d048218.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd511298501f664e0a21061cd7e623f2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information"><a href="#Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information" class="headerlink" title="Transformer Meets Twicing: Harnessing Unattended Residual Information"></a>Transformer Meets Twicing: Harnessing Unattended Residual Information</h2><p><strong>Authors:Laziz Abdullaev, Tan M. Nguyen</strong></p>
<p>Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses kernel twicing procedure in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees and enhanced adversarial robustness. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å·²åœ¨ä¼—å¤šè¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶Transformerçš„æ ¸å¿ƒç»„ä»¶è‡ªæ³¨æ„åŠ›æœºåˆ¶å·²è¯æ˜èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ï¼Œä½†äººä»¬è§‚å¯Ÿåˆ°ï¼Œæ³¨æ„åŠ›çŸ©é˜µåœ¨Transformerå±‚é—´çš„è¡¨ç¤ºèƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œä»è€ŒæŸå®³å…¶æ€»ä½“æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸ä½é€šéå±€éƒ¨å‡å€¼ï¼ˆNLMï¼‰å¹³æ»‘æ»¤æ³¢å™¨ä¹‹é—´çš„è”ç³»ï¼Œæå‡ºäº†Twicing Attentionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒé€šè¿‡éå‚æ•°å›å½’ä¸­çš„æ ¸twicingç¨‹åºæ¥ç¼“è§£ä¸è‡ªæ³¨æ„åŠ›ç›¸å…³çš„NLMå¹³æ»‘çš„ä½é€šè¡Œä¸ºï¼Œå…·æœ‰å¼•äººæ³¨ç›®çš„ç†è®ºä¿è¯å’Œå¢å¼ºçš„å¯¹æŠ—ç¨³å¥æ€§ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—èƒ½å¤Ÿåœ¨æ¯ä¸€å±‚ä¸å®Œç¾çš„å¹³æ»‘æ“ä½œåæå–å’Œå†åˆ©ç”¨æ®‹å·®ä¸­ä¿ç•™çš„æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç›¸å¯¹äºæ ‡å‡†è‡ªæ³¨æ„åŠ›å…·æœ‰ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼š1ï¼‰è¡¨ç¤ºèƒ½åŠ›çš„è¡°å‡é€Ÿåº¦è¾ƒæ…¢ï¼›2ï¼‰åœ¨å„ç§æ•°æ®æ¨¡æ€å’Œä»»åŠ¡ä¸­æé«˜äº†ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šå®è¯åœ°å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹ç›¸å¯¹äºåŸºçº¿Transformerçš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ï¼Œä»¥åŠå¹²å‡€å’ŒæŸåçš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00687v3">PDF</a> 10 pages in the main text. Published at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹Transformeræ¨¡å‹åœ¨æ·±å±‚å­¦ä¹ ä¸­å­˜åœ¨çš„ä¸è¶³ï¼ˆå³æ³¨æ„çŸ©é˜µçš„ä»£è¡¨æ€§å®¹é‡åœ¨Transformerå±‚ä¸­æ˜¾è‘—ä¸‹é™ï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶â€”â€”Twicing Attentionã€‚è¯¥æœºåˆ¶åˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—å’Œå±€éƒ¨å‡å€¼å¹³æ»‘æ»¤æ³¢å™¨ä¹‹é—´çš„è”ç³»ï¼Œé€šè¿‡éå‚æ•°å›å½’ä¸­çš„æ ¸è°ƒæ•´è¿‡ç¨‹æ¥å®ç°å¯¹å¹³æ»‘æ»¤æ³¢å™¨çš„ä½é€šè¡Œä¸ºçš„ç¼“è§£ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå–å’Œé‡ç”¨æ¯ä¸€å±‚çš„ä¸å®Œç¾å¹³æ»‘æ“ä½œåçš„æ®‹å·®ä¸­çš„æœ‰æ„ä¹‰ä¿¡æ¯ã€‚ç›¸æ¯”äºæ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒTwicing Attentionæä¾›äº†ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šä¿è¯çš„ä»£è¡¨æ€§å®¹é‡è¡°å‡è¾ƒæ…¢ï¼Œä»¥åŠåœ¨ä¸åŒæ•°æ®æ¨¡æ€å’Œä»»åŠ¡ä¸Šæé«˜äº†é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚ä½œè€…åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šå®è¯äº†æ¨¡å‹æ€§èƒ½çš„æå‡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ï¼Œä»¥åŠå¹²å‡€å’Œå—æ±¡æŸ“çš„æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†å¤æ‚æ•°æ®æ¨¡å¼æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†æ³¨æ„çŸ©é˜µçš„ä»£è¡¨æ€§å®¹é‡åœ¨Transformerå±‚ä¸­ä¼šæ˜¾è‘—ä¸‹é™ï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶â€”â€”Twicing Attentionï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—å’Œå±€éƒ¨å‡å€¼å¹³æ»‘æ»¤æ³¢å™¨ä¹‹é—´çš„è”ç³»ï¼Œé€šè¿‡æ ¸è°ƒæ•´è¿‡ç¨‹æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Twicing Attentionèƒ½å¤Ÿæå–å’Œé‡ç”¨æ¯ä¸€å±‚çš„ä¸å®Œç¾å¹³æ»‘æ“ä½œåçš„æ®‹å·®ä¸­çš„æœ‰æ„ä¹‰ä¿¡æ¯ã€‚</li>
<li>Twicing Attentionç›¸æ¯”æ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶å…·æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šä¿è¯çš„ä»£è¡¨æ€§å®¹é‡è¡°å‡è¾ƒæ…¢ï¼Œä»¥åŠæé«˜çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†å¯¹åŸºå‡†Transformeræ¨¡å‹çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨å¹²å‡€å’Œå—æ±¡æŸ“çš„æ•°æ®ä¸Šå‡å®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b7eb0c6bc88b1f7980d52e64a30ed09b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33dbab91cb18b20834a539c9c39cbb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9da29fd57acbcffe975cd8e5f69f5a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-542f9c551bb071c51e0076da5f027eb0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Retrieval-Augmented-Generation-in-Multi-Modal-Contexts"><a href="#Benchmarking-Retrieval-Augmented-Generation-in-Multi-Modal-Contexts" class="headerlink" title="Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts"></a>Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts</h2><p><strong>Authors:Zhenghao Liu, Xingsheng Zhu, Tianshuo Zhou, Xinyi Zhang, Xiaoyuan Yi, Yukun Yan, Ge Yu, Maosong Sun</strong></p>
<p>With the rapid advancement of Multi-modal Large Language Models (MLLMs), their capability in understanding both images and text has greatly improved. However, their potential for leveraging multi-modal contextual information in Retrieval-Augmented Generation (RAG) remains largely underexplored. To address this gap, this paper introduces Multi-Modal Retrieval-Augmented Generation (M$^2$RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as contextual input for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments demonstrate the effectiveness of MM-RAIT by significantly improving the quality of responses generated by different RAG models, outperforming MiniCPM-V 2.6 and Qwen2-VL with 34% and 33% gains, respectively. All data and code are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG">https://github.com/NEUIR/M2RAG</a>. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬å¯¹å›¾åƒå’Œæ–‡æœ¬çš„ç†è§£èƒ½åŠ›å¾—åˆ°äº†æå¤§çš„æé«˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­åˆ©ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡å¼•å…¥äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆM^2^RAGï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨å¤šæ¨¡æ€æ£€ç´¢æ–‡æ¡£ä¸­çš„çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å››ä¸ªä»»åŠ¡ï¼šå›¾åƒæè¿°ã€å¤šæ¨¡æ€é—®ç­”ã€å¤šæ¨¡æ€äº‹å®éªŒè¯å’Œå›¾åƒé‡æ’åºã€‚æ‰€æœ‰ä»»åŠ¡éƒ½åœ¨å¼€æ”¾åŸŸç¯å¢ƒä¸­è®¾ç½®ï¼Œè¦æ±‚RAGæ¨¡å‹ä»å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨ä½œRAGå»ºæ¨¡çš„ä¸Šä¸‹æ–‡è¾“å…¥ã€‚ä¸ºäº†æé«˜MLLMsçš„ä¸Šä¸‹æ–‡åˆ©ç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆMM-RAITï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼Œå¯åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­ä¼˜åŒ–MLLMsã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMM-RAITé€šè¿‡æ˜¾è‘—æé«˜ä¸åŒRAGæ¨¡å‹ç”Ÿæˆçš„å“åº”è´¨é‡è€Œæœ‰æ•ˆï¼Œåˆ†åˆ«è¶…è¶Šäº†MiniCPM-V 2.6å’ŒQwen2-VLï¼Œåˆ†åˆ«æé«˜äº†34%å’Œ33%ã€‚æ‰€æœ‰æ•°æ®ä»£ç å‡å¯ä»<a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/NEUIR/M2RAGè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17297v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶å¯¹å›¾åƒå’Œæ–‡æœ¬çš„ç†è§£èƒ½åŠ›å·²å¤§å¹…æå‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åˆ©ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„æ½œåŠ›åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­ä»è¢«å¤§é‡å¿½è§†ã€‚æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆM^2RAGï¼‰è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨å¤šæ¨¡æ€æ£€ç´¢æ–‡æ¡£ä¸­çš„çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å››ä¸ªä»»åŠ¡ï¼šå›¾åƒæè¿°ã€å¤šæ¨¡æ€é—®ç­”ã€å¤šæ¨¡æ€äº‹å®éªŒè¯å’Œå›¾åƒé‡æ’åºã€‚æ‰€æœ‰ä»»åŠ¡å‡åœ¨å¼€æ”¾é¢†åŸŸç¯å¢ƒä¸­è®¾ç½®ï¼Œè¦æ±‚RAGæ¨¡å‹ä»å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨ä½œRAGå»ºæ¨¡çš„ä¸Šä¸‹æ–‡è¾“å…¥ã€‚ä¸ºäº†æå‡MLLMsåœ¨ä¸Šä¸‹æ–‡ä½¿ç”¨æ–¹é¢çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆMM-RAITï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­ä¼˜åŒ–MLLMsçš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒMM-RAITèƒ½æœ‰æ•ˆæå‡ä¸åŒRAGæ¨¡å‹ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ï¼Œç›¸è¾ƒäºMiniCPM-V 2.6å’ŒQwen2-VLåˆ†åˆ«æå‡äº†34%å’Œ33%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½ç†è§£å›¾åƒå’Œæ–‡æœ¬ï¼Œä½†å…¶åˆ©ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ½œåŠ›åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆM^2RAGï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å›¾åƒæè¿°ã€å¤šæ¨¡æ€é—®ç­”ã€å¤šæ¨¡æ€äº‹å®éªŒè¯å’Œå›¾åƒé‡æ’åºå››ä¸ªä»»åŠ¡ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€LLMsçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>M^2RAGè¦æ±‚åœ¨å¼€æ”¾é¢†åŸŸç¯å¢ƒä¸­ï¼ŒRAGæ¨¡å‹èƒ½ä»å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ã€‚</li>
<li>ä¸ºäº†æå‡MLLMsåœ¨ä¸Šä¸‹æ–‡ä½¿ç”¨æ–¹é¢çš„èƒ½åŠ›ï¼Œå¼•å…¥äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆMM-RAITï¼‰æ–¹æ³•ã€‚</li>
<li>MM-RAITèƒ½æœ‰æ•ˆæå‡ä¸åŒRAGæ¨¡å‹ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ï¼Œç›¸æ¯”å…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>M^2RAGå’ŒMM-RAITçš„ç›¸å…³æ•°æ®å’Œä»£ç å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d650c1717a6ad204b9da1c60a89fb2d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6fed17cebbde1749ade6e441f490eb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25bfc21547b1a0390e0cdd8ee5497302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-277bea201f7673a6e25e0ec81b7a015e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0efb417f4de0ad44f23a7566393e400.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm"><a href="#RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm" class="headerlink" title="RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm"></a>RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm</h2><p><strong>Authors:Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng</strong></p>
<p>After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of multimodal interleaved documents remains underutilized for contrastive vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. We compare our dataset with other widely used datasets of equivalent scale for CLIP training. Models pre-trained on RealSyn consistently achieve state-of-the-art performance across various downstream tasks, including linear probe, zero-shot transfer, zero-shot robustness, and zero-shot retrieval. Furthermore, extensive experiments confirm that RealSyn significantly enhances contrastive vision-language representation learning and demonstrates robust scalability. To facilitate future research, the RealSyn dataset and pretrained model weights are released at <a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn">https://github.com/deepglint/RealSyn</a>. </p>
<blockquote>
<p>ç»è¿‡å¯¹å¤§é‡å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒåï¼Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§é‡çš„æœªé…å¯¹çš„å¤šåª’ä½“äº¤äº’æ–‡æ¡£åœ¨å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ä¸­ä»æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›æœªé…å¯¹çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬æœ€åˆå»ºç«‹äº†ä¸€ä¸ªç°å®ä¸–ç•Œæ•°æ®æå–ç®¡é“ï¼Œä»¥æå–é«˜è´¨é‡çš„å›¾ç‰‡å’Œæ–‡æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†å±‚æ£€ç´¢æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°å°†æ¯å¼ å›¾åƒä¸å¤šä¸ªè¯­ä¹‰ä¸Šç›¸å…³çš„ç°å®æ–‡æœ¬ç›¸å…³è”ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå–ç²¾ç»†çš„è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—æ¥è¿›è¡Œåˆæˆæ–‡æœ¬çš„ç”Ÿäº§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥æ¥æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ é•¿å°¾æ¦‚å¿µã€‚åŸºäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æ„å»ºäº†RealSynæ•°æ®é›†ï¼Œèåˆäº†ç°å®å’Œåˆæˆæ–‡æœ¬ï¼Œåˆ†ä¸ºä¸‰ä¸ªè§„æ¨¡ï¼š15Mã€30Må’Œ100Mã€‚æˆ‘ä»¬å°†æ•°æ®é›†ä¸åŒç­‰è§„æ¨¡çš„å…¶ä»–å¸¸ç”¨CLIPè®­ç»ƒæ•°æ®é›†è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒåŸºäºRealSyné¢„è®­ç»ƒçš„æ¨¡å‹å§‹ç»ˆå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬çº¿æ€§æ¢æµ‹ã€é›¶æ ·æœ¬è¿ç§»ã€é›¶æ ·æœ¬é²æ£’æ€§å’Œé›¶æ ·æœ¬æ£€ç´¢ã€‚æ­¤å¤–ï¼Œå¤§é‡å®éªŒè¯å®ï¼ŒRealSynæ˜¾è‘—å¢å¼ºäº†å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶è¡¨ç°å‡ºç¨³å¥çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼ŒRealSynæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/deepglint/RealSynä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12513v3">PDF</a> 15 pages, 12 figures, Accepted by ACM MM2025, Webpage:   <a target="_blank" rel="noopener" href="https://garygutc.github.io/RealSyn">https://garygutc.github.io/RealSyn</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºRealSynæ•°æ®é›†æ¥æå‡å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚RealSynæ•°æ®é›†ç»“åˆäº†çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œå»ºç«‹äº†çœŸå®ä¸–ç•Œæ•°æ®æå–ç®¡é“å’Œåˆ†å±‚æ£€ç´¢æ–¹æ³•ï¼Œä»¥æé«˜å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚æ­¤å¤–ï¼Œé€šè¿‡å›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—å’Œè¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œé¢„è®­ç»ƒåœ¨RealSynä¸Šçš„æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨å¹¿æ³›çš„å¤šæ¨¡æ€äº¤é”™æ–‡æ¡£ä¸Šçš„æ€§èƒ½æœ‰å¾…æå‡ã€‚</li>
<li>å»ºç«‹äº†çœŸå®ä¸–ç•Œæ•°æ®æå–ç®¡é“ä»¥æå–é«˜è´¨é‡å›¾åƒå’Œæ–‡æœ¬ã€‚</li>
<li>è®¾è®¡äº†åˆ†å±‚æ£€ç´¢æ–¹æ³•ï¼Œä½¿æ¯ä¸ªå›¾åƒä¸å¤šä¸ªè¯­ä¹‰ç›¸å…³çš„ç°å®æ–‡æœ¬æœ‰æ•ˆå…³è”ã€‚</li>
<li>é€šè¿‡å›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—æé«˜ç²¾ç»†è§†è§‰ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨è¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œæœ‰åŠ©äºå­¦ä¹ é•¿å°¾æ¦‚å¿µã€‚</li>
<li>æ„å»ºäº†RealSynæ•°æ®é›†ï¼Œç»“åˆäº†çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œæä¾›ä¸‰ç§è§„æ¨¡ä¾›é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-654e4052b5a08f584301f164972971e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68b3b3839b17adbd5d83ea72d1850ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1ed79d3741132dc49cf8024cdb1ba2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3207398aad40c4f07bae705f4a2a25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-854b803ced1c6b4088dfa89873a0d86d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-481418791622bcc2bee0df86fd5fe8ec.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Efficient-Knowledge-Injection-in-LLMs-via-Self-Distillation"><a href="#Efficient-Knowledge-Injection-in-LLMs-via-Self-Distillation" class="headerlink" title="Efficient Knowledge Injection in LLMs via Self-Distillation"></a>Efficient Knowledge Injection in LLMs via Self-Distillation</h2><p><strong>Authors:Kalle KujanpÃ¤Ã¤, Pekka Marttinen, Harri Valpola, Alexander Ilin</strong></p>
<p>In many practical applications, large language models (LLMs) need to acquire new knowledge not present in their pre-training data. Efficiently leveraging this knowledge usually relies on supervised fine-tuning or retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. This paper proposes utilizing prompt distillation, a self-distillation-based method previously explored primarily for style alignment and instruction tuning, to internalize new factual knowledge from free-form documents. Unlike prior methods, our approach requires neither larger teacher models nor structured knowledge formats. Across multiple LLM sizes and model families, we show that prompt distillation outperforms standard supervised fine-tuning and can even surpass RAG. We analyze the key factors contributing to prompt distillationâ€™s effectiveness and examine how it scales. </p>
<blockquote>
<p>åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦è·å–å…¶é¢„è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«çš„æ–°çŸ¥è¯†ã€‚æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›çŸ¥è¯†é€šå¸¸ä¾èµ–äºç›‘ç£å¾®è°ƒæˆ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚å°½ç®¡RAGå·²æˆä¸ºçŸ¥è¯†æ³¨å…¥çš„è¡Œä¸šæ ‡å‡†ï¼Œä½†å¾®è°ƒå°šæœªå–å¾—ä¸ä¹‹ç›¸å½“çš„æˆå°±ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨æç¤ºè’¸é¦æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘è’¸é¦çš„æ–¹æ³•ï¼Œä¹‹å‰ä¸»è¦ç”¨äºé£æ ¼å¯¹é½å’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥ä»è‡ªç”±å½¢å¼çš„æ–‡æ¡£ä¸­å†…åŒ–æ–°çš„äº‹å®çŸ¥è¯†ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢ä¸éœ€è¦æ›´å¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œä¹Ÿä¸éœ€è¦ç»“æ„åŒ–çš„çŸ¥è¯†æ ¼å¼ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªLLMè§„æ¨¡å’Œæ¨¡å‹å®¶æ—ä¸­è¡¨æ˜ï¼Œæç¤ºè’¸é¦æ³•ä¼˜äºæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼Œç”šè‡³å¯ä»¥è¶…è¶ŠRAGã€‚æˆ‘ä»¬åˆ†æäº†æç¤ºè’¸é¦æ³•æœ‰æ•ˆæ€§çš„å…³é”®å› ç´ ï¼Œå¹¶ç ”ç©¶äº†å…¶å¦‚ä½•æ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14964v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦è·å–é¢„è®­ç»ƒæ•°æ®ä»¥å¤–çš„æ–°çŸ¥è¯†ã€‚å°½ç®¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºçŸ¥è¯†æ³¨å…¥çš„è¡Œä¸šæ ‡å‡†ï¼Œä½†å¾®è°ƒå°šæœªå–å¾—ç›¸åº”çš„æˆåŠŸã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨æç¤ºè’¸é¦æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘è’¸é¦çš„æ–¹æ³•ï¼Œä»¥å‰ä¸»è¦ç”¨äºé£æ ¼å¯¹é½å’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥å†…åŒ–æ¥è‡ªè‡ªç”±å½¢å¼æ–‡æ¡£çš„æ–°äº‹å®çŸ¥è¯†ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢ä¸éœ€è¦æ›´å¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œä¹Ÿä¸éœ€è¦ç»“æ„åŒ–çŸ¥è¯†æ ¼å¼ã€‚è·¨å¤šä¸ªLLMå¤§å°å’Œæ¨¡å‹å®¶æ—ï¼Œæˆ‘ä»¬è¯æ˜äº†æç¤ºè’¸é¦æ³•ä¼˜äºæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼Œç”šè‡³å¯ä»¥è¶…è¶ŠRAGã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦è·å–é¢„è®­ç»ƒæ•°æ®ä¹‹å¤–çš„æ–°çŸ¥è¯†ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºçŸ¥è¯†æ³¨å…¥çš„è¡Œä¸šæ ‡å‡†ï¼Œä½†å¾®è°ƒå°šæœªå–å¾—åŒç­‰æˆåŠŸã€‚</li>
<li>æç¤ºè’¸é¦æ³•æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘è’¸é¦çš„æ–¹æ³•ï¼Œå¯ä»¥å†…åŒ–æ¥è‡ªè‡ªç”±å½¢å¼æ–‡æ¡£çš„æ–°äº‹å®çŸ¥è¯†ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæç¤ºè’¸é¦æ³•ä¸éœ€è¦æ›´å¤§çš„æ•™å¸ˆæ¨¡å‹å’Œç»“æ„åŒ–çŸ¥è¯†æ ¼å¼ã€‚</li>
<li>æç¤ºè’¸é¦æ³•åœ¨ä¸åŒè§„æ¨¡å’Œå®¶æ—çš„LLMä¸­è¡¨ç°å‡ºä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒçš„æ€§èƒ½ã€‚</li>
<li>æç¤ºè’¸é¦æ³•çš„æœ‰æ•ˆæ€§å…³é”®åœ¨äºå…¶èƒ½å¤Ÿè‡ªæˆ‘è’¸é¦å¹¶å†…åŒ–æ–°çš„äº‹å®çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64211217265df0957718a867d027efda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97782c26e8558270397361ed858cdcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866b4c72caec10896f768b9718220734.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling"><a href="#Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling" class="headerlink" title="Core Context Aware Transformers for Long Context Language Modeling"></a>Core Context Aware Transformers for Long Context Language Modeling</h2><p><strong>Authors:Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan</strong></p>
<p>Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤§é‡ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¿™ä¸»è¦å½’åŠŸäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶è¦æ±‚ä¸€ä¸ªæ ‡è®°ï¼ˆtokenï¼‰å°†æ‰€æœ‰å…ˆå‰çš„æ ‡è®°ä½œä¸ºå…¶ä¸Šä¸‹æ–‡æ¥è®¡ç®—æ³¨æ„åŠ›ã€‚ç„¶è€Œï¼Œå½“ä¸Šä¸‹æ–‡é•¿åº¦Lå˜å¾—éå¸¸å¤§ï¼ˆä¾‹å¦‚128Kï¼‰æ—¶ï¼Œä¸Šä¸‹æ–‡ä¸­æ½œåœ¨å†—ä½™ä¿¡æ¯çš„æ•°é‡å¾€å¾€ä¼šå¢åŠ ã€‚å†—ä½™çš„ä¸Šä¸‹æ–‡ä¸ä»…é˜»ç¢å»ºæ¨¡è¡¨ç¤ºæ€§èƒ½ï¼Œè€Œä¸”è¿˜äº§ç”Ÿä¸å¿…è¦çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æ ¸å¿ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆCCAï¼‰æ³¨æ„åŠ›ï¼Œç”¨äºé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼š1ï¼‰å…¨å±€æ„ŸçŸ¥æ± æ¨¡å—å¯¹è¾“å…¥æ ‡è®°è¿›è¡Œåˆ†ç»„ï¼Œå¹¶æ ¹æ®å…¶é‡è¦æ€§åŠ¨æ€åœ°å°†æ¯ä¸ªç»„å‹ç¼©æˆä¸€ä¸ªæ ¸å¿ƒæ ‡è®°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è‡ªåŠ¨èšç„¦å’Œå¢å¼ºæ ¸å¿ƒä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶å‡å°‘å†—ä½™ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„é•¿æœŸä¾èµ–å…³ç³»å»ºæ¨¡ã€‚2ï¼‰å±€éƒ¨æ€§ä¿æŒæ¨¡å—ç»“åˆäº†ç›¸é‚»çš„æ ‡è®°ï¼Œä»¥ä¿ç•™å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°è¯¦ç»†çš„è¡¨ç¤ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„CCAæ³¨æ„åŠ›èƒ½å¤Ÿä»¥æœ€å°çš„å¾®è°ƒæˆæœ¬æ›¿æ¢ç°æœ‰LLMä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12465v3">PDF</a> Accepted for publication at ICML 2025</p>
<p><strong>Summary</strong>ï¼šåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´å†—ä½™ä¿¡æ¯é—®é¢˜ï¼Œå½±å“å»ºæ¨¡æ€§èƒ½å¹¶é€ æˆè®¡ç®—å’Œå­˜å‚¨è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ ¸å¿ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆCCAï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å…¨å±€æ„ŸçŸ¥æ± åŒ–æ¨¡å—å’Œå±€éƒ¨ä¿ç•™æ¨¡å—æ¥é«˜æ•ˆå»ºæ¨¡é•¿æ–‡æœ¬ï¼Œå¹¶è‡ªåŠ¨èšç„¦å¼ºåŒ–æ ¸å¿ƒä¸Šä¸‹æ–‡å¹¶å‡å°‘å†—ä½™ã€‚CCAæ³¨æ„åŠ›èƒ½æ›¿æ¢ç°æœ‰LLMä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºäºTransformerçš„LLMåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶å­˜åœ¨å†—ä½™ä¿¡æ¯é—®é¢˜ã€‚</li>
<li>æ ¸å¿ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆCCAï¼‰æ³¨æ„åŠ›æœºåˆ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…æ‹¬å…¨å±€æ„ŸçŸ¥æ± åŒ–æ¨¡å—å’Œå±€éƒ¨ä¿ç•™æ¨¡å—ã€‚</li>
<li>å…¨å±€æ„ŸçŸ¥æ± åŒ–æ¨¡å—æŒ‰é‡è¦æ€§å°†è¾“å…¥ä»¤ç‰Œåˆ†ç»„å¹¶åŠ¨æ€å‹ç¼©ï¼Œä»¥å¼ºåŒ–æ ¸å¿ƒä¸Šä¸‹æ–‡å¹¶å‡å°‘å†—ä½™ã€‚</li>
<li>å±€éƒ¨ä¿ç•™æ¨¡å—ä¿ç•™é‚»è¿‘ä»¤ç‰Œä»¥è¿›è¡Œè¯¦ç»†è¡¨ç¤ºã€‚</li>
<li>CCAæ³¨æ„åŠ›æœºåˆ¶èƒ½æ›¿æ¢ç°æœ‰LLMä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚</li>
<li>å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒCCAæ³¨æ„åŠ›æœºåˆ¶åœ¨é•¿ç¯‡æ–‡æœ¬å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è¾ƒä½çš„å¾®è°ƒæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d9b50b0189a56c71dd6a3140d8e77df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40b602f47d28b43700efe26f86ca568c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87dad71f9789ae4f7588a8ec09e7663a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4f5884df1f8a0703532a02667888f00.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GuARD-Effective-Anomaly-Detection-through-a-Text-Rich-and-Graph-Informed-Language-Model"><a href="#GuARD-Effective-Anomaly-Detection-through-a-Text-Rich-and-Graph-Informed-Language-Model" class="headerlink" title="GuARD: Effective Anomaly Detection through a Text-Rich and   Graph-Informed Language Model"></a>GuARD: Effective Anomaly Detection through a Text-Rich and   Graph-Informed Language Model</h2><p><strong>Authors:Yunhe Pang, Bo Chen, Fanjin Zhang, Yanghui Rao, Evgeny Kharlamov, Jie Tang</strong></p>
<p>Anomaly detection on text-rich graphs is widely prevalent in real life, such as detecting incorrectly assigned academic papers to authors and detecting bots in social networks. The remarkable capabilities of large language models (LLMs) pave a new revenue by utilizing rich-text information for effective anomaly detection. However, simply introducing rich texts into LLMs can obscure essential detection cues and introduce high fine-tuning costs. Moreover, LLMs often overlook the intrinsic structural bias of graphs which is vital for distinguishing normal from abnormal node patterns. To this end, this paper introduces GuARD, a text-rich and graph-informed language model that combines key structural features from graph-based methods with fine-grained semantic attributes extracted via small language models for effective anomaly detection on text-rich graphs. GuARD is optimized with the progressive multi-modal multi-turn instruction tuning framework in the task-guided instruction tuning regime tailed to incorporate both rich-text and structural modalities. Extensive experiments on four datasets reveal that GuARD outperforms graph-based and LLM-based anomaly detection methods, while offering up to 5$\times$ times speedup in training and 5$\times$ times speedup in inference over vanilla long-context LLMs on the large-scale WhoIsWho dataset. </p>
<blockquote>
<p>æ–‡æœ¬ä¸°å¯Œçš„å›¾ä¸Šçš„å¼‚å¸¸æ£€æµ‹åœ¨ç°å®ç”Ÿæ´»ä¸­æ™®éå­˜åœ¨ï¼Œä¾‹å¦‚æ£€æµ‹é”™è¯¯åˆ†é…ç»™ä½œè€…çš„å­¦æœ¯è®ºæ–‡å’Œæ£€æµ‹ç¤¾äº¤ç½‘ç»œä¸­çš„æœºå™¨äººã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ˜¾è‘—èƒ½åŠ›ä¸ºåˆ©ç”¨ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæœ‰æ•ˆçš„å¼‚å¸¸æ£€æµ‹å¼€è¾Ÿäº†æ–°çš„æ”¶å…¥æ¥æºã€‚ç„¶è€Œï¼Œä»…ä»…å°†ä¸°å¯Œæ–‡æœ¬å¼•å…¥LLMä¼šæ©ç›–é‡è¦çš„æ£€æµ‹çº¿ç´¢ï¼Œå¹¶äº§ç”Ÿé«˜æ˜‚çš„å¾®è°ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒLLMé€šå¸¸ä¼šå¿½ç•¥å›¾çš„å†…åœ¨ç»“æ„åè§ï¼Œè¿™å¯¹äºåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸çš„èŠ‚ç‚¹æ¨¡å¼è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†GuARDï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆåŸºäºå›¾çš„ä¸°å¯Œæ–‡æœ¬å’Œå›¾ä¿¡æ¯è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚GuARDç»“åˆäº†åŸºäºå›¾çš„æ–¹æ³•çš„å…³é”®ç»“æ„ç‰¹å¾ä¸é€šè¿‡å°å‹è¯­è¨€æ¨¡å‹æå–çš„ç²¾ç»†è¯­ä¹‰å±æ€§ï¼Œç”¨äºåœ¨æ–‡æœ¬ä¸°å¯Œçš„å›¾ä¸Šæœ‰æ•ˆåœ°è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚GuARDé‡‡ç”¨ä»»åŠ¡å¯¼å‘æŒ‡ä»¤è°ƒæ•´åˆ¶åº¦ä¸‹çš„æ¸è¿›å¼å¤šæ¨¡å¼å¤šè½®æŒ‡ä»¤è°ƒæ•´æ¡†æ¶è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç»“åˆä¸°å¯Œæ–‡æœ¬å’Œç»“æ„æ¨¡å¼ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGuARDåœ¨å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¸Šä¼˜äºåŸºäºå›¾å’ŒåŸºäºLLMçš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å¤§å‹WhoIsWhoæ•°æ®é›†ä¸Šæä¾›äº†é«˜è¾¾5å€çš„åŸ¹è®­å’Œæ¨ç†é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03930v2">PDF</a> Accepted at KDD 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸°å¯Œçš„å›¾å¼‚å¸¸æ£€æµ‹åœ¨ç°å®ç”Ÿæ´»ä¸­åº”ç”¨å¹¿æ³›ï¼Œå¦‚æ£€æµ‹å­¦æœ¯è®ºæ–‡çš„é”™è¯¯åˆ†é…å’Œç¤¾äº¤ç½‘ç»œä¸­æœºå™¨äººçš„æ£€æµ‹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²èƒ½åŠ›ä¸ºåˆ©ç”¨ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæœ‰æ•ˆçš„å¼‚å¸¸æ£€æµ‹å¼€è¾Ÿäº†æ–°çš„æ”¶å…¥é€”å¾„ã€‚ç„¶è€Œï¼Œä»…ä»…å°†ä¸°å¯Œæ–‡æœ¬å¼•å…¥LLMå¯èƒ½ä¼šæ©ç›–é‡è¦çš„æ£€æµ‹çº¿ç´¢å¹¶äº§ç”Ÿé«˜æ˜‚çš„å¾®è°ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒLLMå¾€å¾€ä¼šå¿½è§†å›¾çš„ç»“æ„æ€§åè§ï¼Œè¿™å¯¹äºåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸çš„èŠ‚ç‚¹æ¨¡å¼è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†GuARDï¼Œå®ƒæ˜¯ä¸€ç§æ–‡æœ¬ä¸°å¯Œä¸”åŸºäºå›¾çš„æ¨¡å‹ï¼Œç»“åˆäº†åŸºäºå›¾çš„å’ŒåŸºäºå°å‹è¯­è¨€æ¨¡å‹æå–çš„å…³é”®ç»“æ„ç‰¹å¾ä»¥åŠç²¾ç»†è¯­ä¹‰å±æ€§ï¼Œå¯æœ‰æ•ˆæ£€æµ‹æ–‡æœ¬ä¸°å¯Œçš„å›¾çš„å¼‚å¸¸ã€‚å®éªŒè¡¨æ˜ï¼ŒGuARDåœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºåŸºäºå›¾å’ŒåŸºäºLLMçš„æ–¹æ³•ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾äº”å€çš„è®­ç»ƒé€Ÿåº¦å’Œæ¨ç†é€Ÿåº¦çš„æå‡ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼‚å¸¸æ£€æµ‹åœ¨ç°å®ä¸–ç•Œä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¦‚å­¦æœ¯è®ºå’Œç¤¾äº¤ç½‘ç»œçš„æ£€æµ‹é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨ä¸°å¯Œæ–‡æœ¬è¿›è¡Œå¼‚å¸¸æ£€æµ‹æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ä»…ä½¿ç”¨ä¸°å¯Œæ–‡æœ¬å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½å¯¼è‡´é‡è¦æ£€æµ‹çº¿ç´¢çš„ä¸¢å¤±å’Œè¾ƒé«˜çš„å¾®è°ƒæˆæœ¬ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€å¿½è§†å›¾çš„ç»“æ„æ€§åè§ï¼Œè¿™å¯¹äºåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸èŠ‚ç‚¹æ¨¡å¼è‡³å…³é‡è¦ã€‚</li>
<li>GuARDæ¨¡å‹ç»“åˆäº†åŸºäºå›¾çš„ç²¾ç»†ç»“æ„ç‰¹å¾å’Œé€šè¿‡å°å‹è¯­è¨€æ¨¡å‹æå–çš„è¯­ä¹‰å±æ€§ï¼Œæœ‰æ•ˆæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>GuARDæ¨¡å‹çš„ä¼˜åŒ–åŒ…æ‹¬é‡‡ç”¨å¤šæ¨¡æ€å¤šä»»åŠ¡æŒ‡å¯¼è®­ç»ƒç­–ç•¥ä»¥åŠç»“åˆä¸°å¯Œæ–‡æœ¬å’Œç»“æ„æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e8849d9e82a3b3858fdad0642ac66751.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a03fb974d14810ca8ee1a77f73c3518.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0cb6f5b995d0a0f715cbd67de85df75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88776a1a6faf35e8f45920aab6084ad0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation"><a href="#CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation" class="headerlink" title="CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation"></a>CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation</h2><p><strong>Authors:Hui Zhang, Dexiang Hong, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality. As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation. However, previous methods primarily focus on UNet-based models (\eg SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities. Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities. To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout. To inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities. Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage. Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description. We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality. Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization. These components form CreatiLayout â€“ a systematic solution that integrates the layout model, dataset, and planner for creative layout-to-image generation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿç”Ÿæˆä¸ä»…åœ¨è§†è§‰ä¸Šå¸å¼•äººè€Œä¸”è‰ºæœ¯æ€§å¾ˆé«˜çš„å›¾åƒè€Œå¤‡å—ç©ç›®ã€‚å› æ­¤ï¼Œæå‡ºäº†Layout-to-Imageï¼ˆL2Iï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å®šåŒºåŸŸçš„ä½ç½®å’Œæè¿°æ¥å®ç°æ›´ç²¾ç¡®å’Œå¯æ§çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºUNetçš„æ¨¡å‹ï¼ˆä¾‹å¦‚SD1.5å’ŒSDXLï¼‰ï¼Œå¯¹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMM-DiTï¼‰çš„æ¢ç´¢æœ‰é™ï¼Œåè€…å·²æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚è™½ç„¶ä½¿MM-DiTç”¨äºå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçœ‹ä¼¼ç®€å•ï¼Œä½†ç”±äºå¸ƒå±€å¦‚ä½•å¼•å…¥ã€é›†æˆå’Œå¹³è¡¡åœ¨å¤šæ¨¡æ€ä¹‹é—´çš„å¤æ‚æ€§ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å„ç§ç½‘ç»œå˜ä½“ï¼Œä»¥æœ‰æ•ˆåœ°å°†å¸ƒå±€æŒ‡å¯¼èå…¥MM-DiTï¼Œå¹¶æœ€ç»ˆæ¨å‡ºSiamLayoutã€‚ä¸ºäº†ç»§æ‰¿MM-DiTçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç»„ç‹¬ç«‹çš„ç½‘ç»œæƒé‡æ¥å¤„ç†å¸ƒå±€ï¼Œå°†å…¶è§†ä¸ºä¸å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€åŒæ ·é‡è¦ã€‚åŒæ—¶ï¼Œä¸ºäº†å‡è½»æ¨¡æ€ä¹‹é—´çš„ç«äº‰ï¼Œæˆ‘ä»¬å°†å›¾åƒå¸ƒå±€äº¤äº’åˆ†è§£æˆä¸å›¾åƒæ–‡æœ¬åˆ†æ”¯å¹¶åˆ—çš„å­ªç”Ÿåˆ†æ”¯ï¼Œå¹¶åœ¨åæœŸè¿›è¡Œèåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†ï¼Œåä¸ºLayoutSAMï¼Œå…¶ä¸­åŒ…æ‹¬270ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹å’Œ1070ä¸‡ä¸ªå®ä½“ã€‚æ¯ä¸ªå®ä½“éƒ½å¸¦æœ‰è¾¹ç•Œæ¡†å’Œè¯¦ç»†æè¿°ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LayoutSAM-EvalåŸºå‡†æµ‹è¯•ï¼Œä½œä¸ºè¯„ä¼°L2Iç”Ÿæˆè´¨é‡çš„ç»¼åˆå·¥å…·ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†å¸ƒå±€è®¾è®¡å¸ˆï¼Œå®ƒæŒ–æ˜äº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¸ƒå±€è§„åˆ’ä¸­çš„æ½œåŠ›ï¼Œå°†å…¶è½¬å˜ä¸ºå¸ƒå±€ç”Ÿæˆå’Œä¼˜åŒ–çš„ä¸“å®¶ã€‚è¿™äº›ç»„ä»¶æ„æˆäº†CreatiLayoutâ€”â€”ä¸€ä¸ªç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œé›†æˆäº†å¸ƒå±€æ¨¡å‹ã€æ•°æ®é›†å’Œè§„åˆ’å™¨ï¼Œç”¨äºåˆ›æ„å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03859v3">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¸ƒå±€åˆ°å›¾åƒï¼ˆLayout-to-Imageï¼ŒL2Iï¼‰ç”Ÿæˆçš„æŠ€æœ¯ã€‚æ–‡ç« ä»‹ç»äº†Multimodal Diffusion Transformersï¼ˆMM-DiTï¼‰åœ¨å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºå¦‚ä½•å°†å¸ƒå±€æŒ‡å¯¼æœ‰æ•ˆåœ°èå…¥MM-DiTçš„å¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æ¢ç´¢äº†å„ç§ç½‘ç»œå˜ä½“ï¼Œæœ€ç»ˆæå‡ºäº†SiamLayoutæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨ä¸“é—¨çš„ç½‘ç»œæƒé‡å¤„ç†å¸ƒå±€ï¼Œå¹¶è§£è€¦å›¾åƒä¸å¸ƒå±€çš„äº¤äº’ï¼Œåœ¨åæœŸè¿›è¡Œèåˆã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†LayoutSAMï¼Œå¹¶å»ºç«‹äº†è¯„ä»·L2Iç”Ÿæˆè´¨é‡çš„LayoutSAM-EvalåŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œæ–‡ç« ä»‹ç»äº†Layout Designerï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¸ƒå±€è§„åˆ’ä¸­çš„æ½œåŠ›ï¼Œå°†å…¶è½¬åŒ–ä¸ºå¸ƒå±€ç”Ÿæˆå’Œä¼˜åŒ–çš„ä¸“å®¶ç³»ç»Ÿã€‚è¿™äº›ç»„ä»¶å…±åŒæ„æˆäº†CreatiLayoutï¼Œä¸€ä¸ªæ•´åˆå¸ƒå±€æ¨¡å‹ã€æ•°æ®é›†å’Œè§„åˆ’å™¨çš„åˆ›æ„å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼ŒLayout-to-Imageï¼ˆL2Iï¼‰ç”ŸæˆæŠ€æœ¯åˆ©ç”¨åŒºåŸŸç‰¹å®šä½ç½®å’Œæè¿°æ¥å®ç°æ›´ç²¾ç¡®å’Œå¯æ§çš„ç”Ÿæˆã€‚</li>
<li>ä¹‹å‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨UNet-basedæ¨¡å‹ï¼Œè€ŒMultimodal Diffusion Transformersï¼ˆMM-DiTï¼‰åœ¨å›¾åƒç”Ÿæˆä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†èå…¥å¸ƒå±€æŒ‡å¯¼å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>SiamLayouté€šè¿‡æ¢ç´¢å„ç§ç½‘ç»œå˜ä½“æ¥é«˜æ•ˆèå…¥å¸ƒå±€æŒ‡å¯¼ï¼Œåˆ©ç”¨ä¸“é—¨çš„ç½‘ç»œæƒé‡å¤„ç†å¸ƒå±€ï¼Œå¹¶è§£è€¦å›¾åƒä¸å¸ƒå±€çš„äº¤äº’ã€‚</li>
<li>LayoutSAMæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒã€æ–‡æœ¬å’Œå®ä½“æ ‡æ³¨ï¼Œä¸ºL2Iç”Ÿæˆæä¾›æ•°æ®æ”¯æŒã€‚</li>
<li>LayoutSAM-EvalåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°L2Iç”Ÿæˆè´¨é‡ã€‚</li>
<li>Layout Designeråˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¸ƒå±€è§„åˆ’ä¸­çš„æ½œåŠ›ï¼Œè½¬åŒ–ä¸ºå¸ƒå±€ç”Ÿæˆå’Œä¼˜åŒ–çš„ä¸“å®¶ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-801f5bce18b516eeb9427fd70bf3a4c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26fbd8de78e813b98b0216047d6b9996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0b86c52feca87286e9d69698727f63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8671d9d7259b61533ae357f5325deca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec96ebca697e6cdfdab43f8bdb08c33.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="WSI-LLaVA-A-Multimodal-Large-Language-Model-for-Whole-Slide-Image"><a href="#WSI-LLaVA-A-Multimodal-Large-Language-Model-for-Whole-Slide-Image" class="headerlink" title="WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image"></a>WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image</h2><p><strong>Authors:Yuci Liang, Xinheng Lyu, Meidan Ding, Wenting Chen, Jipeng Zhang, Yuexiang Ren, Xiangjian He, Song Wu, Sen Yang, Xiyue Wang, Xiaohan Xing, Linlin Shen</strong></p>
<p>Recent advancements in computational pathology have produced patch-level Multi-modal Large Language Models (MLLMs), but these models are limited by their inability to analyze whole slide images (WSIs) comprehensively and their tendency to bypass crucial morphological features that pathologists rely on for diagnosis. To address these challenges, we first introduce WSI-Bench, a large-scale morphology-aware benchmark containing 180k VQA pairs from 9,850 WSIs across 30 cancer types, designed to evaluate MLLMsâ€™ understanding of morphological characteristics crucial for accurate diagnosis. Building upon this benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI understanding that employs a three-stage training approach: WSI-text alignment, feature space alignment, and task-specific instruction tuning. To better assess model performance in pathological contexts, we develop two specialized WSI metrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that WSI-LLaVA outperforms existing models across all capability dimensions, with a significant improvement in morphological analysis, establishing a clear correlation between morphological understanding and diagnostic accuracy. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ç»äº§ç”Ÿäº†è¡¥ä¸çº§åˆ«çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä½†è¿™äº›æ¨¡å‹å—åˆ°æ— æ³•å…¨é¢åˆ†æå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„å±€é™ï¼Œå¹¶ä¸”å€¾å‘äºç»•è¿‡ç—…ç†å­¦å®¶èµ–ä»¥è¿›è¡Œè¯Šæ–­çš„å…³é”®å½¢æ€ç‰¹å¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†WSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å½¢æ€æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ª9850å¼ å¹»ç¯ç‰‡å›¾åƒçš„18ä¸‡å¯¹é—®ç­”ï¼ˆVQAï¼‰ï¼Œæ¶‰åŠ3ç§ç™Œç—‡ç±»å‹ï¼Œæ—¨åœ¨è¯„ä¼°MLLMså¯¹å½¢æ€ç‰¹å¾çš„è®¤çŸ¥ç¨‹åº¦ï¼Œè¿™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚åŸºäºæ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†WSI-LLaVAï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºgigapixel WSIç†è§£çš„å…¨æ–°æ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šWSIæ–‡æœ¬å¯¹é½ã€ç‰¹å¾ç©ºé—´å¯¹é½å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ã€‚ä¸ºäº†æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨ç—…ç†ä¸Šä¸‹æ–‡ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªä¸“é—¨çš„WSIæŒ‡æ ‡ï¼šWSIç²¾ç¡®åº¦å’ŒWSIç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWSI-LLaVAåœ¨æ‰€æœ‰èƒ½åŠ›ç»´åº¦ä¸Šéƒ½ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåœ¨å½¢æ€åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå»ºç«‹äº†å½¢æ€ç†è§£ä¸è¯Šæ–­å‡†ç¡®æ€§ä¹‹é—´çš„æ˜ç¡®ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02141v3">PDF</a> ICCV 2025, 38 pages, 22 figures, 35 tables</p>
<p><strong>Summary</strong></p>
<p>è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ç»å‡ºç°äº†åŸºäºpatchçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨åˆ†æå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å…¨é¢åˆ†æå¹¶å®¹æ˜“å¿½ç•¥ç—…ç†åŒ»å¸ˆè¯Šæ–­æ‰€ä¾èµ–çš„å…³é”®å½¢æ€ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†WSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å½¢æ€æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ª9850ä¸ªWSIsçš„18ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠ30ç§ç™Œç—‡ç±»å‹ï¼Œæ—¨åœ¨è¯„ä¼°MLLMså¯¹å…³é”®å½¢æ€ç‰¹å¾çš„ç†è§£èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æå‡ºäº†WSI-LLaVAæ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ³•å¤„ç†å·¨åƒç´ WSIç†è§£é—®é¢˜ï¼ŒåŒ…æ‹¬WSIæ–‡æœ¬å¯¹é½ã€ç‰¹å¾ç©ºé—´å¯¹é½å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ã€‚ä¸ºæ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨ç—…ç†ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸¤ä¸ªä¸“é—¨çš„WSIæŒ‡æ ‡ï¼šWSIç²¾åº¦å’ŒWSIç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWSI-LLaVAåœ¨å„æ–¹é¢æ€§èƒ½å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå½¢æ€åˆ†ææ˜¾è‘—æ”¹å–„ï¼Œå½¢æ€ç†è§£ä¸è¯Šæ–­å‡†ç¡®åº¦çš„ç›¸å…³æ€§æ˜ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦é¢†åŸŸå‡ºç°åŸºäºpatchçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>MLLMsåœ¨åˆ†æå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å…¨é¢åˆ†æå¹¶æ˜“å¿½ç•¥å…³é”®å½¢æ€ç‰¹å¾ã€‚</li>
<li>WSI-Benchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å½¢æ€æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°MLLMså¯¹å½¢æ€ç‰¹å¾çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>WSI-LLaVAæ˜¯ä¸€ä¸ªå¤„ç†å·¨åƒç´ WSIç†è§£çš„æ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ³•æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>WSI-LLaVAåœ¨å½¢æ€åˆ†ææ–¹é¢æ˜¾è‘—æ”¹å–„ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿä¸ºè¯„ä¼°æ¨¡å‹åœ¨ç—…ç†ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œå¼€å‘äº†ä¸¤ä¸ªä¸“é—¨çš„WSIæŒ‡æ ‡ï¼šWSIç²¾åº¦å’ŒWSIç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d0ec3fd4eb35d7ad3186dedaf23ada5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb28db29e76d6685187aea3aea3428fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e89f945a4cced96220b5ef2bda4abb90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caa8b63272b4de47beec9e6e7fff7408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a843eaefd0f146f33a11a106abf8ef6c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs"><a href="#A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs" class="headerlink" title="A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs"></a>A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs</h2><p><strong>Authors:Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang</strong></p>
<p>Transformer neural networks (TNN) excel in natural language processing (NLP), machine translation, and computer vision (CV) without relying on recurrent or convolutional layers. However, they have high computational and memory demands, particularly on resource-constrained devices like FPGAs. Moreover, transformer models vary in processing time across applications, requiring custom models with specific parameters. Designing custom accelerators for each model is complex and time-intensive. Some custom accelerators exist with no runtime adaptability, and they often rely on sparse matrices to reduce latency. However, hardware designs become more challenging due to the need for application-specific sparsity patterns. This paper introduces ADAPTOR, a runtime-adaptive accelerator for dense matrix computations in transformer encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing elements and on-chip memory, enhancing parallelism and reducing latency. It incorporates efficient matrix tiling to distribute resources across FPGA platforms and is fully quantized for computational efficiency and portability. Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively. Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some state-of-the-art FPGA-based accelerators. </p>
<blockquote>
<p>Transformerç¥ç»ç½‘ç»œï¼ˆTNNï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€æœºå™¨ç¿»è¯‘å’Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€ä¾èµ–å¾ªç¯æˆ–å·ç§¯å±‚ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹è®¡ç®—å’Œå†…å­˜çš„éœ€æ±‚å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨FPGAç­‰èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚æ­¤å¤–ï¼Œä¸åŒçš„åº”ç”¨ç¨‹åºä¸­ï¼ŒTransformeræ¨¡å‹çš„å¤„ç†æ—¶é—´ä¼šæœ‰æ‰€ä¸åŒï¼Œå› æ­¤éœ€è¦å…·æœ‰ç‰¹å®šå‚æ•°çš„å®šåˆ¶æ¨¡å‹ã€‚ä¸ºæ¯ç§æ¨¡å‹è®¾è®¡å®šåˆ¶åŠ é€Ÿå™¨æ˜¯å¤æ‚ä¸”è€—æ—¶çš„ã€‚è™½ç„¶å­˜åœ¨ä¸€äº›æ²¡æœ‰è¿è¡Œæ—¶é€‚åº”æ€§çš„å®šåˆ¶åŠ é€Ÿå™¨ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç¨€ç–çŸ©é˜µæ¥é™ä½å»¶è¿Ÿã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦ç‰¹å®šçš„åº”ç”¨ç¨€ç–æ¨¡å¼ï¼Œç¡¬ä»¶è®¾è®¡å˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ADAPTORï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºFPGAä¸ŠTransformerç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„å¯†é›†çŸ©é˜µè®¡ç®—çš„è¿è¡Œæ—¶è‡ªé€‚åº”åŠ é€Ÿå™¨ã€‚ADAPTORæé«˜äº†å¤„ç†å…ƒä»¶å’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ï¼Œå¢å¼ºäº†å¹¶è¡Œæ€§å¹¶é™ä½äº†å»¶è¿Ÿã€‚å®ƒé‡‡ç”¨é«˜æ•ˆçš„çŸ©é˜µåˆ†å—æŠ€æœ¯ï¼Œä»¥åœ¨FPGAå¹³å°ä¹‹é—´åˆ†é…èµ„æºï¼Œå¹¶ä¸”ä¸ºäº†è®¡ç®—æ•ˆç‡å’Œä¾¿æºæ€§è€Œå®Œå…¨é‡åŒ–ã€‚åœ¨Xilinx Alveo U55Cæ•°æ®ä¸­å¿ƒå¡å’ŒVC707å’ŒZCU102ç­‰åµŒå…¥å¼å¹³å°ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è®¾è®¡æ¯”NVIDIA K80 GPUå’Œi7-8700K CPUåˆ†åˆ«é«˜å‡º1.2å€å’Œ2.87å€çš„èƒ½æ•ˆã€‚æ­¤å¤–ï¼Œä¸ä¸€äº›æœ€æ–°çš„FPGAåŠ é€Ÿå™¨ç›¸æ¯”ï¼Œå®ƒå®ç°äº†1.7åˆ°2.25å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18148v4">PDF</a> arXiv admin note: text overlap with arXiv:2409.14023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹FPGAä¸Šå˜å‹å™¨ç¼–ç å™¨è§£ç å™¨ä¸­å¯†é›†çŸ©é˜µè¿ç®—çš„ã€å…·æœ‰è¿è¡Œæ—¶è‡ªé€‚åº”æ€§çš„åŠ é€Ÿå™¨â€”â€”ADAPTORã€‚å®ƒèƒ½æé«˜å¤„ç†å…ƒä»¶å’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ï¼Œå¢å¼ºå¹¶è¡Œæ€§å¹¶é™ä½å»¶è¿Ÿã€‚é€šè¿‡æœ‰æ•ˆçš„çŸ©é˜µåˆ‡ç‰‡æŠ€æœ¯ï¼Œå®ƒèƒ½åˆç†åˆ†é…èµ„æºåœ¨å„ç§FPGAå¹³å°ä¸Šï¼Œå¹¶ä¸”ä¸ºå®ç°è®¡ç®—æ•ˆç‡å’Œä¾¿æºæ€§è¿›è¡Œäº†å…¨é¢é‡åŒ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”NVIDIA K80 GPUå’Œi7-8700K CPUï¼ŒADAPTORåœ¨è®¾è®¡æ•ˆç‡ä¸Šæ›´å…·ä¼˜åŠ¿ï¼Œå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer neural networks (TNN) ä¸ä¾èµ–å¾ªç¯å·ç§¯å±‚å°±èƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€æœºå™¨ç¿»è¯‘å’Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†è®¡ç®—å†…å­˜éœ€æ±‚è¾ƒé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨FPGAç­‰èµ„æºå—é™è®¾å¤‡ä¸Šã€‚</li>
<li>é’ˆå¯¹ä¸åŒåº”ç”¨ï¼ŒTransformeræ¨¡å‹çš„å¤„ç†æ—¶é—´æœ‰æ‰€ä¸åŒï¼Œéœ€è¦å®šåˆ¶å…·æœ‰ç‰¹å®šå‚æ•°çš„æ¨¡å‹ã€‚è®¾è®¡é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„å®šåˆ¶åŠ é€Ÿå™¨æ˜¯å¤æ‚ä¸”è€—æ—¶çš„ã€‚</li>
<li>å½“å‰å­˜åœ¨ä¸€äº›æ— è¿è¡Œæ—¶é€‚åº”æ€§çš„å®šåˆ¶åŠ é€Ÿå™¨ï¼Œå®ƒä»¬é€šå¸¸ä¾èµ–ç¨€ç–çŸ©é˜µæ¥é™ä½å»¶è¿Ÿï¼Œä½†ç”±äºéœ€è¦ç‰¹å®šçš„åº”ç”¨ç¨€ç–æ¨¡å¼ï¼Œç¡¬ä»¶è®¾è®¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ADAPTORï¼Œä¸€ä¸ªé’ˆå¯¹FPGAä¸ŠTransformerç¼–ç å™¨è§£ç å™¨å¯†é›†çŸ©é˜µè¿ç®—çš„ã€å…·æœ‰è¿è¡Œæ—¶è‡ªé€‚åº”æ€§çš„åŠ é€Ÿå™¨ã€‚</li>
<li>ADAPTORé€šè¿‡æé«˜å¤„ç†å…ƒä»¶å’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ã€å¢å¼ºå¹¶è¡Œæ€§å¹¶é™ä½å»¶è¿Ÿæ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>ADAPTORé‡‡ç”¨æœ‰æ•ˆçš„çŸ©é˜µåˆ‡ç‰‡æŠ€æœ¯ï¼Œå¯ä»¥è·¨FPGAå¹³å°åˆ†é…èµ„æºï¼Œä¸”ä¸ºäº†è®¡ç®—æ•ˆç‡å’Œä¾¿æºæ€§è¿›è¡Œäº†å…¨é¢é‡åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc637285f66bf3f658a2220114ff2093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a852501cf32a44d34201e1427b1178d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b21994e7d6d3cc6269f2caff93d1639.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DOTS-Learning-to-Reason-Dynamically-in-LLMs-via-Optimal-Reasoning-Trajectories-Search"><a href="#DOTS-Learning-to-Reason-Dynamically-in-LLMs-via-Optimal-Reasoning-Trajectories-Search" class="headerlink" title="DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning   Trajectories Search"></a>DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning   Trajectories Search</h2><p><strong>Authors:Murong Yue, Wenlin Yao, Haitao Mi, Dian Yu, Ziyu Yao, Dong Yu</strong></p>
<p>Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called â€œreasoning actionsâ€), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œå„ç§æç¤ºç­–ç•¥åœ¨å¸®åŠ©LLMè¿›è¡Œæ¨ç†ï¼ˆç§°ä¸ºâ€œæ¨ç†è¡ŒåŠ¨â€ï¼‰æ–¹é¢æ˜¯è¡Œä¹‹æœ‰æ•ˆçš„ï¼Œä¾‹å¦‚é€æ­¥æ€è€ƒã€ç­”é¢˜å‰æ€è€ƒã€ç¨‹åºè§£é¢˜åŠå…¶ç»„åˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å°†é™æ€çš„é¢„å®šä¹‰æ¨ç†è¡ŒåŠ¨ç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰é—®é¢˜ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°æ¯ä¸ªé—®é¢˜çš„ç‰¹å®šç‰¹å¾æˆ–ä»»åŠ¡è§£å†³LLMçš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DOTSæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½¿LLMèƒ½å¤Ÿé€šè¿‡æœ€ä¼˜æ¨ç†è½¨è¿¹æœç´¢è¿›è¡ŒåŠ¨æ€æ¨ç†çš„æ–¹æ³•ï¼Œå®ƒæ ¹æ®æ¯ä¸ªé—®é¢˜çš„ç‰¹å®šç‰¹å¾å’Œä»»åŠ¡è§£å†³LLMçš„å†…åœ¨èƒ½åŠ›æ¥å®šåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šiï¼‰å®šä¹‰åŸå­æ¨ç†è¡ŒåŠ¨æ¨¡å—ï¼Œè¿™äº›æ¨¡å—å¯ä»¥ç»„åˆæˆå„ç§æ¨ç†è¡ŒåŠ¨è½¨è¿¹ï¼›iiï¼‰é€šè¿‡è¿­ä»£æ¢ç´¢å’Œè¯„ä¼°ï¼Œä¸ºç‰¹å®šçš„ä»»åŠ¡è§£å†³LLMé’ˆå¯¹æ¯ä¸ªè®­ç»ƒé—®é¢˜æœç´¢æœ€ä¼˜è¡ŒåŠ¨è½¨è¿¹ï¼›iiiï¼‰ä½¿ç”¨æ”¶é›†åˆ°çš„æœ€ä¼˜è½¨è¿¹æ¥è®­ç»ƒLLMï¼Œä»¥è§„åˆ’æœªè§é—®é¢˜çš„æ¨ç†è½¨è¿¹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å­¦ä¹ èŒƒå¼ï¼Œå³å¾®è°ƒå¤–éƒ¨LLMä½œä¸ºè§„åˆ’å¸ˆæ¥æŒ‡å¯¼ä»»åŠ¡è§£å†³LLMï¼Œæˆ–ç›´æ¥å¾®è°ƒä»»åŠ¡è§£å†³LLMï¼Œä½¿å…¶å…·å¤‡æ¨ç†è¡ŒåŠ¨è§„åˆ’çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªæ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºé™æ€æ¨ç†æŠ€æœ¯å’Œæ™®é€šçš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿LLMèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è°ƒæ•´å…¶è®¡ç®—ï¼Œå°†æ›´æ·±å…¥çš„æ€è€ƒå’Œæ¨ç†åˆ†é…ç»™æ›´å¤æ‚çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03864v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘å¹´æ¥ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¤‡å—å…³æ³¨ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»å±•ç¤ºäº†å„ç§æç¤ºç­–ç•¥åœ¨è¾…åŠ©LLMè¿›è¡Œæ¨ç†ï¼ˆç§°ä¸ºâ€æ¨ç†è¡ŒåŠ¨â€ï¼‰ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¦‚é€æ­¥æ€è€ƒã€é¢„æ€è€ƒå†å›ç­”ã€ç¨‹åºæ±‚è§£ç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸ç»Ÿä¸€åœ°åº”ç”¨é™æ€çš„ã€é¢„å®šä¹‰çš„æ¨ç†è¡ŒåŠ¨åˆ°æ‰€æœ‰é—®é¢˜ä¸Šï¼Œæ²¡æœ‰è€ƒè™‘åˆ°æ¯ä¸ªé—®é¢˜çš„ç‰¹å®šç‰¹æ€§æˆ–ä»»åŠ¡è§£å†³LLMçš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºDOTSæ–¹æ³•ï¼Œä¸€ç§ä½¿LLMèƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªé—®é¢˜çš„ç‰¹å®šç‰¹æ€§å’Œä»»åŠ¡è§£å†³LLMçš„å†…åœ¨èƒ½åŠ›è¿›è¡ŒåŠ¨æ€æ¨ç†çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šå®šä¹‰å¯ä»¥ç»„åˆæˆå„ç§æ¨ç†è¡ŒåŠ¨è½¨è¿¹çš„åŸå­æ¨ç†è¡ŒåŠ¨æ¨¡å—ï¼›é€šè¿‡è¿­ä»£æ¢ç´¢å’Œè¯„ä¼°ä¸ºæ¯ä¸ªè®­ç»ƒé—®é¢˜æœç´¢ç‰¹å®šçš„ä»»åŠ¡è§£å†³LLMçš„æœ€ä¼˜è¡ŒåŠ¨è½¨è¿¹ï¼›ä½¿ç”¨æ”¶é›†çš„æœ€ä¼˜è½¨è¿¹æ¥è®­ç»ƒLLMï¼Œä»¥è§„åˆ’æœªè§é—®é¢˜çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å­¦ä¹ èŒƒå¼ï¼Œå³å¾®è°ƒå¤–éƒ¨LLMä½œä¸ºè§„åˆ’å™¨æ¥æŒ‡å¯¼ä»»åŠ¡è§£å†³LLMï¼Œæˆ–ç›´æ¥å¾®è°ƒä»»åŠ¡è§£å†³LLMä»¥å…·å¤‡å†…éƒ¨åŒ–çš„æ¨ç†è¡ŒåŠ¨è§„åˆ’èƒ½åŠ›ã€‚åœ¨å…«ä¸ªæ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºé™æ€æ¨ç†æŠ€æœ¯å’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿LLMèƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´è®¡ç®—ï¼Œå°†æ›´æ·±å…¥çš„æ€è€ƒå’Œæ¨ç†åˆ†é…ç»™æ›´éš¾çš„é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMçš„æ¨ç†èƒ½åŠ›æå‡å—åˆ°å…³æ³¨ï¼Œä¹‹å‰çš„æ–¹æ³•å¤šæ˜¯åº”ç”¨é™æ€æ¨ç†è¡ŒåŠ¨åˆ°æ‰€æœ‰é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºDOTSæ–¹æ³•ï¼Œèƒ½åŸºäºé—®é¢˜ç‰¹æ€§å’ŒLLMèƒ½åŠ›è¿›è¡ŒåŠ¨æ€æ¨ç†ã€‚</li>
<li>DOTSåŒ…å«ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šå®šä¹‰åŸå­æ¨ç†æ¨¡å—ã€æœç´¢æœ€ä¼˜è¡ŒåŠ¨è½¨è¿¹ã€è®­ç»ƒLLMè¿›è¡Œè§„åˆ’ã€‚</li>
<li>æå‡ºä¸¤ç§å­¦ä¹ èŒƒå¼ï¼šå¤–éƒ¨LLMè§„åˆ’å™¨æˆ–å†…éƒ¨åŒ–LLMèƒ½åŠ›ã€‚</li>
<li>åœ¨å…«ä¸ªæ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒDOTSæ–¹æ³•ä¼˜äºé™æ€å’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>DOTSæ–¹æ³•ä½¿LLMèƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´è®¡ç®—ï¼Œåˆç†åˆ†é…æ¨ç†èµ„æºã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæå‡LLMåœ¨å®é™…é—®é¢˜ä¸­çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-29e0e26bb4bce9c27a72253b4935a74a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50dca2fb8ce1ae18f12702022d4cd0a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c0f0b2f112e51bfcc0bfb6ceebcadf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa2eb691d80029e6232bd3027534e2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e96dfe671dc6f1f0a3e8ae1abbf7c9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9acde00b43fc0b1fe920d46825989a98.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6eca8c36554ad573a9576ae93ae01c08.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Population-Based Screening and Primary Diagnosis in a Global,   Multiethnic Population (Study Protocol)
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-de452fb78ddc8091a4e3ca8421b239de.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Exploring Superior Function Calls via Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
