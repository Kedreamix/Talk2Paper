<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-08-10  Scaling LLM Planning NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9acde00b43fc0b1fe920d46825989a98.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-10-更新"><a href="#2025-08-10-更新" class="headerlink" title="2025-08-10 更新"></a>2025-08-10 更新</h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Effective agent performance relies on the ability to compose tools and agents into effective workflows. However, progress in Large Language Model (LLM) planning and reasoning is limited by the scarcity of scalable, reliable evaluation data. This study addresses this limitation by identifying a suitable workflow domain for LLM application. I introduce NL2Flow, a fully automated system for parametrically generating planning problems, which are expressed in natural language, a structured intermediate representation, and formal PDDL, and rigorously evaluating the quality of generated plans. NL2Flow generates a dataset of 2296 low-difficulty problems in automated workflow generation and evaluates multiple open-sourced, instruct-tuned LLMs without task-specific optimization or architectural modifications. Results reveal that the highest performing model achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible plans. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. To investigate the potential of LLMs as natural language-to-JSON translators for workflow definition, and to facilitate integration with downstream symbolic computation tools and a symbolic planner, I evaluated the LLM’s translation performance on natural language workflow descriptions. I observed that translating natural language into a JSON representation of a workflow problem yielded a lower success rate than generating a plan directly, suggesting that unnecessary decomposition of the reasoning task may degrade performance and highlighting the benefit of models capable of reasoning directly from natural language to action. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>有效的代理性能取决于将工具和代理组合成有效工作流程的能力。然而，由于缺少可扩展、可靠的评估数据，大型语言模型（LLM）的规划和推理进展受到限制。本研究通过确定适合LLM应用的流程领域来解决这一限制。作者介绍了NL2Flow，这是一个全自动的系统，可以参数化生成规划问题，这些问题以自然语言、结构化中间表示和正式PDDL来表达，并对生成的计划质量进行严格的评估。NL2Flow生成了一个包含2296个低难度问题的数据集，用于自动化工作流程生成，并评估了多个开源、经过指令调整的大型语言模型，无需特定任务的优化或架构修改。结果表明，表现最好的模型在生成有效计划方面达到了86%的成功率，在生成最佳计划方面达到了69%，仅限于具有可行计划的问题。回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。为了研究大型语言模型作为自然语言到JSON的工作流定义翻译器的潜力，并促进其与下游符号计算工具和符号规划器的集成，作者评估了大型语言模型在自然语言工作流描述上的翻译性能。作者观察到，将自然语言翻译成工作流问题的JSON表示形式的成功率低于直接生成计划的成功率，这表明不必要的分解推理任务可能会降低性能，并突出显示直接从自然语言进行推理的模型的好处。随着大型语言模型推理处理越来越复杂的问题，理解这些系统内不断变化的瓶颈和错误来源将至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v3">PDF</a> 26 pages, 7 figures</p>
<p><strong>摘要</strong></p>
<p>本研究针对大型语言模型（LLM）在规划和推理方面的应用，通过识别适合LLM应用的流程领域，解决了评价数据缺乏的问题。引入NL2Flow系统，可参数化生成规划问题，以自然语言、结构化中间表示和正式PDDL表达，并对生成计划的质量进行严谨评估。研究生成了2296个低难度自动化工作流程问题数据集，并对多个开源、指令调优的LLMs进行了评估，无需特定任务优化或架构修改。结果显示，性能最好的模型在生成有效计划方面成功率为86%，在生成最优计划方面成功率为69%，仅限于有可行计划的问题。回归分析显示，问题特征对计划生成的影响取决于模型和提示设计。评估了LLM作为自然语言到JSON翻译器的潜力，用于工作流程定义，并与下游符号计算工具和符号规划器进行集成。发现将自然语言翻译成工作流程问题的JSON表示形式成功率较低，直接生成计划的成功率更高。这表明将推理任务过度分解可能会降低性能，并强调直接从自然语言进行推理的模型的优势。随着LLM推理在越来越复杂的问题上的扩展，理解这些系统中不断变化的瓶颈和错误来源将至关重要。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究通过NL2Flow系统解决了LLM在规划和推理方面的评价数据缺乏问题，该系统可参数化生成规划问题。</li>
<li>评估了多个LLMs在自动化工作流程生成方面的性能，发现性能最好的模型在生成有效计划方面成功率为86%，在生成最优计划方面为69%。</li>
<li>回归分析显示问题特征、模型和提示设计共同影响计划生成。</li>
<li>评估了LLM作为自然语言到JSON翻译器的潜力，发现直接进行计划生成比翻译表示法更成功。</li>
<li>过度分解推理任务可能会降低LLM的性能。</li>
<li>随着LLM推理在更复杂问题上的扩展，理解其瓶颈和错误来源变得至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-db1a26ecfa40678fa6c892a859b25ea4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad8fbb06b9226d44d5933b6f55c6e886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437e6002cf95e423276f26901f987d9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ae47048764fa0ab3f30f98365be80fa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Factuality-for-Dialogue-Response-Generation-via-Graph-Based-Knowledge-Augmentation"><a href="#Improving-Factuality-for-Dialogue-Response-Generation-via-Graph-Based-Knowledge-Augmentation" class="headerlink" title="Improving Factuality for Dialogue Response Generation via Graph-Based   Knowledge Augmentation"></a>Improving Factuality for Dialogue Response Generation via Graph-Based   Knowledge Augmentation</h2><p><strong>Authors:Xiangyan Chen, Yujian Gan, Yimeng Gu, Matthew Purver</strong></p>
<p>Large Language Models (LLMs) succeed in many natural language processing tasks. However, their tendency to hallucinate - generate plausible but inconsistent or factually incorrect text - can cause significant problems in certain tasks, including response generation in dialogue. To mitigate this issue, we propose two novel graph knowledge-augmented frameworks, Dialogue Response Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue Response Generation (GA-DRG), which combine reasoning-guided dialogue reformulation, dialogue sense knowledge selection, and graph-enhanced response generation to improve the factuality of dialogue responses. To evaluate the factuality of generated responses, we propose a dialogue fact score that addresses the limitations of existing fact-score methods in dialogue settings, providing a more reliable assessment of factual consistency. We evaluate our methods using different baselines on the OpendialKG and HybriDialogue datasets. Our methods noticeably improve factuality compared to other graph knowledge-augmentation baselines, including the state-of-the-art G-retriever, achieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in terms of dialogue fact score. The code will be released on GitHub. </p>
<blockquote>
<p>大型语言模型（LLM）在许多自然语言处理任务中取得了成功。然而，它们倾向于产生合理但自相矛盾或事实错误的文本，这在某些任务中（包括对话生成响应）可能会导致严重问题。为了缓解这个问题，我们提出了两种新型的图知识增强框架，即基于文本图的对话响应生成（TG-DRG）和图感知对话响应生成（GA-DRG）。这两个框架结合了推理引导的对话重构、对话常识知识选择和图增强响应生成，以提高对话响应的事实性。为了评估生成响应的事实性，我们提出了对话事实得分，解决了现有事实得分方法在对话环境中的局限性，为事实一致性提供了更可靠的评估。我们在OpendialKG和HybriDialogue数据集上使用了不同的基线来评估我们的方法。我们的方法在事实性上显著改进了其他图形知识增强基线，包括最先进的G检索器，在OpendialKG上提高了3.47%，在HybriDialogue上提高了3.12%，对话事实得分有所提高。代码将在GitHub上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12496v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自然语言处理任务中表现出色，但在对话生成等任务中存在虚构问题。本文提出两种新型的图知识增强框架，即文本化图对话响应生成（TG-DRG）和图感知对话响应生成（GA-DRG），结合推理引导对话重构、对话情境知识选择和图增强响应生成，以提高对话响应的事实性。为评估生成响应的事实性，本文提出对话事实评分方法，解决现有方法在对话环境中的局限性，提供更可靠的评估。实验结果显示，所提方法在OpendialKG和HybriDialogue数据集上显著提高事实性，相较于其他图知识增强基线方法，包括最先进的G-retriever，在对话事实评分上分别提高了3.47%和3.12%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在自然语言处理任务中表现出色，但在对话生成中存在虚构问题。</li>
<li>提出两种新型的图知识增强框架TG-DRG和GA-DRG，以提高对话响应的事实性。</li>
<li>结合推理引导对话重构、对话情境知识选择和图增强响应生成。</li>
<li>提出对话事实评分方法，以更可靠地评估生成响应的事实性。</li>
<li>所提方法在OpendialKG和HybriDialogue数据集上显著提高事实性。</li>
<li>与其他图知识增强基线方法相比，所提方法表现更优，在对话事实评分上有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12496">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-52594d55c544adc7dfc068b89cabeeb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b8353941804bbdeb65a8dd719d3bac4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777e4f461fca04278e61d7a6e3007579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1feb62991787b3be7c9f403567c2c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08b436994d0fc028a5be471e3aa471f7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models"><a href="#Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models" class="headerlink" title="Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models"></a>Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models</h2><p><strong>Authors:Michael Li, Nishant Subramani</strong></p>
<p>Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand today’s language models, we investigate how 25 models - from classical architectures (BERT, DeBERTa, GPT-2) to modern large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) - represent lexical identity and inflectional morphology across six typologically diverse languages. Using linear and nonlinear classifiers trained on hidden activations, we predict word lemmas and inflectional features layer by layer. We find that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout. Additional experiments probe the nature of these encodings: attention and residual analyses examine where within layers information can be recovered, steering vector experiments test what information can be functionally manipulated, and intrinsic dimensionality analyses explore how the representational structure evolves across layers. Remarkably, these encoding patterns emerge across all models we test, despite differences in architecture, size, and training regime (pretrained and instruction-tuned variants). This suggests that, even with substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties are important for next token prediction and are learned early during pretraining. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing">https://github.com/ml5885/model_internal_sleuthing</a> </p>
<blockquote>
<p>现代自然语言处理主要由大型基于transformer的语言模型主导，然而我们对它们如何编码语言信息的理解还仅限于早期模型，如BERT和GPT-2。为了更好地理解当今的语言模型，我们研究了从经典架构（BERT、DeBERTa、GPT-2）到现代大型语言模型（Pythia、OLMo-2、Gemma-2、Qwen2.5、Llama-3.1）的25种模型是如何在六种语言类型丰富的语言中表示词汇身份和屈折形态。我们使用在隐藏激活上训练的线性和非线性分类器逐层预测词干和屈折特征。我们发现模型在早期层次中以线性方式集中词汇信息，而在后期层次中则越来越以非线性方式集中信息，同时保持屈折信息在整个过程中均匀可访问并且可线性分离。额外的实验探讨了这些编码的本质：注意力分析和残差分析检查可以在哪些层次内恢复信息，转向向量实验测试可以操作哪些信息，以及内在维度分析探索层次间表示结构如何演变。值得注意的是，尽管我们在测试所有模型时都发现了这些编码模式，这些模型在架构、规模和训练制度（预训练和指令调整变体）方面存在差异。这表明，尽管大型语言模型技术取得了重大进展，但transformer模型以相似的方式组织语言信息，这表明这些属性对于下一个令牌预测很重要，并且在预训练期间早期就已经学习到了。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ml5885/model_internal_sleuthing找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02132v3">PDF</a> INTERPLAY Workshop COLM 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了现代自然语言处理中大型基于Transformer的语言模型如何编码语言信息。通过对包括经典架构（如BERT和GPT-2）和现代大型语言模型在内的共25种模型的深入研究发现，这些模型在早期层级中采用线性方式集中词汇信息，并在后续层级中逐渐采用非线性方式。同时，这些模型保持将屈折信息均匀分布并始终保持线性可分。研究还通过一系列实验探究了这些编码的特性，包括注意力与残差分析、引导向量实验和内在维度分析。尽管模型在架构、规模和训练机制上存在差异，但它们展现出相似的编码模式。这表明即便是在LLM技术取得了显著进步的背景下，Transformer模型的组织语言信息的方式仍然是稳定的。对于下一步的词预测而言，这些特性尤为重要，并且主要是在预训练阶段早期就形成的。相关的代码可以在相关网站公开获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型基于Transformer的语言模型主导现代NLP领域的研究。它们采用分层的信息处理方式处理词汇信息和屈折信息。词汇信息在前期层级中以线性方式为主，后期层级则逐渐采用非线性方式处理。屈折信息则在整个模型中保持均匀分布和线性可分性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02132">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96cd69d1b9a2218567a77df9dcacbb79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2742a35216d98c65b210c44c504fb414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db330404ca1d94a62043ea30b6d52c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb73a9d3683ad83e7a4738eca31b768b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6bca6895f3fa6aebf4f45de6def30d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tell-Me-Who-Your-Students-Are-GPT-Can-Generate-Valid-Multiple-Choice-Questions-When-Students’-Mis-Understanding-Is-Hinted"><a href="#Tell-Me-Who-Your-Students-Are-GPT-Can-Generate-Valid-Multiple-Choice-Questions-When-Students’-Mis-Understanding-Is-Hinted" class="headerlink" title="Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice   Questions When Students’ (Mis)Understanding Is Hinted"></a>Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice   Questions When Students’ (Mis)Understanding Is Hinted</h2><p><strong>Authors:Machi Shimmei, Masaki Uto, Yuichiroh Matsubayashi, Kentaro Inui, Aditi Mallavarapu, Noboru Matsuda</strong></p>
<p>The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT. </p>
<blockquote>
<p>本研究的主要目标是开发并评估一种创新性的提示技术——AnaQuest，该技术使用预训练的大型语言模型来生成多项选择题（MCQs）。在AnaQuest中，选择项是关于复杂概念的句子级断言。该技术结合了形成性评估和终结性评估。在形成性阶段，学生以自由文本的形式回答关于目标概念的问题。在终结性评估中，AnaQuest分析这些回答以生成正确和错误的断言。为了评估生成的MCQ的有效性，应用项目反应理论（IRT）比较了AnaQuest生成的多项选择题、基线ChatGPT提示和人类制作的题目之间的项目特征。实证研究结果表明，专家导师认为AI模型生成的MCQ与人类导师创建的题目同样有效。然而，基于IRT的分析显示，AnaQuest生成的问题——特别是那些带有错误断言（foil）的问题——在难度和区分度方面更接近于人类制作的题目，而不是ChatGPT生成的题目。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05815v2">PDF</a> This is a pre-print version of a paper to appear in AIED2025. The   camera-ready version is available at   <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-99264-3_16">https://link.springer.com/chapter/10.1007/978-3-031-99264-3_16</a></p>
<p><strong>Summary</strong></p>
<p>AnaQuest是一项旨在利用预训练的大型语言模型生成多选题（MCQs）的创新提示技术。它结合了形成性评估和终结性评估，通过对学生关于目标概念的自由文本回答进行分析，生成正确和错误的断言。研究表明，人工智能模型生成的MCQs与专家教师生成的题目具有相同的效度，但AnaQuest生成的问题在难度和区分度上更接近人类制作的题目。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnaQuest是一种用于生成多选题的创新提示技术，利用预训练的大型语言模型。</li>
<li>该技术整合了形成性和终结性评估来生成正确和错误的断言作为选择题选项。</li>
<li>通过Item Response Theory（IRT）对AnaQuest生成的题目与ChatGPT和人工生成的题目进行比较分析。</li>
<li>专家教师认为AI模型生成的MCQs与人工制作的题目具有相同的效度。</li>
<li>AnaQuest生成的问题，特别是包含错误断言的选项，在难度和区分度上更接近人类制作的题目。</li>
<li>AnaQuest技术有助于提高题目生成的效率和质量，有助于教育评估和学生的学习发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-796127144b71c2dde79b5d1ee4cdc67e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b78ae34db3d2b5dab6b10719dc61c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8da321da25cc845e4bb45e4abb6999ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53b52141e6f246a3f96284c333732c78.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Modality-Barrier-Universal-Embedding-Learning-with-Multimodal-LLMs"><a href="#Breaking-the-Modality-Barrier-Universal-Embedding-Learning-with-Multimodal-LLMs" class="headerlink" title="Breaking the Modality Barrier: Universal Embedding Learning with   Multimodal LLMs"></a>Breaking the Modality Barrier: Universal Embedding Learning with   Multimodal LLMs</h2><p><strong>Authors:Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, Jiankang Deng</strong></p>
<p>The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM&#39;s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）框架已成为广泛应用于多模态表示学习的方法，尤其在图像文本检索和聚类中。然而，它的有效性受到三个主要限制：（1）文本令牌截断，（2）孤立的图像文本编码，以及（3）由于词袋行为导致的成分缺陷。虽然最近的多媒体语言大模型（MLLM）在通用视觉语言理解方面取得了显著进展，但其学习可转移的多模态表示潜力仍未得到充分探索。在本研究中，我们提出了UniME（通用多模态嵌入），这是一个利用MLLM学习用于下游任务的判别表示的新型两阶段框架。在第一阶段，我们通过强大的基于LLM的教师模型进行文本判别知识蒸馏，以增强MLLM的语言组件的嵌入能力。在第二阶段，我们引入了硬负增强指令调整，以进一步推动判别表示学习。具体来说，我们首先缓解假阴性污染，然后在每个批次内为每个实例采样多个硬负样本，迫使模型关注具有挑战性的样本。这种方法不仅提高了判别力，而且提高了下游任务中的指令执行能力。我们在MMEB基准测试和多个检索任务上进行了广泛实验，包括短文本和长文本检索以及组合检索。结果表明，UniME在所有任务上均实现了性能改进，展现出卓越的判别和组合能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17432v3">PDF</a> 13 pages, 8 figures, Accepted by ACM MM2025, Project page:   <a target="_blank" rel="noopener" href="https://garygutc.github.io/UniME">https://garygutc.github.io/UniME</a></p>
<p><strong>Summary</strong></p>
<p>CLIP框架在多模态表示学习方面，尤其在图像文本检索和聚类中，得到了广泛应用。但其存在文本令牌截断、孤立的图像文本编码和因词袋行为导致的成分缺陷等三个关键局限性。虽然最近的多模态大型语言模型（MLLMs）在通用视觉语言理解方面取得了显著进展，但其在学习可转移的多模态表示方面的潜力仍被探索不足。本研究提出UniME（通用多模态嵌入），这是一个利用MLLMs学习判别表示用于多种下游任务的新型两阶段框架。第一阶段，通过强大的LLM模型进行文本判别知识蒸馏，以增强MLLM语言组件的嵌入能力。第二阶段，引入增强指令调整硬负样本进一步优化判别表示学习。通过减少假阴性污染并对每个实例在批次内采样多个硬负样本，迫使模型关注具有挑战性的样本。这种方法不仅提高了判别力，还提高了下游任务中的指令遵循能力。在MMEB基准测试和多个检索任务上的实验表明，UniME在所有任务上实现了性能改进，表现出卓越的判别和组合能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP框架在多模态表示学习中广泛应用，但存在文本令牌截断、图像文本编码孤立和成分缺陷等局限性。</li>
<li>多模态大型语言模型（MLLMs）在通用视觉语言理解方面取得显著进展，但在学习可转移的多模态表示方面潜力未得到充分探索。</li>
<li>UniME是一个利用MLLMs的新型两阶段框架，旨在学习用于多种下游任务的判别表示。</li>
<li>第一阶段通过强大的LLM模型进行文本判别知识蒸馏以增强嵌入能力。</li>
<li>第二阶段引入增强指令调整硬负样本，提高判别力并增强指令遵循能力。</li>
<li>UniME在多个基准测试和检索任务上实现了性能改进，表现出卓越的判别和组合能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17432">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc8ef9ecd92cd1195e100ec30b2c7fdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d95b642890279ec57cc11c85802fdcdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8716d55020311824a2426daecc785a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f5ff6f281eb4279c4068960ddbd21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b69f51a25e9b500301043305c5456397.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ArXivBench-When-You-Should-Avoid-Using-ChatGPT-for-Academic-Writing"><a href="#ArXivBench-When-You-Should-Avoid-Using-ChatGPT-for-Academic-Writing" class="headerlink" title="ArXivBench: When You Should Avoid Using ChatGPT for Academic Writing"></a>ArXivBench: When You Should Avoid Using ChatGPT for Academic Writing</h2><p><strong>Authors:Ning Li, Jingran Zhang, Justin Cui</strong></p>
<p>Large language models (LLMs) demonstrate strong capabilities in reasoning and question answering, yet their tendency to generate factually incorrect content remains a critical challenge. This study evaluates proprietary and open-source LLMs on generating relevant research papers with accurate arXiv links. Our evaluation reveals critical academic risks: LLMs frequently generate incorrect arXiv links or references to non-existent papers, fundamentally undermining their ability to properly attribute research contributions to the actual authors. We introduce arXivBench, a benchmark specifically designed to assess LLM performance across eight major subject categories on arXiv and five subfields within computer science, one of the most popular categories among them. Our findings show concerning accuracy variations across subjects, with Claude-3.5-Sonnet exhibiting a substantial advantage in generating both relevant and accurate responses. Notably, most LLMs perform significantly better in Artificial Intelligence than other subfields. This benchmark provides a standardized tool for evaluating LLM reliability in scientific contexts, promoting more dependable academic use in research environments. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/liningresearch/arXivBench">https://github.com/liningresearch/arXivBench</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/arXivBenchLLM/arXivBench">https://huggingface.co/datasets/arXivBenchLLM/arXivBench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在推理和问答方面表现出强大的能力，但它们生成事实错误内容的倾向仍然是一个关键挑战。本研究评估了专有和开源的LLM在生成带有准确arXiv链接的相关研究论文方面的能力。我们的评估揭示了重要的学术风险：LLM经常生成错误的arXiv链接或引用不存在的论文，从根本上削弱了它们正确归属研究贡献给实际作者的能力。我们推出了arXivBench，这是一个专门用于评估LLM在arXiv八大主题类别和计算机科学五大子领域性能的工具，这是它们中最受欢迎的一个类别。我们的研究发现在不同主题之间出现了令人担忧的准确性变化，其中Claude-3.5-Sonnet在生成既相关又准确响应方面表现出显著优势。值得注意的是，大多数LLM在人工智能方面的表现要好于其他子领域。这个基准工具为评估LLM在科学环境中的可靠性提供了标准化工具，促进了在学术环境中更可靠的用途。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/liningresearch/arXivBench%E5%92%8Chttps://huggingface.co/datasets/arXivBenchLLM/arXivBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liningresearch/arXivBench和https://huggingface.co/datasets/arXivBenchLLM/arXivBench找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10496v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在推理和问答方面表现出强大的能力，但生成事实错误内容的问题仍是关键挑战。本研究对专有和开源LLM生成具有准确arXiv链接的相关研究论文的能力进行评估。评估结果显示出重要的学术风险：LLM经常生成错误的arXiv链接或引用不存在的论文，从根本上破坏了其正确归属研究贡献的能力。为此，研究引入了arXivBench基准测试，该测试专门设计用于评估LLM在arXiv八大主题类别和计算机科学五个子领域的表现。研究发现，各主题之间的准确率存在差异，其中Claude-3.5-Sonnet在生成相关和准确响应方面表现出显著优势。值得注意的是，大多数LLM在人工智能方面的表现优于其他子领域。此基准测试为评估LLM在科学环境中的可靠性提供了标准化工具，促进了研究环境中更可靠的学术使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在生成带有准确arXiv链接的研究论文时存在事实错误的问题。</li>
<li>LLM有时会生成错误的arXiv链接或引用不存在的论文，这影响了对研究贡献的正确归属。</li>
<li>引入了一个新的基准测试工具arXivBench，用于评估LLM在多个学术领域（包括计算机科学的子领域）的表现。</li>
<li>在不同的主题类别中，LLM的表现存在差异，其中Claude-3.5-Sonnet在某些任务上表现较好。</li>
<li>LLM在人工智能领域的表现通常优于其他子领域。</li>
<li>arXivBench提供了一个标准化的工具来评估LLM在科学环境中的可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10496">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a441f7e04cd3954db8e47b9c4112e5c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81bab22a40fe8b5dd2e23c7d267ce4b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-876cc5585637479a30d851190fef08a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4fda89dd95e5b67423fc1c05c1b964.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aab5e979382e06953f30ceaddbb6fcf4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R2Vul-Learning-to-Reason-about-Software-Vulnerabilities-with-Reinforcement-Learning-and-Structured-Reasoning-Distillation"><a href="#R2Vul-Learning-to-Reason-about-Software-Vulnerabilities-with-Reinforcement-Learning-and-Structured-Reasoning-Distillation" class="headerlink" title="R2Vul: Learning to Reason about Software Vulnerabilities with   Reinforcement Learning and Structured Reasoning Distillation"></a>R2Vul: Learning to Reason about Software Vulnerabilities with   Reinforcement Learning and Structured Reasoning Distillation</h2><p><strong>Authors:Martin Weyssow, Chengran Yang, Junkai Chen, Ratnadira Widyasari, Ting Zhang, Huihui Huang, Huu Hung Nguyen, Yan Naing Tun, Tan Bui, Yikun Li, Ang Han Wei, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, David Lo</strong></p>
<p>Large language models (LLMs) have shown promising performance in software vulnerability detection, yet their reasoning capabilities remain unreliable. We propose R2Vul, a method that combines reinforcement learning from AI feedback (RLAIF) and structured reasoning distillation to teach small code LLMs to detect vulnerabilities while generating security-aware explanations. Unlike prior chain-of-thought and instruction tuning approaches, R2Vul rewards well-founded over deceptively plausible vulnerability explanations through RLAIF, which results in more precise detection and high-quality reasoning generation. To support RLAIF, we construct the first multilingual preference dataset for vulnerability detection, comprising 18,000 high-quality samples in C#, JavaScript, Java, Python, and C. We evaluate R2Vul across five programming languages and against four static analysis tools, eight state-of-the-art LLM-based baselines, and various fine-tuning approaches. Our results demonstrate that a 1.5B R2Vul model exceeds the performance of its 32B teacher model and leading commercial LLMs such as Claude-4-Opus. Furthermore, we introduce a lightweight calibration step that reduces false positive rates under varying imbalanced data distributions. Finally, through qualitative analysis, we show that both LLM and human evaluators consistently rank R2Vul model’s reasoning higher than other reasoning-based baselines. </p>
<blockquote>
<p>大型语言模型（LLM）在软件漏洞检测方面表现出有前景的性能，但它们的推理能力仍然不可靠。我们提出了R2Vul方法，它结合了人工智能反馈的强化学习（RLAIF）和结构化推理蒸馏技术，用于教授小型代码LLM检测漏洞，同时生成安全意识的解释。与先前的思维链和指令调整方法不同，R2Vul通过RLAIF奖励基于事实的漏洞解释，这些解释避免了似是而非的欺骗性，从而实现了更精确的检测和高质量的推理生成。为了支持RLAIF，我们构建了用于漏洞检测的第一套多语言偏好数据集，包含C#、JavaScript、Java、Python和C中的18,000个高质量样本。我们对五种编程语言进行了评估，并与四种静态分析工具、八个最新的LLM基准模型和各种微调方法进行了比较。我们的结果表明，一个规模为1.5B的R2Vul模型超过了其规模为32B的教师模型和领先的商业LLM（如Claude-4-Opus）的性能。此外，我们还引入了一个轻量级的校准步骤，该步骤可以降低在不同不平衡数据分布下的误报率。最后，通过定性分析，我们证明LLM和人类评估者都一致地认为R2Vul模型的推理能力高于其他基于推理的基准模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04699v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM在软件漏洞检测中表现优异，但其推理能力尚待提高。为此，研究者提出了R2Vul方法，它结合了强化学习和结构化推理蒸馏技术，用于教授小型代码LLM检测漏洞并生成安全意识的解释。R2Vul通过强化学习从人工智能反馈（RLAIF）奖励有根据的、不欺骗性的漏洞解释，从而提高漏洞检测的精确性和高质量推理生成。为了支持RLAIF，研究者构建了首个用于漏洞检测的多语言偏好数据集，包含C#、JavaScript、Java、Python和C的18,000高质量样本。评估结果显示，R2Vul模型性能超越大型模型及商业LLM，并引入轻量级校准步骤以降低不同不平衡数据分布下的误报率。定性分析显示，LLM和人类评估者均认为R2Vul模型的推理能力高于其他基于推理的基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在软件漏洞检测中表现良好，但推理能力需提升。</li>
<li>R2Vul方法结合了强化学习和结构化推理蒸馏技术，用于教授LLM检测漏洞并生成安全意识的解释。</li>
<li>RLAIF通过奖励有根据的、不欺骗性的漏洞解释，提高漏洞检测的精确性和推理质量。</li>
<li>研究者构建了首个用于漏洞检测的多语言偏好数据集。</li>
<li>R2Vul模型性能超越大型模型及商业LLM。</li>
<li>引入轻量级校准步骤以降低误报率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04699">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2962813dc677a286a0f1ddd6c685ab7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c182b1ab637ac0e1b5995fb3710cbcdf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b85547f1b52c2d5b07c34fbe7eda4939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e656244b8300eb0f9ec21c57d41a7ba6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SciReplicate-Bench-Benchmarking-LLMs-in-Agent-driven-Algorithmic-Reproduction-from-Research-Papers"><a href="#SciReplicate-Bench-Benchmarking-LLMs-in-Agent-driven-Algorithmic-Reproduction-from-Research-Papers" class="headerlink" title="SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic   Reproduction from Research Papers"></a>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic   Reproduction from Research Papers</h2><p><strong>Authors:Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He</strong></p>
<p>This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency&#x2F;API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmark’s difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at <a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench">https://github.com/xyzCS/SciReplicate-Bench</a> and project homepage at <a target="_blank" rel="noopener" href="https://xyzcs.github.io/scireplicate.github.io/">https://xyzcs.github.io/scireplicate.github.io/</a>. </p>
<blockquote>
<p>本研究评估大型语言模型（LLM）在根据最新自然语言处理（NLP）论文中的算法描述生成代码的能力。这项任务需要两个关键技能：（1）算法理解：从论文和学术文献中综合信息，以理解实现逻辑；（2）编码专业知识：识别依赖关系并正确实现必要的API。为了进行严格的评估，我们推出了SciReplicate-Bench，这是一个包含来自2024年发表的36篇NLP论文的100个任务的基准测试，具有详细的注释和全面的测试用例。基于SciReplicate-Bench，我们提出了Sci-Reproducer，这是一个由两个代理组成的框架，包括一个理解文献中算法概念的Paper Agent和一个从存储库中检索依赖关系并实现解决方案的Code Agent。为了评估算法理解，我们引入了推理图准确性，它量化了从代码注释和结构派生的生成推理图与参考推理图之间的相似性。为了评估实现质量，我们采用执行准确性、CodeBLEU以及存储库依赖&#x2F;API召回指标。在我们的实验中，我们评估了各种强大的非推理和推理LLM作为基础模型。表现最佳的LLM使用ModelName~仅达到39%的执行准确性，这突出了该基准测试的困难程度。我们的分析确定了缺失或不一致的算法描述是成功复制的主要障碍。我们在<a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%BB%E9%A1%B5https://xyzcs.github.io/scireplicate.github.io/%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/xyzCS/SciReplicate-Bench和项目主页https://xyzcs.github.io/scireplicate.github.io/上提供了我们的基准测试和代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00255v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究评估大型语言模型（LLM）在根据最新自然语言处理（NLP）论文中的算法描述生成代码的能力。这项任务需要两个关键技能：1）算法理解：从论文和学术文献中综合信息以理解实现逻辑；2）编码专业知识：识别依赖关系并正确实现必要的API。为了进行严格评估，我们推出了SciReplicate-Bench，这是一个由36篇于XXXX年发表的NLP论文中的XXXX个任务组成的基准测试，具有详细的注释和全面的测试用例。基于SciReplicate-Bench，我们提出了Sci-Reproducer，这是一个由论文代理和代码代理组成的双代理框架，论文代理负责从文献中解释算法概念，而代码代理则从存储库中检索依赖关系并实现解决方案。为了评估算法理解，我们引入了推理图准确性，该指标量化了从代码注释和结构派生的生成推理图和参考推理图之间的相似性。为了评估实现质量，我们采用了执行准确性、CodeBLEU以及存储库依赖&#x2F;API召回率指标。在我们的实验中，我们对各种强大的非推理和推理LLM作为基础模型进行了评估。表现最佳的LLM模型名称~仅达到XXXX%的执行准确性，凸显了本基准测试的困难度。我们的分析确定了缺失或不一致的算法描述是成功复制的主要障碍。我们的基准测试和代码可在<a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench%E4%B8%8A%E8%8E%B7%E5%8F%AF%E4%BB%A5%E9%A1%B5%E9%A1%B5%E9%9D%A2%E4%B8%BAhttps://xyzcs.github.io/scireplicate.github.io/">https://github.com/xyzCS/SciReplicate-Bench上获取，项目主页为https://xyzcs.github.io/scireplicate.github.io/。</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本研究评估了大型语言模型（LLM）在根据NLP论文中的算法描述生成代码的能力。</li>
<li>提出了SciReplicate-Bench基准测试，包含来自最新NLP论文的任务，用于严格评估LLM在此任务上的表现。</li>
<li>引入Sci-Reproducer框架，包含论文代理和代码代理，分别负责算法理解和代码实现。</li>
<li>采用多种指标评估算法理解和代码实现质量，包括推理图准确性、执行准确性、CodeBLEU以及存储库依赖&#x2F;API召回率。</li>
<li>实验显示，即使是最优秀的LLM模型，在执行准确性方面仍面临挑战，这突显了此任务的复杂性。</li>
<li>分析和指出算法描述的缺失或不一致是复制过程中的主要难题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-418037b50dd90d2816dc080497441c3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e410978de51867cd150f7d3627453c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a572f9fb0cc02fccde0bd660398cac5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348e4e7f2e6f8fb76e5d7e5b10d42240.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning"><a href="#Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning" class="headerlink" title="Teaching LLMs How to Learn with Contextual Fine-Tuning"></a>Teaching LLMs How to Learn with Contextual Fine-Tuning</h2><p><strong>Authors:Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan</strong></p>
<p>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human’s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, “can prompting help us teach LLMs how to learn”. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model’s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains. </p>
<blockquote>
<p>提示大型语言模型（LLM）或提供预期的模型操作上下文，是引导此类模型的输出以满足其在训练后的人类需求的有效方式。但在快速发展的领域中，通常需要微调LLM，以改善其内存中的知识类型或在新领域进行开放式推理的能力。当人类学习新概念时，我们常常通过将正在研究的新材料与已经学过的概念联系起来来学习。为此，我们的问题是：“提示能否帮助我们教会LLM如何学习”。在这项工作中，我们研究了一种指令微调的新泛化，称为上下文微调，以微调LLM。我们的方法利用指令提示，模仿人类在学习和解决问题中的认知策略，以指导训练过程中的学习过程，旨在提高模型对特定领域知识的解释和理解能力。我们通过实证证明，这种简单而有效的改进提高了LLM在新数据集上的快速微调能力，无论是在医疗还是金融领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09032v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>训练大型语言模型（LLM）时，通过提示特定模型操作模式可以有效引导模型输出以满足人类需求。在快速变化的领域中，需要微调LLM以提高其记忆知识或在新领域进行开放式推理的能力。本研究提出了一种新的指令微调方法——上下文微调，该方法利用模拟人类学习和解决问题的认知策略的指令提示来指导学习过程，旨在提高模型对特定领域的理解和解释能力。本研究通过实证证明，这种简单而有效的方法能提高LLM在新数据集上的微调能力，特别是在医疗和金融领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过提示引导大型语言模型（LLM）的输出以满足人类需求。</li>
<li>在快速变化的领域中需要微调LLM。</li>
<li>提出了一种新的指令微调方法——上下文微调。</li>
<li>上下文微调利用指令提示模拟人类学习和解决问题的认知策略。</li>
<li>上下文微调旨在提高模型对特定领域的理解和解释能力。</li>
<li>实证证明上下文微调能提高LLM在新数据集上的微调能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-38e2747557f574c3120eac5e9d048218.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd511298501f664e0a21061cd7e623f2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information"><a href="#Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information" class="headerlink" title="Transformer Meets Twicing: Harnessing Unattended Residual Information"></a>Transformer Meets Twicing: Harnessing Unattended Residual Information</h2><p><strong>Authors:Laziz Abdullaev, Tan M. Nguyen</strong></p>
<p>Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses kernel twicing procedure in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees and enhanced adversarial robustness. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data. </p>
<blockquote>
<p>基于Transformer的深度学习模型已在众多语言和视觉任务中实现了最先进的性能。虽然Transformer的核心组件自注意力机制已证明能够处理复杂的数据模式，但人们观察到，注意力矩阵在Transformer层间的表示能力会显著下降，从而损害其总体性能。在这项工作中，我们利用自注意力计算与低通非局部均值（NLM）平滑滤波器之间的联系，提出了Twicing Attention，这是一种新型注意力机制。它通过非参数回归中的核twicing程序来缓解与自注意力相关的NLM平滑的低通行为，具有引人注目的理论保证和增强的对抗稳健性。这种方法使得能够在每一层不完美的平滑操作后提取和再利用残差中保留的有意义的信息。我们提出的方法相对于标准自注意力具有两个关键优势：1）表示能力的衰减速度较慢；2）在各种数据模态和任务中提高了稳健性和准确性。我们在多个任务和基准测试上实证地展示了我们的模型相对于基线Transformer的性能提升，包括图像分类和语言建模，以及干净和损坏的数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00687v3">PDF</a> 10 pages in the main text. Published at ICLR 2025</p>
<p><strong>Summary</strong><br>     本文主要介绍了针对Transformer模型在深层学习中存在的不足（即注意矩阵的代表性容量在Transformer层中显著下降），提出了一种新的注意力机制——Twicing Attention。该机制利用自注意力计算和局部均值平滑滤波器之间的联系，通过非参数回归中的核调整过程来实现对平滑滤波器的低通行为的缓解。通过这种方式，该方法能够提取和重用每一层的不完美平滑操作后的残差中的有意义信息。相比于标准的自注意力机制，Twicing Attention提供了两个主要优势：保证的代表性容量衰减较慢，以及在不同数据模态和任务上提高了鲁棒性和准确性。作者在多个任务和基准测试上实证了模型性能的提升，包括图像分类和语言建模，以及干净和受污染的数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型中的自注意力机制在处理复杂数据模式时表现出色，但注意矩阵的代表性容量在Transformer层中会显著下降，影响整体性能。</li>
<li>提出了一种新的注意力机制——Twicing Attention，利用自注意力计算和局部均值平滑滤波器之间的联系，通过核调整过程改善模型性能。</li>
<li>Twicing Attention能够提取和重用每一层的不完美平滑操作后的残差中的有意义信息。</li>
<li>Twicing Attention相比标准自注意力机制具有两个主要优势：保证的代表性容量衰减较慢，以及提高的鲁棒性和准确性。</li>
<li>该方法在图像分类和语言建模等多个任务上实现了对基准Transformer模型的性能提升。</li>
<li>该方法能在干净和受污染的数据上均实现性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b7eb0c6bc88b1f7980d52e64a30ed09b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33dbab91cb18b20834a539c9c39cbb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9da29fd57acbcffe975cd8e5f69f5a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-542f9c551bb071c51e0076da5f027eb0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Retrieval-Augmented-Generation-in-Multi-Modal-Contexts"><a href="#Benchmarking-Retrieval-Augmented-Generation-in-Multi-Modal-Contexts" class="headerlink" title="Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts"></a>Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts</h2><p><strong>Authors:Zhenghao Liu, Xingsheng Zhu, Tianshuo Zhou, Xinyi Zhang, Xiaoyuan Yi, Yukun Yan, Ge Yu, Maosong Sun</strong></p>
<p>With the rapid advancement of Multi-modal Large Language Models (MLLMs), their capability in understanding both images and text has greatly improved. However, their potential for leveraging multi-modal contextual information in Retrieval-Augmented Generation (RAG) remains largely underexplored. To address this gap, this paper introduces Multi-Modal Retrieval-Augmented Generation (M$^2$RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as contextual input for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments demonstrate the effectiveness of MM-RAIT by significantly improving the quality of responses generated by different RAG models, outperforming MiniCPM-V 2.6 and Qwen2-VL with 34% and 33% gains, respectively. All data and code are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG">https://github.com/NEUIR/M2RAG</a>. </p>
<blockquote>
<p>随着多模态大型语言模型（MLLMs）的快速发展，它们对图像和文本的理解能力得到了极大的提高。然而，它们在检索增强生成（RAG）中利用多模态上下文信息的潜力尚未得到充分探索。为了解决这一空白，本文引入了多模态检索增强生成（M^2^RAG）基准测试，旨在评估多模态大型语言模型在利用多模态检索文档中的知识方面的有效性。该基准测试包括四个任务：图像描述、多模态问答、多模态事实验证和图像重排序。所有任务都在开放域环境中设置，要求RAG模型从多模态文档集合中检索与查询相关的信息，并将其用作RAG建模的上下文输入。为了提高MLLMs的上下文利用能力，我们还引入了多模态检索增强指令调整（MM-RAIT），这是一种指令调整方法，可在多模态上下文中优化MLLMs。我们的实验表明，MM-RAIT通过显著提高不同RAG模型生成的响应质量而有效，分别超越了MiniCPM-V 2.6和Qwen2-VL，分别提高了34%和33%。所有数据代码均可从<a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/NEUIR/M2RAG获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17297v2">PDF</a> </p>
<p><strong>Summary</strong>：随着多模态大型语言模型（MLLMs）的快速发展，其对图像和文本的理解能力已大幅提升。然而，它们在利用多模态上下文信息方面的潜力在检索增强生成（RAG）中仍被大量忽视。本文提出了多模态检索增强生成（M^2RAG）这一基准测试，旨在评估多模态大型语言模型在利用多模态检索文档中的知识方面的有效性。该基准测试包括四个任务：图像描述、多模态问答、多模态事实验证和图像重排序。所有任务均在开放领域环境中设置，要求RAG模型从多模态文档集合中检索与查询相关的信息，并将其用作RAG建模的上下文输入。为了提升MLLMs在上下文使用方面的能力，本文还引入了多模态检索增强指令调整（MM-RAIT），这是一种能在多模态上下文中优化MLLMs的指令调整方法。实验证明，MM-RAIT能有效提升不同RAG模型生成答案的质量，相较于MiniCPM-V 2.6和Qwen2-VL分别提升了34%和33%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态大型语言模型（MLLMs）能理解图像和文本，但其利用多模态上下文信息的潜力在检索增强生成（RAG）中尚未被充分探索。</li>
<li>本文提出了多模态检索增强生成（M^2RAG）基准测试，涵盖图像描述、多模态问答、多模态事实验证和图像重排序四个任务，以评估多模态LLMs的有效性。</li>
<li>M^2RAG要求在开放领域环境中，RAG模型能从多模态文档集合中检索相关信息作为上下文输入。</li>
<li>为了提升MLLMs在上下文使用方面的能力，引入了多模态检索增强指令调整（MM-RAIT）方法。</li>
<li>MM-RAIT能有效提升不同RAG模型生成答案的质量，相比其他模型有显著提升。</li>
<li>M^2RAG和MM-RAIT的相关数据和代码已公开，便于研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d650c1717a6ad204b9da1c60a89fb2d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6fed17cebbde1749ade6e441f490eb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25bfc21547b1a0390e0cdd8ee5497302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-277bea201f7673a6e25e0ec81b7a015e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0efb417f4de0ad44f23a7566393e400.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm"><a href="#RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm" class="headerlink" title="RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm"></a>RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm</h2><p><strong>Authors:Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng</strong></p>
<p>After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of multimodal interleaved documents remains underutilized for contrastive vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. We compare our dataset with other widely used datasets of equivalent scale for CLIP training. Models pre-trained on RealSyn consistently achieve state-of-the-art performance across various downstream tasks, including linear probe, zero-shot transfer, zero-shot robustness, and zero-shot retrieval. Furthermore, extensive experiments confirm that RealSyn significantly enhances contrastive vision-language representation learning and demonstrates robust scalability. To facilitate future research, the RealSyn dataset and pretrained model weights are released at <a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn">https://github.com/deepglint/RealSyn</a>. </p>
<blockquote>
<p>经过对大量图像文本对进行预训练后，对比语言图像预训练（CLIP）在各种基准测试上表现出了有前景的性能。然而，大量的未配对的多媒体交互文档在对比视觉语言表示学习中仍未得到充分利用。为了充分利用这些未配对的文档，我们最初建立了一个现实世界数据提取管道，以提取高质量的图片和文本。然后，我们设计了一种分层检索方法，以有效地将每张图像与多个语义上相关的现实文本相关联。为了进一步提取精细的视觉信息，我们提出了一个图像语义增强生成模块来进行合成文本的生产。此外，我们采用了一种语义平衡采样策略来提高数据集多样性，从而能够更好地学习长尾概念。基于这些创新，我们构建了RealSyn数据集，融合了现实和合成文本，分为三个规模：15M、30M和100M。我们将数据集与同等规模的其他常用CLIP训练数据集进行了比较。在各种下游任务中，基于RealSyn预训练的模型始终实现了最先进的性能，包括线性探测、零样本迁移、零样本鲁棒性和零样本检索。此外，大量实验证实，RealSyn显著增强了对比视觉语言表示学习，并表现出稳健的可扩展性。为了方便未来研究，RealSyn数据集和预训练模型权重已在<a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/deepglint/RealSyn上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12513v3">PDF</a> 15 pages, 12 figures, Accepted by ACM MM2025, Webpage:   <a target="_blank" rel="noopener" href="https://garygutc.github.io/RealSyn">https://garygutc.github.io/RealSyn</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于对比视觉语言预训练的改进方法，通过构建RealSyn数据集来提升对比视觉语言表示学习。RealSyn数据集结合了真实和合成文本，建立了真实世界数据提取管道和分层检索方法，以提高图像与文本之间的语义关联。此外，通过图像语义增强生成模块和语义平衡采样策略，提高了模型的性能。在多个下游任务中，预训练在RealSyn上的模型表现出最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP在广泛的多模态交错文档上的性能有待提升。</li>
<li>建立了真实世界数据提取管道以提取高质量图像和文本。</li>
<li>设计了分层检索方法，使每个图像与多个语义相关的现实文本有效关联。</li>
<li>通过图像语义增强生成模块提高精细视觉信息。</li>
<li>采用语义平衡采样策略提高数据集多样性，有助于学习长尾概念。</li>
<li>构建了RealSyn数据集，结合了真实和合成文本，提供三种规模供选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-654e4052b5a08f584301f164972971e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68b3b3839b17adbd5d83ea72d1850ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1ed79d3741132dc49cf8024cdb1ba2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3207398aad40c4f07bae705f4a2a25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-854b803ced1c6b4088dfa89873a0d86d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-481418791622bcc2bee0df86fd5fe8ec.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Efficient-Knowledge-Injection-in-LLMs-via-Self-Distillation"><a href="#Efficient-Knowledge-Injection-in-LLMs-via-Self-Distillation" class="headerlink" title="Efficient Knowledge Injection in LLMs via Self-Distillation"></a>Efficient Knowledge Injection in LLMs via Self-Distillation</h2><p><strong>Authors:Kalle Kujanpää, Pekka Marttinen, Harri Valpola, Alexander Ilin</strong></p>
<p>In many practical applications, large language models (LLMs) need to acquire new knowledge not present in their pre-training data. Efficiently leveraging this knowledge usually relies on supervised fine-tuning or retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. This paper proposes utilizing prompt distillation, a self-distillation-based method previously explored primarily for style alignment and instruction tuning, to internalize new factual knowledge from free-form documents. Unlike prior methods, our approach requires neither larger teacher models nor structured knowledge formats. Across multiple LLM sizes and model families, we show that prompt distillation outperforms standard supervised fine-tuning and can even surpass RAG. We analyze the key factors contributing to prompt distillation’s effectiveness and examine how it scales. </p>
<blockquote>
<p>在许多实际应用中，大型语言模型（LLM）需要获取其预训练数据中未包含的新知识。有效地利用这些知识通常依赖于监督微调或检索增强生成（RAG）。尽管RAG已成为知识注入的行业标准，但微调尚未取得与之相当的成就。本文提出利用提示蒸馏法，这是一种基于自我蒸馏的方法，之前主要用于风格对齐和指令调整，以从自由形式的文档中内化新的事实知识。不同于以前的方法，我们的方法既不需要更大的教师模型，也不需要结构化的知识格式。我们在多个LLM规模和模型家族中表明，提示蒸馏法优于标准的监督微调，甚至可以超越RAG。我们分析了提示蒸馏法有效性的关键因素，并研究了其如何扩展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14964v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在实际应用中需要获取预训练数据以外的新知识。尽管检索增强生成（RAG）已成为知识注入的行业标准，但微调尚未取得相应的成功。本文提出利用提示蒸馏法，这是一种基于自我蒸馏的方法，以前主要用于风格对齐和指令调整，以内化来自自由形式文档的新事实知识。与其他方法不同，我们的方法既不需要更大的教师模型，也不需要结构化知识格式。跨多个LLM大小和模型家族，我们证明了提示蒸馏法优于标准的监督微调，甚至可以超越RAG。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在实际应用中需要获取预训练数据之外的新知识。</li>
<li>检索增强生成（RAG）已成为知识注入的行业标准，但微调尚未取得同等成功。</li>
<li>提示蒸馏法是一种基于自我蒸馏的方法，可以内化来自自由形式文档的新事实知识。</li>
<li>与其他方法相比，提示蒸馏法不需要更大的教师模型和结构化知识格式。</li>
<li>提示蒸馏法在不同规模和家族的LLM中表现出优于标准监督微调的性能。</li>
<li>提示蒸馏法的有效性关键在于其能够自我蒸馏并内化新的事实知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14964">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-64211217265df0957718a867d027efda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97782c26e8558270397361ed858cdcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866b4c72caec10896f768b9718220734.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling"><a href="#Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling" class="headerlink" title="Core Context Aware Transformers for Long Context Language Modeling"></a>Core Context Aware Transformers for Long Context Language Modeling</h2><p><strong>Authors:Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan</strong></p>
<p>Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods. </p>
<blockquote>
<p>基于Transformer的大型语言模型（LLM）在大量任务中取得了显著的成功，这主要归功于自注意力机制。自注意力机制要求一个标记（token）将所有先前的标记作为其上下文来计算注意力。然而，当上下文长度L变得非常大（例如128K）时，上下文中潜在冗余信息的数量往往会增加。冗余的上下文不仅阻碍建模表示性能，而且还产生不必要的计算和存储开销。在本文中，我们提出了一种即插即用的核心上下文感知（CCA）注意力，用于高效的长上下文建模，包括两个互补模块：1）全局感知池模块对输入标记进行分组，并根据其重要性动态地将每个组压缩成一个核心标记。通过这种方式，我们的方法在学习过程中自动聚焦和增强核心上下文，同时减少冗余，从而实现有效的长期依赖关系建模。2）局部性保持模块结合了相邻的标记，以保留局部上下文，从而实现详细的表示。值得注意的是，我们的CCA注意力能够以最小的微调成本替换现有LLM中的自注意力模块。大量的实验结果证明，我们的方法在长上下文建模和计算效率上优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12465v3">PDF</a> Accepted for publication at ICML 2025</p>
<p><strong>Summary</strong>：基于Transformer的大型语言模型（LLM）在处理长文本时面临冗余信息问题，影响建模性能并造成计算和存储负担。本文提出了一种名为核心上下文感知（CCA）注意力机制，通过全局感知池化模块和局部保留模块来高效建模长文本，并自动聚焦强化核心上下文并减少冗余。CCA注意力能替换现有LLM中的自注意力模块，并具有良好的计算效率和性能表现。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>基于Transformer的LLM在处理长文本时存在冗余信息问题。</li>
<li>核心上下文感知（CCA）注意力机制旨在解决这一问题，包括全局感知池化模块和局部保留模块。</li>
<li>全局感知池化模块按重要性将输入令牌分组并动态压缩，以强化核心上下文并减少冗余。</li>
<li>局部保留模块保留邻近令牌以进行详细表示。</li>
<li>CCA注意力机制能替换现有LLM中的自注意力模块。</li>
<li>实证研究结果显示，CCA注意力机制在长篇文本建模和计算效率上表现优越。</li>
<li>该方法具有较低的微调成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12465">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3d9b50b0189a56c71dd6a3140d8e77df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40b602f47d28b43700efe26f86ca568c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87dad71f9789ae4f7588a8ec09e7663a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4f5884df1f8a0703532a02667888f00.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GuARD-Effective-Anomaly-Detection-through-a-Text-Rich-and-Graph-Informed-Language-Model"><a href="#GuARD-Effective-Anomaly-Detection-through-a-Text-Rich-and-Graph-Informed-Language-Model" class="headerlink" title="GuARD: Effective Anomaly Detection through a Text-Rich and   Graph-Informed Language Model"></a>GuARD: Effective Anomaly Detection through a Text-Rich and   Graph-Informed Language Model</h2><p><strong>Authors:Yunhe Pang, Bo Chen, Fanjin Zhang, Yanghui Rao, Evgeny Kharlamov, Jie Tang</strong></p>
<p>Anomaly detection on text-rich graphs is widely prevalent in real life, such as detecting incorrectly assigned academic papers to authors and detecting bots in social networks. The remarkable capabilities of large language models (LLMs) pave a new revenue by utilizing rich-text information for effective anomaly detection. However, simply introducing rich texts into LLMs can obscure essential detection cues and introduce high fine-tuning costs. Moreover, LLMs often overlook the intrinsic structural bias of graphs which is vital for distinguishing normal from abnormal node patterns. To this end, this paper introduces GuARD, a text-rich and graph-informed language model that combines key structural features from graph-based methods with fine-grained semantic attributes extracted via small language models for effective anomaly detection on text-rich graphs. GuARD is optimized with the progressive multi-modal multi-turn instruction tuning framework in the task-guided instruction tuning regime tailed to incorporate both rich-text and structural modalities. Extensive experiments on four datasets reveal that GuARD outperforms graph-based and LLM-based anomaly detection methods, while offering up to 5$\times$ times speedup in training and 5$\times$ times speedup in inference over vanilla long-context LLMs on the large-scale WhoIsWho dataset. </p>
<blockquote>
<p>文本丰富的图上的异常检测在现实生活中普遍存在，例如检测错误分配给作者的学术论文和检测社交网络中的机器人。大型语言模型（LLM）的显著能力为利用丰富文本信息进行有效的异常检测开辟了新的收入来源。然而，仅仅将丰富文本引入LLM会掩盖重要的检测线索，并产生高昂的微调成本。此外，LLM通常会忽略图的内在结构偏见，这对于区分正常和异常的节点模式至关重要。为此，本文介绍了GuARD，这是一种结合基于图的丰富文本和图信息语言模型的方法。GuARD结合了基于图的方法的关键结构特征与通过小型语言模型提取的精细语义属性，用于在文本丰富的图上有效地进行异常检测。GuARD采用任务导向指令调整制度下的渐进式多模式多轮指令调整框架进行了优化，以结合丰富文本和结构模式。在四个数据集上的大量实验表明，GuARD在异常检测方法上优于基于图和基于LLM的方法，同时在大型WhoIsWho数据集上提供了高达5倍的培训和推理速度提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03930v2">PDF</a> Accepted at KDD 2025</p>
<p><strong>Summary</strong><br>文本丰富的图异常检测在现实生活中应用广泛，如检测学术论文的错误分配和社交网络中机器人的检测。大型语言模型（LLM）的出色能力为利用丰富文本信息进行有效的异常检测开辟了新的收入途径。然而，仅仅将丰富文本引入LLM可能会掩盖重要的检测线索并产生高昂的微调成本。此外，LLM往往会忽视图的结构性偏见，这对于区分正常和异常的节点模式至关重要。为此，本文提出了GuARD，它是一种文本丰富且基于图的模型，结合了基于图的和基于小型语言模型提取的关键结构特征以及精细语义属性，可有效检测文本丰富的图的异常。实验表明，GuARD在异常检测方面的性能优于基于图和基于LLM的方法，在大型数据集上实现了高达五倍的训练速度和推理速度的提升。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异常检测在现实世界中有广泛的应用，如学术论和社交网络的检测问题。</li>
<li>大型语言模型在利用丰富文本进行异常检测方面具有巨大潜力。</li>
<li>仅使用丰富文本引入大型语言模型可能导致重要检测线索的丢失和较高的微调成本。</li>
<li>大型语言模型往往忽视图的结构性偏见，这对于区分正常和异常节点模式至关重要。</li>
<li>GuARD模型结合了基于图的精细结构特征和通过小型语言模型提取的语义属性，有效提高了异常检测的准确性。</li>
<li>GuARD模型的优化包括采用多模态多任务指导训练策略以及结合丰富文本和结构模式的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e8849d9e82a3b3858fdad0642ac66751.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a03fb974d14810ca8ee1a77f73c3518.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0cb6f5b995d0a0f715cbd67de85df75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88776a1a6faf35e8f45920aab6084ad0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation"><a href="#CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation" class="headerlink" title="CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation"></a>CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation</h2><p><strong>Authors:Hui Zhang, Dexiang Hong, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality. As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation. However, previous methods primarily focus on UNet-based models (\eg SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities. Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities. To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout. To inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities. Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage. Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description. We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality. Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization. These components form CreatiLayout – a systematic solution that integrates the layout model, dataset, and planner for creative layout-to-image generation. </p>
<blockquote>
<p>扩散模型因其能够生成不仅在视觉上吸引人而且艺术性很高的图像而备受瞩目。因此，提出了Layout-to-Image（L2I）生成方法，利用特定区域的位置和描述来实现更精确和可控的生成。然而，之前的方法主要集中在基于UNet的模型（例如SD1.5和SDXL），对多模态扩散变压器（MM-DiT）的探索有限，后者已显示出强大的图像生成能力。虽然使MM-DiT用于布局到图像生成看似简单，但由于布局如何引入、集成和平衡在多模态之间的复杂性，这仍然是一个挑战。为此，我们探索了各种网络变体，以有效地将布局指导融入MM-DiT，并最终推出SiamLayout。为了继承MM-DiT的优点，我们使用一组独立的网络权重来处理布局，将其视为与图像和文本模态同样重要。同时，为了减轻模态之间的竞争，我们将图像布局交互分解成与图像文本分支并列的孪生分支，并在后期进行融合。此外，我们贡献了一个大规模布局数据集，名为LayoutSAM，其中包括270万张图像文本对和1070万个实体。每个实体都带有边界框和详细描述。我们还构建了LayoutSAM-Eval基准测试，作为评估L2I生成质量的综合工具。最后，我们介绍了布局设计师，它挖掘了大语言模型在布局规划中的潜力，将其转变为布局生成和优化的专家。这些组件构成了CreatiLayout——一个系统解决方案，集成了布局模型、数据集和规划器，用于创意布局到图像生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03859v3">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用扩散模型进行布局到图像（Layout-to-Image，L2I）生成的技术。文章介绍了Multimodal Diffusion Transformers（MM-DiT）在图像生成中的应用，并指出如何将布局指导有效地融入MM-DiT的复杂性。为此，文章探索了各种网络变体，最终提出了SiamLayout方案。该方案利用专门的网络权重处理布局，并解耦图像与布局的交互，在后期进行融合。此外，文章还贡献了一个大规模布局数据集LayoutSAM，并建立了评价L2I生成质量的LayoutSAM-Eval基准测试。最后，文章介绍了Layout Designer，这是一个利用大型语言模型在布局规划中的潜力，将其转化为布局生成和优化的专家系统。这些组件共同构成了CreatiLayout，一个整合布局模型、数据集和规划器的创意布局到图像生成的系统解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够生成高质量图像，Layout-to-Image（L2I）生成技术利用区域特定位置和描述来实现更精确和可控的生成。</li>
<li>之前的方法主要关注UNet-based模型，而Multimodal Diffusion Transformers（MM-DiT）在图像生成中展现出强大能力，但融入布局指导具有挑战性。</li>
<li>SiamLayout通过探索各种网络变体来高效融入布局指导，利用专门的网络权重处理布局，并解耦图像与布局的交互。</li>
<li>LayoutSAM是一个大规模布局数据集，包含图像、文本和实体标注，为L2I生成提供数据支持。</li>
<li>LayoutSAM-Eval基准测试用于评估L2I生成质量。</li>
<li>Layout Designer利用大型语言模型在布局规划中的潜力，转化为布局生成和优化的专家系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-801f5bce18b516eeb9427fd70bf3a4c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26fbd8de78e813b98b0216047d6b9996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0b86c52feca87286e9d69698727f63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8671d9d7259b61533ae357f5325deca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec96ebca697e6cdfdab43f8bdb08c33.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="WSI-LLaVA-A-Multimodal-Large-Language-Model-for-Whole-Slide-Image"><a href="#WSI-LLaVA-A-Multimodal-Large-Language-Model-for-Whole-Slide-Image" class="headerlink" title="WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image"></a>WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image</h2><p><strong>Authors:Yuci Liang, Xinheng Lyu, Meidan Ding, Wenting Chen, Jipeng Zhang, Yuexiang Ren, Xiangjian He, Song Wu, Sen Yang, Xiyue Wang, Xiaohan Xing, Linlin Shen</strong></p>
<p>Recent advancements in computational pathology have produced patch-level Multi-modal Large Language Models (MLLMs), but these models are limited by their inability to analyze whole slide images (WSIs) comprehensively and their tendency to bypass crucial morphological features that pathologists rely on for diagnosis. To address these challenges, we first introduce WSI-Bench, a large-scale morphology-aware benchmark containing 180k VQA pairs from 9,850 WSIs across 30 cancer types, designed to evaluate MLLMs’ understanding of morphological characteristics crucial for accurate diagnosis. Building upon this benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI understanding that employs a three-stage training approach: WSI-text alignment, feature space alignment, and task-specific instruction tuning. To better assess model performance in pathological contexts, we develop two specialized WSI metrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that WSI-LLaVA outperforms existing models across all capability dimensions, with a significant improvement in morphological analysis, establishing a clear correlation between morphological understanding and diagnostic accuracy. </p>
<blockquote>
<p>计算病理学领域的最新进展已经产生了补丁级别的多模态大型语言模型（MLLMs），但这些模型受到无法全面分析全幻灯片图像（WSIs）的局限，并且倾向于绕过病理学家赖以进行诊断的关键形态特征。为了应对这些挑战，我们首先引入了WSI-Bench，这是一个大规模的形态感知基准测试，包含来自9850张幻灯片图像的18万对问答（VQA），涉及3种癌症类型，旨在评估MLLMs对形态特征的认知程度，这对于准确诊断至关重要。基于此基准测试，我们提出了WSI-LLaVA，这是一个用于gigapixel WSI理解的全新框架，采用三阶段训练方法：WSI文本对齐、特征空间对齐和任务特定指令调整。为了更好地评估模型在病理上下文中的性能，我们开发了两个专门的WSI指标：WSI精确度和WSI相关性。实验结果表明，WSI-LLaVA在所有能力维度上都优于现有模型，在形态分析方面取得了显著改进，建立了形态理解与诊断准确性之间的明确相关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02141v3">PDF</a> ICCV 2025, 38 pages, 22 figures, 35 tables</p>
<p><strong>Summary</strong></p>
<p>计算病理学领域的最新进展已经出现了基于patch的多模态大语言模型（MLLMs），但这些模型在分析全切片图像（WSIs）时存在局限性，无法全面分析并容易忽略病理医师诊断所依赖的关键形态特征。为解决这些问题，研究团队推出了WSI-Bench，这是一个大规模的形态感知基准测试，包含来自9850个WSIs的18万个问答对，涉及30种癌症类型，旨在评估MLLMs对关键形态特征的理解能力。在此基础上，研究团队进一步提出了WSI-LLaVA框架，采用三阶段训练法处理巨像素WSI理解问题，包括WSI文本对齐、特征空间对齐和任务特定指令调整。为更好地评估模型在病理环境下的表现，研究团队还开发了两个专门的WSI指标：WSI精度和WSI相关性。实验结果表明，WSI-LLaVA在各方面性能均优于现有模型，形态分析显著改善，形态理解与诊断准确度的相关性明确。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算病理学领域出现基于patch的多模态大语言模型（MLLMs）。</li>
<li>MLLMs在分析全切片图像（WSIs）时存在局限性，无法全面分析并易忽略关键形态特征。</li>
<li>WSI-Bench是一个大规模的形态感知基准测试，用于评估MLLMs对形态特征的理解能力。</li>
<li>WSI-LLaVA是一个处理巨像素WSI理解的框架，采用三阶段训练法提高模型性能。</li>
<li>WSI-LLaVA在形态分析方面显著改善，实验结果表明其性能优于现有模型。</li>
<li>研究团队为评估模型在病理环境下的表现，开发了两个专门的WSI指标：WSI精度和WSI相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d0ec3fd4eb35d7ad3186dedaf23ada5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb28db29e76d6685187aea3aea3428fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e89f945a4cced96220b5ef2bda4abb90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caa8b63272b4de47beec9e6e7fff7408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a843eaefd0f146f33a11a106abf8ef6c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs"><a href="#A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs" class="headerlink" title="A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs"></a>A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs</h2><p><strong>Authors:Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang</strong></p>
<p>Transformer neural networks (TNN) excel in natural language processing (NLP), machine translation, and computer vision (CV) without relying on recurrent or convolutional layers. However, they have high computational and memory demands, particularly on resource-constrained devices like FPGAs. Moreover, transformer models vary in processing time across applications, requiring custom models with specific parameters. Designing custom accelerators for each model is complex and time-intensive. Some custom accelerators exist with no runtime adaptability, and they often rely on sparse matrices to reduce latency. However, hardware designs become more challenging due to the need for application-specific sparsity patterns. This paper introduces ADAPTOR, a runtime-adaptive accelerator for dense matrix computations in transformer encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing elements and on-chip memory, enhancing parallelism and reducing latency. It incorporates efficient matrix tiling to distribute resources across FPGA platforms and is fully quantized for computational efficiency and portability. Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively. Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some state-of-the-art FPGA-based accelerators. </p>
<blockquote>
<p>Transformer神经网络（TNN）在自然语言处理（NLP）、机器翻译和计算机视觉（CV）方面表现出色，无需依赖循环或卷积层。然而，它们对计算和内存的需求很高，特别是在FPGA等资源受限的设备上。此外，不同的应用程序中，Transformer模型的处理时间会有所不同，因此需要具有特定参数的定制模型。为每种模型设计定制加速器是复杂且耗时的。虽然存在一些没有运行时适应性的定制加速器，但它们通常依赖于稀疏矩阵来降低延迟。然而，由于需要特定的应用稀疏模式，硬件设计变得更加具有挑战性。本文介绍了ADAPTOR，这是一个用于FPGA上Transformer编码器和解码器中的密集矩阵计算的运行时自适应加速器。ADAPTOR提高了处理元件和片上内存的利用率，增强了并行性并降低了延迟。它采用高效的矩阵分块技术，以在FPGA平台之间分配资源，并且为了计算效率和便携性而完全量化。在Xilinx Alveo U55C数据中心卡和VC707和ZCU102等嵌入式平台上的评估显示，我们的设计比NVIDIA K80 GPU和i7-8700K CPU分别高出1.2倍和2.87倍的能效。此外，与一些最新的FPGA加速器相比，它实现了1.7到2.25倍的速度提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18148v4">PDF</a> arXiv admin note: text overlap with arXiv:2409.14023</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对FPGA上变压器编码器解码器中密集矩阵运算的、具有运行时自适应性的加速器——ADAPTOR。它能提高处理元件和片上内存的利用率，增强并行性并降低延迟。通过有效的矩阵切片技术，它能合理分配资源在各种FPGA平台上，并且为实现计算效率和便携性进行了全面量化。评估结果表明，相比NVIDIA K80 GPU和i7-8700K CPU，ADAPTOR在设计效率上更具优势，实现了较高的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer neural networks (TNN) 不依赖循环卷积层就能在自然语言处理（NLP）、机器翻译和计算机视觉（CV）等领域表现出色，但计算内存需求较高，特别是在FPGA等资源受限设备上。</li>
<li>针对不同应用，Transformer模型的处理时间有所不同，需要定制具有特定参数的模型。设计针对每个模型的定制加速器是复杂且耗时的。</li>
<li>当前存在一些无运行时适应性的定制加速器，它们通常依赖稀疏矩阵来降低延迟，但由于需要特定的应用稀疏模式，硬件设计更具挑战性。</li>
<li>本文提出了ADAPTOR，一个针对FPGA上Transformer编码器解码器密集矩阵运算的、具有运行时自适应性的加速器。</li>
<li>ADAPTOR通过提高处理元件和片上内存的利用率、增强并行性并降低延迟来优化性能。</li>
<li>ADAPTOR采用有效的矩阵切片技术，可以跨FPGA平台分配资源，且为了计算效率和便携性进行了全面量化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18148">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc637285f66bf3f658a2220114ff2093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a852501cf32a44d34201e1427b1178d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b21994e7d6d3cc6269f2caff93d1639.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DOTS-Learning-to-Reason-Dynamically-in-LLMs-via-Optimal-Reasoning-Trajectories-Search"><a href="#DOTS-Learning-to-Reason-Dynamically-in-LLMs-via-Optimal-Reasoning-Trajectories-Search" class="headerlink" title="DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning   Trajectories Search"></a>DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning   Trajectories Search</h2><p><strong>Authors:Murong Yue, Wenlin Yao, Haitao Mi, Dian Yu, Ziyu Yao, Dong Yu</strong></p>
<p>Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called “reasoning actions”), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems. </p>
<blockquote>
<p>近年来，增强大型语言模型（LLM）的推理能力已经引起了广泛关注。以往的研究表明，各种提示策略在帮助LLM进行推理（称为“推理行动”）方面是行之有效的，例如逐步思考、答题前思考、程序解题及其组合。然而，这些方法通常将静态的预定义推理行动统一应用于所有问题，而没有考虑到每个问题的特定特征或任务解决LLM的能力。在本文中，我们提出了DOTS方法，这是一种使LLM能够通过最优推理轨迹搜索进行动态推理的方法，它根据每个问题的特定特征和任务解决LLM的内在能力来定制。我们的方法涉及三个关键步骤：i）定义原子推理行动模块，这些模块可以组合成各种推理行动轨迹；ii）通过迭代探索和评估，为特定的任务解决LLM针对每个训练问题搜索最优行动轨迹；iii）使用收集到的最优轨迹来训练LLM，以规划未见问题的推理轨迹。特别是，我们提出了两种学习范式，即微调外部LLM作为规划师来指导任务解决LLM，或直接微调任务解决LLM，使其具备推理行动规划的能力。我们在八个推理任务上的实验表明，我们的方法始终优于静态推理技术和普通的指令调整方法。进一步的分析表明，我们的方法使LLM能够根据问题的复杂性调整其计算，将更深入的思考和推理分配给更复杂的问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03864v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>摘要</strong></p>
<p>近年来，提升大语言模型（LLM）的推理能力备受关注。先前的研究已经展示了各种提示策略在辅助LLM进行推理（称为”推理行动”）中的有效性，如逐步思考、预思考再回答、程序求解等。然而，这些方法常常统一地应用静态的、预定义的推理行动到所有问题上，没有考虑到每个问题的特定特性或任务解决LLM的能力。本文提出DOTS方法，一种使LLM能够针对每个问题的特定特性和任务解决LLM的内在能力进行动态推理的方法。该方法包括三个关键步骤：定义可以组合成各种推理行动轨迹的原子推理行动模块；通过迭代探索和评估为每个训练问题搜索特定的任务解决LLM的最优行动轨迹；使用收集的最优轨迹来训练LLM，以规划未见问题的推理轨迹。我们提出了两种学习范式，即微调外部LLM作为规划器来指导任务解决LLM，或直接微调任务解决LLM以具备内部化的推理行动规划能力。在八个推理任务上的实验表明，我们的方法始终优于静态推理技术和指令微调方法。进一步分析表明，我们的方法使LLM能够根据问题复杂度调整计算，将更深入的思考和推理分配给更难的问题。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM的推理能力提升受到关注，之前的方法多是应用静态推理行动到所有问题。</li>
<li>本文提出DOTS方法，能基于问题特性和LLM能力进行动态推理。</li>
<li>DOTS包含三个关键步骤：定义原子推理模块、搜索最优行动轨迹、训练LLM进行规划。</li>
<li>提出两种学习范式：外部LLM规划器或内部化LLM能力。</li>
<li>在八个推理任务上的实验显示，DOTS方法优于静态和指令微调方法。</li>
<li>DOTS方法使LLM能够根据问题复杂度调整计算，合理分配推理资源。</li>
<li>该方法有助于提升LLM在实际问题中的灵活性和适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03864">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-29e0e26bb4bce9c27a72253b4935a74a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50dca2fb8ce1ae18f12702022d4cd0a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c0f0b2f112e51bfcc0bfb6ceebcadf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa2eb691d80029e6232bd3027534e2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e96dfe671dc6f1f0a3e8ae1abbf7c9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9acde00b43fc0b1fe920d46825989a98.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6eca8c36554ad573a9576ae93ae01c08.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-10  Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Population-Based Screening and Primary Diagnosis in a Global,   Multiethnic Population (Study Protocol)
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-de452fb78ddc8091a4e3ca8421b239de.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-10  Exploring Superior Function Calls via Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
